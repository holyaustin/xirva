[{"id": "1609.00074", "submitter": "Junqi Jin", "authors": "Junqi Jin, Ziang Yan, Kun Fu, Nan Jiang, Changshui Zhang", "title": "Neural Network Architecture Optimization through Submodularity and\n  Supermodularity", "comments": "Withdrawn due to incompleteness and some overlaps with existing\n  literatures, I will resubmit adding further results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models' architectures, including depth and width, are key\nfactors influencing models' performance, such as test accuracy and computation\ntime. This paper solves two problems: given computation time budget, choose an\narchitecture to maximize accuracy, and given accuracy requirement, choose an\narchitecture to minimize computation time. We convert this architecture\noptimization into a subset selection problem. With accuracy's submodularity and\ncomputation time's supermodularity, we propose efficient greedy optimization\nalgorithms. The experiments demonstrate our algorithm's ability to find more\naccurate models or faster models. By analyzing architecture evolution with\ngrowing time budget, we discuss relationships among accuracy, time and\narchitecture, and give suggestions on neural network architecture design.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 00:59:30 GMT"}, {"version": "v2", "created": "Sun, 19 Mar 2017 13:23:34 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 03:45:19 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Jin", "Junqi", ""], ["Yan", "Ziang", ""], ["Fu", "Kun", ""], ["Jiang", "Nan", ""], ["Zhang", "Changshui", ""]]}, {"id": "1609.00085", "submitter": "Rajasekar Venkatesan", "authors": "Rajasekar Venkatesan, Meng Joo Er", "title": "A Novel Progressive Learning Technique for Multi-class Classification", "comments": "23 pages, 13 tables, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a progressive learning technique for multi-class\nclassification is proposed. This newly developed learning technique is\nindependent of the number of class constraints and it can learn new classes\nwhile still retaining the knowledge of previous classes. Whenever a new class\n(non-native to the knowledge learnt thus far) is encountered, the neural\nnetwork structure gets remodeled automatically by facilitating new neurons and\ninterconnections, and the parameters are calculated in such a way that it\nretains the knowledge learnt thus far. This technique is suitable for\nreal-world applications where the number of classes is often unknown and online\nlearning from real-time data is required. The consistency and the complexity of\nthe progressive learning technique are analyzed. Several standard datasets are\nused to evaluate the performance of the developed technique. A comparative\nstudy shows that the developed technique is superior.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 01:50:18 GMT"}, {"version": "v2", "created": "Sun, 22 Jan 2017 09:52:06 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Venkatesan", "Rajasekar", ""], ["Er", "Meng Joo", ""]]}, {"id": "1609.00086", "submitter": "Rajasekar Venkatesan", "authors": "Rajasekar Venkatesan, Meng Joo Er, Mihika Dave, Mahardhika Pratama,\n  Shiqian Wu", "title": "A novel online multi-label classifier for high-speed streaming data\n  applications", "comments": "18 pages, 7 tables, 3 figures. arXiv admin note: text overlap with\n  arXiv:1608.08898", "journal-ref": null, "doi": "10.1007/s12530-016-9162-8", "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a high-speed online neural network classifier based on extreme\nlearning machines for multi-label classification is proposed. In multi-label\nclassification, each of the input data sample belongs to one or more than one\nof the target labels. The traditional binary and multi-class classification\nwhere each sample belongs to only one target class forms the subset of\nmulti-label classification. Multi-label classification problems are far more\ncomplex than binary and multi-class classification problems, as both the number\nof target labels and each of the target labels corresponding to each of the\ninput samples are to be identified. The proposed work exploits the high-speed\nnature of the extreme learning machines to achieve real-time multi-label\nclassification of streaming data. A new threshold-based online sequential\nlearning algorithm is proposed for high speed and streaming data classification\nof multi-label problems. The proposed method is experimented with six different\ndatasets from different application domains such as multimedia, text, and\nbiology. The hamming loss, accuracy, training time and testing time of the\nproposed technique is compared with nine different state-of-the-art methods.\nExperimental studies shows that the proposed technique outperforms the existing\nmulti-label classifiers in terms of performance and speed.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 01:58:50 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Venkatesan", "Rajasekar", ""], ["Er", "Meng Joo", ""], ["Dave", "Mihika", ""], ["Pratama", "Mahardhika", ""], ["Wu", "Shiqian", ""]]}, {"id": "1609.00116", "submitter": "Martin  Biehl", "authors": "Nicholas Guttenberg, Martin Biehl, Ryota Kanai", "title": "Neural Coarse-Graining: Extracting slowly-varying latent degrees of\n  freedom with neural networks", "comments": "9 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a loss function for neural networks that encompasses an idea of\ntrivial versus non-trivial predictions, such that the network jointly\ndetermines its own prediction goals and learns to satisfy them. This permits\nthe network to choose sub-sets of a problem which are most amenable to its\nabilities to focus on solving, while discarding 'distracting' elements that\ninterfere with its learning. To do this, the network first transforms the raw\ndata into a higher-level categorical representation, and then trains a\npredictor from that new time series to its future. To prevent a trivial\nsolution of mapping the signal to zero, we introduce a measure of\nnon-triviality via a contrast between the prediction error of the learned model\nwith a naive model of the overall signal statistics. The transform can learn to\ndiscard uninformative and unpredictable components of the signal in favor of\nthe features which are both highly predictive and highly predictable. This\ncreates a coarse-grained model of the time-series dynamics, focusing on\npredicting the slowly varying latent parameters which control the statistics of\nthe time-series, rather than predicting the fast details directly. The result\nis a semi-supervised algorithm which is capable of extracting latent\nparameters, segmenting sections of time-series with differing statistics, and\nbuilding a higher-level representation of the underlying dynamics from\nunlabeled data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 05:34:23 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Guttenberg", "Nicholas", ""], ["Biehl", "Martin", ""], ["Kanai", "Ryota", ""]]}, {"id": "1609.00150", "submitter": "Mohammad Norouzi", "authors": "Mohammad Norouzi, Samy Bengio, Zhifeng Chen, Navdeep Jaitly, Mike\n  Schuster, Yonghui Wu, Dale Schuurmans", "title": "Reward Augmented Maximum Likelihood for Neural Structured Prediction", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key problem in structured output prediction is direct optimization of the\ntask reward function that matters for test evaluation. This paper presents a\nsimple and computationally efficient approach to incorporate task reward into a\nmaximum likelihood framework. By establishing a link between the log-likelihood\nand expected reward objectives, we show that an optimal regularized expected\nreward is achieved when the conditional distribution of the outputs given the\ninputs is proportional to their exponentiated scaled rewards. Accordingly, we\npresent a framework to smooth the predictive probability of the outputs using\ntheir corresponding rewards. We optimize the conditional log-probability of\naugmented outputs that are sampled proportionally to their exponentiated scaled\nrewards. Experiments on neural sequence to sequence models for speech\nrecognition and machine translation show notable improvements over a maximum\nlikelihood baseline by using reward augmented maximum likelihood (RAML), where\nthe rewards are defined as the negative edit distance between the outputs and\nthe ground truth labels.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 09:00:19 GMT"}, {"version": "v2", "created": "Sat, 10 Sep 2016 01:07:04 GMT"}, {"version": "v3", "created": "Wed, 4 Jan 2017 18:10:36 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Norouzi", "Mohammad", ""], ["Bengio", "Samy", ""], ["Chen", "Zhifeng", ""], ["Jaitly", "Navdeep", ""], ["Schuster", "Mike", ""], ["Wu", "Yonghui", ""], ["Schuurmans", "Dale", ""]]}, {"id": "1609.00203", "submitter": "Angelos Valsamis", "authors": "Angelos Valsamis, Konstantinos Tserpes, Dimitrios Zissis, Dimosthenis\n  Anagnostopoulos, Theodora Varvarigou", "title": "Employing traditional machine learning algorithms for big data streams\n  analysis: the case of object trajectory prediction", "comments": "14 pages, 2 figures, 3 tables, 31 references", "journal-ref": null, "doi": "10.1016/j.jss.2016.06.016", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we model the trajectory of sea vessels and provide a service\nthat predicts in near-real time the position of any given vessel in 4', 10',\n20' and 40' time intervals. We explore the necessary tradeoffs between\naccuracy, performance and resource utilization are explored given the large\nvolume and update rates of input data. We start with building models based on\nwell-established machine learning algorithms using static datasets and\nmulti-scan training approaches and identify the best candidate to be used in\nimplementing a single-pass predictive approach, under real-time constraints.\nThe results are measured in terms of accuracy and performance and are compared\nagainst the baseline kinematic equations. Results show that it is possible to\nefficiently model the trajectory of multiple vessels using a single model,\nwhich is trained and evaluated using an adequately large, static dataset, thus\nachieving a significant gain in terms of resource usage while not compromising\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 12:06:20 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Valsamis", "Angelos", ""], ["Tserpes", "Konstantinos", ""], ["Zissis", "Dimitrios", ""], ["Anagnostopoulos", "Dimosthenis", ""], ["Varvarigou", "Theodora", ""]]}, {"id": "1609.00222", "submitter": "Hande Alemdar", "authors": "Hande Alemdar and Vincent Leroy and Adrien Prost-Boucle and\n  Fr\\'ed\\'eric P\\'etrot", "title": "Ternary Neural Networks for Resource-Efficient AI Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation and storage requirements for Deep Neural Networks (DNNs) are\nusually high. This issue limits their deployability on ubiquitous computing\ndevices such as smart phones, wearables and autonomous drones. In this paper,\nwe propose ternary neural networks (TNNs) in order to make deep learning more\nresource-efficient. We train these TNNs using a teacher-student approach based\non a novel, layer-wise greedy methodology. Thanks to our two-stage training\nprocedure, the teacher network is still able to use state-of-the-art methods\nsuch as dropout and batch normalization to increase accuracy and reduce\ntraining time. Using only ternary weights and activations, the student ternary\nnetwork learns to mimic the behavior of its teacher network without using any\nmultiplication. Unlike its -1,1 binary counterparts, a ternary neural network\ninherently prunes the smaller weights by setting them to zero during training.\nThis makes them sparser and thus more energy-efficient. We design a\npurpose-built hardware architecture for TNNs and implement it on FPGA and ASIC.\nWe evaluate TNNs on several benchmark datasets and demonstrate up to 3.1x\nbetter energy efficiency with respect to the state of the art while also\nimproving accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 13:08:47 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 09:44:34 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Alemdar", "Hande", ""], ["Leroy", "Vincent", ""], ["Prost-Boucle", "Adrien", ""], ["P\u00e9trot", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1609.00265", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement L. Canonne, Elena Grigorescu, Siyao Guo, Akash Kumar, Karl\n  Wimmer", "title": "Testing $k$-Monotonicity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Boolean $k$-monotone function defined over a finite poset domain ${\\cal D}$\nalternates between the values $0$ and $1$ at most $k$ times on any ascending\nchain in ${\\cal D}$. Therefore, $k$-monotone functions are natural\ngeneralizations of the classical monotone functions, which are the $1$-monotone\nfunctions. Motivated by the recent interest in $k$-monotone functions in the\ncontext of circuit complexity and learning theory, and by the central role that\nmonotonicity testing plays in the context of property testing, we initiate a\nsystematic study of $k$-monotone functions, in the property testing model. In\nthis model, the goal is to distinguish functions that are $k$-monotone (or are\nclose to being $k$-monotone) from functions that are far from being\n$k$-monotone. Our results include the following:\n  - We demonstrate a separation between testing $k$-monotonicity and testing\nmonotonicity, on the hypercube domain $\\{0,1\\}^d$, for $k\\geq 3$;\n  - We demonstrate a separation between testing and learning on $\\{0,1\\}^d$,\nfor $k=\\omega(\\log d)$: testing $k$-monotonicity can be performed with\n$2^{O(\\sqrt d \\cdot \\log d\\cdot \\log{1/\\varepsilon})}$ queries, while learning\n$k$-monotone functions requires $2^{\\Omega(k\\cdot \\sqrt\nd\\cdot{1/\\varepsilon})}$ queries (Blais et al. (RANDOM 2015)).\n  - We present a tolerant test for functions $f\\colon[n]^d\\to \\{0,1\\}$ with\ncomplexity independent of $n$, which makes progress on a problem left open by\nBerman et al. (STOC 2014).\n  Our techniques exploit the testing-by-learning paradigm, use novel\napplications of Fourier analysis on the grid $[n]^d$, and draw connections to\ndistribution testing techniques.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 15:11:52 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 18:53:51 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["Grigorescu", "Elena", ""], ["Guo", "Siyao", ""], ["Kumar", "Akash", ""], ["Wimmer", "Karl", ""]]}, {"id": "1609.00288", "submitter": "Zhi-Hua Zhou", "authors": "Xi-Zhu Wu and Zhi-Hua Zhou", "title": "A Unified View of Multi-Label Performance Measures", "comments": null, "journal-ref": "ICML 2017", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification deals with the problem where each instance is\nassociated with multiple class labels. Because evaluation in multi-label\nclassification is more complicated than single-label setting, a number of\nperformance measures have been proposed. It is noticed that an algorithm\nusually performs differently on different measures. Therefore, it is important\nto understand which algorithms perform well on which measure(s) and why. In\nthis paper, we propose a unified margin view to revisit eleven performance\nmeasures in multi-label classification. In particular, we define label-wise\nmargin and instance-wise margin, and prove that through maximizing these\nmargins, different corresponding performance measures will be optimized. Based\non the defined margins, a max-margin approach called LIMO is designed and\nempirical results verify our theoretical findings.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 15:49:43 GMT"}, {"version": "v2", "created": "Fri, 1 Sep 2017 08:18:33 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Wu", "Xi-Zhu", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1609.00451", "submitter": "Mauricio Sadinle", "authors": "Mauricio Sadinle, Jing Lei, Larry Wasserman", "title": "Least Ambiguous Set-Valued Classifiers with Bounded Error Levels", "comments": "Final version to be published in the Journal of the American\n  Statistical Association at\n  https://www.tandfonline.com/doi/abs/10.1080/01621459.2017.1395341?journalCode=uasa20", "journal-ref": null, "doi": "10.1080/01621459.2017.1395341", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most classification tasks there are observations that are ambiguous and\ntherefore difficult to correctly label. Set-valued classifiers output sets of\nplausible labels rather than a single label, thereby giving a more appropriate\nand informative treatment to the labeling of ambiguous instances. We introduce\na framework for multiclass set-valued classification, where the classifiers\nguarantee user-defined levels of coverage or confidence (the probability that\nthe true label is contained in the set) while minimizing the ambiguity (the\nexpected size of the output). We first derive oracle classifiers assuming the\ntrue distribution to be known. We show that the oracle classifiers are obtained\nfrom level sets of the functions that define the conditional probability of\neach class. Then we develop estimators with good asymptotic and finite sample\nproperties. The proposed estimators build on existing single-label classifiers.\nThe optimal classifier can sometimes output the empty set, but we provide two\nsolutions to fix this issue that are suitable for various practical needs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 02:46:45 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2018 20:39:01 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Sadinle", "Mauricio", ""], ["Lei", "Jing", ""], ["Wasserman", "Larry", ""]]}, {"id": "1609.00489", "submitter": "Truyen Tran", "authors": "Morakot Choetkiertikul, Hoa Khanh Dam, Truyen Tran, Trang Pham, Aditya\n  Ghose and Tim Menzies", "title": "A deep learning model for estimating story points", "comments": "Submitted to ICSE'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there has been substantial research in software analytics for effort\nestimation in traditional software projects, little work has been done for\nestimation in agile projects, especially estimating user stories or issues.\nStory points are the most common unit of measure used for estimating the effort\ninvolved in implementing a user story or resolving an issue. In this paper, we\noffer for the \\emph{first} time a comprehensive dataset for story points-based\nestimation that contains 23,313 issues from 16 open source projects. We also\npropose a prediction model for estimating story points based on a novel\ncombination of two powerful deep learning architectures: long short-term memory\nand recurrent highway network. Our prediction system is \\emph{end-to-end}\ntrainable from raw input data to prediction outcomes without any manual feature\nengineering. An empirical evaluation demonstrates that our approach\nconsistently outperforms three common effort estimation baselines and two\nalternatives in both Mean Absolute Error and the Standardized Accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 07:42:29 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 06:18:04 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Choetkiertikul", "Morakot", ""], ["Dam", "Hoa Khanh", ""], ["Tran", "Truyen", ""], ["Pham", "Trang", ""], ["Ghose", "Aditya", ""], ["Menzies", "Tim", ""]]}, {"id": "1609.00585", "submitter": "Nikolaas Steenbergen", "authors": "Nikolaas Steenbergen, Sebastian Schelter, Felix Bie{\\ss}mann", "title": "Doubly stochastic large scale kernel learning with the empirical kernel\n  map", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of big data sets, the popularity of kernel methods declined and\nneural networks took over again. The main problem with kernel methods is that\nthe kernel matrix grows quadratically with the number of data points. Most\nattempts to scale up kernel methods solve this problem by discarding data\npoints or basis functions of some approximation of the kernel map. Here we\npresent a simple yet effective alternative for scaling up kernel methods that\ntakes into account the entire data set via doubly stochastic optimization of\nthe emprical kernel map. The algorithm is straightforward to implement, in\nparticular in parallel execution settings; it leverages the full power and\nversatility of classical kernel functions without the need to explicitly\nformulate a kernel map approximation. We provide empirical evidence that the\nalgorithm works on large data sets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 13:20:06 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 11:58:08 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Steenbergen", "Nikolaas", ""], ["Schelter", "Sebastian", ""], ["Bie\u00dfmann", "Felix", ""]]}, {"id": "1609.00629", "submitter": "Elad Richardson", "authors": "Elad Richardson, Rom Herskovitz, Boris Ginsburg, Michael Zibulevsky", "title": "SEBOOST - Boosting Stochastic Learning Using Subspace Optimization\n  Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SEBOOST, a technique for boosting the performance of existing\nstochastic optimization methods. SEBOOST applies a secondary optimization\nprocess in the subspace spanned by the last steps and descent directions. The\nmethod was inspired by the SESOP optimization method for large-scale problems,\nand has been adapted for the stochastic learning framework. It can be applied\non top of any existing optimization method with no need to tweak the internal\nalgorithm. We show that the method is able to boost the performance of\ndifferent algorithms, and make them more robust to changes in their\nhyper-parameters. As the boosting steps of SEBOOST are applied between large\nsets of descent steps, the additional subspace optimization hardly increases\nthe overall computational burden. We introduce two hyper-parameters that\ncontrol the balance between the baseline method and the secondary optimization\nprocess. The method was evaluated on several deep learning tasks, demonstrating\npromising results.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 14:48:16 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Richardson", "Elad", ""], ["Herskovitz", "Rom", ""], ["Ginsburg", "Boris", ""], ["Zibulevsky", "Michael", ""]]}, {"id": "1609.00680", "submitter": "Jinbo Xu", "authors": "Sheng Wang, Siqi Sun, Zhen Li, Renyu Zhang and Jinbo Xu", "title": "Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep\n  Learning Model", "comments": null, "journal-ref": "PLoS Comput Biol 13(1): e1005324, 2017", "doi": "10.1371/journal.pcbi.1005324", "report-no": null, "categories": "q-bio.BM cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently exciting progress has been made on protein contact prediction, but\nthe predicted contacts for proteins without many sequence homologs is still of\nlow quality and not very useful for de novo structure prediction. This paper\npresents a new deep learning method that predicts contacts by integrating both\nevolutionary coupling (EC) and sequence conservation information through an\nultra-deep neural network formed by two deep residual networks. This deep\nneural network allows us to model very complex sequence-contact relationship as\nwell as long-range inter-contact correlation. Our method greatly outperforms\nexisting contact prediction methods and leads to much more accurate\ncontact-assisted protein folding. Tested on three datasets of 579 proteins, the\naverage top L long-range prediction accuracy obtained our method, the\nrepresentative EC method CCMpred and the CASP11 winner MetaPSICOV is 0.47, 0.21\nand 0.30, respectively; the average top L/10 long-range accuracy of our method,\nCCMpred and MetaPSICOV is 0.77, 0.47 and 0.59, respectively. Ab initio folding\nusing our predicted contacts as restraints can yield correct folds (i.e.,\nTMscore>0.6) for 203 test proteins, while that using MetaPSICOV- and\nCCMpred-predicted contacts can do so for only 79 and 62 proteins, respectively.\nFurther, our contact-assisted models have much better quality than\ntemplate-based models. Using our predicted contacts as restraints, we can (ab\ninitio) fold 208 of the 398 membrane proteins with TMscore>0.5. By contrast,\nwhen the training proteins of our method are used as templates, homology\nmodeling can only do so for 10 of them. One interesting finding is that even if\nwe do not train our prediction models with any membrane proteins, our method\nworks very well on membrane protein prediction. Finally, in recent blind CAMEO\nbenchmark our method successfully folded 5 test proteins with a novel fold.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 17:41:54 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 15:39:23 GMT"}, {"version": "v3", "created": "Thu, 15 Sep 2016 03:09:45 GMT"}, {"version": "v4", "created": "Fri, 16 Sep 2016 23:08:52 GMT"}, {"version": "v5", "created": "Mon, 7 Nov 2016 06:01:32 GMT"}, {"version": "v6", "created": "Sun, 27 Nov 2016 22:32:50 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Wang", "Sheng", ""], ["Sun", "Siqi", ""], ["Li", "Zhen", ""], ["Zhang", "Renyu", ""], ["Xu", "Jinbo", ""]]}, {"id": "1609.00686", "submitter": "Makoto Naruse", "authors": "Makoto Naruse, Martin Berthel, Aur\\'elien Drezet, Serge Huant,\n  Hirokazu Hori, Song-Ju Kim", "title": "Single photon in hierarchical architecture for physical reinforcement\n  learning: Photon intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.optics quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and using natural processes for intelligent functionalities,\nreferred to as natural intelligence, has recently attracted interest from a\nvariety of fields, including post-silicon computing for artificial intelligence\nand decision making in the behavioural sciences. In a past study, we\nsuccessfully used the wave-particle duality of single photons to solve the\ntwo-armed bandit problem, which constitutes the foundation of reinforcement\nlearning and decision making. In this study, we propose and confirm a\nhierarchical architecture for single-photon-based reinforcement learning and\ndecision making that verifies the scalability of the principle. Specifically,\nthe four-armed bandit problem is solved given zero prior knowledge in a\ntwo-layer hierarchical architecture, where polarization is autonomously adapted\nin order to effect adequate decision making using single-photon measurements.\nIn the hierarchical structure, the notion of layer-dependent decisions emerges.\nThe optimal solutions in the coarse layer and in the fine layer, however,\nconflict with each other in some contradictive problems. We show that while\nwhat we call a tournament strategy resolves such contradictions, the\nprobabilistic nature of single photons allows for the direct location of the\noptimal solution even for contradictive problems, hence manifesting the\nexploration ability of single photons. This study provides insights into photon\nintelligence in hierarchical architectures for future artificial intelligence\nas well as the potential of natural processes for intelligent functionalities.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 09:32:29 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Naruse", "Makoto", ""], ["Berthel", "Martin", ""], ["Drezet", "Aur\u00e9lien", ""], ["Huant", "Serge", ""], ["Hori", "Hirokazu", ""], ["Kim", "Song-Ju", ""]]}, {"id": "1609.00718", "submitter": "Rie Johnson", "authors": "Rie Johnson and Tong Zhang", "title": "Convolutional Neural Networks for Text Categorization: Shallow\n  Word-level vs. Deep Character-level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports the performances of shallow word-level convolutional\nneural networks (CNN), our earlier work (2015), on the eight datasets with\nrelatively large training data that were used for testing the very deep\ncharacter-level CNN in Conneau et al. (2016). Our findings are as follows. The\nshallow word-level CNNs achieve better error rates than the error rates\nreported in Conneau et al., though the results should be interpreted with some\nconsideration due to the unique pre-processing of Conneau et al. The shallow\nword-level CNN uses more parameters and therefore requires more storage than\nthe deep character-level CNN; however, the shallow word-level CNN computes much\nfaster.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 15:43:27 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Johnson", "Rie", ""], ["Zhang", "Tong", ""]]}, {"id": "1609.00777", "submitter": "Bhuwan Dhingra", "authors": "Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao, Yun-Nung Chen,\n  Faisal Ahmed, Li Deng", "title": "Towards End-to-End Reinforcement Learning of Dialogue Agents for\n  Information Access", "comments": "Accepted at ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes KB-InfoBot -- a multi-turn dialogue agent which helps\nusers search Knowledge Bases (KBs) without composing complicated queries. Such\ngoal-oriented dialogue agents typically need to interact with an external\ndatabase to access real-world knowledge. Previous systems achieved this by\nissuing a symbolic query to the KB to retrieve entries based on their\nattributes. However, such symbolic operations break the differentiability of\nthe system and prevent end-to-end training of neural dialogue agents. In this\npaper, we address this limitation by replacing symbolic queries with an induced\n\"soft\" posterior distribution over the KB that indicates which entities the\nuser is interested in. Integrating the soft retrieval process with a\nreinforcement learner leads to higher task success rate and reward in both\nsimulations and against real users. We also present a fully neural end-to-end\nagent, trained entirely from user feedback, and discuss its application towards\npersonalized dialogue agents. The source code is available at\nhttps://github.com/MiuLab/KB-InfoBot.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2016 01:02:51 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 21:39:31 GMT"}, {"version": "v3", "created": "Thu, 20 Apr 2017 17:26:35 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Dhingra", "Bhuwan", ""], ["Li", "Lihong", ""], ["Li", "Xiujun", ""], ["Gao", "Jianfeng", ""], ["Chen", "Yun-Nung", ""], ["Ahmed", "Faisal", ""], ["Deng", "Li", ""]]}, {"id": "1609.00804", "submitter": "Battista Biggio", "authors": "Samuel Rota Bul\\`o and Battista Biggio and Ignazio Pillai and Marcello\n  Pelillo and Fabio Roli", "title": "Randomized Prediction Games for Adversarial Machine Learning", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 2016", "doi": "10.1109/TNNLS.2016.2593488", "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spam and malware detection, attackers exploit randomization to obfuscate\nmalicious data and increase their chances of evading detection at test time;\ne.g., malware code is typically obfuscated using random strings or byte\nsequences to hide known exploits. Interestingly, randomization has also been\nproposed to improve security of learning algorithms against evasion attacks, as\nit results in hiding information about the classifier to the attacker. Recent\nwork has proposed game-theoretical formulations to learn secure classifiers, by\nsimulating different evasion attacks and modifying the classification function\naccordingly. However, both the classification function and the simulated data\nmanipulations have been modeled in a deterministic manner, without accounting\nfor any form of randomization. In this work, we overcome this limitation by\nproposing a randomized prediction game, namely, a non-cooperative\ngame-theoretic formulation in which the classifier and the attacker make\nrandomized strategy selections according to some probability distribution\ndefined over the respective strategy set. We show that our approach allows one\nto improve the trade-off between attack detection and false alarms with respect\nto state-of-the-art secure classifiers, even against attacks that are different\nfrom those hypothesized during design, on application examples including\nhandwritten digit recognition, spam and malware detection.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2016 09:30:51 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Bul\u00f2", "Samuel Rota", ""], ["Biggio", "Battista", ""], ["Pillai", "Ignazio", ""], ["Pelillo", "Marcello", ""], ["Roli", "Fabio", ""]]}, {"id": "1609.00843", "submitter": "Rajasekar Venkatesan", "authors": "Meng Joo Er, Rajasekar Venkatesan, Ning Wang", "title": "An Online Universal Classifier for Binary, Multi-class and Multi-label\n  Classification", "comments": "6 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification involves the learning of the mapping function that associates\ninput samples to corresponding target label. There are two major categories of\nclassification problems: Single-label classification and Multi-label\nclassification. Traditional binary and multi-class classifications are\nsub-categories of single-label classification. Several classifiers are\ndeveloped for binary, multi-class and multi-label classification problems, but\nthere are no classifiers available in the literature capable of performing all\nthree types of classification. In this paper, a novel online universal\nclassifier capable of performing all the three types of classification is\nproposed. Being a high speed online classifier, the proposed technique can be\napplied to streaming data applications. The performance of the developed\nclassifier is evaluated using datasets from binary, multi-class and multi-label\nproblems. The results obtained are compared with state-of-the-art techniques\nfrom each of the classification types.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2016 17:03:14 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Er", "Meng Joo", ""], ["Venkatesan", "Rajasekar", ""], ["Wang", "Ning", ""]]}, {"id": "1609.00845", "submitter": "Kwang-Sung Jun", "authors": "Kwang-Sung Jun and Robert Nowak", "title": "Graph-Based Active Learning: A New Look at Expected Error Minimization", "comments": "Submitted to GlobalSIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In graph-based active learning, algorithms based on expected error\nminimization (EEM) have been popular and yield good empirical performance. The\nexact computation of EEM optimally balances exploration and exploitation. In\npractice, however, EEM-based algorithms employ various approximations due to\nthe computational hardness of exact EEM. This can result in a lack of either\nexploration or exploitation, which can negatively impact the effectiveness of\nactive learning. We propose a new algorithm TSA (Two-Step Approximation) that\nbalances between exploration and exploitation efficiently while enjoying the\nsame computational complexity as existing approximations. Finally, we\nempirically show the value of balancing between exploration and exploitation in\nboth toy and real-world datasets where our method outperforms several\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2016 17:30:15 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Jun", "Kwang-Sung", ""], ["Nowak", "Robert", ""]]}, {"id": "1609.00878", "submitter": "Joao Papa", "authors": "Silas E. N. Fernandes, Danillo R. Pereira, Caio C. O. Ramos, Andre N.\n  Souza and Joao P. Papa", "title": "A Probabilistic Optimum-Path Forest Classifier for Binary Classification\n  Problems", "comments": "Submitted to Neural Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic-driven classification techniques extend the role of traditional\napproaches that output labels (usually integer numbers) only. Such techniques\nare more fruitful when dealing with problems where one is not interested in\nrecognition/identification only, but also into monitoring the behavior of\nconsumers and/or machines, for instance. Therefore, by means of probability\nestimates, one can take decisions to work better in a number of scenarios. In\nthis paper, we propose a probabilistic-based Optimum Path Forest (OPF)\nclassifier to handle with binary classification problems, and we show it can be\nmore accurate than naive OPF in a number of datasets. In addition to being just\nmore accurate or not, probabilistic OPF turns to be another useful tool to the\nscientific community.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 00:12:04 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Fernandes", "Silas E. N.", ""], ["Pereira", "Danillo R.", ""], ["Ramos", "Caio C. O.", ""], ["Souza", "Andre N.", ""], ["Papa", "Joao P.", ""]]}, {"id": "1609.00904", "submitter": "Eric Holloway", "authors": "Eric Holloway and Robert Marks II", "title": "High Dimensional Human Guided Machine Learning", "comments": "3 pages, 1 figure, HCOMP 2016 submission, work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Have you ever looked at a machine learning classification model and thought,\nI could have made that? Well, that is what we test in this project, comparing\nXGBoost trained on human engineered features to training directly on data. The\nhuman engineered features do not outperform XGBoost trained di- rectly on the\ndata, but they are comparable. This project con- tributes a novel method for\nutilizing human created classifi- cation models on high dimensional datasets.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 08:45:26 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Holloway", "Eric", ""], ["Marks", "Robert", "II"]]}, {"id": "1609.00921", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad and Daoqiang Zhang", "title": "Decoding visual stimuli in human brain by using Anatomical Pattern\n  Analysis on fMRI images", "comments": "The 8th International Conference on Brain Inspired Cognitive Systems\n  (BICS'16), Beijing, China, Nov/28-30/2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A universal unanswered question in neuroscience and machine learning is\nwhether computers can decode the patterns of the human brain. Multi-Voxels\nPattern Analysis (MVPA) is a critical tool for addressing this question.\nHowever, there are two challenges in the previous MVPA methods, which include\ndecreasing sparsity and noises in the extracted features and increasing the\nperformance of prediction. In overcoming mentioned challenges, this paper\nproposes Anatomical Pattern Analysis (APA) for decoding visual stimuli in the\nhuman brain. This framework develops a novel anatomical feature extraction\nmethod and a new imbalance AdaBoost algorithm for binary classification.\nFurther, it utilizes an Error-Correcting Output Codes (ECOC) method for\nmulti-class prediction. APA can automatically detect active regions for each\ncategory of the visual stimuli. Moreover, it enables us to combine homogeneous\ndatasets for applying advanced classification. Experimental studies on 4 visual\ncategories (words, consonants, objects and scrambled photos) demonstrate that\nthe proposed approach achieves superior performance to state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 12:01:50 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "1609.00932", "submitter": "Hao Wu", "authors": "Hao Wu and Frank No\\'e", "title": "Spectral learning of dynamic systems from nonequilibrium data", "comments": null, "journal-ref": "Proceedings of the 29th conference on Neural Information\n  Processing Systems (NIPS), Barcelona, Spain, 2016, pp. 4179-4187", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY math.PR physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observable operator models (OOMs) and related models are one of the most\nimportant and powerful tools for modeling and analyzing stochastic systems.\nThey exactly describe dynamics of finite-rank systems and can be efficiently\nand consistently estimated through spectral learning under the assumption of\nidentically distributed data. In this paper, we investigate the properties of\nspectral learning without this assumption due to the requirements of analyzing\nlarge-time scale systems, and show that the equilibrium dynamics of a system\ncan be extracted from nonequilibrium observation data by imposing an\nequilibrium constraint. In addition, we propose a binless extension of spectral\nlearning for continuous data. In comparison with the other continuous-valued\nspectral algorithms, the binless algorithm can achieve consistent estimation of\nequilibrium dynamics with only linear complexity.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 13:31:36 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 20:30:33 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Wu", "Hao", ""], ["No\u00e9", "Frank", ""]]}, {"id": "1609.00978", "submitter": "Chi Jin", "authors": "Chi Jin, Yuchen Zhang, Sivaraman Balakrishnan, Martin J. Wainwright,\n  Michael Jordan", "title": "Local Maxima in the Likelihood of Gaussian Mixture Models: Structural\n  Results and Algorithmic Consequences", "comments": "Neural Information Processing Systems (NIPS) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide two fundamental results on the population (infinite-sample)\nlikelihood function of Gaussian mixture models with $M \\geq 3$ components. Our\nfirst main result shows that the population likelihood function has bad local\nmaxima even in the special case of equally-weighted mixtures of well-separated\nand spherical Gaussians. We prove that the log-likelihood value of these bad\nlocal maxima can be arbitrarily worse than that of any global optimum, thereby\nresolving an open question of Srebro (2007). Our second main result shows that\nthe EM algorithm (or a first-order variant of it) with random initialization\nwill converge to bad critical points with probability at least\n$1-e^{-\\Omega(M)}$. We further establish that a first-order variant of EM will\nnot converge to strict saddle points almost surely, indicating that the poor\nperformance of the first-order method can be attributed to the existence of bad\nlocal maxima rather than bad saddle points. Overall, our results highlight the\nnecessity of careful initialization when using the EM algorithm in practice,\neven when applied in highly favorable settings.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 19:34:56 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Jin", "Chi", ""], ["Zhang", "Yuchen", ""], ["Balakrishnan", "Sivaraman", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael", ""]]}, {"id": "1609.01000", "submitter": "Yuchen Zhang", "authors": "Yuchen Zhang, Percy Liang, Martin J. Wainwright", "title": "Convexified Convolutional Neural Networks", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the class of convexified convolutional neural networks (CCNNs),\nwhich capture the parameter sharing of convolutional neural networks in a\nconvex manner. By representing the nonlinear convolutional filters as vectors\nin a reproducing kernel Hilbert space, the CNN parameters can be represented as\na low-rank matrix, which can be relaxed to obtain a convex optimization\nproblem. For learning two-layer convolutional neural networks, we prove that\nthe generalization error obtained by a convexified CNN converges to that of the\nbest possible CNN. For learning deeper networks, we train CCNNs in a layer-wise\nmanner. Empirically, CCNNs achieve performance competitive with CNNs trained by\nbackpropagation, SVMs, fully-connected neural networks, stacked denoising\nauto-encoders, and other baseline methods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 23:57:43 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Zhang", "Yuchen", ""], ["Liang", "Percy", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1609.01037", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "Distribution-Specific Hardness of Learning Neural Networks", "comments": "Simpler and more explicit theorems in section 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although neural networks are routinely and successfully trained in practice\nusing simple gradient-based methods, most existing theoretical results are\nnegative, showing that learning such networks is difficult, in a worst-case\nsense over all data distributions. In this paper, we take a more nuanced view,\nand consider whether specific assumptions on the \"niceness\" of the input\ndistribution, or \"niceness\" of the target function (e.g. in terms of\nsmoothness, non-degeneracy, incoherence, random choice of parameters etc.), are\nsufficient to guarantee learnability using gradient-based methods. We provide\nevidence that neither class of assumptions alone is sufficient: On the one\nhand, for any member of a class of \"nice\" target functions, there are difficult\ninput distributions. On the other hand, we identify a family of simple target\nfunctions, which are difficult to learn even if the input distribution is\n\"nice\". To prove our results, we develop some tools which may be of independent\ninterest, such as extending Fourier-based hardness techniques developed in the\ncontext of statistical queries \\cite{blum1994weakly}, from the Boolean cube to\nEuclidean space and to more general classes of functions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 06:47:10 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 08:56:44 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1609.01044", "submitter": "Janne V. Kujala", "authors": "Janne V. Kujala, Tuomas J. Lukka, Harri Holopainen", "title": "Classifying and sorting cluttered piles of unknown objects with robots:\n  a learning approach", "comments": "8 pages, 14 figures (pagination changed in arXiv version); accepted\n  for the International Conference on Intelligent Robots and Systems (IROS)\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sorting a densely cluttered pile of unknown\nobjects using a robot. This yet unsolved problem is relevant in the robotic\nwaste sorting business.\n  By extending previous active learning approaches to grasping, we show a\nsystem that learns the task autonomously. Instead of predicting just whether a\ngrasp succeeds, we predict the classes of the objects that end up being picked\nand thrown onto the target conveyor. Segmenting and identifying objects from\nthe uncluttered target conveyor, as opposed to the working area, is easier due\nto the added structure since the thrown objects will be the only ones present.\n  Instead of trying to segment or otherwise understand the cluttered working\narea in any way, we simply allow the controller to learn a mapping from an RGBD\nimage in the neighborhood of the grasp to a predicted result---all segmentation\netc. in the working area is implicit in the learned function. The grasp\nselection operates in two stages: The first stage is hardcoded and outputs a\ndistribution of possible grasps that sometimes succeed. The second stage uses a\npurely learned criterion to choose the grasp to make from the proposal\ndistribution created by the first stage.\n  In an experiment, the system quickly learned to make good pickups and predict\ncorrectly, in advance, which class of object it was going to pick up and was\nable to sort the objects from a densely cluttered pile by color.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 07:44:40 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Kujala", "Janne V.", ""], ["Lukka", "Tuomas J.", ""], ["Holopainen", "Harri", ""]]}, {"id": "1609.01176", "submitter": "Lucas Maystre", "authors": "Lucas Maystre, Victor Kristof, Antonio J. Gonz\\'alez Ferrer, Matthias\n  Grossglauser", "title": "The Player Kernel: Learning Team Strengths Based on Implicit Player\n  Contributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we draw attention to a connection between skill-based models of\ngame outcomes and Gaussian process classification models. The Gaussian process\nperspective enables a) a principled way of dealing with uncertainty and b) rich\nmodels, specified through kernel functions. Using this connection, we tackle\nthe problem of predicting outcomes of football matches between national teams.\nWe develop a player kernel that relates any two football matches through the\nplayers lined up on the field. This makes it possible to share knowledge gained\nfrom observing matches between clubs (available in large quantities) and\nmatches between national teams (available only in limited quantities). We\nevaluate our approach on the Euro 2008, 2012 and 2016 final tournaments.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 14:21:04 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Maystre", "Lucas", ""], ["Kristof", "Victor", ""], ["Ferrer", "Antonio J. Gonz\u00e1lez", ""], ["Grossglauser", "Matthias", ""]]}, {"id": "1609.01203", "submitter": "L\\'eopold Crestel", "authors": "L\\'eopold Crestel and Philippe Esling", "title": "Live Orchestral Piano, a system for real-time orchestral music\n  generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the first system for performing automatic orchestration\nbased on a real-time piano input. We believe that it is possible to learn the\nunderlying regularities existing between piano scores and their orchestrations\nby notorious composers, in order to automatically perform this task on novel\npiano inputs. To that end, we investigate a class of statistical inference\nmodels called conditional Restricted Boltzmann Machine (cRBM). We introduce a\nspecific evaluation framework for orchestral generation based on a prediction\ntask in order to assess the quality of different models. As prediction and\ncreation are two widely different endeavours, we discuss the potential biases\nin evaluating temporal generative models through prediction tasks and their\nimpact on a creative system. Finally, we introduce an implementation of the\nproposed model called Live Orchestral Piano (LOP), which allows to perform\nreal-time projective orchestration of a MIDI keyboard input.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 15:58:11 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 14:15:30 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Crestel", "L\u00e9opold", ""], ["Esling", "Philippe", ""]]}, {"id": "1609.01226", "submitter": "Pingfan Tang", "authors": "Pingfan Tang, Jeff M. Phillips", "title": "The Robustness of Estimator Composition", "comments": "14 pages, 2 figures, 29th Conference on Neural Information Processing\n  Systems (NIPS 2016), Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize notions of robustness for composite estimators via the notion of\na breakdown point. A composite estimator successively applies two (or more)\nestimators: on data decomposed into disjoint parts, it applies the first\nestimator on each part, then the second estimator on the outputs of the first\nestimator. And so on, if the composition is of more than two estimators.\nInformally, the breakdown point is the minimum fraction of data points which if\nsignificantly modified will also significantly modify the output of the\nestimator, so it is typically desirable to have a large breakdown point. Our\nmain result shows that, under mild conditions on the individual estimators, the\nbreakdown point of the composite estimator is the product of the breakdown\npoints of the individual estimators. We also demonstrate several scenarios,\nranging from regression to statistical testing, where this analysis is easy to\napply, useful in understanding worst case robustness, and sheds powerful\ninsights onto the associated data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 17:27:22 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Tang", "Pingfan", ""], ["Phillips", "Jeff M.", ""]]}, {"id": "1609.01360", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee and Alexander Wong", "title": "Evolutionary Synthesis of Deep Neural Networks via Synaptic\n  Cluster-driven Genetic Encoding", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant recent interest towards achieving highly efficient\ndeep neural network architectures. A promising paradigm for achieving this is\nthe concept of evolutionary deep intelligence, which attempts to mimic\nbiological evolution processes to synthesize highly-efficient deep neural\nnetworks over successive generations. An important aspect of evolutionary deep\nintelligence is the genetic encoding scheme used to mimic heredity, which can\nhave a significant impact on the quality of offspring deep neural networks.\nMotivated by the neurobiological phenomenon of synaptic clustering, we\nintroduce a new genetic encoding scheme where synaptic probability is driven\ntowards the formation of a highly sparse set of synaptic clusters. Experimental\nresults for the task of image classification demonstrated that the synthesized\noffspring networks using this synaptic cluster-driven genetic encoding scheme\ncan achieve state-of-the-art performance while having network architectures\nthat are not only significantly more efficient (with a ~125-fold decrease in\nsynapses for MNIST) compared to the original ancestor network, but also\ntailored for GPU-accelerated machine learning applications.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 01:08:03 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 16:00:01 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Wong", "Alexander", ""]]}, {"id": "1609.01387", "submitter": "Ugo Rosolia", "authors": "Ugo Rosolia and Francesco Borrelli", "title": "Learning Model Predictive Control for iterative tasks. A Data-Driven\n  Control Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Learning Model Predictive Controller (LMPC) for iterative tasks is\npresented. The controller is reference-free and is able to improve its\nperformance by learning from previous iterations. A safe set and a terminal\ncost function are used in order to guarantee recursive feasibility and\nnon-increasing performance at each iteration. The paper presents the control\ndesign approach, and shows how to recursively construct terminal set and\nterminal cost from state and input trajectories of previous iterations.\nSimulation results show the effectiveness of the proposed control logic.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 04:21:33 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 10:12:57 GMT"}, {"version": "v3", "created": "Tue, 7 Feb 2017 18:08:33 GMT"}, {"version": "v4", "created": "Wed, 8 Feb 2017 05:00:17 GMT"}, {"version": "v5", "created": "Wed, 10 May 2017 04:18:54 GMT"}, {"version": "v6", "created": "Thu, 21 Sep 2017 04:29:56 GMT"}, {"version": "v7", "created": "Thu, 14 Dec 2017 00:23:58 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Rosolia", "Ugo", ""], ["Borrelli", "Francesco", ""]]}, {"id": "1609.01468", "submitter": "Kardi Teknomo", "authors": "Wilfredo Badoy Jr. and Kardi Teknomo", "title": "Q-Learning with Basic Emotions", "comments": "7 pages, Badoy, W. and Teknomo, K. (2014) Q-Learning with Basic\n  Emotions, Proceeding of the 7th IEEE International Conference Humanoid,\n  Nanotechnology, Information Technology Communication and Control, Environment\n  and Management (HNICEM) 12-16 November 2014 Hotel Centro, Puerto Princesa,\n  Palawan, Philippines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Q-learning is a simple and powerful tool in solving dynamic problems where\nenvironments are unknown. It uses a balance of exploration and exploitation to\nfind an optimal solution to the problem. In this paper, we propose using four\nbasic emotions: joy, sadness, fear, and anger to influence a Qlearning agent.\nSimulations show that the proposed affective agent requires lesser number of\nsteps to find the optimal path. We found when affective agent finds the optimal\npath, the ratio between exploration to exploitation gradually decreases,\nindicating lower total step count in the long run\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 10:03:27 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Badoy", "Wilfredo", "Jr."], ["Teknomo", "Kardi", ""]]}, {"id": "1609.01491", "submitter": "Christopher M. Poskitt", "authors": "Yuqi Chen, Christopher M. Poskitt, Jun Sun", "title": "Towards Learning and Verifying Invariants of Cyber-Physical Systems by\n  Code Mutation", "comments": "Short paper accepted by the 21st International Symposium on Formal\n  Methods (FM 2016)", "journal-ref": "In Proc. International Symposium on Formal Methods (FM 2016),\n  volume 9995 of LNCS, pages 155-163. Springer, 2016", "doi": "10.1007/978-3-319-48989-6_10", "report-no": null, "categories": "cs.SE cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-physical systems (CPS), which integrate algorithmic control with\nphysical processes, often consist of physically distributed components\ncommunicating over a network. A malfunctioning or compromised component in such\na CPS can lead to costly consequences, especially in the context of public\ninfrastructure. In this short paper, we argue for the importance of\nconstructing invariants (or models) of the physical behaviour exhibited by CPS,\nmotivated by their applications to the control, monitoring, and attestation of\ncomponents. To achieve this despite the inherent complexity of CPS, we propose\na new technique for learning invariants that combines machine learning with\nideas from mutation testing. We present a preliminary study on a water\ntreatment system that suggests the efficacy of this approach, propose\nstrategies for establishing confidence in the correctness of invariants, then\nsummarise some research questions and the steps we are taking to investigate\nthem.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 11:28:40 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Chen", "Yuqi", ""], ["Poskitt", "Christopher M.", ""], ["Sun", "Jun", ""]]}, {"id": "1609.01508", "submitter": "Mohammadi Zaki", "authors": "Aditya Gopalan, Odalric-Ambrym Maillard and Mohammadi Zaki", "title": "Low-rank Bandits with Latent Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of maximizing rewards from recommending items (actions) to\nusers sequentially interacting with a recommender system. Users are modeled as\nlatent mixtures of C many representative user classes, where each class\nspecifies a mean reward profile across actions. Both the user features (mixture\ndistribution over classes) and the item features (mean reward vector per class)\nare unknown a priori. The user identity is the only contextual information\navailable to the learner while interacting. This induces a low-rank structure\non the matrix of expected rewards r a,b from recommending item a to user b. The\nproblem reduces to the well-known linear bandit when either user or item-side\nfeatures are perfectly known. In the setting where each user, with its\nstochastically sampled taste profile, interacts only for a small number of\nsessions, we develop a bandit algorithm for the two-sided uncertainty. It\ncombines the Robust Tensor Power Method of Anandkumar et al. (2014b) with the\nOFUL linear bandit algorithm of Abbasi-Yadkori et al. (2011). We provide the\nfirst rigorous regret analysis of this combination, showing that its regret\nafter T user interactions is $\\tilde O(C\\sqrt{BT})$, with B the number of\nusers. An ingredient towards this result is a novel robustness property of\nOFUL, of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 12:01:30 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Gopalan", "Aditya", ""], ["Maillard", "Odalric-Ambrym", ""], ["Zaki", "Mohammadi", ""]]}, {"id": "1609.01586", "submitter": "Siddhartha Jonnalagadda", "authors": "Ravi Garg, Shu Dong, Sanjiv Shah, Siddhartha R Jonnalagadda", "title": "A Bootstrap Machine Learning Approach to Identify Rare Disease Patients\n  from Electronic Health Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rare diseases are very difficult to identify among large number of other\npossible diagnoses. Better availability of patient data and improvement in\nmachine learning algorithms empower us to tackle this problem computationally.\nIn this paper, we target one such rare disease - cardiac amyloidosis. We aim to\nautomate the process of identifying potential cardiac amyloidosis patients with\nthe help of machine learning algorithms and also learn most predictive factors.\nWith the help of experienced cardiologists, we prepared a gold standard with 73\npositive (cardiac amyloidosis) and 197 negative instances. We achieved high\naverage cross-validation F1 score of 0.98 using an ensemble machine learning\nclassifier. Some of the predictive variables were: Age and Diagnosis of cardiac\narrest, chest pain, congestive heart failure, hypertension, prim open angle\nglaucoma, and shoulder arthritis. Further studies are needed to validate the\naccuracy of the system across an entire health system and its generalizability\nfor other diseases.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 14:54:58 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Garg", "Ravi", ""], ["Dong", "Shu", ""], ["Shah", "Sanjiv", ""], ["Jonnalagadda", "Siddhartha R", ""]]}, {"id": "1609.01596", "submitter": "Arild N{\\o}kland", "authors": "Arild N{\\o}kland", "title": "Direct Feedback Alignment Provides Learning in Deep Neural Networks", "comments": "Accepted for publication at NIPS 2016. [v2] Corrected convolutional\n  results for feedback-alignment. [v3,v4,v5] Corrected theorem and proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks are most commonly trained with the\nback-propagation algorithm, where the gradient for learning is provided by\nback-propagating the error, layer by layer, from the output layer to the hidden\nlayers. A recently discovered method called feedback-alignment shows that the\nweights used for propagating the error backward don't have to be symmetric with\nthe weights used for propagation the activation forward. In fact, random\nfeedback weights work evenly well, because the network learns how to make the\nfeedback useful. In this work, the feedback alignment principle is used for\ntraining hidden layers more independently from the rest of the network, and\nfrom a zero initial condition. The error is propagated through fixed random\nfeedback connections directly from the output layer to each hidden layer. This\nsimple method is able to achieve zero training error even in convolutional\nnetworks and very deep networks, completely without error back-propagation. The\nmethod is a step towards biologically plausible machine learning because the\nerror signal is almost local, and no symmetric or reciprocal weights are\nrequired. Experiments show that the test performance on MNIST and CIFAR is\nalmost as good as those obtained with back-propagation for fully connected\nnetworks. If combined with dropout, the method achieves 1.45% error on the\npermutation invariant MNIST task.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 15:07:32 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 10:12:35 GMT"}, {"version": "v3", "created": "Thu, 6 Oct 2016 16:19:16 GMT"}, {"version": "v4", "created": "Sun, 23 Oct 2016 18:14:54 GMT"}, {"version": "v5", "created": "Wed, 21 Dec 2016 16:36:40 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["N\u00f8kland", "Arild", ""]]}, {"id": "1609.01704", "submitter": "Junyoung Chung", "authors": "Junyoung Chung and Sungjin Ahn and Yoshua Bengio", "title": "Hierarchical Multiscale Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning both hierarchical and temporal representation has been among the\nlong-standing challenges of recurrent neural networks. Multiscale recurrent\nneural networks have been considered as a promising approach to resolve this\nissue, yet there has been a lack of empirical evidence showing that this type\nof models can actually capture the temporal dependencies by discovering the\nlatent hierarchical structure of the sequence. In this paper, we propose a\nnovel multiscale approach, called the hierarchical multiscale recurrent neural\nnetworks, which can capture the latent hierarchical structure in the sequence\nby encoding the temporal dependencies with different timescales using a novel\nupdate mechanism. We show some evidence that our proposed multiscale\narchitecture can discover underlying hierarchical structure in the sequences\nwithout using explicit boundary information. We evaluate our proposed model on\ncharacter-level language modelling and handwriting sequence modelling.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 19:37:57 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2016 18:33:08 GMT"}, {"version": "v3", "created": "Mon, 31 Oct 2016 06:37:30 GMT"}, {"version": "v4", "created": "Wed, 16 Nov 2016 07:44:27 GMT"}, {"version": "v5", "created": "Wed, 14 Dec 2016 18:41:53 GMT"}, {"version": "v6", "created": "Wed, 8 Mar 2017 07:33:38 GMT"}, {"version": "v7", "created": "Thu, 9 Mar 2017 05:22:52 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Chung", "Junyoung", ""], ["Ahn", "Sungjin", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1609.01819", "submitter": "Sujith Ravi", "authors": "Harrie Oosterhuis, Sujith Ravi, Michael Bendersky", "title": "Semantic Video Trailers", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query-based video summarization is the task of creating a brief visual\ntrailer, which captures the parts of the video (or a collection of videos) that\nare most relevant to the user-issued query. In this paper, we propose an\nunsupervised label propagation approach for this task. Our approach effectively\ncaptures the multimodal semantics of queries and videos using state-of-the-art\ndeep neural networks and creates a summary that is both semantically coherent\nand visually attractive. We describe the theoretical framework of our\ngraph-based approach and empirically evaluate its effectiveness in creating\nrelevant and attractive trailers. Finally, we showcase example video trailers\ngenerated by our system.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 03:35:54 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Oosterhuis", "Harrie", ""], ["Ravi", "Sujith", ""], ["Bendersky", "Michael", ""]]}, {"id": "1609.01840", "submitter": "Jinmeng Song", "authors": "Jinmeng Song, Chun Yuan", "title": "Learning Boltzmann Machine with EM-like Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an expectation-maximization-like(EMlike) method to train Boltzmann\nmachine with unconstrained connectivity. It adopts Monte Carlo approximation in\nthe E-step, and replaces the intractable likelihood objective with efficiently\ncomputed objectives or directly approximates the gradient of likelihood\nobjective in the M-step. The EM-like method is a modification of alternating\nminimization. We prove that EM-like method will be the exactly same with\ncontrastive divergence in restricted Boltzmann machine if the M-step of this\nmethod adopts special approximation. We also propose a new measure to assess\nthe performance of Boltzmann machine as generative models of data, and its\ncomputational complexity is O(Rmn). Finally, we demonstrate the performance of\nEM-like method using numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 05:17:30 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Song", "Jinmeng", ""], ["Yuan", "Chun", ""]]}, {"id": "1609.01872", "submitter": "Gabor Balazs", "authors": "G\\'abor Bal\\'azs, Andr\\'as Gy\\\"orgy, Csaba Szepesv\\'ari", "title": "Chaining Bounds for Empirical Risk Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the standard chaining technique to prove excess risk upper\nbounds for empirical risk minimization with random design settings even if the\nmagnitude of the noise and the estimates is unbounded. The bound applies to\nmany loss functions besides the squared loss, and scales only with the\nsub-Gaussian or subexponential parameters without further statistical\nassumptions such as the bounded kurtosis condition over the hypothesis class. A\ndetailed analysis is provided for slope constrained and penalized linear least\nsquares regression with a sub-Gaussian setting, which often proves tight sample\ncomplexity bounds up to logartihmic factors.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 08:18:18 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Bal\u00e1zs", "G\u00e1bor", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1609.01882", "submitter": "Matthijs Douze", "authors": "Matthijs Douze, Herv\\'e J\\'egou and Florent Perronnin", "title": "Polysemous codes", "comments": "The final (author) version of our ECCV'16 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of approximate nearest neighbor search in\nthe compressed domain. We introduce polysemous codes, which offer both the\ndistance estimation quality of product quantization and the efficient\ncomparison of binary codes with Hamming distance. Their design is inspired by\nalgorithms introduced in the 90's to construct channel-optimized vector\nquantizers. At search time, this dual interpretation accelerates the search.\nMost of the indexed vectors are filtered out with Hamming distance, letting\nonly a fraction of the vectors to be ranked with an asymmetric distance\nestimator.\n  The method is complementary with a coarse partitioning of the feature space\nsuch as the inverted multi-index. This is shown by our experiments performed on\nseveral public benchmarks such as the BIGANN dataset comprising one billion\nvectors, for which we report state-of-the-art results for query times below\n0.3\\,millisecond per core. Last but not least, our approach allows the\napproximate computation of the k-NN graph associated with the Yahoo Flickr\nCreative Commons 100M, described by CNN image descriptors, in less than 8 hours\non a single machine.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 08:45:19 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 23:00:00 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Douze", "Matthijs", ""], ["J\u00e9gou", "Herv\u00e9", ""], ["Perronnin", "Florent", ""]]}, {"id": "1609.01885", "submitter": "Abhay Gupta", "authors": "Abhay Gupta, Arjun D'Cunha, Kamal Awasthi, Vineeth Balasubramanian", "title": "DAiSEE: Towards User Engagement Recognition in the Wild", "comments": "12 pages, 14 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce DAiSEE, the first multi-label video classification dataset\ncomprising of 9068 video snippets captured from 112 users for recognizing the\nuser affective states of boredom, confusion, engagement, and frustration in the\nwild. The dataset has four levels of labels namely - very low, low, high, and\nvery high for each of the affective states, which are crowd annotated and\ncorrelated with a gold standard annotation created using a team of expert\npsychologists. We have also established benchmark results on this dataset using\nstate-of-the-art video classification methods that are available today. We\nbelieve that DAiSEE will provide the research community with challenges in\nfeature extraction, context-based inference, and development of suitable\nmachine learning methods for related tasks, thus providing a springboard for\nfurther research. The dataset is available for download at\nhttps://iith.ac.in/~daisee-dataset\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 08:50:11 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 15:24:34 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 11:05:57 GMT"}, {"version": "v4", "created": "Fri, 15 Dec 2017 06:22:10 GMT"}, {"version": "v5", "created": "Thu, 12 Apr 2018 16:40:55 GMT"}, {"version": "v6", "created": "Fri, 13 Apr 2018 04:42:51 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Gupta", "Abhay", ""], ["D'Cunha", "Arjun", ""], ["Awasthi", "Kamal", ""], ["Balasubramanian", "Vineeth", ""]]}, {"id": "1609.01977", "submitter": "Yao Lu", "authors": "Yao Lu, Jukka Corander, Zhirong Yang", "title": "Doubly Stochastic Neighbor Embedding on Spheres", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Neighbor Embedding (SNE) methods minimize the divergence between\nthe similarity matrix of a high-dimensional data set and its counterpart from a\nlow-dimensional embedding, leading to widely applied tools for data\nvisualization. Despite their popularity, the current SNE methods experience a\ncrowding problem when the data include highly imbalanced similarities. This\nimplies that the data points with higher total similarity tend to get crowded\naround the display center. To solve this problem, we introduce a fast\nnormalization method and normalize the similarity matrix to be doubly\nstochastic such that all the data points have equal total similarities.\nFurthermore, we show empirically and theoretically that the doubly\nstochasticity constraint often leads to embeddings which are approximately\nspherical. This suggests replacing a flat space with spheres as the embedding\nspace. The spherical embedding eliminates the discrepancy between the center\nand the periphery in visualization, which efficiently resolves the crowding\nproblem. We compared the proposed method (DOSNES) with the state-of-the-art SNE\nmethod on three real-world datasets and the results clearly indicate that our\nmethod is more favorable in terms of visualization quality.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 13:42:26 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 18:31:32 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Lu", "Yao", ""], ["Corander", "Jukka", ""], ["Yang", "Zhirong", ""]]}, {"id": "1609.01984", "submitter": "Jinyoung Choi", "authors": "Jinyoung Choi, Beom-Jin Lee, and Byoung-Tak Zhang", "title": "Human Body Orientation Estimation using Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personal robots are expected to interact with the user by recognizing the\nuser's face. However, in most of the service robot applications, the user needs\nto move himself/herself to allow the robot to see him/her face to face. To\novercome such limitations, a method for estimating human body orientation is\nrequired. Previous studies used various components such as feature extractors\nand classification models to classify the orientation which resulted in low\nperformance. For a more robust and accurate approach, we propose the light\nweight convolutional neural networks, an end to end system, for estimating\nhuman body orientation. Our body orientation estimation model achieved 81.58%\nand 94% accuracy with the benchmark dataset and our own dataset respectively.\nThe proposed method can be used in a wide range of service robot applications\nwhich depend on the ability to estimate human body orientation. To show its\nusefulness in service robot applications, we designed a simple robot\napplication which allows the robot to move towards the user's frontal plane.\nWith this, we demonstrated an improved face detection rate.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 13:53:26 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Choi", "Jinyoung", ""], ["Lee", "Beom-Jin", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1609.02020", "submitter": "Zhenyu Liao", "authors": "Zhenyu Liao, Romain Couillet", "title": "Random matrices meet machine learning: a large dimensional analysis of\n  LS-SVM", "comments": "wrong article submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a performance analysis of kernel least squares support\nvector machines (LS-SVMs) based on a random matrix approach, in the regime\nwhere both the dimension of data $p$ and their number $n$ grow large at the\nsame rate. Under a two-class Gaussian mixture model for the input data, we\nprove that the LS-SVM decision function is asymptotically normal with means and\ncovariances shown to depend explicitly on the derivatives of the kernel\nfunction. This provides improved understanding along with new insights into the\ninternal workings of SVM-type methods for large datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 15:39:24 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 07:26:00 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Liao", "Zhenyu", ""], ["Couillet", "Romain", ""]]}, {"id": "1609.02036", "submitter": "Zhirong Wu", "authors": "Zhirong Wu, Dahua Lin, Xiaoou Tang", "title": "Deep Markov Random Field for Image Modeling", "comments": "Accepted at ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Random Fields (MRFs), a formulation widely used in generative image\nmodeling, have long been plagued by the lack of expressive power. This issue is\nprimarily due to the fact that conventional MRFs formulations tend to use\nsimplistic factors to capture local patterns. In this paper, we move beyond\nsuch limitations, and propose a novel MRF model that uses fully-connected\nneurons to express the complex interactions among pixels. Through theoretical\nanalysis, we reveal an inherent connection between this model and recurrent\nneural networks, and thereon derive an approximated feed-forward network that\ncouples multiple RNNs along opposite directions. This formulation combines the\nexpressive power of deep neural networks and the cyclic dependency structure of\nMRF in a unified model, bringing the modeling capability to a new level. The\nfeed-forward approximation also allows it to be efficiently learned from data.\nExperimental results on a variety of low-level vision tasks show notable\nimprovement over state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 15:56:36 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Wu", "Zhirong", ""], ["Lin", "Dahua", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1609.02082", "submitter": "Christian Huemmer M.Sc.", "authors": "Christian Huemmer, Ram\\'on Fern\\'andez Astudillo and Walter Kellermann", "title": "An improved uncertainty decoding scheme with weighted samples for\n  DNN-HMM hybrid systems", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we advance a recently-proposed uncertainty decoding scheme for\nDNN-HMM (deep neural network - hidden Markov model) hybrid systems. This\nnumerical sampling concept averages DNN outputs produced by a finite set of\nfeature samples (drawn from a probabilistic distortion model) to approximate\nthe posterior likelihoods of the context-dependent HMM states. As main\ninnovation, we propose a weighted DNN-output averaging based on a minimum\nclassification error criterion and apply it to a probabilistic distortion model\nfor spatial diffuseness features. The experimental evaluation is performed on\nthe 8-channel REVERB Challenge task using a DNN-HMM hybrid system with\nmultichannel front-end signal enhancement. We show that the recognition\naccuracy of the DNN-HMM hybrid system improves by incorporating uncertainty\ndecoding based on random sampling and that the proposed weighted DNN-output\naveraging further reduces the word error rate scores.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 10:11:24 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Huemmer", "Christian", ""], ["Astudillo", "Ram\u00f3n Fern\u00e1ndez", ""], ["Kellermann", "Walter", ""]]}, {"id": "1609.02116", "submitter": "Trapit Bansal", "authors": "Trapit Bansal, David Belanger, Andrew McCallum", "title": "Ask the GRU: Multi-Task Learning for Deep Text Recommendations", "comments": "8 pages", "journal-ref": null, "doi": "10.1145/2959100.2959180", "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a variety of application domains the content to be recommended to users is\nassociated with text. This includes research papers, movies with associated\nplot summaries, news articles, blog posts, etc. Recommendation approaches based\non latent factor models can be extended naturally to leverage text by employing\nan explicit mapping from text to factors. This enables recommendations for new,\nunseen content, and may generalize better, since the factors for all items are\nproduced by a compactly-parametrized model. Previous work has used topic models\nor averages of word embeddings for this mapping. In this paper we present a\nmethod leveraging deep recurrent neural networks to encode the text sequence\ninto a latent vector, specifically gated recurrent units (GRUs) trained\nend-to-end on the collaborative filtering task. For the task of scientific\npaper recommendation, this yields models with significantly higher accuracy. In\ncold-start scenarios, we beat the previous state-of-the-art, all of which\nignore word order. Performance is further improved by multi-task learning,\nwhere the text encoder network is trained for a combination of content\nrecommendation and item metadata prediction. This regularizes the collaborative\nfiltering model, ameliorating the problem of sparsity of the observed rating\nmatrix.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 19:05:42 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 19:27:19 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Bansal", "Trapit", ""], ["Belanger", "David", ""], ["McCallum", "Andrew", ""]]}, {"id": "1609.02132", "submitter": "Iasonas Kokkinos", "authors": "Iasonas Kokkinos", "title": "UberNet: Training a `Universal' Convolutional Neural Network for Low-,\n  Mid-, and High-Level Vision using Diverse Datasets and Limited Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a convolutional neural network (CNN) that jointly\nhandles low-, mid-, and high-level vision tasks in a unified architecture that\nis trained end-to-end. Such a universal network can act like a `swiss knife'\nfor vision tasks; we call this architecture an UberNet to indicate its\noverarching nature.\n  We address two main technical challenges that emerge when broadening up the\nrange of tasks handled by a single CNN: (i) training a deep architecture while\nrelying on diverse training sets and (ii) training many (potentially unlimited)\ntasks with a limited memory budget. Properly addressing these two problems\nallows us to train accurate predictors for a host of tasks, without\ncompromising accuracy.\n  Through these advances we train in an end-to-end manner a CNN that\nsimultaneously addresses (a) boundary detection (b) normal estimation (c)\nsaliency estimation (d) semantic segmentation (e) human part segmentation (f)\nsemantic boundary detection, (g) region proposal generation and object\ndetection. We obtain competitive performance while jointly addressing all of\nthese tasks in 0.7 seconds per frame on a single GPU. A demonstration of this\nsystem can be found at http://cvn.ecp.fr/ubernet/.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 19:35:30 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Kokkinos", "Iasonas", ""]]}, {"id": "1609.02200", "submitter": "Jason Rolfe", "authors": "Jason Tyler Rolfe", "title": "Discrete Variational Autoencoders", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic models with discrete latent variables naturally capture\ndatasets composed of discrete classes. However, they are difficult to train\nefficiently, since backpropagation through discrete variables is generally not\npossible. We present a novel method to train a class of probabilistic models\nwith discrete latent variables using the variational autoencoder framework,\nincluding backpropagation through the discrete latent variables. The associated\nclass of probabilistic models comprises an undirected discrete component and a\ndirected hierarchical continuous component. The discrete component captures the\ndistribution over the disconnected smooth manifolds induced by the continuous\ncomponent. As a result, this class of models efficiently learns both the class\nof objects in an image, and their specific realization in pixels, from\nunsupervised data, and outperforms state-of-the-art methods on the\npermutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 21:41:32 GMT"}, {"version": "v2", "created": "Sat, 22 Apr 2017 01:23:06 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Rolfe", "Jason Tyler", ""]]}, {"id": "1609.02208", "submitter": "Sewoong Oh", "authors": "Weihao Gao and Sewoong Oh and Pramod Viswanath", "title": "Breaking the Bandwidth Barrier: Geometrical Adaptive Entropy Estimation", "comments": "24 pages 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimators of information theoretic measures such as entropy and mutual\ninformation are a basic workhorse for many downstream applications in modern\ndata science. State of the art approaches have been either geometric (nearest\nneighbor (NN) based) or kernel based (with a globally chosen bandwidth). In\nthis paper, we combine both these approaches to design new estimators of\nentropy and mutual information that outperform state of the art methods. Our\nestimator uses local bandwidth choices of $k$-NN distances with a finite $k$,\nindependent of the sample size. Such a local and data dependent choice improves\nperformance in practice, but the bandwidth is vanishing at a fast rate, leading\nto a non-vanishing bias. We show that the asymptotic bias of the proposed\nestimator is universal; it is independent of the underlying distribution.\nHence, it can be pre-computed and subtracted from the estimate. As a byproduct,\nwe obtain a unified way of obtaining both kernel and NN estimators. The\ncorresponding theoretical contribution relating the asymptotic geometry of\nnearest neighbors to order statistics is of independent mathematical interest.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 22:11:39 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Gao", "Weihao", ""], ["Oh", "Sewoong", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1609.02226", "submitter": "Navid Kardan", "authors": "Navid Kardan, Kenneth O. Stanley", "title": "Fitted Learning: Models with Awareness of their Limits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though deep learning has pushed the boundaries of classification forward, in\nrecent years hints of the limits of standard classification have begun to\nemerge. Problems such as fooling, adding new classes over time, and the need to\nretrain learning models only for small changes to the original problem all\npoint to a potential shortcoming in the classic classification regime, where a\ncomprehensive a priori knowledge of the possible classes or concepts is\ncritical. Without such knowledge, classifiers misjudge the limits of their\nknowledge and overgeneralization therefore becomes a serious obstacle to\nconsistent performance. In response to these challenges, this paper extends the\nclassic regime by reframing classification instead with the assumption that\nconcepts present in the training set are only a sample of the hypothetical\nfinal set of concepts. To bring learning models into this new paradigm, a novel\nelaboration of standard architectures called the competitive overcomplete\noutput layer (COOL) neural network is introduced. Experiments demonstrate the\neffectiveness of COOL by applying it to fooling, separable concept learning,\none-class neural networks, and standard classification benchmarks. The results\nsuggest that, unlike conventional classifiers, the amount of generalization in\nCOOL networks can be tuned to match the problem.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 23:59:36 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 06:34:56 GMT"}, {"version": "v3", "created": "Wed, 4 Jan 2017 17:14:41 GMT"}, {"version": "v4", "created": "Mon, 9 Jul 2018 04:21:34 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Kardan", "Navid", ""], ["Stanley", "Kenneth O.", ""]]}, {"id": "1609.02228", "submitter": "Thomas Miconi", "authors": "Thomas Miconi", "title": "Learning to learn with backpropagation of Hebbian plasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hebbian plasticity is a powerful principle that allows biological brains to\nlearn from their lifetime experience. By contrast, artificial neural networks\ntrained with backpropagation generally have fixed connection weights that do\nnot change once training is complete. While recent methods can endow neural\nnetworks with long-term memories, Hebbian plasticity is currently not amenable\nto gradient descent. Here we derive analytical expressions for activity\ngradients in neural networks with Hebbian plastic connections. Using these\nexpressions, we can use backpropagation to train not just the baseline weights\nof the connections, but also their plasticity. As a result, the networks \"learn\nhow to learn\" in order to solve the problem at hand: the trained networks\nautomatically perform fast learning of unpredictable environmental features\nduring their lifetime, expanding the range of solvable problems. We test the\nalgorithm on various on-line learning tasks, including pattern completion,\none-shot learning, and reversal learning. The algorithm successfully learns how\nto learn the relevant associations from one-shot instruction, and fine-tunes\nthe temporal dynamics of plasticity to allow for continual learning in response\nto changing environmental parameters. We conclude that backpropagation of\nHebbian plasticity offers a powerful model for lifelong learning.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 00:02:20 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 17:51:25 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Miconi", "Thomas", ""]]}, {"id": "1609.02383", "submitter": "Parameswaran Kamalaruban Mr.", "authors": "Parameswaran Kamalaruban", "title": "Improved Optimistic Mirror Descent for Sparsity and Curvature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Convex Optimization plays a key role in large scale machine learning.\nEarly approaches to this problem were conservative, in which the main focus was\nprotection against the worst case scenario. But recently several algorithms\nhave been developed for tightening the regret bounds in easy data instances\nsuch as sparsity, predictable sequences, and curved losses. In this work we\nunify some of these existing techniques to obtain new update rules for the\ncases when these easy instances occur together. First we analyse an adaptive\nand optimistic update rule which achieves tighter regret bound when the loss\nsequence is sparse and predictable. Then we explain an update rule that\ndynamically adapts to the curvature of the loss function and utilizes the\npredictable nature of the loss sequence as well. Finally we extend these\nresults to composite losses.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 11:52:05 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Kamalaruban", "Parameswaran", ""]]}, {"id": "1609.02487", "submitter": "Lennart Gulikers", "authors": "Lennart Gulikers, Marc Lelarge, Laurent Massouli\\'e", "title": "Non-Backtracking Spectrum of Degree-Corrected Stochastic Block Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by community detection, we characterise the spectrum of the\nnon-backtracking matrix $B$ in the Degree-Corrected Stochastic Block Model.\n  Specifically, we consider a random graph on $n$ vertices partitioned into two\nequal-sized clusters. The vertices have i.i.d. weights $\\{ \\phi_u \\}_{u=1}^n$\nwith second moment $\\Phi^{(2)}$. The intra-cluster connection probability for\nvertices $u$ and $v$ is $\\frac{\\phi_u \\phi_v}{n}a$ and the inter-cluster\nconnection probability is $\\frac{\\phi_u \\phi_v}{n}b$.\n  We show that with high probability, the following holds: The leading\neigenvalue of the non-backtracking matrix $B$ is asymptotic to $\\rho =\n\\frac{a+b}{2} \\Phi^{(2)}$. The second eigenvalue is asymptotic to $\\mu_2 =\n\\frac{a-b}{2} \\Phi^{(2)}$ when $\\mu_2^2 > \\rho$, but asymptotically bounded by\n$\\sqrt{\\rho}$ when $\\mu_2^2 \\leq \\rho$. All the remaining eigenvalues are\nasymptotically bounded by $\\sqrt{\\rho}$. As a result, a clustering\npositively-correlated with the true communities can be obtained based on the\nsecond eigenvector of $B$ in the regime where $\\mu_2^2 > \\rho.$\n  In a previous work we obtained that detection is impossible when $\\mu_2^2 <\n\\rho,$ meaning that there occurs a phase-transition in the sparse regime of the\nDegree-Corrected Stochastic Block Model.\n  As a corollary, we obtain that Degree-Corrected Erd\\H{o}s-R\\'enyi graphs\nasymptotically satisfy the graph Riemann hypothesis, a quasi-Ramanujan\nproperty.\n  A by-product of our proof is a weak law of large numbers for\nlocal-functionals on Degree-Corrected Stochastic Block Models, which could be\nof independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 16:33:39 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 11:48:10 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Gulikers", "Lennart", ""], ["Lelarge", "Marc", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "1609.02489", "submitter": "Christian Bracher", "authors": "Christian Bracher, Sebastian Heinz and Roland Vollgraf", "title": "Fashion DNA: Merging Content and Sales Data for Recommendation and\n  Article Mapping", "comments": "10 pages, 13 figures. Paper presented at the workshop \"Machine\n  Learning Meets Fashion,\" KDD 2016 Conference, San Francisco, USA, March 14,\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to determine Fashion DNA, coordinate vectors locating\nfashion items in an abstract space. Our approach is based on a deep neural\nnetwork architecture that ingests curated article information such as tags and\nimages, and is trained to predict sales for a large set of frequent customers.\nIn the process, a dual space of customer style preferences naturally arises.\nInterpretation of the metric of these spaces is straightforward: The product of\nFashion DNA and customer style vectors yields the forecast purchase likelihood\nfor the customer-item pair, while the angle between Fashion DNA vectors is a\nmeasure of item similarity. Importantly, our models are able to generate\nunbiased purchase probabilities for fashion items based solely on article\ninformation, even in absence of sales data, thus circumventing the \"cold-start\nproblem\" of collaborative recommendation approaches. Likewise, it generalizes\neasily and reliably to customers outside the training set. We experiment with\nFashion DNA models based on visual and/or tag item data, evaluate their\nrecommendation power, and discuss the resulting article similarities.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 16:48:20 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Bracher", "Christian", ""], ["Heinz", "Sebastian", ""], ["Vollgraf", "Roland", ""]]}, {"id": "1609.02513", "submitter": "Dan Guralnik", "authors": "Jared Culbertson, Dan P. Guralnik, Peter F. Stiller", "title": "Functorial Hierarchical Clustering with Overlaps", "comments": "Minor revisions. 24 pages, 1 figure", "journal-ref": "Discrete Applied Mathematics, Volume 236, 19 February 2018,\n  pp.108--123", "doi": "10.1016/j.dam.2017.10.015", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This work draws inspiration from three important sources of research on\ndissimilarity-based clustering and intertwines those three threads into a\nconsistent principled functorial theory of clustering. Those three are the\noverlapping clustering of Jardine and Sibson, the functorial approach of\nCarlsson and M\\'{e}moli to partition-based clustering, and the Isbell/Dress\nschool's study of injective envelopes. Carlsson and M\\'{e}moli introduce the\nidea of viewing clustering methods as functors from a category of metric spaces\nto a category of clusters, with functoriality subsuming many desirable\nproperties. Our first series of results extends their theory of functorial\nclustering schemes to methods that allow overlapping clusters in the spirit of\nJardine and Sibson. This obviates some of the unpleasant effects of chaining\nthat occur, for example with single-linkage clustering. We prove an equivalence\nbetween these general overlapping clustering functors and projections of weight\nspaces to what we term clustering domains, by focusing on the order structure\ndetermined by the morphisms. As a specific application of this machinery, we\nare able to prove that there are no functorial projections to cut metrics, or\neven to tree metrics. Finally, although we focus less on the construction of\nclustering methods (clustering domains) derived from injective envelopes, we\nlay out some preliminary results, that hopefully will give a feel for how the\nthird leg of the stool comes into play.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 17:52:26 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 18:34:04 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Culbertson", "Jared", ""], ["Guralnik", "Dan P.", ""], ["Stiller", "Peter F.", ""]]}, {"id": "1609.02521", "submitter": "Rohit Babbar", "authors": "Rohit Babbar and Bernhard Shoelkopf", "title": "DiSMEC - Distributed Sparse Machines for Extreme Multi-label\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme multi-label classification refers to supervised multi-label learning\ninvolving hundreds of thousands or even millions of labels. Datasets in extreme\nclassification exhibit fit to power-law distribution, i.e. a large fraction of\nlabels have very few positive instances in the data distribution. Most\nstate-of-the-art approaches for extreme multi-label classification attempt to\ncapture correlation among labels by embedding the label matrix to a\nlow-dimensional linear sub-space. However, in the presence of power-law\ndistributed extremely large and diverse label spaces, structural assumptions\nsuch as low rank can be easily violated.\n  In this work, we present DiSMEC, which is a large-scale distributed framework\nfor learning one-versus-rest linear classifiers coupled with explicit capacity\ncontrol to control model size. Unlike most state-of-the-art methods, DiSMEC\ndoes not make any low rank assumptions on the label matrix. Using double layer\nof parallelization, DiSMEC can learn classifiers for datasets consisting\nhundreds of thousands labels within few hours. The explicit capacity control\nmechanism filters out spurious parameters which keep the model compact in size,\nwithout losing prediction accuracy. We conduct extensive empirical evaluation\non publicly available real-world datasets consisting upto 670,000 labels. We\ncompare DiSMEC with recent state-of-the-art approaches, including - SLEEC which\nis a leading approach for learning sparse local embeddings, and FastXML which\nis a tree-based approach optimizing ranking based loss function. On some of the\ndatasets, DiSMEC can significantly boost prediction accuracies - 10% better\ncompared to SLECC and 15% better compared to FastXML, in absolute terms.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 18:17:25 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Babbar", "Rohit", ""], ["Shoelkopf", "Bernhard", ""]]}, {"id": "1609.02542", "submitter": "Marcello Benedetti", "authors": "Marcello Benedetti, John Realpe-G\\'omez, Rupak Biswas, Alejandro\n  Perdomo-Ortiz", "title": "Quantum-Assisted Learning of Hardware-Embedded Probabilistic Graphical\n  Models", "comments": "17 pages, 8 figures. Minor further revisions. As published in Phys.\n  Rev. X", "journal-ref": "Phys. Rev. X 7, 041052 (2017)", "doi": "10.1103/PhysRevX.7.041052", "report-no": null, "categories": "quant-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mainstream machine-learning techniques such as deep learning and\nprobabilistic programming rely heavily on sampling from generally intractable\nprobability distributions. There is increasing interest in the potential\nadvantages of using quantum computing technologies as sampling engines to speed\nup these tasks or to make them more effective. However, some pressing\nchallenges in state-of-the-art quantum annealers have to be overcome before we\ncan assess their actual performance. The sparse connectivity, resulting from\nthe local interaction between quantum bits in physical hardware\nimplementations, is considered the most severe limitation to the quality of\nconstructing powerful generative unsupervised machine-learning models. Here we\nuse embedding techniques to add redundancy to data sets, allowing us to\nincrease the modeling capacity of quantum annealers. We illustrate our findings\nby training hardware-embedded graphical models on a binarized data set of\nhandwritten digits and two synthetic data sets in experiments with up to 940\nquantum bits. Our model can be trained in quantum hardware without full\nknowledge of the effective parameters specifying the corresponding quantum\nGibbs-like distribution; therefore, this approach avoids the need to infer the\neffective temperature at each iteration, speeding up learning; it also\nmitigates the effect of noise in the control parameters, making it robust to\ndeviations from the reference Gibbs distribution. Our approach demonstrates the\nfeasibility of using quantum annealers for implementing generative models, and\nit provides a suitable framework for benchmarking these quantum technologies on\nmachine-learning-related tasks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 19:30:59 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 16:23:56 GMT"}, {"version": "v3", "created": "Thu, 19 Oct 2017 17:30:12 GMT"}, {"version": "v4", "created": "Thu, 25 Jan 2018 20:34:48 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Benedetti", "Marcello", ""], ["Realpe-G\u00f3mez", "John", ""], ["Biswas", "Rupak", ""], ["Perdomo-Ortiz", "Alejandro", ""]]}, {"id": "1609.02606", "submitter": "Shahin Shahrampour", "authors": "Shahin Shahrampour, Mohammad Noshad, Vahid Tarokh", "title": "On Sequential Elimination Algorithms for Best-Arm Identification in\n  Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2706192", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the best-arm identification problem in multi-armed bandits, which\nfocuses purely on exploration. A player is given a fixed budget to explore a\nfinite set of arms, and the rewards of each arm are drawn independently from a\nfixed, unknown distribution. The player aims to identify the arm with the\nlargest expected reward. We propose a general framework to unify sequential\nelimination algorithms, where the arms are dismissed iteratively until a unique\narm is left. Our analysis reveals a novel performance measure expressed in\nterms of the sampling mechanism and number of eliminated arms at each round.\nBased on this result, we develop an algorithm that divides the budget according\nto a nonlinear function of remaining arms at each round. We provide theoretical\nguarantees for the algorithm, characterizing the suitable nonlinearity for\ndifferent problem environments described by the number of competitive arms.\nMatching the theoretical results, our experiments show that the nonlinear\nalgorithm outperforms the state-of-the-art. We finally study the\nside-observation model, where pulling an arm reveals the rewards of its related\narms, and we establish improved theoretical guarantees in the pure-exploration\nsetting.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 21:46:37 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 16:02:04 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Shahrampour", "Shahin", ""], ["Noshad", "Mohammad", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1609.02612", "submitter": "Carl Vondrick", "authors": "Carl Vondrick and Hamed Pirsiavash and Antonio Torralba", "title": "Generating Videos with Scene Dynamics", "comments": "NIPS 2016. See more at http://web.mit.edu/vondrick/tinyvideo/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We capitalize on large amounts of unlabeled video in order to learn a model\nof scene dynamics for both video recognition tasks (e.g. action classification)\nand video generation tasks (e.g. future prediction). We propose a generative\nadversarial network for video with a spatio-temporal convolutional architecture\nthat untangles the scene's foreground from the background. Experiments suggest\nthis model can generate tiny videos up to a second at full frame rate better\nthan simple baselines, and we show its utility at predicting plausible futures\nof static images. Moreover, experiments and visualizations show the model\ninternally learns useful features for recognizing actions with minimal\nsupervision, suggesting scene dynamics are a promising signal for\nrepresentation learning. We believe generative video models can impact many\napplications in video understanding and simulation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 22:29:52 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 03:13:10 GMT"}, {"version": "v3", "created": "Wed, 26 Oct 2016 13:58:10 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Vondrick", "Carl", ""], ["Pirsiavash", "Hamed", ""], ["Torralba", "Antonio", ""]]}, {"id": "1609.02613", "submitter": "Wei Fu", "authors": "Wei Fu and Vivek Nair and Tim Menzies", "title": "Why is Differential Evolution Better than Grid Search for Tuning Defect\n  Predictors?", "comments": "12 pages, 8 figures, submitted to Information and Software Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: One of the black arts of data mining is learning the magic\nparameters which control the learners. In software analytics, at least for\ndefect prediction, several methods, like grid search and differential evolution\n(DE), have been proposed to learn these parameters, which has been proved to be\nable to improve the performance scores of learners.\n  Objective: We want to evaluate which method can find better parameters in\nterms of performance score and runtime cost.\n  Methods: This paper compares grid search to differential evolution, which is\nan evolutionary algorithm that makes extensive use of stochastic jumps around\nthe search space.\n  Results: We find that the seemingly complete approach of grid search does no\nbetter, and sometimes worse, than the stochastic search. When repeated 20 times\nto check for conclusion validity, DE was over 210 times faster than grid search\nto tune Random Forests on 17 testing data sets with F-Measure\n  Conclusions: These results are puzzling: why does a quick partial search be\njust as effective as a much slower, and much more, extensive search? To answer\nthat question, we turned to the theoretical optimization literature. Bergstra\nand Bengio conjecture that grid search is not more effective than more\nrandomized searchers if the underlying search space is inherently low\ndimensional. This is significant since recent results show that defect\nprediction exhibits very low intrinsic dimensionality-- an observation that\nexplains why a fast method like DE may work as well as a seemingly more\nthorough grid search. This suggests, as a future research direction, that it\nmight be possible to peek at data sets before doing any optimization in order\nto match the optimization algorithm to the problem at hand.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 22:32:44 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 02:15:12 GMT"}, {"version": "v3", "created": "Fri, 10 Mar 2017 17:29:06 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Fu", "Wei", ""], ["Nair", "Vivek", ""], ["Menzies", "Tim", ""]]}, {"id": "1609.02631", "submitter": "Varvara Kollia", "authors": "Varvara Kollia, Oguz H. Elibol", "title": "Distributed Processing of Biosignal-Database for Emotion Recognition\n  with Mahout", "comments": "4 pages, 5 png figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the use of distributed processing on the problem of\nemotion recognition from physiological sensors using a popular machine learning\nlibrary on distributed mode. Specifically, we run a random forests classifier\non the biosignal-data, which have been pre-processed to form exclusive groups\nin an unsupervised fashion, on a Cloudera cluster using Mahout. The use of\ndistributed processing significantly reduces the time required for the offline\ntraining of the classifier, enabling processing of large physiological datasets\nthrough many iterations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 01:13:20 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Kollia", "Varvara", ""], ["Elibol", "Oguz H.", ""]]}, {"id": "1609.02664", "submitter": "Charles Jordan", "authors": "Charles Jordan and {\\L}ukasz Kaiser", "title": "Machine Learning with Guarantees using Descriptive Complexity and SMT\n  Solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is a thriving part of computer science. There are many\nefficient approaches to machine learning that do not provide strong theoretical\nguarantees, and a beautiful general learning theory. Unfortunately, machine\nlearning approaches that give strong theoretical guarantees have not been\nefficient enough to be applicable. In this paper we introduce a logical\napproach to machine learning. Models are represented by tuples of logical\nformulas and inputs and outputs are logical structures. We present our\nframework together with several applications where we evaluate it using SAT and\nSMT solvers. We argue that this approach to machine learning is particularly\nsuited to bridge the gap between efficiency and theoretical soundness. We\nexploit results from descriptive complexity theory to prove strong theoretical\nguarantees for our approach. To show its applicability, we present experimental\nresults including learning complexity-theoretic reductions rules for board\ngames. We also explain how neural networks fit into our framework, although the\ncurrent implementation does not scale to provide guarantees for real-world\nneural networks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 06:04:17 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Jordan", "Charles", ""], ["Kaiser", "\u0141ukasz", ""]]}, {"id": "1609.02678", "submitter": "Nirav Bhatt", "authors": "Jayadev P Satya and Nirav Bhatt and Ramkrishna Pasumarthy and Aravind\n  Rajeswaran", "title": "Identifying Topology of Power Distribution Networks Based on Smart Meter\n  Data", "comments": "Submitted to IEEE transaction on smart grid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a power distribution network, the network topology information is\nessential for an efficient operation of the network. This information of\nnetwork connectivity is not accurately available, at the low voltage level, due\nto uninformed changes that happen from time to time. In this paper, we propose\na novel data--driven approach to identify the underlying network topology\nincluding the load phase connectivity from time series of energy measurements.\nThe proposed method involves the application of Principal Component Analysis\n(PCA) and its graph-theoretic interpretation to infer the topology from smart\nmeter energy measurements. The method is demonstrated through simulation on\nrandomly generated networks and also on IEEE recognized Roy Billinton\ndistribution test system.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 07:52:25 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Satya", "Jayadev P", ""], ["Bhatt", "Nirav", ""], ["Pasumarthy", "Ramkrishna", ""], ["Rajeswaran", "Aravind", ""]]}, {"id": "1609.02715", "submitter": "Amin Fehri", "authors": "Amin Fehri (CMM), Santiago Velasco-Forero (CMM), Fernand Meyer (CMM)", "title": "Automatic Selection of Stochastic Watershed Hierarchies", "comments": "in European Conference of Signal Processing (EUSIPCO), 2016,\n  Budapest, Hungary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation, seen as the association of a partition with an image, is a\ndifficult task. It can be decomposed in two steps: at first, a family of\ncontours associated with a series of nested partitions (or hierarchy) is\ncreated and organized, then pertinent contours are extracted. A coarser\npartition is obtained by merging adjacent regions of a finer partition. The\nstrength of a contour is then measured by the level of the hierarchy for which\nits two adjacent regions merge. We present an automatic segmentation strategy\nusing a wide range of stochastic watershed hierarchies. For a given set of\nhomogeneous images, our approach selects automatically the best hierarchy and\ncut level to perform image simplification given an evaluation score.\nExperimental results illustrate the advantages of our approach on several\nreal-life images datasets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 09:26:22 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Fehri", "Amin", "", "CMM"], ["Velasco-Forero", "Santiago", "", "CMM"], ["Meyer", "Fernand", "", "CMM"]]}, {"id": "1609.02727", "submitter": "Vlad Sandulescu", "authors": "Vlad Sandulescu, Martin Ester", "title": "Detecting Singleton Review Spammers Using Semantic Similarity", "comments": "6 pages, WWW 2015", "journal-ref": "WWW '15 Companion Proceedings of the 24th International Conference\n  on World Wide Web, 2015, p.971-976", "doi": "10.1145/2740908.2742570", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews have increasingly become a very important resource for\nconsumers when making purchases. Though it is becoming more and more difficult\nfor people to make well-informed buying decisions without being deceived by\nfake reviews. Prior works on the opinion spam problem mostly considered\nclassifying fake reviews using behavioral user patterns. They focused on\nprolific users who write more than a couple of reviews, discarding one-time\nreviewers. The number of singleton reviewers however is expected to be high for\nmany review websites. While behavioral patterns are effective when dealing with\nelite users, for one-time reviewers, the review text needs to be exploited. In\nthis paper we tackle the problem of detecting fake reviews written by the same\nperson using multiple names, posting each review under a different name. We\npropose two methods to detect similar reviews and show the results generally\noutperform the vectorial similarity measures used in prior works. The first\nmethod extends the semantic similarity between words to the reviews level. The\nsecond method is based on topic modeling and exploits the similarity of the\nreviews topic distributions using two models: bag-of-words and\nbag-of-opinion-phrases. The experiments were conducted on reviews from three\ndifferent datasets: Yelp (57K reviews), Trustpilot (9K reviews) and Ott dataset\n(800 reviews).\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 09:58:45 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Sandulescu", "Vlad", ""], ["Ester", "Martin", ""]]}, {"id": "1609.02728", "submitter": "Vlad Sandulescu", "authors": "Vlad Sandulescu, Mihai Chiru", "title": "Predicting the future relevance of research institutions - The winning\n  solution of the KDD Cup 2016", "comments": "6 pages, KDD 2016, KDD Cup 2016", "journal-ref": "The KDD Cup Workshop at KDD 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.DL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world's collective knowledge is evolving through research and new\nscientific discoveries. It is becoming increasingly difficult to objectively\nrank the impact research institutes have on global advancements. However, since\nthe funding, governmental support, staff and students quality all mirror the\nprojected quality of the institution, it becomes essential to measure the\naffiliation's rating in a transparent and widely accepted way. We propose and\ninvestigate several methods to rank affiliations based on the number of their\naccepted papers at future academic conferences. We carry out our investigation\nusing publicly available datasets such as the Microsoft Academic Graph, a\nheterogeneous graph which contains various information about academic papers.\nWe analyze several models, starting with a simple probabilities-based method\nand then gradually expand our training dataset, engineer many more features and\nuse mixed models and gradient boosted decision trees models to improve our\npredictions.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 10:14:51 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Sandulescu", "Vlad", ""], ["Chiru", "Mihai", ""]]}, {"id": "1609.02745", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, Parsa Ghaffari, and John G. Breslin", "title": "A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis", "comments": "To be published at EMNLP 2016, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion mining from customer reviews has become pervasive in recent years.\nSentences in reviews, however, are usually classified independently, even\nthough they form part of a review's argumentative structure. Intuitively,\nsentences in a review build and elaborate upon each other; knowledge of the\nreview structure and sentential context should thus inform the classification\nof each sentence. We demonstrate this hypothesis for the task of aspect-based\nsentiment analysis by modeling the interdependencies of sentences in a review\nwith a hierarchical bidirectional LSTM. We show that the hierarchical model\noutperforms two non-hierarchical baselines, obtains results competitive with\nthe state-of-the-art, and outperforms the state-of-the-art on five\nmultilingual, multi-domain datasets without any hand-engineered features or\nexternal resources.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 11:16:15 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Ruder", "Sebastian", ""], ["Ghaffari", "Parsa", ""], ["Breslin", "John G.", ""]]}, {"id": "1609.02746", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, Parsa Ghaffari, and John G. Breslin", "title": "INSIGHT-1 at SemEval-2016 Task 4: Convolutional Neural Networks for\n  Sentiment Classification and Quantification", "comments": "Published in Proceedings of SemEval-2016, 5 pages", "journal-ref": "Proceedings of SemEval (2016): 178-182", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our deep learning-based approach to sentiment analysis\nin Twitter as part of SemEval-2016 Task 4. We use a convolutional neural\nnetwork to determine sentiment and participate in all subtasks, i.e. two-point,\nthree-point, and five-point scale sentiment classification and two-point and\nfive-point scale sentiment quantification. We achieve competitive results for\ntwo-point scale sentiment classification and quantification, ranking fifth and\na close fourth (third and second by alternative metrics) respectively despite\nusing only pre-trained embeddings that contain no sentiment information. We\nachieve good performance on three-point scale sentiment classification, ranking\neighth out of 35, while performing poorly on five-point scale sentiment\nclassification and quantification. An error analysis reveals that this is due\nto low expressiveness of the model to capture negative sentiment as well as an\ninability to take into account ordinal information. We propose improvements in\norder to address these and other issues.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 11:16:56 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Ruder", "Sebastian", ""], ["Ghaffari", "Parsa", ""], ["Breslin", "John G.", ""]]}, {"id": "1609.02748", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, Parsa Ghaffari, and John G. Breslin", "title": "INSIGHT-1 at SemEval-2016 Task 5: Deep Learning for Multilingual\n  Aspect-based Sentiment Analysis", "comments": "Published in Proceedings of SemEval-2016, 7 pages", "journal-ref": "Proceedings of SemEval (2016): 330-336", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our deep learning-based approach to multilingual\naspect-based sentiment analysis as part of SemEval 2016 Task 5. We use a\nconvolutional neural network (CNN) for both aspect extraction and aspect-based\nsentiment analysis. We cast aspect extraction as a multi-label classification\nproblem, outputting probabilities over aspects parameterized by a threshold. To\ndetermine the sentiment towards an aspect, we concatenate an aspect vector with\nevery word embedding and apply a convolution over it. Our constrained system\n(unconstrained for English) achieves competitive results across all languages\nand domains, placing first or second in 5 and 7 out of 11 language-domain pairs\nfor aspect category detection (slot 1) and sentiment polarity (slot 3)\nrespectively, thereby demonstrating the viability of a deep learning-based\napproach for multilingual aspect-based sentiment analysis.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 11:23:51 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 10:04:18 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Ruder", "Sebastian", ""], ["Ghaffari", "Parsa", ""], ["Breslin", "John G.", ""]]}, {"id": "1609.02815", "submitter": "Felix Brockherde", "authors": "Felix Brockherde, Leslie Vogt, Li Li, Mark E. Tuckerman, Kieron Burke,\n  Klaus-Robert M\\\"uller", "title": "By-passing the Kohn-Sham equations with machine learning", "comments": null, "journal-ref": null, "doi": "10.1038/s41467-017-00839-3", "report-no": null, "categories": "physics.comp-ph cs.LG physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Last year, at least 30,000 scientific papers used the Kohn-Sham scheme of\ndensity functional theory to solve electronic structure problems in a wide\nvariety of scientific fields, ranging from materials science to biochemistry to\nastrophysics. Machine learning holds the promise of learning the kinetic energy\nfunctional via examples, by-passing the need to solve the Kohn-Sham equations.\nThis should yield substantial savings in computer time, allowing either larger\nsystems or longer time-scales to be tackled, but attempts to machine-learn this\nfunctional have been limited by the need to find its derivative. The present\nwork overcomes this difficulty by directly learning the density-potential and\nenergy-density maps for test systems and various molecules. Both improved\naccuracy and lower computational cost with this method are demonstrated by\nreproducing DFT energies for a range of molecular geometries generated during\nmolecular dynamics simulations. Moreover, the methodology could be applied\ndirectly to quantum chemical calculations, allowing construction of density\nfunctionals of quantum-chemical accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 14:45:48 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 09:24:40 GMT"}, {"version": "v3", "created": "Thu, 15 Jun 2017 08:19:46 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Brockherde", "Felix", ""], ["Vogt", "Leslie", ""], ["Li", "Li", ""], ["Tuckerman", "Mark E.", ""], ["Burke", "Kieron", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1609.02845", "submitter": "Shahin Shahrampour", "authors": "Shahin Shahrampour, Ali Jadbabaie", "title": "Distributed Online Optimization in Dynamic Environments Using Mirror\n  Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses decentralized online optimization in non-stationary\nenvironments. A network of agents aim to track the minimizer of a global\ntime-varying convex function. The minimizer evolves according to a known\ndynamics corrupted by an unknown, unstructured noise. At each time, the global\nfunction can be cast as a sum of a finite number of local functions, each of\nwhich is assigned to one agent in the network. Moreover, the local functions\nbecome available to agents sequentially, and agents do not have a prior\nknowledge of the future cost functions. Therefore, agents must communicate with\neach other to build an online approximation of the global function. We propose\na decentralized variation of the celebrated Mirror Descent, developed by\nNemirovksi and Yudin. Using the notion of Bregman divergence in lieu of\nEuclidean distance for projection, Mirror Descent has been shown to be a\npowerful tool in large-scale optimization. Our algorithm builds on Mirror\nDescent, while ensuring that agents perform a consensus step to follow the\nglobal function and take into account the dynamics of the global minimizer. To\nmeasure the performance of the proposed online algorithm, we compare it to its\noffline counterpart, where the global functions are available a priori. The gap\nbetween the two is called dynamic regret. We establish a regret bound that\nscales inversely in the spectral gap of the network, and more notably it\nrepresents the deviation of minimizer sequence with respect to the given\ndynamics. We then show that our results subsume a number of results in\ndistributed optimization. We demonstrate the application of our method to\ndecentralized tracking of dynamic parameters and verify the results via\nnumerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 16:00:04 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Shahrampour", "Shahin", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1609.02906", "submitter": "Pan Zhang", "authors": "Pan Zhang", "title": "Robust Spectral Detection of Global Structures in the Data by Learning a\n  Regularization", "comments": "13 pages, 9 figures, Neural Information Processing Systems 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral methods are popular in detecting global structures in the given data\nthat can be represented as a matrix. However when the data matrix is sparse or\nnoisy, classic spectral methods usually fail to work, due to localization of\neigenvectors (or singular vectors) induced by the sparsity or noise. In this\nwork, we propose a general method to solve the localization problem by learning\na regularization matrix from the localized eigenvectors. Using matrix\nperturbation analysis, we demonstrate that the learned regularizations suppress\ndown the eigenvalues associated with localized eigenvectors and enable us to\nrecover the informative eigenvectors representing the global structure. We show\napplications of our method in several inference problems: community detection\nin networks, clustering from pairwise similarities, rank estimation and matrix\ncompletion problems. Using extensive experiments, we illustrate that our method\nsolves the localization problem and works down to the theoretical detectability\nlimits in different kinds of synthetic data. This is in contrast with existing\nspectral algorithms based on data matrix, non-backtracking matrix, Laplacians\nand those with rank-one regularizations, which perform poorly in the sparse\ncase with noise.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 19:48:29 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Zhang", "Pan", ""]]}, {"id": "1609.02907", "submitter": "Thomas Kipf", "authors": "Thomas N. Kipf, Max Welling", "title": "Semi-Supervised Classification with Graph Convolutional Networks", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable approach for semi-supervised learning on\ngraph-structured data that is based on an efficient variant of convolutional\nneural networks which operate directly on graphs. We motivate the choice of our\nconvolutional architecture via a localized first-order approximation of\nspectral graph convolutions. Our model scales linearly in the number of graph\nedges and learns hidden layer representations that encode both local graph\nstructure and features of nodes. In a number of experiments on citation\nnetworks and on a knowledge graph dataset we demonstrate that our approach\noutperforms related methods by a significant margin.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 19:48:41 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 21:25:47 GMT"}, {"version": "v3", "created": "Thu, 3 Nov 2016 18:37:23 GMT"}, {"version": "v4", "created": "Wed, 22 Feb 2017 09:55:36 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Kipf", "Thomas N.", ""], ["Welling", "Max", ""]]}, {"id": "1609.02943", "submitter": "Florian Tram\\`er", "authors": "Florian Tram\\`er, Fan Zhang, Ari Juels, Michael K. Reiter, Thomas\n  Ristenpart", "title": "Stealing Machine Learning Models via Prediction APIs", "comments": "19 pages, 7 figures, Proceedings of USENIX Security 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) models may be deemed confidential due to their\nsensitive training data, commercial value, or use in security applications.\nIncreasingly often, confidential ML models are being deployed with publicly\naccessible query interfaces. ML-as-a-service (\"predictive analytics\") systems\nare an example: Some allow users to train models on potentially sensitive data\nand charge others for access on a pay-per-query basis.\n  The tension between model confidentiality and public access motivates our\ninvestigation of model extraction attacks. In such attacks, an adversary with\nblack-box access, but no prior knowledge of an ML model's parameters or\ntraining data, aims to duplicate the functionality of (i.e., \"steal\") the\nmodel. Unlike in classical learning theory settings, ML-as-a-service offerings\nmay accept partial feature vectors as inputs and include confidence values with\npredictions. Given these practices, we show simple, efficient attacks that\nextract target ML models with near-perfect fidelity for popular model classes\nincluding logistic regression, neural networks, and decision trees. We\ndemonstrate these attacks against the online services of BigML and Amazon\nMachine Learning. We further show that the natural countermeasure of omitting\nconfidence values from model outputs still admits potentially harmful model\nextraction attacks. Our results highlight the need for careful ML model\ndeployment and new model extraction countermeasures.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 20:39:20 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 02:44:14 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Tram\u00e8r", "Florian", ""], ["Zhang", "Fan", ""], ["Juels", "Ari", ""], ["Reiter", "Michael K.", ""], ["Ristenpart", "Thomas", ""]]}, {"id": "1609.02976", "submitter": "Nhien-An Le-Khac", "authors": "Fan Cai, Nhien-An Le-Khac, M-T. Kechadi", "title": "An Integrated Classification Model for Financial Data Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, financial data analysis is becoming increasingly important in the\nbusiness market. As companies collect more and more data from daily operations,\nthey expect to extract useful knowledge from existing collected data to help\nmake reasonable decisions for new customer requests, e.g. user credit category,\nchurn analysis, real estate analysis, etc. Financial institutes have applied\ndifferent data mining techniques to enhance their business performance.\nHowever, simple ap-proach of these techniques could raise a performance issue.\nBesides, there are very few general models for both understanding and\nforecasting different finan-cial fields. We present in this paper a new\nclassification model for analyzing fi-nancial data. We also evaluate this model\nwith different real-world data to show its performance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 23:45:19 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Cai", "Fan", ""], ["Le-Khac", "Nhien-An", ""], ["Kechadi", "M-T.", ""]]}, {"id": "1609.02993", "submitter": "Gabriel Synnaeve", "authors": "Nicolas Usunier, Gabriel Synnaeve, Zeming Lin, Soumith Chintala", "title": "Episodic Exploration for Deep Deterministic Policies: An Application to\n  StarCraft Micromanagement Tasks", "comments": "18 pages, 1 figure (2 plots), 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider scenarios from the real-time strategy game StarCraft as new\nbenchmarks for reinforcement learning algorithms. We propose micromanagement\ntasks, which present the problem of the short-term, low-level control of army\nmembers during a battle. From a reinforcement learning point of view, these\nscenarios are challenging because the state-action space is very large, and\nbecause there is no obvious feature representation for the state-action\nevaluation function. We describe our approach to tackle the micromanagement\nscenarios with deep neural network controllers from raw state features given by\nthe game engine. In addition, we present a heuristic reinforcement learning\nalgorithm which combines direct exploration in the policy space and\nbackpropagation. This algorithm allows for the collection of traces for\nlearning using deterministic policies, which appears much more efficient than,\nfor example, {\\epsilon}-greedy exploration. Experiments show that with this\nalgorithm, we successfully learn non-trivial strategies for scenarios with\narmies of up to 15 agents, where both Q-learning and REINFORCE struggle.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 02:13:02 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 00:18:48 GMT"}, {"version": "v3", "created": "Sat, 26 Nov 2016 19:02:20 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Usunier", "Nicolas", ""], ["Synnaeve", "Gabriel", ""], ["Lin", "Zeming", ""], ["Chintala", "Soumith", ""]]}, {"id": "1609.02997", "submitter": "Young Woong Park", "authors": "Young Woong Park, Diego Klabjan", "title": "Iteratively Reweighted Least Squares Algorithms for L1-Norm Principal\n  Component Analysis", "comments": null, "journal-ref": "2016 IEEE 16th International Conference on Data Mining, Barcelona,\n  Spain (2016): 430-438", "doi": "10.1109/ICDM.2016.0054", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is often used to reduce the dimension of\ndata by selecting a few orthonormal vectors that explain most of the variance\nstructure of the data. L1 PCA uses the L1 norm to measure error, whereas the\nconventional PCA uses the L2 norm. For the L1 PCA problem minimizing the\nfitting error of the reconstructed data, we propose an exact reweighted and an\napproximate algorithm based on iteratively reweighted least squares. We provide\nconvergence analyses, and compare their performance against benchmark\nalgorithms in the literature. The computational experiment shows that the\nproposed algorithms consistently perform best.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 04:06:07 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 21:22:38 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Park", "Young Woong", ""], ["Klabjan", "Diego", ""]]}, {"id": "1609.03054", "submitter": "Ana Ozaki", "authors": "Montserrat Hermo and Ana Ozaki", "title": "New Steps on the Exact Learning of CNF", "comments": "This work improves previous results in the paper: Exact Learning of\n  Multivalued Dependencies, published in ALT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major problem in computational learning theory is whether the class of\nformulas in conjunctive normal form (CNF) is efficiently learnable. Although it\nis known that this class cannot be polynomially learned using either membership\nor equivalence queries alone, it is open whether CNF can be polynomially\nlearned using both types of queries. One of the most important results\nconcerning a restriction of the class CNF is that propositional Horn formulas\nare polynomial time learnable in Angluin's exact learning model with membership\nand equivalence queries. In this work we push this boundary and show that the\nclass of multivalued dependency formulas (MVDF) is polynomially learnable from\ninterpretations. We then provide a notion of reduction between learning\nproblems in Angluin's model, showing that a transformation of the algorithm\nsuffices to efficiently learn multivalued database dependencies from data\nrelations. We also show via reductions that our main result extends well known\nprevious results and allows us to find alternative solutions for them.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 13:41:50 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Hermo", "Montserrat", ""], ["Ozaki", "Ana", ""]]}, {"id": "1609.03126", "submitter": "Junbo Zhao", "authors": "Junbo Zhao, Michael Mathieu and Yann LeCun", "title": "Energy-based Generative Adversarial Network", "comments": "Submitted to ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN)\nwhich views the discriminator as an energy function that attributes low\nenergies to the regions near the data manifold and higher energies to other\nregions. Similar to the probabilistic GANs, a generator is seen as being\ntrained to produce contrastive samples with minimal energies, while the\ndiscriminator is trained to assign high energies to these generated samples.\nViewing the discriminator as an energy function allows to use a wide variety of\narchitectures and loss functionals in addition to the usual binary classifier\nwith logistic output. Among them, we show one instantiation of EBGAN framework\nas using an auto-encoder architecture, with the energy being the reconstruction\nerror, in place of the discriminator. We show that this form of EBGAN exhibits\nmore stable behavior than regular GANs during training. We also show that a\nsingle-scale architecture can be trained to generate high-resolution images.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2016 07:11:13 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 18:07:30 GMT"}, {"version": "v3", "created": "Fri, 4 Nov 2016 20:04:33 GMT"}, {"version": "v4", "created": "Mon, 6 Mar 2017 22:52:53 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Zhao", "Junbo", ""], ["Mathieu", "Michael", ""], ["LeCun", "Yann", ""]]}, {"id": "1609.03164", "submitter": "Steven Van Vaerenbergh", "authors": "Steven Van Vaerenbergh, Jesus Fernandez-Bes, V\\'ictor Elvira", "title": "On the Relationship between Online Gaussian Process Regression and\n  Kernel Least Mean Squares Algorithms", "comments": "Accepted for publication in 2016 IEEE International Workshop on\n  Machine Learning for Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the relationship between online Gaussian process (GP) regression and\nkernel least mean squares (KLMS) algorithms. While the latter have no capacity\nof storing the entire posterior distribution during online learning, we\ndiscover that their operation corresponds to the assumption of a fixed\nposterior covariance that follows a simple parametric model. Interestingly,\nseveral well-known KLMS algorithms correspond to specific cases of this model.\nThe probabilistic perspective allows us to understand how each of them handles\nuncertainty, which could explain some of their performance differences.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2016 14:17:33 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Van Vaerenbergh", "Steven", ""], ["Fernandez-Bes", "Jesus", ""], ["Elvira", "V\u00edctor", ""]]}, {"id": "1609.03193", "submitter": "Gabriel Synnaeve", "authors": "Ronan Collobert, Christian Puhrsch, Gabriel Synnaeve", "title": "Wav2Letter: an End-to-End ConvNet-based Speech Recognition System", "comments": "8 pages, 4 figures (7 plots/schemas), 2 tables (4 tabulars)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple end-to-end model for speech recognition,\ncombining a convolutional network based acoustic model and a graph decoding. It\nis trained to output letters, with transcribed speech, without the need for\nforce alignment of phonemes. We introduce an automatic segmentation criterion\nfor training from sequence annotation without alignment that is on par with CTC\nwhile being simpler. We show competitive results in word error rate on the\nLibrispeech corpus with MFCC features, and promising results from raw waveform.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2016 18:56:53 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 02:49:05 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Collobert", "Ronan", ""], ["Puhrsch", "Christian", ""], ["Synnaeve", "Gabriel", ""]]}, {"id": "1609.03207", "submitter": "Massimo Stella", "authors": "Massimo Stella, Nicole M. Beckage and Markus Brede", "title": "Multiplex lexical networks reveal patterns in early word acquisition in\n  children", "comments": "11 pages, 3 figures and 1 table. This paper was published on\n  Scientific Reports: https://www.nature.com/articles/srep46730", "journal-ref": "Scientific Reports 7, Article number: 46730 (2017)", "doi": "10.1038/srep46730", "report-no": null, "categories": "physics.soc-ph cond-mat.dis-nn cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network models of language have provided a way of linking cognitive processes\nto the structure and connectivity of language. However, one shortcoming of\ncurrent approaches is focusing on only one type of linguistic relationship at a\ntime, missing the complex multi-relational nature of language. In this work, we\novercome this limitation by modelling the mental lexicon of English-speaking\ntoddlers as a multiplex lexical network, i.e. a multi-layered network where\nN=529 words/nodes are connected according to four types of relationships: (i)\nfree associations, (ii) feature sharing, (iii) co-occurrence, and (iv)\nphonological similarity. We provide analysis of the topology of the resulting\nmultiplex and then proceed to evaluate single layers as well as the full\nmultiplex structure on their ability to predict empirically observed age of\nacquisition data of English speaking toddlers. We find that the emerging\nmultiplex network topology is an important proxy of the cognitive processes of\nacquisition, capable of capturing emergent lexicon structure. In fact, we show\nthat the multiplex topology is fundamentally more powerful than individual\nlayers in predicting the ordering with which words are acquired. Furthermore,\nmultiplex analysis allows for a quantification of distinct phases of lexical\nacquisition in early learners: while initially all the multiplex layers\ncontribute to word learning, after about month 23 free associations take the\nlead in driving word acquisition.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2016 19:59:18 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 22:45:02 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Stella", "Massimo", ""], ["Beckage", "Nicole M.", ""], ["Brede", "Markus", ""]]}, {"id": "1609.03219", "submitter": "Shinichi Nakajima", "authors": "Wikor Pronobis, Danny Panknin, Johannes Kirschnick, Vignesh\n  Srinivasan, Wojciech Samek, Volker Markl, Manohar Kaul, Klaus-Robert Mueller,\n  Shinichi Nakajima", "title": "Sharing Hash Codes for Multiple Purposes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locality sensitive hashing (LSH) is a powerful tool for sublinear-time\napproximate nearest neighbor search, and a variety of hashing schemes have been\nproposed for different dissimilarity measures. However, hash codes\nsignificantly depend on the dissimilarity, which prohibits users from adjusting\nthe dissimilarity at query time. In this paper, we propose {multiple purpose\nLSH (mp-LSH) which shares the hash codes for different dissimilarities. mp-LSH\nsupports L2, cosine, and inner product dissimilarities, and their corresponding\nweighted sums, where the weights can be adjusted at query time. It also allows\nus to modify the importance of pre-defined groups of features. Thus, mp-LSH\nenables us, for example, to retrieve similar items to a query with the user\npreference taken into account, to find a similar material to a query with some\nproperties (stability, utility, etc.) optimized, and to turn on or off a part\nof multi-modal information (brightness, color, audio, text, etc.) in\nimage/video retrieval. We theoretically and empirically analyze the performance\nof three variants of mp-LSH, and demonstrate their usefulness on real-world\ndata sets.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2016 21:55:07 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 22:40:23 GMT"}, {"version": "v3", "created": "Thu, 1 Jun 2017 14:53:12 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Pronobis", "Wikor", ""], ["Panknin", "Danny", ""], ["Kirschnick", "Johannes", ""], ["Srinivasan", "Vignesh", ""], ["Samek", "Wojciech", ""], ["Markl", "Volker", ""], ["Kaul", "Manohar", ""], ["Mueller", "Klaus-Robert", ""], ["Nakajima", "Shinichi", ""]]}, {"id": "1609.03240", "submitter": "Anastasios Kyrillidis", "authors": "Dohyung Park, Anastasios Kyrillidis, Constantine Caramanis, Sujay\n  Sanghavi", "title": "Non-square matrix sensing without spurious local minima via the\n  Burer-Monteiro approach", "comments": "14 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the non-square matrix sensing problem, under restricted isometry\nproperty (RIP) assumptions. We focus on the non-convex formulation, where any\nrank-$r$ matrix $X \\in \\mathbb{R}^{m \\times n}$ is represented as $UV^\\top$,\nwhere $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$. In\nthis paper, we complement recent findings on the non-convex geometry of the\nanalogous PSD setting [5], and show that matrix factorization does not\nintroduce any spurious local minima, under RIP.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 01:20:45 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 00:57:44 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Park", "Dohyung", ""], ["Kyrillidis", "Anastasios", ""], ["Caramanis", "Constantine", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1609.03261", "submitter": "Lihua Lei", "authors": "Lihua Lei and Michael I. Jordan", "title": "Less than a Single Pass: Stochastically Controlled Stochastic Gradient\n  Method", "comments": "Add Lemma B.4", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and analyze a procedure for gradient-based optimization that we\nrefer to as stochastically controlled stochastic gradient (SCSG). As a member\nof the SVRG family of algorithms, SCSG makes use of gradient estimates at two\nscales, with the number of updates at the faster scale being governed by a\ngeometric random variable. Unlike most existing algorithms in this family, both\nthe computation cost and the communication cost of SCSG do not necessarily\nscale linearly with the sample size $n$; indeed, these costs are independent of\n$n$ when the target accuracy is low. An experimental evaluation on real\ndatasets confirms the effectiveness of SCSG.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 03:35:29 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 00:25:50 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 04:18:36 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Lei", "Lihua", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1609.03319", "submitter": "Nishant Mehta", "authors": "Nishant A. Mehta and Alistair Rendell and Anish Varghese and\n  Christfried Webers", "title": "CompAdaGrad: A Compressed, Complementary, Computationally-Efficient\n  Adaptive Gradient Method", "comments": "only updated acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adaptive gradient online learning method known as AdaGrad has seen\nwidespread use in the machine learning community in stochastic and adversarial\nonline learning problems and more recently in deep learning methods. The\nmethod's full-matrix incarnation offers much better theoretical guarantees and\npotentially better empirical performance than its diagonal version; however,\nthis version is computationally prohibitive and so the simpler diagonal version\noften is used in practice. We introduce a new method, CompAdaGrad, that\nnavigates the space between these two schemes and show that this method can\nyield results much better than diagonal AdaGrad while avoiding the (effectively\nintractable) $O(n^3)$ computational complexity of full-matrix AdaGrad for\ndimension $n$. CompAdaGrad essentially performs full-matrix regularization in a\nlow-dimensional subspace while performing diagonal regularization in the\ncomplementary subspace. We derive CompAdaGrad's updates for composite mirror\ndescent in case of the squared $\\ell_2$ norm and the $\\ell_1$ norm, demonstrate\nthat its complexity per iteration is linear in the dimension, and establish\nguarantees for the method independent of the choice of composite regularizer.\nFinally, we show preliminary results on several datasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 09:06:44 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 13:03:21 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Mehta", "Nishant A.", ""], ["Rendell", "Alistair", ""], ["Varghese", "Anish", ""], ["Webers", "Christfried", ""]]}, {"id": "1609.03321", "submitter": "Julius Hannink", "authors": "Julius Hannink, Thomas Kautz, Cristian F. Pasluosta, Jens Barth,\n  Samuel Sch\\\"ulein, Karl-G\\\"unter Ga{\\ss}mann, Jochen Klucken, Bjoern M.\n  Eskofier", "title": "Stride Length Estimation with Deep Learning", "comments": null, "journal-ref": null, "doi": "10.1109/JBHI.2017.2679486", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation of spatial gait characteristics is critical to assess\nmotor impairments resulting from neurological or musculoskeletal disease.\nCurrently, however, methodological constraints limit clinical applicability of\nstate-of-the-art double integration approaches to gait patterns with a clear\nzero-velocity phase. We describe a novel approach to stride length estimation\nthat uses deep convolutional neural networks to map stride-specific inertial\nsensor data to the resulting stride length. The model is trained on a publicly\navailable and clinically relevant benchmark dataset consisting of 1220 strides\nfrom 101 geriatric patients. Evaluation is done in a 10-fold cross validation\nand for three different stride definitions. Even though best results are\nachieved with strides defined from mid-stance to mid-stance with average\naccuracy and precision of 0.01 $\\pm$ 5.37 cm, performance does not strongly\ndepend on stride definition. The achieved precision outperforms\nstate-of-the-art methods evaluated on this benchmark dataset by 3.0 cm (36%).\nDue to the independence of stride definition, the proposed method is not\nsubject to the methodological constrains that limit applicability of\nstate-of-the-art double integration methods. Furthermore, precision on the\nbenchmark dataset could be improved. With more precise mobile stride length\nestimation, new insights to the progression of neurological disease or early\nindications might be gained. Due to the independence of stride definition,\npreviously uncharted diseases in terms of mobile gait analysis can now be\ninvestigated by re-training and applying the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 09:23:34 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 10:54:22 GMT"}, {"version": "v3", "created": "Thu, 9 Mar 2017 15:30:28 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Hannink", "Julius", ""], ["Kautz", "Thomas", ""], ["Pasluosta", "Cristian F.", ""], ["Barth", "Jens", ""], ["Sch\u00fclein", "Samuel", ""], ["Ga\u00dfmann", "Karl-G\u00fcnter", ""], ["Klucken", "Jochen", ""], ["Eskofier", "Bjoern M.", ""]]}, {"id": "1609.03323", "submitter": "Julius Hannink", "authors": "Julius Hannink, Thomas Kautz, Cristian F. Pasluosta, Karl-G\\\"unter\n  Ga{\\ss}mann, Jochen Klucken, Bjoern M. Eskofier", "title": "Sensor-based Gait Parameter Extraction with Deep Convolutional Neural\n  Networks", "comments": "in IEEE Journal of Biomedical and Health Informatics (2016)", "journal-ref": null, "doi": "10.1109/JBHI.2016.2636456", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measurement of stride-related, biomechanical parameters is the common\nrationale for objective gait impairment scoring. State-of-the-art double\nintegration approaches to extract these parameters from inertial sensor data\nare, however, limited in their clinical applicability due to the underlying\nassumptions. To overcome this, we present a method to translate the abstract\ninformation provided by wearable sensors to context-related expert features\nbased on deep convolutional neural networks. Regarding mobile gait analysis,\nthis enables integration-free and data-driven extraction of a set of 8\nspatio-temporal stride parameters. To this end, two modelling approaches are\ncompared: A combined network estimating all parameters of interest and an\nensemble approach that spawns less complex networks for each parameter\nindividually. The ensemble approach is outperforming the combined modelling in\nthe current application. On a clinically relevant and publicly available\nbenchmark dataset, we estimate stride length, width and medio-lateral change in\nfoot angle up to ${-0.15\\pm6.09}$ cm, ${-0.09\\pm4.22}$ cm and ${0.13 \\pm\n3.78^\\circ}$ respectively. Stride, swing and stance time as well as heel and\ntoe contact times are estimated up to ${\\pm 0.07}$, ${\\pm0.05}$, ${\\pm 0.07}$,\n${\\pm0.07}$ and ${\\pm0.12}$ s respectively. This is comparable to and in parts\noutperforming or defining state-of-the-art. Our results further indicate that\nthe proposed change in methodology could substitute assumption-driven\ndouble-integration methods and enable mobile assessment of spatio-temporal\nstride parameters in clinically critical situations as e.g. in the case of\nspastic gait impairments.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 09:33:57 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 10:56:32 GMT"}, {"version": "v3", "created": "Fri, 13 Jan 2017 12:30:39 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Hannink", "Julius", ""], ["Kautz", "Thomas", ""], ["Pasluosta", "Cristian F.", ""], ["Ga\u00dfmann", "Karl-G\u00fcnter", ""], ["Klucken", "Jochen", ""], ["Eskofier", "Bjoern M.", ""]]}, {"id": "1609.03344", "submitter": "Ning Xu", "authors": "Ning Xu, Jian Hong, Timothy C.G. Fisher", "title": "Finite-sample and asymptotic analysis of generalization ability with an\n  application to penalized regression", "comments": "The theoretical generalization and extension of arXiv:1606.00142", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST q-fin.EC stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the performance of extremum estimators from the\nperspective of generalization ability (GA): the ability of a model to predict\noutcomes in new samples from the same population. By adapting the classical\nconcentration inequalities, we derive upper bounds on the empirical\nout-of-sample prediction errors as a function of the in-sample errors,\nin-sample data size, heaviness in the tails of the error distribution, and\nmodel complexity. We show that the error bounds may be used for tuning key\nestimation hyper-parameters, such as the number of folds $K$ in\ncross-validation. We also show how $K$ affects the bias-variance trade-off for\ncross-validation. We demonstrate that the $\\mathcal{L}_2$-norm difference\nbetween penalized and the corresponding un-penalized regression estimates is\ndirectly explained by the GA of the estimates and the GA of empirical moment\nconditions. Lastly, we prove that all penalized regression estimates are\n$L_2$-consistent for both the $n \\geqslant p$ and the $n < p$ cases.\nSimulations are used to demonstrate key results.\n  Keywords: generalization ability, upper bound of generalization error,\npenalized regression, cross-validation, bias-variance trade-off,\n$\\mathcal{L}_2$ difference between penalized and unpenalized regression, lasso,\nhigh-dimensional data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 11:09:50 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 09:34:17 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Xu", "Ning", ""], ["Hong", "Jian", ""], ["Fisher", "Timothy C. G.", ""]]}, {"id": "1609.03348", "submitter": "Thomas Ward", "authors": "Thomas H. Ward", "title": "A Threshold-based Scheme for Reinforcement Learning in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generic and scalable Reinforcement Learning scheme for Artificial Neural\nNetworks is presented, providing a general purpose learning machine. By\nreference to a node threshold three features are described 1) A mechanism for\nPrimary Reinforcement, capable of solving linearly inseparable problems 2) The\nlearning scheme is extended to include a mechanism for Conditioned\nReinforcement, capable of forming long term strategy 3) The learning scheme is\nmodified to use a threshold-based deep learning algorithm, providing a robust\nand biologically inspired alternative to backpropagation. The model may be used\nfor supervised as well as unsupervised training regimes.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 11:23:20 GMT"}, {"version": "v2", "created": "Sat, 17 Sep 2016 04:20:01 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2016 05:11:01 GMT"}, {"version": "v4", "created": "Sat, 14 Jan 2017 05:54:29 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Ward", "Thomas H.", ""]]}, {"id": "1609.03426", "submitter": "Sayantan Dasgupta", "authors": "Sayantan Dasgupta", "title": "Multi-Label Learning with Provable Guarantee", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we study the problem of learning labels for large text corpora where\neach text can be assigned a variable number of labels. The problem might seem\ntrivial when the label dimensionality is small and can be easily solved using a\nseries of one-vs-all classifiers. However, as the label dimensionality\nincreases to several thousand, the parameter space becomes extremely large, and\nit is no longer possible to use the one-vs-all technique. Here we propose a\nmodel based on the factorization of higher order moments of the words in the\ncorpora, as well as the cross moment between the labels and the words for\nmulti-label prediction. Our model provides guaranteed convergence bounds on the\nestimated parameters. Further, our model takes only three passes through the\ntraining dataset to extract the parameters, resulting in a highly scalable\nalgorithm that can train on GB's of data consisting of millions of documents\nwith hundreds of thousands of labels using a nominal resource of a single\nprocessor with 16GB RAM. Our model achieves 10x-15x order of speed-up on\nlarge-scale datasets while producing competitive performance in comparison with\nexisting benchmark algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 14:38:08 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 23:26:50 GMT"}, {"version": "v3", "created": "Sun, 18 Sep 2016 14:57:20 GMT"}, {"version": "v4", "created": "Tue, 1 Nov 2016 16:21:54 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Dasgupta", "Sayantan", ""]]}, {"id": "1609.03448", "submitter": "Sundeep Prabhakar Chepuri", "authors": "Sundeep Prabhakar Chepuri, Sijia Liu, Geert Leus, Alfred O. Hero III", "title": "Learning Sparse Graphs Under Smoothness Prior", "comments": "ICASSP 2017 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in learning the underlying graph structure\nbehind training data. Solving this basic problem is essential to carry out any\ngraph signal processing or machine learning task. To realize this, we assume\nthat the data is smooth with respect to the graph topology, and we parameterize\nthe graph topology using an edge sampling function. That is, the graph\nLaplacian is expressed in terms of a sparse edge selection vector, which\nprovides an explicit handle to control the sparsity level of the graph. We\nsolve the sparse graph learning problem given some training data in both the\nnoiseless and noisy settings. Given the true smooth data, the posed sparse\ngraph learning problem can be solved optimally and is based on simple rank\nordering. Given the noisy data, we show that the joint sparse graph learning\nand denoising problem can be simplified to designing only the sparse edge\nselection vector, which can be solved using convex optimization.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 15:31:20 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Chepuri", "Sundeep Prabhakar", ""], ["Liu", "Sijia", ""], ["Leus", "Geert", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1609.03490", "submitter": "Ritambhara Singh", "authors": "Ritambhara Singh, Jack Lanchantin, Gabriel Robins, and Yanjun Qi", "title": "Transfer String Kernel for Cross-Context DNA-Protein Binding Prediction", "comments": "To be published in IEEE/ACM Transactions on Computational Biology and\n  Bioinformatics", "journal-ref": null, "doi": "10.1109/TCBB.2016.2609918", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through sequence-based classification, this paper tries to accurately predict\nthe DNA binding sites of transcription factors (TFs) in an unannotated cellular\ncontext. Related methods in the literature fail to perform such predictions\naccurately, since they do not consider sample distribution shift of sequence\nsegments from an annotated (source) context to an unannotated (target) context.\nWe, therefore, propose a method called \"Transfer String Kernel\" (TSK) that\nachieves improved prediction of transcription factor binding site (TFBS) using\nknowledge transfer via cross-context sample adaptation. TSK maps sequence\nsegments to a high-dimensional feature space using a discriminative mismatch\nstring kernel framework. In this high-dimensional space, labeled examples of\nthe source context are re-weighted so that the revised sample distribution\nmatches the target context more closely. We have experimentally verified TSK\nfor TFBS identifications on fourteen different TFs under a cross-organism\nsetting. We find that TSK consistently outperforms the state-of the-art TFBS\ntools, especially when working with TFs whose binding sequences are not\nconserved across contexts. We also demonstrate the generalizability of TSK by\nshowing its cutting-edge performance on a different set of cross-context tasks\nfor the MHC peptide binding predictions.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 17:16:24 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Singh", "Ritambhara", ""], ["Lanchantin", "Jack", ""], ["Robins", "Gabriel", ""], ["Qi", "Yanjun", ""]]}, {"id": "1609.03499", "submitter": "A\\\"aron van den Oord", "authors": "Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol\n  Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu", "title": "WaveNet: A Generative Model for Raw Audio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces WaveNet, a deep neural network for generating raw audio\nwaveforms. The model is fully probabilistic and autoregressive, with the\npredictive distribution for each audio sample conditioned on all previous ones;\nnonetheless we show that it can be efficiently trained on data with tens of\nthousands of samples per second of audio. When applied to text-to-speech, it\nyields state-of-the-art performance, with human listeners rating it as\nsignificantly more natural sounding than the best parametric and concatenative\nsystems for both English and Mandarin. A single WaveNet can capture the\ncharacteristics of many different speakers with equal fidelity, and can switch\nbetween them by conditioning on the speaker identity. When trained to model\nmusic, we find that it generates novel and often highly realistic musical\nfragments. We also show that it can be employed as a discriminative model,\nreturning promising results for phoneme recognition.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 17:29:40 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 18:04:35 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Oord", "Aaron van den", ""], ["Dieleman", "Sander", ""], ["Zen", "Heiga", ""], ["Simonyan", "Karen", ""], ["Vinyals", "Oriol", ""], ["Graves", "Alex", ""], ["Kalchbrenner", "Nal", ""], ["Senior", "Andrew", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1609.03540", "submitter": "Babak Salimi", "authors": "Babak Salimi, Dan Suciu", "title": "ZaliQL: A SQL-Based Framework for Drawing Causal Inference from Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference from observational data is a subject of active research and\ndevelopment in statistics and computer science. Many toolkits have been\ndeveloped for this purpose that depends on statistical software. However, these\ntoolkits do not scale to large datasets. In this paper we describe a suite of\ntechniques for expressing causal inference tasks from observational data in\nSQL. This suite supports the state-of-the-art methods for causal inference and\nrun at scale within a database engine. In addition, we introduce several\noptimization techniques that significantly speedup causal inference, both in\nthe online and offline setting. We evaluate the quality and performance of our\ntechniques by experiments of real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 19:24:14 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 01:59:05 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Salimi", "Babak", ""], ["Suciu", "Dan", ""]]}, {"id": "1609.03541", "submitter": "David Schwab", "authors": "David J. Schwab, Pankaj Mehta", "title": "Comment on \"Why does deep and cheap learning work so well?\"\n  [arXiv:1608.08225]", "comments": "Comment on arXiv:1608.08225", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper, \"Why does deep and cheap learning work so well?\", Lin and\nTegmark claim to show that the mapping between deep belief networks and the\nvariational renormalization group derived in [arXiv:1410.3831] is invalid, and\npresent a \"counterexample\" that claims to show that this mapping does not hold.\nIn this comment, we show that these claims are incorrect and stem from a\nmisunderstanding of the variational RG procedure proposed by Kadanoff. We also\nexplain why the \"counterexample\" of Lin and Tegmark is compatible with the\nmapping proposed in [arXiv:1410.3831].\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 19:25:27 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Schwab", "David J.", ""], ["Mehta", "Pankaj", ""]]}, {"id": "1609.03544", "submitter": "Xin Jiang", "authors": "Xin Jiang, Rebecca Willett", "title": "Online Data Thinning via Multi-Subspace Tracking", "comments": "32 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an era of ubiquitous large-scale streaming data, the availability of data\nfar exceeds the capacity of expert human analysts. In many settings, such data\nis either discarded or stored unprocessed in datacenters. This paper proposes a\nmethod of online data thinning, in which large-scale streaming datasets are\nwinnowed to preserve unique, anomalous, or salient elements for timely expert\nanalysis. At the heart of this proposed approach is an online anomaly detection\nmethod based on dynamic, low-rank Gaussian mixture models. Specifically, the\nhigh-dimensional covariances matrices associated with the Gaussian components\nare associated with low-rank models. According to this model, most observations\nlie near a union of subspaces. The low-rank modeling mitigates the curse of\ndimensionality associated with anomaly detection for high-dimensional data, and\nrecent advances in subspace clustering and subspace tracking allow the proposed\nmethod to adapt to dynamic environments. Furthermore, the proposed method\nallows subsampling, is robust to missing data, and uses a mini-batch online\noptimization approach. The resulting algorithms are scalable, efficient, and\nare capable of operating in real time. Experiments on wide-area motion imagery\nand e-mail databases illustrate the efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 19:34:02 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Jiang", "Xin", ""], ["Willett", "Rebecca", ""]]}, {"id": "1609.03628", "submitter": "Ren Mao", "authors": "Ren Mao, John S. Baras, Yezhou Yang, Cornelia Fermuller", "title": "Co-active Learning to Adapt Humanoid Movement for Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of robot movement adaptation under\nvarious environmental constraints interactively. Motion primitives are\ngenerally adopted to generate target motion from demonstrations. However, their\ngeneralization capability is weak while facing novel environments.\nAdditionally, traditional motion generation methods do not consider the\nversatile constraints from various users, tasks, and environments. In this\nwork, we propose a co-active learning framework for learning to adapt robot\nend-effector's movement for manipulation tasks. It is designed to adapt the\noriginal imitation trajectories, which are learned from demonstrations, to\nnovel situations with various constraints. The framework also considers user's\nfeedback towards the adapted trajectories, and it learns to adapt movement\nthrough human-in-the-loop interactions. The implemented system generalizes\ntrained motion primitives to various situations with different constraints\nconsidering user preferences. Experiments on a humanoid platform validate the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 22:57:37 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Mao", "Ren", ""], ["Baras", "John S.", ""], ["Yang", "Yezhou", ""], ["Fermuller", "Cornelia", ""]]}, {"id": "1609.03663", "submitter": "Tong Wang", "authors": "Tong Wang, Ping Chen, Kevin Amaral, Jipeng Qiang", "title": "An Experimental Study of LSTM Encoder-Decoder Model for Text\n  Simplification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text simplification (TS) aims to reduce the lexical and structural complexity\nof a text, while still retaining the semantic meaning. Current automatic TS\ntechniques are limited to either lexical-level applications or manually\ndefining a large amount of rules. Since deep neural networks are powerful\nmodels that have achieved excellent performance over many difficult tasks, in\nthis paper, we propose to use the Long Short-Term Memory (LSTM) Encoder-Decoder\nmodel for sentence level TS, which makes minimal assumptions about word\nsequence. We conduct preliminary experiments to find that the model is able to\nlearn operation rules such as reversing, sorting and replacing from sequence\npairs, which shows that the model may potentially discover and apply rules such\nas modifying sentence structure, substituting words, and removing words for TS.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 03:02:32 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Wang", "Tong", ""], ["Chen", "Ping", ""], ["Amaral", "Kevin", ""], ["Qiang", "Jipeng", ""]]}, {"id": "1609.03666", "submitter": "S\\'ebastien Arnold", "authors": "S\\'ebastien Arnold", "title": "A Greedy Algorithm to Cluster Specialists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent deep neural networks experiments leverage the\ngeneralist-specialist paradigm for classification. However, no formal study\ncompared the performance of different clustering algorithms for class\nassignment. In this paper we perform such a study, suggest slight modifications\nto the clustering procedures, and propose a novel algorithm designed to\noptimize the performance of of the specialist-generalist classification system.\nOur experiments on the CIFAR-10 and CIFAR-100 datasets allow us to investigate\nsituations for varying number of classes on similar data. We find that our\n\\emph{greedy pairs} clustering algorithm consistently outperforms other\nalternatives, while the choice of the confusion matrix has little impact on the\nfinal performance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 03:26:42 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Arnold", "S\u00e9bastien", ""]]}, {"id": "1609.03675", "submitter": "Hanjun Dai", "authors": "Hanjun Dai, Yichen Wang, Rakshit Trivedi, Le Song", "title": "Deep Coevolutionary Network: Embedding User and Item Features for\n  Recommendation", "comments": "Recsys Workshop on Deep Learning for Recommendation Systems (DLRS\n  '16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems often use latent features to explain the behaviors of\nusers and capture the properties of items. As users interact with different\nitems over time, user and item features can influence each other, evolve and\nco-evolve over time. The compatibility of user and item's feature further\ninfluence the future interaction between users and items. Recently, point\nprocess based models have been proposed in the literature aiming to capture the\ntemporally evolving nature of these latent features. However, these models\noften make strong parametric assumptions about the evolution process of the\nuser and item latent features, which may not reflect the reality, and has\nlimited power in expressing the complex and nonlinear dynamics underlying these\nprocesses. To address these limitations, we propose a novel deep coevolutionary\nnetwork model (DeepCoevolve), for learning user and item features based on\ntheir interaction graph. DeepCoevolve use recurrent neural network (RNN) over\nevolving networks to define the intensity function in point processes, which\nallows the model to capture complex mutual influence between users and items,\nand the feature evolution over time. We also develop an efficient procedure for\ntraining the model parameters, and show that the learned models lead to\nsignificant improvements in recommendation and activity prediction compared to\nprevious state-of-the-arts parametric models.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 04:39:33 GMT"}, {"version": "v2", "created": "Sat, 5 Nov 2016 00:25:39 GMT"}, {"version": "v3", "created": "Wed, 9 Nov 2016 04:12:13 GMT"}, {"version": "v4", "created": "Tue, 28 Feb 2017 05:37:37 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Dai", "Hanjun", ""], ["Wang", "Yichen", ""], ["Trivedi", "Rakshit", ""], ["Song", "Le", ""]]}, {"id": "1609.03677", "submitter": "Cl\\'ement Godard", "authors": "Cl\\'ement Godard, Oisin Mac Aodha and Gabriel J. Brostow", "title": "Unsupervised Monocular Depth Estimation with Left-Right Consistency", "comments": "CVPR 2017 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning based methods have shown very promising results for the task of\ndepth estimation in single images. However, most existing approaches treat\ndepth prediction as a supervised regression problem and as a result, require\nvast quantities of corresponding ground truth depth data for training. Just\nrecording quality depth data in a range of environments is a challenging\nproblem. In this paper, we innovate beyond existing approaches, replacing the\nuse of explicit depth data during training with easier-to-obtain binocular\nstereo footage.\n  We propose a novel training objective that enables our convolutional neural\nnetwork to learn to perform single image depth estimation, despite the absence\nof ground truth depth data. Exploiting epipolar geometry constraints, we\ngenerate disparity images by training our network with an image reconstruction\nloss. We show that solving for image reconstruction alone results in poor\nquality depth images. To overcome this problem, we propose a novel training\nloss that enforces consistency between the disparities produced relative to\nboth the left and right images, leading to improved performance and robustness\ncompared to existing approaches. Our method produces state of the art results\nfor monocular depth estimation on the KITTI driving dataset, even outperforming\nsupervised methods that have been trained with ground truth depth.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 04:48:31 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 17:18:17 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 14:40:50 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Godard", "Cl\u00e9ment", ""], ["Mac Aodha", "Oisin", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1609.03683", "submitter": "Giorgio Patrini", "authors": "Giorgio Patrini, Alessandro Rozza, Aditya Menon, Richard Nock, Lizhen\n  Qu", "title": "Making Deep Neural Networks Robust to Label Noise: a Loss Correction\n  Approach", "comments": "Oral paper at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a theoretically grounded approach to train deep neural networks,\nincluding recurrent networks, subject to class-dependent label noise. We\npropose two procedures for loss correction that are agnostic to both\napplication domain and network architecture. They simply amount to at most a\nmatrix inversion and multiplication, provided that we know the probability of\neach class being corrupted into another. We further show how one can estimate\nthese probabilities, adapting a recent technique for noise estimation to the\nmulti-class setting, and thus providing an end-to-end framework. Extensive\nexperiments on MNIST, IMDB, CIFAR-10, CIFAR-100 and a large scale dataset of\nclothing images employing a diversity of architectures --- stacking dense,\nconvolutional, pooling, dropout, batch normalization, word embedding, LSTM and\nresidual layers --- demonstrate the noise robustness of our proposals.\nIncidentally, we also prove that, when ReLU is the only non-linearity, the loss\ncurvature is immune to class-dependent label noise.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 05:23:29 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 08:48:02 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Patrini", "Giorgio", ""], ["Rozza", "Alessandro", ""], ["Menon", "Aditya", ""], ["Nock", "Richard", ""], ["Qu", "Lizhen", ""]]}, {"id": "1609.03759", "submitter": "Edward Johns", "authors": "Stephen James and Edward Johns", "title": "3D Simulation for Robot Arm Control with Deep Q-Learning", "comments": "In NIPS 2016 Workshop: Deep Learning for Action and Interaction\n  (https://sites.google.com/site/nips16interaction/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent trends in robot arm control have seen a shift towards end-to-end\nsolutions, using deep reinforcement learning to learn a controller directly\nfrom raw sensor data, rather than relying on a hand-crafted, modular pipeline.\nHowever, the high dimensionality of the state space often means that it is\nimpractical to generate sufficient training data with real-world experiments.\nAs an alternative solution, we propose to learn a robot controller in\nsimulation, with the potential of then transferring this to a real robot.\nBuilding upon the recent success of deep Q-networks, we present an approach\nwhich uses 3D simulations to train a 7-DOF robotic arm in a control task\nwithout any prior knowledge. The controller accepts images of the environment\nas its only input, and outputs motor actions for the task of locating and\ngrasping a cube, over a range of initial configurations. To encourage efficient\nlearning, a structured reward function is designed with intermediate rewards.\nWe also present preliminary results in direct transfer of policies over to a\nreal robot, without any further training.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 10:40:24 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 16:09:17 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["James", "Stephen", ""], ["Johns", "Edward", ""]]}, {"id": "1609.03769", "submitter": "Daniele Calandriello", "authors": "Daniele Calandriello, Alessandro Lazaric and Michal Valko", "title": "Analysis of Kelner and Levin graph sparsification algorithm for a\n  streaming setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a new proof to show that the incremental resparsification algorithm\nproposed by Kelner and Levin (2013) produces a spectral sparsifier in high\nprobability. We rigorously take into account the dependencies across subsequent\nresparsifications using martingale inequalities, fixing a flaw in the original\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 11:18:03 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Calandriello", "Daniele", ""], ["Lazaric", "Alessandro", ""], ["Valko", "Michal", ""]]}, {"id": "1609.03772", "submitter": "Nguyen Tran Quang", "authors": "Nguyen Tran Quang and Alexander Jung", "title": "Learning conditional independence structure for high-dimensional\n  uncorrelated vector processes", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate and analyze a graphical model selection method for inferring the\nconditional independence graph of a high-dimensional nonstationary Gaussian\nrandom process (time series) from a finite-length observation. The observed\nprocess samples are assumed uncorrelated over time and having a time-varying\nmarginal distribution. The selection method is based on testing conditional\nvariances obtained for small subsets of process components. This allows to cope\nwith the high-dimensional regime, where the sample size can be (drastically)\nsmaller than the process dimension. We characterize the required sample size\nsuch that the proposed selection method is successful with high probability.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 11:35:12 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Quang", "Nguyen Tran", ""], ["Jung", "Alexander", ""]]}, {"id": "1609.03777", "submitter": "Kyuyeon Hwang", "authors": "Kyuyeon Hwang, Wonyong Sung", "title": "Character-Level Language Modeling with Hierarchical Recurrent Neural\n  Networks", "comments": "Submitted to NIPS 2016 on May 20, 2016 (v1), accepted to ICASSP 2017\n  (v2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network (RNN) based character-level language models (CLMs)\nare extremely useful for modeling out-of-vocabulary words by nature. However,\ntheir performance is generally much worse than the word-level language models\n(WLMs), since CLMs need to consider longer history of tokens to properly\npredict the next one. We address this problem by proposing hierarchical RNN\narchitectures, which consist of multiple modules with different timescales.\nDespite the multi-timescale structures, the input and output layers operate\nwith the character-level clock, which allows the existing RNN CLM training\napproaches to be directly applicable without any modifications. Our CLM models\nshow better perplexity than Kneser-Ney (KN) 5-gram WLMs on the One Billion Word\nBenchmark with only 2% of parameters. Also, we present real-time\ncharacter-level end-to-end speech recognition examples on the Wall Street\nJournal (WSJ) corpus, where replacing traditional mono-clock RNN CLMs with the\nproposed models results in better recognition accuracies even though the number\nof parameters are reduced to 30%.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 11:41:48 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 13:49:41 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Hwang", "Kyuyeon", ""], ["Sung", "Wonyong", ""]]}, {"id": "1609.03894", "submitter": "Francisco Massa", "authors": "Francisco Massa, Renaud Marlet, Mathieu Aubry", "title": "Crafting a multi-task CNN for viewpoint estimation", "comments": "To appear in BMVC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) were recently shown to provide\nstate-of-the-art results for object category viewpoint estimation. However\ndifferent ways of formulating this problem have been proposed and the competing\napproaches have been explored with very different design choices. This paper\npresents a comparison of these approaches in a unified setting as well as a\ndetailed analysis of the key factors that impact performance. Followingly, we\npresent a new joint training method with the detection task and demonstrate its\nbenefit. We also highlight the superiority of classification approaches over\nregression approaches, quantify the benefits of deeper architectures and\nextended training data, and demonstrate that synthetic data is beneficial even\nwhen using ImageNet training data. By combining all these elements, we\ndemonstrate an improvement of approximately 5% mAVP over previous\nstate-of-the-art results on the Pascal3D+ dataset. In particular for their most\nchallenging 24 view classification task we improve the results from 31.1% to\n36.1% mAVP.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 15:19:38 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Massa", "Francisco", ""], ["Marlet", "Renaud", ""], ["Aubry", "Mathieu", ""]]}, {"id": "1609.03912", "submitter": "Kevin Moon", "authors": "Kevin R. Moon, Morteza Noshad, Salimeh Yasaei Sekeh, Alfred O. Hero\n  III", "title": "Information Theoretic Structure Learning with Confidence", "comments": "10 pages, 3 figures", "journal-ref": "IEEE International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP), pp. 6095-6099, Mar. 2017", "doi": "10.1109/ICASSP.2017.7953327", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theoretic measures (e.g. the Kullback Liebler divergence and\nShannon mutual information) have been used for exploring possibly nonlinear\nmultivariate dependencies in high dimension. If these dependencies are assumed\nto follow a Markov factor graph model, this exploration process is called\nstructure discovery. For discrete-valued samples, estimates of the information\ndivergence over the parametric class of multinomial models lead to structure\ndiscovery methods whose mean squared error achieves parametric convergence\nrates as the sample size grows. However, a naive application of this method to\ncontinuous nonparametric multivariate models converges much more slowly. In\nthis paper we introduce a new method for nonparametric structure discovery that\nuses weighted ensemble divergence estimators that achieve parametric\nconvergence rates and obey an asymptotic central limit theorem that facilitates\nhypothesis testing and other types of statistical validation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 16:20:02 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Moon", "Kevin R.", ""], ["Noshad", "Morteza", ""], ["Sekeh", "Salimeh Yasaei", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1609.03958", "submitter": "Akshay Soni", "authors": "Akshay Soni, Troy Chevalier, Swayambhoo Jain", "title": "Noisy Inductive Matrix Completion Under Sparse Factor Models", "comments": "5 pages. arXiv admin note: text overlap with arXiv:1411.0282", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inductive Matrix Completion (IMC) is an important class of matrix completion\nproblems that allows direct inclusion of available features to enhance\nestimation capabilities. These models have found applications in personalized\nrecommendation systems, multilabel learning, dictionary learning, etc. This\npaper examines a general class of noisy matrix completion tasks where the\nunderlying matrix is following an IMC model i.e., it is formed by a mixing\nmatrix (a priori unknown) sandwiched between two known feature matrices. The\nmixing matrix here is assumed to be well approximated by the product of two\nsparse matrices---referred here to as \"sparse factor models.\" We leverage the\nmain theorem of Soni:2016:NMC and extend it to provide theoretical error bounds\nfor the sparsity-regularized maximum likelihood estimators for the class of\nproblems discussed in this paper. The main result is general in the sense that\nit can be used to derive error bounds for various noise models. In this paper,\nwe instantiate our main result for the case of Gaussian noise and provide\ncorresponding error bounds in terms of squared loss.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 18:08:06 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Soni", "Akshay", ""], ["Chevalier", "Troy", ""], ["Jain", "Swayambhoo", ""]]}, {"id": "1609.03960", "submitter": "Chu Wang", "authors": "Bernard Chazelle, Chu Wang", "title": "Self-Sustaining Iterated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important result from psycholinguistics (Griffiths & Kalish, 2005) states\nthat no language can be learned iteratively by rational agents in a\nself-sustaining manner. We show how to modify the learning process slightly in\norder to achieve self-sustainability. Our work is in two parts. First, we\ncharacterize iterated learnability in geometric terms and show how a slight,\nsteady increase in the lengths of the training sessions ensures\nself-sustainability for any discrete language class. In the second part, we\ntackle the nondiscrete case and investigate self-sustainability for iterated\nlinear regression. We discuss the implications of our findings to issues of\nnon-equilibrium dynamics in natural algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 18:18:38 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Chazelle", "Bernard", ""], ["Wang", "Chu", ""]]}, {"id": "1609.04104", "submitter": "Morteza Mardani", "authors": "Morteza Mardani, Georgios B. Giannakis, and Kamil Ugurbil", "title": "Tracking Tensor Subspaces with Informative Random Sampling for Real-Time\n  MR Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) nowadays serves as an important modality for\ndiagnostic and therapeutic guidance in clinics. However, the {\\it slow\nacquisition} process, the dynamic deformation of organs, as well as the need\nfor {\\it real-time} reconstruction, pose major challenges toward obtaining\nartifact-free images. To cope with these challenges, the present paper\nadvocates a novel subspace learning framework that permeates benefits from\nparallel factor (PARAFAC) decomposition of tensors (multiway data) to low-rank\nmodeling of temporal sequence of images. Treating images as multiway data\narrays, the novel method preserves spatial structures and unravels the latent\ncorrelations across various dimensions by means of the tensor subspace.\nLeveraging the spatio-temporal correlation of images, Tykhonov regularization\nis adopted as a rank surrogate for a least-squares optimization program.\nAlteranating majorization minimization is adopted to develop online algorithms\nthat recursively procure the reconstruction upon arrival of a new undersampled\n$k$-space frame. The developed algorithms are {\\it provably convergent} and\nhighly {\\it parallelizable} with lightweight FFT tasks per iteration. To\nfurther accelerate the acquisition process, randomized subsampling policies are\ndevised that leverage intermediate estimates of the tensor subspace, offered by\nthe online scheme, to {\\it randomly} acquire {\\it informative} $k$-space\nsamples. In a nutshell, the novel approach enables tracking motion dynamics\nunder low acquisition rates `on the fly.' GPU-based tests with real {\\it in\nvivo} MRI datasets of cardiac cine images corroborate the merits of the novel\napproach relative to state-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 01:23:05 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Mardani", "Morteza", ""], ["Giannakis", "Georgios B.", ""], ["Ugurbil", "Kamil", ""]]}, {"id": "1609.04117", "submitter": "Joseph Chow", "authors": "Susan Jia Xu, Mehdi Nourinejad, Xuebo Lai, Joseph Y. J. Chow", "title": "Network learning via multi-agent inverse transportation problems", "comments": null, "journal-ref": "Transportation Science 52(6) 1347-1364 (2018)", "doi": "10.1287/trsc.2017.0805", "report-no": null, "categories": "cs.MA cs.CE cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the ubiquity of transportation data, methods to infer the state\nparameters of a network either ignore sensitivity of route decisions, require\nroute enumeration for parameterizing descriptive models of route selection, or\nrequire complex bilevel models of route assignment behavior. These limitations\nprevent modelers from fully exploiting ubiquitous data in monitoring\ntransportation networks. Inverse optimization methods that capture network\nroute choice behavior can address this gap, but they are designed to take\nobservations of the same model to learn the parameters of that model, which is\nstatistically inefficient (e.g. requires estimating population route and link\nflows). New inverse optimization models and supporting algorithms are proposed\nto learn the parameters of heterogeneous travelers' route behavior to infer\nshared network state parameters (e.g. link capacity dual prices). The inferred\nvalues are consistent with observations of each agent's optimization behavior.\nWe prove that the method can obtain unique dual prices for a network shared by\nthese agents in polynomial time. Four experiments are conducted. The first one,\nconducted on a 4-node network, verifies the methodology to obtain heterogeneous\nlink cost parameters even when multinomial or mixed logit models would not be\nmeaningfully estimated. The second is a parameter recovery test on the\nNguyen-Dupuis network that shows that unique latent link capacity dual prices\ncan be inferred using the proposed method. The third test on the same network\ndemonstrates how a monitoring system in an online learning environment can be\ndesigned using this method. The last test demonstrates this learning on real\ndata obtained from a freeway network in Queens, New York, using only real-time\nGoogle Maps queries.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 03:07:18 GMT"}, {"version": "v2", "created": "Sun, 18 Sep 2016 18:53:26 GMT"}, {"version": "v3", "created": "Mon, 26 Jun 2017 18:20:35 GMT"}, {"version": "v4", "created": "Thu, 7 Sep 2017 22:52:22 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Xu", "Susan Jia", ""], ["Nourinejad", "Mehdi", ""], ["Lai", "Xuebo", ""], ["Chow", "Joseph Y. J.", ""]]}, {"id": "1609.04167", "submitter": "Laurent Jacques", "authors": "V. Abrol, O. Absil, P.-A. Absil, S. Anthoine, P. Antoine, T. Arildsen,\n  N. Bertin, F. Bleichrodt, J. Bobin, A. Bol, A. Bonnefoy, F. Caltagirone, V.\n  Cambareri, C. Chenot, V. Crnojevi\\'c, M. Da\\v{n}kov\\'a, K. Degraux, J.\n  Eisert, J. M. Fadili, M. Gabri\\'e, N. Gac, D. Giacobello, A. Gonzalez, C. A.\n  Gomez Gonzalez, A. Gonz\\'alez, P.-Y. Gousenbourger, M. Gr{\\ae}sb{\\o}ll\n  Christensen, R. Gribonval, S. Gu\\'erit, S. Huang, P. Irofti, L. Jacques, U.\n  S. Kamilov, S. Kitic\\'c, M. Kliesch, F. Krzakala, J. A. Lee, W. Liao, T.\n  Lindstr{\\o}m Jensen, A. Manoel, H. Mansour, A. Mohammad-Djafari, A.\n  Moshtaghpour, F. Ngol\\`e, B. Pairet, M. Pani\\'c, G. Peyr\\'e, A. Pi\\v{z}urica,\n  P. Rajmic, M. Roblin, I. Roth, A. K. Sao, P. Sharma, J.-L. Starck, E. W.\n  Tramel, T. van Waterschoot, D. Vukobratovic, L. Wang, B. Wirth, G. Wunder, H.\n  Zhang", "title": "Proceedings of the third \"international Traveling Workshop on\n  Interactions between Sparse models and Technology\" (iTWIST'16)", "comments": "69 pages, 22 extended abstracts, iTWIST'16 website:\n  http://www.itwist16.es.aau.dk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The third edition of the \"international - Traveling Workshop on Interactions\nbetween Sparse models and Technology\" (iTWIST) took place in Aalborg, the 4th\nlargest city in Denmark situated beautifully in the northern part of the\ncountry, from the 24th to 26th of August 2016. The workshop venue was at the\nAalborg University campus. One implicit objective of this biennial workshop is\nto foster collaboration between international scientific teams by disseminating\nideas through both specific oral/poster presentations and free discussions. For\nthis third edition, iTWIST'16 gathered about 50 international participants and\nfeatures 8 invited talks, 12 oral presentations, and 12 posters on the\nfollowing themes, all related to the theory, application and generalization of\nthe \"sparsity paradigm\": Sparsity-driven data sensing and processing (e.g.,\noptics, computer vision, genomics, biomedical, digital communication, channel\nestimation, astronomy); Application of sparse models in non-convex/non-linear\ninverse problems (e.g., phase retrieval, blind deconvolution, self\ncalibration); Approximate probabilistic inference for sparse problems; Sparse\nmachine learning and inference; \"Blind\" inverse problems and dictionary\nlearning; Optimization for sparse modelling; Information theory, geometry and\nrandomness; Sparsity? What's next? (Discrete-valued signals; Union of\nlow-dimensional spaces, Cosparsity, mixed/group norm, model-based,\nlow-complexity models, ...); Matrix/manifold sensing/processing (graph,\nlow-rank approximation, ...); Complexity/accuracy tradeoffs in numerical\nmethods/optimization; Electronic/optical compressive sensors (hardware).\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 08:27:11 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Abrol", "V.", ""], ["Absil", "O.", ""], ["Absil", "P. -A.", ""], ["Anthoine", "S.", ""], ["Antoine", "P.", ""], ["Arildsen", "T.", ""], ["Bertin", "N.", ""], ["Bleichrodt", "F.", ""], ["Bobin", "J.", ""], ["Bol", "A.", ""], ["Bonnefoy", "A.", ""], ["Caltagirone", "F.", ""], ["Cambareri", "V.", ""], ["Chenot", "C.", ""], ["Crnojevi\u0107", "V.", ""], ["Da\u0148kov\u00e1", "M.", ""], ["Degraux", "K.", ""], ["Eisert", "J.", ""], ["Fadili", "J. M.", ""], ["Gabri\u00e9", "M.", ""], ["Gac", "N.", ""], ["Giacobello", "D.", ""], ["Gonzalez", "A.", ""], ["Gonzalez", "C. A. Gomez", ""], ["Gonz\u00e1lez", "A.", ""], ["Gousenbourger", "P. -Y.", ""], ["Christensen", "M. Gr\u00e6sb\u00f8ll", ""], ["Gribonval", "R.", ""], ["Gu\u00e9rit", "S.", ""], ["Huang", "S.", ""], ["Irofti", "P.", ""], ["Jacques", "L.", ""], ["Kamilov", "U. S.", ""], ["Kitic\u0107", "S.", ""], ["Kliesch", "M.", ""], ["Krzakala", "F.", ""], ["Lee", "J. A.", ""], ["Liao", "W.", ""], ["Jensen", "T. Lindstr\u00f8m", ""], ["Manoel", "A.", ""], ["Mansour", "H.", ""], ["Mohammad-Djafari", "A.", ""], ["Moshtaghpour", "A.", ""], ["Ngol\u00e8", "F.", ""], ["Pairet", "B.", ""], ["Pani\u0107", "M.", ""], ["Peyr\u00e9", "G.", ""], ["Pi\u017eurica", "A.", ""], ["Rajmic", "P.", ""], ["Roblin", "M.", ""], ["Roth", "I.", ""], ["Sao", "A. K.", ""], ["Sharma", "P.", ""], ["Starck", "J. -L.", ""], ["Tramel", "E. W.", ""], ["van Waterschoot", "T.", ""], ["Vukobratovic", "D.", ""], ["Wang", "L.", ""], ["Wirth", "B.", ""], ["Wunder", "G.", ""], ["Zhang", "H.", ""]]}, {"id": "1609.04212", "submitter": "Neil Bramley", "authors": "Neil R. Bramley, Peter Dayan, Thomas L. Griffiths and David A. Lagnado", "title": "Formalizing Neurath's Ship: Approximate Algorithms for Online Causal\n  Learning", "comments": null, "journal-ref": "Psychological Review, Vol 124(3), Apr 2017, 301-338", "doi": "10.1037/rev0000061", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-level cognition depends on the ability to learn models of the world.\nWe can characterize this at the computational level as a structure-learning\nproblem with the goal of best identifying the prevailing causal relationships\namong a set of relata. However, the computational cost of performing exact\nBayesian inference over causal models grows rapidly as the number of relata\nincreases. This implies that the cognitive processes underlying causal learning\nmust be substantially approximate. A powerful class of approximations that\nfocuses on the sequential absorption of successive inputs is captured by the\nNeurath's ship metaphor in philosophy of science, where theory change is cast\nas a stochastic and gradual process shaped as much by people's limited\nwillingness to abandon their current theory when considering alternatives as by\nthe ground truth they hope to approach. Inspired by this metaphor and by\nalgorithms for approximating Bayesian inference in machine learning, we propose\nan algorithmic-level model of causal structure learning under which learners\nrepresent only a single global hypothesis that they update locally as they\ngather evidence. We propose a related scheme for understanding how, under these\nlimitations, learners choose informative interventions that manipulate the\ncausal system to help elucidate its workings. We find support for our approach\nin the analysis of four experiments.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 10:44:51 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 00:16:28 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 16:45:06 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Bramley", "Neil R.", ""], ["Dayan", "Peter", ""], ["Griffiths", "Thomas L.", ""], ["Lagnado", "David A.", ""]]}, {"id": "1609.04243", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi, George Fazekas, Mark Sandler, Kyunghyun Cho", "title": "Convolutional Recurrent Neural Networks for Music Classification", "comments": "5 pages, ICASSP 2017 submitted. Revised to fix previous CNN\n  architectures and update experiment results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a convolutional recurrent neural network (CRNN) for music\ntagging. CRNNs take advantage of convolutional neural networks (CNNs) for local\nfeature extraction and recurrent neural networks for temporal summarisation of\nthe extracted features. We compare CRNN with three CNN structures that have\nbeen used for music tagging while controlling the number of parameters with\nrespect to their performance and training time per sample. Overall, we found\nthat CRNNs show a strong performance with respect to the number of parameter\nand training time, indicating the effectiveness of its hybrid structure in\nmusic feature extraction and feature summarisation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 12:52:08 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 07:50:14 GMT"}, {"version": "v3", "created": "Wed, 21 Dec 2016 06:52:30 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "George", ""], ["Sandler", "Mark", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1609.04309", "submitter": "Edouard Grave", "authors": "Edouard Grave, Armand Joulin, Moustapha Ciss\\'e, David Grangier,\n  Herv\\'e J\\'egou", "title": "Efficient softmax approximation for GPUs", "comments": "Accepted to ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approximate strategy to efficiently train neural network based\nlanguage models over very large vocabularies. Our approach, called adaptive\nsoftmax, circumvents the linear dependency on the vocabulary size by exploiting\nthe unbalanced word distribution to form clusters that explicitly minimize the\nexpectation of computation time. Our approach further reduces the computational\ntime by exploiting the specificities of modern architectures and matrix-matrix\nvector operations, making it particularly suited for graphical processing\nunits. Our experiments carried out on standard benchmarks, such as EuroParl and\nOne Billion Word, show that our approach brings a large gain in efficiency over\nstandard approximations while achieving an accuracy close to that of the full\nsoftmax. The code of our method is available at\nhttps://github.com/facebookresearch/adaptive-softmax.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 15:15:08 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 21:42:39 GMT"}, {"version": "v3", "created": "Mon, 19 Jun 2017 16:33:04 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Grave", "Edouard", ""], ["Joulin", "Armand", ""], ["Ciss\u00e9", "Moustapha", ""], ["Grangier", "David", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1609.04321", "submitter": "Luca Masera", "authors": "Luca Masera, Enrico Blanzieri", "title": "Very Simple Classifier: a Concept Binary Classifier toInvestigate\n  Features Based on Subsampling and Localility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Very Simple Classifier (VSC) a novel method designed to\nincorporate the concepts of subsampling and locality in the definition of\nfeatures to be used as the input of a perceptron. The rationale is that\nlocality theoretically guarantees a bound on the generalization error. Each\nfeature in VSC is a max-margin classifier built on randomly-selected pairs of\nsamples. The locality in VSC is achieved by multiplying the value of the\nfeature by a confidence measure that can be characterized in terms of the\nChebichev inequality. The output of the layer is then fed in a output layer of\nneurons. The weights of the output layer are then determined by a regularized\npseudoinverse. Extensive comparison of VSC against 9 competitors in the task of\nbinary classification is carried out. Results on 22 benchmark datasets with\nfixed parameters show that VSC is competitive with the Multi Layer Perceptron\n(MLP) and outperforms the other competitors. An exploration of the parameter\nspace shows VSC can outperform MLP.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 15:51:46 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Masera", "Luca", ""], ["Blanzieri", "Enrico", ""]]}, {"id": "1609.04375", "submitter": "Ge Wang", "authors": "Ge Wang", "title": "A Perspective on Deep Imaging", "comments": "9 pages, 10 figures, 49 references, and accepted by IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of tomographic imaging and deep learning, or machine learning\nin general, promises to empower not only image analysis but also image\nreconstruction. The latter aspect is considered in this perspective article\nwith an emphasis on medical imaging to develop a new generation of image\nreconstruction theories and techniques. This direction might lead to\nintelligent utilization of domain knowledge from big data, innovative\napproaches for image reconstruction, and superior performance in clinical and\npreclinical applications. To realize the full impact of machine learning on\nmedical imaging, major challenges must be addressed.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 15:45:48 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 13:02:27 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Wang", "Ge", ""]]}, {"id": "1609.04436", "submitter": "Mohammad Ghavamzadeh", "authors": "Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar", "title": "Bayesian Reinforcement Learning: A Survey", "comments": null, "journal-ref": "Foundations and Trends in Machine Learning, Vol. 8: No. 5-6, pp\n  359-492, 2015", "doi": "10.1561/2200000049", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods for machine learning have been widely investigated, yielding\nprincipled methods for incorporating prior information into inference\nalgorithms. In this survey, we provide an in-depth review of the role of\nBayesian methods for the reinforcement learning (RL) paradigm. The major\nincentives for incorporating Bayesian reasoning in RL are: 1) it provides an\nelegant approach to action-selection (exploration/exploitation) as a function\nof the uncertainty in learning; and 2) it provides a machinery to incorporate\nprior knowledge into the algorithms. We first discuss models and methods for\nBayesian inference in the simple single-step Bandit model. We then review the\nextensive recent literature on Bayesian methods for model-based RL, where prior\ninformation can be expressed on the parameters of the Markov model. We also\npresent Bayesian methods for model-free RL, where priors are expressed over the\nvalue function or policy class. The objective of the paper is to provide a\ncomprehensive survey on Bayesian RL algorithms and their theoretical and\nempirical properties.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 20:34:26 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Ghavamzadeh", "Mohammad", ""], ["Mannor", "Shie", ""], ["Pineau", "Joelle", ""], ["Tamar", "Aviv", ""]]}, {"id": "1609.04468", "submitter": "Tom White", "authors": "Tom White", "title": "Sampling Generative Networks", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce several techniques for sampling and visualizing the latent\nspaces of generative models. Replacing linear interpolation with spherical\nlinear interpolation prevents diverging from a model's prior distribution and\nproduces sharper samples. J-Diagrams and MINE grids are introduced as\nvisualizations of manifolds created by analogies and nearest neighbors. We\ndemonstrate two new techniques for deriving attribute vectors: bias-corrected\nvectors with data replication and synthetic vectors with data augmentation.\nBinary classification using attribute vectors is presented as a technique\nsupporting quantitative analysis of the latent space. Most techniques are\nintended to be independent of model type and examples are shown on both\nVariational Autoencoders and Generative Adversarial Networks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 22:42:23 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 09:38:48 GMT"}, {"version": "v3", "created": "Tue, 6 Dec 2016 14:39:05 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["White", "Tom", ""]]}, {"id": "1609.04495", "submitter": "Richard Nock", "authors": "Boris Muzellec and Richard Nock and Giorgio Patrini and Frank Nielsen", "title": "Tsallis Regularized Optimal Transport and Ecological Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal transport is a powerful framework for computing distances between\nprobability distributions. We unify the two main approaches to optimal\ntransport, namely Monge-Kantorovitch and Sinkhorn-Cuturi, into what we define\nas Tsallis regularized optimal transport (\\trot). \\trot~interpolates a rich\nfamily of distortions from Wasserstein to Kullback-Leibler, encompassing as\nwell Pearson, Neyman and Hellinger divergences, to name a few. We show that\nmetric properties known for Sinkhorn-Cuturi generalize to \\trot, and provide\nefficient algorithms for finding the optimal transportation plan with formal\nconvergence proofs. We also present the first application of optimal transport\nto the problem of ecological inference, that is, the reconstruction of joint\ndistributions from their marginals, a problem of large interest in the social\nsciences. \\trot~provides a convenient framework for ecological inference by\nallowing to compute the joint distribution --- that is, the optimal\ntransportation plan itself --- when side information is available, which is\n\\textit{e.g.} typically what census represents in political science.\nExperiments on data from the 2012 US presidential elections display the\npotential of \\trot~in delivering a faithful reconstruction of the joint\ndistribution of ethnic groups and voter preferences.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 02:30:10 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Muzellec", "Boris", ""], ["Nock", "Richard", ""], ["Patrini", "Giorgio", ""], ["Nielsen", "Frank", ""]]}, {"id": "1609.04508", "submitter": "Trang Pham", "authors": "Trang Pham, Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Column Networks for Collective Classification", "comments": "Accepted at AAAI'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational learning deals with data that are characterized by relational\nstructures. An important task is collective classification, which is to jointly\nclassify networked objects. While it holds a great promise to produce a better\naccuracy than non-collective classifiers, collective classification is\ncomputational challenging and has not leveraged on the recent breakthroughs of\ndeep learning. We present Column Network (CLN), a novel deep learning model for\ncollective classification in multi-relational domains. CLN has many desirable\ntheoretical properties: (i) it encodes multi-relations between any two\ninstances; (ii) it is deep and compact, allowing complex functions to be\napproximated at the network level with a small set of free parameters; (iii)\nlocal and relational features are learned simultaneously; (iv) long-range,\nhigher-order dependencies between instances are supported naturally; and (v)\ncrucially, learning and inference are efficient, linear in the size of the\nnetwork and the number of relations. We evaluate CLN on multiple real-world\napplications: (a) delay prediction in software projects, (b) PubMed Diabetes\npublication classification and (c) film genre classification. In all\napplications, CLN demonstrates a higher accuracy than state-of-the-art rivals.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 04:45:11 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 03:59:26 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Pham", "Trang", ""], ["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1609.04557", "submitter": "Sebastian Ewert", "authors": "Sebastian Ewert and Mark B. Sandler", "title": "Structured Dropout for Weak Label and Multi-Instance Learning and Its\n  Application to Score-Informed Source Separation", "comments": null, "journal-ref": "Proceedings of the IEEE International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP), New Orleans, USA, pp. 2277-2281, 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many success stories involving deep neural networks are instances of\nsupervised learning, where available labels power gradient-based learning\nmethods. Creating such labels, however, can be expensive and thus there is\nincreasing interest in weak labels which only provide coarse information, with\nuncertainty regarding time, location or value. Using such labels often leads to\nconsiderable challenges for the learning process. Current methods for\nweak-label training often employ standard supervised approaches that\nadditionally reassign or prune labels during the learning process. The\ninformation gain, however, is often limited as only the importance of labels\nwhere the network already yields reasonable results is boosted. We propose\ntreating weak-label training as an unsupervised problem and use the labels to\nguide the representation learning to induce structure. To this end, we propose\ntwo autoencoder extensions: class activity penalties and structured dropout. We\ndemonstrate the capabilities of our approach in the context of score-informed\nsource separation of music.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 09:50:55 GMT"}, {"version": "v2", "created": "Mon, 26 Dec 2016 14:28:46 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Ewert", "Sebastian", ""], ["Sandler", "Mark B.", ""]]}, {"id": "1609.04608", "submitter": "Andres Hoyos Idrobo", "authors": "Andr\\'es Hoyos-Idrobo (PARIETAL, NEUROSPIN), Ga\\\"el Varoquaux\n  (PARIETAL, NEUROSPIN), Jonas Kahn, Bertrand Thirion (PARIETAL)", "title": "Recursive nearest agglomeration (ReNA): fast clustering for\n  approximation of structured signals", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  Institute of Electrical and Electronics Engineers, In press", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2815524", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we revisit fast dimension reduction approaches, as with random\nprojections and random sampling. Our goal is to summarize the data to decrease\ncomputational costs and memory footprint of subsequent analysis. Such dimension\nreduction can be very efficient when the signals of interest have a strong\nstructure, such as with images. We focus on this setting and investigate\nfeature clustering schemes for data reductions that capture this structure. An\nimpediment to fast dimension reduction is that good clustering comes with large\nalgorithmic costs. We address it by contributing a linear-time agglomerative\nclustering scheme, Recursive Nearest Agglomeration (ReNA). Unlike existing fast\nagglomerative schemes, it avoids the creation of giant clusters. We empirically\nvalidate that it approximates the data as well as traditional\nvariance-minimizing clustering schemes that have a quadratic complexity. In\naddition, we analyze signal approximation with feature clustering and show that\nit can remove noise, improving subsequent analysis steps. As a consequence,\ndata reduction by clustering features with ReNA yields very fast and accurate\nmodels, enabling to process large datasets on budget. Our theoretical analysis\nis backed by extensive experiments on publicly-available data that illustrate\nthe computation efficiency and the denoising properties of the resulting\ndimension reduction scheme.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 12:46:52 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 08:44:46 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Hoyos-Idrobo", "Andr\u00e9s", "", "PARIETAL, NEUROSPIN"], ["Varoquaux", "Ga\u00ebl", "", "PARIETAL, NEUROSPIN"], ["Kahn", "Jonas", "", "PARIETAL"], ["Thirion", "Bertrand", "", "PARIETAL"]]}, {"id": "1609.04746", "submitter": "Robert Hannah", "authors": "Robert Hannah, Wotao Yin", "title": "On Unbounded Delays in Asynchronous Parallel Fixed-Point Algorithms", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for scalable numerical solutions has motivated the development of\nasynchronous parallel algorithms, where a set of nodes run in parallel with\nlittle or no synchronization, thus computing with delayed information. This\npaper studies the convergence of the asynchronous parallel algorithm ARock\nunder potentially unbounded delays.\n  ARock is a general asynchronous algorithm that has many applications. It\nparallelizes fixed-point iterations by letting a set of nodes randomly choose\nsolution coordinates and update them in an asynchronous parallel fashion. ARock\ntakes some recent asynchronous coordinate descent algorithms as special cases\nand gives rise to new asynchronous operator-splitting algorithms. Existing\nanalysis of ARock assumes the delays to be bounded, and uses this bound to set\na step size that is important to both convergence and efficiency. Other work,\nthough allowing unbounded delays, imposes strict conditions on the underlying\nfixed-point operator, resulting in limited applications.\n  In this paper, convergence is established under unbounded delays, which can\nbe either stochastic or deterministic. The proposed step sizes are more\npractical and generally larger than those in the existing work. The step size\nadapts to the delay distribution or the current delay being experienced in the\nsystem. New Lyapunov functions, which are the key to analyzing asynchronous\nalgorithms, are generated to obtain our results. A set of applicable\noptimization algorithms with large-scale applications are given, including\nmachine learning and scientific computing algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 17:30:28 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 05:20:28 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Hannah", "Robert", ""], ["Yin", "Wotao", ""]]}, {"id": "1609.04747", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder", "title": "An overview of gradient descent optimization algorithms", "comments": "Added derivations of AdaMax and Nadam", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient descent optimization algorithms, while increasingly popular, are\noften used as black-box optimizers, as practical explanations of their\nstrengths and weaknesses are hard to come by. This article aims to provide the\nreader with intuitions with regard to the behaviour of different algorithms\nthat will allow her to put them to use. In the course of this overview, we look\nat different variants of gradient descent, summarize challenges, introduce the\nmost common optimization algorithms, review architectures in a parallel and\ndistributed setting, and investigate additional strategies for optimizing\ngradient descent.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 17:32:34 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 13:21:04 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Ruder", "Sebastian", ""]]}, {"id": "1609.04789", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani, George Atia", "title": "Coherence Pursuit: Fast, Simple, and Robust Principal Component Analysis", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing ( Volume: 65, Issue: 23,\n  Dec.1, 1 2017 )", "doi": "10.1109/TSP.2017.2749215", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a remarkably simple, yet powerful, algorithm termed\nCoherence Pursuit (CoP) to robust Principal Component Analysis (PCA). As\ninliers lie in a low dimensional subspace and are mostly correlated, an inlier\nis likely to have strong mutual coherence with a large number of data points.\nBy contrast, outliers either do not admit low dimensional structures or form\nsmall clusters. In either case, an outlier is unlikely to bear strong\nresemblance to a large number of data points. Given that, CoP sets an outlier\napart from an inlier by comparing their coherence with the rest of the data\npoints. The mutual coherences are computed by forming the Gram matrix of the\nnormalized data points. Subsequently, the sought subspace is recovered from the\nspan of the subset of the data points that exhibit strong coherence with the\nrest of the data. As CoP only involves one simple matrix multiplication, it is\nsignificantly faster than the state-of-the-art robust PCA algorithms. We derive\nanalytical performance guarantees for CoP under different models for the\ndistributions of inliers and outliers in both noise-free and noisy settings.\nCoP is the first robust PCA algorithm that is simultaneously non-iterative,\nprovably robust to both unstructured and structured outliers, and can tolerate\na large number of unstructured outliers.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 19:25:55 GMT"}, {"version": "v2", "created": "Sun, 22 Jan 2017 00:13:24 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 01:23:18 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1609.04836", "submitter": "Nitish Shirish Keskar", "authors": "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail\n  Smelyanskiy and Ping Tak Peter Tang", "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp\n  Minima", "comments": "Accepted as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic gradient descent (SGD) method and its variants are algorithms\nof choice for many Deep Learning tasks. These methods operate in a small-batch\nregime wherein a fraction of the training data, say $32$-$512$ data points, is\nsampled to compute an approximation to the gradient. It has been observed in\npractice that when using a larger batch there is a degradation in the quality\nof the model, as measured by its ability to generalize. We investigate the\ncause for this generalization drop in the large-batch regime and present\nnumerical evidence that supports the view that large-batch methods tend to\nconverge to sharp minimizers of the training and testing functions - and as is\nwell known, sharp minima lead to poorer generalization. In contrast,\nsmall-batch methods consistently converge to flat minimizers, and our\nexperiments support a commonly held view that this is due to the inherent noise\nin the gradient estimation. We discuss several strategies to attempt to help\nlarge-batch methods eliminate this generalization gap.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 20:03:06 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 20:38:16 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Keskar", "Nitish Shirish", ""], ["Mudigere", "Dheevatsa", ""], ["Nocedal", "Jorge", ""], ["Smelyanskiy", "Mikhail", ""], ["Tang", "Ping Tak Peter", ""]]}, {"id": "1609.04849", "submitter": "Mark Harmon", "authors": "Mark Harmon, Abdolghani Ebrahimi, Patrick Lucey, Diego Klabjan", "title": "Predicting Shot Making in Basketball Learnt from Adversarial Multiagent\n  Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we predict the likelihood of a player making a shot in\nbasketball from multiagent trajectories. Previous approaches to similar\nproblems center on hand-crafting features to capture domain specific knowledge.\nAlthough intuitive, recent work in deep learning has shown this approach is\nprone to missing important predictive features. To circumvent this issue, we\npresent a convolutional neural network (CNN) approach where we initially\nrepresent the multiagent behavior as an image. To encode the adversarial nature\nof basketball, we use a multi-channel image which we then feed into a CNN.\nAdditionally, to capture the temporal aspect of the trajectories we \"fade\" the\nplayer trajectories. We find that this approach is superior to a traditional\nFFN model. By using gradient ascent to create images using an already trained\nCNN, we discover what features the CNN filters learn. Last, we find that a\ncombined CNN+FFN is the best performing network with an error rate of 39%.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 20:27:26 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 19:30:35 GMT"}, {"version": "v3", "created": "Fri, 24 Feb 2017 22:22:26 GMT"}, {"version": "v4", "created": "Thu, 28 Dec 2017 18:37:06 GMT"}, {"version": "v5", "created": "Fri, 15 Jan 2021 22:56:48 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Harmon", "Mark", ""], ["Ebrahimi", "Abdolghani", ""], ["Lucey", "Patrick", ""], ["Klabjan", "Diego", ""]]}, {"id": "1609.04938", "submitter": "Yuntian Deng", "authors": "Yuntian Deng, Anssi Kanervisto, Jeffrey Ling, Alexander M. Rush", "title": "Image-to-Markup Generation with Coarse-to-Fine Attention", "comments": "Accepted by ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural encoder-decoder model to convert images into\npresentational markup based on a scalable coarse-to-fine attention mechanism.\nOur method is evaluated in the context of image-to-LaTeX generation, and we\nintroduce a new dataset of real-world rendered mathematical expressions paired\nwith LaTeX markup. We show that unlike neural OCR techniques using CTC-based\nmodels, attention-based approaches can tackle this non-standard OCR task. Our\napproach outperforms classical mathematical OCR systems by a large margin on\nin-domain rendered data, and, with pretraining, also performs well on\nout-of-domain handwritten data. To reduce the inference complexity associated\nwith the attention-based approaches, we introduce a new coarse-to-fine\nattention layer that selects a support region before applying attention.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 08:14:50 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 22:48:53 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Deng", "Yuntian", ""], ["Kanervisto", "Anssi", ""], ["Ling", "Jeffrey", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1609.04994", "submitter": "Jan Leike", "authors": "Jan Leike", "title": "Exploration Potential", "comments": "10 pages, including proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce exploration potential, a quantity that measures how much a\nreinforcement learning agent has explored its environment class. In contrast to\ninformation gain, exploration potential takes the problem's reward structure\ninto account. This leads to an exploration criterion that is both necessary and\nsufficient for asymptotic optimality (learning to act optimally across the\nentire environment class). Our experiments in multi-armed bandits use\nexploration potential to illustrate how different algorithms make the tradeoff\nbetween exploration and exploitation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 10:55:27 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 14:22:32 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2016 11:17:56 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Leike", "Jan", ""]]}, {"id": "1609.05058", "submitter": "Jan Leike", "authors": "Jan Leike, Jessica Taylor, Benya Fallenstein", "title": "A Formal Solution to the Grain of Truth Problem", "comments": "UAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian agent acting in a multi-agent environment learns to predict the\nother agents' policies if its prior assigns positive probability to them (in\nother words, its prior contains a \\emph{grain of truth}). Finding a reasonably\nlarge class of policies that contains the Bayes-optimal policies with respect\nto this class is known as the \\emph{grain of truth problem}. Only small classes\nare known to have a grain of truth and the literature contains several related\nimpossibility results. In this paper we present a formal and general solution\nto the full grain of truth problem: we construct a class of policies that\ncontains all computable policies as well as Bayes-optimal policies for every\nlower semicomputable prior over the class. When the environment is unknown,\nBayes-optimal agents may fail to act optimally even asymptotically. However,\nagents based on Thompson sampling converge to play {\\epsilon}-Nash equilibria\nin arbitrary unknown computable multi-agent environments. While these results\nare purely theoretical, we show that they can be computationally approximated\narbitrarily closely.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 14:00:51 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Leike", "Jan", ""], ["Taylor", "Jessica", ""], ["Fallenstein", "Benya", ""]]}, {"id": "1609.05123", "submitter": "Hamid Tizhoosh", "authors": "Shivam Kalra, Aditya Sriram, Shahryar Rahnamayan, H.R. Tizhoosh", "title": "Learning Opposites Using Neural Networks", "comments": "To appear in proceedings of the 23rd International Conference on\n  Pattern Recognition (ICPR 2016), Cancun, Mexico, December 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many research works have successfully extended algorithms such as\nevolutionary algorithms, reinforcement agents and neural networks using\n\"opposition-based learning\" (OBL). Two types of the \"opposites\" have been\ndefined in the literature, namely \\textit{type-I} and \\textit{type-II}. The\nformer are linear in nature and applicable to the variable space, hence easy to\ncalculate. On the other hand, type-II opposites capture the \"oppositeness\" in\nthe output space. In fact, type-I opposites are considered a special case of\ntype-II opposites where inputs and outputs have a linear relationship. However,\nin many real-world problems, inputs and outputs do in fact exhibit a nonlinear\nrelationship. Therefore, type-II opposites are expected to be better in\ncapturing the sense of \"opposition\" in terms of the input-output relation. In\nthe absence of any knowledge about the problem at hand, there seems to be no\nintuitive way to calculate the type-II opposites. In this paper, we introduce\nan approach to learn type-II opposites from the given inputs and their outputs\nusing the artificial neural networks (ANNs). We first perform \\emph{opposition\nmining} on the sample data, and then use the mined data to learn the\nrelationship between input $x$ and its opposite $\\breve{x}$. We have validated\nour algorithm using various benchmark functions to compare it against an\nevolving fuzzy inference approach that has been recently introduced. The\nresults show the better performance of a neural approach to learn the\nopposites. This will create new possibilities for integrating oppositional\nschemes within existing algorithms promising a potential increase in\nconvergence speed and/or accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 16:19:56 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Kalra", "Shivam", ""], ["Sriram", "Aditya", ""], ["Rahnamayan", "Shahryar", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "1609.05162", "submitter": "Wen Sun", "authors": "Wen Sun, Niteesh Sood, Debadeepta Dey, Gireeja Ranade, Siddharth\n  Prakash and Ashish Kapoor", "title": "No-Regret Replanning under Uncertainty", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the problem of path planning under uncertainty.\nSpecifically, we consider online receding horizon based planners that need to\noperate in a latent environment where the latent information can be modeled via\nGaussian Processes. Online path planning in latent environments is challenging\nsince the robot needs to explore the environment to get a more accurate model\nof latent information for better planning later and also achieves the task as\nquick as possible. We propose UCB style algorithms that are popular in the\nbandit settings and show how those analyses can be adapted to the online\nrobotic path planning problems. The proposed algorithm trades-off exploration\nand exploitation in near-optimal manner and has appealing no-regret properties.\nWe demonstrate the efficacy of the framework on the application of aircraft\nflight path planning when the winds are partially observed.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 18:07:49 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Sun", "Wen", ""], ["Sood", "Niteesh", ""], ["Dey", "Debadeepta", ""], ["Ranade", "Gireeja", ""], ["Prakash", "Siddharth", ""], ["Kapoor", "Ashish", ""]]}, {"id": "1609.05181", "submitter": "Ravi  Tandon", "authors": "Mohamed Attia, Ravi Tandon", "title": "Information Theoretic Limits of Data Shuffling for Distributed Learning", "comments": "To be presented at IEEE GLOBECOM, December 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data shuffling is one of the fundamental building blocks for distributed\nlearning algorithms, that increases the statistical gain for each step of the\nlearning process. In each iteration, different shuffled data points are\nassigned by a central node to a distributed set of workers to perform local\ncomputations, which leads to communication bottlenecks. The focus of this paper\nis on formalizing and understanding the fundamental information-theoretic\ntrade-off between storage (per worker) and the worst-case communication\noverhead for the data shuffling problem. We completely characterize the\ninformation theoretic trade-off for $K=2$, and $K=3$ workers, for any value of\nstorage capacity, and show that increasing the storage across workers can\nreduce the communication overhead by leveraging coding. We propose a novel and\nsystematic data delivery and storage update strategy for each data shuffle\niteration, which preserves the structural properties of the storage across the\nworkers, and aids in minimizing the communication overhead in subsequent data\nshuffling iterations.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 19:12:42 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Attia", "Mohamed", ""], ["Tandon", "Ravi", ""]]}, {"id": "1609.05191", "submitter": "Tengyu Ma", "authors": "Moritz Hardt, Tengyu Ma, Benjamin Recht", "title": "Gradient Descent Learns Linear Dynamical Systems", "comments": "updated with more experimental results and references to prior work;\n  published in JMLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that stochastic gradient descent efficiently converges to the global\noptimizer of the maximum likelihood objective of an unknown linear\ntime-invariant dynamical system from a sequence of noisy observations generated\nby the system. Even though the objective function is non-convex, we provide\npolynomial running time and sample complexity bounds under strong but natural\nassumptions. Linear systems identification has been studied for many decades,\nyet, to the best of our knowledge, these are the first polynomial guarantees\nfor the problem we consider.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 19:42:34 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 16:55:24 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Hardt", "Moritz", ""], ["Ma", "Tengyu", ""], ["Recht", "Benjamin", ""]]}, {"id": "1609.05204", "submitter": "Jonathan Dong", "authors": "Jonathan Dong, Sylvain Gigan, Florent Krzakala, Gilles Wainrib", "title": "Scaling up Echo-State Networks with multiple light scattering", "comments": null, "journal-ref": "2018 IEEE Statistical Signal Processing Workshop (SSP), Freiburg\n  im Breisgau, Germany, 2018, pp. 448-452", "doi": "10.1109/SSP.2018.8450698", "report-no": null, "categories": "cs.ET cs.LG physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Echo-State Networks and Reservoir Computing have been studied for more than a\ndecade. They provide a simpler yet powerful alternative to Recurrent Neural\nNetworks, every internal weight is fixed and only the last linear layer is\ntrained. They involve many multiplications by dense random matrices. Very large\nnetworks are difficult to obtain, as the complexity scales quadratically both\nin time and memory. Here, we present a novel optical implementation of\nEcho-State Networks using light-scattering media and a Digital Micromirror\nDevice. As a proof of concept, binary networks have been successfully trained\nto predict the chaotic Mackey-Glass time series. This new method is fast, power\nefficient and easily scalable to very large networks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 21:13:55 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 15:57:46 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 13:43:14 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Dong", "Jonathan", ""], ["Gigan", "Sylvain", ""], ["Krzakala", "Florent", ""], ["Wainrib", "Gilles", ""]]}, {"id": "1609.05284", "submitter": "Po-Sen Huang", "authors": "Yelong Shen, Po-Sen Huang, Jianfeng Gao, Weizhu Chen", "title": "ReasoNet: Learning to Stop Reading in Machine Comprehension", "comments": "in KDD 2017", "journal-ref": null, "doi": "10.1145/3097983.3098177", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teaching a computer to read and answer general questions pertaining to a\ndocument is a challenging yet unsolved problem. In this paper, we describe a\nnovel neural network architecture called the Reasoning Network (ReasoNet) for\nmachine comprehension tasks. ReasoNets make use of multiple turns to\neffectively exploit and then reason over the relation among queries, documents,\nand answers. Different from previous approaches using a fixed number of turns\nduring inference, ReasoNets introduce a termination state to relax this\nconstraint on the reasoning depth. With the use of reinforcement learning,\nReasoNets can dynamically determine whether to continue the comprehension\nprocess after digesting intermediate results, or to terminate reading when it\nconcludes that existing information is adequate to produce an answer. ReasoNets\nhave achieved exceptional performance in machine comprehension datasets,\nincluding unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset,\nand a structured Graph Reachability dataset.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 05:12:50 GMT"}, {"version": "v2", "created": "Sat, 10 Jun 2017 06:29:36 GMT"}, {"version": "v3", "created": "Tue, 20 Jun 2017 01:12:07 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Shen", "Yelong", ""], ["Huang", "Po-Sen", ""], ["Gao", "Jianfeng", ""], ["Chen", "Weizhu", ""]]}, {"id": "1609.05294", "submitter": "Zhourong Chen", "authors": "Zhourong Chen, Nevin L. Zhang, Dit-Yan Yeung, Peixian Chen", "title": "Sparse Boltzmann Machines with Structure Learning as Applied to Text\n  Analysis", "comments": "AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in exploring the possibility and benefits of structure\nlearning for deep models. As the first step, this paper investigates the matter\nfor Restricted Boltzmann Machines (RBMs). We conduct the study with Replicated\nSoftmax, a variant of RBMs for unsupervised text analysis. We present a method\nfor learning what we call Sparse Boltzmann Machines, where each hidden unit is\nconnected to a subset of the visible units instead of all of them. Empirical\nresults show that the method yields models with significantly improved model\nfit and interpretability as compared with RBMs where each hidden unit is\nconnected to all visible units.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 08:17:36 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 11:51:20 GMT"}, {"version": "v3", "created": "Sun, 5 Aug 2018 06:48:43 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Chen", "Zhourong", ""], ["Zhang", "Nevin L.", ""], ["Yeung", "Dit-Yan", ""], ["Chen", "Peixian", ""]]}, {"id": "1609.05342", "submitter": "Reza Borhani", "authors": "Reza Borhani, Jeremy Watt, Aggelos Katsaggelos", "title": "Fast and Effective Algorithms for Symmetric Nonnegative Matrix\n  Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric Nonnegative Matrix Factorization (SNMF) models arise naturally as\nsimple reformulations of many standard clustering algorithms including the\npopular spectral clustering method. Recent work has demonstrated that an\nelementary instance of SNMF provides superior clustering quality compared to\nmany classic clustering algorithms on a variety of synthetic and real world\ndata sets. In this work, we present novel reformulations of this instance of\nSNMF based on the notion of variable splitting and produce two fast and\neffective algorithms for its optimization using i) the provably convergent\nAccelerated Proximal Gradient (APG) procedure and ii) a heuristic version of\nthe Alternating Direction Method of Multipliers (ADMM) framework. Our two\nalgorithms present an interesting tradeoff between computational speed and\nmathematical convergence guarantee: while the former method is provably\nconvergent it is considerably slower than the latter approach, for which we\nalso provide significant but less stringent mathematical proof regarding its\nconvergence. Through extensive experiments we show not only that the efficacy\nof these approaches is equal to that of the state of the art SNMF algorithm,\nbut also that the latter of our algorithms is extremely fast being one to two\norders of magnitude faster in terms of total computation time than the state of\nthe art approach, outperforming even spectral clustering in terms of\ncomputation time on large data sets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 14:41:32 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Borhani", "Reza", ""], ["Watt", "Jeremy", ""], ["Katsaggelos", "Aggelos", ""]]}, {"id": "1609.05353", "submitter": "James P. Crutchfield", "authors": "Alexander B. Boyd, Dibyendu Mandal, and James P. Crutchfield", "title": "Leveraging Environmental Correlations: The Thermodynamics of Requisite\n  Variety", "comments": "19 pages, 9 figures,\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/tom.htm", "journal-ref": null, "doi": "10.1007/s10955-017-1776-0", "report-no": null, "categories": "cond-mat.stat-mech cs.IT cs.LG math.DS math.IT nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key to biological success, the requisite variety that confronts an adaptive\norganism is the set of detectable, accessible, and controllable states in its\nenvironment. We analyze its role in the thermodynamic functioning of\ninformation ratchets---a form of autonomous Maxwellian Demon capable of\nexploiting fluctuations in an external information reservoir to harvest useful\nwork from a thermal bath. This establishes a quantitative paradigm for\nunderstanding how adaptive agents leverage structured thermal environments for\ntheir own thermodynamic benefit. General ratchets behave as memoryful\ncommunication channels, interacting with their environment sequentially and\nstoring results to an output. The bulk of thermal ratchets analyzed to date,\nhowever, assume memoryless environments that generate input signals without\ntemporal correlations. Employing computational mechanics and a new\ninformation-processing Second Law of Thermodynamics (IPSL) we remove these\nrestrictions, analyzing general finite-state ratchets interacting with\nstructured environments that generate correlated input signals. On the one\nhand, we demonstrate that a ratchet need not have memory to exploit an\nuncorrelated environment. On the other, and more appropriate to biological\nadaptation, we show that a ratchet must have memory to most effectively\nleverage structure and correlation in its environment. The lesson is that to\noptimally harvest work a ratchet's memory must reflect the input generator's\nmemory. Finally, we investigate achieving the IPSL bounds on the amount of work\na ratchet can extract from its environment, discovering that finite-state,\noptimal ratchets are unable to reach these bounds. In contrast, we show that\ninfinite-state ratchets can go well beyond these bounds by utilizing their own\ninfinite \"negentropy\". We conclude with an outline of the collective\nthermodynamics of information-ratchet swarms.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 15:33:26 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Boyd", "Alexander B.", ""], ["Mandal", "Dibyendu", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1609.05374", "submitter": "Holakou Rahmanian", "authors": "Holakou Rahmanian, David P. Helmbold, S.V.N. Vishwanathan", "title": "Online Learning of Combinatorial Objects via Extended Formulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard techniques for online learning of combinatorial objects perform\nmultiplicative updates followed by projections into the convex hull of all the\nobjects. However, this methodology can be expensive if the convex hull contains\nmany facets. For example, the convex hull of $n$-symbol Huffman trees is known\nto have exponentially many facets (Maurras et al., 2010). We get around this\ndifficulty by exploiting extended formulations (Kaibel, 2011), which encode the\npolytope of combinatorial objects in a higher dimensional \"extended\" space with\nonly polynomially many facets. We develop a general framework for converting\nextended formulations into efficient online algorithms with good relative loss\nbounds. We present applications of our framework to online learning of Huffman\ntrees and permutations. The regret bounds of the resulting algorithms are\nwithin a factor of $O(\\sqrt{\\log(n)})$ of the state-of-the-art specialized\nalgorithms for permutations, and depending on the loss regimes, improve on or\nmatch the state-of-the-art for Huffman trees. Our method is general and can be\napplied to other combinatorial objects.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 18:38:46 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 19:51:28 GMT"}, {"version": "v3", "created": "Thu, 6 Apr 2017 16:28:45 GMT"}, {"version": "v4", "created": "Thu, 8 Jun 2017 01:49:06 GMT"}, {"version": "v5", "created": "Mon, 30 Oct 2017 20:33:11 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Rahmanian", "Holakou", ""], ["Helmbold", "David P.", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1609.05388", "submitter": "Charalampos Tsourakakis", "authors": "Jaros{\\l}aw B{\\l}asiok, Charalampos E. Tsourakakis", "title": "ADAGIO: Fast Data-aware Near-Isometric Linear Embeddings", "comments": "ICDM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important applications, including signal reconstruction, parameter\nestimation, and signal processing in a compressed domain, rely on a\nlow-dimensional representation of the dataset that preserves {\\em all} pairwise\ndistances between the data points and leverages the inherent geometric\nstructure that is typically present. Recently Hedge, Sankaranarayanan, Yin and\nBaraniuk \\cite{hedge2015} proposed the first data-aware near-isometric linear\nembedding which achieves the best of both worlds. However, their method NuMax\ndoes not scale to large-scale datasets.\n  Our main contribution is a simple, data-aware, near-isometric linear\ndimensionality reduction method which significantly outperforms a\nstate-of-the-art method \\cite{hedge2015} with respect to scalability while\nachieving high quality near-isometries. Furthermore, our method comes with\nstrong worst-case theoretical guarantees that allow us to guarantee the quality\nof the obtained near-isometry. We verify experimentally the efficiency of our\nmethod on numerous real-world datasets, where we find that our method ($<$10\nsecs) is more than 3\\,000$\\times$ faster than the state-of-the-art method\n\\cite{hedge2015} ($>$9 hours) on medium scale datasets with 60\\,000 data points\nin 784 dimensions. Finally, we use our method as a preprocessing step to\nincrease the computational efficiency of a classification application and for\nspeeding up approximate nearest neighbor queries.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 21:01:19 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["B\u0142asiok", "Jaros\u0142aw", ""], ["Tsourakakis", "Charalampos E.", ""]]}, {"id": "1609.05394", "submitter": "Barack Wanjawa Mr.", "authors": "Barack Wamkaya Wanjawa", "title": "Predicting Future Shanghai Stock Market Price using ANN in the Period\n  21-Sep-2016 to 11-Oct-2016", "comments": "10 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the prices of stocks at any stock market remains a quest for many\ninvestors and researchers. Those who trade at the stock market tend to use\ntechnical, fundamental or time series analysis in their predictions. These\nmethods usually guide on trends and not the exact likely prices. It is for this\nreason that Artificial Intelligence systems, such as Artificial Neural Network,\nthat is feedforward multi-layer perceptron with error backpropagation, can be\nused for such predictions. A difficulty in neural network application is the\ndetermination of suitable network parameters. A previous research by the author\nalready determined the network parameters as 5:21:21:1 with 80% training data\nor 4-year of training data as a good enough model for stock prediction. This\nmodel has been put to the test in predicting selected Shanghai Stock Exchange\nstocks in the future period of 21-Sep-2016 to 11-Oct-2016, about one week after\nthe publication of these predictions. The research aims at confirming that\nsimple neural network systems can be quite powerful in typical stock market\npredictions.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 21:37:10 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Wanjawa", "Barack Wamkaya", ""]]}, {"id": "1609.05396", "submitter": "Martin Simonovsky", "authors": "Martin Simonovsky, Benjam\\'in Guti\\'errez-Becker, Diana Mateus, Nassir\n  Navab, Nikos Komodakis", "title": "A Deep Metric for Multimodal Registration", "comments": "Accepted to MICCAI 2016; extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal registration is a challenging problem in medical imaging due the\nhigh variability of tissue appearance under different imaging modalities. The\ncrucial component here is the choice of the right similarity measure. We make a\nstep towards a general learning-based solution that can be adapted to specific\nsituations and present a metric based on a convolutional neural network. Our\nnetwork can be trained from scratch even from a few aligned image pairs. The\nmetric is validated on intersubject deformable registration on a dataset\ndifferent from the one used for training, demonstrating good generalization. In\nthis task, we outperform mutual information by a significant margin.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 21:46:21 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Simonovsky", "Martin", ""], ["Guti\u00e9rrez-Becker", "Benjam\u00edn", ""], ["Mateus", "Diana", ""], ["Navab", "Nassir", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1609.05473", "submitter": "Lantao Yu", "authors": "Lantao Yu, Weinan Zhang, Jun Wang, Yong Yu", "title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient", "comments": "The Thirty-First AAAI Conference on Artificial Intelligence (AAAI\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  As a new way of training generative models, Generative Adversarial Nets (GAN)\nthat uses a discriminative model to guide the training of the generative model\nhas enjoyed considerable success in generating real-valued data. However, it\nhas limitations when the goal is for generating sequences of discrete tokens. A\nmajor reason lies in that the discrete outputs from the generative model make\nit difficult to pass the gradient update from the discriminative model to the\ngenerative model. Also, the discriminative model can only assess a complete\nsequence, while for a partially generated sequence, it is non-trivial to\nbalance its current score and the future one once the entire sequence has been\ngenerated. In this paper, we propose a sequence generation framework, called\nSeqGAN, to solve the problems. Modeling the data generator as a stochastic\npolicy in reinforcement learning (RL), SeqGAN bypasses the generator\ndifferentiation problem by directly performing gradient policy update. The RL\nreward signal comes from the GAN discriminator judged on a complete sequence,\nand is passed back to the intermediate state-action steps using Monte Carlo\nsearch. Extensive experiments on synthetic data and real-world tasks\ndemonstrate significant improvements over strong baselines.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 11:42:23 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 09:44:18 GMT"}, {"version": "v3", "created": "Sun, 25 Sep 2016 13:06:24 GMT"}, {"version": "v4", "created": "Mon, 24 Oct 2016 13:19:26 GMT"}, {"version": "v5", "created": "Fri, 9 Dec 2016 14:37:13 GMT"}, {"version": "v6", "created": "Fri, 25 Aug 2017 16:22:57 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Yu", "Lantao", ""], ["Zhang", "Weinan", ""], ["Wang", "Jun", ""], ["Yu", "Yong", ""]]}, {"id": "1609.05486", "submitter": "Chang Li Mr.", "authors": "Bingbing Jiang, Chang Li, Maarten de Rijke, Xin Yao and Huanhuan Chen", "title": "Probabilistic Feature Selection and Classification Vector Machine", "comments": "26 pages, 10 figures", "journal-ref": "ACM Transactions on Knowledge Discovery from Data (TKDD) April\n  2019 Article No.: 21", "doi": "10.1145/3309541", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Bayesian learning is a state-of-the-art supervised learning algorithm\nthat can choose a subset of relevant samples from the input data and make\nreliable probabilistic predictions. However, in the presence of\nhigh-dimensional data with irrelevant features, traditional sparse Bayesian\nclassifiers suffer from performance degradation and low efficiency by failing\nto eliminate irrelevant features. To tackle this problem, we propose a novel\nsparse Bayesian embedded feature selection method that adopts truncated\nGaussian distributions as both sample and feature priors. The proposed method,\ncalled probabilistic feature selection and classification vector machine\n(PFCVMLP ), is able to simultaneously select relevant features and samples for\nclassification tasks. In order to derive the analytical solutions, Laplace\napproximation is applied to compute approximate posteriors and marginal\nlikelihoods. Finally, parameters and hyperparameters are optimized by the\ntype-II maximum likelihood method. Experiments on three datasets validate the\nperformance of PFCVMLP along two dimensions: classification performance and\neffectiveness for feature selection. Finally, we analyze the generalization\nperformance and derive a generalization error bound for PFCVMLP . By tightening\nthe bound, the importance of feature selection is demonstrated.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 14:01:04 GMT"}, {"version": "v2", "created": "Mon, 2 Jan 2017 13:36:44 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 10:10:48 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Jiang", "Bingbing", ""], ["Li", "Chang", ""], ["de Rijke", "Maarten", ""], ["Yao", "Xin", ""], ["Chen", "Huanhuan", ""]]}, {"id": "1609.05518", "submitter": "Marta Garnelo", "authors": "Marta Garnelo, Kai Arulkumaran, Murray Shanahan", "title": "Towards Deep Symbolic Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (DRL) brings the power of deep neural networks to\nbear on the generic task of trial-and-error learning, and its effectiveness has\nbeen convincingly demonstrated on tasks such as Atari video games and the game\nof Go. However, contemporary DRL systems inherit a number of shortcomings from\nthe current generation of deep learning techniques. For example, they require\nvery large datasets to work effectively, entailing that they are slow to learn\neven when such datasets are available. Moreover, they lack the ability to\nreason on an abstract level, which makes it difficult to implement high-level\ncognitive functions such as transfer learning, analogical reasoning, and\nhypothesis-based reasoning. Finally, their operation is largely opaque to\nhumans, rendering them unsuitable for domains in which verifiability is\nimportant. In this paper, we propose an end-to-end reinforcement learning\narchitecture comprising a neural back end and a symbolic front end with the\npotential to overcome each of these shortcomings. As proof-of-concept, we\npresent a preliminary implementation of the architecture and apply it to\nseveral variants of a simple video game. We show that the resulting system --\nthough just a prototype -- learns effectively, and, by acquiring a set of\nsymbolic rules that are easily comprehensible to humans, dramatically\noutperforms a conventional, fully neural DRL system on a stochastic variant of\nthe game.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 17:28:22 GMT"}, {"version": "v2", "created": "Sat, 1 Oct 2016 16:19:56 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Garnelo", "Marta", ""], ["Arulkumaran", "Kai", ""], ["Shanahan", "Murray", ""]]}, {"id": "1609.05521", "submitter": "Devendra Singh Chaplot", "authors": "Guillaume Lample, Devendra Singh Chaplot", "title": "Playing FPS Games with Deep Reinforcement Learning", "comments": "The authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in deep reinforcement learning have allowed autonomous agents to\nperform well on Atari games, often outperforming humans, using only raw pixels\nto make their decisions. However, most of these games take place in 2D\nenvironments that are fully observable to the agent. In this paper, we present\nthe first architecture to tackle 3D environments in first-person shooter games,\nthat involve partially observable states. Typically, deep reinforcement\nlearning methods only utilize visual input for training. We present a method to\naugment these models to exploit game feature information such as the presence\nof enemies or items, during the training phase. Our model is trained to\nsimultaneously learn these features along with minimizing a Q-learning\nobjective, which is shown to dramatically improve the training speed and\nperformance of our agent. Our architecture is also modularized to allow\ndifferent models to be independently trained for different phases of the game.\nWe show that the proposed architecture substantially outperforms built-in AI\nagents of the game as well as humans in deathmatch scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 17:52:28 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 15:13:59 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Lample", "Guillaume", ""], ["Chaplot", "Devendra Singh", ""]]}, {"id": "1609.05524", "submitter": "Roy Fox", "authors": "Roy Fox, Michal Moshkovitz and Naftali Tishby", "title": "Principled Option Learning in Markov Decision Processes", "comments": null, "journal-ref": "13th European Workshop on Reinforcement Learning (EWRL 2016)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that options can make planning more efficient, among their\nmany benefits. Thus far, algorithms for autonomously discovering a set of\nuseful options were heuristic. Naturally, a principled way of finding a set of\nuseful options may be more promising and insightful. In this paper we suggest a\nmathematical characterization of good sets of options using tools from\ninformation theory. This characterization enables us to find conditions for a\nset of options to be optimal and an algorithm that outputs a useful set of\noptions and illustrate the proposed algorithm in simulation.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 18:19:02 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 22:51:15 GMT"}, {"version": "v3", "created": "Thu, 30 Mar 2017 05:04:42 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Fox", "Roy", ""], ["Moshkovitz", "Michal", ""], ["Tishby", "Naftali", ""]]}, {"id": "1609.05528", "submitter": "Shebuti Rayana", "authors": "Shebuti Rayana, Wen Zhong and Leman Akoglu", "title": "Sequential Ensemble Learning for Outlier Detection: A Bias-Variance\n  Perspective", "comments": "11 pages, 8 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble methods for classification and clustering have been effectively used\nfor decades, while ensemble learning for outlier detection has only been\nstudied recently. In this work, we design a new ensemble approach for outlier\ndetection in multi-dimensional point data, which provides improved accuracy by\nreducing error through both bias and variance. Although classification and\noutlier detection appear as different problems, their theoretical underpinnings\nare quite similar in terms of the bias-variance trade-off [1], where outlier\ndetection is considered as a binary classification task with unobserved labels\nbut a similar bias-variance decomposition of error.\n  In this paper, we propose a sequential ensemble approach called CARE that\nemploys a two-phase aggregation of the intermediate results in each iteration\nto reach the final outcome. Unlike existing outlier ensembles which solely\nincorporate a parallel framework by aggregating the outcomes of independent\nbase detectors to reduce variance, our ensemble incorporates both the parallel\nand sequential building blocks to reduce bias as well as variance by ($i$)\nsuccessively eliminating outliers from the original dataset to build a better\ndata model on which outlierness is estimated (sequentially), and ($ii$)\ncombining the results from individual base detectors and across iterations\n(parallelly). Through extensive experiments on sixteen real-world datasets\nmainly from the UCI machine learning repository [2], we show that CARE performs\nsignificantly better than or at least similar to the individual baselines. We\nalso compare CARE with the state-of-the-art outlier ensembles where it also\nprovides significant improvement when it is the winner and remains close\notherwise.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 18:59:42 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Rayana", "Shebuti", ""], ["Zhong", "Wen", ""], ["Akoglu", "Leman", ""]]}, {"id": "1609.05536", "submitter": "Theja Tulabandhula", "authors": "Theja Tulabandhula", "title": "Learning Personalized Optimal Control for Repeatedly Operated Systems", "comments": "This work was presented at the NIPS 2015 Workshop: Machine Learning\n  From and For Adaptive User Technologies: From Active Learning &\n  Experimentation to Optimization & Personalization (ref.\n  https://sites.google.com/site/mlaihci)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online learning of optimal control for repeatedly\noperated systems in the presence of parametric uncertainty. During each round\nof operation, environment selects system parameters according to a fixed but\nunknown probability distribution. These parameters govern the dynamics of a\nplant. An agent chooses a control input to the plant and is then revealed the\ncost of the choice. In this setting, we design an agent that personalizes the\ncontrol input to this plant taking into account the stochasticity involved. We\ndemonstrate the effectiveness of our approach on a simulated system.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 19:58:48 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Tulabandhula", "Theja", ""]]}, {"id": "1609.05539", "submitter": "Mostafa El Gamal", "authors": "Mostafa El Gamal and Lifeng Lai", "title": "On Randomized Distributed Coordinate Descent with Quantized Updates", "comments": "Accepted at CISS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the randomized distributed coordinate descent\nalgorithm with quantized updates. In the literature, the iteration complexity\nof the randomized distributed coordinate descent algorithm has been\ncharacterized under the assumption that machines can exchange updates with an\ninfinite precision. We consider a practical scenario in which the messages\nexchange occurs over channels with finite capacity, and hence the updates have\nto be quantized. We derive sufficient conditions on the quantization error such\nthat the algorithm with quantized update still converge. We further verify our\ntheoretical results by running an experiment, where we apply the algorithm with\nquantized updates to solve a linear regression problem.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 20:17:00 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 17:37:44 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Gamal", "Mostafa El", ""], ["Lai", "Lifeng", ""]]}, {"id": "1609.05559", "submitter": "He He", "authors": "He He, Jordan Boyd-Graber, Kevin Kwok, Hal Daum\\'e III", "title": "Opponent Modeling in Deep Reinforcement Learning", "comments": "In ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opponent modeling is necessary in multi-agent settings where secondary agents\nwith competing goals also adapt their strategies, yet it remains challenging\nbecause strategies interact with each other and change. Most previous work\nfocuses on developing probabilistic models or parameterized strategies for\nspecific applications. Inspired by the recent success of deep reinforcement\nlearning, we present neural-based models that jointly learn a policy and the\nbehavior of opponents. Instead of explicitly predicting the opponent's action,\nwe encode observation of the opponents into a deep Q-Network (DQN); however, we\nretain explicit modeling (if desired) using multitasking. By using a\nMixture-of-Experts architecture, our model automatically discovers different\nstrategy patterns of opponents without extra supervision. We evaluate our\nmodels on a simulated soccer game and a popular trivia game, showing superior\nperformance over DQN and its variants.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 22:06:36 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["He", "He", ""], ["Boyd-Graber", "Jordan", ""], ["Kwok", "Kevin", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "1609.05587", "submitter": "Vaneet Aggarwal", "authors": "Wenqi Wang and Vaneet Aggarwal and Shuchin Aeron", "title": "Tensor Completion by Alternating Minimization under the Tensor Train\n  (TT) Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the matrix product state (MPS) representation of tensor train\ndecompositions, in this paper we propose a tensor completion algorithm which\nalternates over the matrices (tensors) in the MPS representation. This\ndevelopment is motivated in part by the success of matrix completion algorithms\nwhich alternate over the (low-rank) factors. We comment on the computational\ncomplexity of the proposed algorithm and numerically compare it with existing\nmethods employing low rank tensor train approximation for data completion as\nwell as several other recently proposed methods. We show that our method is\nsuperior to existing ones for a variety of real settings.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 03:25:33 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Wang", "Wenqi", ""], ["Aggarwal", "Vaneet", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1609.05610", "submitter": "Michal Ferov", "authors": "Michal Ferov and Marek Modr\\'y", "title": "Enhancing LambdaMART Using Oblivious Trees", "comments": "Accepted for publication in proceedings of RUSSIR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to rank is a machine learning technique broadly used in many areas\nsuch as document retrieval, collaborative filtering or question answering. We\npresent experimental results which suggest that the performance of the current\nstate-of-the-art learning to rank algorithm LambdaMART, when used for document\nretrieval for search engines, can be improved if standard regression trees are\nreplaced by oblivious trees. This paper provides a comparison of both variants\nand our results demonstrate that the use of oblivious trees can improve the\nperformance by more than $2.2\\%$. Additional experimental analysis of the\ninfluence of a number of features and of a size of the training set is also\nprovided and confirms the desirability of properties of oblivious decision\ntrees.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 07:03:29 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Ferov", "Michal", ""], ["Modr\u00fd", "Marek", ""]]}, {"id": "1609.05772", "submitter": "Christopher Adams", "authors": "Christopher Adams", "title": "Stochastic Matrix Factorization", "comments": "24 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a restriction to non-negative matrix factorization in\nwhich at least one matrix factor is stochastic. That is, the elements of the\nmatrix factors are non-negative and the columns of one matrix factor sum to 1.\nThis restriction includes topic models, a popular method for analyzing\nunstructured data. It also includes a method for storing and finding pictures.\nThe paper presents necessary and sufficient conditions on the observed data\nsuch that the factorization is unique. In addition, the paper characterizes\nnatural bounds on the parameters for any observed data and presents a\nconsistent least squares estimator. The results are illustrated using a topic\nmodel analysis of PhD abstracts in economics and the problem of storing and\nretrieving a set of pictures of faces.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 15:19:44 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Adams", "Christopher", ""]]}, {"id": "1609.05807", "submitter": "Jon Kleinberg", "authors": "Jon Kleinberg, Sendhil Mullainathan, Manish Raghavan", "title": "Inherent Trade-Offs in the Fair Determination of Risk Scores", "comments": "To appear in Proceedings of Innovations in Theoretical Computer\n  Science (ITCS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent discussion in the public sphere about algorithmic classification has\ninvolved tension between competing notions of what it means for a probabilistic\nclassification to be fair to different groups. We formalize three fairness\nconditions that lie at the heart of these debates, and we prove that except in\nhighly constrained special cases, there is no method that can satisfy these\nthree conditions simultaneously. Moreover, even satisfying all three conditions\napproximately requires that the data lie in an approximate version of one of\nthe constrained special cases identified by our theorem. These results suggest\nsome of the ways in which key notions of fairness are incompatible with each\nother, and hence provide a framework for thinking about the trade-offs between\nthem.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 16:08:51 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 16:41:21 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Kleinberg", "Jon", ""], ["Mullainathan", "Sendhil", ""], ["Raghavan", "Manish", ""]]}, {"id": "1609.05820", "submitter": "Yuxin Chen", "authors": "Yuxin Chen and Emmanuel Candes", "title": "The Projected Power Method: An Efficient Algorithm for Joint Alignment\n  from Pairwise Differences", "comments": "Accepted to Communications on Pure and Applied Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various applications involve assigning discrete label values to a collection\nof objects based on some pairwise noisy data. Due to the discrete---and hence\nnonconvex---structure of the problem, computing the optimal assignment\n(e.g.~maximum likelihood assignment) becomes intractable at first sight. This\npaper makes progress towards efficient computation by focusing on a concrete\njoint alignment problem---that is, the problem of recovering $n$ discrete\nvariables $x_i \\in \\{1,\\cdots, m\\}$, $1\\leq i\\leq n$ given noisy observations\nof their modulo differences $\\{x_i - x_j~\\mathsf{mod}~m\\}$. We propose a\nlow-complexity and model-free procedure, which operates in a lifted space by\nrepresenting distinct label values in orthogonal directions, and which attempts\nto optimize quadratic functions over hypercubes. Starting with a first guess\ncomputed via a spectral method, the algorithm successively refines the iterates\nvia projected power iterations. We prove that for a broad class of statistical\nmodels, the proposed projected power method makes no error---and hence\nconverges to the maximum likelihood estimate---in a suitable regime. Numerical\nexperiments have been carried out on both synthetic and real data to\ndemonstrate the practicality of our algorithm. We expect this algorithmic\nframework to be effective for a broad range of discrete assignment problems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 16:29:46 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 21:59:14 GMT"}, {"version": "v3", "created": "Thu, 7 Dec 2017 20:30:54 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Chen", "Yuxin", ""], ["Candes", "Emmanuel", ""]]}, {"id": "1609.05866", "submitter": "Alexandre de Br\\'ebisson", "authors": "Alexandre de Br\\'ebisson, Pascal Vincent", "title": "A Cheap Linear Attention Mechanism with Fast Lookups and Fixed-Size\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The softmax content-based attention mechanism has proven to be very\nbeneficial in many applications of recurrent neural networks. Nevertheless it\nsuffers from two major computational limitations. First, its computations for\nan attention lookup scale linearly in the size of the attended sequence.\nSecond, it does not encode the sequence into a fixed-size representation but\ninstead requires to memorize all the hidden states. These two limitations\nrestrict the use of the softmax attention mechanism to relatively small-scale\napplications with short sequences and few lookups per sequence. In this work we\nintroduce a family of linear attention mechanisms designed to overcome the two\nlimitations listed above. We show that removing the softmax non-linearity from\nthe traditional attention formulation yields constant-time attention lookups\nand fixed-size representations of the attended sequences. These properties make\nthese linear attention mechanisms particularly suitable for large-scale\napplications with extreme query loads, real-time requirements and memory\nconstraints. Early experiments on a question answering task show that these\nlinear mechanisms yield significantly better accuracy results than no\nattention, but obviously worse than their softmax alternative.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 18:55:18 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["de Br\u00e9bisson", "Alexandre", ""], ["Vincent", "Pascal", ""]]}, {"id": "1609.05881", "submitter": "Priyank Jaini", "authors": "Priyank Jaini and Pascal Poupart", "title": "Online and Distributed learning of Gaussian mixture models by Bayesian\n  Moment Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian mixture model is a classic technique for clustering and data\nmodeling that is used in numerous applications. With the rise of big data,\nthere is a need for parameter estimation techniques that can handle streaming\ndata and distribute the computation over several processors. While online\nvariants of the Expectation Maximization (EM) algorithm exist, their data\nefficiency is reduced by a stochastic approximation of the E-step and it is not\nclear how to distribute the computation over multiple processors. We propose a\nBayesian learning technique that lends itself naturally to online and\ndistributed computation. Since the Bayesian posterior is not tractable, we\nproject it onto a family of tractable distributions after each observation by\nmatching a set of sufficient moments. This Bayesian moment matching technique\ncompares favorably to online EM in terms of time and accuracy on a set of data\nmodeling benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 19:44:06 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Jaini", "Priyank", ""], ["Poupart", "Pascal", ""]]}, {"id": "1609.05884", "submitter": "Ammar Daskin", "authors": "Ammar Daskin", "title": "A Quantum Implementation Model for Artificial Neural Networks", "comments": null, "journal-ref": "Quanta, 7, pg: 7-18, 2018", "doi": "10.12743/quanta.v7i1.65", "report-no": null, "categories": "quant-ph cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learning process for multi layered neural networks with many nodes makes\nheavy demands on computational resources. In some neural network models, the\nlearning formulas, such as the Widrow-Hoff formula, do not change the\neigenvectors of the weight matrix while flatting the eigenvalues. In infinity,\nthis iterative formulas result in terms formed by the principal components of\nthe weight matrix: i.e., the eigenvectors corresponding to the non-zero\neigenvalues. In quantum computing, the phase estimation algorithm is known to\nprovide speed-ups over the conventional algorithms for the eigenvalue-related\nproblems. Combining the quantum amplitude amplification with the phase\nestimation algorithm, a quantum implementation model for artificial neural\nnetworks using the Widrow-Hoff learning rule is presented. The complexity of\nthe model is found to be linear in the size of the weight matrix. This provides\na quadratic improvement over the classical algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 19:47:52 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 09:39:52 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Daskin", "Ammar", ""]]}, {"id": "1609.05959", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev and Ivan Nazarov", "title": "Conformalized Kernel Ridge Regression", "comments": "8 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General predictive models do not provide a measure of confidence in\npredictions without Bayesian assumptions. A way to circumvent potential\nrestrictions is to use conformal methods for constructing non-parametric\nconfidence regions, that offer guarantees regarding validity. In this paper we\nprovide a detailed description of a computationally efficient conformal\nprocedure for Kernel Ridge Regression (KRR), and conduct a comparative\nnumerical study to see how well conformal regions perform against the Bayesian\nconfidence sets. The results suggest that conformalized KRR can yield\npredictive confidence regions with specified coverage rate, which is essential\nin constructing anomaly detection systems based on predictive models.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 22:30:36 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Nazarov", "Ivan", ""]]}, {"id": "1609.06026", "submitter": "Ankit Parag Shah", "authors": "Benjamin Elizalde, Ankit Shah, Siddharth Dalmia, Min Hun Lee, Rohan\n  Badlani, Anurag Kumar, Bhiksha Raj and Ian Lane", "title": "An Approach for Self-Training Audio Event Detectors Using Web Data", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio Event Detection (AED) aims to recognize sounds within audio and video\nrecordings. AED employs machine learning algorithms commonly trained and tested\non annotated datasets. However, available datasets are limited in number of\nsamples and hence it is difficult to model acoustic diversity. Therefore, we\npropose combining labeled audio from a dataset and unlabeled audio from the web\nto improve the sound models. The audio event detectors are trained on the\nlabeled audio and ran on the unlabeled audio downloaded from YouTube. Whenever\nthe detectors recognized any of the known sounds with high confidence, the\nunlabeled audio was use to re-train the detectors. The performance of the\nre-trained detectors is compared to the one from the original detectors using\nthe annotated test set. Results showed an improvement of the AED, and uncovered\nchallenges of using web audio from videos.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 05:52:06 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 14:15:13 GMT"}, {"version": "v3", "created": "Tue, 27 Jun 2017 17:09:59 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Elizalde", "Benjamin", ""], ["Shah", "Ankit", ""], ["Dalmia", "Siddharth", ""], ["Lee", "Min Hun", ""], ["Badlani", "Rohan", ""], ["Kumar", "Anurag", ""], ["Raj", "Bhiksha", ""], ["Lane", "Ian", ""]]}, {"id": "1609.06086", "submitter": "Alvin Pastore Mr", "authors": "Alvin Pastore, Umberto Esposito, Eleni Vasilaki", "title": "Modelling Stock-market Investors as Reinforcement Learning Agents\n  [Correction]", "comments": "8 pages (including bibliography and appendix), 5 figures (2 in main\n  body, 3 in appendix). IEEE EAIS 2015 Conference paper erratum", "journal-ref": "Evolving and Adaptive Intelligent Systems (EAIS), 2015 IEEE\n  International Conference on, Douai, 2015, pp. 1-6", "doi": "10.1109/EAIS.2015.7368789", "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision making in uncertain and risky environments is a prominent area of\nresearch. Standard economic theories fail to fully explain human behaviour,\nwhile a potentially promising alternative may lie in the direction of\nReinforcement Learning (RL) theory. We analyse data for 46 players extracted\nfrom a financial market online game and test whether Reinforcement Learning\n(Q-Learning) could capture these players behaviour using a risk measure based\non financial modeling. Moreover we test an earlier hypothesis that players are\n\"na\\\"ive\" (short-sighted). Our results indicate that a simple Reinforcement\nLearning model which considers only the selling component of the task captures\nthe decision-making process for a subset of players but this is not sufficient\nto draw any conclusion on the population. We also find that there is not a\nsignificant improvement of fitting of the players when using a full RL model\nagainst a myopic version, where only immediate reward is valued by the players.\nThis indicates that players, if using a Reinforcement Learning approach, do so\nna\\\"ively\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 10:36:01 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Pastore", "Alvin", ""], ["Esposito", "Umberto", ""], ["Vasilaki", "Eleni", ""]]}, {"id": "1609.06100", "submitter": "Paolo Di Lorenzo", "authors": "P. Di Lorenzo, P. Banelli, S. Barbarossa, S. Sardellitti", "title": "Distributed Adaptive Learning of Graph Signals", "comments": "To appear in IEEE Transactions on Signal Processing, 2017", "journal-ref": null, "doi": "10.1109/TSP.2017.2708035", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to propose distributed strategies for adaptive\nlearning of signals defined over graphs. Assuming the graph signal to be\nbandlimited, the method enables distributed reconstruction, with guaranteed\nperformance in terms of mean-square error, and tracking from a limited number\nof sampled observations taken from a subset of vertices. A detailed mean square\nanalysis is carried out and illustrates the role played by the sampling\nstrategy on the performance of the proposed method. Finally, some useful\nstrategies for distributed selection of the sampling set are provided. Several\nnumerical results validate our theoretical findings, and illustrate the\nperformance of the proposed method for distributed adaptive learning of signals\ndefined over graphs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 11:12:04 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 14:09:39 GMT"}, {"version": "v3", "created": "Sat, 15 Apr 2017 16:20:59 GMT"}, {"version": "v4", "created": "Sat, 13 May 2017 21:06:26 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Di Lorenzo", "P.", ""], ["Banelli", "P.", ""], ["Barbarossa", "S.", ""], ["Sardellitti", "S.", ""]]}, {"id": "1609.06119", "submitter": "Thomas Keck", "authors": "Thomas Keck", "title": "FastBDT: A speed-optimized and cache-friendly implementation of\n  stochastic gradient-boosted decision trees for multivariate classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient-boosted decision trees are widely employed for\nmultivariate classification and regression tasks. This paper presents a\nspeed-optimized and cache-friendly implementation for multivariate\nclassification called FastBDT. FastBDT is one order of magnitude faster during\nthe fitting-phase and application-phase, in comparison with popular\nimplementations in software frameworks like TMVA, scikit-learn and XGBoost. The\nconcepts used to optimize the execution time and performance studies are\ndiscussed in detail in this paper. The key ideas include: An equal-frequency\nbinning on the input data, which allows replacing expensive floating-point with\ninteger operations, while at the same time increasing the quality of the\nclassification; a cache-friendly linear access pattern to the input data, in\ncontrast to usual implementations, which exhibit a random access pattern.\nFastBDT provides interfaces to C/C++, Python and TMVA. It is extensively used\nin the field of high energy physics by the Belle II experiment.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 11:50:52 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Keck", "Thomas", ""]]}, {"id": "1609.06127", "submitter": "Diana Al Jlailaty", "authors": "Diana Jlailaty and Daniela Grigori and Khalid Belhajjame", "title": "A framework for mining process models from emails logs", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its wide use in personal, but most importantly, professional contexts,\nemail represents a valuable source of information that can be harvested for\nunderstanding, reengineering and repurposing undocumented business processes of\ncompanies and institutions. Towards this aim, a few researchers investigated\nthe problem of extracting process oriented information from email logs in order\nto take benefit of the many available process mining techniques and tools. In\nthis paper we go further in this direction, by proposing a new method for\nmining process models from email logs that leverage unsupervised machine\nlearning techniques with little human involvement. Moreover, our method allows\nto semi-automatically label emails with activity names, that can be used for\nactivity recognition in new incoming emails. A use case demonstrates the\nusefulness of the proposed solution using a modest in size, yet real-world,\ndataset containing emails that belong to two different process models.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 12:29:15 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Jlailaty", "Diana", ""], ["Grigori", "Daniela", ""], ["Belhajjame", "Khalid", ""]]}, {"id": "1609.06146", "submitter": "Lars Kotthoff", "authors": "Julia Schiffner, Bernd Bischl, Michel Lang, Jakob Richter, Zachary M.\n  Jones, Philipp Probst, Florian Pfisterer, Mason Gallo, Dominik Kirchhoff,\n  Tobias K\\\"uhn, Janek Thomas, Lars Kotthoff", "title": "mlr Tutorial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document provides and in-depth introduction to the mlr framework for\nmachine learning experiments in R.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 01:08:20 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Schiffner", "Julia", ""], ["Bischl", "Bernd", ""], ["Lang", "Michel", ""], ["Richter", "Jakob", ""], ["Jones", "Zachary M.", ""], ["Probst", "Philipp", ""], ["Pfisterer", "Florian", ""], ["Gallo", "Mason", ""], ["Kirchhoff", "Dominik", ""], ["K\u00fchn", "Tobias", ""], ["Thomas", "Janek", ""], ["Kotthoff", "Lars", ""]]}, {"id": "1609.06335", "submitter": "Anthony Gitter", "authors": "Anthony Gitter, Furong Huang, Ragupathyraj Valluvan, Ernest Fraenkel,\n  Animashree Anandkumar", "title": "Unsupervised learning of transcriptional regulatory networks via latent\n  tree graphical models", "comments": "37 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene expression is a readily-observed quantification of transcriptional\nactivity and cellular state that enables the recovery of the relationships\nbetween regulators and their target genes. Reconstructing transcriptional\nregulatory networks from gene expression data is a problem that has attracted\nmuch attention, but previous work often makes the simplifying (but unrealistic)\nassumption that regulator activity is represented by mRNA levels. We use a\nlatent tree graphical model to analyze gene expression without relying on\ntranscription factor expression as a proxy for regulator activity. The latent\ntree model is a type of Markov random field that includes both observed gene\nvariables and latent (hidden) variables, which factorize on a Markov tree.\nThrough efficient unsupervised learning approaches, we determine which groups\nof genes are co-regulated by hidden regulators and the activity levels of those\nregulators. Post-processing annotates many of these discovered latent variables\nas specific transcription factors or groups of transcription factors. Other\nlatent variables do not necessarily represent physical regulators but instead\nreveal hidden structure in the gene expression such as shared biological\nfunction. We apply the latent tree graphical model to a yeast stress response\ndataset. In addition to novel predictions, such as condition-specific binding\nof the transcription factor Msn4, our model recovers many known aspects of the\nyeast regulatory network. These include groups of co-regulated genes,\ncondition-specific regulator activity, and combinatorial regulation among\ntranscription factors. The latent tree graphical model is a general approach\nfor analyzing gene expression data that requires no prior knowledge of which\npossible regulators exist, regulator activity, or where transcription factors\nphysically bind.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 20:14:15 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Gitter", "Anthony", ""], ["Huang", "Furong", ""], ["Valluvan", "Ragupathyraj", ""], ["Fraenkel", "Ernest", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1609.06354", "submitter": "Yonatan Vaizman", "authors": "Yonatan Vaizman and Katherine Ellis and Gert Lanckriet", "title": "Recognizing Detailed Human Context In-the-Wild from Smartphones and\n  Smartwatches", "comments": "This paper was accepted and is to appear in IEEE Pervasive Computing,\n  vol. 16, no. 4, October-December 2017, pp. 62-74", "journal-ref": "IEEE Pervasive Computing, vol. 16, no. 4, October-December 2017,\n  pp. 62-74", "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to automatically recognize a person's behavioral context can\ncontribute to health monitoring, aging care and many other domains. Validating\ncontext recognition in-the-wild is crucial to promote practical applications\nthat work in real-life settings. We collected over 300k minutes of sensor data\nwith context labels from 60 subjects. Unlike previous studies, our subjects\nused their own personal phone, in any way that was convenient to them, and\nengaged in their routine in their natural environments. Unscripted behavior and\nunconstrained phone usage resulted in situations that are harder to recognize.\nWe demonstrate how fusion of multi-modal sensors is important for resolving\nsuch cases. We present a baseline system, and encourage researchers to use our\npublic dataset to compare methods and improve context recognition in-the-wild.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 20:56:07 GMT"}, {"version": "v2", "created": "Thu, 29 Dec 2016 22:47:22 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 21:44:18 GMT"}, {"version": "v4", "created": "Sat, 30 Sep 2017 15:25:23 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Vaizman", "Yonatan", ""], ["Ellis", "Katherine", ""], ["Lanckriet", "Gert", ""]]}, {"id": "1609.06377", "submitter": "Reza Mahjourian", "authors": "Reza Mahjourian, Martin Wicke, Anelia Angelova", "title": "Geometry-Based Next Frame Prediction from Monocular Video", "comments": "To appear in 2017 IEEE Intelligent Vehicles Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of next frame prediction from video input. A\nrecurrent convolutional neural network is trained to predict depth from\nmonocular video input, which, along with the current video image and the camera\ntrajectory, can then be used to compute the next frame. Unlike prior next-frame\nprediction approaches, we take advantage of the scene geometry and use the\npredicted depth for generating the next frame prediction. Our approach can\nproduce rich next frame predictions which include depth information attached to\neach pixel. Another novel aspect of our approach is that it predicts depth from\na sequence of images (e.g. in a video), rather than from a single still image.\nWe evaluate the proposed approach on the KITTI dataset, a standard dataset for\nbenchmarking tasks relevant to autonomous driving. The proposed method produces\nresults which are visually and numerically superior to existing methods that\ndirectly predict the next frame. We show that the accuracy of depth prediction\nimproves as more prior frames are considered.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 22:49:34 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 21:52:06 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Mahjourian", "Reza", ""], ["Wicke", "Martin", ""], ["Angelova", "Anelia", ""]]}, {"id": "1609.06385", "submitter": "Bernardo \\'Avila Pires", "authors": "Bernardo \\'Avila Pires and Csaba Szepesv\\'ari", "title": "Multiclass Classification Calibration Functions", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we refine the process of computing calibration functions for a\nnumber of multiclass classification surrogate losses. Calibration functions are\na powerful tool for easily converting bounds for the surrogate risk (which can\nbe computed through well-known methods) into bounds for the true risk, the\nprobability of making a mistake. They are particularly suitable in\nnon-parametric settings, where the approximation error can be controlled, and\nprovide tighter bounds than the common technique of upper-bounding the 0-1 loss\nby the surrogate loss.\n  The abstract nature of the more sophisticated existing calibration function\nresults requires calibration functions to be explicitly derived on a\ncase-by-case basis, requiring repeated efforts whenever bounds for a new\nsurrogate loss are required. We devise a streamlined analysis that simplifies\nthe process of deriving calibration functions for a large number of surrogate\nlosses that have been proposed in the literature. The effort of deriving\ncalibration functions is then surmised in verifying, for a chosen surrogate\nloss, a small number of conditions that we introduce.\n  As case studies, we recover existing calibration functions for the well-known\nloss of Lee et al. (2004), and also provide novel calibration functions for\nwell-known losses, including the one-versus-all loss and the logistic\nregression loss, plus a number of other losses that have been shown to be\nclassification-calibrated in the past, but for which no calibration function\nhad been derived.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 23:41:55 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Pires", "Bernardo \u00c1vila", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1609.06390", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Maruan Al-Shedivat, Eric P. Xing", "title": "Learning HMMs with Nonparametric Emissions via Spectral Decompositions\n  of Continuous Matrices", "comments": "To appear in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a surge of interest in using spectral methods for\nestimating latent variable models. However, it is usually assumed that the\ndistribution of the observations conditioned on the latent variables is either\ndiscrete or belongs to a parametric family. In this paper, we study the\nestimation of an $m$-state hidden Markov model (HMM) with only smoothness\nassumptions, such as H\\\"olderian conditions, on the emission densities. By\nleveraging some recent advances in continuous linear algebra and numerical\nanalysis, we develop a computationally efficient spectral algorithm for\nlearning nonparametric HMMs. Our technique is based on computing an SVD on\nnonparametric estimates of density functions by viewing them as\n\\emph{continuous matrices}. We derive sample complexity bounds via\nconcentration results for nonparametric density estimation and novel\nperturbation theory results for continuous matrices. We implement our method\nusing Chebyshev polynomial approximations. Our method is competitive with other\nbaselines on synthetic and real problems and is also very computationally\nefficient.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 00:15:44 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Al-Shedivat", "Maruan", ""], ["Xing", "Eric P.", ""]]}, {"id": "1609.06438", "submitter": "Benjamin Rubinstein", "authors": "Tansu Alpcan, Benjamin I. P. Rubinstein, Christopher Leckie", "title": "Large-Scale Strategic Games and Adversarial Machine Learning", "comments": "7 pages, 1 figure; CDC'16 to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision making in modern large-scale and complex systems such as\ncommunication networks, smart electricity grids, and cyber-physical systems\nmotivate novel game-theoretic approaches. This paper investigates big strategic\n(non-cooperative) games where a finite number of individual players each have a\nlarge number of continuous decision variables and input data points. Such\nhigh-dimensional decision spaces and big data sets lead to computational\nchallenges, relating to efforts in non-linear optimization scaling up to large\nsystems of variables. In addition to these computational challenges, real-world\nplayers often have limited information about their preference parameters due to\nthe prohibitive cost of identifying them or due to operating in dynamic online\nsettings. The challenge of limited information is exacerbated in high\ndimensions and big data sets. Motivated by both computational and information\nlimitations that constrain the direct solution of big strategic games, our\ninvestigation centers around reductions using linear transformations such as\nrandom projection methods and their effect on Nash equilibrium solutions.\nSpecific analytical results are presented for quadratic games and\napproximations. In addition, an adversarial learning game is presented where\nrandom projection and sampling schemes are investigated.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 07:10:13 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Alpcan", "Tansu", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Leckie", "Christopher", ""]]}, {"id": "1609.06457", "submitter": "Pin-Yu Chen", "authors": "Pin-Yu Chen and Thibaut Gensollen and Alfred O. Hero III", "title": "AMOS: An Automated Model Order Selection Algorithm for Spectral Graph\n  Clustering", "comments": "arXiv admin note: substantial text overlap with arXiv:1604.03159", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the longstanding problems in spectral graph clustering (SGC) is the\nso-called model order selection problem: automated selection of the correct\nnumber of clusters. This is equivalent to the problem of finding the number of\nconnected components or communities in an undirected graph. In this paper, we\npropose AMOS, an automated model order selection algorithm for SGC. Based on a\nrecent analysis of clustering reliability for SGC under the random\ninterconnection model, AMOS works by incrementally increasing the number of\nclusters, estimating the quality of identified clusters, and providing a series\nof clustering reliability tests. Consequently, AMOS outputs clusters of minimal\nmodel order with statistical clustering reliability guarantees. Comparing to\nthree other automated graph clustering methods on real-world datasets, AMOS\nshows superior performance in terms of multiple external and internal\nclustering metrics.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 08:14:12 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Gensollen", "Thibaut", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1609.06480", "submitter": "Shihua Zhang", "authors": "Wenwen Min, Juan Liu, Shihua Zhang", "title": "Network-regularized Sparse Logistic Regression Models for Clinical Risk\n  Prediction and Biomarker Discovery", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular profiling data (e.g., gene expression) has been used for clinical\nrisk prediction and biomarker discovery. However, it is necessary to integrate\nother prior knowledge like biological pathways or gene interaction networks to\nimprove the predictive ability and biological interpretability of biomarkers.\nHere, we first introduce a general regularized Logistic Regression (LR)\nframework with regularized term $\\lambda \\|\\bm{w}\\|_1 +\n\\eta\\bm{w}^T\\bm{M}\\bm{w}$, which can reduce to different penalties, including\nLasso, elastic net, and network-regularized terms with different $\\bm{M}$. This\nframework can be easily solved in a unified manner by a cyclic coordinate\ndescent algorithm which can avoid inverse matrix operation and accelerate the\ncomputing speed. However, if those estimated $\\bm{w}_i$ and $\\bm{w}_j$ have\nopposite signs, then the traditional network-regularized penalty may not\nperform well. To address it, we introduce a novel network-regularized sparse LR\nmodel with a new penalty $\\lambda \\|\\bm{w}\\|_1 + \\eta|\\bm{w}|^T\\bm{M}|\\bm{w}|$\nto consider the difference between the absolute values of the coefficients. And\nwe develop two efficient algorithms to solve it. Finally, we test our methods\nand compare them with the related ones using simulated and real data to show\ntheir efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 09:47:32 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Min", "Wenwen", ""], ["Liu", "Juan", ""], ["Zhang", "Shihua", ""]]}, {"id": "1609.06492", "submitter": "Alessia Amelio Dr.", "authors": "Darko Brodic, Alessia Amelio, Zoran N. Milivojevic, Milena Jevtic", "title": "Document Image Coding and Clustering for Script Discrimination", "comments": "8 pages, 4 figures, 2 tables", "journal-ref": "ICIC Express Letters Vol. 10 n. 7 July 2016 pp. 1561-1566", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces a new method for discrimination of documents given in\ndifferent scripts. The document is mapped into a uniformly coded text of\nnumerical values. It is derived from the position of the letters in the text\nline, based on their typographical characteristics. Each code is considered as\na gray level. Accordingly, the coded text determines a 1-D image, on which\ntexture analysis by run-length statistics and local binary pattern is\nperformed. It defines feature vectors representing the script content of the\ndocument. A modified clustering approach employed on document feature vector\ngroups documents written in the same script. Experimentation performed on two\ncustom oriented databases of historical documents in old Cyrillic, angular and\nround Glagolitic as well as Antiqua and Fraktur scripts demonstrates the\nsuperiority of the proposed method with respect to well-known methods in the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 10:52:03 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Brodic", "Darko", ""], ["Amelio", "Alessia", ""], ["Milivojevic", "Zoran N.", ""], ["Jevtic", "Milena", ""]]}, {"id": "1609.06532", "submitter": "Kar Wai Lim", "authors": "Kar Wai Lim and Wray Buntine", "title": "Bibliographic Analysis on Research Publications using Authors,\n  Categorical Labels and the Citation Network", "comments": "Preprint for Journal Machine Learning", "journal-ref": "Machine Learning 103(2):185-213, 2016", "doi": "10.1007/s10994-016-5554-z", "report-no": null, "categories": "cs.DL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bibliographic analysis considers the author's research areas, the citation\nnetwork and the paper content among other things. In this paper, we combine\nthese three in a topic model that produces a bibliographic model of authors,\ntopics and documents, using a nonparametric extension of a combination of the\nPoisson mixed-topic link model and the author-topic model. This gives rise to\nthe Citation Network Topic Model (CNTM). We propose a novel and efficient\ninference algorithm for the CNTM to explore subsets of research publications\nfrom CiteSeerX. The publication datasets are organised into three corpora,\ntotalling to about 168k publications with about 62k authors. The queried\ndatasets are made available online. In three publicly available corpora in\naddition to the queried datasets, our proposed model demonstrates an improved\nperformance in both model fitting and document clustering, compared to several\nbaselines. Moreover, our model allows extraction of additional useful knowledge\nfrom the corpora, such as the visualisation of the author-topics network.\nAdditionally, we propose a simple method to incorporate supervision into topic\nmodelling to achieve further improvement on the clustering task.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 12:44:37 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Lim", "Kar Wai", ""], ["Buntine", "Wray", ""]]}, {"id": "1609.06533", "submitter": "Kajsa M{\\o}llersen", "authors": "Kajsa M{\\o}llersen, Subhra S. Dhar, Fred Godtliebsen", "title": "On Data-Independent Properties for Density-Based Dissimilarity Measures\n  in Hybrid Clustering", "comments": null, "journal-ref": "M{\\o}llersen, K., Dhar, S.S. and Godtliebsen, F. (2016) On\n  Data-Independent Properties for Density-Based Dissimilarity Measures in\n  Hybrid Clustering. Applied Mathematics, 7, 1674-1706", "doi": "10.4236/am.2016.715143", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hybrid clustering combines partitional and hierarchical clustering for\ncomputational effectiveness and versatility in cluster shape. In such\nclustering, a dissimilarity measure plays a crucial role in the hierarchical\nmerging. The dissimilarity measure has great impact on the final clustering,\nand data-independent properties are needed to choose the right dissimilarity\nmeasure for the problem at hand. Properties for distance-based dissimilarity\nmeasures have been studied for decades, but properties for density-based\ndissimilarity measures have so far received little attention. Here, we propose\nsix data-independent properties to evaluate density-based dissimilarity\nmeasures associated with hybrid clustering, regarding equality, orthogonality,\nsymmetry, outlier and noise observations, and light-tailed models for\nheavy-tailed clusters. The significance of the properties is investigated, and\nwe study some well-known dissimilarity measures based on Shannon entropy,\nmisclassification rate, Bhattacharyya distance and Kullback-Leibler divergence\nwith respect to the proposed properties. As none of them satisfy all the\nproposed properties, we introduce a new dissimilarity measure based on the\nKullback-Leibler information and show that it satisfies all proposed\nproperties. The effect of the proposed properties is also illustrated on\nseveral real and simulated data sets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 12:46:09 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["M\u00f8llersen", "Kajsa", ""], ["Dhar", "Subhra S.", ""], ["Godtliebsen", "Fred", ""]]}, {"id": "1609.06570", "submitter": "Guillaume Lemaitre", "authors": "Guillaume Lemaitre and Fernando Nogueira and Christos K. Aridas", "title": "Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced\n  Datasets in Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imbalanced-learn is an open-source python toolbox aiming at providing a wide\nrange of methods to cope with the problem of imbalanced dataset frequently\nencountered in machine learning and pattern recognition. The implemented\nstate-of-the-art methods can be categorized into 4 groups: (i) under-sampling,\n(ii) over-sampling, (iii) combination of over- and under-sampling, and (iv)\nensemble learning methods. The proposed toolbox only depends on numpy, scipy,\nand scikit-learn and is distributed under MIT license. Furthermore, it is fully\ncompatible with scikit-learn and is part of the scikit-learn-contrib supported\nproject. Documentation, unit tests as well as integration tests are provided to\nease usage and contribution. The toolbox is publicly available in GitHub:\nhttps://github.com/scikit-learn-contrib/imbalanced-learn.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 14:16:14 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Lemaitre", "Guillaume", ""], ["Nogueira", "Fernando", ""], ["Aridas", "Christos K.", ""]]}, {"id": "1609.06575", "submitter": "M. Ros\\'ario Oliveira", "authors": "Cl\\'audia Pascoal, M. Ros\\'ario Oliveira, Ant\\'onio Pacheco, and Rui\n  Valadas", "title": "Theoretical Evaluation of Feature Selection Methods based on Mutual\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection methods are usually evaluated by wrapping specific\nclassifiers and datasets in the evaluation process, resulting very often in\nunfair comparisons between methods. In this work, we develop a theoretical\nframework that allows obtaining the true feature ordering of two-dimensional\nsequential forward feature selection methods based on mutual information, which\nis independent of entropy or mutual information estimation methods,\nclassifiers, or datasets, and leads to an undoubtful comparison of the methods.\nMoreover, the theoretical framework unveils problems intrinsic to some methods\nthat are otherwise difficult to detect, namely inconsistencies in the\nconstruction of the objective function used to select the candidate features,\ndue to various types of indeterminations and to the possibility of the entropy\nof continuous random variables taking null and negative values.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 14:23:15 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 22:51:50 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Pascoal", "Cl\u00e1udia", ""], ["Oliveira", "M. Ros\u00e1rio", ""], ["Pacheco", "Ant\u00f3nio", ""], ["Valadas", "Rui", ""]]}, {"id": "1609.06578", "submitter": "Kar Wai Lim", "authors": "Kar Wai Lim, Wray Buntine", "title": "Twitter Opinion Topic Model: Extracting Product Opinions from Tweets by\n  Leveraging Hashtags and Sentiment Lexicon", "comments": "CIKM paper", "journal-ref": "Proceedings of the 23rd ACM International Conference on\n  Information and Knowledge Management (CIKM), pp. 1319-1328. ACM. 2014", "doi": "10.1145/2661829.2662005", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-based opinion mining is widely applied to review data to aggregate or\nsummarize opinions of a product, and the current state-of-the-art is achieved\nwith Latent Dirichlet Allocation (LDA)-based model. Although social media data\nlike tweets are laden with opinions, their \"dirty\" nature (as natural language)\nhas discouraged researchers from applying LDA-based opinion model for product\nreview mining. Tweets are often informal, unstructured and lacking labeled data\nsuch as categories and ratings, making it challenging for product opinion\nmining. In this paper, we propose an LDA-based opinion model named Twitter\nOpinion Topic Model (TOTM) for opinion mining and sentiment analysis. TOTM\nleverages hashtags, mentions, emoticons and strong sentiment words that are\npresent in tweets in its discovery process. It improves opinion prediction by\nmodeling the target-opinion interaction directly, thus discovering target\nspecific opinion words, neglected in existing approaches. Moreover, we propose\na new formulation of incorporating sentiment prior information into a topic\nmodel, by utilizing an existing public sentiment lexicon. This is novel in that\nit learns and updates with the data. We conduct experiments on 9 million tweets\non electronic products, and demonstrate the improved performance of TOTM in\nboth quantitative evaluations and qualitative analysis. We show that\naspect-based opinion analysis on massive volume of tweets provides useful\nopinions on products.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 14:25:23 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Lim", "Kar Wai", ""], ["Buntine", "Wray", ""]]}, {"id": "1609.06582", "submitter": "Emiliano De Cristofaro", "authors": "Apostolos Pyrgelis and Emiliano De Cristofaro and Gordon Ross", "title": "Privacy-Friendly Mobility Analytics using Aggregate Location Data", "comments": "Published at ACM SIGSPATIAL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location data can be extremely useful to study commuting patterns and\ndisruptions, as well as to predict real-time traffic volumes. At the same time,\nhowever, the fine-grained collection of user locations raises serious privacy\nconcerns, as this can reveal sensitive information about the users, such as,\nlife style, political and religious inclinations, or even identities. In this\npaper, we study the feasibility of crowd-sourced mobility analytics over\naggregate location information: users periodically report their location, using\na privacy-preserving aggregation protocol, so that the server can only recover\naggregates -- i.e., how many, but not which, users are in a region at a given\ntime. We experiment with real-world mobility datasets obtained from the\nTransport For London authority and the San Francisco Cabs network, and present\na novel methodology based on time series modeling that is geared to forecast\ntraffic volumes in regions of interest and to detect mobility anomalies in\nthem. In the presence of anomalies, we also make enhanced traffic volume\npredictions by feeding our model with additional information from correlated\nregions. Finally, we present and evaluate a mobile app prototype, called\nMobility Data Donors (MDD), in terms of computation, communication, and energy\noverhead, demonstrating the real-world deployability of our techniques.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 14:31:15 GMT"}, {"version": "v2", "created": "Sun, 9 Oct 2016 15:58:06 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Pyrgelis", "Apostolos", ""], ["De Cristofaro", "Emiliano", ""], ["Ross", "Gordon", ""]]}, {"id": "1609.06666", "submitter": "Martin Engelcke", "authors": "Martin Engelcke, Dushyant Rao, Dominic Zeng Wang, Chi Hay Tong, Ingmar\n  Posner", "title": "Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient\n  Convolutional Neural Networks", "comments": "To be published at the IEEE International Conference on Robotics and\n  Automation 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a computationally efficient approach to detecting objects\nnatively in 3D point clouds using convolutional neural networks (CNNs). In\nparticular, this is achieved by leveraging a feature-centric voting scheme to\nimplement novel convolutional layers which explicitly exploit the sparsity\nencountered in the input. To this end, we examine the trade-off between\naccuracy and speed for different architectures and additionally propose to use\nan L1 penalty on the filter activations to further encourage sparsity in the\nintermediate representations. To the best of our knowledge, this is the first\nwork to propose sparse convolutional layers and L1 regularisation for efficient\nlarge-scale processing of 3D data. We demonstrate the efficacy of our approach\non the KITTI object detection benchmark and show that Vote3Deep models with as\nfew as three layers outperform the previous state of the art in both laser and\nlaser-vision based approaches by margins of up to 40% while remaining highly\ncompetitive in terms of processing time.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 18:32:11 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 15:29:45 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Engelcke", "Martin", ""], ["Rao", "Dushyant", ""], ["Wang", "Dominic Zeng", ""], ["Tong", "Chi Hay", ""], ["Posner", "Ingmar", ""]]}, {"id": "1609.06686", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, Parsa Ghaffari, John G. Breslin", "title": "Character-level and Multi-channel Convolutional Neural Networks for\n  Large-scale Authorship Attribution", "comments": "9 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have demonstrated superior capability\nfor extracting information from raw signals in computer vision. Recently,\ncharacter-level and multi-channel CNNs have exhibited excellent performance for\nsentence classification tasks. We apply CNNs to large-scale authorship\nattribution, which aims to determine an unknown text's author among many\ncandidate authors, motivated by their ability to process character-level\nsignals and to differentiate between a large number of classes, while making\nfast predictions in comparison to state-of-the-art approaches. We extensively\nevaluate CNN-based approaches that leverage word and character channels and\ncompare them against state-of-the-art methods for a large range of author\nnumbers, shedding new light on traditional approaches. We show that\ncharacter-level CNNs outperform the state-of-the-art on four out of five\ndatasets in different domains. Additionally, we present the first application\nof authorship attribution to reddit.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 19:08:15 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Ruder", "Sebastian", ""], ["Ghaffari", "Parsa", ""], ["Breslin", "John G.", ""]]}, {"id": "1609.06693", "submitter": "Armen Aghajanyan", "authors": "Armen Aghajanyan", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting\n  in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are learning models with a very high capacity and\ntherefore prone to over-fitting. Many regularization techniques such as\nDropout, DropConnect, and weight decay all attempt to solve the problem of\nover-fitting by reducing the capacity of their respective models (Srivastava et\nal., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we\nintroduce a new form of regularization that guides the learning problem in a\nway that reduces over-fitting without sacrificing the capacity of the model.\nThe mistakes that models make in early stages of training carry information\nabout the learning problem. By adjusting the labels of the current epoch of\ntraining through a weighted average of the real labels, and an exponential\naverage of the past soft-targets we achieved a regularization scheme as\npowerful as Dropout without necessarily reducing the capacity of the model, and\nsimplified the complexity of the learning problem. SoftTarget regularization\nproved to be an effective tool in various neural network architectures.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 19:31:07 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 00:23:30 GMT"}, {"version": "v3", "created": "Sat, 3 Dec 2016 08:08:53 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Aghajanyan", "Armen", ""]]}, {"id": "1609.06694", "submitter": "Aayush Bansal", "authors": "Aayush Bansal, Xinlei Chen, Bryan Russell, Abhinav Gupta, Deva Ramanan", "title": "PixelNet: Towards a General Pixel-level Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore architectures for general pixel-level prediction problems, from\nlow-level edge detection to mid-level surface normal estimation to high-level\nsemantic segmentation. Convolutional predictors, such as the\nfully-convolutional network (FCN), have achieved remarkable success by\nexploiting the spatial redundancy of neighboring pixels through convolutional\nprocessing. Though computationally efficient, we point out that such approaches\nare not statistically efficient during learning precisely because spatial\nredundancy limits the information learned from neighboring pixels. We\ndemonstrate that (1) stratified sampling allows us to add diversity during\nbatch updates and (2) sampled multi-scale features allow us to explore more\nnonlinear predictors (multiple fully-connected layers followed by ReLU) that\nimprove overall accuracy. Finally, our objective is to show how a architecture\ncan get performance better than (or comparable to) the architectures designed\nfor a particular task. Interestingly, our single architecture produces\nstate-of-the-art results for semantic segmentation on PASCAL-Context, surface\nnormal estimation on NYUDv2 dataset, and edge detection on BSDS without\ncontextual post-processing.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 19:32:46 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Bansal", "Aayush", ""], ["Chen", "Xinlei", ""], ["Russell", "Bryan", ""], ["Gupta", "Abhinav", ""], ["Ramanan", "Deva", ""]]}, {"id": "1609.06783", "submitter": "Kar Wai Lim", "authors": "Kar Wai Lim, Wray Buntine, Changyou Chen, Lan Du", "title": "Nonparametric Bayesian Topic Modelling with the Hierarchical Pitman-Yor\n  Processes", "comments": "Preprint for International Journal of Approximate Reasoning", "journal-ref": "International Journal of Approximate Reasoning, Volume 78, pp.\n  172-191. Elsevier. 2016", "doi": "10.1016/j.ijar.2016.07.007", "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dirichlet process and its extension, the Pitman-Yor process, are\nstochastic processes that take probability distributions as a parameter. These\nprocesses can be stacked up to form a hierarchical nonparametric Bayesian\nmodel. In this article, we present efficient methods for the use of these\nprocesses in this hierarchical context, and apply them to latent variable\nmodels for text analytics. In particular, we propose a general framework for\ndesigning these Bayesian models, which are called topic models in the computer\nscience community. We then propose a specific nonparametric Bayesian topic\nmodel for modelling text from social media. We focus on tweets (posts on\nTwitter) in this article due to their ease of access. We find that our\nnonparametric model performs better than existing parametric models in both\ngoodness of fit and real world applications.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 00:10:16 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Lim", "Kar Wai", ""], ["Buntine", "Wray", ""], ["Chen", "Changyou", ""], ["Du", "Lan", ""]]}, {"id": "1609.06804", "submitter": "Zhouyuan Huo", "authors": "Zhouyuan Huo, Bin Gu, Heng Huang", "title": "Decoupled Asynchronous Proximal Stochastic Gradient Descent with\n  Variance Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of big data, optimizing large scale machine learning problems\nbecomes a challenging task and draws significant attention. Asynchronous\noptimization algorithms come out as a promising solution. Recently, decoupled\nasynchronous proximal stochastic gradient descent (DAP-SGD) is proposed to\nminimize a composite function. It is claimed to be able to off-loads the\ncomputation bottleneck from server to workers by allowing workers to evaluate\nthe proximal operators, therefore, server just need to do element-wise\noperations. However, it still suffers from slow convergence rate because of the\nvariance of stochastic gradient is nonzero. In this paper, we propose a faster\nmethod, decoupled asynchronous proximal stochastic variance reduced gradient\ndescent method (DAP-SVRG). We prove that our method has linear convergence for\nstrongly convex problem. Large-scale experiments are also conducted in this\npaper, and results demonstrate our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 02:50:09 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 01:54:25 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Huo", "Zhouyuan", ""], ["Gu", "Bin", ""], ["Huang", "Heng", ""]]}, {"id": "1609.06826", "submitter": "Kar Wai Lim", "authors": "Kar Wai Lim, Wray Buntine", "title": "Bibliographic Analysis with the Citation Network Topic Model", "comments": "A copy of ACML paper. arXiv admin note: substantial text overlap with\n  arXiv:1609.06532", "journal-ref": "Proceedings of the Sixth Asian Conference on Machine Learning\n  (ACML), pp. 142-158. JMLR. 2014", "doi": null, "report-no": null, "categories": "cs.DL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bibliographic analysis considers author's research areas, the citation\nnetwork and paper content among other things. In this paper, we combine these\nthree in a topic model that produces a bibliographic model of authors, topics\nand documents using a non-parametric extension of a combination of the Poisson\nmixed-topic link model and the author-topic model. We propose a novel and\nefficient inference algorithm for the model to explore subsets of research\npublications from CiteSeerX. Our model demonstrates improved performance in\nboth model fitting and a clustering task compared to several baselines.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 05:46:46 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Lim", "Kar Wai", ""], ["Buntine", "Wray", ""]]}, {"id": "1609.06831", "submitter": "Kar Wai Lim", "authors": "Young Lee, Kar Wai Lim, Cheng Soon Ong", "title": "Hawkes Processes with Stochastic Excitations", "comments": "Copy of ICML paper", "journal-ref": "Proceedings of The 33rd International Conference on Machine\n  Learning (ICML), pp. 79-88. JMLR. 2016", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extension to Hawkes processes by treating the levels of\nself-excitation as a stochastic differential equation. Our new point process\nallows better approximation in application domains where events and intensities\naccelerate each other with correlated levels of contagion. We generalize a\nrecent algorithm for simulating draws from Hawkes processes whose levels of\nexcitation are stochastic processes, and propose a hybrid Markov chain Monte\nCarlo approach for model fitting. Our sampling procedure scales linearly with\nthe number of required events and does not require stationarity of the point\nprocess. A modular inference procedure consisting of a combination between\nGibbs and Metropolis Hastings steps is put forward. We recover expectation\nmaximization as a special case. Our general approach is illustrated for\ncontagion following geometric Brownian motion and exponential Langevin\ndynamics.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 06:18:20 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Lee", "Young", ""], ["Lim", "Kar Wai", ""], ["Ong", "Cheng Soon", ""]]}, {"id": "1609.06840", "submitter": "Philipp Hennig PhD", "authors": "Philipp Hennig and Roman Garnett", "title": "Exact Sampling from Determinantal Point Processes", "comments": "Fixed a nontrivial typo in Eq. 12. Many thanks to Lucy Kuncheva and\n  Joseph Courtney for pointing it out to us", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are an important concept in random\nmatrix theory and combinatorics. They have also recently attracted interest in\nthe study of numerical methods for machine learning, as they offer an elegant\n\"missing link\" between independent Monte Carlo sampling and deterministic\nevaluation on regular grids, applicable to a general set of spaces. This is\nhelpful whenever an algorithm explores to reduce uncertainty, such as in active\nlearning, Bayesian optimization, reinforcement learning, and marginalization in\ngraphical models. To draw samples from a DPP in practice, existing literature\nfocuses on approximate schemes of low cost, or comparably inefficient exact\nalgorithms like rejection sampling. We point out that, for many settings of\nrelevance to machine learning, it is also possible to draw exact samples from\nDPPs on continuous domains. We start from an intuitive example on the real\nline, which is then generalized to multivariate real vector spaces. We also\ncompare to previously studied approximations, showing that exact sampling,\ndespite higher cost, can be preferable where precision is needed.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 07:06:28 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 10:21:59 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Hennig", "Philipp", ""], ["Garnett", "Roman", ""]]}, {"id": "1609.06942", "submitter": "Matan Sela", "authors": "Matan Sela and Ron Kimmel", "title": "Randomized Independent Component Analysis", "comments": "Accepted to ICSEE 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) is a method for recovering statistically\nindependent signals from observations of unknown linear combinations of the\nsources. Some of the most accurate ICA decomposition methods require searching\nfor the inverse transformation which minimizes different approximations of the\nMutual Information, a measure of statistical independence of random vectors.\nTwo such approximations are the Kernel Generalized Variance or the Kernel\nCanonical Correlation which has been shown to reach the highest performance of\nICA methods. However, the computational effort necessary just for computing\nthese measures is cubic in the sample size. Hence, optimizing them becomes even\nmore computationally demanding, in terms of both space and time. Here, we\npropose a couple of alternative novel measures based on randomized features of\nthe samples - the Randomized Generalized Variance and the Randomized Canonical\nCorrelation. The computational complexity of calculating the proposed\nalternatives is linear in the sample size and provide a controllable\napproximation of their Kernel-based non-random versions. We also show that\noptimization of the proposed statistical properties yields a comparable\nseparation error at an order of magnitude faster compared to Kernel-based\nmeasures.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 12:40:58 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Sela", "Matan", ""], ["Kimmel", "Ron", ""]]}, {"id": "1609.06954", "submitter": "Vaishak Belle", "authors": "Vaishak Belle, Luc De Raedt", "title": "Semiring Programming: A Declarative Framework for Generalized Sum\n  Product Problems", "comments": "In AAAI Workshop: Statistical Relational Artificial Intelligence,\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve hard problems, AI relies on a variety of disciplines such as logic,\nprobabilistic reasoning, machine learning and mathematical programming.\nAlthough it is widely accepted that solving real-world problems requires an\nintegration amongst these, contemporary representation methodologies offer\nlittle support for this.\n  In an attempt to alleviate this situation, we introduce a new declarative\nprogramming framework that provides abstractions of well-known problems such as\nSAT, Bayesian inference, generative models, and convex optimization. The\nsemantics of programs is defined in terms of first-order structures with\nsemiring labels, which allows us to freely combine and integrate problems from\ndifferent AI disciplines.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 08:17:40 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 13:55:44 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Belle", "Vaishak", ""], ["De Raedt", "Luc", ""]]}, {"id": "1609.06957", "submitter": "Michal Tadeusiak", "authors": "Robert Bogucki, Jan Lasek, Jan Kanty Milczek, Michal Tadeusiak", "title": "Early Warning System for Seismic Events in Coal Mines Using Machine\n  Learning", "comments": "Winner of AAIA'16 Data Mining Challenge: Predicting Dangerous Seismic\n  Events in Active Coal Mines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes an approach to the problem of predicting dangerous\nseismic events in active coal mines up to 8 hours in advance. It was developed\nas a part of the AAIA'16 Data Mining Challenge: Predicting Dangerous Seismic\nEvents in Active Coal Mines. The solutions presented consist of ensembles of\nvarious predictive models trained on different sets of features. The best one\nachieved a winning score of 0.939 AUC.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 09:35:56 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Bogucki", "Robert", ""], ["Lasek", "Jan", ""], ["Milczek", "Jan Kanty", ""], ["Tadeusiak", "Michal", ""]]}, {"id": "1609.07042", "submitter": "Xiang Xiang", "authors": "Xiang Xiang and Trac D. Tran", "title": "Pose-Selective Max Pooling for Measuring Similarity", "comments": "The tutorial and program associated with this paper are available at\n  https://github.com/eglxiang/ytf yet for non-commercial use", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we deal with two challenges for measuring the similarity of\nthe subject identities in practical video-based face recognition - the\nvariation of the head pose in uncontrolled environments and the computational\nexpense of processing videos. Since the frame-wise feature mean is unable to\ncharacterize the pose diversity among frames, we define and preserve the\noverall pose diversity and closeness in a video. Then, identity will be the\nonly source of variation across videos since the pose varies even within a\nsingle video. Instead of simply using all the frames, we select those faces\nwhose pose point is closest to the centroid of the K-means cluster containing\nthat pose point. Then, we represent a video as a bag of frame-wise deep face\nfeatures while the number of features has been reduced from hundreds to K.\nSince the video representation can well represent the identity, now we measure\nthe subject similarity between two videos as the max correlation among all\npossible pairs in the two bags of features. On the official 5,000 video-pairs\nof the YouTube Face dataset for face verification, our algorithm achieves a\ncomparable performance with VGG-face that averages over deep features of all\nframes. Other vision tasks can also benefit from the generic idea of employing\ngeometric cues to improve the descriptiveness of deep features.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 15:59:38 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 18:21:05 GMT"}, {"version": "v3", "created": "Thu, 10 Nov 2016 04:05:29 GMT"}, {"version": "v4", "created": "Mon, 14 Nov 2016 04:10:09 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Xiang", "Xiang", ""], ["Tran", "Trac D.", ""]]}, {"id": "1609.07061", "submitter": "Itay Hubara", "authors": "Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv and\n  Yoshua Bengio", "title": "Quantized Neural Networks: Training Neural Networks with Low Precision\n  Weights and Activations", "comments": "arXiv admin note: text overlap with arXiv:1602.02830", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to train Quantized Neural Networks (QNNs) --- neural\nnetworks with extremely low precision (e.g., 1-bit) weights and activations, at\nrun-time. At train-time the quantized weights and activations are used for\ncomputing the parameter gradients. During the forward pass, QNNs drastically\nreduce memory size and accesses, and replace most arithmetic operations with\nbit-wise operations. As a result, power consumption is expected to be\ndrastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and\nImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to\ntheir 32-bit counterparts. For example, our quantized version of AlexNet with\n1-bit weights and 2-bit activations achieves $51\\%$ top-1 accuracy. Moreover,\nwe quantize the parameter gradients to 6-bits as well which enables gradients\ncomputation using only bit-wise operation. Quantized recurrent neural networks\nwere tested over the Penn Treebank dataset, and achieved comparable accuracy as\ntheir 32-bit counterparts using only 4-bits. Last but not least, we programmed\na binary matrix multiplication GPU kernel with which it is possible to run our\nMNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering\nany loss in classification accuracy. The QNN code is available online.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 16:48:03 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Hubara", "Itay", ""], ["Courbariaux", "Matthieu", ""], ["Soudry", "Daniel", ""], ["El-Yaniv", "Ran", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1609.07082", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Boris Muzellec and Richard Nock", "title": "Large Margin Nearest Neighbor Classification using Curved Mahalanobis\n  Distances", "comments": "21 pages, 8 figures, 5 tables, extend ICIP 2016 paper entitled\n  \"classification With Mixtures of Curved Mahalanobis Metrics\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the supervised classification problem of machine learning in\nCayley-Klein projective geometries: We show how to learn a curved Mahalanobis\nmetric distance corresponding to either the hyperbolic geometry or the elliptic\ngeometry using the Large Margin Nearest Neighbor (LMNN) framework. We report on\nour experimental results, and further consider the case of learning a mixed\ncurved Mahalanobis distance. Besides, we show that the Cayley-Klein Voronoi\ndiagrams are affine, and can be built from an equivalent (clipped) power\ndiagrams, and that Cayley-Klein balls have Mahalanobis shapes with displaced\ncenters.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 17:41:03 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 18:34:42 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Nielsen", "Frank", ""], ["Muzellec", "Boris", ""], ["Nock", "Richard", ""]]}, {"id": "1609.07087", "submitter": "L.A. Prashanth", "authors": "Xiaowei Hu, Prashanth L.A., Andr\\'as Gy\\\"orgy and Csaba Szepesv\\'ari", "title": "(Bandit) Convex Optimization with Biased Noisy Gradient Oracles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for bandit convex optimization and online learning often rely on\nconstructing noisy gradient estimates, which are then used in appropriately\nadjusted first-order algorithms, replacing actual gradients. Depending on the\nproperties of the function to be optimized and the nature of ``noise'' in the\nbandit feedback, the bias and variance of gradient estimates exhibit various\ntradeoffs. In this paper we propose a novel framework that replaces the\nspecific gradient estimation methods with an abstract oracle. With the help of\nthe new framework we unify previous works, reproducing their results in a clean\nand concise fashion, while, perhaps more importantly, the framework also allows\nus to formally show that to achieve the optimal root-$n$ rate either the\nalgorithms that use existing gradient estimators, or the proof techniques used\nto analyze them have to go beyond what exists today.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 17:56:38 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 22:16:51 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Hu", "Xiaowei", ""], ["A.", "Prashanth L.", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1609.07088", "submitter": "Abhishek Gupta", "authors": "Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, Sergey\n  Levine", "title": "Learning Modular Neural Network Policies for Multi-Task and Multi-Robot\n  Transfer", "comments": "Under review at the International Conference on Robotics and\n  Automation (ICRA) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) can automate a wide variety of robotic skills,\nbut learning each new skill requires considerable real-world data collection\nand manual representation engineering to design policy classes or features.\nUsing deep reinforcement learning to train general purpose neural network\npolicies alleviates some of the burden of manual representation engineering by\nusing expressive policy classes, but exacerbates the challenge of data\ncollection, since such methods tend to be less efficient than RL with\nlow-dimensional, hand-designed representations. Transfer learning can mitigate\nthis problem by enabling us to transfer information from one skill to another\nand even from one robot to another. We show that neural network policies can be\ndecomposed into \"task-specific\" and \"robot-specific\" modules, where the\ntask-specific modules are shared across robots, and the robot-specific modules\nare shared across all tasks on that robot. This allows for sharing task\ninformation, such as perception, between robots and sharing robot information,\nsuch as dynamics and kinematics, between tasks. We exploit this decomposition\nto train mix-and-match modules that can solve new robot-task combinations that\nwere not seen during training. Using a novel neural network architecture, we\ndemonstrate the effectiveness of our transfer method for enabling zero-shot\ngeneralization with a variety of robots and tasks in simulation for both visual\nand non-visual tasks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 17:59:25 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Devin", "Coline", ""], ["Gupta", "Abhishek", ""], ["Darrell", "Trevor", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1609.07093", "submitter": "Andrew Brock", "authors": "Andrew Brock, Theodore Lim, J.M. Ritchie, Nick Weston", "title": "Neural Photo Editing with Introspective Adversarial Networks", "comments": "10 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasingly photorealistic sample quality of generative image models\nsuggests their feasibility in applications beyond image generation. We present\nthe Neural Photo Editor, an interface that leverages the power of generative\nneural networks to make large, semantically coherent changes to existing\nimages. To tackle the challenge of achieving accurate reconstructions without\nloss of feature quality, we introduce the Introspective Adversarial Network, a\nnovel hybridization of the VAE and GAN. Our model efficiently captures\nlong-range dependencies through use of a computational block based on\nweight-shared dilated convolutions, and improves generalization performance\nwith Orthogonal Regularization, a novel weight regularization method. We\nvalidate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples\nand reconstructions with high visual fidelity.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 18:07:56 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 13:16:21 GMT"}, {"version": "v3", "created": "Mon, 6 Feb 2017 18:46:50 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Brock", "Andrew", ""], ["Lim", "Theodore", ""], ["Ritchie", "J. M.", ""], ["Weston", "Nick", ""]]}, {"id": "1609.07132", "submitter": "Se Rim Park", "authors": "Se Rim Park, Jinwon Lee", "title": "A Fully Convolutional Neural Network for Speech Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In hearing aids, the presence of babble noise degrades hearing\nintelligibility of human speech greatly. However, removing the babble without\ncreating artifacts in human speech is a challenging task in a low SNR\nenvironment. Here, we sought to solve the problem by finding a `mapping'\nbetween noisy speech spectra and clean speech spectra via supervised learning.\nSpecifically, we propose using fully Convolutional Neural Networks, which\nconsist of lesser number of parameters than fully connected networks. The\nproposed network, Redundant Convolutional Encoder Decoder (R-CED), demonstrates\nthat a convolutional network can be 12 times smaller than a recurrent network\nand yet achieves better performance, which shows its applicability for an\nembedded system: the hearing aids.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 19:57:08 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Park", "Se Rim", ""], ["Lee", "Jinwon", ""]]}, {"id": "1609.07152", "submitter": "Brandon Amos", "authors": "Brandon Amos, Lei Xu, J. Zico Kolter", "title": "Input Convex Neural Networks", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the input convex neural network architecture. These are\nscalar-valued (potentially deep) neural networks with constraints on the\nnetwork parameters such that the output of the network is a convex function of\n(some of) the inputs. The networks allow for efficient inference via\noptimization over some inputs to the network given others, and can be applied\nto settings including structured prediction, data imputation, reinforcement\nlearning, and others. In this paper we lay the basic groundwork for these\nmodels, proposing methods for inference, optimization and learning, and analyze\ntheir representational power. We show that many existing neural network\narchitectures can be made input-convex with a minor modification, and develop\nspecialized optimization algorithms tailored to this setting. Finally, we\nhighlight the performance of the methods on multi-label prediction, image\ncompletion, and reinforcement learning problems, where we show improvement over\nthe existing state of the art in many cases.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 20:10:57 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 19:46:58 GMT"}, {"version": "v3", "created": "Wed, 14 Jun 2017 17:59:12 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Amos", "Brandon", ""], ["Xu", "Lei", ""], ["Kolter", "J. Zico", ""]]}, {"id": "1609.07200", "submitter": "Pin-Yu Chen", "authors": "Pin-Yu Chen and Alfred O. Hero III", "title": "Multilayer Spectral Graph Clustering via Convex Layer Aggregation", "comments": "To appear at IEEE GlobalSIP 2016", "journal-ref": "IEEE Tran. Signal and Information Processing over Networks, 2017\n  https://arxiv.org/abs/1708.02620", "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayer graphs are commonly used for representing different relations\nbetween entities and handling heterogeneous data processing tasks. New\nchallenges arise in multilayer graph clustering for assigning clusters to a\ncommon multilayer node set and for combining information from each layer. This\npaper presents a theoretical framework for multilayer spectral graph clustering\nof the nodes via convex layer aggregation. Under a novel multilayer signal plus\nnoise model, we provide a phase transition analysis that establishes the\nexistence of a critical value on the noise level that permits reliable cluster\nseparation. The analysis also specifies analytical upper and lower bounds on\nthe critical value, where the bounds become exact when the clusters have\nidentical sizes. Numerical experiments on synthetic multilayer graphs are\nconducted to validate the phase transition analysis and study the effect of\nlayer weights and noise levels on clustering reliability.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 01:16:46 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1609.07215", "submitter": "Rajasekar Venkatesan", "authors": "Mihika Dave, Sahil Tapiawala, Meng Joo Er, Rajasekar Venkatesan", "title": "A Novel Progressive Multi-label Classifier for Classincremental Data", "comments": "5 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a progressive learning algorithm for multi-label\nclassification to learn new labels while retaining the knowledge of previous\nlabels is designed. New output neurons corresponding to new labels are added\nand the neural network connections and parameters are automatically\nrestructured as if the label has been introduced from the beginning. This work\nis the first of the kind in multi-label classifier for class-incremental\nlearning. It is useful for real-world applications such as robotics where\nstreaming data are available and the number of labels is often unknown. Based\non the Extreme Learning Machine framework, a novel universal classifier with\nplug and play capabilities for progressive multi-label classification is\ndeveloped. Experimental results on various benchmark synthetic and real\ndatasets validate the efficiency and effectiveness of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 03:09:24 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Dave", "Mihika", ""], ["Tapiawala", "Sahil", ""], ["Er", "Meng Joo", ""], ["Venkatesan", "Rajasekar", ""]]}, {"id": "1609.07257", "submitter": "Tomas Pevny", "authors": "Tomas Pevny and Petr Somol", "title": "Using Neural Network Formalism to Solve Multiple-Instance Problems", "comments": "Accepted to International Symposium on Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many objects in the real world are difficult to describe by a single\nnumerical vector of a fixed length, whereas describing them by a set of vectors\nis more natural. Therefore, Multiple instance learning (MIL) techniques have\nbeen constantly gaining on importance throughout last years. MIL formalism\nrepresents each object (sample) by a set (bag) of feature vectors (instances)\nof fixed length where knowledge about objects (e.g., class label) is available\non bag level but not necessarily on instance level. Many standard tools\nincluding supervised classifiers have been already adapted to MIL setting since\nthe problem got formalized in late nineties. In this work we propose a neural\nnetwork (NN) based formalism that intuitively bridges the gap between MIL\nproblem definition and the vast existing knowledge-base of standard models and\nclassifiers. We show that the proposed NN formalism is effectively optimizable\nby a modified back-propagation algorithm and can reveal unknown patterns inside\nbags. Comparison to eight types of classifiers from the prior art on a set of\n14 publicly available benchmark datasets confirms the advantages and accuracy\nof the proposed solution.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 07:40:12 GMT"}, {"version": "v2", "created": "Sun, 16 Oct 2016 11:15:43 GMT"}, {"version": "v3", "created": "Tue, 7 Mar 2017 06:38:36 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Pevny", "Tomas", ""], ["Somol", "Petr", ""]]}, {"id": "1609.07272", "submitter": "Toon Van Craenendonck", "authors": "Toon Van Craenendonck, Hendrik Blockeel", "title": "Constraint-Based Clustering Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised clustering methods incorporate a limited amount of\nsupervision into the clustering process. Typically, this supervision is\nprovided by the user in the form of pairwise constraints. Existing methods use\nsuch constraints in one of the following ways: they adapt their clustering\nprocedure, their similarity metric, or both. All of these approaches operate\nwithin the scope of individual clustering algorithms. In contrast, we propose\nto use constraints to choose between clusterings generated by very different\nunsupervised clustering algorithms, run with different parameter settings. We\nempirically show that this simple approach often outperforms existing\nsemi-supervised clustering methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 08:51:14 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Van Craenendonck", "Toon", ""], ["Blockeel", "Hendrik", ""]]}, {"id": "1609.07384", "submitter": "Anurag Kumar", "authors": "Anurag Kumar, Bhiksha Raj, Ndapandula Nakashole", "title": "Discovering Sound Concepts and Acoustic Relations In Text", "comments": "ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe approaches for discovering acoustic concepts and\nrelations in text. The first major goal is to be able to identify text phrases\nwhich contain a notion of audibility and can be termed as a sound or an\nacoustic concept. We also propose a method to define an acoustic scene through\na set of sound concepts. We use pattern matching and parts of speech tags to\ngenerate sound concepts from large scale text corpora. We use dependency\nparsing and LSTM recurrent neural network to predict a set of sound concepts\nfor a given acoustic scene. These methods are not only helpful in creating an\nacoustic knowledge base but in the future can also directly help acoustic event\nand scene detection research.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 14:35:17 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2017 01:09:27 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Kumar", "Anurag", ""], ["Raj", "Bhiksha", ""], ["Nakashole", "Ndapandula", ""]]}, {"id": "1609.07472", "submitter": "Yongxin Yang", "authors": "Yongxin Yang, Yu Zheng, Timothy M. Hospedales", "title": "Gated Neural Networks for Option Pricing: Rationality by Design", "comments": "Accepted to AAAI 2017. 7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.LG q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural network approach to price EU call options that\nsignificantly outperforms some existing pricing models and comes with\nguarantees that its predictions are economically reasonable. To achieve this,\nwe introduce a class of gated neural networks that automatically learn to\ndivide-and-conquer the problem space for robust and accurate pricing. We then\nderive instantiations of these networks that are 'rational by design' in terms\nof naturally encoding a valid call option surface that enforces no arbitrage\nprinciples. This integration of human insight within data-driven learning\nprovides significantly better generalisation in pricing performance due to the\nencoded inductive bias in the learning, guarantees sanity in the model's\npredictions, and provides econometrically useful byproduct such as risk neutral\ndensity.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 20:56:06 GMT"}, {"version": "v2", "created": "Sat, 19 Nov 2016 22:45:38 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 09:25:28 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Yang", "Yongxin", ""], ["Zheng", "Yu", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1609.07478", "submitter": "Martin Jaggi", "authors": "Anant Raj, Jakob Olbrich, Bernd G\\\"artner, Bernhard Sch\\\"olkopf,\n  Martin Jaggi", "title": "Screening Rules for Convex Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for deriving screening rules for convex\noptimization problems. Our approach covers a large class of constrained and\npenalized optimization formulations, and works in two steps. First, given any\napproximate point, the structure of the objective function and the duality gap\nis used to gather information on the optimal solution. In the second step, this\ninformation is used to produce screening rules, i.e. safely identifying\nunimportant weight variables of the optimal solution. Our general framework\nleads to a large variety of useful existing as well as new screening rules for\nmany applications. For example, we provide new screening rules for general\nsimplex and $L_1$-constrained problems, Elastic Net, squared-loss Support\nVector Machines, minimum enclosing ball, as well as structured norm regularized\nproblems, such as group lasso.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 19:59:50 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Raj", "Anant", ""], ["Olbrich", "Jakob", ""], ["G\u00e4rtner", "Bernd", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Jaggi", "Martin", ""]]}, {"id": "1609.07495", "submitter": "Matteo Ruggero Ronchi", "authors": "Matteo Ruggero Ronchi, Joon Sik Kim and Yisong Yue", "title": "A Rotation Invariant Latent Factor Model for Moveme Discovery from\n  Static Poses", "comments": "Long version of the paper accepted at the IEEE ICDM 2016 conference.\n  10 pages, 9 figures, 1 table. Project page:\n  http://www.vision.caltech.edu/~mronchi/projects/RotationInvariantMovemes/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We tackle the problem of learning a rotation invariant latent factor model\nwhen the training data is comprised of lower-dimensional projections of the\noriginal feature space. The main goal is the discovery of a set of 3-D bases\nposes that can characterize the manifold of primitive human motions, or\nmovemes, from a training set of 2-D projected poses obtained from still images\ntaken at various camera angles. The proposed technique for basis discovery is\ndata-driven rather than hand-designed. The learned representation is rotation\ninvariant, and can reconstruct any training instance from multiple viewing\nangles. We apply our method to modeling human poses in sports (via the Leeds\nSports Dataset), and demonstrate the effectiveness of the learned bases in a\nrange of applications such as activity classification, inference of dynamics\nfrom a single frame, and synthetic representation of movements.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 20:00:23 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Ronchi", "Matteo Ruggero", ""], ["Kim", "Joon Sik", ""], ["Yue", "Yisong", ""]]}, {"id": "1609.07521", "submitter": "Michael Hughes", "authors": "Michael C. Hughes and Erik B. Sudderth", "title": "Fast Learning of Clusters and Topics via Sparse Posteriors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models and topic models generate each observation from a single\ncluster, but standard variational posteriors for each observation assign\npositive probability to all possible clusters. This requires dense storage and\nruntime costs that scale with the total number of clusters, even though\ntypically only a few clusters have significant posterior mass for any data\npoint. We propose a constrained family of sparse variational distributions that\nallow at most $L$ non-zero entries, where the tunable threshold $L$ trades off\nspeed for accuracy. Previous sparse approximations have used hard assignments\n($L=1$), but we find that moderate values of $L>1$ provide superior\nperformance. Our approach easily integrates with stochastic or incremental\noptimization algorithms to scale to millions of examples. Experiments training\nmixture models of image patches and topic models for news articles show that\nour approach produces better-quality models in far less time than baseline\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 21:18:31 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Hughes", "Michael C.", ""], ["Sudderth", "Erik B.", ""]]}, {"id": "1609.07537", "submitter": "Cesar A. Uribe", "authors": "Angelia Nedi\\'c, Alex Olshevsky and C\\'esar A. Uribe", "title": "A Tutorial on Distributed (Non-Bayesian) Learning: Problem, Algorithms\n  and Results", "comments": "Tutorial Presented in CDC2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.MA cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We overview some results on distributed learning with focus on a family of\nrecently proposed algorithms known as non-Bayesian social learning. We consider\ndifferent approaches to the distributed learning problem and its algorithmic\nsolutions for the case of finitely many hypotheses. The original centralized\nproblem is discussed at first, and then followed by a generalization to the\ndistributed setting. The results on convergence and convergence rate are\npresented for both asymptotic and finite time regimes. Various extensions are\ndiscussed such as those dealing with directed time-varying networks, Nesterov's\nacceleration technique and a continuum sets of hypothesis.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 23:12:06 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Nedi\u0107", "Angelia", ""], ["Olshevsky", "Alex", ""], ["Uribe", "C\u00e9sar A.", ""]]}, {"id": "1609.07540", "submitter": "Zhifei Zhang", "authors": "Zhifei Zhang, Yang Song, Wei Wang, and Hairong Qi", "title": "Derivative Delay Embedding: Online Modeling of Streaming Time Series", "comments": "Accepted by The 25th ACM International Conference on Information and\n  Knowledge Management (CIKM 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The staggering amount of streaming time series coming from the real world\ncalls for more efficient and effective online modeling solution. For time\nseries modeling, most existing works make some unrealistic assumptions such as\nthe input data is of fixed length or well aligned, which requires extra effort\non segmentation or normalization of the raw streaming data. Although some\nliterature claim their approaches to be invariant to data length and\nmisalignment, they are too time-consuming to model a streaming time series in\nan online manner. We propose a novel and more practical online modeling and\nclassification scheme, DDE-MGM, which does not make any assumptions on the time\nseries while maintaining high efficiency and state-of-the-art performance. The\nderivative delay embedding (DDE) is developed to incrementally transform time\nseries to the embedding space, where the intrinsic characteristics of data is\npreserved as recursive patterns regardless of the stream length and\nmisalignment. Then, a non-parametric Markov geographic model (MGM) is proposed\nto both model and classify the pattern in an online manner. Experimental\nresults demonstrate the effectiveness and superior classification accuracy of\nthe proposed DDE-MGM in an online setting as compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 00:03:49 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Zhang", "Zhifei", ""], ["Song", "Yang", ""], ["Wang", "Wei", ""], ["Qi", "Hairong", ""]]}, {"id": "1609.07560", "submitter": "Lantao Liu", "authors": "Kai-Chieh Ma, Lantao Liu, Gaurav S. Sukhatme", "title": "Informative Planning and Online Learning with Sparse Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A big challenge in environmental monitoring is the spatiotemporal variation\nof the phenomena to be observed. To enable persistent sensing and estimation in\nsuch a setting, it is beneficial to have a time-varying underlying\nenvironmental model. Here we present a planning and learning method that\nenables an autonomous marine vehicle to perform persistent ocean monitoring\ntasks by learning and refining an environmental model. To alleviate the\ncomputational bottleneck caused by large-scale data accumulated, we propose a\nframework that iterates between a planning component aimed at collecting the\nmost information-rich data, and a sparse Gaussian Process learning component\nwhere the environmental model and hyperparameters are learned online by taking\nadvantage of only a subset of data that provides the greatest contribution. Our\nsimulations with ground-truth ocean data shows that the proposed method is both\naccurate and efficient.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 02:56:25 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Ma", "Kai-Chieh", ""], ["Liu", "Lantao", ""], ["Sukhatme", "Gaurav S.", ""]]}, {"id": "1609.07574", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Hamid Nazerzadeh", "title": "Dynamic Pricing in High-dimensions", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the pricing problem faced by a firm that sells a large number of\nproducts, described via a wide range of features, to customers that arrive over\ntime. Customers independently make purchasing decisions according to a general\nchoice model that includes products features and customers' characteristics,\nencoded as $d$-dimensional numerical vectors, as well as the price offered. The\nparameters of the choice model are a priori unknown to the firm, but can be\nlearned as the (binary-valued) sales data accrues over time. The firm's\nobjective is to minimize the regret, i.e., the expected revenue loss against a\nclairvoyant policy that knows the parameters of the choice model in advance,\nand always offers the revenue-maximizing price. This setting is motivated in\npart by the prevalence of online marketplaces that allow for real-time pricing.\nWe assume a structured choice model, parameters of which depend on $s_0$ out of\nthe $d$ product features. We propose a dynamic policy, called Regularized\nMaximum Likelihood Pricing (RMLP) that leverages the (sparsity) structure of\nthe high-dimensional model and obtains a logarithmic regret in $T$. More\nspecifically, the regret of our algorithm is of $O(s_0 \\log d \\cdot \\log T)$.\nFurthermore, we show that no policy can obtain regret better than $O(s_0 (\\log\nd + \\log T))$.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 06:02:24 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 21:11:00 GMT"}, {"version": "v3", "created": "Mon, 21 Nov 2016 02:48:54 GMT"}, {"version": "v4", "created": "Mon, 1 Jan 2018 04:00:13 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Javanmard", "Adel", ""], ["Nazerzadeh", "Hamid", ""]]}, {"id": "1609.07672", "submitter": "Roy Fox", "authors": "Roy Fox", "title": "Information-Theoretic Methods for Planning and Learning in Partially\n  Observable Markov Decision Processes", "comments": "PhD thesis, Hebrew University of Jerusalem, 9/2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bounded agents are limited by intrinsic constraints on their ability to\nprocess information that is available in their sensors and memory and choose\nactions and memory updates. In this dissertation, we model these constraints as\ninformation-rate constraints on communication channels connecting these various\ninternal components of the agent. We make four major contributions detailed\nbelow and many smaller contributions detailed in each section. First, we\nformulate the problem of optimizing the agent under both extrinsic and\nintrinsic constraints and develop the main tools for solving it. Second, we\nidentify another reason for the challenging convergence properties of the\noptimization algorithm, which is the bifurcation structure of the update\noperator near phase transitions. Third, we study the special case of\nlinear-Gaussian dynamics and quadratic cost (LQG), where the optimal solution\nhas a particularly simple and solvable form. Fourth, we explore the learning\ntask, where the model of the world dynamics is unknown and sample-based updates\nare used instead.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 20:45:37 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 04:57:49 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Fox", "Roy", ""]]}, {"id": "1609.07706", "submitter": "Lana Sinapayen", "authors": "Lana Sinapayen, Atsushi Masumori, Takashi Ikegami", "title": "Learning by Stimulation Avoidance: A Principle to Control Spiking Neural\n  Networks Dynamics", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0170388", "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning based on networks of real neurons, and by extension biologically\ninspired models of neural networks, has yet to find general learning rules\nleading to widespread applications. In this paper, we argue for the existence\nof a principle allowing to steer the dynamics of a biologically inspired neural\nnetwork. Using carefully timed external stimulation, the network can be driven\ntowards a desired dynamical state. We term this principle \"Learning by\nStimulation Avoidance\" (LSA). We demonstrate through simulation that the\nminimal sufficient conditions leading to LSA in artificial networks are also\nsufficient to reproduce learning results similar to those obtained in\nbiological neurons by Shahaf and Marom [1]. We examine the mechanism's basic\ndynamics in a reduced network, and demonstrate how it scales up to a network of\n100 neurons. We show that LSA has a higher explanatory power than existing\nhypotheses about the response of biological neural networks to external\nsimulation, and can be used as a learning rule for an embodied application:\nlearning of wall avoidance by a simulated robot. The surge in popularity of\nartificial neural networks is mostly directed to disembodied models of neurons\nwith biologically irrelevant dynamics: to the authors' knowledge, this is the\nfirst work demonstrating sensory-motor learning with random spiking networks\nthrough pure Hebbian learning.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 06:44:42 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 05:38:53 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Sinapayen", "Lana", ""], ["Masumori", "Atsushi", ""], ["Ikegami", "Takashi", ""]]}, {"id": "1609.07724", "submitter": "Athanasios Vlontzos", "authors": "Athanasios Vlontzos", "title": "The RNN-ELM Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine learning methods combining the Random Neural\nNetwork, a biologically inspired neural network and the Extreme Learning\nMachine that achieve state of the art classification performance while\nrequiring much shorter training time. The Random Neural Network is a integrate\nand fire computational model of a neural network whose mathematical structure\npermits the efficient analysis of large ensembles of neurons. An activation\nfunction is derived from the RNN and used in an Extreme Learning Machine. We\ncompare the performance of this combination against the ELM with various\nactivation functions, we reduce the input dimensionality via PCA and compare\nits performance vs. autoencoder based versions of the RNN-ELM.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 10:18:19 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Vlontzos", "Athanasios", ""]]}, {"id": "1609.07750", "submitter": "Ahmed Abdelsalam Mr", "authors": "Ahmed M. Abdelsalam, J.M. Pierre Langlois and F. Cheriet", "title": "Accurate and Efficient Hyperbolic Tangent Activation Function on FPGA\n  using the DCT Interpolation Filter", "comments": "8 pages, 6 figures, 5 tables, submitted for the 25th ACM/SIGDA\n  International Symposium on Field-Programmable Gate Arrays (ISFPGA), 22-24\n  February 2017, California, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Implementing an accurate and fast activation function with low cost is a\ncrucial aspect to the implementation of Deep Neural Networks (DNNs) on FPGAs.\nWe propose a high-accuracy approximation approach for the hyperbolic tangent\nactivation function of artificial neurons in DNNs. It is based on the Discrete\nCosine Transform Interpolation Filter (DCTIF). The proposed architecture\ncombines simple arithmetic operations on stored samples of the hyperbolic\ntangent function and on input data. The proposed DCTIF implementation achieves\ntwo orders of magnitude greater precision than previous work while using the\nsame or fewer computational resources. Various combinations of DCTIF parameters\ncan be chosen to tradeoff the accuracy and complexity of the hyperbolic tangent\nfunction. In one case, the proposed architecture approximates the hyperbolic\ntangent activation function with 10E-5 maximum error while requiring only 1.52\nKbits memory and 57 LUTs of a Virtex-7 FPGA. We also discuss how the activation\nfunction accuracy affects the performance of DNNs in terms of their training\nand testing accuracies. We show that a high accuracy approximation can be\nnecessary in order to maintain the same DNN training and testing performances\nrealized by the exact function.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 14:30:33 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Abdelsalam", "Ahmed M.", ""], ["Langlois", "J. M. Pierre", ""], ["Cheriet", "F.", ""]]}, {"id": "1609.07770", "submitter": "Felan Carlo Garcia", "authors": "Felan Carlo C. Garcia, Felix P. Muga II", "title": "Random Forest for Malware Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The challenge in engaging malware activities involves the correct\nidentification and classification of different malware variants. Various\nmalwares incorporate code obfuscation methods that alters their code signatures\neffectively countering antimalware detection techniques utilizing static\nmethods and signature database. In this study, we utilized an approach of\nconverting a malware binary into an image and use Random Forest to classify\nvarious malware families. The resulting accuracy of 0.9562 exhibits the\neffectivess of the method in detecting malware\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 16:43:44 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Garcia", "Felan Carlo C.", ""], ["Muga", "Felix P.", "II"]]}, {"id": "1609.07916", "submitter": "Michael Tschannen", "authors": "Michael Tschannen, Lukas Cavigelli, Fabian Mentzer, Thomas Wiatowski,\n  Luca Benini", "title": "Deep Structured Features for Semantic Segmentation", "comments": "EUSIPCO 2017, 5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a highly structured neural network architecture for semantic\nsegmentation with an extremely small model size, suitable for low-power\nembedded and mobile platforms. Specifically, our architecture combines i) a\nHaar wavelet-based tree-like convolutional neural network (CNN), ii) a random\nlayer realizing a radial basis function kernel approximation, and iii) a linear\nclassifier. While stages i) and ii) are completely pre-specified, only the\nlinear classifier is learned from data. We apply the proposed architecture to\noutdoor scene and aerial image semantic segmentation and show that the accuracy\nof our architecture is competitive with conventional pixel classification CNNs.\nFurthermore, we demonstrate that the proposed architecture is data efficient in\nthe sense of matching the accuracy of pixel classification CNNs when trained on\na much smaller data set.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 10:33:13 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 16:23:14 GMT"}, {"version": "v3", "created": "Fri, 16 Jun 2017 15:12:49 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Tschannen", "Michael", ""], ["Cavigelli", "Lukas", ""], ["Mentzer", "Fabian", ""], ["Wiatowski", "Thomas", ""], ["Benini", "Luca", ""]]}, {"id": "1609.08009", "submitter": "Alban Laflaqui\\`ere Dr", "authors": "Alban Laflaqui\\`ere and Nikolas Hemion", "title": "Grounding object perception in a naive agent's sensorimotor experience", "comments": "7 pages, 4 figures, ICDL-Epirob 2015 conference", "journal-ref": null, "doi": "10.1109/DEVLRN.2015.7346156", "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial object perception usually relies on a priori defined models and\nfeature extraction algorithms. We study how the concept of object can be\ngrounded in the sensorimotor experience of a naive agent. Without any knowledge\nabout itself or the world it is immersed in, the agent explores its\nsensorimotor space and identifies objects as consistent networks of\nsensorimotor transitions, independent from their context. A fundamental drive\nfor prediction is assumed to explain the emergence of such networks from a\ndevelopmental standpoint. An algorithm is proposed and tested to illustrate the\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 15:05:08 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Laflaqui\u00e8re", "Alban", ""], ["Hemion", "Nikolas", ""]]}, {"id": "1609.08017", "submitter": "Xuezhe Ma", "authors": "Xuezhe Ma, Yingkai Gao, Zhiting Hu, Yaoliang Yu, Yuntian Deng, Eduard\n  Hovy", "title": "Dropout with Expectation-linear Regularization", "comments": "Published as a conference paper at ICLR 2017. Camera-ready Version.\n  23 pages (paper + appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout, a simple and effective way to train deep neural networks, has led to\na number of impressive empirical successes and spawned many recent theoretical\ninvestigations. However, the gap between dropout's training and inference\nphases, introduced due to tractability considerations, has largely remained\nunder-appreciated. In this work, we first formulate dropout as a tractable\napproximation of some latent variable model, leading to a clean view of\nparameter sharing and enabling further theoretical analysis. Then, we introduce\n(approximate) expectation-linear dropout neural networks, whose inference gap\nwe are able to formally characterize. Algorithmically, we show that our\nproposed measure of the inference gap can be used to regularize the standard\ndropout training objective, resulting in an \\emph{explicit} control of the gap.\nOur method is as simple and efficient as standard dropout. We further prove the\nupper bounds on the loss in accuracy due to expectation-linearization, describe\nclasses of input distributions that expectation-linearize easily. Experiments\non three image classification benchmark datasets demonstrate that reducing the\ninference gap can indeed improve the performance consistently.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 15:14:05 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 18:04:11 GMT"}, {"version": "v3", "created": "Wed, 15 Feb 2017 19:40:29 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Ma", "Xuezhe", ""], ["Gao", "Yingkai", ""], ["Hu", "Zhiting", ""], ["Yu", "Yaoliang", ""], ["Deng", "Yuntian", ""], ["Hovy", "Eduard", ""]]}, {"id": "1609.08144", "submitter": "Mike Schuster", "authors": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,\n  Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff\n  Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, {\\L}ukasz Kaiser,\n  Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens,\n  George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason\n  Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey\n  Dean", "title": "Google's Neural Machine Translation System: Bridging the Gap between\n  Human and Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (NMT) is an end-to-end learning approach for\nautomated translation, with the potential to overcome many of the weaknesses of\nconventional phrase-based translation systems. Unfortunately, NMT systems are\nknown to be computationally expensive both in training and in translation\ninference. Also, most NMT systems have difficulty with rare words. These issues\nhave hindered NMT's use in practical deployments and services, where both\naccuracy and speed are essential. In this work, we present GNMT, Google's\nNeural Machine Translation system, which attempts to address many of these\nissues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder\nlayers using attention and residual connections. To improve parallelism and\ntherefore decrease training time, our attention mechanism connects the bottom\nlayer of the decoder to the top layer of the encoder. To accelerate the final\ntranslation speed, we employ low-precision arithmetic during inference\ncomputations. To improve handling of rare words, we divide words into a limited\nset of common sub-word units (\"wordpieces\") for both input and output. This\nmethod provides a good balance between the flexibility of \"character\"-delimited\nmodels and the efficiency of \"word\"-delimited models, naturally handles\ntranslation of rare words, and ultimately improves the overall accuracy of the\nsystem. Our beam search technique employs a length-normalization procedure and\nuses a coverage penalty, which encourages generation of an output sentence that\nis most likely to cover all the words in the source sentence. On the WMT'14\nEnglish-to-French and English-to-German benchmarks, GNMT achieves competitive\nresults to state-of-the-art. Using a human side-by-side evaluation on a set of\nisolated simple sentences, it reduces translation errors by an average of 60%\ncompared to Google's phrase-based production system.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 19:59:55 GMT"}, {"version": "v2", "created": "Sat, 8 Oct 2016 19:10:41 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Wu", "Yonghui", ""], ["Schuster", "Mike", ""], ["Chen", "Zhifeng", ""], ["Le", "Quoc V.", ""], ["Norouzi", "Mohammad", ""], ["Macherey", "Wolfgang", ""], ["Krikun", "Maxim", ""], ["Cao", "Yuan", ""], ["Gao", "Qin", ""], ["Macherey", "Klaus", ""], ["Klingner", "Jeff", ""], ["Shah", "Apurva", ""], ["Johnson", "Melvin", ""], ["Liu", "Xiaobing", ""], ["Kaiser", "\u0141ukasz", ""], ["Gouws", "Stephan", ""], ["Kato", "Yoshikiyo", ""], ["Kudo", "Taku", ""], ["Kazawa", "Hideto", ""], ["Stevens", "Keith", ""], ["Kurian", "George", ""], ["Patil", "Nishant", ""], ["Wang", "Wei", ""], ["Young", "Cliff", ""], ["Smith", "Jason", ""], ["Riesa", "Jason", ""], ["Rudnick", "Alex", ""], ["Vinyals", "Oriol", ""], ["Corrado", "Greg", ""], ["Hughes", "Macduff", ""], ["Dean", "Jeffrey", ""]]}, {"id": "1609.08151", "submitter": "Yonghua Yin", "authors": "Yonghua Yin, Erol Gelenbe", "title": "Nonnegative autoencoder with simplified random neural network", "comments": "10 pages (a small edit to the abstract)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes new nonnegative (shallow and multi-layer) autoencoders by\ncombining the spiking Random Neural Network (RNN) model, the network\narchitecture typical used in deep-learning area and the training technique\ninspired from nonnegative matrix factorization (NMF). The shallow autoencoder\nis a simplified RNN model, which is then stacked into a multi-layer\narchitecture. The learning algorithm is based on the weight update rules in\nNMF, subject to the nonnegative probability constraints of the RNN. The\nautoencoders equipped with this learning algorithm are tested on typical image\ndatasets including the MNIST, Yale face and CIFAR-10 datasets, and also using\n16 real-world datasets from different areas. The results obtained through these\ntests yield the desired high learning and recognition accuracy. Also, numerical\nsimulations of the stochastic spiking behavior of this RNN auto encoder, show\nthat it can be implemented in a highly-distributed manner.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 13:47:08 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 11:02:29 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Yin", "Yonghua", ""], ["Gelenbe", "Erol", ""]]}, {"id": "1609.08209", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev and Ivan Koptelov and German Novikov and Timur Khanipov", "title": "Automatic Construction of a Recurrent Neural Network based Classifier\n  for Vehicle Passage Detection", "comments": "6 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are extensively used for time-series\nmodeling and prediction. We propose an approach for automatic construction of a\nbinary classifier based on Long Short-Term Memory RNNs (LSTM-RNNs) for\ndetection of a vehicle passage through a checkpoint. As an input to the\nclassifier we use multidimensional signals of various sensors that are\ninstalled on the checkpoint. Obtained results demonstrate that the previous\napproach to handcrafting a classifier, consisting of a set of deterministic\nrules, can be successfully replaced by an automatic RNN training on an\nappropriately labelled data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 22:11:05 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Koptelov", "Ivan", ""], ["Novikov", "German", ""], ["Khanipov", "Timur", ""]]}, {"id": "1609.08221", "submitter": "Rui Liu", "authors": "Rui Liu, Hossein Nejati, Seyed Hamid Safavi, Ngai-Man Cheung", "title": "Simultaneous Low-rank Component and Graph Estimation for\n  High-dimensional Graph Signals: Application to Brain Imaging", "comments": "Accepted by ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm to uncover the intrinsic low-rank component of a\nhigh-dimensional, graph-smooth and grossly-corrupted dataset, under the\nsituations that the underlying graph is unknown. Based on a model with a\nlow-rank component plus a sparse perturbation, and an initial graph estimation,\nour proposed algorithm simultaneously learns the low-rank component and refines\nthe graph. Our evaluations using synthetic and real brain imaging data in\nunsupervised and supervised classification tasks demonstrate encouraging\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 23:24:27 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 03:23:43 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Liu", "Rui", ""], ["Nejati", "Hossein", ""], ["Safavi", "Seyed Hamid", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1609.08281", "submitter": "Tao Hong", "authors": "Tao Hong and Zhihui Zhu", "title": "An Efficient Method for Robust Projection Matrix Design", "comments": "8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our objective is to efficiently design a robust projection matrix $\\Phi$ for\nthe Compressive Sensing (CS) systems when applied to the signals that are not\nexactly sparse. The optimal projection matrix is obtained by mainly minimizing\nthe average coherence of the equivalent dictionary. In order to drop the\nrequirement of the sparse representation error (SRE) for a set of training data\nas in [15] [16], we introduce a novel penalty function independent of a\nparticular SRE matrix. Without requiring of training data, we can efficiently\ndesign the robust projection matrix and apply it for most of CS systems, like a\nCS system for image processing with a conventional wavelet dictionary in which\nthe SRE matrix is generally not available. Simulation results demonstrate the\nefficiency and effectiveness of the proposed approach compared with the\nstate-of-the-art methods. In addition, we experimentally demonstrate with\nnatural images that under similar compression rate, a CS system with a learned\ndictionary in high dimensions outperforms the one in low dimensions in terms of\nreconstruction accuracy. This together with the fact that our proposed method\ncan efficiently work in high dimension suggests that a CS system can be\npotentially implemented beyond the small patches in sparsity-based image\nprocessing.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 06:59:11 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 17:01:06 GMT"}, {"version": "v3", "created": "Wed, 6 Sep 2017 17:53:44 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Hong", "Tao", ""], ["Zhu", "Zhihui", ""]]}, {"id": "1609.08286", "submitter": "Weixiang Shao", "authors": "Weixiang Shao, Lifang He, Chun-Ta Lu, Xiaokai Wei, Philip S. Yu", "title": "Online Unsupervised Multi-view Feature Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of big data, it is becoming common to have data with multiple\nmodalities or coming from multiple sources, known as \"multi-view data\".\nMulti-view data are usually unlabeled and come from high-dimensional spaces\n(such as language vocabularies), unsupervised multi-view feature selection is\ncrucial to many applications. However, it is nontrivial due to the following\nchallenges. First, there are too many instances or the feature dimensionality\nis too large. Thus, the data may not fit in memory. How to select useful\nfeatures with limited memory space? Second, how to select features from\nstreaming data and handles the concept drift? Third, how to leverage the\nconsistent and complementary information from different views to improve the\nfeature selection in the situation when the data are too big or come in as\nstreams? To the best of our knowledge, none of the previous works can solve all\nthe challenges simultaneously. In this paper, we propose an Online unsupervised\nMulti-View Feature Selection, OMVFS, which deals with large-scale/streaming\nmulti-view data in an online fashion. OMVFS embeds unsupervised feature\nselection into a clustering algorithm via NMF with sparse learning. It further\nincorporates the graph regularization to preserve the local structure\ninformation and help select discriminative features. Instead of storing all the\nhistorical data, OMVFS processes the multi-view data chunk by chunk and\naggregates all the necessary information into several small matrices. By using\nthe buffering technique, the proposed OMVFS can reduce the computational and\nstorage cost while taking advantage of the structure information. Furthermore,\nOMVFS can capture the concept drifts in the data streams. Extensive experiments\non four real-world datasets show the effectiveness and efficiency of the\nproposed OMVFS method. More importantly, OMVFS is about 100 times faster than\nthe off-line methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 07:10:16 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Shao", "Weixiang", ""], ["He", "Lifang", ""], ["Lu", "Chun-Ta", ""], ["Wei", "Xiaokai", ""], ["Yu", "Philip S.", ""]]}, {"id": "1609.08312", "submitter": "Chung Chan", "authors": "Chung Chan, Ali Al-Bashabsheh, Qiaoqiao Zhou, Tie Liu", "title": "Duality between Feature Selection and Data Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The feature-selection problem is formulated from an information-theoretic\nperspective. We show that the problem can be efficiently solved by an extension\nof the recently proposed info-clustering paradigm. This reveals the fundamental\nduality between feature selection and data clustering,which is a consequence of\nthe more general duality between the principal partition and the principal\nlattice of partitions in combinatorial optimization.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 08:29:39 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 14:01:49 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Chan", "Chung", ""], ["Al-Bashabsheh", "Ali", ""], ["Zhou", "Qiaoqiao", ""], ["Liu", "Tie", ""]]}, {"id": "1609.08326", "submitter": "Shuxin Zheng", "authors": "Shuxin Zheng, Qi Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhi-Ming\n  Ma, Tie-Yan Liu", "title": "Asynchronous Stochastic Gradient Descent with Delay Compensation", "comments": "20 pages, 5 figures", "journal-ref": "International Conference on Machine Learning. 2017: 4120-4129", "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the fast development of deep learning, it has become common to learn big\nneural networks using massive training data. Asynchronous Stochastic Gradient\nDescent (ASGD) is widely adopted to fulfill this task for its efficiency, which\nis, however, known to suffer from the problem of delayed gradients. That is,\nwhen a local worker adds its gradient to the global model, the global model may\nhave been updated by other workers and this gradient becomes \"delayed\". We\npropose a novel technology to compensate this delay, so as to make the\noptimization behavior of ASGD closer to that of sequential SGD. This is\nachieved by leveraging Taylor expansion of the gradient function and efficient\napproximation to the Hessian matrix of the loss function. We call the new\nalgorithm Delay Compensated ASGD (DC-ASGD). We evaluated the proposed algorithm\non CIFAR-10 and ImageNet datasets, and the experimental results demonstrate\nthat DC-ASGD outperforms both synchronous SGD and asynchronous SGD, and nearly\napproaches the performance of sequential SGD.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 09:22:03 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 17:53:10 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 09:02:32 GMT"}, {"version": "v4", "created": "Wed, 14 Jun 2017 12:45:50 GMT"}, {"version": "v5", "created": "Wed, 21 Aug 2019 12:34:15 GMT"}, {"version": "v6", "created": "Tue, 18 Feb 2020 15:04:38 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Zheng", "Shuxin", ""], ["Meng", "Qi", ""], ["Wang", "Taifeng", ""], ["Chen", "Wei", ""], ["Yu", "Nenghai", ""], ["Ma", "Zhi-Ming", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1609.08337", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Lantian Li and Dong Wang", "title": "Multi-task Recurrent Model for True Multilingual Speech Recognition", "comments": "APSIPA 2016. arXiv admin note: text overlap with arXiv:1603.09643", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on multilingual speech recognition remains attractive yet\nchallenging. Recent studies focus on learning shared structures under the\nmulti-task paradigm, in particular a feature sharing structure. This approach\nhas been found effective to improve performance on each individual language.\nHowever, this approach is only useful when the deployed system supports just\none language. In a true multilingual scenario where multiple languages are\nallowed, performance will be significantly reduced due to the competition among\nlanguages in the decoding space. This paper presents a multi-task recurrent\nmodel that involves a multilingual speech recognition (ASR) component and a\nlanguage recognition (LR) component, and the ASR component is informed of the\nlanguage information by the LR component, leading to a language-aware\nrecognition. We tested the approach on an English-Chinese bilingual recognition\ntask. The results show that the proposed multi-task recurrent model can improve\nperformance of multilingual recognition systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 09:56:09 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Li", "Lantian", ""], ["Wang", "Dong", ""]]}, {"id": "1609.08349", "submitter": "Luca Martino", "authors": "Jesse Read, Luca Martino, Jaakko Hollm\\'en", "title": "Multi-label Methods for Prediction with Sequential Data", "comments": null, "journal-ref": "Pattern Recognition, Volume 63, Pages 45-55, 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of methods available for classification of multi-label data has\nincreased rapidly over recent years, yet relatively few links have been made\nwith the related task of classification of sequential data. If labels indices\nare considered as time indices, the problems can often be seen as equivalent.\nIn this paper we detect and elaborate on connections between multi-label\nmethods and Markovian models, and study the suitability of multi-label methods\nfor prediction in sequential data. From this study we draw upon the most\nsuitable techniques from the area and develop two novel competitive approaches\nwhich can be applied to either kind of data. We carry out an empirical\nevaluation investigating performance on real-world sequential-prediction tasks:\nelectricity demand, and route prediction. As well as showing that several\npopular multi-label algorithms are in fact easily applicable to sequencing\ntasks, our novel approaches, which benefit from a unified view of these areas,\nprove very competitive against established methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 10:53:37 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 09:02:28 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Read", "Jesse", ""], ["Martino", "Luca", ""], ["Hollm\u00e9n", "Jaakko", ""]]}, {"id": "1609.08395", "submitter": "Juan Pablo Carbajal", "authors": "Juan Pablo Carbajal, Jo\\~ao Paulo Leit\\~ao, Carlo Albert, J\\\"org\n  Rieckermann", "title": "Appraisal of data-driven and mechanistic emulators of nonlinear\n  hydrodynamic urban drainage simulators", "comments": "This article was published in Environmental Modelling and Software, 7\n  figures, 4 tables", "journal-ref": null, "doi": "10.1016/j.envsoft.2017.02.006", "report-no": null, "categories": "stat.ME cs.CE cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Many model based scientific and engineering methodologies, such as system\nidentification, sensitivity analysis, optimization and control, require a large\nnumber of model evaluations. In particular, model based real-time control of\nurban water infrastructures and online flood alarm systems require fast\nprediction of the network response at different actuation and/or parameter\nvalues. General purpose urban drainage simulators are too slow for this\napplication. Fast surrogate models, so-called emulators, provide a solution to\nthis efficiency demand. Emulators are attractive, because they sacrifice\nunneeded accuracy in favor of speed. However, they have to be fine-tuned to\npredict the system behavior satisfactorily. Also, some emulators fail to\nextrapolate the system behavior beyond the training set. Although, there are\nmany strategies for developing emulators, up until now the selection of the\nemulation strategy remains subjective. In this paper, we therefore compare the\nperformance of two families of emulators for open channel flows in the context\nof urban drainage simulators. We compare emulators that explicitly use\nknowledge of the simulator's equations, i.e. mechanistic emulators based on\nGaussian Processes, with purely data-driven emulators using matrix\nfactorization. Our results suggest that in many urban applications, naive\ndata-driven emulation outperforms mechanistic emulation. Nevertheless, we\ndiscuss scenarios in which we think that mechanistic emulation might be\nfavorable for i) extrapolation in time and ii) dealing with sparse and unevenly\nsampled data. We also provide many references to advances in the field of\nMachine Learning that have not yet permeated into the Bayesian environmental\nscience community.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 21:57:34 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 12:30:52 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Carbajal", "Juan Pablo", ""], ["Leit\u00e3o", "Jo\u00e3o Paulo", ""], ["Albert", "Carlo", ""], ["Rieckermann", "J\u00f6rg", ""]]}, {"id": "1609.08397", "submitter": "Qi Meng", "authors": "Qi Meng, Yue Wang, Wei Chen, Taifeng Wang, Zhi-Ming Ma, and Tie-Yan\n  Liu", "title": "Generalization Error Bounds for Optimization Algorithms via Stability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning tasks can be formulated as Regularized Empirical Risk\nMinimization (R-ERM), and solved by optimization algorithms such as gradient\ndescent (GD), stochastic gradient descent (SGD), and stochastic variance\nreduction (SVRG). Conventional analysis on these optimization algorithms\nfocuses on their convergence rates during the training process, however, people\nin the machine learning community may care more about the generalization\nperformance of the learned model on unseen test data. In this paper, we\ninvestigate on this issue, by using stability as a tool. In particular, we\ndecompose the generalization error for R-ERM, and derive its upper bound for\nboth convex and non-convex cases. In convex cases, we prove that the\ngeneralization error can be bounded by the convergence rate of the optimization\nalgorithm and the stability of the R-ERM process, both in expectation (in the\norder of $\\mathcal{O}((1/n)+\\mathbb{E}\\rho(T))$, where $\\rho(T)$ is the\nconvergence error and $T$ is the number of iterations) and in high probability\n(in the order of\n$\\mathcal{O}\\left(\\frac{\\log{1/\\delta}}{\\sqrt{n}}+\\rho(T)\\right)$ with\nprobability $1-\\delta$). For non-convex cases, we can also obtain a similar\nexpected generalization error bound. Our theorems indicate that 1) along with\nthe training process, the generalization error will decrease for all the\noptimization algorithms under our investigation; 2) Comparatively speaking,\nSVRG has better generalization ability than GD and SGD. We have conducted\nexperiments on both convex and non-convex problems, and the experimental\nresults verify our theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 13:10:57 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Meng", "Qi", ""], ["Wang", "Yue", ""], ["Chen", "Wei", ""], ["Wang", "Taifeng", ""], ["Ma", "Zhi-Ming", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1609.08408", "submitter": "Ilyas Potamitis", "authors": "Ilyas Potamitis", "title": "Deep learning for detection of bird vocalisations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on reliable detection of bird sound emissions as recorded\nin the open field. Acoustic detection of avian sounds can be used for the\nautomatized monitoring of multiple bird taxa and querying in long-term\nrecordings for species of interest for researchers, conservation practitioners,\nand decision makers. Recordings in the wild can be very noisy due to the\nexposure of the microphones to a large number of audio sources originating from\nall distances and directions, the number and identity of which cannot be known\na-priori. The co-existence of the target vocalizations with abiotic\ninterferences in an unconstrained environment is inefficiently treated by\ncurrent approaches of audio signal enhancement. A technique that would spot\nonly bird vocalization while ignoring other audio sources is of prime\nimportance. These difficulties are tackled in this work, presenting a deep\nautoencoder that maps the audio spectrogram of bird vocalizations to its\ncorresponding binary mask that encircles the spectral blobs of vocalizations\nwhile suppressing other audio sources. The procedure requires minimum human\nattendance, it is very fast during execution, thus suitable to scan massive\nvolumes of data, in order to analyze them, evaluate insights and hypotheses,\nidentify patterns of bird activity that, hopefully, finally lead to design\npolicies on biodiversity issues.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 15:56:06 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Potamitis", "Ilyas", ""]]}, {"id": "1609.08435", "submitter": "Qi Meng", "authors": "Qi Meng, Wei Chen, Jingcheng Yu, Taifeng Wang, Zhi-Ming Ma, Tie-Yan\n  Liu", "title": "Asynchronous Stochastic Proximal Optimization Algorithms with Variance\n  Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized empirical risk minimization (R-ERM) is an important branch of\nmachine learning, since it constrains the capacity of the hypothesis space and\nguarantees the generalization ability of the learning algorithm. Two classic\nproximal optimization algorithms, i.e., proximal stochastic gradient descent\n(ProxSGD) and proximal stochastic coordinate descent (ProxSCD) have been widely\nused to solve the R-ERM problem. Recently, variance reduction technique was\nproposed to improve ProxSGD and ProxSCD, and the corresponding ProxSVRG and\nProxSVRCD have better convergence rate. These proximal algorithms with variance\nreduction technique have also achieved great success in applications at small\nand moderate scales. However, in order to solve large-scale R-ERM problems and\nmake more practical impacts, the parallel version of these algorithms are\nsorely needed. In this paper, we propose asynchronous ProxSVRG (Async-ProxSVRG)\nand asynchronous ProxSVRCD (Async-ProxSVRCD) algorithms, and prove that\nAsync-ProxSVRG can achieve near linear speedup when the training data is\nsparse, while Async-ProxSVRCD can achieve near linear speedup regardless of the\nsparse condition, as long as the number of block partitions are appropriately\nset. We have conducted experiments on a regularized logistic regression task.\nThe results verified our theoretical findings and demonstrated the practical\nefficiency of the asynchronous stochastic proximal algorithms with variance\nreduction.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 13:40:00 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Meng", "Qi", ""], ["Chen", "Wei", ""], ["Yu", "Jingcheng", ""], ["Wang", "Taifeng", ""], ["Ma", "Zhi-Ming", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1609.08441", "submitter": "Lantian Li Mr.", "authors": "Lantian Li, Yixiang Chen, Dong Wang, Chenghui Zhao", "title": "Weakly Supervised PLDA Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PLDA is a popular normalization approach for the i-vector model, and it has\ndelivered state-of-the-art performance in speaker verification. However, PLDA\ntraining requires a large amount of labelled development data, which is highly\nexpensive in most cases. We present a cheap PLDA training approach, which\nassumes that speakers in the same session can be easily separated, and speakers\nin different sessions are simply different. This results in `weak labels' which\nare not fully accurate but cheap, leading to a weak PLDA training.\n  Our experimental results on real-life large-scale telephony customer service\nachieves demonstrated that the weak training can offer good performance when\nhuman-labelled data are limited. More interestingly, the weak training can be\nemployed as a discriminative adaptation approach, which is more efficient than\nthe prevailing unsupervised method when human-labelled data are insufficient.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 13:46:55 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 10:19:15 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Li", "Lantian", ""], ["Chen", "Yixiang", ""], ["Wang", "Dong", ""], ["Zhao", "Chenghui", ""]]}, {"id": "1609.08496", "submitter": "Jipeng Qiang", "authors": "Jipeng Qiang, Ping Chen, Tong Wang, Xindong Wu", "title": "Topic Modeling over Short Texts by Incorporating Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring topics from the overwhelming amount of short texts becomes a\ncritical but challenging task for many content analysis tasks, such as content\ncharactering, user interest profiling, and emerging topic detecting. Existing\nmethods such as probabilistic latent semantic analysis (PLSA) and latent\nDirichlet allocation (LDA) cannot solve this prob- lem very well since only\nvery limited word co-occurrence information is available in short texts. This\npaper studies how to incorporate the external word correlation knowledge into\nshort texts to improve the coherence of topic modeling. Based on recent results\nin word embeddings that learn se- mantically representations for words from a\nlarge corpus, we introduce a novel method, Embedding-based Topic Model (ETM),\nto learn latent topics from short texts. ETM not only solves the problem of\nvery limited word co-occurrence information by aggregating short texts into\nlong pseudo- texts, but also utilizes a Markov Random Field regularized model\nthat gives correlated words a better chance to be put into the same topic. The\nexperiments on real-world datasets validate the effectiveness of our model\ncomparing with the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 15:26:07 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Qiang", "Jipeng", ""], ["Chen", "Ping", ""], ["Wang", "Tong", ""], ["Wu", "Xindong", ""]]}, {"id": "1609.08550", "submitter": "Sander Stepanov Dr.", "authors": "Sander Stepanov", "title": "Correct classification for big/smart/fast data machine learning", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Table (database) / Relational database Classification for big/smart/fast data\nmachine learning is one of the most important tasks of predictive analytics and\nextracting valuable information from data. It is core applied technique for\nwhat now understood under data science and/or artificial intelligence. Widely\nused Decision Tree (Random Forest) and rare used rule based PRISM , VFST, etc\nclassifiers are empirical substitutions of theoretically correct to use Boolean\nfunctions minimization. Developing Minimization of Boolean functions algorithms\nis started long time ago by Edward Veitch's 1952. Since it, big efforts by wide\nscientific/industrial community was done to find feasible solution of Boolean\nfunctions minimization. In this paper we propose consider table data\nclassification from mathematical point of view, as minimization of Boolean\nfunctions. It is shown that data representation may be transformed to Boolean\nfunctions form and how to use known algorithms. For simplicity, binary output\nfunction is used for development, what opens doors for multivalued outputs\ndevelopments.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 17:50:41 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Stepanov", "Sander", ""]]}, {"id": "1609.08663", "submitter": "Safoora Yousefi", "authors": "Safoora Yousefi, Congzheng Song, Nelson Nauata, Lee Cooper", "title": "Learning Genomic Representations to Predict Clinical Outcomes in Cancer", "comments": "ICLR 2016 Workshop Track- May 2nd 2016 International Conference on\n  Learning Representations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genomics are rapidly transforming medical practice and basic biomedical\nresearch, providing insights into disease mechanisms and improving therapeutic\nstrategies, particularly in cancer. The ability to predict the future course of\na patient's disease from high-dimensional genomic profiling will be essential\nin realizing the promise of genomic medicine, but presents significant\nchallenges for state-of-the-art survival analysis methods. In this abstract we\npresent an investigation in learning genomic representations with neural\nnetworks to predict patient survival in cancer. We demonstrate the advantages\nof this approach over existing survival analysis methods using brain tumor\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 20:53:16 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Yousefi", "Safoora", ""], ["Song", "Congzheng", ""], ["Nauata", "Nelson", ""], ["Cooper", "Lee", ""]]}, {"id": "1609.08789", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Ying Shi, Dong Wang, Yang Feng and Shiyue Zhang", "title": "Memory Visualization for Gated Recurrent Neural Networks in Speech\n  Recognition", "comments": "ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have shown clear superiority in sequence\nmodeling, particularly the ones with gated units, such as long short-term\nmemory (LSTM) and gated recurrent unit (GRU). However, the dynamic properties\nbehind the remarkable performance remain unclear in many applications, e.g.,\nautomatic speech recognition (ASR). This paper employs visualization techniques\nto study the behavior of LSTM and GRU when performing speech recognition tasks.\nOur experiments show some interesting patterns in the gated memory, and some of\nthem have inspired simple yet effective modifications on the network structure.\nWe report two of such modifications: (1) lazy cell update in LSTM, and (2)\nshortcut connections for residual learning. Both modifications lead to more\ncomprehensible and powerful networks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 06:26:16 GMT"}, {"version": "v2", "created": "Mon, 26 Dec 2016 09:25:14 GMT"}, {"version": "v3", "created": "Mon, 27 Feb 2017 02:07:34 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Shi", "Ying", ""], ["Wang", "Dong", ""], ["Feng", "Yang", ""], ["Zhang", "Shiyue", ""]]}, {"id": "1609.08905", "submitter": "Giorgio Corani", "authors": "Giorgio Corani and Alessio Benavoli and Janez Dem\\v{s}ar and Francesca\n  Mangili and Marco Zaffalon", "title": "Statistical comparison of classifiers through Bayesian hierarchical\n  modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usually one compares the accuracy of two competing classifiers via null\nhypothesis significance tests (nhst). Yet the nhst tests suffer from important\nshortcomings, which can be overcome by switching to Bayesian hypothesis\ntesting. We propose a Bayesian hierarchical model which jointly analyzes the\ncross-validation results obtained by two classifiers on multiple data sets. It\nreturns the posterior probability of the accuracies of the two classifiers\nbeing practically equivalent or significantly different. A further strength of\nthe hierarchical model is that, by jointly analyzing the results obtained on\nall data sets, it reduces the estimation error compared to the usual approach\nof averaging the cross-validation results obtained on a given data set.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 13:30:31 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 08:23:38 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2016 15:16:45 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Corani", "Giorgio", ""], ["Benavoli", "Alessio", ""], ["Dem\u0161ar", "Janez", ""], ["Mangili", "Francesca", ""], ["Zaffalon", "Marco", ""]]}, {"id": "1609.08913", "submitter": "George Monta\\~nez", "authors": "George D. Montanez", "title": "The Famine of Forte: Few Search Problems Greatly Favor Your Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Casting machine learning as a type of search, we demonstrate that the\nproportion of problems that are favorable for a fixed algorithm is strictly\nbounded, such that no single algorithm can perform well over a large fraction\nof them. Our results explain why we must either continue to develop new\nlearning methods year after year or move towards highly parameterized models\nthat are both flexible and sensitive to their hyperparameters. We further give\nan upper bound on the expected performance for a search algorithm as a function\nof the mutual information between the target and the information resource\n(e.g., training dataset), proving the importance of certain types of dependence\nfor machine learning. Lastly, we show that the expected per-query probability\nof success for an algorithm is mathematically equivalent to a single-query\nprobability of success under a distribution (called a search strategy), and\nprove that the proportion of favorable strategies is also strictly bounded.\nThus, whether one holds fixed the search algorithm and considers all possible\nproblems or one fixes the search problem and looks at all possible search\nstrategies, favorable matches are exceedingly rare. The forte (strength) of any\nalgorithm is quantifiably restricted.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 13:52:17 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 14:21:53 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Montanez", "George D.", ""]]}, {"id": "1609.08934", "submitter": "Ioannis Avramopoulos", "authors": "Ioannis Avramopoulos", "title": "Multiplicative weights, equalizers, and P=PPAD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that, by using multiplicative weights in a game-theoretic thought\nexperiment (and an important convexity result on the composition of\nmultiplicative weights with the relative entropy function), a symmetric\nbimatrix game (that is, a bimatrix matrix wherein the payoff matrix of each\nplayer is the transpose of the payoff matrix of the other) either has an\ninterior symmetric equilibrium or there is a pure strategy that is weakly\ndominated by some mixed strategy. Weakly dominated pure strategies can be\ndetected and eliminated in polynomial time by solving a linear program.\nFurthermore, interior symmetric equilibria are a special case of a more general\nnotion, namely, that of an \"equalizer,\" which can also be computed efficiently\nin polynomial time by solving a linear program. An elegant \"symmetrization\nmethod\" of bimatrix games [Jurg et al., 1992] and the well-known\nPPAD-completeness results on equilibrium computation in bimatrix games\n[Daskalakis et al., 2009, Chen et al., 2009] imply then the compelling P =\nPPAD.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 14:43:51 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Avramopoulos", "Ioannis", ""]]}, {"id": "1609.08976", "submitter": "Chunyuan Li", "authors": "Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew\n  Stevens, Lawrence Carin", "title": "Variational Autoencoder for Deep Learning of Images, Labels and Captions", "comments": "NIPS 2016 (To appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel variational autoencoder is developed to model images, as well as\nassociated labels or captions. The Deep Generative Deconvolutional Network\n(DGDN) is used as a decoder of the latent image features, and a deep\nConvolutional Neural Network (CNN) is used as an image encoder; the CNN is used\nto approximate a distribution for the latent DGDN features/code. The latent\ncode is also linked to generative models for labels (Bayesian support vector\nmachine) or captions (recurrent neural network). When predicting a\nlabel/caption for a new image at test, averaging is performed across the\ndistribution of latent codes; this is computationally efficient as a\nconsequence of the learned CNN-based encoder. Since the framework is capable of\nmodeling the image in the presence/absence of associated labels/captions, a new\nsemi-supervised setting is manifested for CNN learning with images; the\nframework even allows unsupervised CNN learning, based on images alone.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 15:56:15 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Pu", "Yunchen", ""], ["Gan", "Zhe", ""], ["Henao", "Ricardo", ""], ["Yuan", "Xin", ""], ["Li", "Chunyuan", ""], ["Stevens", "Andrew", ""], ["Carin", "Lawrence", ""]]}, {"id": "1609.09001", "submitter": "Aviv Tamar", "authors": "Aviv Tamar, Garrett Thomas, Tianhao Zhang, Sergey Levine, Pieter\n  Abbeel", "title": "Learning from the Hindsight Plan -- Episodic MPC Improvement", "comments": "Additional experiments for neural network generalization and for\n  varying the planning horizon. Paper accepted to ICRA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model predictive control (MPC) is a popular control method that has proved\neffective for robotics, among other fields. MPC performs re-planning at every\ntime step. Re-planning is done with a limited horizon per computational and\nreal-time constraints and often also for robustness to potential model errors.\nHowever, the limited horizon leads to suboptimal performance. In this work, we\nconsider the iterative learning setting, where the same task can be repeated\nseveral times, and propose a policy improvement scheme for MPC. The main idea\nis that between executions we can, offline, run MPC with a longer horizon,\nresulting in a hindsight plan. To bring the next real-world execution closer to\nthe hindsight plan, our approach learns to re-shape the original cost function\nwith the goal of satisfying the following property: short horizon planning (as\nrealistic during real executions) with respect to the shaped cost should result\nin mimicking the hindsight plan. This effectively consolidates long-term\nreasoning into the short-horizon planning. We empirically evaluate our approach\nin contact-rich manipulation tasks both in simulated and real environments,\nsuch as peg insertion by a real PR2 robot.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 16:43:18 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 22:29:23 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Tamar", "Aviv", ""], ["Thomas", "Garrett", ""], ["Zhang", "Tianhao", ""], ["Levine", "Sergey", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1609.09007", "submitter": "Ke Tran", "authors": "Ke Tran, Yonatan Bisk, Ashish Vaswani, Daniel Marcu, Kevin Knight", "title": "Unsupervised Neural Hidden Markov Models", "comments": "accepted at EMNLP 2016, Workshop on Structured Prediction for NLP.\n  Oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present the first results for neuralizing an Unsupervised\nHidden Markov Model. We evaluate our approach on tag in- duction. Our approach\noutperforms existing generative models and is competitive with the\nstate-of-the-art though with a simpler model easily extended to include\nadditional context.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 16:55:52 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Tran", "Ke", ""], ["Bisk", "Yonatan", ""], ["Vaswani", "Ashish", ""], ["Marcu", "Daniel", ""], ["Knight", "Kevin", ""]]}, {"id": "1609.09025", "submitter": "Lerrel Pinto Mr", "authors": "Lerrel Pinto and Abhinav Gupta", "title": "Learning to Push by Grasping: Using multiple tasks for effective\n  learning", "comments": "Under review at the International Conference on Robotics and\n  Automation (ICRA) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, end-to-end learning frameworks are gaining prevalence in the field\nof robot control. These frameworks input states/images and directly predict the\ntorques or the action parameters. However, these approaches are often critiqued\ndue to their huge data requirements for learning a task. The argument of the\ndifficulty in scalability to multiple tasks is well founded, since training\nthese tasks often require hundreds or thousands of examples. But do end-to-end\napproaches need to learn a unique model for every task? Intuitively, it seems\nthat sharing across tasks should help since all tasks require some common\nunderstanding of the environment. In this paper, we attempt to take the next\nstep in data-driven end-to-end learning frameworks: move from the realm of\ntask-specific models to joint learning of multiple robot tasks. In an\nastonishing result we show that models with multi-task learning tend to perform\nbetter than task-specific models trained with same amounts of data. For\nexample, a deep-network learned with 2.5K grasp and 2.5K push examples performs\nbetter on grasping than a network trained on 5K grasp examples.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 18:13:02 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Pinto", "Lerrel", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1609.09049", "submitter": "Marvin Zhang", "authors": "Marvin Zhang, Xinyang Geng, Jonathan Bruce, Ken Caluwaerts, Massimo\n  Vespignani, Vytas SunSpiral, Pieter Abbeel, Sergey Levine", "title": "Deep Reinforcement Learning for Tensegrity Robot Locomotion", "comments": "International Conference on Robotics and Automation (ICRA), 2017.\n  Project website link is http://rll.berkeley.edu/drl_tensegrity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensegrity robots, composed of rigid rods connected by elastic cables, have a\nnumber of unique properties that make them appealing for use as planetary\nexploration rovers. However, control of tensegrity robots remains a difficult\nproblem due to their unusual structures and complex dynamics. In this work, we\nshow how locomotion gaits can be learned automatically using a novel extension\nof mirror descent guided policy search (MDGPS) applied to periodic locomotion\nmovements, and we demonstrate the effectiveness of our approach on tensegrity\nrobot locomotion. We evaluate our method with real-world and simulated\nexperiments on the SUPERball tensegrity robot, showing that the learned\npolicies generalize to changes in system parameters, unreliable sensor\nmeasurements, and variation in environmental conditions, including varied\nterrains and a range of different gravities. Our experiments demonstrate that\nour method not only learns fast, power-efficient feedback policies for rolling\ngaits, but that these policies can succeed with only the limited onboard\nsensing provided by SUPERball's accelerometers. We compare the learned feedback\npolicies to learned open-loop policies and hand-engineered controllers, and\ndemonstrate that the learned policy enables the first continuous, reliable\nlocomotion gait for the real SUPERball robot. Our code and other supplementary\nmaterials are available from http://rll.berkeley.edu/drl_tensegrity\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 19:38:03 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 02:03:41 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 01:52:36 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Zhang", "Marvin", ""], ["Geng", "Xinyang", ""], ["Bruce", "Jonathan", ""], ["Caluwaerts", "Ken", ""], ["Vespignani", "Massimo", ""], ["SunSpiral", "Vytas", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1609.09106", "submitter": "David Ha", "authors": "David Ha, Andrew Dai and Quoc V. Le", "title": "HyperNetworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores hypernetworks: an approach of using a one network, also\nknown as a hypernetwork, to generate the weights for another network.\nHypernetworks provide an abstraction that is similar to what is found in\nnature: the relationship between a genotype - the hypernetwork - and a\nphenotype - the main network. Though they are also reminiscent of HyperNEAT in\nevolution, our hypernetworks are trained end-to-end with backpropagation and\nthus are usually faster. The focus of this work is to make hypernetworks useful\nfor deep convolutional networks and long recurrent networks, where\nhypernetworks can be viewed as relaxed form of weight-sharing across layers.\nOur main result is that hypernetworks can generate non-shared weights for LSTM\nand achieve near state-of-the-art results on a variety of sequence modelling\ntasks including character-level language modelling, handwriting generation and\nneural machine translation, challenging the weight-sharing paradigm for\nrecurrent networks. Our results also show that hypernetworks applied to\nconvolutional networks still achieve respectable results for image recognition\ntasks compared to state-of-the-art baseline models while requiring fewer\nlearnable parameters.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 05:57:00 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 02:04:56 GMT"}, {"version": "v3", "created": "Fri, 28 Oct 2016 00:28:32 GMT"}, {"version": "v4", "created": "Thu, 1 Dec 2016 10:08:15 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Ha", "David", ""], ["Dai", "Andrew", ""], ["Le", "Quoc V.", ""]]}, {"id": "1609.09116", "submitter": "Yu Ding", "authors": "Yu Ding", "title": "Analysis of Massive Heterogeneous Temporal-Spatial Data with 3D\n  Self-Organizing Map and Time Vector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-organizing map(SOM) have been widely applied in clustering, this paper\nfocused on centroids of clusters and what they reveal. When the input vectors\nconsists of time, latitude and longitude, the map can be strongly linked to\nphysical world, providing valuable information. Beyond basic clustering, a\nnovel approach to address the temporal element is developed, enabling 3D SOM to\ntrack behaviors in multiple periods concurrently. Combined with adaptations\ntargeting to process heterogeneous data relating to distribution in time and\nspace, the paper offers a fresh scope for business and services based on\ntemporal-spatial pattern.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 08:25:40 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Ding", "Yu", ""]]}, {"id": "1609.09156", "submitter": "Minyoung Kim", "authors": "Minyoung Kim, Stefano Alletto, Luca Rigazio", "title": "Similarity Mapping with Enhanced Siamese Network for Multi-Object\n  Tracking", "comments": "1) accepted as a poster presentation at WiML (Women in Machine\n  Learning) workshop 2016, colocated with NIPS 2016 in Barcelona, Spain, 2)\n  accepted as a poster presentation at MLITS (Machine Learning for Intelligent\n  Transportation Systems) Workshop held in conjunction with the NIPS 2016 in\n  Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-object tracking has recently become an important area of computer\nvision, especially for Advanced Driver Assistance Systems (ADAS). Despite\ngrowing attention, achieving high performance tracking is still challenging,\nwith state-of-the- art systems resulting in high complexity with a large number\nof hyper parameters. In this paper, we focus on reducing overall system\ncomplexity and the number hyper parameters that need to be tuned to a specific\nenvironment. We introduce a novel tracking system based on similarity mapping\nby Enhanced Siamese Neural Network (ESNN), which accounts for both appearance\nand geometric information, and is trainable end-to-end. Our system achieves\ncompetitive performance in both speed and accuracy on MOT16 challenge, compared\nto known state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 23:52:50 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 00:51:57 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Kim", "Minyoung", ""], ["Alletto", "Stefano", ""], ["Rigazio", "Luca", ""]]}, {"id": "1609.09162", "submitter": "Sauptik Dhar", "authors": "Sauptik Dhar, Naveen Ramakrishnan, Vladimir Cherkassky, Mohak Shah", "title": "Universum Learning for Multiclass SVM", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Universum learning for multiclass problems and propose a novel\nformulation for multiclass universum SVM (MU-SVM). We also propose a span bound\nfor MU-SVM that can be used for model selection thereby avoiding resampling.\nEmpirical results demonstrate the effectiveness of MU-SVM and the proposed\nbound.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 00:43:03 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Dhar", "Sauptik", ""], ["Ramakrishnan", "Naveen", ""], ["Cherkassky", "Vladimir", ""], ["Shah", "Mohak", ""]]}, {"id": "1609.09178", "submitter": "Wenbin Li", "authors": "Wenbin Li, Yang Gao, Lei Wang, Luping Zhou, Jing Huo, Yinghuan Shi", "title": "OPML: A One-Pass Closed-Form Solution for Online Metric Learning", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve a low computational cost when performing online metric learning\nfor large-scale data, we present a one-pass closed-form solution namely OPML in\nthis paper. Typically, the proposed OPML first adopts a one-pass triplet\nconstruction strategy, which aims to use only a very small number of triplets\nto approximate the representation ability of whole original triplets obtained\nby batch-manner methods. Then, OPML employs a closed-form solution to update\nthe metric for new coming samples, which leads to a low space (i.e., $O(d)$)\nand time (i.e., $O(d^2)$) complexity, where $d$ is the feature dimensionality.\nIn addition, an extension of OPML (namely COPML) is further proposed to enhance\nthe robustness when in real case the first several samples come from the same\nclass (i.e., cold start problem). In the experiments, we have systematically\nevaluated our methods (OPML and COPML) on three typical tasks, including UCI\ndata classification, face verification, and abnormal event detection in videos,\nwhich aims to fully evaluate the proposed methods on different sample number,\ndifferent feature dimensionalities and different feature extraction ways (i.e.,\nhand-crafted and deeply-learned). The results show that OPML and COPML can\nobtain the promising performance with a very low computational cost. Also, the\neffectiveness of COPML under the cold start setting is experimentally verified.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 02:18:06 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Li", "Wenbin", ""], ["Gao", "Yang", ""], ["Wang", "Lei", ""], ["Zhou", "Luping", ""], ["Huo", "Jing", ""], ["Shi", "Yinghuan", ""]]}, {"id": "1609.09188", "submitter": "Leonard K.M. Poon", "authors": "Leonard K.M. Poon and Nevin L. Zhang", "title": "Topic Browsing for Research Papers with Hierarchical Latent Tree\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Academic researchers often need to face with a large collection of research\npapers in the literature. This problem may be even worse for postgraduate\nstudents who are new to a field and may not know where to start. To address\nthis problem, we have developed an online catalog of research papers where the\npapers have been automatically categorized by a topic model. The catalog\ncontains 7719 papers from the proceedings of two artificial intelligence\nconferences from 2000 to 2015. Rather than the commonly used Latent Dirichlet\nAllocation, we use a recently proposed method called hierarchical latent tree\nanalysis for topic modeling. The resulting topic model contains a hierarchy of\ntopics so that users can browse the topics from the top level to the bottom\nlevel. The topic model contains a manageable number of general topics at the\ntop level and allows thousands of fine-grained topics at the bottom level. It\nalso can detect topics that have emerged recently.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 03:22:01 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Poon", "Leonard K. M.", ""], ["Zhang", "Nevin L.", ""]]}, {"id": "1609.09194", "submitter": "R Vivek", "authors": "Priyanka H U and Vivek R", "title": "Multi Model Data mining approach for Heart failure prediction", "comments": null, "journal-ref": null, "doi": "10.5121/ijdkp.2016.6503", "report-no": null, "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing predictive modelling solutions for risk estimation is extremely\nchallenging in health-care informatics. Risk estimation involves integration of\nheterogeneous clinical sources having different representation from different\nhealth-care provider making the task increasingly complex. Such sources are\ntypically voluminous, diverse, and significantly change over the time.\nTherefore, distributed and parallel computing tools collectively termed big\ndata tools are in need which can synthesize and assist the physician to make\nright clinical decisions. In this work we propose multi-model predictive\narchitecture, a novel approach for combining the predictive ability of multiple\nmodels for better prediction accuracy. We demonstrate the effectiveness and\nefficiency of the proposed work on data from Framingham Heart study. Results\nshow that the proposed multi-model predictive architecture is able to provide\nbetter accuracy than best model approach. By modelling the error of predictive\nmodels we are able to choose sub set of models which yields accurate results.\nMore information was modelled into system by multi-level mining which has\nresulted in enhanced predictive accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 03:53:22 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["U", "Priyanka H", ""], ["R", "Vivek", ""]]}, {"id": "1609.09196", "submitter": "Davis Blalock", "authors": "Davis W. Blalock, John V. Guttag", "title": "EXTRACT: Strong Examples from Weakly-Labeled Sensor Data", "comments": "To appear in IEEE International Conference on Data Mining 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the rise of wearable and connected devices, sensor-generated time\nseries comprise a large and growing fraction of the world's data.\nUnfortunately, extracting value from this data can be challenging, since\nsensors report low-level signals (e.g., acceleration), not the high-level\nevents that are typically of interest (e.g., gestures). We introduce a\ntechnique to bridge this gap by automatically extracting examples of real-world\nevents in low-level data, given only a rough estimate of when these events have\ntaken place.\n  By identifying sets of features that repeat in the same temporal arrangement,\nwe isolate examples of such diverse events as human actions, power consumption\npatterns, and spoken words with up to 96% precision and recall. Our method is\nfast enough to run in real time and assumes only minimal knowledge of which\nvariables are relevant or the lengths of events. Our evaluation uses numerous\npublicly available datasets and over 1 million samples of manually labeled\nsensor data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 04:02:31 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Blalock", "Davis W.", ""], ["Guttag", "John V.", ""]]}, {"id": "1609.09199", "submitter": "Yael Yankelevsky", "authors": "Yael Yankelevsky and Michael Elad", "title": "Structure-Aware Classification using Supervised Dictionary Learning", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2017.7952992", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a supervised dictionary learning algorithm that\naims to preserve the local geometry in both dimensions of the data. A\ngraph-based regularization explicitly takes into account the local manifold\nstructure of the data points. A second graph regularization gives similar\ntreatment to the feature domain and helps in learning a more robust dictionary.\nBoth graphs can be constructed from the training data or learned and adapted\nalong the dictionary learning process. The combination of these two terms\npromotes the discriminative power of the learned sparse representations and\nleads to improved classification accuracy. The proposed method was evaluated on\nseveral different datasets, representing both single-label and multi-label\nclassification problems, and demonstrated better performance compared with\nother dictionary based approaches.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 04:30:10 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Yankelevsky", "Yael", ""], ["Elad", "Michael", ""]]}, {"id": "1609.09247", "submitter": "Yue Zhang", "authors": "Zhenghua Li, Yue Zhang, Jiayuan Chao, Min Zhang", "title": "Training Dependency Parsers with Partial Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, these has been a surge on studying how to obtain partially\nannotated data for model supervision. However, there still lacks a systematic\nstudy on how to train statistical models with partial annotation (PA). Taking\ndependency parsing as our case study, this paper describes and compares two\nstraightforward approaches for three mainstream dependency parsers. The first\napproach is previously proposed to directly train a log-linear graph-based\nparser (LLGPar) with PA based on a forest-based objective. This work for the\nfirst time proposes the second approach to directly training a linear\ngraph-based parse (LGPar) and a linear transition-based parser (LTPar) with PA\nbased on the idea of constrained decoding. We conduct extensive experiments on\nPenn Treebank under three different settings for simulating PA, i.e., random\ndependencies, most uncertain dependencies, and dependencies with divergent\noutputs from the three parsers. The results show that LLGPar is most effective\nin learning from PA and LTPar lags behind the graph-based counterparts by large\nmargin. Moreover, LGPar and LTPar can achieve best performance by using LLGPar\nto complete PA into full annotation (FA).\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 08:12:14 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Li", "Zhenghua", ""], ["Zhang", "Yue", ""], ["Chao", "Jiayuan", ""], ["Zhang", "Min", ""]]}, {"id": "1609.09341", "submitter": "Francesco Trov\\'o Mr.", "authors": "Giuseppe De Nittis and Francesco Trov\\`o", "title": "Machine Learning Techniques for Stackelberg Security Games: a Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present survey aims at presenting the current machine learning techniques\nemployed in security games domains. Specifically, we focused on papers and\nworks developed by the Teamcore of University of Southern California, which\ndeepened different directions in this field. After a brief introduction on\nStackelberg Security Games (SSGs) and the poaching setting, the rest of the\nwork presents how to model a boundedly rational attacker taking into account\nher human behavior, then describes how to face the problem of having attacker's\npayoffs not defined and how to estimate them and, finally, presents how online\nlearning techniques have been exploited to learn a model of the attacker.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 13:53:26 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["De Nittis", "Giuseppe", ""], ["Trov\u00f2", "Francesco", ""]]}, {"id": "1609.09353", "submitter": "Di Chen", "authors": "Di Chen, Yexiang Xue, Shuo Chen, Daniel Fink, Carla Gomes", "title": "Deep Multi-Species Embedding", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.PE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how species are distributed across landscapes over time is a\nfundamental question in biodiversity research. Unfortunately, most species\ndistribution models only target a single species at a time, despite strong\necological evidence that species are not independently distributed. We propose\nDeep Multi-Species Embedding (DMSE), which jointly embeds vectors corresponding\nto multiple species as well as vectors representing environmental covariates\ninto a common high-dimensional feature space via a deep neural network. Applied\nto bird observational data from the citizen science project \\textit{eBird}, we\ndemonstrate how the DMSE model discovers inter-species relationships to\noutperform single-species distribution models (random forests and SVMs) as well\nas competing multi-label models. Additionally, we demonstrate the benefit of\nusing a deep neural network to extract features within the embedding and show\nhow they improve the predictive performance of species distribution modelling.\nAn important domain contribution of the DMSE model is the ability to discover\nand describe species interactions while simultaneously learning the shared\nhabitat preferences among species. As an additional contribution, we provide a\ngraphical embedding of hundreds of bird species in the Northeast US.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 00:39:47 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 22:20:18 GMT"}, {"version": "v3", "created": "Mon, 20 Feb 2017 11:12:38 GMT"}, {"version": "v4", "created": "Tue, 21 Feb 2017 15:35:11 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Chen", "Di", ""], ["Xue", "Yexiang", ""], ["Chen", "Shuo", ""], ["Fink", "Daniel", ""], ["Gomes", "Carla", ""]]}, {"id": "1609.09365", "submitter": "Julie Dequaire", "authors": "Julie Dequaire, Dushyant Rao, Peter Ondruska, Dominic Wang and Ingmar\n  Posner", "title": "Deep Tracking on the Move: Learning to Track the World from a Moving\n  Vehicle using Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an end-to-end approach for tracking static and dynamic\nobjects for an autonomous vehicle driving through crowded urban environments.\nUnlike traditional approaches to tracking, this method is learned end-to-end,\nand is able to directly predict a full unoccluded occupancy grid map from raw\nlaser input data. Inspired by the recently presented DeepTracking approach\n[Ondruska, 2016], we employ a recurrent neural network (RNN) to capture the\ntemporal evolution of the state of the environment, and propose to use Spatial\nTransformer modules to exploit estimates of the egomotion of the vehicle. Our\nresults demonstrate the ability to track a range of objects, including cars,\nbuses, pedestrians, and cyclists through occlusion, from both moving and\nstationary platforms, using a single learned model. Experimental results\ndemonstrate that the model can also predict the future states of objects from\ncurrent inputs, with greater accuracy than previous work.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 14:39:10 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 21:35:15 GMT"}, {"version": "v3", "created": "Wed, 19 Apr 2017 14:31:32 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Dequaire", "Julie", ""], ["Rao", "Dushyant", ""], ["Ondruska", "Peter", ""], ["Wang", "Dominic", ""], ["Posner", "Ingmar", ""]]}, {"id": "1609.09430", "submitter": "Shawn Hershey", "authors": "Shawn Hershey, Sourish Chaudhuri, Daniel P. W. Ellis, Jort F. Gemmeke,\n  Aren Jansen, R. Channing Moore, Manoj Plakal, Devin Platt, Rif A. Saurous,\n  Bryan Seybold, Malcolm Slaney, Ron J. Weiss, Kevin Wilson", "title": "CNN Architectures for Large-Scale Audio Classification", "comments": "Accepted for publication at ICASSP 2017 Changes: Added definitions of\n  mAP, AUC, and d-prime. Updated mAP/AUC/d-prime numbers for Audio Set based on\n  changes of latest Audio Set revision. Changed wording to fit 4 page limit\n  with new additions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have proven very effective in image\nclassification and show promise for audio. We use various CNN architectures to\nclassify the soundtracks of a dataset of 70M training videos (5.24 million\nhours) with 30,871 video-level labels. We examine fully connected Deep Neural\nNetworks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We\ninvestigate varying the size of both training set and label vocabulary, finding\nthat analogs of the CNNs used in image classification do well on our audio\nclassification task, and larger training and label sets help up to a point. A\nmodel using embeddings from these classifiers does much better than raw\nfeatures on the Audio Set [5] Acoustic Event Detection (AED) classification\ntask.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 17:04:50 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 18:06:51 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Hershey", "Shawn", ""], ["Chaudhuri", "Sourish", ""], ["Ellis", "Daniel P. W.", ""], ["Gemmeke", "Jort F.", ""], ["Jansen", "Aren", ""], ["Moore", "R. Channing", ""], ["Plakal", "Manoj", ""], ["Platt", "Devin", ""], ["Saurous", "Rif A.", ""], ["Seybold", "Bryan", ""], ["Slaney", "Malcolm", ""], ["Weiss", "Ron J.", ""], ["Wilson", "Kevin", ""]]}, {"id": "1609.09444", "submitter": "Arnab Ghosh", "authors": "Arnab Ghosh and Viveka Kulharia and Amitabha Mukerjee and Vinay\n  Namboodiri and Mohit Bansal", "title": "Contextual RNN-GANs for Abstract Reasoning Diagram Generation", "comments": "To Appear in AAAI-17 and NIPS Workshop on Adversarial Training", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding, predicting, and generating object motions and transformations\nis a core problem in artificial intelligence. Modeling sequences of evolving\nimages may provide better representations and models of motion and may\nultimately be used for forecasting, simulation, or video generation.\nDiagrammatic Abstract Reasoning is an avenue in which diagrams evolve in\ncomplex patterns and one needs to infer the underlying pattern sequence and\ngenerate the next image in the sequence. For this, we develop a novel\nContextual Generative Adversarial Network based on Recurrent Neural Networks\n(Context-RNN-GANs), where both the generator and the discriminator modules are\nbased on contextual history (modeled as RNNs) and the adversarial discriminator\nguides the generator to produce realistic images for the particular time step\nin the image sequence. We evaluate the Context-RNN-GAN model (and its variants)\non a novel dataset of Diagrammatic Abstract Reasoning, where it performs\ncompetitively with 10th-grade human performance but there is still scope for\ninteresting improvements as compared to college-grade human performance. We\nalso evaluate our model on a standard video next-frame prediction task,\nachieving improved performance over comparable state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 17:56:32 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 13:14:09 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Ghosh", "Arnab", ""], ["Kulharia", "Viveka", ""], ["Mukerjee", "Amitabha", ""], ["Namboodiri", "Vinay", ""], ["Bansal", "Mohit", ""]]}, {"id": "1609.09471", "submitter": "Lovedeep Gondara", "authors": "Lovedeep Gondara", "title": "Classifier comparison using precision", "comments": "Extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New proposed models are often compared to state-of-the-art using statistical\nsignificance testing. Literature is scarce for classifier comparison using\nmetrics other than accuracy. We present a survey of statistical methods that\ncan be used for classifier comparison using precision, accounting for\ninter-precision correlation arising from use of same dataset. Comparisons are\nmade using per-class precision and methods presented to test global null\nhypothesis of an overall model comparison. Comparisons are extended to multiple\nmulti-class classifiers and to models using cross validation or its variants.\nPartial Bayesian update to precision is introduced when population prevalence\nof a class is known. Applications to compare deep architectures are studied.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 19:19:29 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 01:43:21 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Gondara", "Lovedeep", ""]]}, {"id": "1609.09475", "submitter": "Andy Zeng", "authors": "Andy Zeng, Kuan-Ting Yu, Shuran Song, Daniel Suo, Ed Walker Jr.,\n  Alberto Rodriguez and Jianxiong Xiao", "title": "Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the\n  Amazon Picking Challenge", "comments": "To appear at the International Conference on Robotics and Automation\n  (ICRA) 2017. Project webpage: http://apc.cs.princeton.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot warehouse automation has attracted significant interest in recent\nyears, perhaps most visibly in the Amazon Picking Challenge (APC). A fully\nautonomous warehouse pick-and-place system requires robust vision that reliably\nrecognizes and locates objects amid cluttered environments, self-occlusions,\nsensor noise, and a large variety of objects. In this paper we present an\napproach that leverages multi-view RGB-D data and self-supervised, data-driven\nlearning to overcome those difficulties. The approach was part of the\nMIT-Princeton Team system that took 3rd- and 4th- place in the stowing and\npicking tasks, respectively at APC 2016. In the proposed approach, we segment\nand label multiple views of a scene with a fully convolutional neural network,\nand then fit pre-scanned 3D object models to the resulting segmentation to get\nthe 6D object pose. Training a deep neural network for segmentation typically\nrequires a large amount of training data. We propose a self-supervised method\nto generate a large labeled dataset without tedious manual segmentation. We\ndemonstrate that our system can reliably estimate the 6D pose of objects under\na variety of scenarios. All code, data, and benchmarks are available at\nhttp://apc.cs.princeton.edu/\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 19:39:13 GMT"}, {"version": "v2", "created": "Sun, 2 Oct 2016 00:24:29 GMT"}, {"version": "v3", "created": "Sun, 7 May 2017 20:12:55 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Zeng", "Andy", ""], ["Yu", "Kuan-Ting", ""], ["Song", "Shuran", ""], ["Suo", "Daniel", ""], ["Walker", "Ed", "Jr."], ["Rodriguez", "Alberto", ""], ["Xiao", "Jianxiong", ""]]}, {"id": "1609.09481", "submitter": "Vu Dinh", "authors": "Vu Dinh, Lam Si Tung Ho, Duy Nguyen, Binh T. Nguyen", "title": "Fast learning rates with heavy-tailed losses", "comments": "Advances in Neural Information Processing Systems (NIPS 2016): 11\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study fast learning rates when the losses are not necessarily bounded and\nmay have a distribution with heavy tails. To enable such analyses, we introduce\ntwo new conditions: (i) the envelope function $\\sup_{f \\in \\mathcal{F}}|\\ell\n\\circ f|$, where $\\ell$ is the loss function and $\\mathcal{F}$ is the\nhypothesis class, exists and is $L^r$-integrable, and (ii) $\\ell$ satisfies the\nmulti-scale Bernstein's condition on $\\mathcal{F}$. Under these assumptions, we\nprove that learning rate faster than $O(n^{-1/2})$ can be obtained and,\ndepending on $r$ and the multi-scale Bernstein's powers, can be arbitrarily\nclose to $O(n^{-1})$. We then verify these assumptions and derive fast learning\nrates for the problem of vector quantization by $k$-means clustering with\nheavy-tailed distributions. The analyses enable us to obtain novel learning\nrates that extend and complement existing results in the literature from both\ntheoretical and practical viewpoints.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 19:46:13 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Dinh", "Vu", ""], ["Ho", "Lam Si Tung", ""], ["Nguyen", "Duy", ""], ["Nguyen", "Binh T.", ""]]}, {"id": "1609.09519", "submitter": "James Hook", "authors": "James Hook", "title": "Max-plus statistical leverage scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical leverage scores of a complex matrix $A\\in\\mathbb{C}^{n\\times\nd}$ record the degree of alignment between col$(A)$ and the coordinate axes in\n$\\mathbb{C}^n$. These score are used in random sampling algorithms for solving\ncertain numerical linear algebra problems. In this paper we present a max-plus\nalgebraic analogue for statistical leverage scores. We show that max-plus\nstatistical leverage scores can be used to calculate the exact asymptotic\nbehavior of the conventional statistical leverage scores of a generic matrices\nof Puiseux series and also provide a novel way to approximate the conventional\nstatistical leverage scores of a fixed or complex matrix. The advantage of\napproximating a complex matrices scores with max-plus scores is that the\nmax-plus scores can be computed very quickly. This approximation is typically\naccurate to within an order or magnitude and should be useful in practical\nproblems where the true scores are known to vary widely.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 20:31:10 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Hook", "James", ""]]}, {"id": "1609.09522", "submitter": "Armen Aghajanyan", "authors": "Armen Aghajanyan", "title": "Charged Point Normalization: An Efficient Solution to the Saddle Point\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the problem of local minima in very high dimensional non-convex\noptimization has been challenged and the problem of saddle points has been\nintroduced. This paper introduces a dynamic type of normalization that forces\nthe system to escape saddle points. Unlike other saddle point escaping\nalgorithms, second order information is not utilized, and the system can be\ntrained with an arbitrary gradient descent learner. The system drastically\nimproves learning in a range of deep neural networks on various data-sets in\ncomparison to non-CPN neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 20:43:21 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 08:29:40 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Aghajanyan", "Armen", ""]]}, {"id": "1609.09525", "submitter": "Yoann Isaac", "authors": "Yoann Isaac, Quentin Barth\\'elemy, C\\'edric Gouy-Pailler, Mich\\`ele\n  Sebag, Jamal Atif", "title": "Multi-dimensional signal approximation with sparse structured priors\n  using split Bregman iterations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the structurally-constrained sparse decomposition of\nmulti-dimensional signals onto overcomplete families of vectors, called\ndictionaries. The contribution of the paper is threefold. Firstly, a generic\nspatio-temporal regularization term is designed and used together with the\nstandard $\\ell_1$ regularization term to enforce a sparse decomposition\npreserving the spatio-temporal structure of the signal. Secondly, an\noptimization algorithm based on the split Bregman approach is proposed to\nhandle the associated optimization problem, and its convergence is analyzed.\nOur well-founded approach yields same accuracy as the other algorithms at the\nstate-of-the-art, with significant gains in terms of convergence speed.\nThirdly, the empirical validation of the approach on artificial and real-world\nproblems demonstrates the generality and effectiveness of the method. On\nartificial problems, the proposed regularization subsumes the Total Variation\nminimization and recovers the expected decomposition. On the real-world problem\nof electro-encephalography brainwave decomposition, the approach outperforms\nsimilar approaches in terms of P300 evoked potentials detection, using\nstructured spatial priors to guide the decomposition.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 20:50:16 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Isaac", "Yoann", ""], ["Barth\u00e9lemy", "Quentin", ""], ["Gouy-Pailler", "C\u00e9dric", ""], ["Sebag", "Mich\u00e8le", ""], ["Atif", "Jamal", ""]]}, {"id": "1609.09544", "submitter": "Shuchin Aeron", "authors": "Josh Girson and Shuchin Aeron", "title": "Algorithms for item categorization based on ordinal ranking data", "comments": "To appear in IEEE Allerton conference on computing, communications\n  and control, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for identifying the latent categorization of items\nbased on their rankings. Complimenting a recent work that uses a Dirichlet\nprior on preference vectors and variational inference, we show that this\nproblem can be effectively dealt with using existing community detection\nalgorithms, with the communities corresponding to item categories. In\nparticular we convert the bipartite ranking data to a unipartite graph of item\naffinities, and apply community detection algorithms. In this context we modify\nan existing algorithm - namely the label propagation algorithm to a variant\nthat uses the distance between the nodes for weighting the label propagation -\nto identify the categories. We propose and analyze a synthetic ordinal ranking\nmodel and show its relation to the recently much studied stochastic block\nmodel. We test our algorithms on synthetic data and compare performance with\nseveral popular community detection algorithms. We also test the method on real\ndata sets of movie categorization from the Movie Lens database. In all of the\ncases our algorithm is able to identify the categories for a suitable choice of\ntuning parameter.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 22:59:45 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Girson", "Josh", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1609.09563", "submitter": "Ming Yan", "authors": "Inci M. Baytas and Ming Yan and Anil K. Jain and Jiayu Zhou", "title": "Asynchronous Multi-Task Learning", "comments": "IEEE International Conference on Data Mining (ICDM) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world machine learning applications involve several learning tasks\nwhich are inter-related. For example, in healthcare domain, we need to learn a\npredictive model of a certain disease for many hospitals. The models for each\nhospital may be different because of the inherent differences in the\ndistributions of the patient populations. However, the models are also closely\nrelated because of the nature of the learning tasks modeling the same disease.\nBy simultaneously learning all the tasks, multi-task learning (MTL) paradigm\nperforms inductive knowledge transfer among tasks to improve the generalization\nperformance. When datasets for the learning tasks are stored at different\nlocations, it may not always be feasible to transfer the data to provide a\ndata-centralized computing environment due to various practical issues such as\nhigh data volume and privacy. In this paper, we propose a principled MTL\nframework for distributed and asynchronous optimization to address the\naforementioned challenges. In our framework, gradient update does not wait for\ncollecting the gradient information from all the tasks. Therefore, the proposed\nmethod is very efficient when the communication delay is too high for some task\nnodes. We show that many regularized MTL formulations can benefit from this\nframework, including the low-rank MTL for shared subspace learning. Empirical\nstudies on both synthetic and real-world datasets demonstrate the efficiency\nand effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 01:23:15 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Baytas", "Inci M.", ""], ["Yan", "Ming", ""], ["Jain", "Anil K.", ""], ["Zhou", "Jiayu", ""]]}, {"id": "1609.09597", "submitter": "Xing Zhang", "authors": "Xing Zhang, Zhenglei Yi, Zhi Yan, Geyong Min, Wenbo Wang, Sabita\n  Maharjan, Yan Zhang", "title": "Social Computing for Mobile Big Data in Wireless Networks", "comments": "8 papges, 3 figures, 1 tables", "journal-ref": "Computer, vol.49, no. 9, pp. 86-90, Sept. 2016", "doi": "10.1109/MC.2016.267", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Mobile big data contains vast statistical features in various dimensions,\nincluding spatial, temporal, and the underlying social domain. Understanding\nand exploiting the features of mobile data from a social network perspective\nwill be extremely beneficial to wireless networks, from planning, operation,\nand maintenance to optimization and marketing. In this paper, we categorize and\nanalyze the big data collected from real wireless cellular networks. Then, we\nstudy the social characteristics of mobile big data and highlight several\nresearch directions for mobile big data in the social computing areas.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 05:20:24 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Zhang", "Xing", ""], ["Yi", "Zhenglei", ""], ["Yan", "Zhi", ""], ["Min", "Geyong", ""], ["Wang", "Wenbo", ""], ["Maharjan", "Sabita", ""], ["Zhang", "Yan", ""]]}, {"id": "1609.09619", "submitter": "Philippe Besse", "authors": "Philippe Besse (IMT), Brendan Guillouet (IMT), Jean-Michel Loubes\n  (IMT)", "title": "Big Data analytics. Three use cases with R, Python and Spark", "comments": "in French, Apprentissage Statistique et Donn{\\'e}es Massives,\n  Technip, 2017, Journ{\\'e}es d'Etudes en Statistisque", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Management and analysis of big data are systematically associated with a data\ndistributed architecture in the Hadoop and now Spark frameworks. This article\noffers an introduction for statisticians to these technologies by comparing the\nperformance obtained by the direct use of three reference environments: R,\nPython Scikit-learn, Spark MLlib on three public use cases: character\nrecognition, recommending films, categorizing products. As main result, it\nappears that, if Spark is very efficient for data munging and recommendation by\ncollaborative filtering (non-negative factorization), current implementations\nof conventional learning methods (logistic regression, random forests) in MLlib\nor SparkML do not ou poorly compete habitual use of these methods (R, Python\nScikit-learn) in an integrated or undistributed architecture\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 07:35:49 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Besse", "Philippe", "", "IMT"], ["Guillouet", "Brendan", "", "IMT"], ["Loubes", "Jean-Michel", "", "IMT"]]}, {"id": "1609.09660", "submitter": "Junyang Jin", "authors": "J. Jin, Y. Yuan, W. Pan, D. L.T. Pham, C. J. Tomlin, A.Webb, J.\n  Goncalves", "title": "On Identification of Sparse Multivariable ARX Model: A Sparse Bayesian\n  Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper begins with considering the identification of sparse linear\ntime-invariant networks described by multivariable ARX models. Such models\npossess relatively simple structure thus used as a benchmark to promote further\nresearch. With identifiability of the network guaranteed, this paper presents\nan identification method that infers both the Boolean structure of the network\nand the internal dynamics between nodes. Identification is performed directly\nfrom data without any prior knowledge of the system, including its order. The\nproposed method solves the identification problem using Maximum a posteriori\nestimation (MAP) but with inseparable penalties for complexity, both in terms\nof element (order of nonzero connections) and group sparsity (network\ntopology). Such an approach is widely applied in Compressive Sensing (CS) and\nknown as Sparse Bayesian Learning (SBL). We then propose a novel scheme that\ncombines sparse Bayesian and group sparse Bayesian to efficiently solve the\nproblem. The resulted algorithm has a similar form of the standard Sparse Group\nLasso (SGL) while with known noise variance, it simplifies to exact re-weighted\nSGL. The method and the developed toolbox can be applied to infer networks from\na wide range of fields, including systems biology applications such as\nsignaling and genetic regulatory networks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 10:17:47 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Jin", "J.", ""], ["Yuan", "Y.", ""], ["Pan", "W.", ""], ["Pham", "D. L. T.", ""], ["Tomlin", "C. J.", ""], ["Webb", "A.", ""], ["Goncalves", "J.", ""]]}, {"id": "1609.09681", "submitter": "Emmanuel Dauc\\'e", "authors": "Emmanuel Dauc\\'e", "title": "Predicting the consequence of action in digital control state spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this dissertation is to shed light on some fundamental\nimpediments in learning control laws in continuous state spaces. In particular,\nif one wants to build artificial devices capable to learn motor tasks the same\nway they learn to classify signals and images, one needs to establish control\nrules that do not necessitate comparisons between quantities of the surrounding\nspace. We propose, in that context, to take inspiration from the \"end effector\ncontrol\" principle, as suggested by neuroscience studies, as opposed to the\n\"displacement control\" principle used in the classical control theory.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 11:52:00 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Dauc\u00e9", "Emmanuel", ""]]}, {"id": "1609.09799", "submitter": "Remi Flamary", "authors": "R\\'emi Flamary, C\\'edric F\\'evotte, Nicolas Courty, Valentin Emiya", "title": "Optimal spectral transportation with application to music transcription", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many spectral unmixing methods rely on the non-negative decomposition of\nspectral data onto a dictionary of spectral templates. In particular,\nstate-of-the-art music transcription systems decompose the spectrogram of the\ninput signal onto a dictionary of representative note spectra. The typical\nmeasures of fit used to quantify the adequacy of the decomposition compare the\ndata and template entries frequency-wise. As such, small displacements of\nenergy from a frequency bin to another as well as variations of timber can\ndisproportionally harm the fit. We address these issues by means of optimal\ntransportation and propose a new measure of fit that treats the frequency\ndistributions of energy holistically as opposed to frequency-wise. Building on\nthe harmonic nature of sound, the new measure is invariant to shifts of energy\nto harmonically-related frequencies, as well as to small and local\ndisplacements of energy. Equipped with this new measure of fit, the dictionary\nof note templates can be considerably simplified to a set of Dirac vectors\nlocated at the target fundamental frequencies (musical pitch values). This in\nturns gives ground to a very fast and simple decomposition algorithm that\nachieves state-of-the-art performance on real musical data.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 16:28:12 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 06:42:07 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Flamary", "R\u00e9mi", ""], ["F\u00e9votte", "C\u00e9dric", ""], ["Courty", "Nicolas", ""], ["Emiya", "Valentin", ""]]}, {"id": "1609.09823", "submitter": "Ravi  Tandon", "authors": "Mohamed Attia and Ravi Tandon", "title": "On the Worst-case Communication Overhead for Distributed Data Shuffling", "comments": "To appear in Allerton 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed learning platforms for processing large scale data-sets are\nbecoming increasingly prevalent. In typical distributed implementations, a\ncentralized master node breaks the data-set into smaller batches for parallel\nprocessing across distributed workers to achieve speed-up and efficiency.\nSeveral computational tasks are of sequential nature, and involve multiple\npasses over the data. At each iteration over the data, it is common practice to\nrandomly re-shuffle the data at the master node, assigning different batches\nfor each worker to process. This random re-shuffling operation comes at the\ncost of extra communication overhead, since at each shuffle, new data points\nneed to be delivered to the distributed workers.\n  In this paper, we focus on characterizing the information theoretically\noptimal communication overhead for the distributed data shuffling problem. We\npropose a novel coded data delivery scheme for the case of no excess storage,\nwhere every worker can only store the assigned data batches under processing.\nOur scheme exploits a new type of coding opportunity and is applicable to any\narbitrary shuffle, and for any number of workers. We also present an\ninformation theoretic lower bound on the minimum communication overhead for\ndata shuffling, and show that the proposed scheme matches this lower bound for\nthe worst-case communication overhead.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 17:23:03 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Attia", "Mohamed", ""], ["Tandon", "Ravi", ""]]}, {"id": "1609.09869", "submitter": "Rahul Gopal Krishnan", "authors": "Rahul G. Krishnan, Uri Shalit, David Sontag", "title": "Structured Inference Networks for Nonlinear State Space Models", "comments": "To appear in the Thirty-First AAAI Conference on Artificial\n  Intelligence, February 2017, 13 pages, 11 figures with supplement, changed to\n  AAAI formatting style, added references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian state space models have been used for decades as generative models\nof sequential data. They admit an intuitive probabilistic interpretation, have\na simple functional form, and enjoy widespread adoption. We introduce a unified\nalgorithm to efficiently learn a broad class of linear and non-linear state\nspace models, including variants where the emission and transition\ndistributions are modeled by deep neural networks. Our learning algorithm\nsimultaneously learns a compiled inference network and the generative model,\nleveraging a structured variational approximation parameterized by recurrent\nneural networks to mimic the posterior distribution. We apply the learning\nalgorithm to both synthetic and real-world datasets, demonstrating its\nscalability and versatility. We find that using the structured approximation to\nthe posterior results in models with significantly higher held-out likelihood.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 19:53:11 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 19:10:10 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Krishnan", "Rahul G.", ""], ["Shalit", "Uri", ""], ["Sontag", "David", ""]]}]