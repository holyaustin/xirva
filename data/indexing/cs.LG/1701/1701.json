[{"id": "1701.00008", "submitter": "Daniel George", "authors": "Daniel George, E. A. Huerta", "title": "Deep Neural Networks to Enable Real-time Multimessenger Astrophysics", "comments": "v3: Added results submitted to PRD on October 18, 2017; incorporated\n  suggestions from the community", "journal-ref": "Phys. Rev. D 97, 044039 (2018)", "doi": "10.1103/PhysRevD.97.044039", "report-no": null, "categories": "astro-ph.IM astro-ph.GA astro-ph.HE cs.LG gr-qc", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gravitational wave astronomy has set in motion a scientific revolution. To\nfurther enhance the science reach of this emergent field, there is a pressing\nneed to increase the depth and speed of the gravitational wave algorithms that\nhave enabled these groundbreaking discoveries. To contribute to this effort, we\nintroduce Deep Filtering, a new highly scalable method for end-to-end\ntime-series signal processing, based on a system of two deep convolutional\nneural networks, which we designed for classification and regression to rapidly\ndetect and estimate parameters of signals in highly noisy time-series data\nstreams. We demonstrate a novel training scheme with gradually increasing noise\nlevels, and a transfer learning procedure between the two networks. We showcase\nthe application of this method for the detection and parameter estimation of\ngravitational waves from binary black hole mergers. Our results indicate that\nDeep Filtering significantly outperforms conventional machine learning\ntechniques, achieves similar performance compared to matched-filtering while\nbeing several orders of magnitude faster thus allowing real-time processing of\nraw big data with minimal resources. More importantly, Deep Filtering extends\nthe range of gravitational wave signals that can be detected with ground-based\ngravitational wave detectors. This framework leverages recent advances in\nartificial intelligence algorithms and emerging hardware architectures, such as\ndeep-learning-optimized GPUs, to facilitate real-time searches of gravitational\nwave sources and their electromagnetic and astro-particle counterparts.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 21:00:02 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 18:56:50 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 18:50:10 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["George", "Daniel", ""], ["Huerta", "E. A.", ""]]}, {"id": "1701.00160", "submitter": "Ian Goodfellow", "authors": "Ian Goodfellow", "title": "NIPS 2016 Tutorial: Generative Adversarial Networks", "comments": "v2-v4 are all typo fixes. No substantive changes relative to v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report summarizes the tutorial presented by the author at NIPS 2016 on\ngenerative adversarial networks (GANs). The tutorial describes: (1) Why\ngenerative modeling is a topic worth studying, (2) how generative models work,\nand how GANs compare to other generative models, (3) the details of how GANs\nwork, (4) research frontiers in GANs, and (5) state-of-the-art image models\nthat combine GANs with other methods. Finally, the tutorial contains three\nexercises for readers to complete, and the solutions to these exercises.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 19:17:17 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2017 19:44:59 GMT"}, {"version": "v3", "created": "Mon, 9 Jan 2017 21:55:45 GMT"}, {"version": "v4", "created": "Mon, 3 Apr 2017 21:57:48 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Goodfellow", "Ian", ""]]}, {"id": "1701.00167", "submitter": "David Picard", "authors": "David Picard", "title": "Very Fast Kernel SVM under Budget Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper we propose a fast online Kernel SVM algorithm under tight\nbudget constraints. We propose to split the input space using LVQ and train a\nKernel SVM in each cluster. To allow for online training, we propose to limit\nthe size of the support vector set of each cluster using different strategies.\nWe show in the experiment that our algorithm is able to achieve high accuracy\nwhile having a very high number of samples processed per second both in\ntraining and in the evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 21:17:08 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Picard", "David", ""]]}, {"id": "1701.00178", "submitter": "Jan-Peter Calliess", "authors": "Jan-Peter Calliess", "title": "Lazily Adapted Constant Kinky Inference for Nonparametric Regression and\n  Model-Reference Adaptive Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques known as Nonlinear Set Membership prediction, Lipschitz\nInterpolation or Kinky Inference are approaches to machine learning that\nutilise presupposed Lipschitz properties to compute inferences over unobserved\nfunction values. Provided a bound on the true best Lipschitz constant of the\ntarget function is known a priori they offer convergence guarantees as well as\nbounds around the predictions. Considering a more general setting that builds\non Hoelder continuity relative to pseudo-metrics, we propose an online method\nfor estimating the Hoelder constant online from function value observations\nthat possibly are corrupted by bounded observational errors. Utilising this to\ncompute adaptive parameters within a kinky inference rule gives rise to a\nnonparametric machine learning method, for which we establish strong universal\napproximation guarantees. That is, we show that our prediction rule can learn\nany continuous function in the limit of increasingly dense data to within a\nworst-case error bound that depends on the level of observational uncertainty.\nWe apply our method in the context of nonparametric model-reference adaptive\ncontrol (MRAC). Across a range of simulated aircraft roll-dynamics and\nperformance metrics our approach outperforms recently proposed alternatives\nthat were based on Gaussian processes and RBF-neural networks. For\ndiscrete-time systems, we provide guarantees on the tracking success of our\nlearning-based controllers both for the batch and the online learning setting.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 23:25:59 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 15:36:08 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 20:14:45 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Calliess", "Jan-Peter", ""]]}, {"id": "1701.00220", "submitter": "Asaf Shabtai", "authors": "Andrey Finkelstein, Ron Biton, Rami Puzis, Asaf Shabtai", "title": "Classification of Smartphone Users Using Internet Traffic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, smartphone devices are owned by a large portion of the population and\nhave become a very popular platform for accessing the Internet. Smartphones\nprovide the user with immediate access to information and services. However,\nthey can easily expose the user to many privacy risks. Applications that are\ninstalled on the device and entities with access to the device's Internet\ntraffic can reveal private information about the smartphone user and steal\nsensitive content stored on the device or transmitted by the device over the\nInternet. In this paper, we present a method to reveal various demographics and\ntechnical computer skills of smartphone users by their Internet traffic\nrecords, using machine learning classification models. We implement and\nevaluate the method on real life data of smartphone users and show that\nsmartphone users can be classified by their gender, smoking habits, software\nprogramming experience, and other characteristics.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 08:12:49 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Finkelstein", "Andrey", ""], ["Biton", "Ron", ""], ["Puzis", "Rami", ""], ["Shabtai", "Asaf", ""]]}, {"id": "1701.00251", "submitter": "Jiashi Feng", "authors": "Jiashi Feng, Huan Xu, Shie Mannor", "title": "Outlier Robust Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning from noisy data in practical settings\nwhere the size of data is too large to store on a single machine. More\nchallenging, the data coming from the wild may contain malicious outliers. To\naddress the scalability and robustness issues, we present an online robust\nlearning (ORL) approach. ORL is simple to implement and has provable robustness\nguarantee -- in stark contrast to existing online learning approaches that are\ngenerally fragile to outliers. We specialize the ORL approach for two concrete\ncases: online robust principal component analysis and online linear regression.\nWe demonstrate the efficiency and robustness advantages of ORL through\ncomprehensive simulations and predicting image tags on a large-scale data set.\nWe also discuss extension of the ORL to distributed learning and provide\nexperimental evaluations.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 15:18:13 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Feng", "Jiashi", ""], ["Xu", "Huan", ""], ["Mannor", "Shie", ""]]}, {"id": "1701.00299", "submitter": "Lanlan Liu", "authors": "Lanlan Liu, Jia Deng", "title": "Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs\n  by Selective Execution", "comments": "fixed typos; updated CIFAR-10 results and added more details;\n  corrected the cascade D2NN configuration details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Dynamic Deep Neural Networks (D2NN), a new type of feed-forward\ndeep neural network that allows selective execution. Given an input, only a\nsubset of D2NN neurons are executed, and the particular subset is determined by\nthe D2NN itself. By pruning unnecessary computation depending on input, D2NNs\nprovide a way to improve computational efficiency. To achieve dynamic selective\nexecution, a D2NN augments a feed-forward deep neural network (directed acyclic\ngraph of differentiable modules) with controller modules. Each controller\nmodule is a sub-network whose output is a decision that controls whether other\nmodules can execute. A D2NN is trained end to end. Both regular and controller\nmodules in a D2NN are learnable and are jointly trained to optimize both\naccuracy and efficiency. Such training is achieved by integrating\nbackpropagation with reinforcement learning. With extensive experiments of\nvarious D2NN architectures on image classification tasks, we demonstrate that\nD2NNs are general and flexible, and can effectively optimize\naccuracy-efficiency trade-offs.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 00:09:14 GMT"}, {"version": "v2", "created": "Fri, 8 Dec 2017 02:22:10 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 02:03:00 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Liu", "Lanlan", ""], ["Deng", "Jia", ""]]}, {"id": "1701.00485", "submitter": "Wenjia Meng", "authors": "Wenjia Meng, Zonghua Gu, Ming Zhang, Zhaohui Wu", "title": "Two-Bit Networks for Deep Learning on Resource-Constrained Embedded\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 04:28:16 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 13:54:51 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Meng", "Wenjia", ""], ["Gu", "Zonghua", ""], ["Zhang", "Ming", ""], ["Wu", "Zhaohui", ""]]}, {"id": "1701.00573", "submitter": "Gonzalo Otazu", "authors": "Gonzalo H Otazu", "title": "Robust method for finding sparse solutions to linear inverse problems\n  using an L2 regularization", "comments": "13 pages, 6 figures. Code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyzed the performance of a biologically inspired algorithm called the\nCorrected Projections Algorithm (CPA) when a sparseness constraint is required\nto unambiguously reconstruct an observed signal using atoms from an\novercomplete dictionary. By changing the geometry of the estimation problem,\nCPA gives an analytical expression for a binary variable that indicates the\npresence or absence of a dictionary atom using an L2 regularizer. The\nregularized solution can be implemented using an efficient real-time\nKalman-filter type of algorithm. The smoother L2 regularization of CPA makes it\nvery robust to noise, and CPA outperforms other methods in identifying known\natoms in the presence of strong novel atoms in the signal.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 03:31:03 GMT"}, {"version": "v2", "created": "Sun, 8 Jan 2017 05:38:58 GMT"}, {"version": "v3", "created": "Wed, 22 Mar 2017 22:19:57 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Otazu", "Gonzalo H", ""]]}, {"id": "1701.00593", "submitter": "Yeeleng Vang", "authors": "Yeeleng Scott Vang and Xiaohui Xie", "title": "HLA class I binding prediction via convolutional neural networks", "comments": "12 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many biological processes are governed by protein-ligand interactions. One\nsuch example is the recognition of self and nonself cells by the immune system.\nThis immune response process is regulated by the major histocompatibility\ncomplex (MHC) protein which is encoded by the human leukocyte antigen (HLA)\ncomplex. Understanding the binding potential between MHC and peptides can lead\nto the design of more potent, peptide-based vaccines and immunotherapies for\ninfectious autoimmune diseases.\n  We apply machine learning techniques from the natural language processing\n(NLP) domain to address the task of MHC-peptide binding prediction. More\nspecifically, we introduce a new distributed representation of amino acids,\nname HLA-Vec, that can be used for a variety of downstream proteomic machine\nlearning tasks. We then propose a deep convolutional neural network\narchitecture, name HLA-CNN, for the task of HLA class I-peptide binding\nprediction. Experimental results show combining the new distributed\nrepresentation with our HLA-CNN architecture achieves state-of-the-art results\nin the majority of the latest two Immune Epitope Database (IEDB) weekly\nautomated benchmark datasets. We further apply our model to predict binding on\nthe human genome and identify 15 genes with potential for self binding.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 06:08:52 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 23:47:27 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Vang", "Yeeleng Scott", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1701.00597", "submitter": "Karamjit Singh", "authors": "Karamjit Singh, Garima Gupta, Lovekesh Vig, Gautam Shroff, and Puneet\n  Agarwal", "title": "Deep Convolutional Neural Networks for Pairwise Causality", "comments": "Published at NIPS 2016 Workshop \"What If\" and Won the best Poster\n  Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering causal models from observational and interventional data is an\nimportant first step preceding what-if analysis or counterfactual reasoning. As\nhas been shown before, the direction of pairwise causal relations can, under\ncertain conditions, be inferred from observational data via standard\ngradient-boosted classifiers (GBC) using carefully engineered statistical\nfeatures. In this paper we apply deep convolutional neural networks (CNNs) to\nthis problem by plotting attribute pairs as 2-D scatter plots that are fed to\nthe CNN as images. We evaluate our approach on the 'Cause- Effect Pairs' NIPS\n2013 Data Challenge. We observe that a weighted ensemble of CNN with the\nearlier GBC approach yields significant improvement. Further, we observe that\nwhen less training data is available, our approach performs better than the GBC\nbased approach suggesting that CNN models pre-trained to determine the\ndirection of pairwise causal direction could have wider applicability in causal\ndiscovery and enabling what-if or counterfactual analysis.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 07:07:14 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Singh", "Karamjit", ""], ["Gupta", "Garima", ""], ["Vig", "Lovekesh", ""], ["Shroff", "Gautam", ""], ["Agarwal", "Puneet", ""]]}, {"id": "1701.00609", "submitter": "Shuai Li", "authors": "Shuai Li", "title": "Akid: A Library for Neural Network Research and Production from a\n  Dataism Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are a revolutionary but immature technique that is fast\nevolving and heavily relies on data. To benefit from the newest development and\nnewly available data, we want the gap between research and production as small\nas possibly. On the other hand, differing from traditional machine learning\nmodels, neural network is not just yet another statistic model, but a model for\nthe natural processing engine --- the brain. In this work, we describe a neural\nnetwork library named {\\texttt akid}. It provides higher level of abstraction\nfor entities (abstracted as blocks) in nature upon the abstraction done on\nsignals (abstracted as tensors) by Tensorflow, characterizing the dataism\nobservation that all entities in nature processes input and emit out in some\nways. It includes a full stack of software that provides abstraction to let\nresearchers focus on research instead of implementation, while at the same time\nthe developed program can also be put into production seamlessly in a\ndistributed environment, and be production ready. At the top application stack,\nit provides out-of-box tools for neural network applications. Lower down, akid\nprovides a programming paradigm that lets user easily build customized models.\nThe distributed computing stack handles the concurrency and communication, thus\nletting models be trained or deployed to a single GPU, multiple GPUs, or a\ndistributed environment without affecting how a model is specified in the\nprogramming paradigm stack. Lastly, the distributed deployment stack handles\nhow the distributed computing is deployed, thus decoupling the research\nprototype environment with the actual production environment, and is able to\ndynamically allocate computing resources, so development (Devs) and operations\n(Ops) could be separated. Please refer to http://akid.readthedocs.io/en/latest/\nfor documentation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 09:18:22 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Li", "Shuai", ""]]}, {"id": "1701.00677", "submitter": "Ashkan Esmaeili", "authors": "Mohammad Amin Fakharian, Ashkan Esmaeili, and Farokh Marvasti", "title": "New Methods of Enhancing Prediction Accuracy in Linear Models with\n  Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, prediction for linear systems with missing information is\ninvestigated. New methods are introduced to improve the Mean Squared Error\n(MSE) on the test set in comparison to state-of-the-art methods, through\nappropriate tuning of Bias-Variance trade-off. First, the use of proposed Soft\nWeighted Prediction (SWP) algorithm and its efficacy are depicted and compared\nto previous works for non-missing scenarios. The algorithm is then modified and\noptimized for missing scenarios. It is shown that controlled over-fitting by\nsuggested algorithms will improve prediction accuracy in various cases.\nSimulation results approve our heuristics in enhancing the prediction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 12:33:53 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Fakharian", "Mohammad Amin", ""], ["Esmaeili", "Ashkan", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1701.00705", "submitter": "Ankita Mangal", "authors": "Ankita Mangal and Nishant Kumar", "title": "Using Big Data to Enhance the Bosch Production Line Performance: A\n  Kaggle Challenge", "comments": "IEEE Big Data 2016 Conference", "journal-ref": null, "doi": "10.1109/BigData.2016.7840826", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our approach to the Bosch production line performance\nchallenge run by Kaggle.com. Maximizing the production yield is at the heart of\nthe manufacturing industry. At the Bosch assembly line, data is recorded for\nproducts as they progress through each stage. Data science methods are applied\nto this huge data repository consisting records of tests and measurements made\nfor each component along the assembly line to predict internal failures. We\nfound that it is possible to train a model that predicts which parts are most\nlikely to fail. Thus a smarter failure detection system can be built and the\nparts tagged likely to fail can be salvaged to decrease operating costs and\nincrease the profit margins.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 20:40:42 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Mangal", "Ankita", ""], ["Kumar", "Nishant", ""]]}, {"id": "1701.00737", "submitter": "Morteza Ashraphijuo", "authors": "Morteza Ashraphijuo and Xiaodong Wang and Vaneet Aggarwal", "title": "Deterministic and Probabilistic Conditions for Finite Completability of\n  Low-rank Multi-View Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.AG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the multi-view data completion problem, i.e., to complete a\nmatrix $\\mathbf{U}=[\\mathbf{U}_1|\\mathbf{U}_2]$ where the ranks of\n$\\mathbf{U},\\mathbf{U}_1$, and $\\mathbf{U}_2$ are given. In particular, we\ninvestigate the fundamental conditions on the sampling pattern, i.e., locations\nof the sampled entries for finite completability of such a multi-view data\ngiven the corresponding rank constraints. In contrast with the existing\nanalysis on Grassmannian manifold for a single-view matrix, i.e., conventional\nmatrix completion, we propose a geometric analysis on the manifold structure\nfor multi-view data to incorporate more than one rank constraint. We provide a\ndeterministic necessary and sufficient condition on the sampling pattern for\nfinite completability. We also give a probabilistic condition in terms of the\nnumber of samples per column that guarantees finite completability with high\nprobability. Finally, using the developed tools, we derive the deterministic\nand probabilistic guarantees for unique completability.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 16:23:48 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 01:06:44 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Ashraphijuo", "Morteza", ""], ["Wang", "Xiaodong", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1701.00754", "submitter": "Ibrahim Ighneiwa", "authors": "Ibrahim Ighneiwaa, Salwa Hamidatoua, and Fadia Ben Ismaela", "title": "Using Artificial Neural Networks (ANN) to Control Chaos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlling Chaos could be a big factor in getting great stable amounts of\nenergy out of small amounts of not necessarily stable resources. By definition,\nChaos is getting huge changes in the system's output due to unpredictable small\nchanges in initial conditions, and that means we could take advantage of this\nfact and select the proper control system to manipulate system's initial\nconditions and inputs in general and get a desirable output out of otherwise a\nChaotic system. That was accomplished by first building some known chaotic\ncircuit (Chua circuit) and the NI's MultiSim was used to simulate the ANN\ncontrol system. It was shown that this technique can also be used to stabilize\nsome hard to stabilize electronic systems.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 05:17:58 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Ighneiwaa", "Ibrahim", ""], ["Hamidatoua", "Salwa", ""], ["Ismaela", "Fadia Ben", ""]]}, {"id": "1701.00757", "submitter": "Pedro Mercado", "authors": "Pedro Mercado, Francesco Tudisco and Matthias Hein", "title": "Clustering Signed Networks with the Geometric Mean of Laplacians", "comments": "14 pages, 5 figures. Accepted in Neural Information Processing\n  Systems (NIPS), 2016", "journal-ref": "Advances in Neural Information Processing Systems 29,\n  pp.4421--4429, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signed networks allow to model positive and negative relationships. We\nanalyze existing extensions of spectral clustering to signed networks. It turns\nout that existing approaches do not recover the ground truth clustering in\nseveral situations where either the positive or the negative network structures\ncontain no noise. Our analysis shows that these problems arise as existing\napproaches take some form of arithmetic mean of the Laplacians of the positive\nand negative part. As a solution we propose to use the geometric mean of the\nLaplacians of positive and negative part and show that it outperforms the\nexisting approaches. While the geometric mean of matrices is computationally\nexpensive, we show that eigenvectors of the geometric mean can be computed\nefficiently, leading to a numerical scheme for sparse matrices which is of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 17:42:34 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Mercado", "Pedro", ""], ["Tudisco", "Francesco", ""], ["Hein", "Matthias", ""]]}, {"id": "1701.00831", "submitter": "Alessandro Rossi", "authors": "Marco Gori, Marco Maggini, Alessandro Rossi", "title": "Collapsing of dimensionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a new approach to Machine Learning coming from a modification of\nclassical regularization networks by casting the process in the time dimension,\nleading to a sort of collapse of dimensionality in the problem of learning the\nmodel parameters. This approach allows the definition of a online learning\nalgorithm that progressively accumulates the knowledge provided in the input\ntrajectory. The regularization principle leads to a solution based on a\ndynamical system that is paired with a procedure to develop a graph structure\nthat stores the input regularities acquired from the temporal evolution. We\nreport an extensive experimental exploration on the behavior of the parameter\nof the proposed model and an evaluation on artificial dataset.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 20:54:52 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Gori", "Marco", ""], ["Maggini", "Marco", ""], ["Rossi", "Alessandro", ""]]}, {"id": "1701.00851", "submitter": "Herman Kamper", "authors": "Herman Kamper", "title": "Unsupervised neural and Bayesian models for zero-resource speech\n  processing", "comments": "PhD thesis, University of Edinburgh, 107 pages, submitted and\n  accepted 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In settings where only unlabelled speech data is available, zero-resource\nspeech technology needs to be developed without transcriptions, pronunciation\ndictionaries, or language modelling text. There are two central problems in\nzero-resource speech processing: (i) finding frame-level feature\nrepresentations which make it easier to discriminate between linguistic units\n(phones or words), and (ii) segmenting and clustering unlabelled speech into\nmeaningful units. In this thesis, we argue that a combination of top-down and\nbottom-up modelling is advantageous in tackling these two problems.\n  To address the problem of frame-level representation learning, we present the\ncorrespondence autoencoder (cAE), a neural network trained with weak top-down\nsupervision from an unsupervised term discovery system. By combining this\ntop-down supervision with unsupervised bottom-up initialization, the cAE yields\nmuch more discriminative features than previous approaches. We then present our\nunsupervised segmental Bayesian model that segments and clusters unlabelled\nspeech into hypothesized words. By imposing a consistent top-down segmentation\nwhile also using bottom-up knowledge from detected syllable boundaries, our\nsystem outperforms several others on multi-speaker conversational English and\nXitsonga speech data. Finally, we show that the clusters discovered by the\nsegmental Bayesian model can be made less speaker- and gender-specific by using\nfeatures from the cAE instead of traditional acoustic features.\n  In summary, the different models and systems presented in this thesis show\nthat both top-down and bottom-up modelling can improve representation learning,\nsegmentation and clustering of unlabelled speech data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 22:26:10 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Kamper", "Herman", ""]]}, {"id": "1701.00874", "submitter": "Xuezhe Ma", "authors": "Xuezhe Ma, Eduard Hovy", "title": "Neural Probabilistic Model for Non-projective MST Parsing", "comments": "To appear in IJCNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a probabilistic parsing model, which defines a\nproper conditional probability distribution over non-projective dependency\ntrees for a given sentence, using neural representations as inputs. The neural\nnetwork architecture is based on bi-directional LSTM-CNNs which benefits from\nboth word- and character-level representations automatically, by using\ncombination of bidirectional LSTM and CNN. On top of the neural network, we\nintroduce a probabilistic structured layer, defining a conditional log-linear\nmodel over non-projective trees. We evaluate our model on 17 different\ndatasets, across 14 different languages. By exploiting Kirchhoff's Matrix-Tree\nTheorem (Tutte, 1984), the partition functions and marginals can be computed\nefficiently, leading to a straight-forward end-to-end model training procedure\nvia back-propagation. Our parser achieves state-of-the-art parsing performance\non nine datasets.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 00:10:17 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 04:09:29 GMT"}, {"version": "v3", "created": "Thu, 13 Apr 2017 23:06:11 GMT"}, {"version": "v4", "created": "Sun, 3 Sep 2017 21:12:40 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Ma", "Xuezhe", ""], ["Hovy", "Eduard", ""]]}, {"id": "1701.00877", "submitter": "Tom Hanika", "authors": "Daniel Borchmann, Tom Hanika, Sergei Obiedkov", "title": "On the Usability of Probably Approximately Correct Implication Bases", "comments": "17 pages, 8 figures; typos added, corrected x-label on graphs", "journal-ref": null, "doi": "10.1007/978-3-319-59271-8_5", "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the notion of probably approximately correct implication bases\nfrom the literature and present a first formulation in the language of formal\nconcept analysis, with the goal to investigate whether such bases represent a\nsuitable substitute for exact implication bases in practical use-cases. To this\nend, we quantitatively examine the behavior of probably approximately correct\nimplication bases on artificial and real-world data sets and compare their\nprecision and recall with respect to their corresponding exact implication\nbases. Using a small example, we also provide qualitative insight that\nimplications from probably approximately correct bases can still represent\nmeaningful knowledge from a given data set.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 00:45:37 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2017 23:39:05 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Borchmann", "Daniel", ""], ["Hanika", "Tom", ""], ["Obiedkov", "Sergei", ""]]}, {"id": "1701.00903", "submitter": "Lakshmi Narasimhan Govindarajan", "authors": "Li Liu and Yongzhong Yang and Lakshmi Narasimhan Govindarajan and Shu\n  Wang and Bin Hu and Li Cheng and David S. Rosenblum", "title": "An Interval-Based Bayesian Generative Model for Human Complex Activity\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex activity recognition is challenging due to the inherent uncertainty\nand diversity of performing a complex activity. Normally, each instance of a\ncomplex activity has its own configuration of atomic actions and their temporal\ndependencies. We propose in this paper an atomic action-based Bayesian model\nthat constructs Allen's interval relation networks to characterize complex\nactivities with structural varieties in a probabilistic generative way: By\nintroducing latent variables from the Chinese restaurant process, our approach\nis able to capture all possible styles of a particular complex activity as a\nunique set of distributions over atomic actions and relations. We also show\nthat local temporal dependencies can be retained and are globally consistent in\nthe resulting interval network. Moreover, network structure can be learned from\nempirical data. A new dataset of complex hand activities has been constructed\nand made publicly available, which is much larger in size than any existing\ndatasets. Empirical evaluations on benchmark datasets as well as our in-house\ndataset demonstrate the competitiveness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 05:53:46 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Liu", "Li", ""], ["Yang", "Yongzhong", ""], ["Govindarajan", "Lakshmi Narasimhan", ""], ["Wang", "Shu", ""], ["Hu", "Bin", ""], ["Cheng", "Li", ""], ["Rosenblum", "David S.", ""]]}, {"id": "1701.00939", "submitter": "Dmitry Krotov", "authors": "Dmitry Krotov, John J Hopfield", "title": "Dense Associative Memory is Robust to Adversarial Inputs", "comments": null, "journal-ref": "Neural Computation Volume 30, Issue 12, December 2018 p.3151-3167", "doi": "10.1162/neco_a_01143", "report-no": null, "categories": "cs.LG cs.CR cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) trained in a supervised way suffer from two known\nproblems. First, the minima of the objective function used in learning\ncorrespond to data points (also known as rubbish examples or fooling images)\nthat lack semantic similarity with the training data. Second, a clean input can\nbe changed by a small, and often imperceptible for human vision, perturbation,\nso that the resulting deformed input is misclassified by the network. These\nfindings emphasize the differences between the ways DNN and humans classify\npatterns, and raise a question of designing learning algorithms that more\naccurately mimic human perception compared to the existing methods.\n  Our paper examines these questions within the framework of Dense Associative\nMemory (DAM) models. These models are defined by the energy function, with\nhigher order (higher than quadratic) interactions between the neurons. We show\nthat in the limit when the power of the interaction vertex in the energy\nfunction is sufficiently large, these models have the following three\nproperties. First, the minima of the objective function are free from rubbish\nimages, so that each minimum is a semantically meaningful pattern. Second,\nartificial patterns poised precisely at the decision boundary look ambiguous to\nhuman subjects and share aspects of both classes that are separated by that\ndecision boundary. Third, adversarial images constructed by models with small\npower of the interaction vertex, which are equivalent to DNN with rectified\nlinear units (ReLU), fail to transfer to and fool the models with higher order\ninteractions. This opens up a possibility to use higher order models for\ndetecting and stopping malicious adversarial attacks. The presented results\nsuggest that DAM with higher order energy functions are closer to human visual\nperception than DNN with ReLUs.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 09:40:09 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Krotov", "Dmitry", ""], ["Hopfield", "John J", ""]]}, {"id": "1701.01000", "submitter": "Tao Hong", "authors": "Tao Hong and Zhihui Zhu", "title": "Online Learning Sensing Matrix and Sparsifying Dictionary Simultaneously\n  for Compressive Sensing", "comments": "6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of simultaneously learning the Sensing\nMatrix and Sparsifying Dictionary (SMSD) on a large training dataset. To\naddress the formulated joint learning problem, we propose an online algorithm\nthat consists of a closed-form solution for optimizing the sensing matrix with\na fixed sparsifying dictionary and a stochastic method for learning the\nsparsifying dictionary on a large dataset when the sensing matrix is given.\nBenefiting from training on a large dataset, the obtained compressive sensing\n(CS) system by the proposed algorithm yields a much better performance in terms\nof signal recovery accuracy than the existing ones. The simulation results on\nnatural images demonstrate the effectiveness of the suggested online algorithm\ncompared with the existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 13:26:57 GMT"}, {"version": "v2", "created": "Sun, 28 May 2017 18:26:05 GMT"}, {"version": "v3", "created": "Tue, 7 Nov 2017 20:03:29 GMT"}, {"version": "v4", "created": "Sat, 2 Jun 2018 16:09:47 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Hong", "Tao", ""], ["Zhu", "Zhihui", ""]]}, {"id": "1701.01036", "submitter": "Yanghao Li", "authors": "Yanghao Li, Naiyan Wang, Jiaying Liu and Xiaodi Hou", "title": "Demystifying Neural Style Transfer", "comments": "Accepted by IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Style Transfer has recently demonstrated very exciting results which\ncatches eyes in both academia and industry. Despite the amazing results, the\nprinciple of neural style transfer, especially why the Gram matrices could\nrepresent style remains unclear. In this paper, we propose a novel\ninterpretation of neural style transfer by treating it as a domain adaptation\nproblem. Specifically, we theoretically show that matching the Gram matrices of\nfeature maps is equivalent to minimize the Maximum Mean Discrepancy (MMD) with\nthe second order polynomial kernel. Thus, we argue that the essence of neural\nstyle transfer is to match the feature distributions between the style images\nand the generated images. To further support our standpoint, we experiment with\nseveral other distribution alignment methods, and achieve appealing results. We\nbelieve this novel interpretation connects these two important research fields,\nand could enlighten future researches.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 14:54:20 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 13:21:11 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Li", "Yanghao", ""], ["Wang", "Naiyan", ""], ["Liu", "Jiaying", ""], ["Hou", "Xiaodi", ""]]}, {"id": "1701.01095", "submitter": "Audrey Durand", "authors": "Audrey Durand, Christian Gagn\\'e", "title": "Estimating Quality in Multi-Objective Bandits Optimization", "comments": "Submitted to ECML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications are characterized by a number of conflicting\nperformance measures. As optimizing in a multi-objective setting leads to a set\nof non-dominated solutions, a preference function is required for selecting the\nsolution with the appropriate trade-off between the objectives. The question\nis: how good do estimations of these objectives have to be in order for the\nsolution maximizing the preference function to remain unchanged? In this paper,\nwe introduce the concept of preference radius to characterize the robustness of\nthe preference function and provide guidelines for controlling the quality of\nestimations in the multi-objective setting. More specifically, we provide a\ngeneral formulation of multi-objective optimization under the bandits setting.\nWe show how the preference radius relates to the optimal gap and we use this\nconcept to provide a theoretical analysis of the Thompson sampling algorithm\nfrom multivariate normal priors. We finally present experiments to support the\ntheoretical results and highlight the fact that one cannot simply scalarize\nmulti-objective problems into single-objective problems.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 18:20:47 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 21:23:44 GMT"}, {"version": "v3", "created": "Thu, 20 Apr 2017 20:37:39 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Durand", "Audrey", ""], ["Gagn\u00e9", "Christian", ""]]}, {"id": "1701.01218", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny and Ahmed Elgammal", "title": "Overlapping Cover Local Regression Machines", "comments": "Long Article with more experiments and analysis of conference paper\n  \"Overlapping Domain Cover for Scalable and Accurate Regression Kernel\n  Machines\", presented orally 2015 at the British Machine Vision Conference\n  2015 (BMVC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Overlapping Domain Cover (ODC) notion for kernel machines, as\na set of overlapping subsets of the data that covers the entire training set\nand optimized to be spatially cohesive as possible. We show how this notion\nbenefit the speed of local kernel machines for regression in terms of both\nspeed while achieving while minimizing the prediction error. We propose an\nefficient ODC framework, which is applicable to various regression models and\nin particular reduces the complexity of Twin Gaussian Processes (TGP)\nregression from cubic to quadratic. Our notion is also applicable to several\nkernel methods (e.g., Gaussian Process Regression(GPR) and IWTGP regression, as\nshown in our experiments). We also theoretically justified the idea behind our\nmethod to improve local prediction by the overlapping cover. We validated and\nanalyzed our method on three benchmark human pose estimation datasets and\ninteresting findings are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 06:04:53 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1701.01293", "submitter": "Giuseppe Casalicchio", "authors": "Giuseppe Casalicchio, Jakob Bossek, Michel Lang, Dominik Kirchhoff,\n  Pascal Kerschke, Benjamin Hofner, Heidi Seibold, Joaquin Vanschoren, Bernd\n  Bischl", "title": "OpenML: An R Package to Connect to the Machine Learning Platform OpenML", "comments": null, "journal-ref": "Computational Statistics, 2019, 34. Jg., Nr. 3, S. 977-991", "doi": "10.1007/s00180-017-0742-2", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenML is an online machine learning platform where researchers can easily\nshare data, machine learning tasks and experiments as well as organize them\nonline to work and collaborate more efficiently. In this paper, we present an R\npackage to interface with the OpenML platform and illustrate its usage in\ncombination with the machine learning R package mlr. We show how the OpenML\npackage allows R users to easily search, download and upload data sets and\nmachine learning tasks. Furthermore, we also show how to upload results of\nexperiments, share them with others and download results from other users.\nBeyond ensuring reproducibility of results, the OpenML platform automates much\nof the drudge work, speeds up research, facilitates collaboration and increases\nthe users' visibility online.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 12:33:19 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 07:03:28 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Casalicchio", "Giuseppe", ""], ["Bossek", "Jakob", ""], ["Lang", "Michel", ""], ["Kirchhoff", "Dominik", ""], ["Kerschke", "Pascal", ""], ["Hofner", "Benjamin", ""], ["Seibold", "Heidi", ""], ["Vanschoren", "Joaquin", ""], ["Bischl", "Bernd", ""]]}, {"id": "1701.01302", "submitter": "Andrew Critch PhD", "authors": "Andrew Critch", "title": "Toward negotiable reinforcement learning: shifting priorities in Pareto\n  optimal sequential decision-making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing multi-objective reinforcement learning (MORL) algorithms do not\naccount for objectives that arise from players with differing beliefs.\nConcretely, consider two players with different beliefs and utility functions\nwho may cooperate to build a machine that takes actions on their behalf. A\nrepresentation is needed for how much the machine's policy will prioritize each\nplayer's interests over time. Assuming the players have reached common\nknowledge of their situation, this paper derives a recursion that any Pareto\noptimal policy must satisfy. Two qualitative observations can be made from the\nrecursion: the machine must (1) use each player's own beliefs in evaluating how\nwell an action will serve that player's utility function, and (2) shift the\nrelative priority it assigns to each player's expected utilities over time, by\na factor proportional to how well that player's beliefs predict the machine's\ninputs. Observation (2) represents a substantial divergence from na\\\"{i}ve\nlinear utility aggregation (as in Harsanyi's utilitarian theorem, and existing\nMORL algorithms), which is shown here to be inadequate for Pareto optimal\nsequential decision-making on behalf of players with different beliefs.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 13:00:05 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 16:06:30 GMT"}, {"version": "v3", "created": "Sat, 13 May 2017 08:33:46 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Critch", "Andrew", ""]]}, {"id": "1701.01325", "submitter": "Ramakrishnan Kannan", "authors": "Ramakrishnan Kannan, Hyenkyun Woo, Charu C. Aggarwal, Haesun Park", "title": "Outlier Detection for Text Data : An Extended Version", "comments": "Accepted at 2017 SIAM Data Mining Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of outlier detection is extremely challenging in many domains\nsuch as text, in which the attribute values are typically non-negative, and\nmost values are zero. In such cases, it often becomes difficult to separate the\noutliers from the natural variations in the patterns in the underlying data. In\nthis paper, we present a matrix factorization method, which is naturally able\nto distinguish the anomalies with the use of low rank approximations of the\nunderlying data. Our iterative algorithm TONMF is based on block coordinate\ndescent (BCD) framework. We define blocks over the term-document matrix such\nthat the function becomes solvable. Given most recently updated values of other\nmatrix blocks, we always update one block at a time to its optimal. Our\napproach has significant advantages over traditional methods for text outlier\ndetection. Finally, we present experimental results illustrating the\neffectiveness of our method over competing methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 14:14:52 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Kannan", "Ramakrishnan", ""], ["Woo", "Hyenkyun", ""], ["Aggarwal", "Charu C.", ""], ["Park", "Haesun", ""]]}, {"id": "1701.01329", "submitter": "Marwin Segler", "authors": "Marwin H.S. Segler, Thierry Kogej, Christian Tyrchan, Mark P. Waller", "title": "Generating Focussed Molecule Libraries for Drug Discovery with Recurrent\n  Neural Networks", "comments": "17 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG physics.chem-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In de novo drug design, computational strategies are used to generate novel\nmolecules with good affinity to the desired biological target. In this work, we\nshow that recurrent neural networks can be trained as generative models for\nmolecular structures, similar to statistical language models in natural\nlanguage processing. We demonstrate that the properties of the generated\nmolecules correlate very well with the properties of the molecules used to\ntrain the model. In order to enrich libraries with molecules active towards a\ngiven biological target, we propose to fine-tune the model with small sets of\nmolecules, which are known to be active against that target.\n  Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test\nmolecules that medicinal chemists designed, whereas against Plasmodium\nfalciparum (Malaria) it reproduced 28% of 1240 test molecules. When coupled\nwith a scoring function, our model can perform the complete de novo drug design\ncycle to generate large sets of novel molecules for drug discovery.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 14:28:34 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Segler", "Marwin H. S.", ""], ["Kogej", "Thierry", ""], ["Tyrchan", "Christian", ""], ["Waller", "Mark P.", ""]]}, {"id": "1701.01358", "submitter": "Huan Liu Huan Liu", "authors": "Hongjun Lu and Rudy Setiono and Huan Liu", "title": "NeuroRule: A Connectionist Approach to Data Mining", "comments": "VLDB1995", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification, which involves finding rules that partition a given data set\ninto disjoint groups, is one class of data mining problems. Approaches proposed\nso far for mining classification rules for large databases are mainly decision\ntree based symbolic learning methods. The connectionist approach based on\nneural networks has been thought not well suited for data mining. One of the\nmajor reasons cited is that knowledge generated by neural networks is not\nexplicitly represented in the form of rules suitable for verification or\ninterpretation by humans. This paper examines this issue. With our newly\ndeveloped algorithms, rules which are similar to, or more concise than those\ngenerated by the symbolic methods can be extracted from the neural networks.\nThe data mining process using neural networks with the emphasis on rule\nextraction is described. Experimental results and comparison with previously\npublished works are presented.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 15:40:44 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Lu", "Hongjun", ""], ["Setiono", "Rudy", ""], ["Liu", "Huan", ""]]}, {"id": "1701.01394", "submitter": "Andrew Knyazev", "authors": "Andrew V. Knyazev", "title": "On spectral partitioning of signed graphs", "comments": "12 pages, 10 figures. Rev 2 to appear in proceedings of the SIAM\n  Workshop on Combinatorial Scientific Computing 2018 (CSC18)", "journal-ref": null, "doi": null, "report-no": "Rev. 1 MERL TR2017-001", "categories": "cs.DS cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that the standard graph Laplacian is preferable for spectral\npartitioning of signed graphs compared to the signed Laplacian. Simple examples\ndemonstrate that partitioning based on signs of components of the leading\neigenvectors of the signed Laplacian may be meaningless, in contrast to\npartitioning based on the Fiedler vector of the standard graph Laplacian for\nsigned graphs. We observe that negative eigenvalues are beneficial for spectral\npartitioning of signed graphs, making the Fiedler vector easier to compute.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 17:31:16 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 00:51:22 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Knyazev", "Andrew V.", ""]]}, {"id": "1701.01497", "submitter": "Joris Gu\\'erin", "authors": "Joris Guerin, Olivier Gibaru, Eric Nyiri and Stephane Thiery", "title": "Learning local trajectories for high precision robotic tasks :\n  application to KUKA LBR iiwa Cartesian positioning", "comments": "6 pages, double column, 6 figures and one table. Published in:\n  Industrial Electronics Society , IECON 2016 - 42nd Annual Conference of the\n  IEEE", "journal-ref": "Industrial Electronics Society, IECON 2016-42nd Annual Conference\n  of the IEEE Pages 5316--5321", "doi": "10.1109/IECON.2016.7793388", "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To ease the development of robot learning in industry, two conditions need to\nbe fulfilled. Manipulators must be able to learn high accuracy and precision\ntasks while being safe for workers in the factory. In this paper, we extend\npreviously submitted work which consists in rapid learning of local high\naccuracy behaviors. By exploration and regression, linear and quadratic models\nare learnt for respectively the dynamics and cost function. Iterative Linear\nQuadratic Gaussian Regulator combined with cost quadratic regression can\nconverge rapidly in the final stages towards high accuracy behavior as the cost\nfunction is modelled quite precisely. In this paper, both a different cost\nfunction and a second order improvement method are implemented within this\nframework. We also propose an analysis of the algorithm parameters through\nsimulation for a positioning task. Finally, an experimental validation on a\nKUKA LBR iiwa robot is carried out. This collaborative robot manipulator can be\neasily programmed into safety mode, which makes it qualified for the second\nindustry constraint stated above.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 23:01:08 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Guerin", "Joris", ""], ["Gibaru", "Olivier", ""], ["Nyiri", "Eric", ""], ["Thiery", "Stephane", ""]]}, {"id": "1701.01722", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu and Yuanzhi Li", "title": "Follow the Compressed Leader: Faster Online Learning of Eigenvectors and\n  Faster MMWU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The online problem of computing the top eigenvector is fundamental to machine\nlearning. In both adversarial and stochastic settings, previous results (such\nas matrix multiplicative weight update, follow the regularized leader, follow\nthe compressed leader, block power method) either achieve optimal regret but\nrun slow, or run fast at the expense of loosing a $\\sqrt{d}$ factor in total\nregret where $d$ is the matrix dimension.\n  We propose a $\\textit{follow-the-compressed-leader (FTCL)}$ framework which\nachieves optimal regret without sacrificing the running time. Our idea is to\n\"compress\" the matrix strategy to dimension 3 in the adversarial setting, or\ndimension 1 in the stochastic setting. These respectively resolve two open\nquestions regarding the design of optimal and efficient algorithms for the\nonline eigenvector problem.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 18:43:53 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 18:18:21 GMT"}, {"version": "v3", "created": "Mon, 18 Sep 2017 01:04:08 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1701.01887", "submitter": "John Cristian Borges Gamboa", "authors": "John Cristian Borges Gamboa", "title": "Deep Learning for Time-Series Analysis", "comments": "Written as part of the Seminar on Collaborative Intelligence in the\n  TU Kaiserslautern. January 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world application, e.g., speech recognition or sleep stage\nclassification, data are captured over the course of time, constituting a\nTime-Series. Time-Series often contain temporal dependencies that cause two\notherwise identical points of time to belong to different classes or predict\ndifferent behavior. This characteristic generally increases the difficulty of\nanalysing them. Existing techniques often depended on hand-crafted features\nthat were expensive to create and required expert knowledge of the field. With\nthe advent of Deep Learning new models of unsupervised learning of features for\nTime-series analysis and forecast have been developed. Such new developments\nare the topic of this paper: a review of the main Deep Learning techniques is\npresented, and some applications on Time-Series analysis are summaried. The\nresults make it clear that Deep Learning has a lot to contribute to the field.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2017 21:44:04 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Gamboa", "John Cristian Borges", ""]]}, {"id": "1701.01917", "submitter": "Lina Zhu", "authors": "Xun Zhou, Changle Li, Zhe Liu, Tom H. Luan, Zhifang Miao, Lina Zhu and\n  Lei Xiong", "title": "See the Near Future: A Short-Term Predictive Methodology to Traffic Load\n  in ITS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Intelligent Transportation System (ITS) targets to a coordinated traffic\nsystem by applying the advanced wireless communication technologies for road\ntraffic scheduling. Towards an accurate road traffic control, the short-term\ntraffic forecasting to predict the road traffic at the particular site in a\nshort period is often useful and important. In existing works, Seasonal\nAutoregressive Integrated Moving Average (SARIMA) model is a popular approach.\nThe scheme however encounters two challenges: 1) the analysis on related data\nis insufficient whereas some important features of data may be neglected; and\n2) with data presenting different features, it is unlikely to have one\npredictive model that can fit all situations. To tackle above issues, in this\nwork, we develop a hybrid model to improve accuracy of SARIMA. In specific, we\nfirst explore the autocorrelation and distribution features existed in traffic\nflow to revise structure of the time series model. Based on the Gaussian\ndistribution of traffic flow, a hybrid model with a Bayesian learning algorithm\nis developed which can effectively expand the application scenarios of SARIMA.\nWe show the efficiency and accuracy of our proposal using both analysis and\nexperimental studies. Using the real-world trace data, we show that the\nproposed predicting approach can achieve satisfactory performance in practice.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 06:11:34 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Zhou", "Xun", ""], ["Li", "Changle", ""], ["Liu", "Zhe", ""], ["Luan", "Tom H.", ""], ["Miao", "Zhifang", ""], ["Zhu", "Lina", ""], ["Xiong", "Lei", ""]]}, {"id": "1701.02026", "submitter": "Peter Bloem", "authors": "Peter Bloem and Steven de Rooij", "title": "Large-scale network motif analysis using compression", "comments": null, "journal-ref": null, "doi": "10.1007/s10618-020-00691-y", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new method for finding network motifs: interesting or\ninformative subgraph patterns in a network. Subgraphs are motifs when their\nfrequency in the data is high compared to the expected frequency under a null\nmodel. To compute this expectation, a full or approximate count of the\noccurrences of a motif is normally repeated on as many as 1000 random graphs\nsampled from the null model; a prohibitively expensive step. We use ideas from\nthe Minimum Description Length (MDL) literature to define a new measure of\nmotif relevance. With our method, samples from the null model are not required.\nInstead we compute the probability of the data under the null model and compare\nthis to the probability under a specially designed alternative model. With this\nnew relevance test, we can search for motifs by random sampling, rather than\nrequiring an accurate count of all instances of a motif. This allows motif\nanalysis to scale to networks with billions of links.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 22:25:04 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 22:12:45 GMT"}, {"version": "v3", "created": "Sat, 18 May 2019 15:10:29 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Bloem", "Peter", ""], ["de Rooij", "Steven", ""]]}, {"id": "1701.02046", "submitter": "Ping Li", "authors": "Ping Li", "title": "Tunable GMM Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed \"generalized min-max\" (GMM) kernel can be efficiently\nlinearized, with direct applications in large-scale statistical learning and\nfast near neighbor search. The linearized GMM kernel was extensively compared\nin with linearized radial basis function (RBF) kernel. On a large number of\nclassification tasks, the tuning-free GMM kernel performs (surprisingly) well\ncompared to the best-tuned RBF kernel. Nevertheless, one would naturally expect\nthat the GMM kernel ought to be further improved if we introduce tuning\nparameters.\n  In this paper, we study three simple constructions of tunable GMM kernels:\n(i) the exponentiated-GMM (or eGMM) kernel, (ii) the powered-GMM (or pGMM)\nkernel, and (iii) the exponentiated-powered-GMM (epGMM) kernel. The pGMM kernel\ncan still be efficiently linearized by modifying the original hashing procedure\nfor the GMM kernel. On about 60 publicly available classification datasets, we\nverify that the proposed tunable GMM kernels typically improve over the\noriginal GMM kernel. On some datasets, the improvements can be astonishingly\nsignificant.\n  For example, on 11 popular datasets which were used for testing deep learning\nalgorithms and tree methods, our experiments show that the proposed tunable GMM\nkernels are strong competitors to trees and deep nets. The previous studies\ndeveloped tree methods including \"abc-robust-logitboost\" and demonstrated the\nexcellent performance on those 11 datasets (and other datasets), by\nestablishing the second-order tree-split formula and new derivatives for\nmulti-class logistic loss. Compared to tree methods like\n\"abc-robust-logitboost\" (which are slow and need substantial model sizes), the\ntunable GMM kernels produce largely comparable results.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 01:20:55 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 17:25:16 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1701.02058", "submitter": "Mehmet Basbug", "authors": "Mehmet E. Basbug, Barbara E. Engelhardt", "title": "Coupled Compound Poisson Factorization", "comments": "Under review at AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a general framework, the coupled compound Poisson factorization\n(CCPF), to capture the missing-data mechanism in extremely sparse data sets by\ncoupling a hierarchical Poisson factorization with an arbitrary data-generating\nmodel. We derive a stochastic variational inference algorithm for the resulting\nmodel and, as examples of our framework, implement three different\ndata-generating models---a mixture model, linear regression, and factor\nanalysis---to robustly model non-random missing data in the context of\nclustering, prediction, and matrix factorization. In all three cases, we test\nour framework against models that ignore the missing-data mechanism on large\nscale studies with non-random missing data, and we show that explicitly\nmodeling the missing-data mechanism substantially improves the quality of the\nresults, as measured using data log likelihood on a held-out test set.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 03:49:26 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Basbug", "Mehmet E.", ""], ["Engelhardt", "Barbara E.", ""]]}, {"id": "1701.02133", "submitter": "Michele Svanera", "authors": "Michele Svanera, Sergio Benini, Gal Raz, Talma Hendler, Rainer Goebel,\n  and Giancarlo Valente", "title": "Deep driven fMRI decoding of visual categories", "comments": "Presented at MLINI-2016 workshop, 2016 (arXiv:1701.01437)", "journal-ref": null, "doi": null, "report-no": "MLINI/2016/01", "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been developed drawing inspiration from the brain\nvisual pathway, implementing an end-to-end approach: from image data to video\nobject classes. However building an fMRI decoder with the typical structure of\nConvolutional Neural Network (CNN), i.e. learning multiple level of\nrepresentations, seems impractical due to lack of brain data. As a possible\nsolution, this work presents the first hybrid fMRI and deep features decoding\napproach: collected fMRI and deep learnt representations of video object\nclasses are linked together by means of Kernel Canonical Correlation Analysis.\nIn decoding, this allows exploiting the discriminatory power of CNN by relating\nthe fMRI representation to the last layer of CNN (fc7). We show the\neffectiveness of embedding fMRI data onto a subspace related to deep features\nin distinguishing semantic visual categories based solely on brain imaging\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 11:06:39 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Svanera", "Michele", ""], ["Benini", "Sergio", ""], ["Raz", "Gal", ""], ["Hendler", "Talma", ""], ["Goebel", "Rainer", ""], ["Valente", "Giancarlo", ""]]}, {"id": "1701.02145", "submitter": "Elike Hodo Mr", "authors": "Elike Hodo, Xavier Bellekens, Andrew Hamilton, Christos Tachtatzis and\n  Robert Atkinson", "title": "Shallow and Deep Networks Intrusion Detection System: A Taxonomy and\n  Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrusion detection has attracted a considerable interest from researchers\nand industries. The community, after many years of research, still faces the\nproblem of building reliable and efficient IDS that are capable of handling\nlarge quantities of data, with changing patterns in real time situations. The\nwork presented in this manuscript classifies intrusion detection systems (IDS).\nMoreover, a taxonomy and survey of shallow and deep networks intrusion\ndetection systems is presented based on previous and current works. This\ntaxonomy and survey reviews machine learning techniques and their performance\nin detecting anomalies. Feature selection which influences the effectiveness of\nmachine learning (ML) IDS is discussed to explain the role of feature selection\nin the classification and training phase of ML IDS. Finally, a discussion of\nthe false and true positive alarm rates is presented to help researchers model\nreliable and efficient machine learning based intrusion detection systems.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 11:46:58 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Hodo", "Elike", ""], ["Bellekens", "Xavier", ""], ["Hamilton", "Andrew", ""], ["Tachtatzis", "Christos", ""], ["Atkinson", "Robert", ""]]}, {"id": "1701.02284", "submitter": "Tian Zhao", "authors": "Tian Zhao, Xiaobing Huang, Yu Cao", "title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Deep Learning (DL) has found great success in domains such\nas multimedia understanding. However, the complex nature of multimedia data\nmakes it difficult to develop DL-based software. The state-of-the art tools,\nsuch as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their\napplicable domains, are programming libraries with fixed user interface,\ninternal representation, and execution environment. This makes it difficult to\nimplement portable and customized DL applications.\n  In this paper, we present DeepDSL, a domain specific language (DSL) embedded\nin Scala, that compiles deep networks written in DeepDSL to Java source code.\nDeep DSL provides (1) intuitive constructs to support compact encoding of deep\nnetworks; (2) symbolic gradient derivation of the networks; (3) static analysis\nfor memory consumption and error detection; and (4) DSL-level optimization to\nimprove memory and runtime efficiency.\n  DeepDSL programs are compiled into compact, efficient, customizable, and\nportable Java source code, which operates the CUDA and CUDNN interfaces running\non Nvidia GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL\nwith a number of popular DL networks. Our experiments show that the compiled\nprograms have very competitive runtime performance and memory efficiency\ncompared to the existing libraries.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 18:02:13 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Zhao", "Tian", ""], ["Huang", "Xiaobing", ""], ["Cao", "Yu", ""]]}, {"id": "1701.02291", "submitter": "Tapabrata Ghosh", "authors": "Tapabrata Ghosh", "title": "QuickNet: Maximizing Efficiency and Efficacy in Deep Architectures", "comments": "Updated once", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present QuickNet, a fast and accurate network architecture that is both\nfaster and significantly more accurate than other fast deep architectures like\nSqueezeNet. Furthermore, it uses less parameters than previous networks, making\nit more memory efficient. We do this by making two major modifications to the\nreference Darknet model (Redmon et al, 2015): 1) The use of depthwise separable\nconvolutions and 2) The use of parametric rectified linear units. We make the\nobservation that parametric rectified linear units are computationally\nequivalent to leaky rectified linear units at test time and the observation\nthat separable convolutions can be interpreted as a compressed Inception\nnetwork (Chollet, 2016). Using these observations, we derive a network\narchitecture, which we call QuickNet, that is both faster and more accurate\nthan previous models. Our architecture provides at least four major advantages:\n(1) A smaller model size, which is more tenable on memory constrained systems;\n(2) A significantly faster network which is more tenable on computationally\nconstrained systems; (3) A high accuracy of 95.7 percent on the CIFAR-10\nDataset which outperforms all but one result published so far, although we note\nthat our works are orthogonal approaches and can be combined (4) Orthogonality\nto previous model compression approaches allowing for further speed gains to be\nrealized.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 18:29:07 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 07:44:17 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Ghosh", "Tapabrata", ""]]}, {"id": "1701.02302", "submitter": "Greg Yang", "authors": "Greg Yang", "title": "A Homological Theory of Functions", "comments": "72 pages, 22 figures. Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AC cs.CC cs.DM cs.LG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computational complexity, a complexity class is given by a set of problems\nor functions, and a basic challenge is to show separations of complexity\nclasses $A \\not= B$ especially when $A$ is known to be a subset of $B$. In this\npaper we introduce a homological theory of functions that can be used to\nestablish complexity separations, while also providing other interesting\nconsequences. We propose to associate a topological space $S_A$ to each class\nof functions $A$, such that, to separate complexity classes $A \\subseteq B'$,\nit suffices to observe a change in \"the number of holes\", i.e. homology, in\n$S_A$ as a subclass $B$ of $B'$ is added to $A$. In other words, if the\nhomologies of $S_A$ and $S_{A \\cup B}$ are different, then $A \\not= B'$. We\ndevelop the underlying theory of functions based on combinatorial and\nhomological commutative algebra and Stanley-Reisner theory, and recover Minsky\nand Papert's 1969 result that parity cannot be computed by nonmaximal degree\npolynomial threshold functions. In the process, we derive a \"maximal principle\"\nfor polynomial threshold functions that is used to extend this result further\nto arbitrary symmetric functions. A surprising coincidence is demonstrated,\nwhere the maximal dimension of \"holes\" in $S_A$ upper bounds the VC dimension\nof $A$, with equality for common computational cases such as the class of\npolynomial threshold functions or the class of linear functionals in $\\mathbb\nF_2$, or common algebraic cases such as when the Stanley-Reisner ring of $S_A$\nis Cohen-Macaulay. As another interesting application of our theory, we prove a\nresult that a priori has nothing to do with complexity separation: it\ncharacterizes when a vector subspace intersects the positive cone, in terms of\nhomological conditions. By analogy to Farkas' result doing the same with\n*linear conditions*, we call our theorem the Homological Farkas Lemma.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 18:58:27 GMT"}, {"version": "v2", "created": "Sat, 14 Jan 2017 19:09:32 GMT"}, {"version": "v3", "created": "Thu, 6 Apr 2017 07:30:15 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Yang", "Greg", ""]]}, {"id": "1701.02377", "submitter": "Alessandro Rossi", "authors": "Marco Gori, Marco Maggini, Alessandro Rossi", "title": "The principle of cognitive action - Preliminary experimental analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this document we shows a first implementation and some preliminary results\nof a new theory, facing Machine Learning problems in the frameworks of\nClassical Mechanics and Variational Calculus. We give a general formulation of\nthe problem and then we studies basic behaviors of the model on simple\npractical implementations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 22:29:08 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Gori", "Marco", ""], ["Maggini", "Marco", ""], ["Rossi", "Alessandro", ""]]}, {"id": "1701.02386", "submitter": "Ilya Tolstikhin", "authors": "Ilya Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann\n  Simon-Gabriel and Bernhard Sch\\\"olkopf", "title": "AdaGAN: Boosting Generative Models", "comments": "Updated with MNIST pictures and discussions + Unrolled GAN\n  experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) are an\neffective method for training generative models of complex data such as natural\nimages. However, they are notoriously hard to train and can suffer from the\nproblem of missing modes where the model is not able to produce examples in\ncertain regions of the space. We propose an iterative procedure, called AdaGAN,\nwhere at every step we add a new component into a mixture model by running a\nGAN algorithm on a reweighted sample. This is inspired by boosting algorithms,\nwhere many potentially weak individual predictors are greedily aggregated to\nform a strong composite predictor. We prove that such an incremental procedure\nleads to convergence to the true distribution in a finite number of steps if\neach step is optimal, and convergence at an exponential rate otherwise. We also\nillustrate experimentally that this procedure addresses the problem of missing\nmodes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 23:19:28 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 11:45:00 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Tolstikhin", "Ilya", ""], ["Gelly", "Sylvain", ""], ["Bousquet", "Olivier", ""], ["Simon-Gabriel", "Carl-Johann", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1701.02392", "submitter": "Tanmay Shankar", "authors": "Tanmay Shankar, Santosha K. Dwivedy, Prithwijit Guha", "title": "Reinforcement Learning via Recurrent Convolutional Neural Networks", "comments": "Accepted at the International Conference on Pattern Recognition, ICPR\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Reinforcement Learning has enabled the learning of policies for complex\ntasks in partially observable environments, without explicitly learning the\nunderlying model of the tasks. While such model-free methods achieve\nconsiderable performance, they often ignore the structure of task. We present a\nnatural representation of to Reinforcement Learning (RL) problems using\nRecurrent Convolutional Neural Networks (RCNNs), to better exploit this\ninherent structure. We define 3 such RCNNs, whose forward passes execute an\nefficient Value Iteration, propagate beliefs of state in partially observable\nenvironments, and choose optimal actions respectively. Backpropagating\ngradients through these RCNNs allows the system to explicitly learn the\nTransition Model and Reward Function associated with the underlying MDP,\nserving as an elegant alternative to classical model-based RL. We evaluate the\nproposed algorithms in simulation, considering a robot planning problem. We\ndemonstrate the capability of our framework to reduce the cost of replanning,\nlearn accurate MDP models, and finally re-plan with learnt models to achieve\nnear-optimal policies.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 23:36:05 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Shankar", "Tanmay", ""], ["Dwivedy", "Santosha K.", ""], ["Guha", "Prithwijit", ""]]}, {"id": "1701.02440", "submitter": "Maziar Raissi", "authors": "Maziar Raissi and George Em. Karniadakis", "title": "Machine Learning of Linear Differential Equations using Gaussian\n  Processes", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2017.07.050", "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work leverages recent advances in probabilistic machine learning to\ndiscover conservation laws expressed by parametric linear equations. Such\nequations involve, but are not limited to, ordinary and partial differential,\nintegro-differential, and fractional order operators. Here, Gaussian process\npriors are modified according to the particular form of such operators and are\nemployed to infer parameters of the linear equations from scarce and possibly\nnoisy observations. Such observations may come from experiments or \"black-box\"\ncomputer simulations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 05:14:22 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Raissi", "Maziar", ""], ["Karniadakis", "George Em.", ""]]}, {"id": "1701.02477", "submitter": "Abhinav Thanda", "authors": "Abhinav Thanda, Shankar M Venkatesan", "title": "Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic\n  Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) involves the simultaneous training of two or more\nrelated tasks over shared representations. In this work, we apply MTL to\naudio-visual automatic speech recognition(AV-ASR). Our primary task is to learn\na mapping between audio-visual fused features and frame labels obtained from\nacoustic GMM/HMM model. This is combined with an auxiliary task which maps\nvisual features to frame labels obtained from a separate visual GMM/HMM model.\nThe MTL model is tested at various levels of babble noise and the results are\ncompared with a base-line hybrid DNN-HMM AV-ASR model. Our results indicate\nthat MTL is especially useful at higher level of noise. Compared to base-line,\nupto 7\\% relative improvement in WER is reported at -3 SNR dB\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 08:47:56 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Thanda", "Abhinav", ""], ["Venkatesan", "Shankar M", ""]]}, {"id": "1701.02481", "submitter": "Yang Xu", "authors": "Yang Xu and Jiawei Liu", "title": "Implicitly Incorporating Morphological Information into Word Embedding", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose three novel models to enhance word embedding by\nimplicitly using morphological information. Experiments on word similarity and\nsyntactic analogy show that the implicit models are superior to traditional\nexplicit ones. Our models outperform all state-of-the-art baselines and\nsignificantly improve the performance on both tasks. Moreover, our performance\non the smallest corpus is similar to the performance of CBOW on the corpus\nwhich is five times the size of ours. Parameter analysis indicates that the\nimplicit models can supplement semantic information during the word embedding\ntraining process.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 08:59:38 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 01:35:53 GMT"}, {"version": "v3", "created": "Mon, 8 May 2017 03:19:20 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Xu", "Yang", ""], ["Liu", "Jiawei", ""]]}, {"id": "1701.02490", "submitter": "Han Cai", "authors": "Han Cai, Kan Ren, Weinan Zhang, Kleanthis Malialis, Jun Wang, Yong Yu,\n  Defeng Guo", "title": "Real-Time Bidding by Reinforcement Learning in Display Advertising", "comments": "WSDM 2017", "journal-ref": null, "doi": "10.1145/3018661.3018702", "report-no": null, "categories": "cs.LG cs.AI cs.GT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The majority of online display ads are served through real-time bidding (RTB)\n--- each ad display impression is auctioned off in real-time when it is just\nbeing generated from a user visit. To place an ad automatically and optimally,\nit is critical for advertisers to devise a learning algorithm to cleverly bid\nan ad impression in real-time. Most previous works consider the bid decision as\na static optimization problem of either treating the value of each impression\nindependently or setting a bid price to each segment of ad volume. However, the\nbidding for a given ad campaign would repeatedly happen during its life span\nbefore the budget runs out. As such, each bid is strategically correlated by\nthe constrained budget and the overall effectiveness of the campaign (e.g., the\nrewards from generated clicks), which is only observed after the campaign has\ncompleted. Thus, it is of great interest to devise an optimal bidding strategy\nsequentially so that the campaign budget can be dynamically allocated across\nall the available impressions on the basis of both the immediate and future\nrewards. In this paper, we formulate the bid decision process as a\nreinforcement learning problem, where the state space is represented by the\nauction information and the campaign's real-time parameters, while an action is\nthe bid price to set. By modeling the state transition via auction competition,\nwe build a Markov Decision Process framework for learning the optimal bidding\npolicy to optimize the advertising performance in the dynamic real-time bidding\nenvironment. Furthermore, the scalability problem from the large real-world\nauction volume and campaign budget is well handled by state value approximation\nusing neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 09:30:29 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 01:37:39 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Cai", "Han", ""], ["Ren", "Kan", ""], ["Zhang", "Weinan", ""], ["Malialis", "Kleanthis", ""], ["Wang", "Jun", ""], ["Yu", "Yong", ""], ["Guo", "Defeng", ""]]}, {"id": "1701.02511", "submitter": "Feng Liu", "authors": "Feng Liu, Guanquan Zhang, Jie Lu", "title": "Heterogeneous domain adaptation: An unsupervised approach", "comments": "This paper has been accepted by IEEE transactions on neural networks\n  and learning systems (TNNLS)", "journal-ref": null, "doi": "10.1109/TNNLS.2020.2973293", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation leverages the knowledge in one domain - the source domain -\nto improve learning efficiency in another domain - the target domain. Existing\nheterogeneous domain adaptation research is relatively well-progressed, but\nonly in situations where the target domain contains at least a few labeled\ninstances. In contrast, heterogeneous domain adaptation with an unlabeled\ntarget domain has not been well-studied. To contribute to the research in this\nemerging field, this paper presents: (1) an unsupervised knowledge transfer\ntheorem that guarantees the correctness of transferring knowledge; and (2) a\nprincipal angle-based metric to measure the distance between two pairs of\ndomains: one pair comprises the original source and target domains and the\nother pair comprises two homogeneous representations of two domains. The\ntheorem and the metric have been implemented in an innovative transfer model,\ncalled a Grassmann-Linear monotonic maps-geodesic flow kernel (GLG), that is\nspecifically designed for heterogeneous unsupervised domain adaptation (HeUDA).\nThe linear monotonic maps meet the conditions of the theorem and are used to\nconstruct homogeneous representations of the heterogeneous domains. The metric\nshows the extent to which the homogeneous representations have preserved the\ninformation in the original source and target domains. By minimizing the\nproposed metric, the GLG model learns the homogeneous representations of\nheterogeneous domains and transfers knowledge through these learned\nrepresentations via a geodesic flow kernel. To evaluate the model, five public\ndatasets were reorganized into ten HeUDA tasks across three applications:\ncancer detection, credit assessment, and text classification. The experiments\ndemonstrate that the proposed model delivers superior performance over the\nexisting baselines.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 10:42:25 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 03:22:17 GMT"}, {"version": "v3", "created": "Tue, 2 Jan 2018 23:04:57 GMT"}, {"version": "v4", "created": "Thu, 4 Oct 2018 23:54:18 GMT"}, {"version": "v5", "created": "Mon, 10 Feb 2020 01:31:57 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Liu", "Feng", ""], ["Zhang", "Guanquan", ""], ["Lu", "Jie", ""]]}, {"id": "1701.02676", "submitter": "Hao Dong", "authors": "Hao Dong, Paarth Neekhara, Chao Wu, Yike Guo", "title": "Unsupervised Image-to-Image Translation with Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It's useful to automatically transform an image from its original form to\nsome synthetic form (style, partial contents, etc.), while keeping the original\nstructure or semantics. We define this requirement as the \"image-to-image\ntranslation\" problem, and propose a general approach to achieve it, based on\ndeep convolutional and conditional generative adversarial networks (GANs),\nwhich has gained a phenomenal success to learn mapping images from noise input\nsince 2014. In this work, we develop a two step (unsupervised) learning method\nto translate images between different domains by using unlabeled images without\nspecifying any correspondence between them, so that to avoid the cost of\nacquiring labeled data. Compared with prior works, we demonstrated the capacity\nof generality in our model, by which variance of translations can be conduct by\na single type of model. Such capability is desirable in applications like\nbidirectional translation\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 16:43:03 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Dong", "Hao", ""], ["Neekhara", "Paarth", ""], ["Wu", "Chao", ""], ["Guo", "Yike", ""]]}, {"id": "1701.02720", "submitter": "Mohammad Pezeshki", "authors": "Ying Zhang, Mohammad Pezeshki, Philemon Brakel, Saizheng Zhang, Cesar\n  Laurent Yoshua Bengio, Aaron Courville", "title": "Towards End-to-End Speech Recognition with Deep Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are effective models for reducing\nspectral variations and modeling spectral correlations in acoustic features for\nautomatic speech recognition (ASR). Hybrid speech recognition systems\nincorporating CNNs with Hidden Markov Models/Gaussian Mixture Models\n(HMMs/GMMs) have achieved the state-of-the-art in various benchmarks.\nMeanwhile, Connectionist Temporal Classification (CTC) with Recurrent Neural\nNetworks (RNNs), which is proposed for labeling unsegmented sequences, makes it\nfeasible to train an end-to-end speech recognition system instead of hybrid\nsettings. However, RNNs are computationally expensive and sometimes difficult\nto train. In this paper, inspired by the advantages of both CNNs and the CTC\napproach, we propose an end-to-end speech framework for sequence labeling, by\ncombining hierarchical CNNs with CTC directly without recurrent connections. By\nevaluating the approach on the TIMIT phoneme recognition task, we show that the\nproposed model is not only computationally efficient, but also competitive with\nthe existing baseline systems. Moreover, we argue that CNNs have the capability\nto model temporal correlations with appropriate context information.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 18:30:11 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Zhang", "Ying", ""], ["Pezeshki", "Mohammad", ""], ["Brakel", "Philemon", ""], ["Zhang", "Saizheng", ""], ["Bengio", "Cesar Laurent Yoshua", ""], ["Courville", "Aaron", ""]]}, {"id": "1701.02789", "submitter": "Rajat Sen", "authors": "Rajat Sen, Karthikeyan Shanmugam, Alexandros G. Dimakis, and Sanjay\n  Shakkottai", "title": "Identifying Best Interventions through Online Importance Sampling", "comments": "30 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in computational advertising and systems biology,\nwe consider the problem of identifying the best out of several possible soft\ninterventions at a source node $V$ in an acyclic causal directed graph, to\nmaximize the expected value of a target node $Y$ (located downstream of $V$).\nOur setting imposes a fixed total budget for sampling under various\ninterventions, along with cost constraints on different types of interventions.\nWe pose this as a best arm identification bandit problem with $K$ arms where\neach arm is a soft intervention at $V,$ and leverage the information leakage\namong the arms to provide the first gap dependent error and simple regret\nbounds for this problem. Our results are a significant improvement over the\ntraditional best arm identification results. We empirically show that our\nalgorithms outperform the state of the art in the Flow Cytometry data-set, and\nalso apply our algorithm for model interpretation of the Inception-v3 deep net\nthat classifies images.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 21:26:03 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 01:19:33 GMT"}, {"version": "v3", "created": "Thu, 9 Mar 2017 22:50:38 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Sen", "Rajat", ""], ["Shanmugam", "Karthikeyan", ""], ["Dimakis", "Alexandros G.", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "1701.02804", "submitter": "Kristjan Greenewald", "authors": "Kristjan Greenewald, Stephen Kelley, Brandon Oselio, Alfred O. Hero\n  III", "title": "Similarity Function Tracking using Pairwise Comparisons", "comments": "submitted to IEEE transactions on signal processing. arXiv admin\n  note: substantial text overlap with arXiv:1610.03090, arXiv:1603.03678", "journal-ref": null, "doi": "10.1109/TSP.2017.2739100", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in distance metric learning has focused on learning\ntransformations of data that best align with specified pairwise similarity and\ndissimilarity constraints, often supplied by a human observer. The learned\ntransformations lead to improved retrieval, classification, and clustering\nalgorithms due to the better adapted distance or similarity measures. Here, we\naddress the problem of learning these transformations when the underlying\nconstraint generation process is nonstationary. This nonstationarity can be due\nto changes in either the ground-truth clustering used to generate constraints\nor changes in the feature subspaces in which the class structure is apparent.\nWe propose Online Convex Ensemble StrongLy Adaptive Dynamic Learning (OCELAD),\na general adaptive, online approach for learning and tracking optimal metrics\nas they change over time that is highly robust to a variety of nonstationary\nbehaviors in the changing metric. We apply the OCELAD framework to an ensemble\nof online learners. Specifically, we create a retro-initialized composite\nobjective mirror descent (COMID) ensemble (RICE) consisting of a set of\nparallel COMID learners with different learning rates, and demonstrate\nparameter-free RICE-OCELAD metric learning on both synthetic data and a highly\nnonstationary Twitter dataset. We show significant performance improvements and\nincreased robustness to nonstationary effects relative to previously proposed\nbatch and online distance metric learning algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2017 03:44:54 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Greenewald", "Kristjan", ""], ["Kelley", "Stephen", ""], ["Oselio", "Brandon", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1701.02815", "submitter": "Bo Dai", "authors": "Bo Dai, Ruiqi Guo, Sanjiv Kumar, Niao He, Le Song", "title": "Stochastic Generative Hashing", "comments": "21 pages, 40 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based binary hashing has become a powerful paradigm for fast search\nand retrieval in massive databases. However, due to the requirement of discrete\noutputs for the hash functions, learning such functions is known to be very\nchallenging. In addition, the objective functions adopted by existing hashing\ntechniques are mostly chosen heuristically. In this paper, we propose a novel\ngenerative approach to learn hash functions through Minimum Description Length\nprinciple such that the learned hash codes maximally compress the dataset and\ncan also be used to regenerate the inputs. We also develop an efficient\nlearning algorithm based on the stochastic distributional gradient, which\navoids the notorious difficulty caused by binary output constraints, to jointly\noptimize the parameters of the hash function and the associated generative\nmodel. Extensive experiments on a variety of large-scale datasets show that the\nproposed method achieves better retrieval results than the existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 00:23:34 GMT"}, {"version": "v2", "created": "Sat, 12 Aug 2017 21:36:09 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Dai", "Bo", ""], ["Guo", "Ruiqi", ""], ["Kumar", "Sanjiv", ""], ["He", "Niao", ""], ["Song", "Le", ""]]}, {"id": "1701.02886", "submitter": "Edouard Pauwels", "authors": "Jean-Bernard Lasserre and Edouard Pauwels", "title": "The empirical Christoffel function with applications in data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We illustrate the potential applications in machine learning of the\nChristoffel function, or more precisely, its empirical counterpart associated\nwith a counting measure uniformly supported on a finite set of points. Firstly,\nwe provide a thresholding scheme which allows to approximate the support of a\nmeasure from a finite subset of its moments with strong asymptotic guaranties.\nSecondly, we provide a consistency result which relates the empirical\nChristoffel function and its population counterpart in the limit of large\nsamples. Finally, we illustrate the relevance of our results on simulated and\nreal world datasets for several applications in statistics and machine\nlearning: (a) density and support estimation from finite samples, (b) outlier\nand novelty detection and (c) affine matching.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 08:36:54 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 08:05:15 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 13:29:04 GMT"}, {"version": "v4", "created": "Thu, 7 Feb 2019 08:26:12 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Lasserre", "Jean-Bernard", ""], ["Pauwels", "Edouard", ""]]}, {"id": "1701.02892", "submitter": "Xiaowei Zhang", "authors": "Xiaowei Zhang and Chi Xu and Yu Zhang and Tingshao Zhu and Li Cheng", "title": "Multivariate Regression with Grossly Corrupted Observations: A Robust\n  Approach and its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of multivariate linear regression where a\nportion of the observations is grossly corrupted or is missing, and the\nmagnitudes and locations of such occurrences are unknown in priori. To deal\nwith this problem, we propose a new approach by explicitly consider the error\nsource as well as its sparseness nature. An interesting property of our\napproach lies in its ability of allowing individual regression output elements\nor tasks to possess their unique noise levels. Moreover, despite working with a\nnon-smooth optimization problem, our approach still guarantees to converge to\nits optimal solution. Experiments on synthetic data demonstrate the\ncompetitiveness of our approach compared with existing multivariate regression\nmodels. In addition, empirically our approach has been validated with very\npromising results on two exemplar real-world applications: The first concerns\nthe prediction of \\textit{Big-Five} personality based on user behaviors at\nsocial network sites (SNSs), while the second is 3D human hand pose estimation\nfrom depth images. The implementation of our approach and comparison methods as\nwell as the involved datasets are made publicly available in support of the\nopen-source and reproducible research initiatives.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 08:52:53 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Zhang", "Xiaowei", ""], ["Xu", "Chi", ""], ["Zhang", "Yu", ""], ["Zhu", "Tingshao", ""], ["Cheng", "Li", ""]]}, {"id": "1701.02960", "submitter": "Johan Jonasson", "authors": "Johan Jonasson", "title": "Fast mixing for Latent Dirichlet allocation", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) algorithms are ubiquitous in probability\ntheory in general and in machine learning in particular. A Markov chain is\ndevised so that its stationary distribution is some probability distribution of\ninterest. Then one samples from the given distribution by running the Markov\nchain for a \"long time\" until it appears to be stationary and then collects the\nsample. However these chains are often very complex and there are no\ntheoretical guarantees that stationarity is actually reached. In this paper we\nstudy the Gibbs sampler of the posterior distribution of a very simple case of\nLatent Dirichlet Allocation, the arguably most well known Bayesian unsupervised\nlearning model for text generation and text classification. It is shown that\nwhen the corpus consists of two long documents of equal length $m$ and the\nvocabulary consists of only two different words, the mixing time is at most of\norder $m^2\\log m$ (which corresponds to $m\\log m$ rounds over the corpus). It\nwill be apparent from our analysis that it seems very likely that the mixing\ntime is not much worse in the more relevant case when the number of documents\nand the size of the vocabulary are also large as long as each word is\nrepresented a large number in each document, even though the computations\ninvolved may be intractable.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 13:08:52 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 14:01:34 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Jonasson", "Johan", ""]]}, {"id": "1701.03006", "submitter": "Xin Yuan", "authors": "Xin Yuan, Yunchen Pu, Lawrence Carin", "title": "Compressive Sensing via Convolutional Factor Analysis", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve the compressive sensing problem via convolutional factor analysis,\nwhere the convolutional dictionaries are learned {\\em in situ} from the\ncompressed measurements. An alternating direction method of multipliers (ADMM)\nparadigm for compressive sensing inversion based on convolutional factor\nanalysis is developed. The proposed algorithm provides reconstructed images as\nwell as features, which can be directly used for recognition ($e.g.$,\nclassification) tasks. When a deep (multilayer) model is constructed, a\nstochastic unpooling process is employed to build a generative model. During\nreconstruction and testing, we project the upper layer dictionary to the data\nlevel and only a single layer deconvolution is required. We demonstrate that\nusing $\\sim30\\%$ (relative to pixel numbers) compressed measurements, the\nproposed model achieves the classification accuracy comparable to the original\ndata on MNIST. We also observe that when the compressed measurements are very\nlimited ($e.g.$, $<10\\%$), the upper layer dictionary can provide better\nreconstruction results than the bottom layer.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 15:18:18 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Yuan", "Xin", ""], ["Pu", "Yunchen", ""], ["Carin", "Lawrence", ""]]}, {"id": "1701.03077", "submitter": "Jonathan Barron", "authors": "Jonathan T. Barron", "title": "A General and Adaptive Robust Loss Function", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generalization of the Cauchy/Lorentzian, Geman-McClure,\nWelsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2\nloss functions. By introducing robustness as a continuous parameter, our loss\nfunction allows algorithms built around robust loss minimization to be\ngeneralized, which improves performance on basic vision tasks such as\nregistration and clustering. Interpreting our loss as the negative log of a\nunivariate density yields a general probability distribution that includes\nnormal and Cauchy distributions as special cases. This probabilistic\ninterpretation enables the training of neural networks in which the robustness\nof the loss automatically adapts itself during training, which improves\nperformance on learning-based tasks such as generative image synthesis and\nunsupervised monocular depth estimation, without requiring any manual parameter\ntuning.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 17:39:14 GMT"}, {"version": "v10", "created": "Thu, 4 Apr 2019 20:05:33 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 21:16:44 GMT"}, {"version": "v3", "created": "Thu, 9 Mar 2017 18:11:17 GMT"}, {"version": "v4", "created": "Fri, 26 Jan 2018 23:48:44 GMT"}, {"version": "v5", "created": "Sun, 11 Feb 2018 17:33:16 GMT"}, {"version": "v6", "created": "Mon, 5 Nov 2018 17:36:46 GMT"}, {"version": "v7", "created": "Tue, 11 Dec 2018 17:53:58 GMT"}, {"version": "v8", "created": "Sat, 9 Feb 2019 20:56:30 GMT"}, {"version": "v9", "created": "Sat, 30 Mar 2019 00:48:11 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Barron", "Jonathan T.", ""]]}, {"id": "1701.03102", "submitter": "Xiang Xiang", "authors": "Xiang Xiang, Trac D. Tran", "title": "Linear Disentangled Representation Learning for Facial Actions", "comments": "Codes available at https://github.com/eglxiang/icassp15_emotion and\n  https://github.com/eglxiang/FacialAU. arXiv admin note: text overlap with\n  arXiv:1410.1606", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited annotated data available for the recognition of facial expression and\naction units embarrasses the training of deep networks, which can learn\ndisentangled invariant features. However, a linear model with just several\nparameters normally is not demanding in terms of training data. In this paper,\nwe propose an elegant linear model to untangle confounding factors in\nchallenging realistic multichannel signals such as 2D face videos. The simple\nyet powerful model does not rely on huge training data and is natural for\nrecognizing facial actions without explicitly disentangling the identity. Base\non well-understood intuitive linear models such as Sparse Representation based\nClassification (SRC), previous attempts require a prepossessing of explicit\ndecoupling which is practically inexact. Instead, we exploit the low-rank\nproperty across frames to subtract the underlying neutral faces which are\nmodeled jointly with sparse representation on the action components with group\nsparsity enforced. On the extended Cohn-Kanade dataset (CK+), our one-shot\nautomatic method on raw face videos performs as competitive as SRC applied on\nmanually prepared action components and performs even better than SRC in terms\nof true positive rate. We apply the model to the even more challenging task of\nfacial action unit recognition, verified on the MPI Face Video Database\n(MPI-VDB) achieving a decent performance. All the programs and data have been\nmade publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 16:34:29 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Xiang", "Xiang", ""], ["Tran", "Trac D.", ""]]}, {"id": "1701.03162", "submitter": "Yifan Yang", "authors": "Yifan Yang and Tian Qin and Yu-Heng Lei", "title": "Real-time eSports Match Result Prediction", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we try to predict the winning team of a match in the\nmultiplayer eSports game Dota 2. To address the weaknesses of previous work, we\nconsider more aspects of prior (pre-match) features from individual players'\nmatch history, as well as real-time (during-match) features at each minute as\nthe match progresses. We use logistic regression, the proposed Attribute\nSequence Model, and their combinations as the prediction models. In a dataset\nof 78362 matches where 20631 matches contain replay data, our experiments show\nthat adding more aspects of prior features improves accuracy from 58.69% to\n71.49%, and introducing real-time features achieves up to 93.73% accuracy when\npredicting at the 40th minute.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 06:30:25 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Yang", "Yifan", ""], ["Qin", "Tian", ""], ["Lei", "Yu-Heng", ""]]}, {"id": "1701.03198", "submitter": "Haoqi Li", "authors": "Haoqi Li, Brian Baucom, Panayiotis Georgiou", "title": "Unsupervised Latent Behavior Manifold Learning from Acoustic Features:\n  audio2behavior", "comments": "Accepted by ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavioral annotation using signal processing and machine learning is highly\ndependent on training data and manual annotations of behavioral labels.\nPrevious studies have shown that speech information encodes significant\nbehavioral information and be used in a variety of automated behavior\nrecognition tasks. However, extracting behavior information from speech is\nstill a difficult task due to the sparseness of training data coupled with the\ncomplex, high-dimensionality of speech, and the complex and multiple\ninformation streams it encodes. In this work we exploit the slow varying\nproperties of human behavior. We hypothesize that nearby segments of speech\nshare the same behavioral context and hence share a similar underlying\nrepresentation in a latent space. Specifically, we propose a Deep Neural\nNetwork (DNN) model to connect behavioral context and derive the behavioral\nmanifold in an unsupervised manner. We evaluate the proposed manifold in the\ncouples therapy domain and also provide examples from publicly available data\n(e.g. stand-up comedy). We further investigate training within the couples'\ntherapy domain and from movie data. The results are extremely encouraging and\npromise improved behavioral quantification in an unsupervised manner and\nwarrants further investigation in a range of applications.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 01:02:22 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Li", "Haoqi", ""], ["Baucom", "Brian", ""], ["Georgiou", "Panayiotis", ""]]}, {"id": "1701.03212", "submitter": "Wei Guo", "authors": "Wei Guo, Krithika Manohar, Steven L. Brunton and Ashis G. Banerjee", "title": "Sparse-TDA: Sparse Realization of Topological Data Analysis for\n  Multi-Way Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological data analysis (TDA) has emerged as one of the most promising\ntechniques to reconstruct the unknown shapes of high-dimensional spaces from\nobserved data samples. TDA, thus, yields key shape descriptors in the form of\npersistent topological features that can be used for any supervised or\nunsupervised learning task, including multi-way classification. Sparse\nsampling, on the other hand, provides a highly efficient technique to\nreconstruct signals in the spatial-temporal domain from just a few\ncarefully-chosen samples. Here, we present a new method, referred to as the\nSparse-TDA algorithm, that combines favorable aspects of the two techniques.\nThis combination is realized by selecting an optimal set of sparse pixel\nsamples from the persistent features generated by a vector-based TDA algorithm.\nThese sparse samples are selected from a low-rank matrix representation of\npersistent features using QR pivoting. We show that the Sparse-TDA method\ndemonstrates promising performance on three benchmark problems related to human\nposture recognition and image texture classification.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 02:22:45 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 23:49:37 GMT"}, {"version": "v3", "created": "Fri, 10 Nov 2017 08:13:59 GMT"}, {"version": "v4", "created": "Mon, 13 Nov 2017 01:31:28 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Guo", "Wei", ""], ["Manohar", "Krithika", ""], ["Brunton", "Steven L.", ""], ["Banerjee", "Ashis G.", ""]]}, {"id": "1701.03227", "submitter": "Angela Fan", "authors": "Angela Fan, Finale Doshi-Velez, Luke Miratrix", "title": "Prior matters: simple and general methods for evaluating and improving\n  topic quality in topic modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet Allocation (LDA) models trained without stopword removal\noften produce topics with high posterior probabilities on uninformative words,\nobscuring the underlying corpus content. Even when canonical stopwords are\nmanually removed, uninformative words common in that corpus will still dominate\nthe most probable words in a topic. In this work, we first show how the\nstandard topic quality measures of coherence and pointwise mutual information\nact counter-intuitively in the presence of common but irrelevant words, making\nit difficult to even quantitatively identify situations in which topics may be\ndominated by stopwords. We propose an additional topic quality metric that\ntargets the stopword problem, and show that it, unlike the standard measures,\ncorrectly correlates with human judgements of quality. We also propose a\nsimple-to-implement strategy for generating topics that are evaluated to be of\nmuch higher quality by both human assessment and our new metric. This approach,\na collection of informative priors easily introduced into most LDA-style\ninference methods, automatically promotes terms with domain relevance and\ndemotes domain-specific stop words. We demonstrate this approach's\neffectiveness in three very different domains: Department of Labor accident\nreports, online health forum posts, and NIPS abstracts. Overall we find that\ncurrent practices thought to solve this problem do not do so adequately, and\nthat our proposal offers a substantial improvement for those interested in\ninterpreting their topics as objects in their own right.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 04:26:00 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 23:17:12 GMT"}, {"version": "v3", "created": "Sat, 14 Oct 2017 18:25:03 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Fan", "Angela", ""], ["Doshi-Velez", "Finale", ""], ["Miratrix", "Luke", ""]]}, {"id": "1701.03281", "submitter": "Tao Wei", "authors": "Tao Wei, Changhu Wang, Chang Wen Chen", "title": "Modularized Morphing of Neural Networks", "comments": "12 pages, 6 figures, Under review as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the problem of network morphism, an effective learning\nscheme to morph a well-trained neural network to a new one with the network\nfunction completely preserved. Different from existing work where basic\nmorphing types on the layer level were addressed, we target at the central\nproblem of network morphism at a higher level, i.e., how a convolutional layer\ncan be morphed into an arbitrary module of a neural network. To simplify the\nrepresentation of a network, we abstract a module as a graph with blobs as\nvertices and convolutional layers as edges, based on which the morphing process\nis able to be formulated as a graph transformation problem. Two atomic morphing\noperations are introduced to compose the graphs, based on which modules are\nclassified into two families, i.e., simple morphable modules and complex\nmodules. We present practical morphing solutions for both of these two\nfamilies, and prove that any reasonable module can be morphed from a single\nconvolutional layer. Extensive experiments have been conducted based on the\nstate-of-the-art ResNet on benchmark datasets, and the effectiveness of the\nproposed solution has been verified.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 09:48:53 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Wei", "Tao", ""], ["Wang", "Changhu", ""], ["Chen", "Chang Wen", ""]]}, {"id": "1701.03360", "submitter": "Jaeyoung Kim", "authors": "Jaeyoung Kim, Mostafa El-Khamy, and Jungwon Lee", "title": "Residual LSTM: Design of a Deep Recurrent Architecture for Distant\n  Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel architecture for a deep recurrent neural network,\nresidual LSTM is introduced. A plain LSTM has an internal memory cell that can\nlearn long term dependencies of sequential data. It also provides a temporal\nshortcut path to avoid vanishing or exploding gradients in the temporal domain.\nThe residual LSTM provides an additional spatial shortcut path from lower\nlayers for efficient training of deep networks with multiple LSTM layers.\nCompared with the previous work, highway LSTM, residual LSTM separates a\nspatial shortcut path with temporal one by using output layers, which can help\nto avoid a conflict between spatial and temporal-domain gradient flows.\nFurthermore, residual LSTM reuses the output projection matrix and the output\ngate of LSTM to control the spatial information flow instead of additional gate\nnetworks, which effectively reduces more than 10% of network parameters. An\nexperiment for distant speech recognition on the AMI SDM corpus shows that\n10-layer plain and highway LSTM networks presented 13.7% and 6.2% increase in\nWER over 3-layer aselines, respectively. On the contrary, 10-layer residual\nLSTM networks provided the lowest WER 41.0%, which corresponds to 3.3% and 2.8%\nWER reduction over plain and highway LSTM networks, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 20:03:37 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 00:23:45 GMT"}, {"version": "v3", "created": "Mon, 5 Jun 2017 18:51:08 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Kim", "Jaeyoung", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""]]}, {"id": "1701.03400", "submitter": "Nicholas Fraser", "authors": "Nicholas J. Fraser, Yaman Umuroglu, Giulio Gambardella, Michaela\n  Blott, Philip Leong, Magnus Jahre and Kees Vissers", "title": "Scaling Binarized Neural Networks on Reconfigurable Logic", "comments": "To appear in the PARMA-DITAM workshop at HiPEAC 2017, January 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binarized neural networks (BNNs) are gaining interest in the deep learning\ncommunity due to their significantly lower computational and memory cost. They\nare particularly well suited to reconfigurable logic devices, which contain an\nabundance of fine-grained compute resources and can result in smaller, lower\npower implementations, or conversely in higher classification rates. Towards\nthis end, the Finn framework was recently proposed for building fast and\nflexible field programmable gate array (FPGA) accelerators for BNNs. Finn\nutilized a novel set of optimizations that enable efficient mapping of BNNs to\nhardware and implemented fully connected, non-padded convolutional and pooling\nlayers, with per-layer compute resources being tailored to user-provided\nthroughput requirements. However, FINN was not evaluated on larger topologies\ndue to the size of the chosen FPGA, and exhibited decreased accuracy due to\nlack of padding. In this paper, we improve upon Finn to show how padding can be\nemployed on BNNs while still maintaining a 1-bit datapath and high accuracy.\nBased on this technique, we demonstrate numerous experiments to illustrate\nflexibility and scalability of the approach. In particular, we show that a\nlarge BNN requiring 1.2 billion operations per frame running on an ADM-PCIE-8K5\nplatform can classify images at 12 kFPS with 671 us latency while drawing less\nthan 41 W board power and classifying CIFAR-10 images at 88.7% accuracy. Our\nimplementation of this network achieves 14.8 trillion operations per second. We\nbelieve this is the fastest classification rate reported to date on this\nbenchmark at this level of accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 16:42:47 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 09:12:48 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Fraser", "Nicholas J.", ""], ["Umuroglu", "Yaman", ""], ["Gambardella", "Giulio", ""], ["Blott", "Michaela", ""], ["Leong", "Philip", ""], ["Jahre", "Magnus", ""], ["Vissers", "Kees", ""]]}, {"id": "1701.03449", "submitter": "Andreas Damianou Dr", "authors": "Andreas Damianou, Neil D. Lawrence and Carl Henrik Ek", "title": "Manifold Alignment Determination: finding correspondences across\n  different data views", "comments": "NIPS workshop on Multi-Modal Machine Learning, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Manifold Alignment Determination (MAD), an algorithm for learning\nalignments between data points from multiple views or modalities. The approach\nis capable of learning correspondences between views as well as correspondences\nbetween individual data-points. The proposed method requires only a few aligned\nexamples from which it is capable to recover a global alignment through a\nprobabilistic model. The strong, yet flexible regularization provided by the\ngenerative model is sufficient to align the views. We provide experiments on\nboth synthetic and real data to highlight the benefit of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 18:36:47 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Damianou", "Andreas", ""], ["Lawrence", "Neil D.", ""], ["Ek", "Carl Henrik", ""]]}, {"id": "1701.03458", "submitter": "Tina Woolf", "authors": "Deanna Needell, Tina Woolf", "title": "An Asynchronous Parallel Approach to Sparse Recovery", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous parallel computing and sparse recovery are two areas that have\nreceived recent interest. Asynchronous algorithms are often studied to solve\noptimization problems where the cost function takes the form $\\sum_{i=1}^M\nf_i(x)$, with a common assumption that each $f_i$ is sparse; that is, each\n$f_i$ acts only on a small number of components of $x\\in\\mathbb{R}^n$. Sparse\nrecovery problems, such as compressed sensing, can be formulated as\noptimization problems, however, the cost functions $f_i$ are dense with respect\nto the components of $x$, and instead the signal $x$ is assumed to be sparse,\nmeaning that it has only $s$ non-zeros where $s\\ll n$. Here we address how one\nmay use an asynchronous parallel architecture when the cost functions $f_i$ are\nnot sparse in $x$, but rather the signal $x$ is sparse. We propose an\nasynchronous parallel approach to sparse recovery via a stochastic greedy\nalgorithm, where multiple processors asynchronously update a vector in shared\nmemory containing information on the estimated signal support. We include\nnumerical simulations that illustrate the potential benefits of our proposed\nasynchronous method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 05:14:40 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Needell", "Deanna", ""], ["Woolf", "Tina", ""]]}, {"id": "1701.03537", "submitter": "Adel Javanmard", "authors": "Adel Javanmard", "title": "Perishability of Data: Dynamic Pricing under Varying-Coefficient Models", "comments": "30 pages, 2 figures; accepted for publication in the Journal of\n  Machine Learning Research (JMLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a firm that sells a large number of products to its customers in\nan online fashion. Each product is described by a high dimensional feature\nvector, and the market value of a product is assumed to be linear in the values\nof its features. Parameters of the valuation model are unknown and can change\nover time. The firm sequentially observes a product's features and can use the\nhistorical sales data (binary sale/no sale feedbacks) to set the price of\ncurrent product, with the objective of maximizing the collected revenue. We\nmeasure the performance of a dynamic pricing policy via regret, which is the\nexpected revenue loss compared to a clairvoyant that knows the sequence of\nmodel parameters in advance.\n  We propose a pricing policy based on projected stochastic gradient descent\n(PSGD) and characterize its regret in terms of time $T$, features dimension\n$d$, and the temporal variability in the model parameters, $\\delta_t$. We\nconsider two settings. In the first one, feature vectors are chosen\nantagonistically by nature and we prove that the regret of PSGD pricing policy\nis of order $O(\\sqrt{T} + \\sum_{t=1}^T \\sqrt{t}\\delta_t)$. In the second\nsetting (referred to as stochastic features model), the feature vectors are\ndrawn independently from an unknown distribution. We show that in this case,\nthe regret of PSGD pricing policy is of order $O(d^2 \\log T + \\sum_{t=1}^T\nt\\delta_t/d)$.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 01:08:35 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 21:53:21 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Javanmard", "Adel", ""]]}, {"id": "1701.03577", "submitter": "Avner May", "authors": "Avner May, Alireza Bagheri Garakani, Zhiyun Lu, Dong Guo, Kuan Liu,\n  Aur\\'elien Bellet, Linxi Fan, Michael Collins, Daniel Hsu, Brian Kingsbury,\n  Michael Picheny, Fei Sha", "title": "Kernel Approximation Methods for Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study large-scale kernel methods for acoustic modeling in speech\nrecognition and compare their performance to deep neural networks (DNNs). We\nperform experiments on four speech recognition datasets, including the TIMIT\nand Broadcast News benchmark tasks, and compare these two types of models on\nframe-level performance metrics (accuracy, cross-entropy), as well as on\nrecognition metrics (word/character error rate). In order to scale kernel\nmethods to these large datasets, we use the random Fourier feature method of\nRahimi and Recht (2007). We propose two novel techniques for improving the\nperformance of kernel acoustic models. First, in order to reduce the number of\nrandom features required by kernel models, we propose a simple but effective\nmethod for feature selection. The method is able to explore a large number of\nnon-linear features while maintaining a compact model more efficiently than\nexisting approaches. Second, we present a number of frame-level metrics which\ncorrelate very strongly with recognition performance when computed on the\nheldout set; we take advantage of these correlations by monitoring these\nmetrics during training in order to decide when to stop learning. This\ntechnique can noticeably improve the recognition performance of both DNN and\nkernel models, while narrowing the gap between them. Additionally, we show that\nthe linear bottleneck method of Sainath et al. (2013) improves the performance\nof our kernel models significantly, in addition to speeding up training and\nmaking the models more compact. Together, these three methods dramatically\nimprove the performance of kernel acoustic models, making their performance\ncomparable to DNNs on the tasks we explored.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 07:24:18 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["May", "Avner", ""], ["Garakani", "Alireza Bagheri", ""], ["Lu", "Zhiyun", ""], ["Guo", "Dong", ""], ["Liu", "Kuan", ""], ["Bellet", "Aur\u00e9lien", ""], ["Fan", "Linxi", ""], ["Collins", "Michael", ""], ["Hsu", "Daniel", ""], ["Kingsbury", "Brian", ""], ["Picheny", "Michael", ""], ["Sha", "Fei", ""]]}, {"id": "1701.03619", "submitter": "Ori Katz", "authors": "Ori Katz, Ronen Talmon, Yu-Lun Lo and Hau-Tieng Wu", "title": "Diffusion-based nonlinear filtering for multimodal data fusion with\n  application to sleep stage assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of information fusion from multiple data-sets acquired by\nmultimodal sensors has drawn significant research attention over the years. In\nthis paper, we focus on a particular problem setting consisting of a physical\nphenomenon or a system of interest observed by multiple sensors. We assume that\nall sensors measure some aspects of the system of interest with additional\nsensor-specific and irrelevant components. Our goal is to recover the variables\nrelevant to the observed system and to filter out the nuisance effects of the\nsensor-specific variables. We propose an approach based on manifold learning,\nwhich is particularly suitable for problems with multiple modalities, since it\naims to capture the intrinsic structure of the data and relies on minimal prior\nmodel knowledge. Specifically, we propose a nonlinear filtering scheme, which\nextracts the hidden sources of variability captured by two or more sensors,\nthat are independent of the sensor-specific components. In addition to\npresenting a theoretical analysis, we demonstrate our technique on real\nmeasured data for the purpose of sleep stage assessment based on multiple,\nmultimodal sensor measurements. We show that without prior knowledge on the\ndifferent modalities and on the measured system, our method gives rise to a\ndata-driven representation that is well correlated with the underlying sleep\nprocess and is robust to noise and sensor-specific effects.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 10:45:01 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 16:39:50 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Katz", "Ori", ""], ["Talmon", "Ronen", ""], ["Lo", "Yu-Lun", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "1701.03633", "submitter": "Riccardo Satta", "authors": "Riccardo Satta, Stefano Cavallari, Eraldo Pomponi, Daniele Grasselli,\n  Davide Picheo, Carlo Annis", "title": "A dissimilarity-based approach to predictive maintenance with\n  application to HVAC systems", "comments": "keywords: predictive maintenance, condition-based maintenance,\n  prognosis, machine learning, dissimilarity-based representation, HVAC. 15\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of predictive maintenance is to forecast the occurrence of faults of\nan appliance, in order to proactively take the necessary actions to ensure its\navailability. In many application scenarios, predictive maintenance is applied\nto a set of homogeneous appliances. In this paper, we firstly review taxonomies\nand main methodologies currently used for condition-based maintenance;\nsecondly, we argue that the mutual dissimilarities of the behaviours of all\nappliances of this set (the \"cohort\") can be exploited to detect upcoming\nfaults. Specifically, inspired by dissimilarity-based representations, we\npropose a novel machine learning approach based on the analysis of concurrent\nmutual differences of the measurements coming from the cohort. We evaluate our\nmethod over one year of historical data from a cohort of 17 HVAC (Heating,\nVentilation and Air Conditioning) systems installed in an Italian hospital. We\nshow that certain kinds of faults can be foreseen with an accuracy, measured in\nterms of area under the ROC curve, as high as 0.96.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 11:31:35 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Satta", "Riccardo", ""], ["Cavallari", "Stefano", ""], ["Pomponi", "Eraldo", ""], ["Grasselli", "Daniele", ""], ["Picheo", "Davide", ""], ["Annis", "Carlo", ""]]}, {"id": "1701.03641", "submitter": "Jan \\v{Z}egklitz", "authors": "Jan \\v{Z}egklitz, Petr Po\\v{s}\\'ik", "title": "Symbolic Regression Algorithms with Built-in Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several algorithms for symbolic regression (SR) emerged which\nemploy a form of multiple linear regression (LR) to produce generalized linear\nmodels. The use of LR allows the algorithms to create models with relatively\nsmall error right from the beginning of the search; such algorithms are thus\nclaimed to be (sometimes by orders of magnitude) faster than SR algorithms\nbased on vanilla genetic programming. However, a systematic comparison of these\nalgorithms on a common set of problems is still missing. In this paper we\nconceptually and experimentally compare several representatives of such\nalgorithms (GPTIPS, FFX, and EFS). They are applied as off-the-shelf,\nready-to-use techniques, mostly using their default settings. The methods are\ncompared on several synthetic and real-world SR benchmark problems. Their\nperformance is also related to the performance of three conventional machine\nlearning algorithms --- multiple regression, random forests and support vector\nregression.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 12:23:10 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 10:24:16 GMT"}, {"version": "v3", "created": "Fri, 10 Mar 2017 11:14:17 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["\u017degklitz", "Jan", ""], ["Po\u0161\u00edk", "Petr", ""]]}, {"id": "1701.03647", "submitter": "Jielei Chu", "authors": "Jielei Chu, Hongjun Wang, Hua Meng, Peng Jin and Tianrui Li (Senior\n  member, IEEE)", "title": "Restricted Boltzmann Machines with Gaussian Visible Units Guided by\n  Pairwise Constraints", "comments": "13pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann machines (RBMs) and their variants are usually trained\nby contrastive divergence (CD) learning, but the training procedure is an\nunsupervised learning approach, without any guidances of the background\nknowledge. To enhance the expression ability of traditional RBMs, in this\npaper, we propose pairwise constraints restricted Boltzmann machine with\nGaussian visible units (pcGRBM) model, in which the learning procedure is\nguided by pairwise constraints and the process of encoding is conducted under\nthese guidances. The pairwise constraints are encoded in hidden layer features\nof pcGRBM. Then, some pairwise hidden features of pcGRBM flock together and\nanother part of them are separated by the guidances. In order to deal with\nreal-valued data, the binary visible units are replaced by linear units with\nGausian noise in the pcGRBM model. In the learning process of pcGRBM, the\npairwise constraints are iterated transitions between visible and hidden units\nduring CD learning procedure. Then, the proposed model is inferred by\napproximative gradient descent method and the corresponding learning algorithm\nis designed in this paper. In order to compare the availability of pcGRBM and\ntraditional RBMs with Gaussian visible units, the features of the pcGRBM and\nRBMs hidden layer are used as input 'data' for K-means, spectral clustering\n(SP) and affinity propagation (AP) algorithms, respectively. A thorough\nexperimental evaluation is performed with sixteen image datasets of Microsoft\nResearch Asia Multimedia (MSRA-MM). The experimental results show that the\nclustering performance of K-means, SP and AP algorithms based on pcGRBM model\nare significantly better than traditional RBMs. In addition, the pcGRBM model\nfor clustering task shows better performance than some semi-supervised\nclustering algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 12:43:58 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 13:43:12 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Chu", "Jielei", "", "Senior\n  member, IEEE"], ["Wang", "Hongjun", "", "Senior\n  member, IEEE"], ["Meng", "Hua", "", "Senior\n  member, IEEE"], ["Jin", "Peng", "", "Senior\n  member, IEEE"], ["Li", "Tianrui", "", "Senior\n  member, IEEE"]]}, {"id": "1701.03655", "submitter": "Karin Schnass", "authors": "Valeriya Naumova and Karin Schnass", "title": "Dictionary Learning from Incomplete Data", "comments": "22 pages, 9 figures, (this version with bug fix for wksvd)", "journal-ref": null, "doi": "10.1186/s13634-018-0533-0", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the recently proposed and theoretically justified\niterative thresholding and $K$ residual means algorithm ITKrM to learning\ndicionaries from incomplete/masked training data (ITKrMM). It further adapts\nthe algorithm to the presence of a low rank component in the data and provides\na strategy for recovering this low rank component again from incomplete data.\nSeveral synthetic experiments show the advantages of incorporating information\nabout the corruption into the algorithm. Finally, image inpainting is\nconsidered as application example, which demonstrates the superior performance\nof ITKrMM in terms of speed at similar or better reconstruction quality\ncompared to its closest dictionary learning counterpart.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 13:06:47 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 13:37:00 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Naumova", "Valeriya", ""], ["Schnass", "Karin", ""]]}, {"id": "1701.03743", "submitter": "Arnim Bleier", "authors": "Arnim Bleier", "title": "Truncation-free Hybrid Inference for DPMM", "comments": "NIPS 2016 Workshop: Advances in Approximate Bayesian Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dirichlet process mixture models (DPMM) are a cornerstone of Bayesian\nnon-parametrics. While these models free from choosing the number of components\na-priori, computationally attractive variational inference often reintroduces\nthe need to do so, via a truncation on the variational distribution. In this\npaper we present a truncation-free hybrid inference for DPMM, combining the\nadvantages of sampling-based MCMC and variational methods. The proposed\nhybridization enables more efficient variational updates, while increasing\nmodel complexity only if needed. We evaluate the properties of the hybrid\nupdates and their empirical performance in single- as well as mixed-membership\nmodels. Our method is easy to implement and performs favorably compared to\nexisting schemas.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 17:28:09 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Bleier", "Arnim", ""]]}, {"id": "1701.03757", "submitter": "Dustin Tran", "authors": "Dustin Tran, Matthew D. Hoffman, Rif A. Saurous, Eugene Brevdo, Kevin\n  Murphy, David M. Blei", "title": "Deep Probabilistic Programming", "comments": "Appears in International Conference on Learning Representations,\n  2017. A companion webpage for this paper is available at\n  http://edwardlib.org/iclr2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.PL stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Edward, a Turing-complete probabilistic programming language.\nEdward defines two compositional representations---random variables and\ninference. By treating inference as a first class citizen, on a par with\nmodeling, we show that probabilistic programming can be as flexible and\ncomputationally efficient as traditional deep learning. For flexibility, Edward\nmakes it easy to fit the same model using a variety of composable inference\nmethods, ranging from point estimation to variational inference to MCMC. In\naddition, Edward can reuse the modeling representation as part of inference,\nfacilitating the design of rich variational models and generative adversarial\nnetworks. For efficiency, Edward is integrated into TensorFlow, providing\nsignificant speedups over existing probabilistic systems. For example, we show\non a benchmark logistic regression task that Edward is at least 35x faster than\nStan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it\nis as fast as handwritten TensorFlow.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 17:52:07 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 18:41:45 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Tran", "Dustin", ""], ["Hoffman", "Matthew D.", ""], ["Saurous", "Rif A.", ""], ["Brevdo", "Eugene", ""], ["Murphy", "Kevin", ""], ["Blei", "David M.", ""]]}, {"id": "1701.03866", "submitter": "Steven Hansen", "authors": "Steven Stenberg Hansen", "title": "Long Timescale Credit Assignment in NeuralNetworks with External Memory", "comments": "Accepted into the NIPS 2016 workshop on Continual Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Credit assignment in traditional recurrent neural networks usually involves\nback-propagating through a long chain of tied weight matrices. The length of\nthis chain scales linearly with the number of time-steps as the same network is\nrun at each time-step. This creates many problems, such as vanishing gradients,\nthat have been well studied. In contrast, a NNEM's architecture recurrent\nactivity doesn't involve a long chain of activity (though some architectures\nsuch as the NTM do utilize a traditional recurrent architecture as a\ncontroller). Rather, the externally stored embedding vectors are used at each\ntime-step, but no messages are passed from previous time-steps. This means that\nvanishing gradients aren't a problem, as all of the necessary gradient paths\nare short. However, these paths are extremely numerous (one per embedding\nvector in memory) and reused for a very long time (until it leaves the memory).\nThus, the forward-pass information of each memory must be stored for the entire\nduration of the memory. This is problematic as this additional storage far\nsurpasses that of the actual memories, to the extent that large memories on\ninfeasible to back-propagate through in high dimensional settings. One way to\nget around the need to hold onto forward-pass information is to recalculate the\nforward-pass whenever gradient information is available. However, if the\nobservations are too large to store in the domain of interest, direct\nreinstatement of a forward pass cannot occur. Instead, we rely on a learned\nautoencoder to reinstate the observation, and then use the embedding network to\nrecalculate the forward-pass. Since the recalculated embedding vector is\nunlikely to perfectly match the one stored in memory, we try out 2\napproximations to utilize error gradient w.r.t. the vector in memory.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 01:47:54 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Hansen", "Steven Stenberg", ""]]}, {"id": "1701.03891", "submitter": "Ali Mousavi", "authors": "Ali Mousavi, Richard G. Baraniuk", "title": "Learning to Invert: Signal Recovery via Deep Convolutional Networks", "comments": "Accepted at The 42nd IEEE International Conference on Acoustics,\n  Speech and Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The promise of compressive sensing (CS) has been offset by two significant\nchallenges. First, real-world data is not exactly sparse in a fixed basis.\nSecond, current high-performance recovery algorithms are slow to converge,\nwhich limits CS to either non-real-time applications or scenarios where massive\nback-end computing is available. In this paper, we attack both of these\nchallenges head-on by developing a new signal recovery framework we call {\\em\nDeepInverse} that learns the inverse transformation from measurement vectors to\nsignals using a {\\em deep convolutional network}. When trained on a set of\nrepresentative images, the network learns both a representation for the signals\n(addressing challenge one) and an inverse map approximating a greedy or convex\nrecovery algorithm (addressing challenge two). Our experiments indicate that\nthe DeepInverse network closely approximates the solution produced by\nstate-of-the-art CS recovery algorithms yet is hundreds of times faster in run\ntime. The tradeoff for the ultrafast run time is a computationally intensive,\noff-line training procedure typical to deep networks. However, the training\nneeds to be completed only once, which makes the approach attractive for a host\nof sparse recovery problems.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 08:42:19 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Mousavi", "Ali", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1701.03916", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Ke Sun and St\\'ephane Marchand-Maillet", "title": "On H\\\"older projective divergences", "comments": "25 pages", "journal-ref": null, "doi": "10.3390/e19030122", "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a framework to build distances by measuring the tightness of\ninequalities, and introduce the notion of proper statistical divergences and\nimproper pseudo-divergences. We then consider the H\\\"older ordinary and reverse\ninequalities, and present two novel classes of H\\\"older divergences and\npseudo-divergences that both encapsulate the special case of the Cauchy-Schwarz\ndivergence. We report closed-form formulas for those statistical\ndissimilarities when considering distributions belonging to the same\nexponential family provided that the natural parameter space is a cone (e.g.,\nmultivariate Gaussians), or affine (e.g., categorical distributions). Those new\nclasses of H\\\"older distances are invariant to rescaling, and thus do not\nrequire distributions to be normalized. Finally, we show how to compute\nstatistical H\\\"older centroids with respect to those divergences, and carry out\ncenter-based clustering toy experiments on a set of Gaussian distributions that\ndemonstrate empirically that symmetrized H\\\"older divergences outperform the\nsymmetric Cauchy-Schwarz divergence.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 12:57:44 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Nielsen", "Frank", ""], ["Sun", "Ke", ""], ["Marchand-Maillet", "St\u00e9phane", ""]]}, {"id": "1701.03918", "submitter": "Yongqing Wang", "authors": "Yongqing Wang, Shenghua Liu, Huawei Shen, Xueqi Cheng", "title": "Marked Temporal Dynamics Modeling based on Recurrent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are now witnessing the increasing availability of event stream data, i.e.,\na sequence of events with each event typically being denoted by the time it\noccurs and its mark information (e.g., event type). A fundamental problem is to\nmodel and predict such kind of marked temporal dynamics, i.e., when the next\nevent will take place and what its mark will be. Existing methods either\npredict only the mark or the time of the next event, or predict both of them,\nyet separately. Indeed, in marked temporal dynamics, the time and the mark of\nthe next event are highly dependent on each other, requiring a method that\ncould simultaneously predict both of them. To tackle this problem, in this\npaper, we propose to model marked temporal dynamics by using a mark-specific\nintensity function to explicitly capture the dependency between the mark and\nthe time of the next event. Extensive experiments on two datasets demonstrate\nthat the proposed method outperforms state-of-the-art methods at predicting\nmarked temporal dynamics.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 13:26:39 GMT"}], "update_date": "2017-02-12", "authors_parsed": [["Wang", "Yongqing", ""], ["Liu", "Shenghua", ""], ["Shen", "Huawei", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1701.03940", "submitter": "Rafael Pinto", "authors": "Rafael Pinto, Paulo Engel", "title": "Scalable and Incremental Learning of Gaussian Mixture Models", "comments": "13 pages, 1 figure, submitted for peer-review. arXiv admin note:\n  substantial text overlap with arXiv:1506.04422", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a fast and scalable algorithm for incremental learning of\nGaussian mixture models. By performing rank-one updates on its precision\nmatrices and determinants, its asymptotic time complexity is of \\BigO{NKD^2}\nfor $N$ data points, $K$ Gaussian components and $D$ dimensions. The resulting\nalgorithm can be applied to high dimensional tasks, and this is confirmed by\napplying it to the classification datasets MNIST and CIFAR-10. Additionally, in\norder to show the algorithm's applicability to function approximation and\ncontrol tasks, it is applied to three reinforcement learning tasks and its\ndata-efficiency is evaluated.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 16:15:44 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Pinto", "Rafael", ""], ["Engel", "Paulo", ""]]}, {"id": "1701.03961", "submitter": "Soomin Lee", "authors": "Guanghui Lan, Soomin Lee, and Yi Zhou", "title": "Communication-Efficient Algorithms for Decentralized and Stochastic\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new class of decentralized first-order methods for nonsmooth and\nstochastic optimization problems defined over multiagent networks. Considering\nthat communication is a major bottleneck in decentralized optimization, our\nmain goal in this paper is to develop algorithmic frameworks which can\nsignificantly reduce the number of inter-node communications. We first propose\na decentralized primal-dual method which can find an $\\epsilon$-solution both\nin terms of functional optimality gap and feasibility residual in\n$O(1/\\epsilon)$ inter-node communication rounds when the objective functions\nare convex and the local primal subproblems are solved exactly. Our major\ncontribution is to present a new class of decentralized primal-dual type\nalgorithms, namely the decentralized communication sliding (DCS) methods, which\ncan skip the inter-node communications while agents solve the primal\nsubproblems iteratively through linearizations of their local objective\nfunctions. By employing DCS, agents can still find an $\\epsilon$-solution in\n$O(1/\\epsilon)$ (resp., $O(1/\\sqrt{\\epsilon})$) communication rounds for\ngeneral convex functions (resp., strongly convex functions), while maintaining\nthe $O(1/\\epsilon^2)$ (resp., $O(1/\\epsilon)$) bound on the total number of\nintra-node subgradient evaluations. We also present a stochastic counterpart\nfor these algorithms, denoted by SDCS, for solving stochastic optimization\nproblems whose objective function cannot be evaluated exactly. In comparison\nwith existing results for decentralized nonsmooth and stochastic optimization,\nwe can reduce the total number of inter-node communication rounds by orders of\nmagnitude while still maintaining the optimal complexity bounds on intra-node\nstochastic subgradient evaluations. The bounds on the subgradient evaluations\nare actually comparable to those required for centralized nonsmooth and\nstochastic optimization.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 19:48:49 GMT"}, {"version": "v2", "created": "Sat, 4 Feb 2017 16:45:06 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Lan", "Guanghui", ""], ["Lee", "Soomin", ""], ["Zhou", "Yi", ""]]}, {"id": "1701.03974", "submitter": "Tianyi Chen", "authors": "Tianyi Chen, Qing Ling, Georgios B. Giannakis", "title": "An Online Convex Optimization Approach to Dynamic Network Resource\n  Allocation", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2750109", "report-no": null, "categories": "cs.SY cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to online convex optimization (OCO) make sequential\none-slot-ahead decisions, which lead to (possibly adversarial) losses that\ndrive subsequent decision iterates. Their performance is evaluated by the\nso-called regret that measures the difference of losses between the online\nsolution and the best yet fixed overall solution in hindsight. The present\npaper deals with online convex optimization involving adversarial loss\nfunctions and adversarial constraints, where the constraints are revealed after\nmaking decisions, and can be tolerable to instantaneous violations but must be\nsatisfied in the long term. Performance of an online algorithm in this setting\nis assessed by: i) the difference of its losses relative to the best dynamic\nsolution with one-slot-ahead information of the loss function and the\nconstraint (that is here termed dynamic regret); and, ii) the accumulated\namount of constraint violations (that is here termed dynamic fit). In this\ncontext, a modified online saddle-point (MOSP) scheme is developed, and proved\nto simultaneously yield sub-linear dynamic regret and fit, provided that the\naccumulated variations of per-slot minimizers and constraints are sub-linearly\ngrowing with time. MOSP is also applied to the dynamic network resource\nallocation task, and it is compared with the well-known stochastic dual\ngradient method. Under various scenarios, numerical experiments demonstrate the\nperformance gain of MOSP relative to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 23:28:21 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 05:33:29 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Chen", "Tianyi", ""], ["Ling", "Qing", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1701.04077", "submitter": "Federico Cabitza", "authors": "Federico Cabitza", "title": "Breeding electric zebras in the fields of Medicine", "comments": "Work-in-progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A few notes on the use of machine learning in medicine and the related\nunintended consequences.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 17:06:08 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 13:33:07 GMT"}, {"version": "v3", "created": "Fri, 27 Jan 2017 09:11:59 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Cabitza", "Federico", ""]]}, {"id": "1701.04079", "submitter": "David Abel", "authors": "David Abel, John Salvatier, Andreas Stuhlm\\\"uller, Owain Evans", "title": "Agent-Agnostic Human-in-the-Loop Reinforcement Learning", "comments": "Presented at the NIPS Workshop on the Future of Interactive Learning\n  Machines, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing Reinforcement Learning agents with expert advice can dramatically\nimprove various aspects of learning. Prior work has developed teaching\nprotocols that enable agents to learn efficiently in complex environments; many\nof these methods tailor the teacher's guidance to agents with a particular\nrepresentation or underlying learning scheme, offering effective but\nspecialized teaching procedures. In this work, we explore protocol programs, an\nagent-agnostic schema for Human-in-the-Loop Reinforcement Learning. Our goal is\nto incorporate the beneficial properties of a human teacher into Reinforcement\nLearning without making strong assumptions about the inner workings of the\nagent. We show how to represent existing approaches such as action pruning,\nreward shaping, and training in simulation as special cases of our schema and\nconduct preliminary experiments on simple domains.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 17:14:40 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Abel", "David", ""], ["Salvatier", "John", ""], ["Stuhlm\u00fcller", "Andreas", ""], ["Evans", "Owain", ""]]}, {"id": "1701.04099", "submitter": "Yuchin Juan", "authors": "Yuchin Juan, Damien Lefortier, Olivier Chapelle", "title": "Field-aware Factorization Machines in a Real-world Online Advertising\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting user response is one of the core machine learning tasks in\ncomputational advertising. Field-aware Factorization Machines (FFM) have\nrecently been established as a state-of-the-art method for that problem and in\nparticular won two Kaggle challenges. This paper presents some results from\nimplementing this method in a production system that predicts click-through and\nconversion rates for display advertising and shows that this method it is not\nonly effective to win challenges but is also valuable in a real-world\nprediction system. We also discuss some specific challenges and solutions to\nreduce the training time, namely the use of an innovative seeding algorithm and\na distributed learning mechanism.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 19:13:22 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 16:47:44 GMT"}, {"version": "v3", "created": "Thu, 23 Feb 2017 05:26:04 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Juan", "Yuchin", ""], ["Lefortier", "Damien", ""], ["Chapelle", "Olivier", ""]]}, {"id": "1701.04113", "submitter": "David Abel", "authors": "David Abel, D. Ellis Hershkowitz, Michael L. Littman", "title": "Near Optimal Behavior via Approximate State Abstraction", "comments": "Earlier version published at ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combinatorial explosion that plagues planning and reinforcement learning\n(RL) algorithms can be moderated using state abstraction. Prohibitively large\ntask representations can be condensed such that essential information is\npreserved, and consequently, solutions are tractably computable. However, exact\nabstractions, which treat only fully-identical situations as equivalent, fail\nto present opportunities for abstraction in environments where no two\nsituations are exactly alike. In this work, we investigate approximate state\nabstractions, which treat nearly-identical situations as equivalent. We present\ntheoretical guarantees of the quality of behaviors derived from four types of\napproximate abstractions. Additionally, we empirically demonstrate that\napproximate abstractions lead to reduction in task complexity and bounded loss\nof optimality of behavior in a variety of environments.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 21:24:45 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Abel", "David", ""], ["Hershkowitz", "D. Ellis", ""], ["Littman", "Michael L.", ""]]}, {"id": "1701.04128", "submitter": "Wenjie Luo", "authors": "Wenjie Luo and Yujia Li and Raquel Urtasun and Richard Zemel", "title": "Understanding the Effective Receptive Field in Deep Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study characteristics of receptive fields of units in deep convolutional\nnetworks. The receptive field size is a crucial issue in many visual tasks, as\nthe output must respond to large enough areas in the image to capture\ninformation about large objects. We introduce the notion of an effective\nreceptive field, and show that it both has a Gaussian distribution and only\noccupies a fraction of the full theoretical receptive field. We analyze the\neffective receptive field in several architecture designs, and the effect of\nnonlinear activations, dropout, sub-sampling and skip connections on it. This\nleads to suggestions for ways to address its tendency to be too small.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 23:52:49 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 06:32:29 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Luo", "Wenjie", ""], ["Li", "Yujia", ""], ["Urtasun", "Raquel", ""], ["Zemel", "Richard", ""]]}, {"id": "1701.04143", "submitter": "Vahid Behzadan", "authors": "Vahid Behzadan and Arslan Munir", "title": "Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks", "comments": "14 pages, 5 figures, pre-print of submission to MLDM '17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning classifiers are known to be inherently vulnerable to\nmanipulation by intentionally perturbed inputs, named adversarial examples. In\nthis work, we establish that reinforcement learning techniques based on Deep\nQ-Networks (DQNs) are also vulnerable to adversarial input perturbations, and\nverify the transferability of adversarial examples across different DQN models.\nFurthermore, we present a novel class of attacks based on this vulnerability\nthat enable policy manipulation and induction in the learning process of DQNs.\nWe propose an attack mechanism that exploits the transferability of adversarial\nexamples to implement policy induction attacks on DQNs, and demonstrate its\nefficacy and impact through experimental study of a game-learning scenario.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 02:39:01 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Behzadan", "Vahid", ""], ["Munir", "Arslan", ""]]}, {"id": "1701.04222", "submitter": "Aristide Tossou", "authors": "Aristide C. Y. Tossou and Christos Dimitrakakis", "title": "Achieving Privacy in the Adversarial Multi-Armed Bandit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we improve the previously best known regret bound to achieve\n$\\epsilon$-differential privacy in oblivious adversarial bandits from\n$\\mathcal{O}{(T^{2/3}/\\epsilon)}$ to $\\mathcal{O}{(\\sqrt{T} \\ln T /\\epsilon)}$.\nThis is achieved by combining a Laplace Mechanism with EXP3. We show that\nthough EXP3 is already differentially private, it leaks a linear amount of\ninformation in $T$. However, we can improve this privacy by relying on its\nintrinsic exponential mechanism for selecting actions. This allows us to reach\n$\\mathcal{O}{(\\sqrt{\\ln T})}$-DP, with a regret of $\\mathcal{O}{(T^{2/3})}$\nthat holds against an adaptive adversary, an improvement from the best known of\n$\\mathcal{O}{(T^{3/4})}$. This is done by using an algorithm that run EXP3 in a\nmini-batch loop. Finally, we run experiments that clearly demonstrate the\nvalidity of our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 10:04:05 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Tossou", "Aristide C. Y.", ""], ["Dimitrakakis", "Christos", ""]]}, {"id": "1701.04238", "submitter": "Aristide Charles Yedia Tossou", "authors": "Aristide C. Y. Tossou, Christos Dimitrakakis, Devdatt Dubhashi", "title": "Thompson Sampling For Stochastic Bandits with Graph Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel extension of Thompson Sampling for stochastic sequential\ndecision problems with graph feedback, even when the graph structure itself is\nunknown and/or changing. We provide theoretical guarantees on the Bayesian\nregret of the algorithm, linking its performance to the underlying properties\nof the graph. Thompson Sampling has the advantage of being applicable without\nthe need to construct complicated upper confidence bounds for different\nproblems. We illustrate its performance through extensive experimental results\non real and simulated networks with graph feedback. More specifically, we\ntested our algorithms on power law, planted partitions and Erdo's-Renyi graphs,\nas well as on graphs derived from Facebook and Flixster data. These all show\nthat our algorithms clearly outperform related methods that employ upper\nconfidence bounds, even if the latter use more information about the graph.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 10:52:51 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Tossou", "Aristide C. Y.", ""], ["Dimitrakakis", "Christos", ""], ["Dubhashi", "Devdatt", ""]]}, {"id": "1701.04245", "submitter": "Xiaolei Ma", "authors": "Xiaolei Ma, Zhuang Dai, Zhengbing He, Jihui Na, Yong Wang and Yunpeng\n  Wang", "title": "Learning Traffic as Images: A Deep Convolutional Neural Network for\n  Large-Scale Transportation Network Speed Prediction", "comments": null, "journal-ref": "Sensors,2017", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a convolutional neural network (CNN)-based method that\nlearns traffic as images and predicts large-scale, network-wide traffic speed\nwith a high accuracy. Spatiotemporal traffic dynamics are converted to images\ndescribing the time and space relations of traffic flow via a two-dimensional\ntime-space matrix. A CNN is applied to the image following two consecutive\nsteps: abstract traffic feature extraction and network-wide traffic speed\nprediction. The effectiveness of the proposed method is evaluated by taking two\nreal-world transportation networks, the second ring road and north-east\ntransportation network in Beijing, as examples, and comparing the method with\nfour prevailing algorithms, namely, ordinary least squares, k-nearest\nneighbors, artificial neural network, and random forest, and three deep\nlearning architectures, namely, stacked autoencoder, recurrent neural network,\nand long-short-term memory network. The results show that the proposed method\noutperforms other algorithms by an average accuracy improvement of 42.91%\nwithin an acceptable execution time. The CNN can train the model in a\nreasonable time and, thus, is suitable for large-scale transportation networks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 11:22:38 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 10:41:03 GMT"}, {"version": "v3", "created": "Fri, 10 Feb 2017 00:55:34 GMT"}, {"version": "v4", "created": "Mon, 10 Apr 2017 06:25:18 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Ma", "Xiaolei", ""], ["Dai", "Zhuang", ""], ["He", "Zhengbing", ""], ["Na", "Jihui", ""], ["Wang", "Yong", ""], ["Wang", "Yunpeng", ""]]}, {"id": "1701.04249", "submitter": "Dmitry Yarotsky", "authors": "Dmitry Yarotsky", "title": "Geometric features for voxel-based surface recognition", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a library of geometric voxel features for CAD surface\nrecognition/retrieval tasks. Our features include local versions of the\nintrinsic volumes (the usual 3D volume, surface area, integrated mean and\nGaussian curvature) and a few closely related quantities. We also compute Haar\nwavelet and statistical distribution features by aggregating raw voxel\nfeatures. We apply our features to object classification on the ESB data set\nand demonstrate accurate results with a small number of shallow decision trees.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 11:30:31 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Yarotsky", "Dmitry", ""]]}, {"id": "1701.04271", "submitter": "Alon Gonen", "authors": "Alon Gonen and Shai Shalev-Shwartz", "title": "Fast Rates for Empirical Risk Minimization of Strict Saddle Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive bounds on the sample complexity of empirical risk minimization\n(ERM) in the context of minimizing non-convex risks that admit the strict\nsaddle property. Recent progress in non-convex optimization has yielded\nefficient algorithms for minimizing such functions. Our results imply that\nthese efficient algorithms are statistically stable and also generalize well.\nIn particular, we derive fast rates which resemble the bounds that are often\nattained in the strongly convex setting. We specify our bounds to Principal\nComponent Analysis and Independent Component Analysis. Our results and\ntechniques may pave the way for statistical analyses of additional strict\nsaddle problems.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 12:55:23 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 14:45:29 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 19:47:45 GMT"}, {"version": "v4", "created": "Sun, 4 Jun 2017 16:11:24 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Gonen", "Alon", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1701.04313", "submitter": "Kartik Audhkhasi", "authors": "Kartik Audhkhasi, Andrew Rosenberg, Abhinav Sethy, Bhuvana\n  Ramabhadran, Brian Kingsbury", "title": "End-to-End ASR-free Keyword Search from Speech", "comments": "Published in the IEEE 2017 International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2017), scheduled for 5-9 March 2017 in\n  New Orleans, Louisiana, USA", "journal-ref": null, "doi": "10.1109/JSTSP.2017.2759726", "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end (E2E) systems have achieved competitive results compared to\nconventional hybrid hidden Markov model (HMM)-deep neural network based\nautomatic speech recognition (ASR) systems. Such E2E systems are attractive due\nto the lack of dependence on alignments between input acoustic and output\ngrapheme or HMM state sequence during training. This paper explores the design\nof an ASR-free end-to-end system for text query-based keyword search (KWS) from\nspeech trained with minimal supervision. Our E2E KWS system consists of three\nsub-systems. The first sub-system is a recurrent neural network (RNN)-based\nacoustic auto-encoder trained to reconstruct the audio through a\nfinite-dimensional representation. The second sub-system is a character-level\nRNN language model using embeddings learned from a convolutional neural\nnetwork. Since the acoustic and text query embeddings occupy different\nrepresentation spaces, they are input to a third feed-forward neural network\nthat predicts whether the query occurs in the acoustic utterance or not. This\nE2E ASR-free KWS system performs respectably despite lacking a conventional ASR\nsystem and trains much faster.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 15:05:39 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Audhkhasi", "Kartik", ""], ["Rosenberg", "Andrew", ""], ["Sethy", "Abhinav", ""], ["Ramabhadran", "Bhuvana", ""], ["Kingsbury", "Brian", ""]]}, {"id": "1701.04355", "submitter": "Hadrien Bertrand", "authors": "Hadrien Bertrand, Matthieu Perrot, Roberto Ardon, Isabelle Bloch", "title": "Classification of MRI data using Deep Learning and Gaussian\n  Process-based Model Selection", "comments": "Accepted at ISBI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of MRI images according to the anatomical field of view is\na necessary task to solve when faced with the increasing quantity of medical\nimages. In parallel, advances in deep learning makes it a suitable tool for\ncomputer vision problems. Using a common architecture (such as AlexNet)\nprovides quite good results, but not sufficient for clinical use. Improving the\nmodel is not an easy task, due to the large number of hyper-parameters\ngoverning both the architecture and the training of the network, and to the\nlimited understanding of their relevance. Since an exhaustive search is not\ntractable, we propose to optimize the network first by random search, and then\nby an adaptive search based on Gaussian Processes and Probability of\nImprovement. Applying this method on a large and varied MRI dataset, we show a\nsubstantial improvement between the baseline network and the final one (up to\n20\\% for the most difficult classes).\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 17:02:31 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Bertrand", "Hadrien", ""], ["Perrot", "Matthieu", ""], ["Ardon", "Roberto", ""], ["Bloch", "Isabelle", ""]]}, {"id": "1701.04465", "submitter": "Aditya Sharma", "authors": "Aditya Sharma, Nikolas Wolfe, Bhiksha Raj", "title": "The Incredible Shrinking Neural Network: New Perspectives on Learning\n  Representations Through The Lens of Pruning", "comments": "30 pages, 36 figures, submission to ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How much can pruning algorithms teach us about the fundamentals of learning\nrepresentations in neural networks? And how much can these fundamentals help\nwhile devising new pruning techniques? A lot, it turns out. Neural network\npruning has become a topic of great interest in recent years, and many\ndifferent techniques have been proposed to address this problem. The decision\nof what to prune and when to prune necessarily forces us to confront our\nassumptions about how neural networks actually learn to represent patterns in\ndata. In this work, we set out to test several long-held hypotheses about\nneural network learning representations, approaches to pruning and the\nrelevance of one in the context of the other. To accomplish this, we argue in\nfavor of pruning whole neurons as opposed to the traditional method of pruning\nweights from optimally trained networks. We first review the historical\nliterature, point out some common assumptions it makes, and propose methods to\ndemonstrate the inherent flaws in these assumptions. We then propose our novel\napproach to pruning and set about analyzing the quality of the decisions it\nmakes. Our analysis led us to question the validity of many widely-held\nassumptions behind pruning algorithms and the trade-offs we often make in the\ninterest of reducing computational complexity. We discovered that there is a\nstraightforward way, however expensive, to serially prune 40-70% of the neurons\nin a trained network with minimal effect on the learning representation and\nwithout any re-training. It is to be noted here that the motivation behind this\nwork is not to propose an algorithm that would outperform all existing methods,\nbut to shed light on what some inherent flaws in these methods can teach us\nabout learning representations and how this can lead us to superior pruning\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 21:49:47 GMT"}, {"version": "v2", "created": "Sat, 25 Nov 2017 09:15:28 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Sharma", "Aditya", ""], ["Wolfe", "Nikolas", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1701.04489", "submitter": "Tapabrata Ghosh", "authors": "Tapabrata Ghosh", "title": "Towards a New Interpretation of Separable Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent times, the use of separable convolutions in deep convolutional\nneural network architectures has been explored. Several researchers, most\nnotably (Chollet, 2016) and (Ghosh, 2017) have used separable convolutions in\ntheir deep architectures and have demonstrated state of the art or close to\nstate of the art performance. However, the underlying mechanism of action of\nseparable convolutions are still not fully understood. Although their\nmathematical definition is well understood as a depthwise convolution followed\nby a pointwise convolution, deeper interpretations such as the extreme\nInception hypothesis (Chollet, 2016) have failed to provide a thorough\nexplanation of their efficacy. In this paper, we propose a hybrid\ninterpretation that we believe is a better model for explaining the efficacy of\nseparable convolutions.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 23:57:33 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Ghosh", "Tapabrata", ""]]}, {"id": "1701.04503", "submitter": "Garrett Goh", "authors": "Garrett B. Goh, Nathan O. Hodas, Abhinav Vishnu", "title": "Deep Learning for Computational Chemistry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CE cs.LG physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise and fall of artificial neural networks is well documented in the\nscientific literature of both computer science and computational chemistry. Yet\nalmost two decades later, we are now seeing a resurgence of interest in deep\nlearning, a machine learning algorithm based on multilayer neural networks.\nWithin the last few years, we have seen the transformative impact of deep\nlearning in many domains, particularly in speech recognition and computer\nvision, to the extent that the majority of expert practitioners in those field\nare now regularly eschewing prior established models in favor of deep learning\nmodels. In this review, we provide an introductory overview into the theory of\ndeep neural networks and their unique properties that distinguish them from\ntraditional machine learning algorithms used in cheminformatics. By providing\nan overview of the variety of emerging applications of deep neural networks, we\nhighlight its ubiquity and broad applicability to a wide range of challenges in\nthe field, including QSAR, virtual screening, protein structure prediction,\nquantum chemistry, materials design and property prediction. In reviewing the\nperformance of deep neural networks, we observed a consistent outperformance\nagainst non-neural networks state-of-the-art models across disparate research\ntopics, and deep neural network based models often exceeded the \"glass ceiling\"\nexpectations of their respective tasks. Coupled with the maturity of\nGPU-accelerated computing for training deep neural networks and the exponential\ngrowth of chemical data on which to train these networks on, we anticipate that\ndeep learning algorithms will be a valuable tool for computational chemistry.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 01:15:14 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Goh", "Garrett B.", ""], ["Hodas", "Nathan O.", ""], ["Vishnu", "Abhinav", ""]]}, {"id": "1701.04508", "submitter": "Chandan Gautam", "authors": "Chandan Gautam, Aruna Tiwari, Sundaram Suresh and Kapil Ahuja", "title": "Online Learning with Regularized Kernel for One-class Classification", "comments": "Paper has been submitted to special issue of IEEE Transactions on\n  Systems, Man and Cybernetics: Systems with Manuscript ID: SMCA-16-09-1033,\n  3rd submission ID: SMCA-18-03-0322", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an online learning with regularized kernel based\none-class extreme learning machine (ELM) classifier and is referred as online\nRK-OC-ELM. The baseline kernel hyperplane model considers whole data in a\nsingle chunk with regularized ELM approach for offline learning in case of\none-class classification (OCC). Further, the basic hyper plane model is adapted\nin an online fashion from stream of training samples in this paper. Two\nframeworks viz., boundary and reconstruction are presented to detect the target\nclass in online RKOC-ELM. Boundary framework based one-class classifier\nconsists of single node output architecture and classifier endeavors to\napproximate all data to any real number. However, one-class classifier based on\nreconstruction framework is an autoencoder architecture, where output nodes are\nidentical to input nodes and classifier endeavor to reconstruct input layer at\nthe output layer. Both these frameworks employ regularized kernel ELM based\nonline learning and consistency based model selection has been employed to\nselect learning algorithm parameters. The performance of online RK-OC-ELM has\nbeen evaluated on standard benchmark datasets as well as on artificial datasets\nand the results are compared with existing state-of-the art one-class\nclassifiers. The results indicate that the online learning one-class classifier\nis slightly better or same as batch learning based approaches. As, base\nclassifier used for the proposed classifiers are based on the ELM, hence,\nproposed classifiers would also inherit the benefit of the base classifier i.e.\nit will perform faster computation compared to traditional autoencoder based\none-class classifier.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 01:40:07 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 05:29:03 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Gautam", "Chandan", ""], ["Tiwari", "Aruna", ""], ["Suresh", "Sundaram", ""], ["Ahuja", "Kapil", ""]]}, {"id": "1701.04516", "submitter": "Chandan Gautam", "authors": "Chandan Gautam, Aruna Tiwari and Qian Leng", "title": "On The Construction of Extreme Learning Machine for Online and Offline\n  One-Class Classification - An Expanded Toolbox", "comments": "This paper has been accepted in Neurocomputing Journal (Elsevier)\n  with Manuscript id: NEUCOM-D-15-02856", "journal-ref": null, "doi": "10.1016/j.neucom.2016.04.070", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-Class Classification (OCC) has been prime concern for researchers and\neffectively employed in various disciplines. But, traditional methods based\none-class classifiers are very time consuming due to its iterative process and\nvarious parameters tuning. In this paper, we present six OCC methods based on\nextreme learning machine (ELM) and Online Sequential ELM (OSELM). Our proposed\nclassifiers mainly lie in two categories: reconstruction based and boundary\nbased, which supports both types of learning viz., online and offline learning.\nOut of various proposed methods, four are offline and remaining two are online\nmethods. Out of four offline methods, two methods perform random feature\nmapping and two methods perform kernel feature mapping. Kernel feature mapping\nbased approaches have been tested with RBF kernel and online version of\none-class classifiers are tested with both types of nodes viz., additive and\nRBF. It is well known fact that threshold decision is a crucial factor in case\nof OCC, so, three different threshold deciding criteria have been employed so\nfar and analyses the effectiveness of one threshold deciding criteria over\nanother. Further, these methods are tested on two artificial datasets to check\nthere boundary construction capability and on eight benchmark datasets from\ndifferent discipline to evaluate the performance of the classifiers. Our\nproposed classifiers exhibit better performance compared to ten traditional\none-class classifiers and ELM based two one-class classifiers. Through proposed\none-class classifiers, we intend to expand the functionality of the most used\ntoolbox for OCC i.e. DD toolbox. All of our methods are totally compatible with\nall the present features of the toolbox.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 02:55:51 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Gautam", "Chandan", ""], ["Tiwari", "Aruna", ""], ["Leng", "Qian", ""]]}, {"id": "1701.04518", "submitter": "Rohitash Chandra", "authors": "Rohitash Chandra", "title": "Towards prediction of rapid intensification in tropical cyclones with\n  recurrent neural networks", "comments": "Technical Report: Artificial Intelligence and Cybernetics Research\n  Group, Software Foundation, Nausori, Fiji", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem where a tropical cyclone intensifies dramatically within a short\nperiod of time is known as rapid intensification. This has been one of the\nmajor challenges for tropical weather forecasting. Recurrent neural networks\nhave been promising for time series problems which makes them appropriate for\nrapid intensification. In this paper, recurrent neural networks are used to\npredict rapid intensification cases of tropical cyclones from the South Pacific\nand South Indian Ocean regions. A class imbalanced problem is encountered which\nmakes it very challenging to achieve promising performance. A simple strategy\nwas proposed to include more positive cases for detection where the false\npositive rate was slightly improved. The limitations of building an efficient\nsystem remains due to the challenges of addressing the class imbalance problem\nencountered for rapid intensification prediction. This motivates further\nresearch in using innovative machine learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 03:08:12 GMT"}], "update_date": "2017-02-12", "authors_parsed": [["Chandra", "Rohitash", ""]]}, {"id": "1701.04600", "submitter": "Amit Awekar", "authors": "Siddhesh Khandelwal, Amit Awekar", "title": "Faster K-Means Cluster Estimation", "comments": "6 pages, Accepted at ECIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been considerable work on improving popular clustering algorithm\n`K-means' in terms of mean squared error (MSE) and speed, both. However, most\nof the k-means variants tend to compute distance of each data point to each\ncluster centroid for every iteration. We propose a fast heuristic to overcome\nthis bottleneck with only marginal increase in MSE. We observe that across all\niterations of K-means, a data point changes its membership only among a small\nsubset of clusters. Our heuristic predicts such clusters for each data point by\nlooking at nearby clusters after the first iteration of k-means. We augment\nwell known variants of k-means with our heuristic to demonstrate effectiveness\nof our heuristic. For various synthetic and real-world datasets, our heuristic\nachieves speed-up of up-to 3 times when compared to efficient variants of\nk-means.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 10:00:51 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Khandelwal", "Siddhesh", ""], ["Awekar", "Amit", ""]]}, {"id": "1701.04693", "submitter": "Sepehr Valipour", "authors": "Sepehr Valipour, Camilo Perez, Martin Jagersand", "title": "Incremental Learning for Robot Perception through HRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene understanding and object recognition is a difficult to achieve yet\ncrucial skill for robots. Recently, Convolutional Neural Networks (CNN), have\nshown success in this task. However, there is still a gap between their\nperformance on image datasets and real-world robotics scenarios. We present a\nnovel paradigm for incrementally improving a robot's visual perception through\nactive human interaction. In this paradigm, the user introduces novel objects\nto the robot by means of pointing and voice commands. Given this information,\nthe robot visually explores the object and adds images from it to re-train the\nperception module. Our base perception module is based on recent development in\nobject detection and recognition using deep learning. Our method leverages\nstate of the art CNNs from off-line batch learning, human guidance, robot\nexploration and incremental on-line learning.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 14:29:05 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Valipour", "Sepehr", ""], ["Perez", "Camilo", ""], ["Jagersand", "Martin", ""]]}, {"id": "1701.04722", "submitter": "Lars Mescheder", "authors": "Lars Mescheder, Sebastian Nowozin and Andreas Geiger", "title": "Adversarial Variational Bayes: Unifying Variational Autoencoders and\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Autoencoders (VAEs) are expressive latent variable models that\ncan be used to learn complex probability distributions from training data.\nHowever, the quality of the resulting model crucially relies on the\nexpressiveness of the inference model. We introduce Adversarial Variational\nBayes (AVB), a technique for training Variational Autoencoders with arbitrarily\nexpressive inference models. We achieve this by introducing an auxiliary\ndiscriminative network that allows to rephrase the maximum-likelihood-problem\nas a two-player game, hence establishing a principled connection between VAEs\nand Generative Adversarial Networks (GANs). We show that in the nonparametric\nlimit our method yields an exact maximum-likelihood assignment for the\nparameters of the generative model, as well as the exact posterior distribution\nover the latent variables given an observation. Contrary to competing\napproaches which combine VAEs with GANs, our approach has a clear theoretical\njustification, retains most advantages of standard Variational Autoencoders and\nis easy to implement.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 15:18:31 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 13:46:01 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 12:32:57 GMT"}, {"version": "v4", "created": "Mon, 11 Jun 2018 12:19:02 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Mescheder", "Lars", ""], ["Nowozin", "Sebastian", ""], ["Geiger", "Andreas", ""]]}, {"id": "1701.04724", "submitter": "Alexander Jung", "authors": "Nguyen Q. Tran and Oleksii Abramenko and Alexander Jung", "title": "On the Sample Complexity of Graphical Model Selection for Non-Stationary\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the sample size required for accurate graphical model\nselection from non-stationary samples. The observed data is modeled as a\nvector-valued zero-mean Gaussian random process whose samples are uncorrelated\nbut have different covariance matrices. This model contains as special cases\nthe standard setting of i.i.d. samples as well as the case of samples forming a\nstationary or underspread (non-stationary) processes. More generally, our model\napplies to any process model for which an efficient decorrelation can be\nobtained. By analyzing a particular model selection method, we derive a\nsufficient condition on the required sample size for accurate graphical model\nselection based on non-stationary data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 15:19:44 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 08:14:21 GMT"}, {"version": "v3", "created": "Wed, 9 May 2018 10:37:23 GMT"}, {"version": "v4", "created": "Sun, 26 May 2019 14:47:03 GMT"}, {"version": "v5", "created": "Thu, 27 Jun 2019 13:10:36 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Tran", "Nguyen Q.", ""], ["Abramenko", "Oleksii", ""], ["Jung", "Alexander", ""]]}, {"id": "1701.04739", "submitter": "Octavian Suciu", "authors": "Rock Stevens, Octavian Suciu, Andrew Ruef, Sanghyun Hong, Michael\n  Hicks, Tudor Dumitra\\c{s}", "title": "Summoning Demons: The Pursuit of Exploitable Bugs in Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Governments and businesses increasingly rely on data analytics and machine\nlearning (ML) for improving their competitive edge in areas such as consumer\nsatisfaction, threat intelligence, decision making, and product efficiency.\nHowever, by cleverly corrupting a subset of data used as input to a target's ML\nalgorithms, an adversary can perturb outcomes and compromise the effectiveness\nof ML technology. While prior work in the field of adversarial machine learning\nhas studied the impact of input manipulation on correct ML algorithms, we\nconsider the exploitation of bugs in ML implementations. In this paper, we\ncharacterize the attack surface of ML programs, and we show that malicious\ninputs exploiting implementation bugs enable strictly more powerful attacks\nthan the classic adversarial machine learning techniques. We propose a\nsemi-automated technique, called steered fuzzing, for exploring this attack\nsurface and for discovering exploitable bugs in machine learning programs, in\norder to demonstrate the magnitude of this threat. As a result of our work, we\nresponsibly disclosed five vulnerabilities, established three new CVE-IDs, and\nilluminated a common insecure practice across many machine learning systems.\nFinally, we outline several research directions for further understanding and\nmitigating this threat.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 15:59:17 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Stevens", "Rock", ""], ["Suciu", "Octavian", ""], ["Ruef", "Andrew", ""], ["Hong", "Sanghyun", ""], ["Hicks", "Michael", ""], ["Dumitra\u015f", "Tudor", ""]]}, {"id": "1701.04783", "submitter": "Lei Zheng", "authors": "Lei Zheng, Vahid Noroozi, Philip S. Yu", "title": "Joint Deep Modeling of Users and Items Using Reviews for Recommendation", "comments": "WSDM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A large amount of information exists in reviews written by users. This source\nof information has been ignored by most of the current recommender systems\nwhile it can potentially alleviate the sparsity problem and improve the quality\nof recommendations. In this paper, we present a deep model to learn item\nproperties and user behaviors jointly from review text. The proposed model,\nnamed Deep Cooperative Neural Networks (DeepCoNN), consists of two parallel\nneural networks coupled in the last layers. One of the networks focuses on\nlearning user behaviors exploiting reviews written by the user, and the other\none learns item properties from the reviews written for the item. A shared\nlayer is introduced on the top to couple these two networks together. The\nshared layer enables latent factors learned for users and items to interact\nwith each other in a manner similar to factorization machine techniques.\nExperimental results demonstrate that DeepCoNN significantly outperforms all\nbaseline recommender systems on a variety of datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 17:46:04 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Zheng", "Lei", ""], ["Noroozi", "Vahid", ""], ["Yu", "Philip S.", ""]]}, {"id": "1701.04862", "submitter": "Martin Arjovsky", "authors": "Martin Arjovsky, L\\'eon Bottou", "title": "Towards Principled Methods for Training Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is not to introduce a single algorithm or method, but\nto make theoretical steps towards fully understanding the training dynamics of\ngenerative adversarial networks. In order to substantiate our theoretical\nanalysis, we perform targeted experiments to verify our assumptions, illustrate\nour claims, and quantify the phenomena. This paper is divided into three\nsections. The first section introduces the problem at hand. The second section\nis dedicated to studying and proving rigorously the problems including\ninstability and saturation that arize when training generative adversarial\nnetworks. The third section examines a practical and theoretically grounded\ndirection towards solving these problems, while introducing new tools to study\nthem.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 20:46:21 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Arjovsky", "Martin", ""], ["Bottou", "L\u00e9on", ""]]}, {"id": "1701.04869", "submitter": "Samuel Kadoury", "authors": "Samuel Kadoury, William Mandel, Marjolaine Roy-Beaudry, Marie-Lyne\n  Nault, Stefan Parent", "title": "3D Morphology Prediction of Progressive Spinal Deformities from\n  Probabilistic Modeling of Discriminant Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach for predicting the progression of adolescent\nidiopathic scoliosis from 3D spine models reconstructed from biplanar X-ray\nimages. Recent progress in machine learning have allowed to improve\nclassification and prognosis rates, but lack a probabilistic framework to\nmeasure uncertainty in the data. We propose a discriminative probabilistic\nmanifold embedding where locally linear mappings transform data points from\nhigh-dimensional space to corresponding low-dimensional coordinates. A\ndiscriminant adjacency matrix is constructed to maximize the separation between\nprogressive and non-progressive groups of patients diagnosed with scoliosis,\nwhile minimizing the distance in latent variables belonging to the same class.\nTo predict the evolution of deformation, a baseline reconstruction is projected\nonto the manifold, from which a spatiotemporal regression model is built from\nparallel transport curves inferred from neighboring exemplars. Rate of\nprogression is modulated from the spine flexibility and curve magnitude of the\n3D spine deformation. The method was tested on 745 reconstructions from 133\nsubjects using longitudinal 3D reconstructions of the spine, with results\ndemonstrating the discriminatory framework can identify between progressive and\nnon-progressive of scoliotic patients with a classification rate of 81% and\nprediction differences of 2.1$^{o}$ in main curve angulation, outperforming\nother manifold learning methods. Our method achieved a higher prediction\naccuracy and improved the modeling of spatiotemporal morphological changes in\nhighly deformed spines compared to other learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 21:15:56 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 12:37:29 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Kadoury", "Samuel", ""], ["Mandel", "William", ""], ["Roy-Beaudry", "Marjolaine", ""], ["Nault", "Marie-Lyne", ""], ["Parent", "Stefan", ""]]}, {"id": "1701.04926", "submitter": "Chung Chan", "authors": "Chung Chan, Ali Al-Bashabsheh, Qiaoqiao Zhou", "title": "Agglomerative Info-Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An agglomerative clustering of random variables is proposed, where clusters\nof random variables sharing the maximum amount of multivariate mutual\ninformation are merged successively to form larger clusters. Compared to the\nprevious info-clustering algorithms, the agglomerative approach allows the\ncomputation to stop earlier when clusters of desired size and accuracy are\nobtained. An efficient algorithm is also derived based on the submodularity of\nentropy and the duality between the principal sequence of partitions and the\nprincipal sequence for submodular functions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 02:18:08 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 13:37:13 GMT"}, {"version": "v3", "created": "Fri, 24 Feb 2017 13:25:40 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Chan", "Chung", ""], ["Al-Bashabsheh", "Ali", ""], ["Zhou", "Qiaoqiao", ""]]}, {"id": "1701.04944", "submitter": "Hemant Ishwaran", "authors": "Min Lu and Hemant Ishwaran", "title": "A Machine Learning Alternative to P-values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an alternative approach to p-values in regression\nsettings. This approach, whose origins can be traced to machine learning, is\nbased on the leave-one-out bootstrap for prediction error. In machine learning\nthis is called the out-of-bag (OOB) error. To obtain the OOB error for a model,\none draws a bootstrap sample and fits the model to the in-sample data. The\nout-of-sample prediction error for the model is obtained by calculating the\nprediction error for the model using the out-of-sample data. Repeating and\naveraging yields the OOB error, which represents a robust cross-validated\nestimate of the accuracy of the underlying model. By a simple modification to\nthe bootstrap data involving \"noising up\" a variable, the OOB method yields a\nvariable importance (VIMP) index, which directly measures how much a specific\nvariable contributes to the prediction precision of a model. VIMP provides a\nscientifically interpretable measure of the effect size of a variable, we call\nthe \"predictive effect size\", that holds whether the researcher's model is\ncorrect or not, unlike the p-value whose calculation is based on the assumed\ncorrectness of the model. We also discuss a marginal VIMP index, also easily\ncalculated, which measures the marginal effect of a variable, or what we call\n\"the discovery effect\". The OOB procedure can be applied to both parametric and\nnonparametric regression models and requires only that the researcher can\nrepeatedly fit their model to bootstrap and modified bootstrap data. We\nillustrate this approach on a survival data set involving patients with\nsystolic heart failure and to a simulated survival data set where the model is\nincorrectly specified to illustrate its robustness to model misspecification.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 05:07:03 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 01:59:00 GMT"}, {"version": "v3", "created": "Sat, 21 Jan 2017 15:09:41 GMT"}, {"version": "v4", "created": "Wed, 25 Jan 2017 00:01:13 GMT"}, {"version": "v5", "created": "Mon, 20 Feb 2017 19:12:33 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Lu", "Min", ""], ["Ishwaran", "Hemant", ""]]}, {"id": "1701.04949", "submitter": "Volodymyr Turchenko", "authors": "Volodymyr Turchenko, Eric Chalmers, Artur Luczak", "title": "A Deep Convolutional Auto-Encoder with Pooling - Unpooling Layers in\n  Caffe", "comments": "21 pages, 11 figures, 5 tables, 62 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the development of several models of a deep convolutional\nauto-encoder in the Caffe deep learning framework and their experimental\nevaluation on the example of MNIST dataset. We have created five models of a\nconvolutional auto-encoder which differ architecturally by the presence or\nabsence of pooling and unpooling layers in the auto-encoder's encoder and\ndecoder parts. Our results show that the developed models provide very good\nresults in dimensionality reduction and unsupervised clustering tasks, and\nsmall classification errors when we used the learned internal code as an input\nof a supervised linear classifier and multi-layer perceptron. The best results\nwere provided by a model where the encoder part contains convolutional and\npooling layers, followed by an analogous decoder part with deconvolution and\nunpooling layers without the use of switch variables in the decoder part. The\npaper also discusses practical details of the creation of a deep convolutional\nauto-encoder in the very popular Caffe deep learning framework. We believe that\nour approach and results presented in this paper could help other researchers\nto build efficient deep neural network architectures in the future.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 05:24:24 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Turchenko", "Volodymyr", ""], ["Chalmers", "Eric", ""], ["Luczak", "Artur", ""]]}, {"id": "1701.04968", "submitter": "Peng Zhao", "authors": "Zhao Peng", "title": "Multilayer Perceptron Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Networks(ANN) has been phenomenally successful on various\npattern recognition tasks. However, the design of neural networks rely heavily\non the experience and intuitions of individual developers. In this article, the\nauthor introduces a mathematical structure called MLP algebra on the set of all\nMultilayer Perceptron Neural Networks(MLP), which can serve as a guiding\nprinciple to build MLPs accommodating to the particular data sets, and to build\ncomplex MLPs from simpler ones.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 06:49:03 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Peng", "Zhao", ""]]}, {"id": "1701.05053", "submitter": "Burak Civek", "authors": "Burak C. Civek, Ibrahim Delibalta and Suleyman S. Kozat", "title": "Highly Efficient Hierarchical Online Nonlinear Regression Using Second\n  Order Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce highly efficient online nonlinear regression algorithms that are\nsuitable for real life applications. We process the data in a truly online\nmanner such that no storage is needed, i.e., the data is discarded after being\nused. For nonlinear modeling we use a hierarchical piecewise linear approach\nbased on the notion of decision trees where the space of the regressor vectors\nis adaptively partitioned based on the performance. As the first time in the\nliterature, we learn both the piecewise linear partitioning of the regressor\nspace as well as the linear models in each region using highly effective second\norder methods, i.e., Newton-Raphson Methods. Hence, we avoid the well known\nover fitting issues by using piecewise linear models, however, since both the\nregion boundaries as well as the linear models in each region are trained using\nthe second order methods, we achieve substantial performance compared to the\nstate of the art. We demonstrate our gains over the well known benchmark data\nsets and provide performance results in an individual sequence manner\nguaranteed to hold without any statistical assumptions. Hence, the introduced\nalgorithms address computational complexity issues widely encountered in real\nlife applications while providing superior guaranteed performance in a strong\ndeterministic sense.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 13:23:21 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Civek", "Burak C.", ""], ["Delibalta", "Ibrahim", ""], ["Kozat", "Suleyman S.", ""]]}, {"id": "1701.05217", "submitter": "Radu Balan", "authors": "Radu Balan, Maneesh Singh, Dongmian Zou", "title": "Lipschitz Properties for Deep Convolutional Networks", "comments": "25 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss the stability properties of convolutional neural\nnetworks. Convolutional neural networks are widely used in machine learning. In\nclassification they are mainly used as feature extractors. Ideally, we expect\nsimilar features when the inputs are from the same class. That is, we hope to\nsee a small change in the feature vector with respect to a deformation on the\ninput signal. This can be established mathematically, and the key step is to\nderive the Lipschitz properties. Further, we establish that the stability\nresults can be extended for more general networks. We give a formula for\ncomputing the Lipschitz bound, and compare it with other methods to show it is\ncloser to the optimal value.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 19:51:28 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Balan", "Radu", ""], ["Singh", "Maneesh", ""], ["Zou", "Dongmian", ""]]}, {"id": "1701.05221", "submitter": "Nikolaos Fragoulis Dr", "authors": "I. Theodorakopoulos, V. Pothos, D. Kastaniotis and N. Fragoulis", "title": "Parsimonious Inference on Convolutional Neural Networks: Learning and\n  applying on-line kernel activation rules", "comments": "17 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new, radical CNN design approach is presented in this paper, considering\nthe reduction of the total computational load during inference. This is\nachieved by a new holistic intervention on both the CNN architecture and the\ntraining procedure, which targets to the parsimonious inference by learning to\nexploit or remove the redundant capacity of a CNN architecture. This is\naccomplished, by the introduction of a new structural element that can be\ninserted as an add-on to any contemporary CNN architecture, whilst preserving\nor even improving its recognition accuracy. Our approach formulates a\nsystematic and data-driven method for developing CNNs that are trained to\neventually change size and form in real-time during inference, targeting to the\nsmaller possible computational footprint. Results are provided for the optimal\nimplementation on a few modern, high-end mobile computing platforms indicating\na significant speed-up of up to x3 times.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 20:03:12 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 06:43:02 GMT"}, {"version": "v3", "created": "Wed, 25 Jan 2017 08:57:29 GMT"}, {"version": "v4", "created": "Thu, 26 Jan 2017 08:58:52 GMT"}, {"version": "v5", "created": "Tue, 31 Jan 2017 12:15:43 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Theodorakopoulos", "I.", ""], ["Pothos", "V.", ""], ["Kastaniotis", "D.", ""], ["Fragoulis", "N.", ""]]}, {"id": "1701.05228", "submitter": "Konstantina Christakopoulou", "authors": "Konstantina Christakopoulou, Jaya Kawale, Arindam Banerjee", "title": "Recommendation under Capacity Constraints", "comments": "Extended methods section and experimental section to include bayesian\n  personalized ranking objective as well", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the common scenario where every candidate item\nfor recommendation is characterized by a maximum capacity, i.e., number of\nseats in a Point-of-Interest (POI) or size of an item's inventory. Despite the\nprevalence of the task of recommending items under capacity constraints in a\nvariety of settings, to the best of our knowledge, none of the known\nrecommender methods is designed to respect capacity constraints. To close this\ngap, we extend three state-of-the art latent factor recommendation approaches:\nprobabilistic matrix factorization (PMF), geographical matrix factorization\n(GeoMF), and bayesian personalized ranking (BPR), to optimize for both\nrecommendation accuracy and expected item usage that respects the capacity\nconstraints. We introduce the useful concepts of user propensity to listen and\nitem capacity. Our experimental results in real-world datasets, both for the\ndomain of item recommendation and POI recommendation, highlight the benefit of\nour method for the setting of recommendation under capacity constraints.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 20:45:57 GMT"}, {"version": "v2", "created": "Sun, 12 Mar 2017 23:33:18 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Christakopoulou", "Konstantina", ""], ["Kawale", "Jaya", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1701.05265", "submitter": "Wilson Hsu", "authors": "Wilson Hsu, Agastya Kalra, Pascal Poupart", "title": "Online Structure Learning for Sum-Product Networks with Gaussian Leaves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sum-product networks have recently emerged as an attractive representation\ndue to their dual view as a special type of deep neural network with clear\nsemantics and a special type of probabilistic graphical model for which\ninference is always tractable. Those properties follow from some conditions\n(i.e., completeness and decomposability) that must be respected by the\nstructure of the network. As a result, it is not easy to specify a valid\nsum-product network by hand and therefore structure learning techniques are\ntypically used in practice. This paper describes the first online structure\nlearning technique for continuous SPNs with Gaussian leaves. We also introduce\nan accompanying new parameter learning technique.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 00:42:01 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Hsu", "Wilson", ""], ["Kalra", "Agastya", ""], ["Poupart", "Pascal", ""]]}, {"id": "1701.05335", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Mieczys{\\l}aw A. K{\\l}opotek", "title": "Validity of Clusters Produced By kernel-$k$-means With Kernel-Trick", "comments": "27 pages", "journal-ref": "an extension of the paper in Foundations of Intelligent Systems.,\n  LNCS vol 10352. Springer, Cham, pp. 97-104 (2017)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper corrects the proof of the Theorem 2 from the Gower's paper\n\\cite[page 5]{Gower:1982} as well as corrects the Theorem 7 from Gower's paper\n\\cite{Gower:1986}. The first correction is needed in order to establish the\nexistence of the kernel function used commonly in the kernel trick e.g. for\n$k$-means clustering algorithm, on the grounds of distance matrix. The\ncorrection encompasses the missing if-part proof and dropping unnecessary\nconditions. The second correction deals with transformation of the kernel\nmatrix into a one embeddable in Euclidean space.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 08:55:20 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 11:59:52 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 12:00:27 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["K\u0142opotek", "Mieczys\u0142aw A.", ""]]}, {"id": "1701.05363", "submitter": "Arthur Mensch", "authors": "Arthur Mensch (PARIETAL, NEUROSPIN), Julien Mairal (Thoth), Bertrand\n  Thirion (PARIETAL, NEUROSPIN), Gael Varoquaux (NEUROSPIN, PARIETAL)", "title": "Stochastic Subsampling for Factorizing Huge Matrices", "comments": "IEEE Transactions on Signal Processing, Institute of Electrical and\n  Electronics Engineers, A Para\\^itre", "journal-ref": "IEEE Transactions on Signal Processing, 2018, 66 (1), pp 113-128", "doi": "10.1109/TSP.2017.2752697", "report-no": null, "categories": "stat.ML cs.LG math.OC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a matrix-factorization algorithm that scales to input matrices\nwith both huge number of rows and columns. Learned factors may be sparse or\ndense and/or non-negative, which makes our algorithm suitable for dictionary\nlearning, sparse component analysis, and non-negative matrix factorization. Our\nalgorithm streams matrix columns while subsampling them to iteratively learn\nthe matrix factors. At each iteration, the row dimension of a new sample is\nreduced by subsampling, resulting in lower time complexity compared to a simple\nstreaming algorithm. Our method comes with convergence guarantees to reach a\nstationary point of the matrix-factorization problem. We demonstrate its\nefficiency on massive functional Magnetic Resonance Imaging data (2 TB), and on\npatches extracted from hyperspectral images (103 GB). For both problems, which\ninvolve different penalties on rows and columns, we obtain significant\nspeed-ups compared to state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 10:35:01 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 14:29:34 GMT"}, {"version": "v3", "created": "Mon, 30 Oct 2017 09:24:27 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Mensch", "Arthur", "", "PARIETAL, NEUROSPIN"], ["Mairal", "Julien", "", "Thoth"], ["Thirion", "Bertrand", "", "PARIETAL, NEUROSPIN"], ["Varoquaux", "Gael", "", "NEUROSPIN, PARIETAL"]]}, {"id": "1701.05369", "submitter": "Dmitry Molchanov", "authors": "Dmitry Molchanov, Arsenii Ashukha and Dmitry Vetrov", "title": "Variational Dropout Sparsifies Deep Neural Networks", "comments": "Published in ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a recently proposed Variational Dropout technique that provided an\nelegant Bayesian interpretation to Gaussian Dropout. We extend Variational\nDropout to the case when dropout rates are unbounded, propose a way to reduce\nthe variance of the gradient estimator and report first experimental results\nwith individual dropout rates per weight. Interestingly, it leads to extremely\nsparse solutions both in fully-connected and convolutional layers. This effect\nis similar to automatic relevance determination effect in empirical Bayes but\nhas a number of advantages. We reduce the number of parameters up to 280 times\non LeNet architectures and up to 68 times on VGG-like networks with a\nnegligible decrease of accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 10:44:55 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 20:43:27 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 11:01:55 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Molchanov", "Dmitry", ""], ["Ashukha", "Arsenii", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "1701.05487", "submitter": "Martin Grohe", "authors": "Martin Grohe and Martin Ritzert", "title": "Learning first-order definable concepts over structures of small degree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a declarative framework for machine learning where concepts and\nhypotheses are defined by formulas of a logic over some background structure.\nWe show that within this framework, concepts defined by first-order formulas\nover a background structure of at most polylogarithmic degree can be learned in\npolylogarithmic time in the \"probably approximately correct\" learning sense.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 15:48:11 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Grohe", "Martin", ""], ["Ritzert", "Martin", ""]]}, {"id": "1701.05512", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "Fisher consistency for prior probability shift", "comments": "28 pages, 2 figures, 8 tables, introduction extended", "journal-ref": "Journal of Machine Learning Research 18, 3338-3369, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Fisher consistency in the sense of unbiasedness as a desirable\nproperty for estimators of class prior probabilities. Lack of Fisher\nconsistency could be used as a criterion to dismiss estimators that are\nunlikely to deliver precise estimates in test datasets under prior probability\nand more general dataset shift. The usefulness of this unbiasedness concept is\ndemonstrated with three examples of classifiers used for quantification:\nAdjusted Classify & Count, EM-algorithm and CDE-Iterate. We find that Adjusted\nClassify & Count and EM-algorithm are Fisher consistent. A counter-example\nshows that CDE-Iterate is not Fisher consistent and, therefore, cannot be\ntrusted to deliver reliable estimates of class probabilities.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 17:07:21 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 20:04:39 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "1701.05517", "submitter": "Tim Salimans", "authors": "Tim Salimans, Andrej Karpathy, Xi Chen, Diederik P. Kingma", "title": "PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture\n  Likelihood and Other Modifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PixelCNNs are a recently proposed class of powerful generative models with\ntractable likelihood. Here we discuss our implementation of PixelCNNs which we\nmake available at https://github.com/openai/pixel-cnn. Our implementation\ncontains a number of modifications to the original model that both simplify its\nstructure and improve its performance. 1) We use a discretized logistic mixture\nlikelihood on the pixels, rather than a 256-way softmax, which we find to speed\nup training. 2) We condition on whole pixels, rather than R/G/B sub-pixels,\nsimplifying the model structure. 3) We use downsampling to efficiently capture\nstructure at multiple resolutions. 4) We introduce additional short-cut\nconnections to further speed up optimization. 5) We regularize the model using\ndropout. Finally, we present state-of-the-art log likelihood results on\nCIFAR-10 to demonstrate the usefulness of these modifications.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 17:29:06 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Salimans", "Tim", ""], ["Karpathy", "Andrej", ""], ["Chen", "Xi", ""], ["Kingma", "Diederik P.", ""]]}, {"id": "1701.05549", "submitter": "Krzysztof Cios", "authors": "Krzysztof J. Cios", "title": "Deep Neural Networks - A Brief History", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction to deep neural networks and their history.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 18:43:56 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Cios", "Krzysztof J.", ""]]}, {"id": "1701.05573", "submitter": "Aaron Schein", "authors": "Aaron Schein, Mingyuan Zhou, Hanna Wallach", "title": "Poisson--Gamma Dynamical Systems", "comments": "Appeared in the Proceedings of the 29th Advances in Neural\n  Information Processing Systems (NIPS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new dynamical system for sequentially observed multivariate\ncount data. This model is based on the gamma--Poisson construction---a natural\nchoice for count data---and relies on a novel Bayesian nonparametric prior that\nties and shrinks the model parameters, thus avoiding overfitting. We present an\nefficient MCMC inference algorithm that advances recent work on augmentation\nschemes for inference in negative binomial models. Finally, we demonstrate the\nmodel's inductive bias using a variety of real-world data sets, showing that it\nexhibits superior predictive performance over other models and infers highly\ninterpretable latent structure.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 19:28:37 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Schein", "Aaron", ""], ["Zhou", "Mingyuan", ""], ["Wallach", "Hanna", ""]]}, {"id": "1701.05644", "submitter": "Yunlong Wang", "authors": "Yong Cai, Yunlong Wang, Dong Dai", "title": "Rare Disease Physician Targeting: A Factor Graph Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In rare disease physician targeting, a major challenge is how to identify\nphysicians who are treating diagnosed or underdiagnosed rare diseases patients.\nRare diseases have extremely low incidence rate. For a specified rare disease,\nonly a small number of patients are affected and a fractional of physicians are\ninvolved. The existing targeting methodologies, such as segmentation and\nprofiling, are developed under mass market assumption. They are not suitable\nfor rare disease market where the target classes are extremely imbalanced. The\nauthors propose a graphical model approach to predict targets by jointly\nmodeling physician and patient features from different data spaces and\nutilizing the extra relational information. Through an empirical example with\nmedical claim and prescription data, the proposed approach demonstrates better\naccuracy in finding target physicians. The graph representation also provides\nvisual interpretability of relationship among physicians and patients. The\nmodel can be extended to incorporate more complex dependency structures. This\narticle contributes to the literature of exploring the benefit of utilizing\nrelational dependencies among entities in healthcare industry.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 23:43:54 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Cai", "Yong", ""], ["Wang", "Yunlong", ""], ["Dai", "Dong", ""]]}, {"id": "1701.05681", "submitter": "Edwin Dauber Jr.", "authors": "Edwin Dauber, Aylin Caliskan, Richard Harang, Gregory Shearer, Michael\n  Weisman, Frederica Nelson, Rachel Greenstadt", "title": "Git Blame Who?: Stylistic Authorship Attribution of Small, Incomplete\n  Source Code Fragments", "comments": null, "journal-ref": "Dauber, E., Caliskan, A., Harang, R., et al. (2019). Git Blame\n  Who?: Stylistic Authorship Attribution of Small, Incomplete Source Code\n  Fragments. Proceedings on Privacy Enhancing Technologies, 2019(3), pp.\n  389-408", "doi": "10.2478/popets-2019-0053", "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program authorship attribution has implications for the privacy of\nprogrammers who wish to contribute code anonymously. While previous work has\nshown that complete files that are individually authored can be attributed, we\nshow here for the first time that accounts belonging to open source\ncontributors containing short, incomplete, and typically uncompilable fragments\ncan also be effectively attributed.\n  We propose a technique for authorship attribution of contributor accounts\ncontaining small source code samples, such as those that can be obtained from\nversion control systems or other direct comparison of sequential versions. We\nshow that while application of previous methods to individual small source code\nsamples yields an accuracy of about 73% for 106 programmers as a baseline, by\nensembling and averaging the classification probabilities of a sufficiently\nlarge set of samples belonging to the same author we achieve 99% accuracy for\nassigning the set of samples to the correct author. Through these results, we\ndemonstrate that attribution is an important threat to privacy for programmers\neven in real-world collaborative environments such as GitHub. Additionally, we\npropose the use of calibration curves to identify samples by unknown and\npreviously unencountered authors in the open world setting. We show that we can\nalso use these calibration curves in the case that we do not have linking\ninformation and thus are forced to classify individual samples directly. This\nis because the calibration curves allow us to identify which samples are more\nlikely to have been correctly attributed. Using such a curve can help an\nanalyst choose a cut-off point which will prevent most misclassifications, at\nthe cost of causing the rejection of some of the more dubious correct\nattributions.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 04:17:30 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 22:18:24 GMT"}, {"version": "v3", "created": "Fri, 26 Jul 2019 00:43:15 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Dauber", "Edwin", ""], ["Caliskan", "Aylin", ""], ["Harang", "Richard", ""], ["Shearer", "Gregory", ""], ["Weisman", "Michael", ""], ["Nelson", "Frederica", ""], ["Greenstadt", "Rachel", ""]]}, {"id": "1701.05691", "submitter": "Lei Lin", "authors": "Lei Lin, Qian Wang, Adel W. Sadek", "title": "Real-time Traffic Accident Risk Prediction based on Frequent Pattern\n  Tree", "comments": "OPT-i 2014 - 1st International Conference on Engineering and Applied\n  Sciences Optimization, Proceedings", "journal-ref": null, "doi": null, "report-no": "2-s2.0-84911904129", "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traffic accident data are usually noisy, contain missing values, and\nheterogeneous. How to select the most important variables to improve real-time\ntraffic accident risk prediction has become a concern of many recent studies.\nThis paper proposes a novel variable selection method based on the Frequent\nPattern tree (FP tree) algorithm. First, all the frequent patterns in the\ntraffic accident dataset are discovered. Then for each frequent pattern, a new\ncriterion, called the Relative Object Purity Ratio (ROPR) which we proposed, is\ncalculated. This ROPR is added to the importance score of the variables that\ndifferentiate one frequent pattern from the others. To test the proposed\nmethod, a dataset was compiled from the traffic accidents records detected by\nonly one detector on interstate highway I-64 in Virginia in 2005. This dataset\nwas then linked to other variables such as real-time traffic information and\nweather conditions. Both the proposed method based on the FP tree algorithm, as\nwell as the widely utilized, random forest method, were then used to identify\nthe important variables or the Virginia dataset. The results indicate that\nthere are some differences between the variables deemed important by the FP\ntree and those selected by the random forest method. Following this, two\nbaseline models (i.e. a nearest neighbor (k-NN) method and a Bayesian network)\nwere developed to predict accident risk based on the variables identified by\nboth the FP tree method and the random forest method. The results show that the\nmodels based on the variable selection using the FP tree performed better than\nthose based on the random forest method for several versions of the k-NN and\nBayesian network models.The best results were derived from a Bayesian network\nmodel using variables from FP tree. That model could predict 61.11% of\naccidents accurately while having a false alarm rate of 38.16%.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 05:05:20 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 17:13:09 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Lin", "Lei", ""], ["Wang", "Qian", ""], ["Sadek", "Adel W.", ""]]}, {"id": "1701.05779", "submitter": "Sungho Jeon", "authors": "Sungho Jeon, Jong-Woo Shin, Young-Jun Lee, Woong-Hee Kim, YoungHyoun\n  Kwon, and Hae-Yong Yang", "title": "Empirical Study of Drone Sound Detection in Real-Life Environment with\n  Deep Neural Networks", "comments": "IEEE 5 Pages, Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to investigate the use of deep neural network to detect\ncommercial hobby drones in real-life environments by analyzing their sound\ndata. The purpose of work is to contribute to a system for detecting drones\nused for malicious purposes, such as for terrorism. Specifically, we present a\nmethod capable of detecting the presence of commercial hobby drones as a binary\nclassification problem based on sound event detection. We recorded the sound\nproduced by a few popular commercial hobby drones, and then augmented this data\nwith diverse environmental sound data to remedy the scarcity of drone sound\ndata in diverse environments. We investigated the effectiveness of\nstate-of-the-art event sound classification methods, i.e., a Gaussian Mixture\nModel (GMM), Convolutional Neural Network (CNN), and Recurrent Neural Network\n(RNN), for drone sound detection. Our empirical results, which were obtained\nwith a testing dataset collected on an urban street, confirmed the\neffectiveness of these models for operating in a real environment. In summary,\nour RNN models showed the best detection performance with an F-Score of 0.8009\nwith 240 ms of input audio with a short processing time, indicating their\napplicability to real-time detection systems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 12:48:02 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Jeon", "Sungho", ""], ["Shin", "Jong-Woo", ""], ["Lee", "Young-Jun", ""], ["Kim", "Woong-Hee", ""], ["Kwon", "YoungHyoun", ""], ["Yang", "Hae-Yong", ""]]}, {"id": "1701.05804", "submitter": "Daniele Tantari", "authors": "Paolo Barucca, Fabrizio Lillo, Piero Mazzarisi, Daniele Tantari", "title": "Disentangling group and link persistence in Dynamic Stochastic Block\n  models", "comments": "13 pages, 8 figures; Final Section added; figures updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the inference of a model of dynamic networks in which both\ncommunities and links keep memory of previous network states. By considering\nmaximum likelihood inference from single snapshot observations of the network,\nwe show that link persistence makes the inference of communities harder,\ndecreasing the detectability threshold, while community persistence tends to\nmake it easier. We analytically show that communities inferred from single\nnetwork snapshot can share a maximum overlap with the underlying communities of\na specific previous instant in time. This leads to time-lagged inference: the\nidentification of past communities rather than present ones. Finally we compute\nthe time lag and propose a corrected algorithm, the Lagged Snapshot Dynamic\n(LSD) algorithm, for community detection in dynamic networks. We analytically\nand numerically characterize the detectability transitions of such algorithm as\na function of the memory parameters of the model and we make a comparison with\na full dynamic inference.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 14:33:45 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 11:40:17 GMT"}, {"version": "v3", "created": "Fri, 10 Nov 2017 17:52:52 GMT"}, {"version": "v4", "created": "Wed, 19 Dec 2018 17:53:42 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Barucca", "Paolo", ""], ["Lillo", "Fabrizio", ""], ["Mazzarisi", "Piero", ""], ["Tantari", "Daniele", ""]]}, {"id": "1701.05931", "submitter": "Loren Lugosch", "authors": "Loren Lugosch, Warren J. Gross", "title": "Neural Offset Min-Sum Decoding", "comments": "Published as a conference paper at the 2017 International Symposium\n  on Information Theory (ISIT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it was shown that if multiplicative weights are assigned to the\nedges of a Tanner graph used in belief propagation decoding, it is possible to\nuse deep learning techniques to find values for the weights which improve the\nerror-correction performance of the decoder. Unfortunately, this approach\nrequires many multiplications, which are generally expensive operations. In\nthis paper, we suggest a more hardware-friendly approach in which offset\nmin-sum decoding is augmented with learnable offset parameters. Our method uses\nno multiplications and has a parameter count less than half that of the\nmultiplicative algorithm. This both speeds up training and provides a feasible\npath to hardware architectures. After describing our method, we compare the\nperformance of the two neural decoding algorithms and show that our method\nachieves error-correction performance within 0.1 dB of the multiplicative\napproach and as much as 1 dB better than traditional belief propagation for the\ncodes under consideration.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 21:55:03 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 06:28:28 GMT"}, {"version": "v3", "created": "Thu, 27 Jul 2017 19:46:30 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Lugosch", "Loren", ""], ["Gross", "Warren J.", ""]]}, {"id": "1701.05954", "submitter": "Ioannis Paschalidis", "authors": "Manjesh K. Hanawal, Hao Liu, Henghui Zhu, Ioannis Ch. Paschalidis", "title": "Learning Policies for Markov Decision Processes from Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a policy for a Markov decision process\nconsistent with data captured on the state-actions pairs followed by the\npolicy. We assume that the policy belongs to a class of parameterized policies\nwhich are defined using features associated with the state-action pairs. The\nfeatures are known a priori, however, only an unknown subset of them could be\nrelevant. The policy parameters that correspond to an observed target policy\nare recovered using $\\ell_1$-regularized logistic regression that best fits the\nobserved state-action samples. We establish bounds on the difference between\nthe average reward of the estimated and the original policy (regret) in terms\nof the generalization error and the ergodic coefficient of the underlying\nMarkov chain. To that end, we combine sample complexity theory and sensitivity\nanalysis of the stationary distribution of Markov chains. Our analysis suggests\nthat to achieve regret within order $O(\\sqrt{\\epsilon})$, it suffices to use\ntraining sample size on the order of $\\Omega(\\log n \\cdot poly(1/\\epsilon))$,\nwhere $n$ is the number of the features. We demonstrate the effectiveness of\nour method on a synthetic robot navigation example.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 00:11:06 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Hanawal", "Manjesh K.", ""], ["Liu", "Hao", ""], ["Zhu", "Henghui", ""], ["Paschalidis", "Ioannis Ch.", ""]]}, {"id": "1701.06075", "submitter": "Linhong Zhu", "authors": "Dingxiong Deng, Fan Bai, Yiqi Tang, Shuigeng Zhou, Cyrus Shahabi,\n  Linhong Zhu", "title": "Label Propagation on K-partite Graphs with Heterophily", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, for the first time, we study label propagation in\nheterogeneous graphs under heterophily assumption. Homophily label propagation\n(i.e., two connected nodes share similar labels) in homogeneous graph (with\nsame types of vertices and relations) has been extensively studied before.\nUnfortunately, real-life networks are heterogeneous, they contain different\ntypes of vertices (e.g., users, images, texts) and relations (e.g.,\nfriendships, co-tagging) and allow for each node to propagate both the same and\nopposite copy of labels to its neighbors. We propose a $\\mathcal{K}$-partite\nlabel propagation model to handle the mystifying combination of heterogeneous\nnodes/relations and heterophily propagation. With this model, we develop a\nnovel label inference algorithm framework with update rules in near-linear time\ncomplexity. Since real networks change over time, we devise an incremental\napproach, which supports fast updates for both new data and evidence (e.g.,\nground truth labels) with guaranteed efficiency. We further provide a utility\nfunction to automatically determine whether an incremental or a re-modeling\napproach is favored. Extensive experiments on real datasets have verified the\neffectiveness and efficiency of our approach, and its superiority over the\nstate-of-the-art label propagation methods.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 19:47:38 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Deng", "Dingxiong", ""], ["Bai", "Fan", ""], ["Tang", "Yiqi", ""], ["Zhou", "Shuigeng", ""], ["Shahabi", "Cyrus", ""], ["Zhu", "Linhong", ""]]}, {"id": "1701.06078", "submitter": "Sungkyun Chang", "authors": "Sungkyun Chang, Kyogu Lee", "title": "Lyrics-to-Audio Alignment by Unsupervised Discovery of Repetitive\n  Patterns in Vowel Acoustics", "comments": "13 pages", "journal-ref": "IEEE Access, Vol. 5, (2017) 16635-16648", "doi": "10.1109/ACCESS.2017.2738558", "report-no": null, "categories": "cs.SD cs.AI cs.IR cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the previous approaches to lyrics-to-audio alignment used a\npre-developed automatic speech recognition (ASR) system that innately suffered\nfrom several difficulties to adapt the speech model to individual singers. A\nsignificant aspect missing in previous works is the self-learnability of\nrepetitive vowel patterns in the singing voice, where the vowel part used is\nmore consistent than the consonant part. Based on this, our system first learns\na discriminative subspace of vowel sequences, based on weighted symmetric\nnon-negative matrix factorization (WS-NMF), by taking the self-similarity of a\nstandard acoustic feature as an input. Then, we make use of canonical time\nwarping (CTW), derived from a recent computer vision technique, to find an\noptimal spatiotemporal transformation between the text and the acoustic\nsequences. Experiments with Korean and English data sets showed that deploying\nthis method after a pre-developed, unsupervised, singing source separation\nachieved more promising results than other state-of-the-art unsupervised\napproaches and an existing ASR-based system.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 20:15:08 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 16:25:15 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Chang", "Sungkyun", ""], ["Lee", "Kyogu", ""]]}, {"id": "1701.06106", "submitter": "Sahil Garg", "authors": "Sahil Garg, Irina Rish, Guillermo Cecchi, Aurelie Lozano", "title": "Neurogenesis-Inspired Dictionary Learning: Online Model Adaption in a\n  Changing World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on online representation learning in non-stationary\nenvironments which may require continuous adaptation of model architecture. We\npropose a novel online dictionary-learning (sparse-coding) framework which\nincorporates the addition and deletion of hidden units (dictionary elements),\nand is inspired by the adult neurogenesis phenomenon in the dentate gyrus of\nthe hippocampus, known to be associated with improved cognitive function and\nadaptation to new environments. In the online learning setting, where new input\ninstances arrive sequentially in batches, the neuronal-birth is implemented by\nadding new units with random initial weights (random dictionary elements); the\nnumber of new units is determined by the current performance (representation\nerror) of the dictionary, higher error causing an increase in the birth rate.\nNeuronal-death is implemented by imposing l1/l2-regularization (group sparsity)\non the dictionary within the block-coordinate descent optimization at each\niteration of our online alternating minimization scheme, which iterates between\nthe code and dictionary updates. Finally, hidden unit connectivity adaptation\nis facilitated by introducing sparsity in dictionary elements. Our empirical\nevaluation on several real-life datasets (images and language) as well as on\nsynthetic data demonstrates that the proposed approach can considerably\noutperform the state-of-art fixed-size (nonadaptive) online sparse coding of\nMairal et al. (2009) in the presence of nonstationary data. Moreover, we\nidentify certain properties of the data (e.g., sparse inputs with nearly\nnon-overlapping supports) and of the model (e.g., dictionary sparsity)\nassociated with such improvements.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 00:35:24 GMT"}, {"version": "v2", "created": "Sun, 19 Feb 2017 08:15:55 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Garg", "Sahil", ""], ["Rish", "Irina", ""], ["Cecchi", "Guillermo", ""], ["Lozano", "Aurelie", ""]]}, {"id": "1701.06120", "submitter": "Zhongnan Zhang", "authors": "Tingxi Wen, Zhongnan Zhang", "title": "Effective and Extensible Feature Extraction Method Using Genetic\n  Algorithm-Based Frequency-Domain Feature Search for Epileptic EEG\n  Multi-classification", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a genetic algorithm-based frequency-domain feature search\n(GAFDS) method is proposed for the electroencephalogram (EEG) analysis of\nepilepsy. In this method, frequency-domain features are first searched and then\ncombined with nonlinear features. Subsequently, these features are selected and\noptimized to classify EEG signals. The extracted features are analyzed\nexperimentally. The features extracted by GAFDS show remarkable independence,\nand they are superior to the nonlinear features in terms of the ratio of\ninter-class distance and intra-class distance. Moreover, the proposed feature\nsearch method can additionally search for features of instantaneous frequency\nin a signal after Hilbert transformation. The classification results achieved\nusing these features are reasonable, thus, GAFDS exhibits good extensibility.\nMultiple classic classifiers (i.e., $k$-nearest neighbor, linear discriminant\nanalysis, decision tree, AdaBoost, multilayer perceptron, and Na\\\"ive Bayes)\nachieve good results by using the features generated by GAFDS method and the\noptimized selection. Specifically, the accuracies for the two-classification\nand three-classification problems may reach up to 99% and 97%, respectively.\nResults of several cross-validation experiments illustrate that GAFDS is\neffective in feature extraction for EEG classification. Therefore, the proposed\nfeature selection and optimization model can improve classification accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 04:20:52 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Wen", "Tingxi", ""], ["Zhang", "Zhongnan", ""]]}, {"id": "1701.06123", "submitter": "Mete Ozay", "authors": "Mete Ozay, Takayuki Okatani", "title": "Optimization on Product Submanifolds of Convolution Kernels", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in optimization methods used for training convolutional\nneural networks (CNNs) with kernels, which are normalized according to\nparticular constraints, have shown remarkable success. This work introduces an\napproach for training CNNs using ensembles of joint spaces of kernels\nconstructed using different constraints. For this purpose, we address a problem\nof optimization on ensembles of products of submanifolds (PEMs) of convolution\nkernels. To this end, we first propose three strategies to construct ensembles\nof PEMs in CNNs. Next, we expound their geometric properties (metric and\ncurvature properties) in CNNs. We make use of our theoretical results by\ndeveloping a geometry-aware SGD algorithm (G-SGD) for optimization on ensembles\nof PEMs to train CNNs. Moreover, we analyze convergence properties of G-SGD\nconsidering geometric properties of PEMs. In the experimental analyses, we\nemploy G-SGD to train CNNs on Cifar-10, Cifar-100 and Imagenet datasets. The\nresults show that geometric adaptive step size computation methods of G-SGD can\nimprove training loss and convergence properties of CNNs. Moreover, we observe\nthat classification performance of baseline CNNs can be boosted using G-SGD on\nensembles of PEMs identified by multiple constraints.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 05:35:39 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 09:08:19 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Ozay", "Mete", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1701.06225", "submitter": "Omar Montasser", "authors": "Omar Montasser and Daniel Kifer", "title": "Predicting Demographics of High-Resolution Geographies with Geotagged\n  Tweets", "comments": "6 pages, AAAI-17 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of predicting demographics of\ngeographic units given geotagged Tweets that are composed within these units.\nTraditional survey methods that offer demographics estimates are usually\nlimited in terms of geographic resolution, geographic boundaries, and time\nintervals. Thus, it would be highly useful to develop computational methods\nthat can complement traditional survey methods by offering demographics\nestimates at finer geographic resolutions, with flexible geographic boundaries\n(i.e. not confined to administrative boundaries), and at different time\nintervals. While prior work has focused on predicting demographics and health\nstatistics at relatively coarse geographic resolutions such as the county-level\nor state-level, we introduce an approach to predict demographics at finer\ngeographic resolutions such as the blockgroup-level. For the task of predicting\ngender and race/ethnicity counts at the blockgroup-level, an approach adapted\nfrom prior work to our problem achieves an average correlation of 0.389\n(gender) and 0.569 (race) on a held-out test dataset. Our approach outperforms\nthis prior approach with an average correlation of 0.671 (gender) and 0.692\n(race).\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 22:16:46 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Montasser", "Omar", ""], ["Kifer", "Daniel", ""]]}, {"id": "1701.06233", "submitter": "Tianran Hu", "authors": "Tianran Hu, Haoyuan Xiao, Thuy-vy Thi Nguyen, Jiebo Luo", "title": "What the Language You Tweet Says About Your Occupation", "comments": "Published at the 10th International AAAI Conference on Web and Social\n  Media (ICWSM-16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many aspects of people's lives are proven to be deeply connected to their\njobs. In this paper, we first investigate the distinct characteristics of major\noccupation categories based on tweets. From multiple social media platforms, we\ngather several types of user information. From users' LinkedIn webpages, we\nlearn their proficiencies. To overcome the ambiguity of self-reported\ninformation, a soft clustering approach is applied to extract occupations from\ncrowd-sourced data. Eight job categories are extracted, including Marketing,\nAdministrator, Start-up, Editor, Software Engineer, Public Relation, Office\nClerk, and Designer. Meanwhile, users' posts on Twitter provide cues for\nunderstanding their linguistic styles, interests, and personalities. Our\nresults suggest that people of different jobs have unique tendencies in certain\nlanguage styles and interests. Our results also clearly reveal distinctive\nlevels in terms of Big Five Traits for different jobs. Finally, a classifier is\nbuilt to predict job types based on the features extracted from tweets. A high\naccuracy indicates a strong discrimination power of language features for job\nprediction task.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 23:03:11 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Hu", "Tianran", ""], ["Xiao", "Haoyuan", ""], ["Nguyen", "Thuy-vy Thi", ""], ["Luo", "Jiebo", ""]]}, {"id": "1701.06247", "submitter": "Hongjie Shi", "authors": "Hongjie Shi, Takashi Ushio, Mitsuru Endo, Katsuyoshi Yamagami, Noriaki\n  Horii", "title": "A Multichannel Convolutional Neural Network For Cross-language Dialog\n  State Tracking", "comments": "Copyright 2016 IEEE. Published in the 2016 IEEE Workshop on Spoken\n  Language Technology (SLT 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fifth Dialog State Tracking Challenge (DSTC5) introduces a new\ncross-language dialog state tracking scenario, where the participants are asked\nto build their trackers based on the English training corpus, while evaluating\nthem with the unlabeled Chinese corpus. Although the computer-generated\ntranslations for both English and Chinese corpus are provided in the dataset,\nthese translations contain errors and careless use of them can easily hurt the\nperformance of the built trackers. To address this problem, we propose a\nmultichannel Convolutional Neural Networks (CNN) architecture, in which we\ntreat English and Chinese language as different input channels of one single\nCNN model. In the evaluation of DSTC5, we found that such multichannel\narchitecture can effectively improve the robustness against translation errors.\nAdditionally, our method for DSTC5 is purely machine learning based and\nrequires no prior knowledge about the target language. We consider this a\ndesirable property for building a tracker in the cross-language context, as not\nevery developer will be familiar with both languages.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 01:36:10 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Shi", "Hongjie", ""], ["Ushio", "Takashi", ""], ["Endo", "Mitsuru", ""], ["Yamagami", "Katsuyoshi", ""], ["Horii", "Noriaki", ""]]}, {"id": "1701.06279", "submitter": "Patrick Ng", "authors": "Patrick Ng", "title": "dna2vec: Consistent vector representations of variable-length k-mers", "comments": "10 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the ubiquitous representation of long DNA sequence is dividing it into\nshorter k-mer components. Unfortunately, the straightforward vector encoding of\nk-mer as a one-hot vector is vulnerable to the curse of dimensionality. Worse\nyet, the distance between any pair of one-hot vectors is equidistant. This is\nparticularly problematic when applying the latest machine learning algorithms\nto solve problems in biological sequence analysis. In this paper, we propose a\nnovel method to train distributed representations of variable-length k-mers.\nOur method is based on the popular word embedding model word2vec, which is\ntrained on a shallow two-layer neural network. Our experiments provide evidence\nthat the summing of dna2vec vectors is akin to nucleotides concatenation. We\nalso demonstrate that there is correlation between Needleman-Wunsch similarity\nscore and cosine similarity of dna2vec vectors.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 07:21:43 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Ng", "Patrick", ""]]}, {"id": "1701.06421", "submitter": "Emilie Poisson Caillault", "authors": "Thi-Thu-Hong Phan (LISIC), Emilie Poisson Caillault (LISIC), Andr\\'e\n  Bigand (LISIC)", "title": "Comparative study on supervised learning methods for identifying\n  phytoplankton species", "comments": null, "journal-ref": "2016 IEEE Sixth International Conference on Communications and\n  Electronics (ICCE), Jul 2016, Ha Long, Vietnam. pp.283 - 288, 2016", "doi": "10.1109/CCE.2016.7562650", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phytoplankton plays an important role in marine ecosystem. It is defined as a\nbiological factor to assess marine quality. The identification of phytoplankton\nspecies has a high potential for monitoring environmental, climate changes and\nfor evaluating water quality. However, phytoplankton species identification is\nnot an easy task owing to their variability and ambiguity due to thousands of\nmicro and pico-plankton species. Therefore, the aim of this paper is to build a\nframework for identifying phytoplankton species and to perform a comparison on\ndifferent features types and classifiers. We propose a new features type\nextracted from raw signals of phytoplankton species. We then analyze the\nperformance of various classifiers on the proposed features type as well as two\nother features types for finding the robust one. Through experiments, it is\nfound that Random Forest using the proposed features gives the best\nclassification results with average accuracy up to 98.24%.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 14:45:20 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Phan", "Thi-Thu-Hong", "", "LISIC"], ["Caillault", "Emilie Poisson", "", "LISIC"], ["Bigand", "Andr\u00e9", "", "LISIC"]]}, {"id": "1701.06452", "submitter": "Giovanni Montana", "authors": "Petros-Pavlos Ypsilantis and Giovanni Montana", "title": "Learning what to look in chest X-rays with a recurrent visual attention\n  model", "comments": "NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-rays are commonly performed imaging tests that use small amounts of\nradiation to produce pictures of the organs, tissues, and bones of the body.\nX-rays of the chest are used to detect abnormalities or diseases of the\nairways, blood vessels, bones, heart, and lungs. In this work we present a\nstochastic attention-based model that is capable of learning what regions\nwithin a chest X-ray scan should be visually explored in order to conclude that\nthe scan contains a specific radiological abnormality. The proposed model is a\nrecurrent neural network (RNN) that learns to sequentially sample the entire\nX-ray and focus only on informative areas that are likely to contain the\nrelevant information. We report on experiments carried out with more than\n$100,000$ X-rays containing enlarged hearts or medical devices. The model has\nbeen trained using reinforcement learning methods to learn task-specific\npolicies.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 15:29:47 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Ypsilantis", "Petros-Pavlos", ""], ["Montana", "Giovanni", ""]]}, {"id": "1701.06511", "submitter": "Bikash Joshi", "authors": "Bikash Joshi, Massih-Reza Amini, Ioannis Partalas, Franck Iutzeler,\n  Yury Maximov", "title": "Aggressive Sampling for Multi-class to Binary Reduction with\n  Applications to Text Classification", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of multi-class classification in the case where the\nnumber of classes is very large. We propose a double sampling strategy on top\nof a multi-class to binary reduction strategy, which transforms the original\nmulti-class problem into a binary classification problem over pairs of\nexamples. The aim of the sampling strategy is to overcome the curse of\nlong-tailed class distributions exhibited in majority of large-scale\nmulti-class classification problems and to reduce the number of pairs of\nexamples in the expanded data. We show that this strategy does not alter the\nconsistency of the empirical risk minimization principle defined over the\ndouble sample reduction. Experiments are carried out on DMOZ and Wikipedia\ncollections with 10,000 to 100,000 classes where we show the efficiency of the\nproposed approach in terms of training and prediction time, memory consumption,\nand predictive performance with respect to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 17:14:02 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 22:02:17 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 09:34:40 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Joshi", "Bikash", ""], ["Amini", "Massih-Reza", ""], ["Partalas", "Ioannis", ""], ["Iutzeler", "Franck", ""], ["Maximov", "Yury", ""]]}, {"id": "1701.06532", "submitter": "Jan Jakubuv", "authors": "Jan Jakub\\r{u}v, Josef Urban", "title": "ENIGMA: Efficient Learning-based Inference Guiding Machine", "comments": "Submitted to LPAR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ENIGMA is a learning-based method for guiding given clause selection in\nsaturation-based theorem provers. Clauses from many proof searches are\nclassified as positive and negative based on their participation in the proofs.\nAn efficient classification model is trained on this data, using fast\nfeature-based characterization of the clauses . The learned model is then\ntightly linked with the core prover and used as a basis of a new parameterized\nevaluation heuristic that provides fast ranking of all generated clauses. The\napproach is evaluated on the E prover and the CASC 2016 AIM benchmark, showing\na large increase of E's performance.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 18:03:52 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Jakub\u016fv", "Jan", ""], ["Urban", "Josef", ""]]}, {"id": "1701.06538", "submitter": "Noam Shazeer", "authors": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc\n  Le, Geoffrey Hinton, Jeff Dean", "title": "Outrageously Large Neural Networks: The Sparsely-Gated\n  Mixture-of-Experts Layer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capacity of a neural network to absorb information is limited by its\nnumber of parameters. Conditional computation, where parts of the network are\nactive on a per-example basis, has been proposed in theory as a way of\ndramatically increasing model capacity without a proportional increase in\ncomputation. In practice, however, there are significant algorithmic and\nperformance challenges. In this work, we address these challenges and finally\nrealize the promise of conditional computation, achieving greater than 1000x\nimprovements in model capacity with only minor losses in computational\nefficiency on modern GPU clusters. We introduce a Sparsely-Gated\nMixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward\nsub-networks. A trainable gating network determines a sparse combination of\nthese experts to use for each example. We apply the MoE to the tasks of\nlanguage modeling and machine translation, where model capacity is critical for\nabsorbing the vast quantities of knowledge available in the training corpora.\nWe present model architectures in which a MoE with up to 137 billion parameters\nis applied convolutionally between stacked LSTM layers. On large language\nmodeling and machine translation benchmarks, these models achieve significantly\nbetter results than state-of-the-art at lower computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 18:10:00 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Shazeer", "Noam", ""], ["Mirhoseini", "Azalia", ""], ["Maziarz", "Krzysztof", ""], ["Davis", "Andy", ""], ["Le", "Quoc", ""], ["Hinton", "Geoffrey", ""], ["Dean", "Jeff", ""]]}, {"id": "1701.06548", "submitter": "George Tucker", "authors": "Gabriel Pereyra, George Tucker, Jan Chorowski, {\\L}ukasz Kaiser,\n  Geoffrey Hinton", "title": "Regularizing Neural Networks by Penalizing Confident Output\n  Distributions", "comments": "Submitted to ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We systematically explore regularizing neural networks by penalizing low\nentropy output distributions. We show that penalizing low entropy output\ndistributions, which has been shown to improve exploration in reinforcement\nlearning, acts as a strong regularizer in supervised learning. Furthermore, we\nconnect a maximum entropy based confidence penalty to label smoothing through\nthe direction of the KL divergence. We exhaustively evaluate the proposed\nconfidence penalty and label smoothing on 6 common benchmarks: image\nclassification (MNIST and Cifar-10), language modeling (Penn Treebank), machine\ntranslation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ).\nWe find that both label smoothing and the confidence penalty improve\nstate-of-the-art models across benchmarks without modifying existing\nhyperparameters, suggesting the wide applicability of these regularizers.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 18:35:28 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Pereyra", "Gabriel", ""], ["Tucker", "George", ""], ["Chorowski", "Jan", ""], ["Kaiser", "\u0141ukasz", ""], ["Hinton", "Geoffrey", ""]]}, {"id": "1701.06551", "submitter": "Maryam Ahmadi", "authors": "Masood Tehrani and Mary Ahmadi", "title": "On the Parametric Study of Lubricating Oil Production using an\n  Artificial Neural Network (ANN) Approach", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, an Artificial Neural Network (ANN) approach is utilized to\nperform a parametric study on the process of extraction of lubricants from\nheavy petroleum cuts. To train the model, we used field data collected from an\nindustrial plant. Operational conditions of feed and solvent flow rate,\nTemperature of streams and mixing rate were considered as the input to the\nmodel, whereas the flow rate of the main product was considered as the output\nof the ANN model. A feed-forward Multi-Layer Perceptron Neural Network was\nsuccessfully applied to capture the relationship between inputs and output\nparameters.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 18:13:42 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Tehrani", "Masood", ""], ["Ahmadi", "Mary", ""]]}, {"id": "1701.06605", "submitter": "Jalal Etesami", "authors": "Saber Salehkaleybar and Jalal Etesami and Negar Kiyavash", "title": "Identifying Nonlinear 1-Step Causal Influences in Presence of Latent\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for learning the causal structure in stochastic\ndynamical systems with a $1$-step functional dependency in the presence of\nlatent variables. We propose an information-theoretic approach that allows us\nto recover the causal relations among the observed variables as long as the\nlatent variables evolve without exogenous noise. We further propose an\nefficient learning method based on linear regression for the special sub-case\nwhen the dynamics are restricted to be linear. We validate the performance of\nour approach via numerical simulations.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 19:48:11 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Salehkaleybar", "Saber", ""], ["Etesami", "Jalal", ""], ["Kiyavash", "Negar", ""]]}, {"id": "1701.06624", "submitter": "Amita Gajewar", "authors": "Amita Gajewar, Gagan Bansal", "title": "Revenue Forecasting for Enterprise Products", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any business, planning is a continuous process, and typically\nbusiness-owners focus on making both long-term planning aligned with a\nparticular strategy as well as short-term planning that accommodates the\ndynamic market situations. An ability to perform an accurate financial forecast\nis crucial for effective planning. In this paper, we focus on providing an\nintelligent and efficient solution that will help in forecasting revenue using\nmachine learning algorithms. We experiment with three different revenue\nforecasting models, and here we provide detailed insights into the methodology\nand their relative performance measured on real finance data. As a real-world\napplication of our models, we partner with Microsoft's Finance organization\n(department that reports Microsoft's finances) to provide them a guidance on\nthe projected revenue for upcoming quarters.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 20:41:12 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Gajewar", "Amita", ""], ["Bansal", "Gagan", ""]]}, {"id": "1701.06652", "submitter": "Ian Manchester", "authors": "Mark M. Tobenkin and Ian R. Manchester and Alexandre Megretski", "title": "Convex Parameterizations and Fidelity Bounds for Nonlinear\n  Identification and Reduced-Order Modelling", "comments": "Conditionally accepted to IEEE TAC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model instability and poor prediction of long-term behavior are common\nproblems when modeling dynamical systems using nonlinear \"black-box\"\ntechniques. Direct optimization of the long-term predictions, often called\nsimulation error minimization, leads to optimization problems that are\ngenerally non-convex in the model parameters and suffer from multiple local\nminima. In this work we present methods which address these problems through\nconvex optimization, based on Lagrangian relaxation, dissipation inequalities,\ncontraction theory, and semidefinite programming. We demonstrate the proposed\nmethods with a model order reduction task for electronic circuit design and the\nidentification of a pneumatic actuator from experiment.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 22:13:59 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Tobenkin", "Mark M.", ""], ["Manchester", "Ian R.", ""], ["Megretski", "Alexandre", ""]]}, {"id": "1701.06655", "submitter": "Chiwoo Park", "authors": "Chiwoo Park and Daniel Apley", "title": "Patchwork Kriging for Large-scale Gaussian Process Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for Gaussian process (GP) regression for\nlarge datasets. The approach involves partitioning the regression input domain\ninto multiple local regions with a different local GP model fitted in each\nregion. Unlike existing local partitioned GP approaches, we introduce a\ntechnique for patching together the local GP models nearly seamlessly to ensure\nthat the local GP models for two neighboring regions produce nearly the same\nresponse prediction and prediction error variance on the boundary between the\ntwo regions. This largely mitigates the well-known discontinuity problem that\ndegrades the boundary accuracy of existing local partitioned GP methods. Our\nmain innovation is to represent the continuity conditions as additional\npseudo-observations that the differences between neighboring GP responses are\nidentically zero at an appropriately chosen set of boundary input locations. To\npredict the response at any input location, we simply augment the actual\nresponse observations with the pseudo-observations and apply standard GP\nprediction methods to the augmented data. In contrast to heuristic continuity\nadjustments, this has an advantage of working within a formal GP framework, so\nthat the GP-based predictive uncertainty quantification remains valid. Our\napproach also inherits a sparse block-like structure for the sample covariance\nmatrix, which results in computationally efficient closed-form expressions for\nthe predictive mean and variance. In addition, we provide a new spatial\npartitioning scheme based on a recursive space partitioning along local\nprincipal component directions, which makes the proposed approach applicable\nfor regression domains having more than two dimensions. Using three spatial\ndatasets and three higher dimensional datasets, we investigate the numerical\nperformance of the approach and compare it to several state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 22:20:47 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 19:40:22 GMT"}, {"version": "v3", "created": "Fri, 9 Mar 2018 19:17:16 GMT"}, {"version": "v4", "created": "Sat, 7 Jul 2018 16:55:04 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Park", "Chiwoo", ""], ["Apley", "Daniel", ""]]}, {"id": "1701.06725", "submitter": "Linqi Song", "authors": "Linqi Song and Jie Xu", "title": "A Contextual Bandit Approach for Stream-Based Active Learning", "comments": "arXiv admin note: text overlap with arXiv:1607.03182", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandit algorithms -- a class of multi-armed bandit algorithms that\nexploit the contextual information -- have been shown to be effective in\nsolving sequential decision making problems under uncertainty. A common\nassumption adopted in the literature is that the realized (ground truth) reward\nby taking the selected action is observed by the learner at no cost, which,\nhowever, is not realistic in many practical scenarios. When observing the\nground truth reward is costly, a key challenge for the learner is how to\njudiciously acquire the ground truth by assessing the benefits and costs in\norder to balance learning efficiency and learning cost. From the information\ntheoretic perspective, a perhaps even more interesting question is how much\nefficiency might be lost due to this cost. In this paper, we design a novel\ncontextual bandit-based learning algorithm and endow it with the active\nlearning capability. The key feature of our algorithm is that in addition to\nsending a query to an annotator for the ground truth, prior information about\nthe ground truth learned by the learner is sent together, thereby reducing the\nquery cost. We prove that by carefully choosing the algorithm parameters, the\nlearning regret of the proposed algorithm achieves the same order as that of\nconventional contextual bandit algorithms in cost-free scenarios, implying\nthat, surprisingly, cost due to acquiring the ground truth does not increase\nthe learning regret in the long-run. Our analysis shows that prior information\nabout the ground truth plays a critical role in improving the system\nperformance in scenarios where active learning is necessary.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 04:12:25 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Song", "Linqi", ""], ["Xu", "Jie", ""]]}, {"id": "1701.06751", "submitter": "Qiongkai Xu", "authors": "Qiongkai Xu, Qing Wang, Chenchen Xu and Lizhen Qu", "title": "Collective Vertex Classification Using Recursive Neural Network", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective classification of vertices is a task of assigning categories to\neach vertex in a graph based on both vertex attributes and link structure.\nNevertheless, some existing approaches do not use the features of neighbouring\nvertices properly, due to the noise introduced by these features. In this\npaper, we propose a graph-based recursive neural network framework for\ncollective vertex classification. In this framework, we generate hidden\nrepresentations from both attributes of vertices and representations of\nneighbouring vertices via recursive neural networks. Under this framework, we\nexplore two types of recursive neural units, naive recursive neural unit and\nlong short-term memory unit. We have conducted experiments on four real-world\nnetwork datasets. The experimental results show that our frame- work with long\nshort-term memory model achieves better results and outperforms several\ncompetitive baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 07:07:15 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Xu", "Qiongkai", ""], ["Wang", "Qing", ""], ["Xu", "Chenchen", ""], ["Qu", "Lizhen", ""]]}, {"id": "1701.06796", "submitter": "Gaurav Pandey", "authors": "Gaurav Pandey and Ambedkar Dukkipati", "title": "Discriminative Neural Topic Models", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural network based approach for learning topics from text and\nimage datasets. The model makes no assumptions about the conditional\ndistribution of the observed features given the latent topics. This allows us\nto perform topic modelling efficiently using sentences of documents and patches\nof images as observed features, rather than limiting ourselves to words.\nMoreover, the proposed approach is online, and hence can be used for streaming\ndata. Furthermore, since the approach utilizes neural networks, it can be\nimplemented on GPU with ease, and hence it is very scalable.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 10:29:31 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 14:17:16 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Pandey", "Gaurav", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1701.06806", "submitter": "Srinivasan Arunachalam", "authors": "Srinivasan Arunachalam (CWI) and Ronald de Wolf (CWI and U of\n  Amsterdam)", "title": "A Survey of Quantum Learning Theory", "comments": "26 pages LaTeX. v2: many small changes to improve the presentation.\n  This version will appear as Complexity Theory Column in SIGACT News in June\n  2017. v3: fixed a small ambiguity in the definition of gamma(C) and updated a\n  reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper surveys quantum learning theory: the theoretical aspects of\nmachine learning using quantum computers. We describe the main results known\nfor three models of learning: exact learning from membership queries, and\nProbably Approximately Correct (PAC) and agnostic learning from classical or\nquantum examples.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 10:53:07 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 16:48:10 GMT"}, {"version": "v3", "created": "Fri, 28 Jul 2017 09:40:37 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Arunachalam", "Srinivasan", "", "CWI"], ["de Wolf", "Ronald", "", "CWI and U of\n  Amsterdam"]]}, {"id": "1701.06972", "submitter": "Sarah Loos", "authors": "Sarah Loos, Geoffrey Irving, Christian Szegedy, Cezary Kaliszyk", "title": "Deep Network Guided Proof Search", "comments": null, "journal-ref": "In Thomas Eiter and David Sands, editors, 21st International\n  Conference on Logic for Programming, Artificial Intelligence and Reasoning\n  (LPAR-21). EPiC Series in Computing, vol. 46, pages 85-105, EasyChair, 2017.\n  ISSN 2398-7340", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques lie at the heart of several significant AI advances\nin recent years including object recognition and detection, image captioning,\nmachine translation, speech recognition and synthesis, and playing the game of\nGo. Automated first-order theorem provers can aid in the formalization and\nverification of mathematical theorems and play a crucial role in program\nanalysis, theory reasoning, security, interpolation, and system verification.\nHere we suggest deep learning based guidance in the proof search of the theorem\nprover E. We train and compare several deep neural network models on the traces\nof existing ATP proofs of Mizar statements and use them to select processed\nclauses during proof search. We give experimental evidence that with a hybrid,\ntwo-phase approach, deep learning based guidance can significantly reduce the\naverage number of proof search steps while increasing the number of theorems\nproved. Using a few proof guidance strategies that leverage deep neural\nnetworks, we have found first-order proofs of 7.36% of the first-order logic\ntranslations of the Mizar Mathematical Library theorems that did not previously\nhave ATP generated proofs. This increases the ratio of statements in the corpus\nwith ATP generated proofs from 56% to 59%.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 16:39:05 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Loos", "Sarah", ""], ["Irving", "Geoffrey", ""], ["Szegedy", "Christian", ""], ["Kaliszyk", "Cezary", ""]]}, {"id": "1701.07114", "submitter": "Nayyar Zaidi", "authors": "Nayyar A. Zaidi, Yang Du, Geoffrey I. Webb", "title": "On the Effectiveness of Discretizing Quantitative Attributes in Linear\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning algorithms that learn linear models often have high representation\nbias on real-world problems. In this paper, we show that this representation\nbias can be greatly reduced by discretization. Discretization is a common\nprocedure in machine learning that is used to convert a quantitative attribute\ninto a qualitative one. It is often motivated by the limitation of some\nlearners to qualitative data. Discretization loses information, as fewer\ndistinctions between instances are possible using discretized data relative to\nundiscretized data. In consequence, where discretization is not essential, it\nmight appear desirable to avoid it. However, it has been shown that\ndiscretization often substantially reduces the error of the linear generative\nBayesian classifier naive Bayes. This motivates a systematic study of the\neffectiveness of discretizing quantitative attributes for other linear\nclassifiers. In this work, we study the effect of discretization on the\nperformance of linear classifiers optimizing three distinct discriminative\nobjective functions --- logistic regression (optimizing negative\nlog-likelihood), support vector classifiers (optimizing hinge loss) and a\nzero-hidden layer artificial neural network (optimizing mean-square-error). We\nshow that discretization can greatly increase the accuracy of these linear\ndiscriminative learners by reducing their representation bias, especially on\nbig datasets. We substantiate our claims with an empirical study on $42$\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 23:57:32 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Zaidi", "Nayyar A.", ""], ["Du", "Yang", ""], ["Webb", "Geoffrey I.", ""]]}, {"id": "1701.07125", "submitter": "EPTCS", "authors": "Emilio Jes\\'us Gallego Arias (MINES ParisTech, PSL Research\n  University, France), Beno\\^it Pin (MINES ParisTech, PSL Research University,\n  France), Pierre Jouvelot (MINES ParisTech, PSL Research University, France)", "title": "jsCoq: Towards Hybrid Theorem Proving Interfaces", "comments": "In Proceedings UITP 2016, arXiv:1701.06745", "journal-ref": "EPTCS 239, 2017, pp. 15-27", "doi": "10.4204/EPTCS.239.2", "report-no": null, "categories": "cs.PL cs.HC cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe jsCcoq, a new platform and user environment for the Coq\ninteractive proof assistant. The jsCoq system targets the HTML5-ECMAScript 2015\nspecification, and it is typically run inside a standards-compliant browser,\nwithout the need of external servers or services. Targeting educational use,\njsCoq allows the user to start interaction with proof scripts right away,\nthanks to its self-contained nature. Indeed, a full Coq environment is packed\nalong the proof scripts, easing distribution and installation. Starting to use\njsCoq is as easy as clicking on a link. The current release ships more than 10\npopular Coq libraries, and supports popular books such as Software Foundations\nor Certified Programming with Dependent Types. The new target platform has\nopened up new interaction and display possibilities. It has also fostered the\ndevelopment of some new Coq-related technology. In particular, we have\nimplemented a new serialization-based protocol for interaction with the proof\nassistant, as well as a new package format for library distribution.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 01:21:14 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Arias", "Emilio Jes\u00fas Gallego", "", "MINES ParisTech, PSL Research\n  University, France"], ["Pin", "Beno\u00eet", "", "MINES ParisTech, PSL Research University,\n  France"], ["Jouvelot", "Pierre", "", "MINES ParisTech, PSL Research University, France"]]}, {"id": "1701.07148", "submitter": "Marcella Astrid", "authors": "Marcella Astrid and Seung-Ik Lee", "title": "CP-decomposition with Tensor Power Method for Convolutional Neural\n  Networks Compression", "comments": "Accepted as a conference paper at BigComp 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) has shown a great success in many areas\nincluding complex image classification tasks. However, they need a lot of\nmemory and computational cost, which hinders them from running in relatively\nlow-end smart devices such as smart phones. We propose a CNN compression method\nbased on CP-decomposition and Tensor Power Method. We also propose an iterative\nfine tuning, with which we fine-tune the whole network after decomposing each\nlayer, but before decomposing the next layer. Significant reduction in memory\nand computation cost is achieved compared to state-of-the-art previous work\nwith no more accuracy loss.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 02:58:06 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Astrid", "Marcella", ""], ["Lee", "Seung-Ik", ""]]}, {"id": "1701.07166", "submitter": "Shaowei Wang", "authors": "Shaowei Wang, Liusheng Huang, Pengzhan Wang, Hongli Xu, Wei Yang", "title": "Personalized Classifier Ensemble Pruning Framework for Mobile\n  Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble learning has been widely employed by mobile applications, ranging\nfrom environmental sensing to activity recognitions. One of the fundamental\nissue in ensemble learning is the trade-off between classification accuracy and\ncomputational costs, which is the goal of ensemble pruning. During\ncrowdsourcing, the centralized aggregator releases ensemble learning models to\na large number of mobile participants for task evaluation or as the\ncrowdsourcing learning results, while different participants may seek for\ndifferent levels of the accuracy-cost trade-off. However, most of existing\nensemble pruning approaches consider only one identical level of such\ntrade-off. In this study, we present an efficient ensemble pruning framework\nfor personalized accuracy-cost trade-offs via multi-objective optimization.\nSpecifically, for the commonly used linear-combination style of the trade-off,\nwe provide an objective-mixture optimization to further reduce the number of\nensemble candidates. Experimental results show that our framework is highly\nefficient for personalized ensemble pruning, and achieves much better pruning\nperformance with objective-mixture optimization when compared to state-of-art\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 05:22:35 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Wang", "Shaowei", ""], ["Huang", "Liusheng", ""], ["Wang", "Pengzhan", ""], ["Xu", "Hongli", ""], ["Yang", "Wei", ""]]}, {"id": "1701.07179", "submitter": "Doyen Sahoo", "authors": "Doyen Sahoo, Chenghao Liu, and Steven C.H. Hoi", "title": "Malicious URL Detection using Machine Learning: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malicious URL, a.k.a. malicious website, is a common and serious threat to\ncybersecurity. Malicious URLs host unsolicited content (spam, phishing,\ndrive-by exploits, etc.) and lure unsuspecting users to become victims of scams\n(monetary loss, theft of private information, and malware installation), and\ncause losses of billions of dollars every year. It is imperative to detect and\nact on such threats in a timely manner. Traditionally, this detection is done\nmostly through the usage of blacklists. However, blacklists cannot be\nexhaustive, and lack the ability to detect newly generated malicious URLs. To\nimprove the generality of malicious URL detectors, machine learning techniques\nhave been explored with increasing attention in recent years. This article aims\nto provide a comprehensive survey and a structural understanding of Malicious\nURL Detection techniques using machine learning. We present the formal\nformulation of Malicious URL Detection as a machine learning task, and\ncategorize and review the contributions of literature studies that addresses\ndifferent dimensions of this problem (feature representation, algorithm design,\netc.). Further, this article provides a timely and comprehensive survey for a\nrange of different audiences, not only for machine learning researchers and\nengineers in academia, but also for professionals and practitioners in\ncybersecurity industry, to help them understand the state of the art and\nfacilitate their own research and practical applications. We also discuss\npractical issues in system design, open research challenges, and point out some\nimportant directions for future research.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 06:46:14 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 07:12:50 GMT"}, {"version": "v3", "created": "Wed, 21 Aug 2019 10:38:24 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Sahoo", "Doyen", ""], ["Liu", "Chenghao", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "1701.07194", "submitter": "Dacheng Tao", "authors": "Shan You, Chang Xu, Yunhe Wang, Chao Xu, Dacheng Tao", "title": "Privileged Multi-label Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents privileged multi-label learning (PrML) to explore and\nexploit the relationship between labels in multi-label learning problems. We\nsuggest that for each individual label, it cannot only be implicitly connected\nwith other labels via the low-rank constraint over label predictors, but also\nits performance on examples can receive the explicit comments from other labels\ntogether acting as an \\emph{Oracle teacher}. We generate privileged label\nfeature for each example and its individual label, and then integrate it into\nthe framework of low-rank based multi-label learning. The proposed algorithm\ncan therefore comprehensively explore and exploit label relationships by\ninheriting all the merits of privileged information and low-rank constraints.\nWe show that PrML can be efficiently solved by dual coordinate descent\nalgorithm using iterative optimization strategy with cheap updates. Experiments\non benchmark datasets show that through privileged label features, the\nperformance can be significantly improved and PrML is superior to several\ncompeting methods in most cases.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 07:43:13 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["You", "Shan", ""], ["Xu", "Chang", ""], ["Wang", "Yunhe", ""], ["Xu", "Chao", ""], ["Tao", "Dacheng", ""]]}, {"id": "1701.07204", "submitter": "Kasper Green Larsen", "authors": "Allan Gr{\\o}nlund and Kasper Green Larsen and Alexander Mathiasen and\n  Jesper Sindahl Nielsen and Stefan Schneider and Mingzhou Song", "title": "Fast Exact k-Means, k-Medians and Bregman Divergence Clustering in 1D", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-Means clustering problem on $n$ points is NP-Hard for any dimension\n$d\\ge 2$, however, for the 1D case there exists exact polynomial time\nalgorithms. Previous literature reported an $O(kn^2)$ time dynamic programming\nalgorithm that uses $O(kn)$ space. It turns out that the problem has been\nconsidered under a different name more than twenty years ago. We present all\nthe existing work that had been overlooked and compare the various solutions\ntheoretically. Moreover, we show how to reduce the space usage for some of\nthem, as well as generalize them to data structures that can quickly report an\noptimal $k$-Means clustering for any $k$. Finally we also generalize all the\nalgorithms to work for the absolute distance and to work for any Bregman\nDivergence. We complement our theoretical contributions by experiments that\ncompare the practical performance of the various algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 08:44:04 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 20:40:50 GMT"}, {"version": "v3", "created": "Fri, 30 Jun 2017 10:37:16 GMT"}, {"version": "v4", "created": "Wed, 25 Apr 2018 10:36:08 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Gr\u00f8nlund", "Allan", ""], ["Larsen", "Kasper Green", ""], ["Mathiasen", "Alexander", ""], ["Nielsen", "Jesper Sindahl", ""], ["Schneider", "Stefan", ""], ["Song", "Mingzhou", ""]]}, {"id": "1701.07232", "submitter": "Rishabh Singh", "authors": "Patrice Godefroid, Hila Peleg, Rishabh Singh", "title": "Learn&Fuzz: Machine Learning for Input Fuzzing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.LG cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzing consists of repeatedly testing an application with modified, or\nfuzzed, inputs with the goal of finding security vulnerabilities in\ninput-parsing code. In this paper, we show how to automate the generation of an\ninput grammar suitable for input fuzzing using sample inputs and\nneural-network-based statistical machine-learning techniques. We present a\ndetailed case study with a complex input format, namely PDF, and a large\ncomplex security-critical parser for this format, namely, the PDF parser\nembedded in Microsoft's new Edge browser. We discuss (and measure) the tension\nbetween conflicting learning and fuzzing goals: learning wants to capture the\nstructure of well-formed inputs, while fuzzing wants to break that structure in\norder to cover unexpected code paths and find bugs. We also present a new\nalgorithm for this learn&fuzz challenge which uses a learnt input probability\ndistribution to intelligently guide where to fuzz inputs.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 10:01:39 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Godefroid", "Patrice", ""], ["Peleg", "Hila", ""], ["Singh", "Rishabh", ""]]}, {"id": "1701.07243", "submitter": "Francois Meyer", "authors": "Fran\\c{c}ois G. Meyer, Alexander M. Benison, Zachariah Smith, and\n  Daniel S. Barth", "title": "Decoding Epileptogenesis in a Reduced State Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe here the recent results of a multidisciplinary effort to design a\nbiomarker that can actively and continuously decode the progressive changes in\nneuronal organization leading to epilepsy, a process known as epileptogenesis.\nUsing an animal model of acquired epilepsy, wechronically record hippocampal\nevoked potentials elicited by an auditory stimulus. Using a set of reduced\ncoordinates, our algorithm can identify universal smooth low-dimensional\nconfigurations of the auditory evoked potentials that correspond to distinct\nstages of epileptogenesis. We use a hidden Markov model to learn the dynamics\nof the evoked potential, as it evolves along these smooth low-dimensional\nsubsets. We provide experimental evidence that the biomarker is able to exploit\nsubtle changes in the evoked potential to reliably decode the stage of\nepileptogenesis and predict whether an animal will eventually recover from the\ninjury, or develop spontaneous seizures.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 10:25:59 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Meyer", "Fran\u00e7ois G.", ""], ["Benison", "Alexander M.", ""], ["Smith", "Zachariah", ""], ["Barth", "Daniel S.", ""]]}, {"id": "1701.07266", "submitter": "Kfir Levy Yehuda", "authors": "Oren Anava, Kfir Y. Levy", "title": "k*-Nearest Neighbors: From Global to Local", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The weighted k-nearest neighbors algorithm is one of the most fundamental\nnon-parametric methods in pattern recognition and machine learning. The\nquestion of setting the optimal number of neighbors as well as the optimal\nweights has received much attention throughout the years, nevertheless this\nproblem seems to have remained unsettled. In this paper we offer a simple\napproach to locally weighted regression/classification, where we make the\nbias-variance tradeoff explicit. Our formulation enables us to phrase a notion\nof optimal weights, and to efficiently find these weights as well as the\noptimal number of neighbors efficiently and adaptively, for each data point\nwhose value we wish to estimate. The applicability of our approach is\ndemonstrated on several datasets, showing superior performance over standard\nlocally weighted methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 11:18:18 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Anava", "Oren", ""], ["Levy", "Kfir Y.", ""]]}, {"id": "1701.07274", "submitter": "Yuxi Li", "authors": "Yuxi Li", "title": "Deep Reinforcement Learning: An Overview", "comments": "Please see Deep Reinforcement Learning, arXiv:1810.06339, for a\n  significant update", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an overview of recent exciting achievements of deep reinforcement\nlearning (RL). We discuss six core elements, six important mechanisms, and\ntwelve applications. We start with background of machine learning, deep\nlearning and reinforcement learning. Next we discuss core RL elements,\nincluding value function, in particular, Deep Q-Network (DQN), policy, reward,\nmodel, planning, and exploration. After that, we discuss important mechanisms\nfor RL, including attention and memory, unsupervised learning, transfer\nlearning, multi-agent RL, hierarchical RL, and learning to learn. Then we\ndiscuss various applications of RL, including games, in particular, AlphaGo,\nrobotics, natural language processing, including dialogue systems, machine\ntranslation, and text generation, computer vision, neural architecture design,\nbusiness management, finance, healthcare, Industry 4.0, smart grid, intelligent\ntransportation systems, and computer systems. We mention topics not reviewed\nyet, and list a collection of RL resources. After presenting a brief summary,\nwe close with discussions.\n  Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant\nupdate.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 11:52:11 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 16:38:08 GMT"}, {"version": "v3", "created": "Sat, 15 Jul 2017 01:49:43 GMT"}, {"version": "v4", "created": "Sun, 3 Sep 2017 12:39:11 GMT"}, {"version": "v5", "created": "Fri, 15 Sep 2017 13:12:26 GMT"}, {"version": "v6", "created": "Mon, 26 Nov 2018 04:56:31 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Li", "Yuxi", ""]]}, {"id": "1701.07403", "submitter": "Alexander Keller", "authors": "Ken Dahm and Alexander Keller", "title": "Learning Light Transport the Reinforced Way", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the equations of reinforcement learning and light transport\nsimulation are related integral equations. Based on this correspondence, a\nscheme to learn importance while sampling path space is derived. The new\napproach is demonstrated in a consistent light transport simulation algorithm\nthat uses reinforcement learning to progressively learn where light comes from.\nAs using this information for importance sampling includes information about\nvisibility, too, the number of light transport paths with zero contribution is\ndramatically reduced, resulting in much less noisy images within a fixed time\nbudget.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 17:50:19 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 12:57:10 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Dahm", "Ken", ""], ["Keller", "Alexander", ""]]}, {"id": "1701.07422", "submitter": "Amirhossein Javaheri", "authors": "Amirhossein Javaheri, Hadi Zayyani and Farokh Marvasti", "title": "A Convex Similarity Index for Sparse Recovery of Missing Image Samples", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of recovering missing samples using\nmethods based on sparse representation adapted especially for image signals.\nInstead of $l_2$-norm or Mean Square Error (MSE), a new perceptual quality\nmeasure is used as the similarity criterion between the original and the\nreconstructed images. The proposed criterion called Convex SIMilarity (CSIM)\nindex is a modified version of the Structural SIMilarity (SSIM) index, which\ndespite its predecessor, is convex and uni-modal. We derive mathematical\nproperties for the proposed index and show how to optimally choose the\nparameters of the proposed criterion, investigating the Restricted Isometry\n(RIP) and error-sensitivity properties. We also propose an iterative sparse\nrecovery method based on a constrained $l_1$-norm minimization problem,\nincorporating CSIM as the fidelity criterion. The resulting convex optimization\nproblem is solved via an algorithm based on Alternating Direction Method of\nMultipliers (ADMM). Taking advantage of the convexity of the CSIM index, we\nalso prove the convergence of the algorithm to the globally optimal solution of\nthe proposed optimization problem, starting from any arbitrary point.\nSimulation results confirm the performance of the new similarity index as well\nas the proposed algorithm for missing sample recovery of image patch signals.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 18:49:45 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 18:59:41 GMT"}, {"version": "v3", "created": "Tue, 17 Oct 2017 15:17:59 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Javaheri", "Amirhossein", ""], ["Zayyani", "Hadi", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1701.07429", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Robust mixture of experts modeling using the $t$ distribution", "comments": "arXiv admin note: substantial text overlap with arXiv:1506.06707,\n  arXiv:1612.06879", "journal-ref": "Neural Networks 79: 20-36 (2016)", "doi": "10.1016/j.neunet.2016.03.002", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of Experts (MoE) is a popular framework for modeling heterogeneity in\ndata for regression, classification, and clustering. For regression and cluster\nanalyses of continuous data, MoE usually use normal experts following the\nGaussian distribution. However, for a set of data containing a group or groups\nof observations with heavy tails or atypical observations, the use of normal\nexperts is unsuitable and can unduly affect the fit of the MoE model. We\nintroduce a robust MoE modeling using the $t$ distribution. The proposed $t$\nMoE (TMoE) deals with these issues regarding heavy-tailed and noisy data. We\ndevelop a dedicated expectation-maximization (EM) algorithm to estimate the\nparameters of the proposed model by monotonically maximizing the observed data\nlog-likelihood. We describe how the presented model can be used in prediction\nand in model-based clustering of regression data. The proposed model is\nvalidated on numerical experiments carried out on simulated data, which show\nthe effectiveness and the robustness of the proposed model in terms of modeling\nnon-linear regression functions as well as in model-based clustering. Then, it\nis applied to the real-world data of tone perception for musical data analysis,\nand the one of temperature anomalies for the analysis of climate change data.\nThe obtained results show the usefulness of the TMoE model for practical\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 14:42:40 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1701.07474", "submitter": "Zhengping Che", "authors": "Zhengping Che, Yu Cheng, Zhaonan Sun, Yan Liu", "title": "Exploiting Convolutional Neural Network for Risk Prediction with Medical\n  Feature Embedding", "comments": "NIPS 2016 Workshop on Machine Learning for Health (ML4HC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread availability of electronic health records (EHRs) promises to\nusher in the era of personalized medicine. However, the problem of extracting\nuseful clinical representations from longitudinal EHR data remains challenging.\nIn this paper, we explore deep neural network models with learned medical\nfeature embedding to deal with the problems of high dimensionality and\ntemporality. Specifically, we use a multi-layer convolutional neural network\n(CNN) to parameterize the model and is thus able to capture complex non-linear\nlongitudinal evolution of EHRs. Our model can effectively capture local/short\ntemporal dependency in EHRs, which is beneficial for risk prediction. To\naccount for high dimensionality, we use the embedding medical features in the\nCNN model which hold the natural medical concepts. Our initial experiments\nproduce promising results and demonstrate the effectiveness of both the medical\nfeature embedding and the proposed convolutional neural network in risk\nprediction on cohorts of congestive heart failure and diabetes patients\ncompared with several strong baselines.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 20:25:29 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Che", "Zhengping", ""], ["Cheng", "Yu", ""], ["Sun", "Zhaonan", ""], ["Liu", "Yan", ""]]}, {"id": "1701.07483", "submitter": "Ashwin Venkataraman", "authors": "Srikanth Jagabathula, Lakshminarayanan Subramanian, Ashwin\n  Venkataraman", "title": "A Model-based Projection Technique for Segmenting Customers", "comments": "51 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of segmenting a large population of customers into\nnon-overlapping groups with similar preferences, using diverse preference\nobservations such as purchases, ratings, clicks, etc. over subsets of items. We\nfocus on the setting where the universe of items is large (ranging from\nthousands to millions) and unstructured (lacking well-defined attributes) and\neach customer provides observations for only a few items. These data\ncharacteristics limit the applicability of existing techniques in marketing and\nmachine learning. To overcome these limitations, we propose a model-based\nprojection technique, which transforms the diverse set of observations into a\nmore comparable scale and deals with missing data by projecting the transformed\ndata onto a low-dimensional space. We then cluster the projected data to obtain\nthe customer segments. Theoretically, we derive precise necessary and\nsufficient conditions that guarantee asymptotic recovery of the true customer\nsegments. Empirically, we demonstrate the speed and performance of our method\nin two real-world case studies: (a) 84% improvement in the accuracy of new\nmovie recommendations on the MovieLens data set and (b) 6% improvement in the\nperformance of similar item recommendations algorithm on an offline dataset at\neBay. We show that our method outperforms standard latent-class and\ndemographic-based techniques.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 20:47:40 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Jagabathula", "Srikanth", ""], ["Subramanian", "Lakshminarayanan", ""], ["Venkataraman", "Ashwin", ""]]}, {"id": "1701.07543", "submitter": "Jekan Thangavelautham", "authors": "Pranay Gankidi and Jekan Thangavelautham", "title": "FPGA Architecture for Deep Learning and its application to Planetary\n  Robotics", "comments": "8 pages, 10 figures in Proceedings of the IEEE Aerospace Conference\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG astro-ph.IM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous control systems onboard planetary rovers and spacecraft benefit\nfrom having cognitive capabilities like learning so that they can adapt to\nunexpected situations in-situ. Q-learning is a form of reinforcement learning\nand it has been efficient in solving certain class of learning problems.\nHowever, embedded systems onboard planetary rovers and spacecraft rarely\nimplement learning algorithms due to the constraints faced in the field, like\nprocessing power, chip size, convergence rate and costs due to the need for\nradiation hardening. These challenges present a compelling need for a portable,\nlow-power, area efficient hardware accelerator to make learning algorithms\npractical onboard space hardware. This paper presents a FPGA implementation of\nQ-learning with Artificial Neural Networks (ANN). This method matches the\nmassive parallelism inherent in neural network software with the fine-grain\nparallelism of an FPGA hardware thereby dramatically reducing processing time.\nMars Science Laboratory currently uses Xilinx-Space-grade Virtex FPGA devices\nfor image processing, pyrotechnic operation control and obstacle avoidance. We\nsimulate and program our architecture on a Xilinx Virtex 7 FPGA. The\narchitectural implementation for a single neuron Q-learning and a more complex\nMultilayer Perception (MLP) Q-learning accelerator has been demonstrated. The\nresults show up to a 43-fold speed up by Virtex 7 FPGAs compared to a\nconventional Intel i5 2.3 GHz CPU. Finally, we simulate the proposed\narchitecture using the Symphony simulator and compiler from Xilinx, and\nevaluate the performance and power consumption.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 01:52:11 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Gankidi", "Pranay", ""], ["Thangavelautham", "Jekan", ""]]}, {"id": "1701.07570", "submitter": "Lijun Zhang", "authors": "Lijun Zhang, Tianbao Yang, Rong Jin, Zhi-Hua Zhou", "title": "Dynamic Regret of Strongly Adaptive Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To cope with changing environments, recent developments in online learning\nhave introduced the concepts of adaptive regret and dynamic regret\nindependently. In this paper, we illustrate an intrinsic connection between\nthese two concepts by showing that the dynamic regret can be expressed in terms\nof the adaptive regret and the functional variation. This observation implies\nthat strongly adaptive algorithms can be directly leveraged to minimize the\ndynamic regret. As a result, we present a series of strongly adaptive\nalgorithms that have small dynamic regrets for convex functions, exponentially\nconcave functions, and strongly convex functions, respectively. To the best of\nour knowledge, this is the first time that exponential concavity is utilized to\nupper bound the dynamic regret. Moreover, all of those adaptive algorithms do\nnot need any prior knowledge of the functional variation, which is a\nsignificant advantage over previous specialized methods for minimizing dynamic\nregret.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 03:54:21 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 15:12:55 GMT"}, {"version": "v3", "created": "Mon, 4 Jun 2018 13:03:22 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Zhang", "Lijun", ""], ["Yang", "Tianbao", ""], ["Jin", "Rong", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1701.07681", "submitter": "Patrick Sch\\\"afer", "authors": "Patrick Sch\\\"afer and Ulf Leser", "title": "Fast and Accurate Time Series Classification with WEASEL", "comments": null, "journal-ref": "Proceedings of the 2017 ACM on Conference on Information and\n  Knowledge Management (CIKM '17). ACM, 637-646", "doi": "10.1145/3132847.3132980", "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time series (TS) occur in many scientific and commercial applications,\nranging from earth surveillance to industry automation to the smart grids. An\nimportant type of TS analysis is classification, which can, for instance,\nimprove energy load forecasting in smart grids by detecting the types of\nelectronic devices based on their energy consumption profiles recorded by\nautomatic sensors. Such sensor-driven applications are very often characterized\nby (a) very long TS and (b) very large TS datasets needing classification.\nHowever, current methods to time series classification (TSC) cannot cope with\nsuch data volumes at acceptable accuracy; they are either scalable but offer\nonly inferior classification quality, or they achieve state-of-the-art\nclassification quality but cannot scale to large data volumes.\n  In this paper, we present WEASEL (Word ExtrAction for time SEries\ncLassification), a novel TSC method which is both scalable and accurate. Like\nother state-of-the-art TSC methods, WEASEL transforms time series into feature\nvectors, using a sliding-window approach, which are then analyzed through a\nmachine learning classifier. The novelty of WEASEL lies in its specific method\nfor deriving features, resulting in a much smaller yet much more discriminative\nfeature set. On the popular UCR benchmark of 85 TS datasets, WEASEL is more\naccurate than the best current non-ensemble algorithms at orders-of-magnitude\nlower classification and training times, and it is almost as accurate as\nensemble classifiers, whose computational complexity makes them inapplicable\neven for mid-size datasets. The outstanding robustness of WEASEL is also\nconfirmed by experiments on two real smart grid datasets, where it\nout-of-the-box achieves almost the same accuracy as highly tuned,\ndomain-specific methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 13:09:48 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Sch\u00e4fer", "Patrick", ""], ["Leser", "Ulf", ""]]}, {"id": "1701.07761", "submitter": "M. Ros\\'ario Oliveira", "authors": "Francisco Macedo and M. Ros\\'ario Oliveira and Ant\\'onio Pacheco and\n  Rui Valadas", "title": "Theoretical Foundations of Forward Feature Selection Methods based on\n  Mutual Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection problems arise in a variety of applications, such as\nmicroarray analysis, clinical prediction, text categorization, image\nclassification and face recognition, multi-label learning, and classification\nof internet traffic. Among the various classes of methods, forward feature\nselection methods based on mutual information have become very popular and are\nwidely used in practice. However, comparative evaluations of these methods have\nbeen limited by being based on specific datasets and classifiers. In this\npaper, we develop a theoretical framework that allows evaluating the methods\nbased on their theoretical properties. Our framework is grounded on the\nproperties of the target objective function that the methods try to\napproximate, and on a novel categorization of features, according to their\ncontribution to the explanation of the class; we derive upper and lower bounds\nfor the target objective function and relate these bounds with the feature\ntypes. Then, we characterize the types of approximations taken by the methods,\nand analyze how these approximations cope with the good properties of the\ntarget objective function. Additionally, we develop a distributional setting\ndesigned to illustrate the various deficiencies of the methods, and provide\nseveral examples of wrong feature selections. Based on our work, we identify\nclearly the methods that should be avoided, and the methods that currently have\nthe best performance.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 16:23:39 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 16:19:23 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Macedo", "Francisco", ""], ["Oliveira", "M. Ros\u00e1rio", ""], ["Pacheco", "Ant\u00f3nio", ""], ["Valadas", "Rui", ""]]}, {"id": "1701.07767", "submitter": "Konstantinos Slavakis", "authors": "Konstantinos Slavakis and Shiva Salsabilian and David S. Wack and\n  Sarah F. Muldoon and Henry E. Baidoo-Williams and Jean M. Vettel and Matthew\n  Cieslak and Scott T. Grafton", "title": "Riemannian-geometry-based modeling and clustering of network-wide\n  non-stationary time series: The brain-network case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper advocates Riemannian multi-manifold modeling in the context of\nnetwork-wide non-stationary time-series analysis. Time-series data, collected\nsequentially over time and across a network, yield features which are viewed as\npoints in or close to a union of multiple submanifolds of a Riemannian\nmanifold, and distinguishing disparate time series amounts to clustering\nmultiple Riemannian submanifolds. To support the claim that exploiting the\nlatent Riemannian geometry behind many statistical features of time series is\nbeneficial to learning from network data, this paper focuses on brain networks\nand puts forth two feature-generation schemes for network-wide dynamic time\nseries. The first is motivated by Granger-causality arguments and uses an\nauto-regressive moving average model to map low-rank linear vector subspaces,\nspanned by column vectors of appropriately defined observability matrices, to\npoints into the Grassmann manifold. The second utilizes (non-linear)\ndependencies among network nodes by introducing kernel-based partial\ncorrelations to generate points in the manifold of positive-definite matrices.\nCapitilizing on recently developed research on clustering Riemannian\nsubmanifolds, an algorithm is provided for distinguishing time series based on\ntheir geometrical properties, revealed within Riemannian feature spaces.\nExtensive numerical tests demonstrate that the proposed framework outperforms\nclassical and state-of-the-art techniques in clustering brain-network\nstates/structures hidden beneath synthetic fMRI time series and brain-activity\nsignals generated from real brain-network structural connectivity matrices.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 16:38:00 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Slavakis", "Konstantinos", ""], ["Salsabilian", "Shiva", ""], ["Wack", "David S.", ""], ["Muldoon", "Sarah F.", ""], ["Baidoo-Williams", "Henry E.", ""], ["Vettel", "Jean M.", ""], ["Cieslak", "Matthew", ""], ["Grafton", "Scott T.", ""]]}, {"id": "1701.07808", "submitter": "Chao Qu", "authors": "Chao Qu, Huan Xu", "title": "Linear convergence of SDCA in statistical estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider stochastic dual coordinate (SDCA) {\\em without}\nstrongly convex assumption or convex assumption. We show that SDCA converges\nlinearly under mild conditions termed restricted strong convexity. This covers\na wide array of popular statistical models including Lasso, group Lasso, and\nlogistic regression with $\\ell_1$ regularization, corrected Lasso and linear\nregression with SCAD regularizer. This significantly improves previous\nconvergence results on SDCA for problems that are not strongly convex. As a by\nproduct, we derive a dual free form of SDCA that can handle general\nregularization term, which is of interest by itself.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 18:37:34 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 17:54:55 GMT"}, {"version": "v3", "created": "Fri, 10 Mar 2017 22:04:09 GMT"}, {"version": "v4", "created": "Sun, 2 Apr 2017 18:43:11 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Qu", "Chao", ""], ["Xu", "Huan", ""]]}, {"id": "1701.07842", "submitter": "Nicholas V. Lewchenko", "authors": "Arjun Radhakrishna, Nicholas V. Lewchenko, Shawn Meier, Sergio Mover,\n  Krishna Chaitanya Sripada, Damien Zufferey, Bor-Yuh Evan Chang, and Pavol\n  \\v{C}ern\\'y", "title": "DroidStar: Callback Typestates for Android Classes", "comments": "Appearing at ICSE 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-driven programming frameworks, such as Android, are based on components\nwith asynchronous interfaces. The protocols for interacting with these\ncomponents can often be described by finite-state machines we dub *callback\ntypestates*. Callback typestates are akin to classical typestates, with the\ndifference that their outputs (callbacks) are produced asynchronously. While\nuseful, these specifications are not commonly available, because writing them\nis difficult and error-prone.\n  Our goal is to make the task of producing callback typestates significantly\neasier. We present a callback typestate assistant tool, DroidStar, that\nrequires only limited user interaction to produce a callback typestate. Our\napproach is based on an active learning algorithm, L*. We improved the\nscalability of equivalence queries (a key component of L*), thus making active\nlearning tractable on the Android system.\n  We use DroidStar to learn callback typestates for Android classes both for\ncases where one is already provided by the documentation, and for cases where\nthe documentation is unclear. The results show that DroidStar learns callback\ntypestates accurately and efficiently. Moreover, in several cases, the\nsynthesized callback typestates uncovered surprising and undocumented\nbehaviors.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 19:06:45 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 23:43:09 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 18:45:04 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Radhakrishna", "Arjun", ""], ["Lewchenko", "Nicholas V.", ""], ["Meier", "Shawn", ""], ["Mover", "Sergio", ""], ["Sripada", "Krishna Chaitanya", ""], ["Zufferey", "Damien", ""], ["Chang", "Bor-Yuh Evan", ""], ["\u010cern\u00fd", "Pavol", ""]]}, {"id": "1701.07852", "submitter": "Jeff Heaton Ph.D.", "authors": "Jeff Heaton", "title": "An Empirical Analysis of Feature Engineering for Predictive Modeling", "comments": null, "journal-ref": "SoutheastCon 2016", "doi": "10.1109/SECON.2016.7506650", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models, such as neural networks, decision trees, random\nforests, and gradient boosting machines, accept a feature vector, and provide a\nprediction. These models learn in a supervised fashion where we provide feature\nvectors mapped to the expected output. It is common practice to engineer new\nfeatures from the provided feature set. Such engineered features will either\naugment or replace portions of the existing feature vector. These engineered\nfeatures are essentially calculated fields based on the values of the other\nfeatures.\n  Engineering such features is primarily a manual, time-consuming task.\nAdditionally, each type of model will respond differently to different kinds of\nengineered features. This paper reports empirical research to demonstrate what\nkinds of engineered features are best suited to various machine learning model\ntypes. We provide this recommendation by generating several datasets that we\ndesigned to benefit from a particular type of engineered feature. The\nexperiment demonstrates to what degree the machine learning model can\nsynthesize the needed feature on its own. If a model can synthesize a planned\nfeature, it is not necessary to provide that feature. The research demonstrated\nthat the studied models do indeed perform differently with various types of\nengineered features.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 19:29:41 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 18:35:36 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Heaton", "Jeff", ""]]}, {"id": "1701.07875", "submitter": "Martin Arjovsky", "authors": "Martin Arjovsky, Soumith Chintala, L\\'eon Bottou", "title": "Wasserstein GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new algorithm named WGAN, an alternative to traditional GAN\ntraining. In this new model, we show that we can improve the stability of\nlearning, get rid of problems like mode collapse, and provide meaningful\nlearning curves useful for debugging and hyperparameter searches. Furthermore,\nwe show that the corresponding optimization problem is sound, and provide\nextensive theoretical work highlighting the deep connections to other distances\nbetween distributions.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 21:10:29 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 23:12:26 GMT"}, {"version": "v3", "created": "Wed, 6 Dec 2017 20:01:54 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Arjovsky", "Martin", ""], ["Chintala", "Soumith", ""], ["Bottou", "L\u00e9on", ""]]}, {"id": "1701.07895", "submitter": "Adarsh Barik", "authors": "Adarsh Barik, Jean Honorio, Mohit Tawarmalani", "title": "Information Theoretic Limits for Linear Prediction with Graph-Structured\n  Sparsity", "comments": null, "journal-ref": "2017 IEEE International Symposium on Information Theory (ISIT)", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the necessary number of samples for sparse vector recovery in a\nnoisy linear prediction setup. This model includes problems such as linear\nregression and classification. We focus on structured graph models. In\nparticular, we prove that sufficient number of samples for the weighted graph\nmodel proposed by Hegde and others is also necessary. We use the Fano's\ninequality on well constructed ensembles as our main tool in establishing\ninformation theoretic lower bounds.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 22:43:20 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 23:11:33 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Barik", "Adarsh", ""], ["Honorio", "Jean", ""], ["Tawarmalani", "Mohit", ""]]}, {"id": "1701.07953", "submitter": "Karan Singh", "authors": "Naman Agarwal and Karan Singh", "title": "The Price of Differential Privacy For Online Learning", "comments": "To appear in the Proceedings of the 34th International Conference on\n  Machine Learning (ICML), Sydney, Australia, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design differentially private algorithms for the problem of online linear\noptimization in the full information and bandit settings with optimal\n$\\tilde{O}(\\sqrt{T})$ regret bounds. In the full-information setting, our\nresults demonstrate that $\\epsilon$-differential privacy may be ensured for\nfree -- in particular, the regret bounds scale as\n$O(\\sqrt{T})+\\tilde{O}\\left(\\frac{1}{\\epsilon}\\right)$. For bandit linear\noptimization, and as a special case, for non-stochastic multi-armed bandits,\nthe proposed algorithm achieves a regret of\n$\\tilde{O}\\left(\\frac{1}{\\epsilon}\\sqrt{T}\\right)$, while the previously known\nbest regret bound was\n$\\tilde{O}\\left(\\frac{1}{\\epsilon}T^{\\frac{2}{3}}\\right)$.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 06:17:14 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 21:25:12 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Agarwal", "Naman", ""], ["Singh", "Karan", ""]]}, {"id": "1701.07974", "submitter": "Haiping Huang", "authors": "Haiping Huang and Taro Toyoizumi", "title": "Reinforced stochastic gradient descent for deep neural network learning", "comments": "12 pages and 9 figures, nearly final version as a technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is a standard optimization method to\nminimize a training error with respect to network parameters in modern neural\nnetwork learning. However, it typically suffers from proliferation of saddle\npoints in the high-dimensional parameter space. Therefore, it is highly\ndesirable to design an efficient algorithm to escape from these saddle points\nand reach a parameter region of better generalization capabilities. Here, we\npropose a simple extension of SGD, namely reinforced SGD, which simply adds\nprevious first-order gradients in a stochastic manner with a probability that\nincreases with learning time. As verified in a simple synthetic dataset, this\nmethod significantly accelerates learning compared with the original SGD.\nSurprisingly, it dramatically reduces over-fitting effects, even compared with\nstate-of-the-art adaptive learning algorithm---Adam. For a benchmark\nhandwritten digits dataset, the learning performance is comparable to Adam, yet\nwith an extra advantage of requiring one-fold less computer memory. The\nreinforced SGD is also compared with SGD with fixed or adaptive momentum\nparameter and Nesterov's momentum, which shows that the proposed framework is\nable to reach a similar generalization accuracy with less computational costs.\nOverall, our method introduces stochastic memory into gradients, which plays an\nimportant role in understanding how gradient-based training algorithms can work\nand its relationship with generalization abilities of deep networks.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 08:49:19 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 05:34:08 GMT"}, {"version": "v3", "created": "Wed, 24 May 2017 09:30:54 GMT"}, {"version": "v4", "created": "Tue, 19 Sep 2017 02:57:24 GMT"}, {"version": "v5", "created": "Wed, 22 Nov 2017 08:27:39 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Huang", "Haiping", ""], ["Toyoizumi", "Taro", ""]]}, {"id": "1701.08055", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J. Kir\\'aly and Zhaozhi Qian", "title": "Modelling Competitive Sports: Bradley-Terry-\\'{E}l\\H{o} Models for\n  Supervised and On-Line Learning of Paired Competition Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction and modelling of competitive sports outcomes has received much\nrecent attention, especially from the Bayesian statistics and machine learning\ncommunities. In the real world setting of outcome prediction, the seminal\n\\'{E}l\\H{o} update still remains, after more than 50 years, a valuable baseline\nwhich is difficult to improve upon, though in its original form it is a\nheuristic and not a proper statistical \"model\". Mathematically, the \\'{E}l\\H{o}\nrating system is very closely related to the Bradley-Terry models, which are\nusually used in an explanatory fashion rather than in a predictive supervised\nor on-line learning setting.\n  Exploiting this close link between these two model classes and some newly\nobserved similarities, we propose a new supervised learning framework with\nclose similarities to logistic regression, low-rank matrix completion and\nneural networks. Building on it, we formulate a class of structured log-odds\nmodels, unifying the desirable properties found in the above: supervised\nprobabilistic prediction of scores and wins/draws/losses, batch/epoch and\non-line learning, as well as the possibility to incorporate features in the\nprediction, without having to sacrifice simplicity, parsimony of the\nBradley-Terry models, or computational efficiency of \\'{E}l\\H{o}'s original\napproach.\n  We validate the structured log-odds modelling approach in synthetic\nexperiments and English Premier League outcomes, where the added expressivity\nyields the best predictions reported in the state-of-art, close to the quality\nof contemporary betting odds.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 14:01:53 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""], ["Qian", "Zhaozhi", ""]]}, {"id": "1701.08074", "submitter": "Frederik Ruelens", "authors": "Bert J. Claessens, Dirk Vanhoudt, Johan Desmedt, Frederik Ruelens", "title": "Model-Free Control of Thermostatically Controlled Loads Connected to a\n  District Heating Network", "comments": "Under review at Elsevier: Energy and buildings 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal control of thermostatically controlled loads connected to a district\nheating network is considered a sequential decision- making problem under\nuncertainty. The practicality of a direct model-based approach is compromised\nby two challenges, namely scalability due to the large dimensionality of the\nproblem and the system identification required to identify an accurate model.\nTo help in mitigating these problems, this paper leverages on recent\ndevelopments in reinforcement learning in combination with a market-based\nmulti-agent system to obtain a scalable solution that obtains a significant\nperformance improvement in a practical learning time. The control approach is\napplied on a scenario comprising 100 thermostatically controlled loads\nconnected to a radial district heating network supplied by a central combined\nheat and power plant. Both for an energy arbitrage and a peak shaving\nobjective, the control approach requires 60 days to obtain a performance within\n65% of a theoretical lower bound on the cost.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 15:15:54 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 15:59:34 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Claessens", "Bert J.", ""], ["Vanhoudt", "Dirk", ""], ["Desmedt", "Johan", ""], ["Ruelens", "Frederik", ""]]}, {"id": "1701.08106", "submitter": "Vivek Nair", "authors": "Vivek Nair, Tim Menzies, Norbert Siegmund, Sven Apel", "title": "Faster Discovery of Faster System Configurations with Spectral Learning", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": "10.1007/s1051", "report-no": null, "categories": "cs.SE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the huge spread and economical importance of configurable software\nsystems, there is unsatisfactory support in utilizing the full potential of\nthese systems with respect to finding performance-optimal configurations. Prior\nwork on predicting the performance of software configurations suffered from\neither (a) requiring far too many sample configurations or (b) large variances\nin their predictions. Both these problems can be avoided using the WHAT\nspectral learner. WHAT's innovation is the use of the spectrum (eigenvalues) of\nthe distance matrix between the configurations of a configurable software\nsystem, to perform dimensionality reduction. Within that reduced configuration\nspace, many closely associated configurations can be studied by executing only\na few sample configurations. For the subject systems studied here, a few dozen\nsamples yield accurate and stable predictors - less than 10% prediction error,\nwith a standard deviation of less than 2%. When compared to the state of the\nart, WHAT (a) requires 2 to 10 times fewer samples to achieve similar\nprediction accuracies, and (b) its predictions are more stable (i.e., have\nlower standard deviation). Furthermore, we demonstrate that predictive models\ngenerated by WHAT can be used by optimizers to discover system configurations\nthat closely approach the optimal performance.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 16:36:09 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 21:15:47 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Nair", "Vivek", ""], ["Menzies", "Tim", ""], ["Siegmund", "Norbert", ""], ["Apel", "Sven", ""]]}, {"id": "1701.08305", "submitter": "Pan Li", "authors": "Pan Li and Olgica Milenkovic", "title": "Multiclass MinMax Rank Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new family of minmax rank aggregation problems under two\ndistance measures, the Kendall {\\tau} and the Spearman footrule. As the\nproblems are NP-hard, we proceed to describe a number of constant-approximation\nalgorithms for solving them. We conclude with illustrative applications of the\naggregation methods on the Mallows model and genomic data.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 17:45:58 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Li", "Pan", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1701.08318", "submitter": "Xueliang (Leon) Liu", "authors": "Xueliang Liu", "title": "Deep Recurrent Neural Network for Protein Function Prediction from\n  Sequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As high-throughput biological sequencing becomes faster and cheaper, the need\nto extract useful information from sequencing becomes ever more paramount,\noften limited by low-throughput experimental characterizations. For proteins,\naccurate prediction of their functions directly from their primary amino-acid\nsequences has been a long standing challenge. Here, machine learning using\nartificial recurrent neural networks (RNN) was applied towards classification\nof protein function directly from primary sequence without sequence alignment,\nheuristic scoring or feature engineering. The RNN models containing\nlong-short-term-memory (LSTM) units trained on public, annotated datasets from\nUniProt achieved high performance for in-class prediction of four important\nprotein functions tested, particularly compared to other machine learning\nalgorithms using sequence-derived protein features. RNN models were used also\nfor out-of-class predictions of phylogenetically distinct protein families with\nsimilar functions, including proteins of the CRISPR-associated nuclease,\nferritin-like iron storage and cytochrome P450 families. Applying the trained\nRNN models on the partially unannotated UniRef100 database predicted not only\ncandidates validated by existing annotations but also currently unannotated\nsequences. Some RNN predictions for the ferritin-like iron sequestering\nfunction were experimentally validated, even though their sequences differ\nsignificantly from known, characterized proteins and from each other and cannot\nbe easily predicted using popular bioinformatics methods. As sequencing and\nexperimental characterization data increases rapidly, the machine-learning\napproach based on RNN could be useful for discovery and prediction of\nhomologues for a wide range of protein functions.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 19:33:59 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Liu", "Xueliang", ""]]}, {"id": "1701.08374", "submitter": "Habib Ghaffari Hadigheh", "authors": "Habib Ghaffari Hadigheh and Ghazali bin sulong", "title": "Feature base fusion for splicing forgery detection based on neuro fuzzy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of researches on image forensics have been mainly focused on detection\nof artifacts introduced by a single processing tool. They lead in the\ndevelopment of many specialized algorithms looking for one or more particular\nfootprints under specific settings. Naturally, the performance of such\nalgorithms are not perfect, and accordingly the provided output might be noisy,\ninaccurate and only partially correct. Furthermore, a forged image in practical\nscenarios is often the result of utilizing several tools available by\nimage-processing software systems. Therefore, reliable tamper detection\nrequires developing more poweful tools to deal with various tempering\nscenarios. Fusion of forgery detection tools based on Fuzzy Inference System\nhas been used before for addressing this problem. Adjusting the membership\nfunctions and defining proper fuzzy rules for attaining to better results are\ntime-consuming processes. This can be accounted as main disadvantage of fuzzy\ninference systems. In this paper, a Neuro-Fuzzy inference system for fusion of\nforgery detection tools is developed. The neural network characteristic of\nthese systems provides appropriate tool for automatically adjusting the\nmembership functions. Moreover, initial fuzzy inference system is generated\nbased on fuzzy clustering techniques. The proposed framework is implemented and\nvalidated on a benchmark image splicing data set in which three forgery\ndetection tools are fused based on adaptive Neuro-Fuzzy inference system. The\noutcome of the proposed method reveals that applying Neuro Fuzzy inference\nsystems could be a better approach for fusion of forgery detection tools.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 13:19:07 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Hadigheh", "Habib Ghaffari", ""], ["sulong", "Ghazali bin", ""]]}, {"id": "1701.08401", "submitter": "Dimitri Van De Ville", "authors": "Dimitri Van De Ville, Robin Demesmaeker, Maria Giulia Preti", "title": "When Slepian Meets Fiedler: Putting a Focus on the Graph Spectrum", "comments": "4 pages, 5 figures, submitted to IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2017.2704359", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of complex systems benefits from graph models and their analysis.\nIn particular, the eigendecomposition of the graph Laplacian lets emerge\nproperties of global organization from local interactions; e.g., the Fiedler\nvector has the smallest non-zero eigenvalue and plays a key role for graph\nclustering. Graph signal processing focusses on the analysis of signals that\nare attributed to the graph nodes. The eigendecomposition of the graph\nLaplacian allows to define the graph Fourier transform and extend conventional\nsignal-processing operations to graphs. Here, we introduce the design of\nSlepian graph signals, by maximizing energy concentration in a predefined\nsubgraph for a graph spectral bandlimit. We establish a novel link with\nclassical Laplacian embedding and graph clustering, which provides a meaning to\nlocalized graph frequencies.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 17:11:13 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 12:53:26 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Van De Ville", "Dimitri", ""], ["Demesmaeker", "Robin", ""], ["Preti", "Maria Giulia", ""]]}, {"id": "1701.08423", "submitter": "Vincent Cohen-Addad", "authors": "Vincent Cohen-Addad, Chris Schwiegelshohn", "title": "On the Local Structure of Stable Clustering Instances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classic $k$-median and $k$-means clustering objectives in the\nbeyond-worst-case scenario. We consider three well-studied notions of\nstructured data that aim at characterizing real-world inputs: Distribution\nStability (introduced by Awasthi, Blum, and Sheffet, FOCS 2010), Spectral\nSeparability (introduced by Kumar and Kannan, FOCS 2010), Perturbation\nResilience (introduced by Bilu and Linial, ICS 2010).\n  We prove structural results showing that inputs satisfying at least one of\nthe conditions are inherently \"local\". Namely, for any such input, any local\noptimum is close both in term of structure and in term of objective value to\nthe global optima.\n  As a corollary we obtain that the widely-used Local Search algorithm has\nstrong performance guarantees for both the tasks of recovering the underlying\noptimal clustering and obtaining a clustering of small cost. This is a\nsignificant step toward understanding the success of local search heuristics in\nclustering applications.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 19:55:27 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 08:46:26 GMT"}, {"version": "v3", "created": "Thu, 10 Aug 2017 09:46:07 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Schwiegelshohn", "Chris", ""]]}, {"id": "1701.08435", "submitter": "Joost van Amersfoort", "authors": "Joost van Amersfoort, Anitha Kannan, Marc'Aurelio Ranzato, Arthur\n  Szlam, Du Tran and Soumith Chintala", "title": "Transformation-Based Models of Video Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a simple unsupervised approach for next frame\nprediction in video. Instead of directly predicting the pixels in a frame given\npast frames, we predict the transformations needed for generating the next\nframe in a sequence, given the transformations of the past frames. This leads\nto sharper results, while using a smaller prediction model.\n  In order to enable a fair comparison between different video frame prediction\nmodels, we also propose a new evaluation protocol. We use generated frames as\ninput to a classifier trained with ground truth sequences. This criterion\nguarantees that models scoring high are those producing sequences which\npreserve discrim- inative features, as opposed to merely penalizing any\ndeviation, plausible or not, from the ground truth. Our proposed approach\ncompares favourably against more sophisticated ones on the UCF-101 data set,\nwhile also being more efficient in terms of the number of parameters and\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 21:39:05 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 20:20:40 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["van Amersfoort", "Joost", ""], ["Kannan", "Anitha", ""], ["Ranzato", "Marc'Aurelio", ""], ["Szlam", "Arthur", ""], ["Tran", "Du", ""], ["Chintala", "Soumith", ""]]}, {"id": "1701.08466", "submitter": "EPTCS", "authors": "Andrew Healy (Maynooth University), Rosemary Monahan (Maynooth\n  University), James F. Power (Maynooth University)", "title": "Predicting SMT Solver Performance for Software Verification", "comments": "In Proceedings F-IDE 2016, arXiv:1701.07925", "journal-ref": "EPTCS 240, 2017, pp. 20-37", "doi": "10.4204/EPTCS.240.2", "report-no": null, "categories": "cs.SE cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Why3 IDE and verification system facilitates the use of a wide range of\nSatisfiability Modulo Theories (SMT) solvers through a driver-based\narchitecture. We present Where4: a portfolio-based approach to discharge Why3\nproof obligations. We use data analysis and machine learning techniques on\nstatic metrics derived from program source code. Our approach benefits software\nengineers by providing a single utility to delegate proof obligations to the\nsolvers most likely to return a useful result. It does this in a time-efficient\nway using existing Why3 and solver installations - without requiring low-level\nknowledge about SMT solver operation from the user.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 03:32:24 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Healy", "Andrew", "", "Maynooth University"], ["Monahan", "Rosemary", "", "Maynooth\n  University"], ["Power", "James F.", "", "Maynooth University"]]}, {"id": "1701.08473", "submitter": "Quang N. Tran", "authors": "Ba-Ngu Vo, Quang N. Tran, Dinh Phung, Ba-Tuong Vo", "title": "Model-based Classification and Novelty Detection For Point Pattern Data", "comments": "Prepint: 23rd Int. Conf. Pattern Recognition (ICPR). Cancun, Mexico,\n  December 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point patterns are sets or multi-sets of unordered elements that can be found\nin numerous data sources. However, in data analysis tasks such as\nclassification and novelty detection, appropriate statistical models for point\npattern data have not received much attention. This paper proposes the\nmodelling of point pattern data via random finite sets (RFS). In particular, we\npropose appropriate likelihood functions, and a maximum likelihood estimator\nfor learning a tractable family of RFS models. In novelty detection, we propose\nnovel ranking functions based on RFS models, which substantially improve\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 03:47:44 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 03:44:39 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Vo", "Ba-Ngu", ""], ["Tran", "Quang N.", ""], ["Phung", "Dinh", ""], ["Vo", "Ba-Tuong", ""]]}, {"id": "1701.08511", "submitter": "Diego Valsesia", "authors": "Diego Valsesia, Enrico Magli", "title": "Binary adaptive embeddings from order statistics of random projections", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2016.2639036", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use some of the largest order statistics of the random projections of a\nreference signal to construct a binary embedding that is adapted to signals\ncorrelated with such signal. The embedding is characterized from the analytical\nstandpoint and shown to provide improved performance on tasks such as\nclassification in a reduced-dimensionality space.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 08:37:25 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Valsesia", "Diego", ""], ["Magli", "Enrico", ""]]}, {"id": "1701.08528", "submitter": "Martin J\\\"anicke", "authors": "David Bannach, Martin J\\\"anicke, Vitor F. Rey, Sven Tomforde, Bernhard\n  Sick, Paul Lukowicz", "title": "Self-Adaptation of Activity Recognition Systems to New Sensors", "comments": "26 pages, very descriptive figures, comprehensive evaluation on\n  real-life datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional activity recognition systems work on the basis of training,\ntaking a fixed set of sensors into account. In this article, we focus on the\nquestion how pattern recognition can leverage new information sources without\nany, or with minimal user input. Thus, we present an approach for opportunistic\nactivity recognition, where ubiquitous sensors lead to dynamically changing\ninput spaces. Our method is a variation of well-established principles of\nmachine learning, relying on unsupervised clustering to discover structure in\ndata and inferring cluster labels from a small number of labeled dates in a\nsemi-supervised manner. Elaborating the challenges, evaluations of over 3000\nsensor combinations from three multi-user experiments are presented in detail\nand show the potential benefit of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 10:01:38 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Bannach", "David", ""], ["J\u00e4nicke", "Martin", ""], ["Rey", "Vitor F.", ""], ["Tomforde", "Sven", ""], ["Sick", "Bernhard", ""], ["Lukowicz", "Paul", ""]]}, {"id": "1701.08585", "submitter": "Yichen Wang", "authors": "Yichen Wang, Grady Williams, Evangelos Theodorou, Le Song", "title": "Variational Policy for Guiding Point Processes", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal point processes have been widely applied to model event sequence\ndata generated by online users. In this paper, we consider the problem of how\nto design the optimal control policy for point processes, such that the\nstochastic system driven by the point process is steered to a target state. In\nparticular, we exploit the key insight to view the stochastic optimal control\nproblem from the perspective of optimal measure and variational inference. We\nfurther propose a convex optimization framework and an efficient algorithm to\nupdate the policy adaptively to the current system state. Experiments on\nsynthetic and real-world data show that our algorithm can steer the user\nactivities much more accurately and efficiently than other stochastic control\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 13:24:07 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 18:44:24 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 19:18:19 GMT"}, {"version": "v4", "created": "Fri, 10 Nov 2017 15:09:15 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Wang", "Yichen", ""], ["Williams", "Grady", ""], ["Theodorou", "Evangelos", ""], ["Song", "Le", ""]]}, {"id": "1701.08694", "submitter": "Saiful Islam Md", "authors": "Md. Saiful Islam, Fazla Elahi Md Jubayer and Syed Ikhtiar Ahmed", "title": "A Comparative Study on Different Types of Approaches to Bengali document\n  Categorization", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document categorization is a technique where the category of a document is\ndetermined. In this paper three well-known supervised learning techniques which\nare Support Vector Machine(SVM), Na\\\"ive Bayes(NB) and Stochastic Gradient\nDescent(SGD) compared for Bengali document categorization. Besides classifier,\nclassification also depends on how feature is selected from dataset. For\nanalyzing those classifier performances on predicting a document against twelve\ncategories several feature selection techniques are also applied in this\narticle namely Chi square distribution, normalized TFIDF (term\nfrequency-inverse document frequency) with word analyzer. So, we attempt to\nexplore the efficiency of those three-classification algorithms by using two\ndifferent feature selection techniques in this article.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 13:08:08 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Islam", "Md. Saiful", ""], ["Jubayer", "Fazla Elahi Md", ""], ["Ahmed", "Syed Ikhtiar", ""]]}, {"id": "1701.08711", "submitter": "Yan Chi Vinci Chow", "authors": "Vinci Chow", "title": "Predicting Auction Price of Vehicle License Plate with Deep Recurrent\n  Neural Network", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2019.113008", "report-no": null, "categories": "cs.CL cs.LG q-fin.EC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Chinese societies, superstition is of paramount importance, and vehicle\nlicense plates with desirable numbers can fetch very high prices in auctions.\nUnlike other valuable items, license plates are not allocated an estimated\nprice before auction. I propose that the task of predicting plate prices can be\nviewed as a natural language processing (NLP) task, as the value depends on the\nmeaning of each individual character on the plate and its semantics. I\nconstruct a deep recurrent neural network (RNN) to predict the prices of\nvehicle license plates in Hong Kong, based on the characters on a plate. I\ndemonstrate the importance of having a deep network and of retraining.\nEvaluated on 13 years of historical auction prices, the deep RNN's predictions\ncan explain over 80 percent of price variations, outperforming previous models\nby a significant margin. I also demonstrate how the model can be extended to\nbecome a search engine for plates and to provide estimates of the expected\nprice distribution.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 17:14:25 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 17:41:35 GMT"}, {"version": "v3", "created": "Tue, 14 Feb 2017 16:36:38 GMT"}, {"version": "v4", "created": "Wed, 13 Feb 2019 08:34:32 GMT"}, {"version": "v5", "created": "Tue, 8 Oct 2019 16:25:45 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Chow", "Vinci", ""]]}, {"id": "1701.08716", "submitter": "Shi Zong", "authors": "Shi Zong, Branislav Kveton, Shlomo Berkovsky, Azin Ashkan, Nikos\n  Vlassis, Zheng Wen", "title": "Does Weather Matter? Causal Analysis of TV Logs", "comments": "Companion of the 26th International World Wide Web Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weather affects our mood and behaviors, and many aspects of our life. When it\nis sunny, most people become happier; but when it rains, some people get\ndepressed. Despite this evidence and the abundance of data, weather has mostly\nbeen overlooked in the machine learning and data science research. This work\npresents a causal analysis of how weather affects TV watching patterns. We show\nthat some weather attributes, such as pressure and precipitation, cause major\nchanges in TV watching patterns. To the best of our knowledge, this is the\nfirst large-scale causal study of the impact of weather on TV watching\npatterns.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 17:07:33 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 18:14:26 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Zong", "Shi", ""], ["Kveton", "Branislav", ""], ["Berkovsky", "Shlomo", ""], ["Ashkan", "Azin", ""], ["Vlassis", "Nikos", ""], ["Wen", "Zheng", ""]]}, {"id": "1701.08718", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "Caglar Gulcehre, Sarath Chandar, Yoshua Bengio", "title": "Memory Augmented Neural Networks with Wormhole Connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent empirical results on long-term dependency tasks have shown that neural\nnetworks augmented with an external memory can learn the long-term dependency\ntasks more easily and achieve better generalization than vanilla recurrent\nneural networks (RNN). We suggest that memory augmented neural networks can\nreduce the effects of vanishing gradients by creating shortcut (or wormhole)\nconnections. Based on this observation, we propose a novel memory augmented\nneural network model called TARDIS (Temporal Automatic Relation Discovery in\nSequences). The controller of TARDIS can store a selective set of embeddings of\nits own previous hidden states into an external memory and revisit them as and\nwhen needed. For TARDIS, memory acts as a storage for wormhole connections to\nthe past to propagate the gradients more effectively and it helps to learn the\ntemporal dependencies. The memory structure of TARDIS has similarities to both\nNeural Turing Machines (NTM) and Dynamic Neural Turing Machines (D-NTM), but\nboth read and write operations of TARDIS are simpler and more efficient. We use\ndiscrete addressing for read/write operations which helps to substantially to\nreduce the vanishing gradient problem with very long sequences. Read and write\noperations in TARDIS are tied with a heuristic once the memory becomes full,\nand this makes the learning problem simpler when compared to NTM or D-NTM type\nof architectures. We provide a detailed analysis on the gradient propagation in\ngeneral for MANNs. We evaluate our models on different long-term dependency\ntasks and report competitive results in all of them.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 17:34:51 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Chandar", "Sarath", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1701.08734", "submitter": "Chrisantha Fernando Dr", "authors": "Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols,\n  David Ha, Andrei A. Rusu, Alexander Pritzel, Daan Wierstra", "title": "PathNet: Evolution Channels Gradient Descent in Super Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For artificial general intelligence (AGI) it would be efficient if multiple\nusers trained the same giant neural network, permitting parameter reuse,\nwithout catastrophic forgetting. PathNet is a first step in this direction. It\nis a neural network algorithm that uses agents embedded in the neural network\nwhose task is to discover which parts of the network to re-use for new tasks.\nAgents are pathways (views) through the network which determine the subset of\nparameters that are used and updated by the forwards and backwards passes of\nthe backpropogation algorithm. During learning, a tournament selection genetic\nalgorithm is used to select pathways through the neural network for replication\nand mutation. Pathway fitness is the performance of that pathway measured\naccording to a cost function. We demonstrate successful transfer learning;\nfixing the parameters along a path learned on task A and re-evolving a new\npopulation of paths for task B, allows task B to be learned faster than it\ncould be learned from scratch or after fine-tuning. Paths evolved on task B\nre-use parts of the optimal path evolved on task A. Positive transfer was\ndemonstrated for binary MNIST, CIFAR, and SVHN supervised learning\nclassification tasks, and a set of Atari and Labyrinth reinforcement learning\ntasks, suggesting PathNets have general applicability for neural network\ntraining. Finally, PathNet also significantly improves the robustness to\nhyperparameter choices of a parallel asynchronous reinforcement learning\nalgorithm (A3C).\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 18:06:07 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Fernando", "Chrisantha", ""], ["Banarse", "Dylan", ""], ["Blundell", "Charles", ""], ["Zwols", "Yori", ""], ["Ha", "David", ""], ["Rusu", "Andrei A.", ""], ["Pritzel", "Alexander", ""], ["Wierstra", "Daan", ""]]}, {"id": "1701.08744", "submitter": "Junaid Effendi", "authors": "Muhammad Junaid Effendi and Syed Abbas Ali", "title": "Click Through Rate Prediction for Contextual Advertisment Using Linear\n  Regression", "comments": "8 pages, 13 Figures, 11 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This research presents an innovative and unique way of solving the\nadvertisement prediction problem which is considered as a learning problem over\nthe past several years. Online advertising is a multi-billion-dollar industry\nand is growing every year with a rapid pace. The goal of this research is to\nenhance click through rate of the contextual advertisements using Linear\nRegression. In order to address this problem, a new technique propose in this\npaper to predict the CTR which will increase the overall revenue of the system\nby serving the advertisements more suitable to the viewers with the help of\nfeature extraction and displaying the advertisements based on context of the\npublishers. The important steps include the data collection, feature\nextraction, CTR prediction and advertisement serving. The statistical results\nobtained from the dynamically used technique show an efficient outcome by\nfitting the data close to perfection for the LR technique using optimized\nfeature selection.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 18:32:59 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Effendi", "Muhammad Junaid", ""], ["Ali", "Syed Abbas", ""]]}, {"id": "1701.08757", "submitter": "Dmitry Ignatov", "authors": "Mikhail V. Goubko and Sergey O. Kuznetsov and Alexey A. Neznanov and\n  Dmitry I. Ignatov", "title": "Bayesian Learning of Consumer Preferences for Residential Demand\n  Response", "comments": null, "journal-ref": "IFAC-PapersOnLine, 49(32), 2016, p. 24-29, ISSN 2405-8963", "doi": "10.1016/j.ifacol.2016.12.184", "report-no": null, "categories": "cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In coming years residential consumers will face real-time electricity tariffs\nwith energy prices varying day to day, and effective energy saving will require\nautomation - a recommender system, which learns consumer's preferences from her\nactions. A consumer chooses a scenario of home appliance use to balance her\ncomfort level and the energy bill. We propose a Bayesian learning algorithm to\nestimate the comfort level function from the history of appliance use. In\nnumeric experiments with datasets generated from a simulation model of a\nconsumer interacting with small home appliances the algorithm outperforms\npopular regression analysis tools. Our approach can be extended to control an\nair heating and conditioning system, which is responsible for up to half of a\nhousehold's energy bill.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 20:45:31 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Goubko", "Mikhail V.", ""], ["Kuznetsov", "Sergey O.", ""], ["Neznanov", "Alexey A.", ""], ["Ignatov", "Dmitry I.", ""]]}, {"id": "1701.08795", "submitter": "Karan Singh", "authors": "Angela Zhou, Irineo Cabreros, Karan Singh", "title": "Dynamic Task Allocation for Crowdsourcing Settings", "comments": "Presented at the Data Efficient Machine Learning Workshop at\n  International Conference on Machine Learning (ICML) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of optimal budget allocation for crowdsourcing\nproblems, allocating users to tasks to maximize our final confidence in the\ncrowdsourced answers. Such an optimized worker assignment method allows us to\nboost the efficacy of any popular crowdsourcing estimation algorithm. We\nconsider a mutual information interpretation of the crowdsourcing problem,\nwhich leads to a stochastic subset selection problem with a submodular\nobjective function. We present experimental simulation results which\ndemonstrate the effectiveness of our dynamic task allocation method for\nachieving higher accuracy, possibly requiring fewer labels, as well as\nimproving upon a previous method which is sensitive to the proportion of users\nto questions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 19:37:21 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 07:02:37 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Zhou", "Angela", ""], ["Cabreros", "Irineo", ""], ["Singh", "Karan", ""]]}, {"id": "1701.08796", "submitter": "Tong Liu", "authors": "Tong Liu and Qijin Cheng and Christopher M. Homan and Vincent M.B.\n  Silenzio", "title": "Learning from various labeling strategies for suicide-related messages\n  on social media: An experimental study", "comments": "8 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suicide is an important but often misunderstood problem, one that researchers\nare now seeking to better understand through social media. Due in large part to\nthe fuzzy nature of what constitutes suicidal risks, most supervised approaches\nfor learning to automatically detect suicide-related activity in social media\nrequire a great deal of human labor to train. However, humans themselves have\ndiverse or conflicting views on what constitutes suicidal thoughts. So how to\nobtain reliable gold standard labels is fundamentally challenging and, we\nhypothesize, depends largely on what is asked of the annotators and what slice\nof the data they label. We conducted multiple rounds of data labeling and\ncollected annotations from crowdsourcing workers and domain experts. We\naggregated the resulting labels in various ways to train a series of supervised\nmodels. Our preliminary evaluations show that using unanimously agreed labels\nfrom multiple annotators is helpful to achieve robust machine models.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 19:41:04 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Liu", "Tong", ""], ["Cheng", "Qijin", ""], ["Homan", "Christopher M.", ""], ["Silenzio", "Vincent M. B.", ""]]}, {"id": "1701.08810", "submitter": "Romain Laroche", "authors": "Romain Laroche and Raphael Feraud", "title": "Reinforcement Learning Algorithm Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper formalises the problem of online algorithm selection in the\ncontext of Reinforcement Learning. The setup is as follows: given an episodic\ntask and a finite number of off-policy RL algorithms, a meta-algorithm has to\ndecide which RL algorithm is in control during the next episode so as to\nmaximize the expected return. The article presents a novel meta-algorithm,\ncalled Epochal Stochastic Bandit Algorithm Selection (ESBAS). Its principle is\nto freeze the policy updates at each epoch, and to leave a rebooted stochastic\nbandit in charge of the algorithm selection. Under some assumptions, a thorough\ntheoretical analysis demonstrates its near-optimality considering the\nstructural sampling budget limitations. ESBAS is first empirically evaluated on\na dialogue task where it is shown to outperform each individual algorithm in\nmost configurations. ESBAS is then adapted to a true online setting where\nalgorithms update their policies after each transition, which we call SSBAS.\nSSBAS is evaluated on a fruit collection task where it is shown to adapt the\nstepsize parameter more efficiently than the classical hyperbolic decay, and on\nan Atari game, where it improves the performance by a wide margin.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 20:13:17 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 19:20:40 GMT"}, {"version": "v3", "created": "Tue, 14 Nov 2017 21:08:17 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Laroche", "Romain", ""], ["Feraud", "Raphael", ""]]}, {"id": "1701.08816", "submitter": "Alexey Novikov", "authors": "Alexey A. Novikov, Dimitrios Lenis, David Major, Jiri Hlad\\r{u}vka,\n  Maria Wimmer, Katja B\\\"uhler", "title": "Fully Convolutional Architectures for Multi-Class Segmentation in Chest\n  Radiographs", "comments": "Final pre-print version accepted for publication in TMI Added new\n  content: * additional evaluations * additional figures * improving the old\n  content", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep convolutional neural networks on image classification and\nrecognition tasks has led to new applications in very diversified contexts,\nincluding the field of medical imaging. In this paper we investigate and\npropose neural network architectures for automated multi-class segmentation of\nanatomical organs in chest radiographs, namely for lungs, clavicles and heart.\nWe address several open challenges including model overfitting, reducing number\nof parameters and handling of severely imbalanced data in CXR by fusing recent\nconcepts in convolutional networks and adapting them to the segmentation\nproblem task in CXR. We demonstrate that our architecture combining delayed\nsubsampling, exponential linear units, highly restrictive regularization and a\nlarge number of high resolution low level abstract features outperforms\nstate-of-the-art methods on all considered organs, as well as the human\nobserver on lungs and heart. The models use a multi-class configuration with\nthree target classes and are trained and tested on the publicly available JSRT\ndatabase, consisting of 247 X-ray images the ground-truth masks for which are\navailable in the SCR database. Our best performing model, trained with the loss\nfunction based on the Dice coefficient, reached mean Jaccard overlap scores of\n95.0\\% for lungs, 86.8\\% for clavicles and 88.2\\% for heart. This architecture\noutperformed the human observer results for lungs and heart.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 20:21:57 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 14:42:02 GMT"}, {"version": "v3", "created": "Tue, 18 Apr 2017 13:02:51 GMT"}, {"version": "v4", "created": "Tue, 13 Feb 2018 16:12:40 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Novikov", "Alexey A.", ""], ["Lenis", "Dimitrios", ""], ["Major", "David", ""], ["Hlad\u016fvka", "Jiri", ""], ["Wimmer", "Maria", ""], ["B\u00fchler", "Katja", ""]]}, {"id": "1701.08837", "submitter": "Dipan Pal", "authors": "Dipan K. Pal, Vishnu Boddeti, Marios Savvides", "title": "Emergence of Selective Invariance in Hierarchical Feed Forward Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many theories have emerged which investigate how in- variance is generated in\nhierarchical networks through sim- ple schemes such as max and mean pooling.\nThe restriction to max/mean pooling in theoretical and empirical studies has\ndiverted attention away from a more general way of generating invariance to\nnuisance transformations. We con- jecture that hierarchically building\nselective invariance (i.e. carefully choosing the range of the transformation\nto be in- variant to at each layer of a hierarchical network) is im- portant\nfor pattern recognition. We utilize a novel pooling layer called adaptive\npooling to find linear pooling weights within networks. These networks with the\nlearnt pooling weights have performances on object categorization tasks that\nare comparable to max/mean pooling networks. In- terestingly, adaptive pooling\ncan converge to mean pooling (when initialized with random pooling weights),\nfind more general linear pooling schemes or even decide not to pool at all. We\nillustrate the general notion of selective invari- ance through object\ncategorization experiments on large- scale datasets such as SVHN and ILSVRC\n2012.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 21:44:27 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Pal", "Dipan K.", ""], ["Boddeti", "Vishnu", ""], ["Savvides", "Marios", ""]]}, {"id": "1701.08840", "submitter": "Andre Goncalves", "authors": "Andr\\'e R. Gon\\c{c}alves, Arindam Banerjee, Fernando J. Von Zuben", "title": "Spatial Projection of Multiple Climate Variables using Hierarchical\n  Multitask Learning", "comments": "Accepted for the 31st AAAI Conference on Artificial Intelligence\n  (AAAI-17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future projection of climate is typically obtained by combining outputs from\nmultiple Earth System Models (ESMs) for several climate variables such as\ntemperature and precipitation. While IPCC has traditionally used a simple model\noutput average, recent work has illustrated potential advantages of using a\nmultitask learning (MTL) framework for projections of individual climate\nvariables. In this paper we introduce a framework for hierarchical multitask\nlearning (HMTL) with two levels of tasks such that each super-task, i.e., task\nat the top level, is itself a multitask learning problem over sub-tasks. For\nclimate projections, each super-task focuses on projections of specific climate\nvariables spatially using an MTL formulation. For the proposed HMTL approach, a\ngroup lasso regularization is added to couple parameters across the\nsuper-tasks, which in the climate context helps exploit relationships among the\nbehavior of different climate variables at a given spatial location. We show\nthat some recent works on MTL based on learning task dependency structures can\nbe viewed as special cases of HMTL. Experiments on synthetic and real climate\ndata show that HMTL produces better results than decoupled MTL methods applied\nseparately on the super-tasks and HMTL significantly outperforms baselines for\nclimate projection.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 21:56:18 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Gon\u00e7alves", "Andr\u00e9 R.", ""], ["Banerjee", "Arindam", ""], ["Von Zuben", "Fernando J.", ""]]}, {"id": "1701.08848", "submitter": "Simona Colabrese", "authors": "Simona Colabrese, Kristian Gustavsson, Antonio Celani and Luca\n  Biferale", "title": "Flow Navigation by Smart Microswimmers via Reinforcement Learning", "comments": "Published on Physical Review Letters (April 12, 2017)", "journal-ref": "Phys. Rev. Lett. 118, 158004 (2017)", "doi": "10.1103/PhysRevLett.118.158004", "report-no": null, "categories": "physics.flu-dyn cond-mat.stat-mech cs.LG nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart active particles can acquire some limited knowledge of the fluid\nenvironment from simple mechanical cues and exert a control on their preferred\nsteering direction. Their goal is to learn the best way to navigate by\nexploiting the underlying flow whenever possible. As an example, we focus our\nattention on smart gravitactic swimmers. These are active particles whose task\nis to reach the highest altitude within some time horizon, given the\nconstraints enforced by fluid mechanics. By means of numerical experiments, we\nshow that swimmers indeed learn nearly optimal strategies just by experience. A\nreinforcement learning algorithm allows particles to learn effective strategies\neven in difficult situations when, in the absence of control, they would end up\nbeing trapped by flow structures. These strategies are highly nontrivial and\ncannot be easily guessed in advance. This Letter illustrates the potential of\nreinforcement learning algorithms to model adaptive behavior in complex flows\nand paves the way towards the engineering of smart microswimmers that solve\ndifficult navigation problems.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 22:09:04 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 09:38:46 GMT"}, {"version": "v3", "created": "Wed, 26 Jul 2017 14:14:43 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Colabrese", "Simona", ""], ["Gustavsson", "Kristian", ""], ["Celani", "Antonio", ""], ["Biferale", "Luca", ""]]}, {"id": "1701.08886", "submitter": "Moustafa Alzantot", "authors": "Moustafa Alzantot, Supriyo Chakraborty, Mani B. Srivastava", "title": "SenseGen: A Deep Learning Architecture for Synthetic Sensor Data\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our ability to synthesize sensory data that preserves specific statistical\nproperties of the real data has had tremendous implications on data privacy and\nbig data analytics. The synthetic data can be used as a substitute for\nselective real data segments,that are sensitive to the user, thus protecting\nprivacy and resulting in improved analytics.However, increasingly adversarial\nroles taken by data recipients such as mobile apps, or other cloud-based\nanalytics services, mandate that the synthetic data, in addition to preserving\nstatistical properties, should also be difficult to distinguish from the real\ndata. Typically, visual inspection has been used as a test to distinguish\nbetween datasets. But more recently, sophisticated classifier models\n(discriminators), corresponding to a set of events, have also been employed to\ndistinguish between synthesized and real data. The model operates on both\ndatasets and the respective event outputs are compared for consistency. In this\npaper, we take a step towards generating sensory data that can pass a deep\nlearning based discriminator model test, and make two specific contributions:\nfirst, we present a deep learning based architecture for synthesizing sensory\ndata. This architecture comprises of a generator model, which is a stack of\nmultiple Long-Short-Term-Memory (LSTM) networks and a Mixture Density Network.\nsecond, we use another LSTM network based discriminator model for\ndistinguishing between the true and the synthesized data. Using a dataset of\naccelerometer traces, collected using smartphones of users doing their daily\nactivities, we show that the deep learning based discriminator model can only\ndistinguish between the real and synthesized traces with an accuracy in the\nneighborhood of 50%.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 01:59:58 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Alzantot", "Moustafa", ""], ["Chakraborty", "Supriyo", ""], ["Srivastava", "Mani B.", ""]]}, {"id": "1701.08936", "submitter": "Da Zhang", "authors": "Da Zhang, Hamid Maei, Xin Wang, Yuan-Fang Wang", "title": "Deep Reinforcement Learning for Visual Object Tracking in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a fully end-to-end approach for visual tracking in\nvideos that learns to predict the bounding box locations of a target object at\nevery frame. An important insight is that the tracking problem can be\nconsidered as a sequential decision-making process and historical semantics\nencode highly relevant information for future decisions. Based on this\nintuition, we formulate our model as a recurrent convolutional neural network\nagent that interacts with a video overtime, and our model can be trained with\nreinforcement learning (RL) algorithms to learn good tracking policies that pay\nattention to continuous, inter-frame correlation and maximize tracking\nperformance in the long run. The proposed tracking algorithm achieves\nstate-of-the-art performance in an existing tracking benchmark and operates at\nframe-rates faster than real-time. To the best of our knowledge, our tracker is\nthe first neural-network tracker that combines convolutional and recurrent\nnetworks with RL algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 07:48:56 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 20:34:43 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Zhang", "Da", ""], ["Maei", "Hamid", ""], ["Wang", "Xin", ""], ["Wang", "Yuan-Fang", ""]]}, {"id": "1701.08939", "submitter": "Jeffrey Bilmes", "authors": "Jeffrey Bilmes, Wenruo Bai", "title": "Deep Submodular Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We start with an overview of a class of submodular functions called SCMMs\n(sums of concave composed with non-negative modular functions plus a final\narbitrary modular). We then define a new class of submodular functions we call\n{\\em deep submodular functions} or DSFs. We show that DSFs are a flexible\nparametric family of submodular functions that share many of the properties and\nadvantages of deep neural networks (DNNs). DSFs can be motivated by considering\na hierarchy of descriptive concepts over ground elements and where one wishes\nto allow submodular interaction throughout this hierarchy. Results in this\npaper show that DSFs constitute a strictly larger class of submodular functions\nthan SCMMs. We show that, for any integer $k>0$, there are $k$-layer DSFs that\ncannot be represented by a $k'$-layer DSF for any $k'<k$. This implies that,\nlike DNNs, there is a utility to depth, but unlike DNNs, the family of DSFs\nstrictly increase with depth. Despite this, we show (using a \"backpropagation\"\nlike method) that DSFs, even with arbitrarily large $k$, do not comprise all\nsubmodular functions. In offering the above results, we also define the notion\nof an antitone superdifferential of a concave function and show how this\nrelates to submodular functions (in general), DSFs (in particular), negative\nsecond-order partial derivatives, continuous submodularity, and concave\nextensions. To further motivate our analysis, we provide various special case\nresults from matroid theory, comparing DSFs with forms of matroid rank, in\nparticular the laminar matroid. Lastly, we discuss strategies to learn DSFs,\nand define the classes of deep supermodular functions, deep difference of\nsubmodular functions, and deep multivariate submodular functions, and discuss\nwhere these can be useful in applications.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 08:06:33 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Bilmes", "Jeffrey", ""], ["Bai", "Wenruo", ""]]}, {"id": "1701.08946", "submitter": "Abdelghafour Talibi", "authors": "Abdelghafour Talibi and Boujem\\^aa Achchab and Rafik Lasri", "title": "Variable selection for clustering with Gaussian mixture models: state of\n  the art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixture models have become widely used in clustering, given its\nprobabilistic framework in which its based, however, for modern databases that\nare characterized by their large size, these models behave disappointingly in\nsetting out the model, making essential the selection of relevant variables for\nthis type of clustering. After recalling the basics of clustering based on a\nmodel, this article will examine the variable selection methods for model-based\nclustering, as well as presenting opportunities for improvement of these\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 08:51:59 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Talibi", "Abdelghafour", ""], ["Achchab", "Boujem\u00e2a", ""], ["Lasri", "Rafik", ""]]}, {"id": "1701.08954", "submitter": "Marco Baroni", "authors": "Marco Baroni, Armand Joulin, Allan Jabri, Germ\\`an Kruszewski,\n  Angeliki Lazaridou, Klemen Simonic, Tomas Mikolov", "title": "CommAI: Evaluating the first steps towards a useful general AI", "comments": "Published in ICLR 2017 Workshop Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With machine learning successfully applied to new daunting problems almost\nevery day, general AI starts looking like an attainable goal. However, most\ncurrent research focuses instead on important but narrow applications, such as\nimage classification or machine translation. We believe this to be largely due\nto the lack of objective ways to measure progress towards broad machine\nintelligence. In order to fill this gap, we propose here a set of concrete\ndesiderata for general AI, together with a platform to test machines on how\nwell they satisfy such desiderata, while keeping all further complexities to a\nminimum.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 09:20:17 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 18:47:01 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Baroni", "Marco", ""], ["Joulin", "Armand", ""], ["Jabri", "Allan", ""], ["Kruszewski", "Germ\u00e0n", ""], ["Lazaridou", "Angeliki", ""], ["Simonic", "Klemen", ""], ["Mikolov", "Tomas", ""]]}, {"id": "1701.08974", "submitter": "Pedro Costa", "authors": "Pedro Costa, Adrian Galdran, Maria In\\^es Meyer, Michael David\n  Abr\\`amoff, Meindert Niemeijer, Ana Maria Mendon\\c{c}a, Aur\\'elio Campilho", "title": "Towards Adversarial Retinal Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing images of the eye fundus is a challenging task that has been\npreviously approached by formulating complex models of the anatomy of the eye.\nNew images can then be generated by sampling a suitable parameter space. In\nthis work, we propose a method that learns to synthesize eye fundus images\ndirectly from data. For that, we pair true eye fundus images with their\nrespective vessel trees, by means of a vessel segmentation technique. These\npairs are then used to learn a mapping from a binary vessel tree to a new\nretinal image. For this purpose, we use a recent image-to-image translation\ntechnique, based on the idea of adversarial learning. Experimental results show\nthat the original and the generated images are visually different in terms of\ntheir global appearance, in spite of sharing the same vessel tree.\nAdditionally, a quantitative quality analysis of the synthetic retinal images\nconfirms that the produced images retain a high proportion of the true image\nset quality.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 10:17:13 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Costa", "Pedro", ""], ["Galdran", "Adrian", ""], ["Meyer", "Maria In\u00eas", ""], ["Abr\u00e0moff", "Michael David", ""], ["Niemeijer", "Meindert", ""], ["Mendon\u00e7a", "Ana Maria", ""], ["Campilho", "Aur\u00e9lio", ""]]}, {"id": "1701.08978", "submitter": "Naveen Mellempudi", "authors": "Naveen Mellempudi, Abhisek Kundu, Dipankar Das, Dheevatsa Mudigere,\n  and Bharat Kaul", "title": "Mixed Low-precision Deep Learning Inference using Dynamic Fixed Point", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a cluster-based quantization method to convert pre-trained full\nprecision weights into ternary weights with minimal impact on the accuracy. In\naddition, we also constrain the activations to 8-bits thus enabling sub 8-bit\nfull integer inference pipeline. Our method uses smaller clusters of N filters\nwith a common scaling factor to minimize the quantization loss, while also\nmaximizing the number of ternary operations. We show that with a cluster size\nof N=4 on Resnet-101, can achieve 71.8% TOP-1 accuracy, within 6% of the best\nfull precision results while replacing ~85% of all multiplications with 8-bit\naccumulations. Using the same method with 4-bit weights achieves 76.3% TOP-1\naccuracy which within 2% of the full precision result. We also study the impact\nof the size of the cluster on both performance and accuracy, larger cluster\nsizes N=64 can replace ~98% of the multiplications with ternary operations but\nintroduces significant drop in accuracy which necessitates fine tuning the\nparameters with retraining the network at lower precision. To address this we\nhave also trained low-precision Resnet-50 with 8-bit activations and ternary\nweights by pre-initializing the network with full precision weights and achieve\n68.9% TOP-1 accuracy within 4 additional epochs. Our final quantized model can\nrun on a full 8-bit compute pipeline, with a potential 16x improvement in\nperformance compared to baseline full-precision models.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 10:28:37 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 04:09:31 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Mellempudi", "Naveen", ""], ["Kundu", "Abhisek", ""], ["Das", "Dipankar", ""], ["Mudigere", "Dheevatsa", ""], ["Kaul", "Bharat", ""]]}, {"id": "1701.09083", "submitter": "Pan Li", "authors": "Pan Li, Arya Mazumdar and Olgica Milenkovic", "title": "Efficient Rank Aggregation via Lehmer Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel rank aggregation method based on converting permutations\ninto their corresponding Lehmer codes or other subdiagonal images. Lehmer\ncodes, also known as inversion vectors, are vector representations of\npermutations in which each coordinate can take values not restricted by the\nvalues of other coordinates. This transformation allows for decoupling of the\ncoordinates and for performing aggregation via simple scalar median or mode\ncomputations. We present simulation results illustrating the performance of\nthis completely parallelizable approach and analytically prove that both the\nmode and median aggregation procedure recover the correct centroid aggregate\nwith small sample complexity when the permutations are drawn according to the\nwell-known Mallows models. The proposed Lehmer code approach may also be used\non partial rankings, with similar performance guarantees.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 19:28:29 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Li", "Pan", ""], ["Mazumdar", "Arya", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1701.09175", "submitter": "Emin Orhan", "authors": "A. Emin Orhan, Xaq Pitkow", "title": "Skip Connections Eliminate Singularities", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skip connections made the training of very deep networks possible and have\nbecome an indispensable component in a variety of neural architectures. A\ncompletely satisfactory explanation for their success remains elusive. Here, we\npresent a novel explanation for the benefits of skip connections in training\nvery deep networks. The difficulty of training deep networks is partly due to\nthe singularities caused by the non-identifiability of the model. Several such\nsingularities have been identified in previous works: (i) overlap singularities\ncaused by the permutation symmetry of nodes in a given layer, (ii) elimination\nsingularities corresponding to the elimination, i.e. consistent deactivation,\nof nodes, (iii) singularities generated by the linear dependence of the nodes.\nThese singularities cause degenerate manifolds in the loss landscape that slow\ndown learning. We argue that skip connections eliminate these singularities by\nbreaking the permutation symmetry of nodes, by reducing the possibility of node\nelimination and by making the nodes less linearly dependent. Moreover, for\ntypical initializations, skip connections move the network away from the\n\"ghosts\" of these singularities and sculpt the landscape around them to\nalleviate the learning slow-down. These hypotheses are supported by evidence\nfrom simplified models, as well as from experiments with deep networks trained\non real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 18:41:07 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 17:53:43 GMT"}, {"version": "v3", "created": "Thu, 9 Feb 2017 18:33:24 GMT"}, {"version": "v4", "created": "Mon, 13 Feb 2017 16:47:10 GMT"}, {"version": "v5", "created": "Mon, 22 May 2017 15:18:09 GMT"}, {"version": "v6", "created": "Wed, 24 May 2017 16:36:39 GMT"}, {"version": "v7", "created": "Tue, 20 Jun 2017 17:50:41 GMT"}, {"version": "v8", "created": "Sun, 4 Mar 2018 22:23:18 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Orhan", "A. Emin", ""], ["Pitkow", "Xaq", ""]]}, {"id": "1701.09177", "submitter": "Hongteng Xu", "authors": "Hongteng Xu and Hongyuan Zha", "title": "A Dirichlet Mixture Model of Hawkes Processes for Event Sequence\n  Clustering", "comments": null, "journal-ref": "31st Conference on Neural Information Processing Systems (NIPS\n  2017), Long Beach, CA, USA", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an effective method to solve the event sequence clustering\nproblems based on a novel Dirichlet mixture model of a special but significant\ntype of point processes --- Hawkes process. In this model, each event sequence\nbelonging to a cluster is generated via the same Hawkes process with specific\nparameters, and different clusters correspond to different Hawkes processes.\nThe prior distribution of the Hawkes processes is controlled via a Dirichlet\ndistribution. We learn the model via a maximum likelihood estimator (MLE) and\npropose an effective variational Bayesian inference algorithm. We specifically\nanalyze the resulting EM-type algorithm in the context of inner-outer\niterations and discuss several inner iteration allocation strategies. The\nidentifiability of our model, the convergence of our learning method, and its\nsample complexity are analyzed in both theoretical and empirical ways, which\ndemonstrate the superiority of our method to other competitors. The proposed\nmethod learns the number of clusters automatically and is robust to model\nmisspecification. Experiments on both synthetic and real-world data show that\nour method can learn diverse triggering patterns hidden in asynchronous event\nsequences and achieve encouraging performance on clustering purity and\nconsistency.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 18:42:19 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 16:28:30 GMT"}, {"version": "v3", "created": "Mon, 17 Apr 2017 03:12:55 GMT"}, {"version": "v4", "created": "Thu, 27 Apr 2017 22:38:34 GMT"}, {"version": "v5", "created": "Thu, 21 Sep 2017 15:47:34 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Xu", "Hongteng", ""], ["Zha", "Hongyuan", ""]]}]