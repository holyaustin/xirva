[{"id": "0909.0400", "submitter": "Noam Shental", "authors": "Noam Shental, Amnon Amir and Or Zuk", "title": "Rare-Allele Detection Using Compressed Se(que)nsing", "comments": "29 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.IT cs.LG math.IT q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of rare variants by resequencing is important for the\nidentification of individuals carrying disease variants. Rapid sequencing by\nnew technologies enables low-cost resequencing of target regions, although it\nis still prohibitive to test more than a few individuals. In order to improve\ncost trade-offs, it has recently been suggested to apply pooling designs which\nenable the detection of carriers of rare alleles in groups of individuals.\nHowever, this was shown to hold only for a relatively low number of individuals\nin a pool, and requires the design of pooling schemes for particular cases.\n  We propose a novel pooling design, based on a compressed sensing approach,\nwhich is both general, simple and efficient. We model the experimental\nprocedure and show via computer simulations that it enables the recovery of\nrare allele carriers out of larger groups than were possible before, especially\nin situations where high coverage is obtained for each individual.\n  Our approach can also be combined with barcoding techniques to enhance\nperformance and provide a feasible solution based on current resequencing\ncosts. For example, when targeting a small enough genomic region (~100\nbase-pairs) and using only ~10 sequencing lanes and ~10 distinct barcodes, one\ncan recover the identity of 4 rare allele carriers out of a population of over\n4000 individuals.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2009 13:25:48 GMT"}], "update_date": "2009-09-03", "authors_parsed": [["Shental", "Noam", ""], ["Amir", "Amnon", ""], ["Zuk", "Or", ""]]}, {"id": "0909.0635", "submitter": "Fabrice Rossi", "authors": "Michel Verleysen (DICE - MLG), Fabrice Rossi (LTCI), Damien\n  Fran\\c{c}ois (CESAME)", "title": "Advances in Feature Selection with Mutual Information", "comments": null, "journal-ref": "Similarity-Based Clustering, Villmann, Th.; Biehl, M.; Hammer, B.;\n  Verleysen, M. (Ed.) (2009) 52-69", "doi": "10.1007/978-3-642-01805-3_4", "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The selection of features that are relevant for a prediction or\nclassification problem is an important problem in many domains involving\nhigh-dimensional data. Selecting features helps fighting the curse of\ndimensionality, improving the performances of prediction or classification\nmethods, and interpreting the application. In a nonlinear context, the mutual\ninformation is widely used as relevance criterion for features and sets of\nfeatures. Nevertheless, it suffers from at least three major limitations:\nmutual information estimators depend on smoothing parameters, there is no\ntheoretically justified stopping criterion in the feature selection greedy\nprocedure, and the estimation itself suffers from the curse of dimensionality.\nThis chapter shows how to deal with these problems. The two first ones are\naddressed by using resampling techniques that provide a statistical basis to\nselect the estimator parameters and to stop the search procedure. The third one\nis addressed by modifying the mutual information criterion into a measure of\nhow features are complementary (and not only informative) for the problem at\nhand.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2009 12:04:57 GMT"}], "update_date": "2009-09-04", "authors_parsed": [["Verleysen", "Michel", "", "DICE - MLG"], ["Rossi", "Fabrice", "", "LTCI"], ["Fran\u00e7ois", "Damien", "", "CESAME"]]}, {"id": "0909.0638", "submitter": "Fabrice Rossi", "authors": "Barbara Hammer, Alexander Hasenfu{\\ss}, Fabrice Rossi (LTCI)", "title": "Median topographic maps for biomedical data sets", "comments": null, "journal-ref": "Similarity-Based Clustering, Villmann, Th.; Biehl, M.; Hammer, B.;\n  Verleysen, M. (Ed.) (2009) 92-117", "doi": "10.1007/978-3-642-01805-3_6", "report-no": null, "categories": "cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Median clustering extends popular neural data analysis methods such as the\nself-organizing map or neural gas to general data structures given by a\ndissimilarity matrix only. This offers flexible and robust global data\ninspection methods which are particularly suited for a variety of data as\noccurs in biomedical domains. In this chapter, we give an overview about median\nclustering and its properties and extensions, with a particular focus on\nefficient implementations adapted to large scale data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2009 12:09:08 GMT"}], "update_date": "2009-09-04", "authors_parsed": [["Hammer", "Barbara", "", "LTCI"], ["Hasenfu\u00df", "Alexander", "", "LTCI"], ["Rossi", "Fabrice", "", "LTCI"]]}, {"id": "0909.0737", "submitter": "Irmtraud Meyer", "authors": "Tin Yin Lam and Irmtraud M. Meyer", "title": "Efficient algorithms for training the parameters of hidden Markov models\n  using stochastic expectation maximization EM training and Viterbi training", "comments": "32 pages including 9 figures and 2 tables", "journal-ref": "BMC Algorithms for Molecular Biology (2010) 5:38", "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Hidden Markov models are widely employed by numerous\nbioinformatics programs used today. Applications range widely from comparative\ngene prediction to time-series analyses of micro-array data. The parameters of\nthe underlying models need to be adjusted for specific data sets, for example\nthe genome of a particular species, in order to maximize the prediction\naccuracy. Computationally efficient algorithms for parameter training are thus\nkey to maximizing the usability of a wide range of bioinformatics applications.\n  Results: We introduce two computationally efficient training algorithms, one\nfor Viterbi training and one for stochastic expectation maximization (EM)\ntraining, which render the memory requirements independent of the sequence\nlength. Unlike the existing algorithms for Viterbi and stochastic EM training\nwhich require a two-step procedure, our two new algorithms require only one\nstep and scan the input sequence in only one direction. We also implement these\ntwo new algorithms and the already published linear-memory algorithm for EM\ntraining into the hidden Markov model compiler HMM-Converter and examine their\nrespective practical merits for three small example models.\n  Conclusions: Bioinformatics applications employing hidden Markov models can\nuse the two algorithms in order to make Viterbi training and stochastic EM\ntraining more computationally efficient. Using these algorithms, parameter\ntraining can thus be attempted for more complex models and longer training\nsequences. The two new algorithms have the added advantage of being easier to\nimplement than the corresponding default algorithms for Viterbi training and\nstochastic EM training.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2009 19:29:56 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2012 21:57:24 GMT"}], "update_date": "2012-10-18", "authors_parsed": [["Lam", "Tin Yin", ""], ["Meyer", "Irmtraud M.", ""]]}, {"id": "0909.0801", "submitter": "Marcus Hutter", "authors": "Joel Veness and Kee Siong Ng and Marcus Hutter and William Uther and\n  David Silver", "title": "A Monte Carlo AIXI Approximation", "comments": "51 LaTeX pages, 11 figures, 6 tables, 4 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a principled approach for the design of a scalable\ngeneral reinforcement learning agent. Our approach is based on a direct\napproximation of AIXI, a Bayesian optimality notion for general reinforcement\nlearning agents. Previously, it has been unclear whether the theory of AIXI\ncould motivate the design of practical algorithms. We answer this hitherto open\nquestion in the affirmative, by providing the first computationally feasible\napproximation to the AIXI agent. To develop our approximation, we introduce a\nnew Monte-Carlo Tree Search algorithm along with an agent-specific extension to\nthe Context Tree Weighting algorithm. Empirically, we present a set of\nencouraging results on a variety of stochastic and partially observable\ndomains. We conclude by proposing a number of directions for future research.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2009 03:13:58 GMT"}, {"version": "v2", "created": "Sun, 26 Dec 2010 11:01:10 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Veness", "Joel", ""], ["Ng", "Kee Siong", ""], ["Hutter", "Marcus", ""], ["Uther", "William", ""], ["Silver", "David", ""]]}, {"id": "0909.0844", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Rocquencourt)", "title": "High-Dimensional Non-Linear Variable Selection through Hierarchical\n  Kernel Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of high-dimensional non-linear variable selection for\nsupervised learning. Our approach is based on performing linear selection among\nexponentially many appropriately defined positive definite kernels that\ncharacterize non-linear interactions between the original variables. To select\nefficiently from these many kernels, we use the natural hierarchical structure\nof the problem to extend the multiple kernel learning framework to kernels that\ncan be embedded in a directed acyclic graph; we show that it is then possible\nto perform kernel selection through a graph-adapted sparsity-inducing norm, in\npolynomial time in the number of selected kernels. Moreover, we study the\nconsistency of variable selection in high-dimensional settings, showing that\nunder certain assumptions, our regularization framework allows a number of\nirrelevant variables which is exponential in the number of observations. Our\nsimulations on synthetic datasets and datasets from the UCI repository show\nstate-of-the-art predictive performance for non-linear regression problems.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2009 09:43:38 GMT"}], "update_date": "2009-09-08", "authors_parsed": [["Bach", "Francis", "", "INRIA Rocquencourt"]]}, {"id": "0909.1062", "submitter": "Ankan Saha", "authors": "Ankan Saha (1), S.V.N. Vishwanathan (2), Xinhua Zhang (3) ((1)\n  University of Chicago, (2) Purdue University, (3) University of Alberta)", "title": "New Approximation Algorithms for Minimum Enclosing Convex Shapes", "comments": "18 Pages Accepted in SODA 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $n$ points in a $d$ dimensional Euclidean space, the Minimum Enclosing\nBall (MEB) problem is to find the ball with the smallest radius which contains\nall $n$ points. We give a $O(nd\\Qcal/\\sqrt{\\epsilon})$ approximation algorithm\nfor producing an enclosing ball whose radius is at most $\\epsilon$ away from\nthe optimum (where $\\Qcal$ is an upper bound on the norm of the points). This\nimproves existing results using \\emph{coresets}, which yield a $O(nd/\\epsilon)$\ngreedy algorithm. Finding the Minimum Enclosing Convex Polytope (MECP) is a\nrelated problem wherein a convex polytope of a fixed shape is given and the aim\nis to find the smallest magnification of the polytope which encloses the given\npoints. For this problem we present a $O(mnd\\Qcal/\\epsilon)$ approximation\nalgorithm, where $m$ is the number of faces of the polytope. Our algorithms\nborrow heavily from convex duality and recently developed techniques in\nnon-smooth optimization, and are in contrast with existing methods which rely\non geometric arguments. In particular, we specialize the excessive gap\nframework of \\citet{Nesterov05a} to obtain our results.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2009 23:24:32 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2009 16:01:53 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2009 14:30:54 GMT"}, {"version": "v4", "created": "Wed, 15 Sep 2010 00:54:09 GMT"}], "update_date": "2010-09-16", "authors_parsed": [["Saha", "Ankan", ""], ["Vishwanathan", "S. V. N.", ""], ["Zhang", "Xinhua", ""]]}, {"id": "0909.1308", "submitter": "Olivier Cappe", "authors": "Nataliya Sokolovska (LTCI), Thomas Lavergne (LIMSI), Olivier Capp\\'e\n  (LTCI), Fran\\c{c}ois Yvon (LIMSI)", "title": "Efficient Learning of Sparse Conditional Random Fields for Supervised\n  Sequence Labelling", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2010.2076150", "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Random Fields (CRFs) constitute a popular and efficient approach\nfor supervised sequence labelling. CRFs can cope with large description spaces\nand can integrate some form of structural dependency between labels. In this\ncontribution, we address the issue of efficient feature selection for CRFs\nbased on imposing sparsity through an L1 penalty. We first show how sparsity of\nthe parameter set can be exploited to significantly speed up training and\nlabelling. We then introduce coordinate descent parameter update schemes for\nCRFs with L1 regularization. We finally provide some empirical comparisons of\nthe proposed approach with state-of-the-art CRF training strategies. In\nparticular, it is shown that the proposed approach is able to take profit of\nthe sparsity to speed up processing and hence potentially handle larger\ndimensional models.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2009 18:48:42 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2010 16:48:14 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Sokolovska", "Nataliya", "", "LTCI"], ["Lavergne", "Thomas", "", "LIMSI"], ["Capp\u00e9", "Olivier", "", "LTCI"], ["Yvon", "Fran\u00e7ois", "", "LIMSI"]]}, {"id": "0909.1334", "submitter": "Ankan Saha", "authors": "Ankan Saha (1), Xinhua Zhang (2), S.V.N. Vishwanathan (3) ((1)\n  University of Chicago, (2) Australian National University, NICTA, (3) Purdue\n  University)", "title": "Lower Bounds for BMRM and Faster Rates for Training SVMs", "comments": "21 pages, 49 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized risk minimization with the binary hinge loss and its variants\nlies at the heart of many machine learning problems. Bundle methods for\nregularized risk minimization (BMRM) and the closely related SVMStruct are\nconsidered the best general purpose solvers to tackle this problem. It was\nrecently shown that BMRM requires $O(1/\\epsilon)$ iterations to converge to an\n$\\epsilon$ accurate solution. In the first part of the paper we use the\nHadamard matrix to construct a regularized risk minimization problem and show\nthat these rates cannot be improved. We then show how one can exploit the\nstructure of the objective function to devise an algorithm for the binary hinge\nloss which converges to an $\\epsilon$ accurate solution in\n$O(1/\\sqrt{\\epsilon})$ iterations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2009 20:58:47 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2009 22:00:12 GMT"}], "update_date": "2009-09-09", "authors_parsed": [["Saha", "Ankan", ""], ["Zhang", "Xinhua", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "0909.1776", "submitter": "Xin (Luna) Dong", "authors": "Laure Berti-Equille (Universite de Rennes 1), Anish Das Sarma\n  (Stanford University), Xin (Luna) Dong (AT&T Labs-Research), Amelie Marian\n  (Rutgus University), Divesh Srivastava (ATT Labs-Research)", "title": "Sailing the Information Ocean with Awareness of Currents: Discovery and\n  Application of Source Dependence", "comments": "CIDR 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The Web has enabled the availability of a huge amount of useful information,\nbut has also eased the ability to spread false information and rumors across\nmultiple sources, making it hard to distinguish between what is true and what\nis not. Recent examples include the premature Steve Jobs obituary, the second\nbankruptcy of United airlines, the creation of Black Holes by the operation of\nthe Large Hadron Collider, etc. Since it is important to permit the expression\nof dissenting and conflicting opinions, it would be a fallacy to try to ensure\nthat the Web provides only consistent information. However, to help in\nseparating the wheat from the chaff, it is essential to be able to determine\ndependence between sources. Given the huge number of data sources and the vast\nvolume of conflicting data available on the Web, doing so in a scalable manner\nis extremely challenging and has not been addressed by existing work yet.\n  In this paper, we present a set of research problems and propose some\npreliminary solutions on the issues involved in discovering dependence between\nsources. We also discuss how this knowledge can benefit a variety of\ntechnologies, such as data integration and Web 2.0, that help users manage and\naccess the totality of the available information from various sources.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 18:10:07 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Berti-Equille", "Laure", "", "Universite de Rennes 1"], ["Sarma", "Anish Das", "", "Stanford University"], ["Xin", "", "", "Luna"], ["Dong", "", "", "AT&T Labs-Research"], ["Marian", "Amelie", "", "Rutgus University"], ["Srivastava", "Divesh", "", "ATT Labs-Research"]]}, {"id": "0909.1933", "submitter": "Liva Ralaivola", "authors": "Liva Ralaivola (LIF), Marie Szafranski (IBISC), Guillaume Stempfel\n  (LIF)", "title": "Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and\n  Stationary $\\beta$-Mixing Processes", "comments": "Long version of the AISTATS 09 paper:\n  http://jmlr.csail.mit.edu/proceedings/papers/v5/ralaivola09a/ralaivola09a.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pac-Bayes bounds are among the most accurate generalization bounds for\nclassifiers learned from independently and identically distributed (IID) data,\nand it is particularly so for margin classifiers: there have been recent\ncontributions showing how practical these bounds can be either to perform model\nselection (Ambroladze et al., 2007) or even to directly guide the learning of\nlinear classifiers (Germain et al., 2009). However, there are many practical\nsituations where the training data show some dependencies and where the\ntraditional IID assumption does not hold. Stating generalization bounds for\nsuch frameworks is therefore of the utmost interest, both from theoretical and\npractical standpoints. In this work, we propose the first - to the best of our\nknowledge - Pac-Bayes generalization bounds for classifiers trained on data\nexhibiting interdependencies. The approach undertaken to establish our results\nis based on the decomposition of a so-called dependency graph that encodes the\ndependencies within the data, in sets of independent data, thanks to graph\nfractional covers. Our bounds are very general, since being able to find an\nupper bound on the fractional chromatic number of the dependency graph is\nsufficient to get new Pac-Bayes bounds for specific settings. We show how our\nresults can be used to derive bounds for ranking statistics (such as Auc) and\nclassifiers trained on data distributed according to a stationary {\\ss}-mixing\nprocess. In the way, we show how our approach seemlessly allows us to deal with\nU-processes. As a side note, we also provide a Pac-Bayes generalization bound\nfor classifiers learned on data from stationary $\\varphi$-mixing distributions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2009 11:51:10 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2010 08:43:38 GMT"}], "update_date": "2010-06-09", "authors_parsed": [["Ralaivola", "Liva", "", "LIF"], ["Szafranski", "Marie", "", "IBISC"], ["Stempfel", "Guillaume", "", "LIF"]]}, {"id": "0909.2194", "submitter": "Dominique Tschopp", "authors": "Dominique Tschopp, Suhas Diggavi", "title": "Approximate Nearest Neighbor Search through Comparisons", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of finding the nearest neighbor (or one of\nthe R-nearest neighbors) of a query object q in a database of n objects. In\ncontrast with most existing approaches, we can only access the ``hidden'' space\nin which the objects live through a similarity oracle. The oracle, given two\nreference objects and a query object, returns the reference object closest to\nthe query object. The oracle attempts to model the behavior of human users,\ncapable of making statements about similarity, but not of assigning meaningful\nnumerical values to distances between objects.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2009 15:32:03 GMT"}], "update_date": "2009-09-14", "authors_parsed": [["Tschopp", "Dominique", ""], ["Diggavi", "Suhas", ""]]}, {"id": "0909.2234", "submitter": "Jayakrishnan Unnikrishnan", "authors": "Jayakrishnan Unnikrishnan, Dayu Huang, Sean Meyn, Amit Surana and\n  Venugopal Veeravalli", "title": "Universal and Composite Hypothesis Testing via Mismatched Divergence", "comments": "Accepted to IEEE Transactions on Information Theory, July 2010", "journal-ref": null, "doi": "10.1109/TIT.2011.2104670", "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the universal hypothesis testing problem, where the goal is to decide\nbetween the known null hypothesis distribution and some other unknown\ndistribution, Hoeffding proposed a universal test in the nineteen sixties.\nHoeffding's universal test statistic can be written in terms of\nKullback-Leibler (K-L) divergence between the empirical distribution of the\nobservations and the null hypothesis distribution. In this paper a modification\nof Hoeffding's test is considered based on a relaxation of the K-L divergence\ntest statistic, referred to as the mismatched divergence. The resulting\nmismatched test is shown to be a generalized likelihood-ratio test (GLRT) for\nthe case where the alternate distribution lies in a parametric family of the\ndistributions characterized by a finite dimensional parameter, i.e., it is a\nsolution to the corresponding composite hypothesis testing problem. For certain\nchoices of the alternate distribution, it is shown that both the Hoeffding test\nand the mismatched test have the same asymptotic performance in terms of error\nexponents. A consequence of this result is that the GLRT is optimal in\ndifferentiating a particular distribution from others in an exponential family.\nIt is also shown that the mismatched test has a significant advantage over the\nHoeffding test in terms of finite sample size performance. This advantage is\ndue to the difference in the asymptotic variances of the two test statistics\nunder the null hypothesis. In particular, the variance of the K-L divergence\ngrows linearly with the alphabet size, making the test impractical for\napplications involving large alphabet distributions. The variance of the\nmismatched divergence on the other hand grows linearly with the dimension of\nthe parameter space, and can hence be controlled through a prudent choice of\nthe function class defining the mismatched divergence.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2009 18:35:52 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2010 16:49:01 GMT"}, {"version": "v3", "created": "Thu, 9 Sep 2010 06:56:44 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Unnikrishnan", "Jayakrishnan", ""], ["Huang", "Dayu", ""], ["Meyn", "Sean", ""], ["Surana", "Amit", ""], ["Veeravalli", "Venugopal", ""]]}, {"id": "0909.2927", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman", "title": "Distribution-Specific Agnostic Boosting", "comments": null, "journal-ref": "Proceedings of Innovations in Computer Science (ICS), 2010, pp\n  241-250", "doi": null, "report-no": null, "categories": "cs.LG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of boosting the accuracy of weak learning algorithms\nin the agnostic learning framework of Haussler (1992) and Kearns et al. (1992).\nKnown algorithms for this problem (Ben-David et al., 2001; Gavinsky, 2002;\nKalai et al., 2008) follow the same strategy as boosting algorithms in the PAC\nmodel: the weak learner is executed on the same target function but over\ndifferent distributions on the domain. We demonstrate boosting algorithms for\nthe agnostic learning framework that only modify the distribution on the labels\nof the points (or, equivalently, modify the target function). This allows\nboosting a distribution-specific weak agnostic learner to a strong agnostic\nlearner with respect to the same distribution.\n  When applied to the weak agnostic parity learning algorithm of Goldreich and\nLevin (1989) our algorithm yields a simple PAC learning algorithm for DNF and\nan agnostic learning algorithm for decision trees over the uniform distribution\nusing membership queries. These results substantially simplify Jackson's famous\nDNF learning algorithm (1994) and the recent result of Gopalan et al. (2008).\n  We also strengthen the connection to hard-core set constructions discovered\nby Klivans and Servedio (1999) by demonstrating that hard-core set\nconstructions that achieve the optimal hard-core set size (given by Holenstein\n(2005) and Barak et al. (2009)) imply distribution-specific agnostic boosting\nalgorithms. Conversely, our boosting algorithm gives a simple hard-core set\nconstruction with an (almost) optimal hard-core set size.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2009 06:19:12 GMT"}], "update_date": "2012-02-22", "authors_parsed": [["Feldman", "Vitaly", ""]]}, {"id": "0909.2934", "submitter": "Ron Meir", "authors": "D. Di Castro and R. Meir", "title": "A Convergent Online Single Time Scale Actor Critic Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actor-Critic based approaches were among the first to address reinforcement\nlearning in a general setting. Recently, these algorithms have gained renewed\ninterest due to their generality, good convergence properties, and possible\nbiological relevance. In this paper, we introduce an online temporal difference\nbased actor-critic algorithm which is proved to converge to a neighborhood of a\nlocal maximum of the average reward. Linear function approximation is used by\nthe critic in order estimate the value function, and the temporal difference\nsignal, which is passed from the critic to the actor. The main distinguishing\nfeature of the present convergence proof is that both the actor and the critic\noperate on a similar time scale, while in most current convergence proofs they\nare required to have very different time scales in order to converge. Moreover,\nthe same temporal difference signal is used to update the parameters of both\nthe actor and the critic. A limitation of the proposed approach, compared to\nresults available for two time scale convergence, is that convergence is\nguaranteed only to a neighborhood of an optimal value, rather to an optimal\nvalue itself. The single time scale and identical temporal difference signal\nused by the actor and the critic, may provide a step towards constructing more\nbiologically realistic models of reinforcement learning in the brain.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2009 07:08:44 GMT"}], "update_date": "2009-09-17", "authors_parsed": [["Di Castro", "D.", ""], ["Meir", "R.", ""]]}, {"id": "0909.3123", "submitter": "Gilad Lerman Dr", "authors": "Teng Zhang, Arthur Szlam and Gilad Lerman", "title": "Median K-flats for hybrid linear modeling with many outliers", "comments": null, "journal-ref": "Proc. of 2nd IEEE International Workshop on Subspace Methods\n  (Subspace 2009), pp. 234-241 (2009)", "doi": "10.1109/ICCVW.2009.5457695", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the Median K-Flats (MKF) algorithm, a simple online method for\nhybrid linear modeling, i.e., for approximating data by a mixture of flats.\nThis algorithm simultaneously partitions the data into clusters while finding\ntheir corresponding best approximating l1 d-flats, so that the cumulative l1\nerror is minimized. The current implementation restricts d-flats to be\nd-dimensional linear subspaces. It requires a negligible amount of storage, and\nits complexity, when modeling data consisting of N points in D-dimensional\nEuclidean space with K d-dimensional linear subspaces, is of order O(n K d D+n\nd^2 D), where n is the number of iterations required for convergence\n(empirically on the order of 10^4). Since it is an online algorithm, data can\nbe supplied to it incrementally and it can incrementally produce the\ncorresponding output. The performance of the algorithm is carefully evaluated\nusing synthetic and real data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2009 23:09:16 GMT"}], "update_date": "2010-05-10", "authors_parsed": [["Zhang", "Teng", ""], ["Szlam", "Arthur", ""], ["Lerman", "Gilad", ""]]}, {"id": "0909.3593", "submitter": "Zhi-Hua Zhou", "authors": "Min-Ling Zhang and Zhi-Hua Zhou", "title": "Exploiting Unlabeled Data to Enhance Ensemble Diversity", "comments": "10 pages, to appear in ICDM 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble learning aims to improve generalization ability by using multiple\nbase learners. It is well-known that to construct a good ensemble, the base\nlearners should be accurate as well as diverse. In this paper, unlabeled data\nis exploited to facilitate ensemble learning by helping augment the diversity\namong the base learners. Specifically, a semi-supervised ensemble method named\nUDEED is proposed. Unlike existing semi-supervised ensemble methods where\nerror-prone pseudo-labels are estimated for unlabeled data to enlarge the\nlabeled data to improve accuracy, UDEED works by maximizing accuracies of base\nlearners on labeled data while maximizing diversity among them on unlabeled\ndata. Experiments show that UDEED can effectively utilize unlabeled data for\nensemble learning and is highly competitive to well-established semi-supervised\nensemble methods.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2009 16:10:19 GMT"}, {"version": "v2", "created": "Sat, 25 Sep 2010 02:27:46 GMT"}], "update_date": "2010-09-28", "authors_parsed": [["Zhang", "Min-Ling", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "0909.3606", "submitter": "Vinay Jethava", "authors": "Vinay Jethava", "title": "Extension of Path Probability Method to Approximate Inference over Time", "comments": "M.Sc.(Research) Thesis, 64 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  There has been a tremendous growth in publicly available digital video\nfootage over the past decade. This has necessitated the development of new\ntechniques in computer vision geared towards efficient analysis, storage and\nretrieval of such data. Many mid-level computer vision tasks such as\nsegmentation, object detection, tracking, etc. involve an inference problem\nbased on the video data available. Video data has a high degree of spatial and\ntemporal coherence. The property must be intelligently leveraged in order to\nobtain better results.\n  Graphical models, such as Markov Random Fields, have emerged as a powerful\ntool for such inference problems. They are naturally suited for expressing the\nspatial dependencies present in video data, It is however, not clear, how to\nextend the existing techniques for the problem of inference over time. This\nthesis explores the Path Probability Method, a variational technique in\nstatistical mechanics, in the context of graphical models and approximate\ninference problems. It extends the method to a general framework for problems\ninvolving inference in time, resulting in an algorithm, \\emph{DynBP}. We\nexplore the relation of the algorithm with existing techniques, and find the\nalgorithm competitive with existing approaches.\n  The main contribution of this thesis are the extended GBP algorithm, the\nextension of Path Probability Methods to the DynBP algorithm and the\nrelationship between them. We have also explored some applications in computer\nvision involving temporal evolution with promising results.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2009 23:46:21 GMT"}], "update_date": "2009-09-22", "authors_parsed": [["Jethava", "Vinay", ""]]}, {"id": "0909.3609", "submitter": "Vinay Jethava", "authors": "Vinay Jethava, Krishnan Suresh, Chiranjib Bhattacharyya, Ramesh\n  Hariharan", "title": "Randomized Algorithms for Large scale SVMs", "comments": "17 pages, Submitted to Machine Learning journal (October 2008) -\n  under revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a randomized algorithm for training Support vector machines(SVMs)\non large datasets. By using ideas from Random projections we show that the\ncombinatorial dimension of SVMs is $O({log} n)$ with high probability. This\nestimate of combinatorial dimension is used to derive an iterative algorithm,\ncalled RandSVM, which at each step calls an existing solver to train SVMs on a\nrandomly chosen subset of size $O({log} n)$. The algorithm has probabilistic\nguarantees and is capable of training SVMs with Kernels for both classification\nand regression problems. Experiments done on synthetic and real life data sets\ndemonstrate that the algorithm scales up existing SVM learners, without loss of\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2009 23:40:10 GMT"}], "update_date": "2009-09-22", "authors_parsed": [["Jethava", "Vinay", ""], ["Suresh", "Krishnan", ""], ["Bhattacharyya", "Chiranjib", ""], ["Hariharan", "Ramesh", ""]]}, {"id": "0909.4588", "submitter": "Marcus Hutter", "authors": "Marcus Hutter", "title": "Discrete MDL Predicts in Total Variation", "comments": "15 LaTeX pages", "journal-ref": "Advances in Neural Information Processing Systems 22 (NIPS 2009)\n  pages 817-825", "doi": null, "report-no": null, "categories": "math.PR cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Description Length (MDL) principle selects the model that has the\nshortest code for data plus model. We show that for a countable class of\nmodels, MDL predictions are close to the true distribution in a strong sense.\nThe result is completely general. No independence, ergodicity, stationarity,\nidentifiability, or other assumption on the model class need to be made. More\nformally, we show that for any countable class of models, the distributions\nselected by MDL (or MAP) asymptotically predict (merge with) the true measure\nin the class in total variation distance. Implications for non-i.i.d. domains\nlike time-series forecasting, discriminative learning, and reinforcement\nlearning are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2009 02:57:17 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Hutter", "Marcus", ""]]}, {"id": "0909.4603", "submitter": "James Petterson", "authors": "James Petterson, Tiberio Caetano", "title": "Scalable Inference for Latent Dirichlet Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of learning a topic model - the well-known Latent\nDirichlet Allocation - in a distributed manner, using a cluster of C processors\nand dividing the corpus to be learned equally among them. We propose a simple\napproximated method that can be tuned, trading speed for accuracy according to\nthe task at hand. Our approach is asynchronous, and therefore suitable for\nclusters of heterogenous machines.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2009 05:23:33 GMT"}], "update_date": "2009-09-28", "authors_parsed": [["Petterson", "James", ""], ["Caetano", "Tiberio", ""]]}, {"id": "0909.5000", "submitter": "Hrushikesh Mhaskar", "authors": "H. N. Mhaskar", "title": "Eignets for function approximation on manifolds", "comments": "28 pages. Articles in press; Applied and Computational Harmonic\n  Analysis, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\XX$ be a compact, smooth, connected, Riemannian manifold without\nboundary, $G:\\XX\\times\\XX\\to \\RR$ be a kernel. Analogous to a radial basis\nfunction network, an eignet is an expression of the form $\\sum_{j=1}^M\na_jG(\\circ,y_j)$, where $a_j\\in\\RR$, $y_j\\in\\XX$, $1\\le j\\le M$. We describe a\ndeterministic, universal algorithm for constructing an eignet for approximating\nfunctions in $L^p(\\mu;\\XX)$ for a general class of measures $\\mu$ and kernels\n$G$. Our algorithm yields linear operators. Using the minimal separation\namongst the centers $y_j$ as the cost of approximation, we give modulus of\nsmoothness estimates for the degree of approximation by our eignets, and show\nby means of a converse theorem that these are the best possible for every\n\\emph{individual function}. We also give estimates on the coefficients $a_j$ in\nterms of the norm of the eignet. Finally, we demonstrate that if any sequence\nof eignets satisfies the optimal estimates for the degree of approximation of a\nsmooth function, measured in terms of the minimal separation, then the\nderivatives of the eignets also approximate the corresponding derivatives of\nthe target function in an optimal manner.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2009 04:25:03 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Mhaskar", "H. N.", ""]]}, {"id": "0909.5175", "submitter": "Raghu Meka", "authors": "Prahladh Harsha, Adam Klivans, Raghu Meka", "title": "Bounding the Sensitivity of Polynomial Threshold Functions", "comments": "Fixed an important flaw. Some proofs are simplified from last version", "journal-ref": "Theory of Computing, 10(1):1-26, 2013", "doi": "10.4086/toc.2014.v010a001", "report-no": null, "categories": "cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first non-trivial upper bounds on the average sensitivity and\nnoise sensitivity of polynomial threshold functions. More specifically, for a\nBoolean function f on n variables equal to the sign of a real, multivariate\npolynomial of total degree d we prove\n  1) The average sensitivity of f is at most O(n^{1-1/(4d+6)}) (we also give a\ncombinatorial proof of the bound O(n^{1-1/2^d}).\n  2) The noise sensitivity of f with noise rate \\delta is at most\nO(\\delta^{1/(4d+6)}).\n  Previously, only bounds for the linear case were known. Along the way we show\nnew structural theorems about random restrictions of polynomial threshold\nfunctions obtained via hypercontractivity. These structural results may be of\nindependent interest as they provide a generic template for transforming\nproblems related to polynomial threshold functions defined on the Boolean\nhypercube to polynomial threshold functions defined in Gaussian space.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2009 19:55:46 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2009 02:35:36 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2009 05:34:34 GMT"}, {"version": "v4", "created": "Mon, 9 Nov 2009 23:23:19 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Harsha", "Prahladh", ""], ["Klivans", "Adam", ""], ["Meka", "Raghu", ""]]}, {"id": "0909.5457", "submitter": "Prateek Jain", "authors": "Raghu Meka, Prateek Jain, Inderjit S. Dhillon", "title": "Guaranteed Rank Minimization via Singular Value Projection", "comments": "An earlier version of this paper was submitted to NIPS-2009 on June\n  5, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing the rank of a matrix subject to affine constraints is a\nfundamental problem with many important applications in machine learning and\nstatistics. In this paper we propose a simple and fast algorithm SVP (Singular\nValue Projection) for rank minimization with affine constraints (ARMP) and show\nthat SVP recovers the minimum rank solution for affine constraints that satisfy\nthe \"restricted isometry property\" and show robustness of our method to noise.\nOur results improve upon a recent breakthrough by Recht, Fazel and Parillo\n(RFP07) and Lee and Bresler (LB09) in three significant ways:\n  1) our method (SVP) is significantly simpler to analyze and easier to\nimplement,\n  2) we give recovery guarantees under strictly weaker isometry assumptions\n  3) we give geometric convergence guarantees for SVP even in presense of noise\nand, as demonstrated empirically, SVP is significantly faster on real-world and\nsynthetic problems.\n  In addition, we address the practically important problem of low-rank matrix\ncompletion (MCP), which can be seen as a special case of ARMP. We empirically\ndemonstrate that our algorithm recovers low-rank incoherent matrices from an\nalmost optimal number of uniformly sampled entries. We make partial progress\ntowards proving exact recovery and provide some intuition for the strong\nperformance of SVP applied to matrix completion by showing a more restricted\nisometry property. Our algorithm outperforms existing methods, such as those of\n\\cite{RFP07,CR08,CT09,CCS08,KOM09,LB09}, for ARMP and the matrix-completion\nproblem by an order of magnitude and is also significantly more robust to\nnoise.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2009 14:44:54 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2009 19:32:44 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2009 21:21:57 GMT"}], "update_date": "2009-10-20", "authors_parsed": [["Meka", "Raghu", ""], ["Jain", "Prateek", ""], ["Dhillon", "Inderjit S.", ""]]}]