[{"id": "1406.0013", "submitter": "Marina Meila", "authors": "Dominique Perrault-Joncas and Marina Meila", "title": "Estimating Vector Fields on Manifolds and the Embedding of Directed\n  Graphs", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of embedding directed graphs in Euclidean\nspace while retaining directional information. We model a directed graph as a\nfinite set of observations from a diffusion on a manifold endowed with a vector\nfield. This is the first generative model of its kind for directed graphs. We\nintroduce a graph embedding algorithm that estimates all three features of this\nmodel: the low-dimensional embedding of the manifold, the data density and the\nvector field. In the process, we also obtain new theoretical results on the\nlimits of \"Laplacian type\" matrices derived from directed graphs. The\napplication of our method to both artificially constructed and real data\nhighlights its strengths.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 20:45:50 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Perrault-Joncas", "Dominique", ""], ["Meila", "Marina", ""]]}, {"id": "1406.0118", "submitter": "Marina Meila", "authors": "Dominique Perrault-Joncas and Marina Meila", "title": "Improved graph Laplacian via geometric self-consistency", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of setting the kernel bandwidth used by Manifold\nLearning algorithms to construct the graph Laplacian. Exploiting the connection\nbetween manifold geometry, represented by the Riemannian metric, and the\nLaplace-Beltrami operator, we set the bandwidth by optimizing the Laplacian's\nability to preserve the geometry of the data. Experiments show that this\nprincipled approach is effective and robust.\n", "versions": [{"version": "v1", "created": "Sat, 31 May 2014 23:00:36 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Perrault-Joncas", "Dominique", ""], ["Meila", "Marina", ""]]}, {"id": "1406.0156", "submitter": "Sheng Han", "authors": "Sheng Han, Suzhen Wang, Xinyu Wu", "title": "$l_1$-regularized Outlier Isolation and Regression", "comments": "Outlier Detection, Robust Regression, Robust Rank Factorization,\n  $l_1$-regularization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed a new regression model called $l_1$-regularized outlier\nisolation and regression (LOIRE) and a fast algorithm based on block coordinate\ndescent to solve this model. Besides, assuming outliers are gross errors\nfollowing a Bernoulli process, this paper also presented a Bernoulli estimate\nmodel which, in theory, should be very accurate and robust due to its complete\nelimination of affections caused by outliers. Though this Bernoulli estimate is\nhard to solve, it could be approximately achieved through a process which takes\nLOIRE as an important intermediate step. As a result, the approximate Bernoulli\nestimate is a good combination of Bernoulli estimate's accuracy and LOIRE\nregression's efficiency with several simulations conducted to strongly verify\nthis point. Moreover, LOIRE can be further extended to realize robust rank\nfactorization which is powerful in recovering low-rank component from massive\ncorruptions. Extensive experimental results showed that the proposed method\noutperforms state-of-the-art methods like RPCA and GoDec in the aspect of\ncomputation speed with a competitive performance.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 11:52:19 GMT"}, {"version": "v2", "created": "Thu, 20 Nov 2014 08:58:09 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Han", "Sheng", ""], ["Wang", "Suzhen", ""], ["Wu", "Xinyu", ""]]}, {"id": "1406.0167", "submitter": "Saurabh Paul", "authors": "Saurabh Paul, Malik Magdon-Ismail and Petros Drineas", "title": "Feature Selection for Linear SVM with Provable Guarantees", "comments": "Appearing in Proceedings of 18th AISTATS, JMLR W&CP, vol 38, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give two provably accurate feature-selection techniques for the linear\nSVM. The algorithms run in deterministic and randomized time respectively. Our\nalgorithms can be used in an unsupervised or supervised setting. The supervised\napproach is based on sampling features from support vectors. We prove that the\nmargin in the feature space is preserved to within $\\epsilon$-relative error of\nthe margin in the full feature space in the worst-case. In the unsupervised\nsetting, we also provide worst-case guarantees of the radius of the minimum\nenclosing ball, thereby ensuring comparable generalization as in the full\nfeature space and resolving an open problem posed in Dasgupta et al. We present\nextensive experiments on real-world datasets to support our theory and to\ndemonstrate that our method is competitive and often better than prior\nstate-of-the-art, for which there are no known provable guarantees.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 14:37:54 GMT"}, {"version": "v2", "created": "Mon, 20 Oct 2014 14:20:00 GMT"}, {"version": "v3", "created": "Fri, 6 Feb 2015 13:43:54 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Paul", "Saurabh", ""], ["Magdon-Ismail", "Malik", ""], ["Drineas", "Petros", ""]]}, {"id": "1406.0189", "submitter": "Nikolai Slavov", "authors": "Dmitry Malioutov and Nikolai Slavov", "title": "Convex Total Least Squares", "comments": "9 pages, 4 figures", "journal-ref": "JMLR W&CP 32 (1) :109-117, 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the total least squares (TLS) problem that generalizes least squares\nregression by allowing measurement errors in both dependent and independent\nvariables. TLS is widely used in applied fields including computer vision,\nsystem identification and econometrics. The special case when all dependent and\nindependent variables have the same level of uncorrelated Gaussian noise, known\nas ordinary TLS, can be solved by singular value decomposition (SVD). However,\nSVD cannot solve many important practical TLS problems with realistic noise\nstructure, such as having varying measurement noise, known structure on the\nerrors, or large outliers requiring robust error-norms. To solve such problems,\nwe develop convex relaxation approaches for a general class of structured TLS\n(STLS). We show both theoretically and experimentally, that while the plain\nnuclear norm relaxation incurs large approximation errors for STLS, the\nre-weighted nuclear norm approach is very effective, and achieves better\naccuracy on challenging STLS problems than popular non-convex solvers. We\ndescribe a fast solution based on augmented Lagrangian formulation, and apply\nour approach to an important class of biological problems that use population\naverage measurements to infer cell-type and physiological-state specific\nexpression levels that are very hard to measure directly.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 18:13:08 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Malioutov", "Dmitry", ""], ["Slavov", "Nikolai", ""]]}, {"id": "1406.0193", "submitter": "Nikolai Slavov", "authors": "Nikolai Slavov", "title": "Inference of Sparse Networks with Unobserved Variables. Application to\n  Gene Regulatory Networks", "comments": "8 pages, 5 figures", "journal-ref": "JMLR W&CP 9:757-764, 2010", "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.MN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are a unifying framework for modeling complex systems and network\ninference problems are frequently encountered in many fields. Here, I develop\nand apply a generative approach to network inference (RCweb) for the case when\nthe network is sparse and the latent (not observed) variables affect the\nobserved ones. From all possible factor analysis (FA) decompositions explaining\nthe variance in the data, RCweb selects the FA decomposition that is consistent\nwith a sparse underlying network. The sparsity constraint is imposed by a novel\nmethod that significantly outperforms (in terms of accuracy, robustness to\nnoise, complexity scaling, and computational efficiency) Bayesian methods and\nMLE methods using l1 norm relaxation such as K-SVD and l1--based sparse\nprinciple component analysis (PCA). Results from simulated models demonstrate\nthat RCweb recovers exactly the model structures for sparsity as low (as\nnon-sparse) as 50% and with ratio of unobserved to observed variables as high\nas 2. RCweb is robust to noise, with gradual decrease in the parameter ranges\nas the noise level increases.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 19:09:14 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Slavov", "Nikolai", ""]]}, {"id": "1406.0223", "submitter": "Saima Aman", "authors": "Saima Aman, Yogesh Simmhan, Viktor K. Prasanna", "title": "Holistic Measures for Evaluating Prediction Models in Smart Grids", "comments": "14 Pages, 8 figures, Accepted and to appear in IEEE Transactions on\n  Knowledge and Data Engineering, 2014. Authors' final version. Copyright\n  transferred to IEEE", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering, Volume: 27,\n  Issue: 2, Feb. 1 2015", "doi": "10.1109/TKDE.2014.2327022", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of prediction models is often based on \"abstract metrics\"\nthat estimate the model's ability to limit residual errors between the observed\nand predicted values. However, meaningful evaluation and selection of\nprediction models for end-user domains requires holistic and\napplication-sensitive performance measures. Inspired by energy consumption\nprediction models used in the emerging \"big data\" domain of Smart Power Grids,\nwe propose a suite of performance measures to rationally compare models along\nthe dimensions of scale independence, reliability, volatility and cost. We\ninclude both application independent and dependent measures, the latter\nparameterized to allow customization by domain experts to fit their scenario.\nWhile our measures are generalizable to other domains, we offer an empirical\nanalysis using real energy use data for three Smart Grid applications:\nplanning, customer education and demand response, which are relevant for energy\nsustainability. Our results underscore the value of the proposed measures to\noffer a deeper insight into models' behavior and their impact on real\napplications, which benefit both data mining researchers and practitioners.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 00:34:24 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Aman", "Saima", ""], ["Simmhan", "Yogesh", ""], ["Prasanna", "Viktor K.", ""]]}, {"id": "1406.0281", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina, David M. J. Tax, Marco Loog", "title": "On Classification with Bags, Groups and Sets", "comments": null, "journal-ref": "Pattern Recognition Letters Volume 59, 2015, Pages 11 - 17", "doi": "10.1016/j.patrec.2015.03.008", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classification problems can be difficult to formulate directly in terms\nof the traditional supervised setting, where both training and test samples are\nindividual feature vectors. There are cases in which samples are better\ndescribed by sets of feature vectors, that labels are only available for sets\nrather than individual samples, or, if individual labels are available, that\nthese are not independent. To better deal with such problems, several\nextensions of supervised learning have been proposed, where either training\nand/or test objects are sets of feature vectors. However, having been proposed\nrather independently of each other, their mutual similarities and differences\nhave hitherto not been mapped out. In this work, we provide an overview of such\nlearning scenarios, propose a taxonomy to illustrate the relationships between\nthem, and discuss directions for further research in these areas.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 08:06:12 GMT"}, {"version": "v2", "created": "Tue, 7 Oct 2014 14:55:44 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Cheplygina", "Veronika", ""], ["Tax", "David M. J.", ""], ["Loog", "Marco", ""]]}, {"id": "1406.0304", "submitter": "Markus Schneider", "authors": "Markus Schneider and Fabio Ramos", "title": "Transductive Learning for Multi-Task Copula Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of multi-task learning with copula process.\nMultivariable prediction in spatial and spatial-temporal processes such as\nnatural resource estimation and pollution monitoring have been typically\naddressed using techniques based on Gaussian processes and co-Kriging. While\nthe Gaussian prior assumption is convenient from analytical and computational\nperspectives, nature is dominated by non-Gaussian likelihoods. Copula processes\nare an elegant and flexible solution to handle various non-Gaussian likelihoods\nby capturing the dependence structure of random variables with cumulative\ndistribution functions rather than their marginals. We show how multi-task\nlearning for copula processes can be used to improve multivariable prediction\nfor problems where the simple Gaussianity prior assumption does not hold. Then,\nwe present a transductive approximation for multi-task learning and derive\nanalytical expressions for the copula process model. The approach is evaluated\nand compared to other techniques in one artificial dataset and two publicly\navailable datasets for natural resource estimation and concrete slump\nprediction.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 09:22:49 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Schneider", "Markus", ""], ["Ramos", "Fabio", ""]]}, {"id": "1406.0554", "submitter": "Krishnamurthy Dvijotham", "authors": "Krishnamurthy Dvijotham, Maryam Fazel and Emanuel Todorov", "title": "Universal Convexification via Risk-Aversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for convexifying a fairly general class of\noptimization problems. Under additional assumptions, we analyze the\nsuboptimality of the solution to the convexified problem relative to the\noriginal nonconvex problem and prove additive approximation guarantees. We then\ndevelop algorithms based on stochastic gradient methods to solve the resulting\noptimization problems and show bounds on convergence rates. %We show a simple\napplication of this framework to supervised learning, where one can perform\nintegration explicitly and can use standard (non-stochastic) optimization\nalgorithms with better convergence guarantees. We then extend this framework to\napply to a general class of discrete-time dynamical systems. In this context,\nour convexification approach falls under the well-studied paradigm of\nrisk-sensitive Markov Decision Processes. We derive the first known model-based\nand model-free policy gradient optimization algorithms with guaranteed\nconvergence to the optimal solution. Finally, we present numerical results\nvalidating our formulation in different applications.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 00:00:38 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Dvijotham", "Krishnamurthy", ""], ["Fazel", "Maryam", ""], ["Todorov", "Emanuel", ""]]}, {"id": "1406.0728", "submitter": "Fei Tian", "authors": "Di He, Wei Chen, Liwei Wang, Tie-Yan Liu", "title": "A Game-theoretic Machine Learning Approach for Revenue Maximization in\n  Sponsored Search", "comments": "Twenty-third International Conference on Artificial Intelligence\n  (IJCAI 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Sponsored search is an important monetization channel for search engines, in\nwhich an auction mechanism is used to select the ads shown to users and\ndetermine the prices charged from advertisers. There have been several pieces\nof work in the literature that investigate how to design an auction mechanism\nin order to optimize the revenue of the search engine. However, due to some\nunrealistic assumptions used, the practical values of these studies are not\nvery clear. In this paper, we propose a novel \\emph{game-theoretic machine\nlearning} approach, which naturally combines machine learning and game theory,\nand learns the auction mechanism using a bilevel optimization framework. In\nparticular, we first learn a Markov model from historical data to describe how\nadvertisers change their bids in response to an auction mechanism, and then for\nany given auction mechanism, we use the learnt model to predict its\ncorresponding future bid sequences. Next we learn the auction mechanism through\nempirical revenue maximization on the predicted bid sequences. We show that the\nempirical revenue will converge when the prediction period approaches infinity,\nand a Genetic Programming algorithm can effectively optimize this empirical\nrevenue. Our experiments indicate that the proposed approach is able to produce\na much more effective auction mechanism than several baselines.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 14:41:56 GMT"}, {"version": "v2", "created": "Wed, 4 Jun 2014 07:11:20 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["He", "Di", ""], ["Chen", "Wei", ""], ["Wang", "Liwei", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1406.0824", "submitter": "Sukru Burc Eryilmaz", "authors": "Sercan Arik, Sukru Burc Eryilmaz, Adam Goldberg", "title": "Supervised classification-based stock prediction and portfolio\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CE cs.LG q-fin.PM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the number of publicly traded companies as well as the amount of their\nfinancial data grows rapidly, it is highly desired to have tracking, analysis,\nand eventually stock selections automated. There have been few works focusing\non estimating the stock prices of individual companies. However, many of those\nhave worked with very small number of financial parameters. In this work, we\napply machine learning techniques to address automated stock picking, while\nusing a larger number of financial parameters for individual companies than the\nprevious studies. Our approaches are based on the supervision of prediction\nparameters using company fundamentals, time-series properties, and correlation\ninformation between different stocks. We examine a variety of supervised\nlearning techniques and found that using stock fundamentals is a useful\napproach for the classification problem, when combined with the high\ndimensional data handling capabilities of support vector machine. The portfolio\nour system suggests by predicting the behavior of stocks results in a 3% larger\ngrowth on average than the overall market within a 3-month time period, as the\nout-of-sample test suggests.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 19:32:09 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Arik", "Sercan", ""], ["Eryilmaz", "Sukru Burc", ""], ["Goldberg", "Adam", ""]]}, {"id": "1406.1078", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry\n  Bahdanau, Fethi Bougares, Holger Schwenk and Yoshua Bengio", "title": "Learning Phrase Representations using RNN Encoder-Decoder for\n  Statistical Machine Translation", "comments": "EMNLP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel neural network model called RNN\nEncoder-Decoder that consists of two recurrent neural networks (RNN). One RNN\nencodes a sequence of symbols into a fixed-length vector representation, and\nthe other decodes the representation into another sequence of symbols. The\nencoder and decoder of the proposed model are jointly trained to maximize the\nconditional probability of a target sequence given a source sequence. The\nperformance of a statistical machine translation system is empirically found to\nimprove by using the conditional probabilities of phrase pairs computed by the\nRNN Encoder-Decoder as an additional feature in the existing log-linear model.\nQualitatively, we show that the proposed model learns a semantically and\nsyntactically meaningful representation of linguistic phrases.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 17:47:08 GMT"}, {"version": "v2", "created": "Thu, 24 Jul 2014 20:07:13 GMT"}, {"version": "v3", "created": "Wed, 3 Sep 2014 00:25:02 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Cho", "Kyunghyun", ""], ["van Merrienboer", "Bart", ""], ["Gulcehre", "Caglar", ""], ["Bahdanau", "Dzmitry", ""], ["Bougares", "Fethi", ""], ["Schwenk", "Holger", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1406.1102", "submitter": "Pinghua Gong", "authors": "Pinghua Gong and Jieping Ye", "title": "Linear Convergence of Variance-Reduced Stochastic Gradient without\n  Strong Convexity", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient algorithms estimate the gradient based on only one or a\nfew samples and enjoy low computational cost per iteration. They have been\nwidely used in large-scale optimization problems. However, stochastic gradient\nalgorithms are usually slow to converge and achieve sub-linear convergence\nrates, due to the inherent variance in the gradient computation. To accelerate\nthe convergence, some variance-reduced stochastic gradient algorithms, e.g.,\nproximal stochastic variance-reduced gradient (Prox-SVRG) algorithm, have\nrecently been proposed to solve strongly convex problems. Under the strongly\nconvex condition, these variance-reduced stochastic gradient algorithms achieve\na linear convergence rate. However, many machine learning problems are convex\nbut not strongly convex. In this paper, we introduce Prox-SVRG and its\nprojected variant called Variance-Reduced Projected Stochastic Gradient (VRPSG)\nto solve a class of non-strongly convex optimization problems widely used in\nmachine learning. As the main technical contribution of this paper, we show\nthat both VRPSG and Prox-SVRG achieve a linear convergence rate without strong\nconvexity. A key ingredient in our proof is a Semi-Strongly Convex (SSC)\ninequality which is the first to be rigorously proved for a class of\nnon-strongly convex problems in both constrained and regularized settings.\nMoreover, the SSC inequality is independent of algorithms and may be applied to\nanalyze other stochastic gradient algorithms besides VRPSG and Prox-SVRG, which\nmay be of independent interest. To the best of our knowledge, this is the first\nwork that establishes the linear convergence rate for the variance-reduced\nstochastic gradient algorithms on solving both constrained and regularized\nproblems without strong convexity.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 16:37:33 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2015 14:44:37 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Gong", "Pinghua", ""], ["Ye", "Jieping", ""]]}, {"id": "1406.1111", "submitter": "Wesley Calvert", "authors": "Wesley Calvert", "title": "PAC Learning, VC Dimension, and the Arithmetic Hierarchy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compute that the index set of PAC-learnable concept classes is\n$m$-complete $\\Sigma^0_3$ within the set of indices for all concept classes of\na reasonable form. All concept classes considered are computable enumerations\nof computable $\\Pi^0_1$ classes, in a sense made precise here. This family of\nconcept classes is sufficient to cover all standard examples, and also has the\nproperty that PAC learnability is equivalent to finite VC dimension.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 16:59:33 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Calvert", "Wesley", ""]]}, {"id": "1406.1167", "submitter": "Xu-Cheng Yin", "authors": "Xu-Cheng Yin and Chun Yang and Hong-Wei Hao", "title": "Learning to Diversify via Weighted Kernels for Classifier Ensemble", "comments": "Submitted to IEEE Trans. Pattern Analysis and Machine Intelligence\n  (TPAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifier ensemble generally should combine diverse component classifiers.\nHowever, it is difficult to give a definitive connection between diversity\nmeasure and ensemble accuracy. Given a list of available component classifiers,\nhow to adaptively and diversely ensemble classifiers becomes a big challenge in\nthe literature. In this paper, we argue that diversity, not direct diversity on\nsamples but adaptive diversity with data, is highly correlated to ensemble\naccuracy, and we propose a novel technology for classifier ensemble, learning\nto diversify, which learns to adaptively combine classifiers by considering\nboth accuracy and diversity. Specifically, our approach, Learning TO Diversify\nvia Weighted Kernels (L2DWK), performs classifier combination by optimizing a\ndirect but simple criterion: maximizing ensemble accuracy and adaptive\ndiversity simultaneously by minimizing a convex loss function. Given a measure\nformulation, the diversity is calculated with weighted kernels (i.e., the\ndiversity is measured on the component classifiers' outputs which are kernelled\nand weighted), and the kernel weights are automatically learned. We minimize\nthis loss function by estimating the kernel weights in conjunction with the\nclassifier weights, and propose a self-training algorithm for conducting this\nconvex optimization procedure iteratively. Extensive experiments on a variety\nof 32 UCI classification benchmark datasets show that the proposed approach\nconsistently outperforms state-of-the-art ensembles such as Bagging, AdaBoost,\nRandom Forests, Gasen, Regularized Selective Ensemble, and Ensemble Pruning via\nSemi-Definite Programming.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 09:16:42 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Yin", "Xu-Cheng", ""], ["Yang", "Chun", ""], ["Hao", "Hong-Wei", ""]]}, {"id": "1406.1222", "submitter": "Greg Ver Steeg", "authors": "Greg Ver Steeg and Aram Galstyan", "title": "Discovering Structure in High-Dimensional Data Through Correlation\n  Explanation", "comments": "15 pages, 6 figures. Includes supplementary material and link to\n  code. Published in the proceedings of the 28th Annual Conference on Neural\n  Information Processing Systems, NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to learn a hierarchy of successively more abstract\nrepresentations of complex data based on optimizing an information-theoretic\nobjective. Intuitively, the optimization searches for a set of latent factors\nthat best explain the correlations in the data as measured by multivariate\nmutual information. The method is unsupervised, requires no model assumptions,\nand scales linearly with the number of variables which makes it an attractive\napproach for very high dimensional systems. We demonstrate that Correlation\nExplanation (CorEx) automatically discovers meaningful structure for data from\ndiverse sources including personality tests, DNA, and human language.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 21:46:30 GMT"}, {"version": "v2", "created": "Fri, 31 Oct 2014 02:43:28 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1406.1231", "submitter": "George Dahl", "authors": "George E. Dahl and Navdeep Jaitly and Ruslan Salakhutdinov", "title": "Multi-task Neural Networks for QSAR Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although artificial neural networks have occasionally been used for\nQuantitative Structure-Activity/Property Relationship (QSAR/QSPR) studies in\nthe past, the literature has of late been dominated by other machine learning\ntechniques such as random forests. However, a variety of new neural net\ntechniques along with successful applications in other domains have renewed\ninterest in network approaches. In this work, inspired by the winning team's\nuse of neural networks in a recent QSAR competition, we used an artificial\nneural network to learn a function that predicts activities of compounds for\nmultiple assays at the same time. We conducted experiments leveraging recent\nmethods for dealing with overfitting in neural networks as well as other tricks\nfrom the neural networks literature. We compared our methods to alternative\nmethods reported to perform well on these tasks and found that our neural net\nmethods provided superior performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 23:00:05 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Dahl", "George E.", ""], ["Jaitly", "Navdeep", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1406.1305", "submitter": "Dan Garber", "authors": "Dan Garber, Elad Hazan", "title": "Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Frank-Wolfe method (a.k.a. conditional gradient algorithm) for smooth\noptimization has regained much interest in recent years in the context of large\nscale optimization and machine learning. A key advantage of the method is that\nit avoids projections - the computational bottleneck in many applications -\nreplacing it by a linear optimization step. Despite this advantage, the known\nconvergence rates of the FW method fall behind standard first order methods for\nmost settings of interest. It is an active line of research to derive faster\nlinear optimization-based algorithms for various settings of convex\noptimization.\n  In this paper we consider the special case of optimization over strongly\nconvex sets, for which we prove that the vanila FW method converges at a rate\nof $\\frac{1}{t^2}$. This gives a quadratic improvement in convergence rate\ncompared to the general case, in which convergence is of the order\n$\\frac{1}{t}$, and known to be tight. We show that various balls induced by\n$\\ell_p$ norms, Schatten norms and group norms are strongly convex on one hand\nand on the other hand, linear optimization over these sets is straightforward\nand admits a closed-form solution. We further show how several previous\nfast-rate results for the FW method follow easily from our analysis.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 09:25:22 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2015 18:15:14 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Garber", "Dan", ""], ["Hazan", "Elad", ""]]}, {"id": "1406.1385", "submitter": "Onur Dikmen", "authors": "Onur Dikmen and Zhirong Yang and Erkki Oja", "title": "Learning the Information Divergence", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information divergence that measures the difference between two nonnegative\nmatrices or tensors has found its use in a variety of machine learning\nproblems. Examples are Nonnegative Matrix/Tensor Factorization, Stochastic\nNeighbor Embedding, topic models, and Bayesian network optimization. The\nsuccess of such a learning task depends heavily on a suitable divergence. A\nlarge variety of divergences have been suggested and analyzed, but very few\nresults are available for an objective choice of the optimal divergence for a\ngiven task. Here we present a framework that facilitates automatic selection of\nthe best divergence among a given family, based on standard maximum likelihood\nestimation. We first propose an approximated Tweedie distribution for the\nbeta-divergence family. Selecting the best beta then becomes a machine learning\nproblem solved by maximum likelihood. Next, we reformulate alpha-divergence in\nterms of beta-divergence, which enables automatic selection of alpha by maximum\nlikelihood with reuse of the learning principle for beta-divergence.\nFurthermore, we show the connections between gamma and beta-divergences as well\nas R\\'enyi and alpha-divergences, such that our automatic selection framework\nis extended to non-separable divergences. Experiments on both synthetic and\nreal-world data demonstrate that our method can quite accurately select\ninformation divergence across different learning problems and various\ndivergence families.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 13:44:25 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Dikmen", "Onur", ""], ["Yang", "Zhirong", ""], ["Oja", "Erkki", ""]]}, {"id": "1406.1411", "submitter": "Denis Mau\\'a Dr.", "authors": "Siqi Nie, Denis Deratani Maua, Cassio Polpo de Campos, Qiang Ji", "title": "Advances in Learning Bayesian Networks of Bounded Treewidth", "comments": "23 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents novel algorithms for learning Bayesian network structures\nwith bounded treewidth. Both exact and approximate methods are developed. The\nexact method combines mixed-integer linear programming formulations for\nstructure learning and treewidth computation. The approximate method consists\nin uniformly sampling $k$-trees (maximal graphs of treewidth $k$), and\nsubsequently selecting, exactly or approximately, the best structure whose\nmoral graph is a subgraph of that $k$-tree. Some properties of these methods\nare discussed and proven. The approaches are empirically compared to each other\nand to a state-of-the-art method for learning bounded treewidth structures on a\ncollection of public data sets with up to 100 variables. The experiments show\nthat our exact algorithm outperforms the state of the art, and that the\napproximate approach is fairly accurate.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 15:10:40 GMT"}, {"version": "v2", "created": "Fri, 6 Jun 2014 19:51:07 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["Nie", "Siqi", ""], ["Maua", "Denis Deratani", ""], ["de Campos", "Cassio Polpo", ""], ["Ji", "Qiang", ""]]}, {"id": "1406.1485", "submitter": "Li Yao", "authors": "Tapani Raiko, Li Yao, Kyunghyun Cho and Yoshua Bengio", "title": "Iterative Neural Autoregressive Distribution Estimator (NADE-k)", "comments": "Accepted at Neural Information Processing Systems (NIPS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training of the neural autoregressive density estimator (NADE) can be viewed\nas doing one step of probabilistic inference on missing values in data. We\npropose a new model that extends this inference scheme to multiple steps,\narguing that it is easier to learn to improve a reconstruction in $k$ steps\nrather than to learn to reconstruct in a single inference step. The proposed\nmodel is an unsupervised building block for deep learning that combines the\ndesirable properties of NADE and multi-predictive training: (1) Its test\nlikelihood can be computed analytically, (2) it is easy to generate independent\nsamples from it, and (3) it uses an inference engine that is a superset of\nvariational inference for Boltzmann machines. The proposed NADE-k is\ncompetitive with the state-of-the-art in density estimation on the two datasets\ntested.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 19:13:51 GMT"}, {"version": "v2", "created": "Wed, 11 Jun 2014 11:28:36 GMT"}, {"version": "v3", "created": "Sat, 6 Dec 2014 00:22:00 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Raiko", "Tapani", ""], ["Yao", "Li", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1406.1509", "submitter": "Wojciech Ja\\'skowski", "authors": "Wojciech Ja\\'skowski", "title": "Systematic N-tuple Networks for Position Evaluation: Exceeding 90% in\n  the Othello League", "comments": "Added technical report number", "journal-ref": "ICGA Journal 37(2), 2014, pp. 85-96", "doi": null, "report-no": "RA-06/2014", "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  N-tuple networks have been successfully used as position evaluation functions\nfor board games such as Othello or Connect Four. The effectiveness of such\nnetworks depends on their architecture, which is determined by the placement of\nconstituent n-tuples, sequences of board locations, providing input to the\nnetwork. The most popular method of placing n-tuples consists in randomly\ngenerating a small number of long, snake-shaped board location sequences. In\ncomparison, we show that learning n-tuple networks is significantly more\neffective if they involve a large number of systematically placed, short,\nstraight n-tuples. Moreover, we demonstrate that in order to obtain the best\nperformance and the steepest learning curve for Othello it is enough to use\nn-tuples of size just 2, yielding a network consisting of only 288 weights. The\nbest such network evolved in this study has been evaluated in the online\nOthello League, obtaining the performance of nearly 96% --- more than any other\nplayer to date.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 20:10:48 GMT"}, {"version": "v2", "created": "Tue, 24 Jun 2014 10:06:29 GMT"}, {"version": "v3", "created": "Wed, 25 Jun 2014 20:12:19 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Ja\u015bkowski", "Wojciech", ""]]}, {"id": "1406.1580", "submitter": "Vijay Bhaskar Semwal", "authors": "Vishwanath Bijalwan, Pinki Kumari, Jordan Pascual and Vijay Bhaskar\n  Semwal", "title": "Machine learning approach for text and document mining", "comments": "arXiv admin note: text overlap with arXiv:1003.1795, arXiv:1212.2065\n  by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text Categorization (TC), also known as Text Classification, is the task of\nautomatically classifying a set of text documents into different categories\nfrom a predefined set. If a document belongs to exactly one of the categories,\nit is a single-label classification task; otherwise, it is a multi-label\nclassification task. TC uses several tools from Information Retrieval (IR) and\nMachine Learning (ML) and has received much attention in the last years from\nboth researchers in the academia and industry developers. In this paper, we\nfirst categorize the documents using KNN based machine learning approach and\nthen return the most relevant documents.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 04:37:19 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["Bijalwan", "Vishwanath", ""], ["Kumari", "Pinki", ""], ["Pascual", "Jordan", ""], ["Semwal", "Vijay Bhaskar", ""]]}, {"id": "1406.1584", "submitter": "Wojciech Zaremba", "authors": "Wojciech Zaremba, Karol Kurach, Rob Fergus", "title": "Learning to Discover Efficient Mathematical Identities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore how machine learning techniques can be applied to\nthe discovery of efficient mathematical identities. We introduce an attribute\ngrammar framework for representing symbolic expressions. Given a set of grammar\nrules we build trees that combine different rules, looking for branches which\nyield compositions that are analytically equivalent to a target expression, but\nof lower computational complexity. However, as the size of the trees grows\nexponentially with the complexity of the target expression, brute force search\nis impractical for all but the simplest of expressions. Consequently, we\nintroduce two novel learning approaches that are able to learn from simpler\nexpressions to guide the tree search. The first of these is a simple n-gram\nmodel, the other being a recursive neural-network. We show how these approaches\nenable us to derive complex identities, beyond reach of brute-force search, or\nhuman derivation.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 05:28:48 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 03:49:51 GMT"}, {"version": "v3", "created": "Thu, 6 Nov 2014 02:56:34 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Zaremba", "Wojciech", ""], ["Kurach", "Karol", ""], ["Fergus", "Rob", ""]]}, {"id": "1406.1621", "submitter": "Matthias Seibert", "authors": "Matthias Seibert, Julian W\\\"ormann, R\\'emi Gribonval, Martin\n  Kleinsteuber", "title": "Separable Cosparse Analysis Operator Learning", "comments": "5 pages, 3 figures, accepted at EUSIPCO 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of having a sparse representation for a certain class of signals\nhas many applications in data analysis, image processing, and other research\nfields. Among sparse representations, the cosparse analysis model has recently\ngained increasing interest. Many signals exhibit a multidimensional structure,\ne.g. images or three-dimensional MRI scans. Most data analysis and learning\nalgorithms use vectorized signals and thereby do not account for this\nunderlying structure. The drawback of not taking the inherent structure into\naccount is a dramatic increase in computational cost. We propose an algorithm\nfor learning a cosparse Analysis Operator that adheres to the preexisting\nstructure of the data, and thus allows for a very efficient implementation.\nThis is achieved by enforcing a separable structure on the learned operator.\nOur learning algorithm is able to deal with multidimensional data of arbitrary\norder. We evaluate our method on volumetric data at the example of\nthree-dimensional MRI scans.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 09:33:59 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["Seibert", "Matthias", ""], ["W\u00f6rmann", "Julian", ""], ["Gribonval", "R\u00e9mi", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "1406.1655", "submitter": "Justin Bayer", "authors": "Justin Bayer, Christian Osendorfer", "title": "Variational inference of latent state sequences using Recurrent Networks", "comments": "This paper has been withdrawn due to a derivation/implementation\n  error and the resulting invalidation of the results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the estimation of deep directed graphical models and\nrecurrent networks let us contribute to the removal of a blind spot in the area\nof probabilistc modelling of time series. The proposed methods i) can infer\ndistributed latent state-space trajectories with nonlinear transitions, ii)\nscale to large data sets thanks to the use of a stochastic objective and fast,\napproximate inference, iii) enable the design of rich emission models which iv)\nwill naturally lead to structured outputs. Two different paths of introducing\nlatent state sequences are pursued, leading to the variational recurrent auto\nencoder (VRAE) and the variational one step predictor (VOSP). The use of\nindependent Wiener processes as priors on the latent state sequence is a viable\ncompromise between efficient computation of the Kullback-Leibler divergence\nfrom the variational approximation of the posterior and maintaining a\nreasonable belief in the dynamics. We verify our methods empirically, obtaining\nresults close or superior to the state of the art. We also show qualitative\nresults for denoising and missing value imputation.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 11:53:46 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 08:04:58 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Bayer", "Justin", ""], ["Osendorfer", "Christian", ""]]}, {"id": "1406.1770", "submitter": "Tomaso Poggio", "authors": "Tomaso Poggio, Jim Mutch, Leyla Isik", "title": "Computational role of eccentricity dependent cortical magnification", "comments": null, "journal-ref": null, "doi": null, "report-no": "CBMM memo 17", "categories": "cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a sampling extension of M-theory focused on invariance to scale\nand translation. Quite surprisingly, the theory predicts an architecture of\nearly vision with increasing receptive field sizes and a high resolution fovea\n-- in agreement with data about the cortical magnification factor, V1 and the\nretina. From the slope of the inverse of the magnification factor, M-theory\npredicts a cortical \"fovea\" in V1 in the order of $40$ by $40$ basic units at\neach receptive field size -- corresponding to a foveola of size around $26$\nminutes of arc at the highest resolution, $\\approx 6$ degrees at the lowest\nresolution. It also predicts uniform scale invariance over a fixed range of\nscales independently of eccentricity, while translation invariance should\ndepend linearly on spatial frequency. Bouma's law of crowding follows in the\ntheory as an effect of cortical area-by-cortical area pooling; the Bouma\nconstant is the value expected if the signature responsible for recognition in\nthe crowding experiments originates in V2. From a broader perspective, the\nemerging picture suggests that visual recognition under natural conditions\ntakes place by composing information from a set of fixations, with each\nfixation providing recognition from a space-scale image fragment -- that is an\nimage patch represented at a set of increasing sizes and decreasing\nresolutions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 18:49:56 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["Poggio", "Tomaso", ""], ["Mutch", "Jim", ""], ["Isik", "Leyla", ""]]}, {"id": "1406.1822", "submitter": "Anna Choromanska", "authors": "Anna Choromanska and John Langford", "title": "Logarithmic Time Online Multiclass prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of multiclass classification with an extremely large\nnumber of classes (k), with the goal of obtaining train and test time\ncomplexity logarithmic in the number of classes. We develop top-down tree\nconstruction approaches for constructing logarithmic depth trees. On the\ntheoretical front, we formulate a new objective function, which is optimized at\neach node of the tree and creates dynamic partitions of the data which are both\npure (in terms of class labels) and balanced. We demonstrate that under\nfavorable conditions, we can construct logarithmic depth trees that have leaves\nwith low label entropy. However, the objective function at the nodes is\nchallenging to optimize computationally. We address the empirical problem with\na new online decision tree construction procedure. Experiments demonstrate that\nthis online algorithm quickly achieves improvement in test error compared to\nmore common logarithmic training time approaches, which makes it a plausible\nmethod in computationally constrained large-k applications.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 21:52:25 GMT"}, {"version": "v10", "created": "Fri, 22 May 2015 23:35:53 GMT"}, {"version": "v11", "created": "Wed, 3 Jun 2015 16:29:53 GMT"}, {"version": "v12", "created": "Thu, 6 Aug 2015 04:02:41 GMT"}, {"version": "v13", "created": "Sat, 14 Nov 2015 23:02:33 GMT"}, {"version": "v2", "created": "Wed, 6 Aug 2014 08:44:27 GMT"}, {"version": "v3", "created": "Thu, 7 Aug 2014 16:53:01 GMT"}, {"version": "v4", "created": "Sat, 25 Oct 2014 22:57:31 GMT"}, {"version": "v5", "created": "Sun, 14 Dec 2014 20:43:46 GMT"}, {"version": "v6", "created": "Fri, 6 Feb 2015 00:54:09 GMT"}, {"version": "v7", "created": "Fri, 27 Mar 2015 21:04:14 GMT"}, {"version": "v8", "created": "Mon, 18 May 2015 00:08:13 GMT"}, {"version": "v9", "created": "Tue, 19 May 2015 01:14:10 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Choromanska", "Anna", ""], ["Langford", "John", ""]]}, {"id": "1406.1827", "submitter": "Samuel Bowman", "authors": "Samuel R. Bowman, Christopher Potts, Christopher D. Manning", "title": "Recursive Neural Networks Can Learn Logical Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree-structured recursive neural networks (TreeRNNs) for sentence meaning\nhave been successful for many applications, but it remains an open question\nwhether the fixed-length representations that they learn can support tasks as\ndemanding as logical deduction. We pursue this question by evaluating whether\ntwo such models---plain TreeRNNs and tree-structured neural tensor networks\n(TreeRNTNs)---can correctly learn to identify logical relationships such as\nentailment and contradiction using these representations. In our first set of\nexperiments, we generate artificial data from a logical grammar and use it to\nevaluate the models' ability to learn to handle basic relational reasoning,\nrecursive structures, and quantification. We then evaluate the models on the\nmore natural SICK challenge data. Both models perform competitively on the SICK\ndata and generalize well in all three experiments on simulated data, suggesting\nthat they can learn suitable representations for logical inference in natural\nlanguage.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 22:09:27 GMT"}, {"version": "v2", "created": "Sun, 14 Dec 2014 20:37:33 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2015 19:48:45 GMT"}, {"version": "v4", "created": "Thu, 14 May 2015 19:37:38 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Bowman", "Samuel R.", ""], ["Potts", "Christopher", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1406.1831", "submitter": "Ben Poole", "authors": "Ben Poole, Jascha Sohl-Dickstein, Surya Ganguli", "title": "Analyzing noise in autoencoders and deep networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoders have emerged as a useful framework for unsupervised learning of\ninternal representations, and a wide variety of apparently conceptually\ndisparate regularization techniques have been proposed to generate useful\nfeatures. Here we extend existing denoising autoencoders to additionally inject\nnoise before the nonlinearity, and at the hidden unit activations. We show that\na wide variety of previous methods, including denoising, contractive, and\nsparse autoencoders, as well as dropout can be interpreted using this\nframework. This noise injection framework reaps practical benefits by providing\na unified strategy to develop new internal representations by designing the\nnature of the injected noise. We show that noisy autoencoders outperform\ndenoising autoencoders at the very task of denoising, and are competitive with\nother single-layer techniques on MNIST, and CIFAR-10. We also show that types\nof noise other than dropout improve performance in a deep network through\nsparsifying, decorrelating, and spreading information across representations.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 22:49:11 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Poole", "Ben", ""], ["Sohl-Dickstein", "Jascha", ""], ["Ganguli", "Surya", ""]]}, {"id": "1406.1833", "submitter": "Kenneth Stanley", "authors": "Paul A. Szerlip, Gregory Morse, Justin K. Pugh, and Kenneth O. Stanley", "title": "Unsupervised Feature Learning through Divergent Discriminative Feature\n  Accumulation", "comments": "Corrected citation formatting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike unsupervised approaches such as autoencoders that learn to reconstruct\ntheir inputs, this paper introduces an alternative approach to unsupervised\nfeature learning called divergent discriminative feature accumulation (DDFA)\nthat instead continually accumulates features that make novel discriminations\namong the training set. Thus DDFA features are inherently discriminative from\nthe start even though they are trained without knowledge of the ultimate\nclassification problem. Interestingly, DDFA also continues to add new features\nindefinitely (so it does not depend on a hidden layer size), is not based on\nminimizing error, and is inherently divergent instead of convergent, thereby\nproviding a unique direction of research for unsupervised feature learning. In\nthis paper the quality of its learned features is demonstrated on the MNIST\ndataset, where its performance confirms that indeed DDFA is a viable technique\nfor learning useful features.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 23:45:03 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 03:37:45 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Szerlip", "Paul A.", ""], ["Morse", "Gregory", ""], ["Pugh", "Justin K.", ""], ["Stanley", "Kenneth O.", ""]]}, {"id": "1406.1837", "submitter": "Kai-Wei Chang", "authors": "Kai-Wei Chang, He He, Hal Daum\\'e III, John Langford, Stephane Ross", "title": "A Credit Assignment Compiler for Joint Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning applications involve jointly predicting multiple\nmutually dependent output variables. Learning to search is a family of methods\nwhere the complex decision problem is cast into a sequence of decisions via a\nsearch space. Although these methods have shown promise both in theory and in\npractice, implementing them has been burdensomely awkward. In this paper, we\nshow the search space can be defined by an arbitrary imperative program,\nturning learning to search into a credit assignment compiler. Altogether with\nthe algorithmic improvements for the compiler, we radically reduce the\ncomplexity of programming and the running time. We demonstrate the feasibility\nof our approach on multiple joint prediction tasks. In all cases, we obtain\naccuracies as high as alternative approaches, at drastically reduced execution\nand programming time.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 00:24:42 GMT"}, {"version": "v2", "created": "Fri, 13 Jun 2014 13:16:36 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2015 23:44:33 GMT"}, {"version": "v4", "created": "Fri, 5 Jun 2015 16:26:05 GMT"}, {"version": "v5", "created": "Wed, 1 Jun 2016 05:35:31 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Chang", "Kai-Wei", ""], ["He", "He", ""], ["Daum\u00e9", "Hal", "III"], ["Langford", "John", ""], ["Ross", "Stephane", ""]]}, {"id": "1406.1853", "submitter": "Ian Osband", "authors": "Ian Osband, Benjamin Van Roy", "title": "Model-based Reinforcement Learning and the Eluder Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning to optimize an unknown Markov decision\nprocess (MDP). We show that, if the MDP can be parameterized within some known\nfunction class, we can obtain regret bounds that scale with the dimensionality,\nrather than cardinality, of the system. We characterize this dependence\nexplicitly as $\\tilde{O}(\\sqrt{d_K d_E T})$ where $T$ is time elapsed, $d_K$ is\nthe Kolmogorov dimension and $d_E$ is the \\emph{eluder dimension}. These\nrepresent the first unified regret bounds for model-based reinforcement\nlearning and provide state of the art guarantees in several important settings.\nMoreover, we present a simple and computationally efficient algorithm\n\\emph{posterior sampling for reinforcement learning} (PSRL) that satisfies\nthese bounds.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 03:02:09 GMT"}, {"version": "v2", "created": "Fri, 31 Oct 2014 23:36:00 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1406.1856", "submitter": "Haipeng Luo", "authors": "Haipeng Luo and Robert E. Schapire", "title": "A Drifting-Games Analysis for Online Learning and Applications to\n  Boosting", "comments": "In NIPS2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a general mechanism to design online learning algorithms based on\na minimax analysis within a drifting-games framework. Different online learning\nsettings (Hedge, multi-armed bandit problems and online convex optimization)\nare studied by converting into various kinds of drifting games. The original\nminimax analysis for drifting games is then used and generalized by applying a\nseries of relaxations, starting from choosing a convex surrogate of the 0-1\nloss function. With different choices of surrogates, we not only recover\nexisting algorithms, but also propose new algorithms that are totally\nparameter-free and enjoy other useful properties. Moreover, our drifting-games\nframework naturally allows us to study high probability bounds without\nresorting to any concentration results, and also a generalized notion of regret\nthat measures how good the algorithm is compared to all but the top small\nfraction of candidates. Finally, we translate our new Hedge algorithm into a\nnew adaptive boosting algorithm that is computationally faster as shown in\nexperiments, since it ignores a large number of examples on each round.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 03:11:05 GMT"}, {"version": "v2", "created": "Thu, 30 Oct 2014 17:40:59 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Luo", "Haipeng", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1406.2035", "submitter": "Dani Yogatama", "authors": "Dani Yogatama and Manaal Faruqui and Chris Dyer and Noah A. Smith", "title": "Learning Word Representations with Hierarchical Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for learning word representations using hierarchical\nregularization in sparse coding inspired by the linguistic study of word\nmeanings. We show an efficient learning algorithm based on stochastic proximal\nmethods that is significantly faster than previous approaches, making it\npossible to perform hierarchical sparse coding on a corpus of billions of word\ntokens. Experiments on various benchmark tasks---word similarity ranking,\nanalogies, sentence completion, and sentiment analysis---demonstrate that the\nmethod outperforms or is competitive with state-of-the-art methods. Our word\nrepresentations are available at\n\\url{http://www.ark.cs.cmu.edu/dyogatam/wordvecs/}.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jun 2014 22:35:09 GMT"}, {"version": "v2", "created": "Thu, 6 Nov 2014 14:26:21 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Yogatama", "Dani", ""], ["Faruqui", "Manaal", ""], ["Dyer", "Chris", ""], ["Smith", "Noah A.", ""]]}, {"id": "1406.2080", "submitter": "Sainbayar Sukhbaatar", "authors": "Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev and\n  Rob Fergus", "title": "Training Convolutional Networks with Noisy Labels", "comments": "Accepted as a workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large labeled datasets has allowed Convolutional Network\nmodels to achieve impressive recognition results. However, in many settings\nmanual annotation of the data is impractical; instead our data has noisy\nlabels, i.e. there is some freely available label for each image which may or\nmay not be accurate. In this paper, we explore the performance of\ndiscriminatively-trained Convnets when trained on such noisy data. We introduce\nan extra noise layer into the network which adapts the network outputs to match\nthe noisy label distribution. The parameters of this noise layer can be\nestimated as part of the training process and involve simple modifications to\ncurrent training infrastructures for deep networks. We demonstrate the\napproaches on several datasets, including large scale experiments on the\nImageNet classification benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 05:45:12 GMT"}, {"version": "v2", "created": "Sat, 20 Dec 2014 21:10:03 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2015 21:13:47 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2015 16:44:00 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Sukhbaatar", "Sainbayar", ""], ["Bruna", "Joan", ""], ["Paluri", "Manohar", ""], ["Bourdev", "Lubomir", ""], ["Fergus", "Rob", ""]]}, {"id": "1406.2082", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas and Ryan J. Tibshirani", "title": "Fast and Flexible ADMM Algorithms for Trend Filtering", "comments": "22 pages, 10 figures; published in Journal of Computational and\n  Graphical Statistics, 2015", "journal-ref": null, "doi": "10.1080/10618600.2015.1054033", "report-no": null, "categories": "stat.ML cs.LG cs.NA math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fast and robust algorithm for trend filtering, a\nrecently developed nonparametric regression tool. It has been shown that, for\nestimating functions whose derivatives are of bounded variation, trend\nfiltering achieves the minimax optimal error rate, while other popular methods\nlike smoothing splines and kernels do not. Standing in the way of a more\nwidespread practical adoption, however, is a lack of scalable and numerically\nstable algorithms for fitting trend filtering estimates. This paper presents a\nhighly efficient, specialized ADMM routine for trend filtering. Our algorithm\nis competitive with the specialized interior point methods that are currently\nin use, and yet is far more numerically robust. Furthermore, the proposed ADMM\nimplementation is very simple, and importantly, it is flexible enough to extend\nto many interesting related problems, such as sparse trend filtering and\nisotonic trend filtering. Software for our method is freely available, in both\nthe C and R languages.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 05:50:20 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 19:24:51 GMT"}, {"version": "v3", "created": "Mon, 18 May 2015 21:09:02 GMT"}, {"version": "v4", "created": "Sat, 29 Aug 2015 00:46:34 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1406.2083", "submitter": "Aaditya Ramdas", "authors": "Sashank J. Reddi, Aaditya Ramdas, Barnab\\'as P\\'oczos, Aarti Singh and\n  Larry Wasserman", "title": "On the Decreasing Power of Kernel and Distance based Nonparametric\n  Hypothesis Tests in High Dimensions", "comments": "19 pages, 9 figures, published in AAAI-15: The 29th AAAI Conference\n  on Artificial Intelligence (with author order reversed from ArXiv)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about two related decision theoretic problems, nonparametric\ntwo-sample testing and independence testing. There is a belief that two\nrecently proposed solutions, based on kernels and distances between pairs of\npoints, behave well in high-dimensional settings. We identify different sources\nof misconception that give rise to the above belief. Specifically, we\ndifferentiate the hardness of estimation of test statistics from the hardness\nof testing whether these statistics are zero or not, and explicitly discuss a\nnotion of \"fair\" alternative hypotheses for these problems as dimension\nincreases. We then demonstrate that the power of these tests actually drops\npolynomially with increasing dimension against fair alternatives. We end with\nsome theoretical insights and shed light on the \\textit{median heuristic} for\nkernel bandwidth selection. Our work advances the current understanding of the\npower of modern nonparametric hypothesis tests in high dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 05:59:21 GMT"}, {"version": "v2", "created": "Mon, 24 Nov 2014 00:23:35 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Reddi", "Sashank J.", ""], ["Ramdas", "Aaditya", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1406.2210", "submitter": "Juan Pablo Carbajal", "authors": "Juan Pablo Carbajal and Joni Dambre and Michiel Hermans and Benjamin\n  Schrauwen", "title": "Memristor models for machine learning", "comments": "4 figures, no tables. Submitted to neural computation", "journal-ref": null, "doi": "10.1162/NECO_a_00694", "report-no": null, "categories": "cs.LG cond-mat.mtrl-sci", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In the quest for alternatives to traditional CMOS, it is being suggested that\ndigital computing efficiency and power can be improved by matching the\nprecision to the application. Many applications do not need the high precision\nthat is being used today. In particular, large gains in area- and power\nefficiency could be achieved by dedicated analog realizations of approximate\ncomputing engines. In this work, we explore the use of memristor networks for\nanalog approximate computation, based on a machine learning framework called\nreservoir computing. Most experimental investigations on the dynamics of\nmemristors focus on their nonvolatile behavior. Hence, the volatility that is\npresent in the developed technologies is usually unwanted and it is not\nincluded in simulation models. In contrast, in reservoir computing, volatility\nis not only desirable but necessary. Therefore, in this work, we propose two\ndifferent ways to incorporate it into memristor simulation models. The first is\nan extension of Strukov's model and the second is an equivalent Wiener model\napproximation. We analyze and compare the dynamical properties of these models\nand discuss their implications for the memory and the nonlinear processing\ncapacity of memristor networks. Our results indicate that device variability,\nincreasingly causing problems in traditional computer design, is an asset in\nthe context of reservoir computing. We conclude that, although both models\ncould lead to useful memristor based reservoir computing systems, their\ncomputational performance will differ. Therefore, experimental modeling\nresearch is required for the development of accurate volatile memristor models.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 15:16:21 GMT"}, {"version": "v2", "created": "Mon, 14 Jul 2014 14:54:22 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Carbajal", "Juan Pablo", ""], ["Dambre", "Joni", ""], ["Hermans", "Michiel", ""], ["Schrauwen", "Benjamin", ""]]}, {"id": "1406.2235", "submitter": "Michael Smith", "authors": "Michael R. Smith, Tony Martinez, Michael Gashler", "title": "A Hybrid Latent Variable Neural Network Model for Item Recommendation", "comments": "10 pages, 3 tables. arXiv admin note: text overlap with\n  arXiv:1312.5394", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is used to recommend items to a user without\nrequiring a knowledge of the item itself and tends to outperform other\ntechniques. However, collaborative filtering suffers from the cold-start\nproblem, which occurs when an item has not yet been rated or a user has not\nrated any items. Incorporating additional information, such as item or user\ndescriptions, into collaborative filtering can address the cold-start problem.\nIn this paper, we present a neural network model with latent input variables\n(latent neural network or LNN) as a hybrid collaborative filtering technique\nthat addresses the cold-start problem. LNN outperforms a broad selection of\ncontent-based filters (which make recommendations based on item descriptions)\nand other hybrid approaches while maintaining the accuracy of state-of-the-art\ncollaborative filtering techniques.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 16:21:11 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Smith", "Michael R.", ""], ["Martinez", "Tony", ""], ["Gashler", "Michael", ""]]}, {"id": "1406.2237", "submitter": "Michael Smith", "authors": "Michael R. Smith, Tony Martinez", "title": "Reducing the Effects of Detrimental Instances", "comments": "6 pages, 5 tables, 2 figures. arXiv admin note: substantial text\n  overlap with arXiv:1403.1893", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Not all instances in a data set are equally beneficial for inducing a model\nof the data. Some instances (such as outliers or noise) can be detrimental.\nHowever, at least initially, the instances in a data set are generally\nconsidered equally in machine learning algorithms. Many current approaches for\nhandling noisy and detrimental instances make a binary decision about whether\nan instance is detrimental or not. In this paper, we 1) extend this paradigm by\nweighting the instances on a continuous scale and 2) present a methodology for\nmeasuring how detrimental an instance may be for inducing a model of the data.\nWe call our method of identifying and weighting detrimental instances reduced\ndetrimental instance learning (RDIL). We examine RIDL on a set of 54 data sets\nand 5 learning algorithms and compare RIDL with other weighting and filtering\napproaches. RDIL is especially useful for learning algorithms where every\ninstance can affect the classification boundary and the training instances are\nconsidered individually, such as multilayer perceptrons trained with\nbackpropagation (MLPs). Our results also suggest that a more accurate estimate\nof which instances are detrimental can have a significant positive impact for\nhandling them.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 16:34:51 GMT"}, {"version": "v2", "created": "Tue, 14 Oct 2014 22:31:36 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Smith", "Michael R.", ""], ["Martinez", "Tony", ""]]}, {"id": "1406.2390", "submitter": "Xu Chen", "authors": "Xu Chen, Xiuyuan Cheng and St\\'ephane Mallat", "title": "Unsupervised Deep Haar Scattering on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of high-dimensional data defined on graphs is particularly\ndifficult when the graph geometry is unknown. We introduce a Haar scattering\ntransform on graphs, which computes invariant signal descriptors. It is\nimplemented with a deep cascade of additions, subtractions and absolute values,\nwhich iteratively compute orthogonal Haar wavelet transforms. Multiscale\nneighborhoods of unknown graphs are estimated by minimizing an average total\nvariation, with a pair matching algorithm of polynomial complexity. Supervised\nclassification with dimension reduction is tested on data bases of scrambled\nimages, and for signals sampled on unknown irregular grids on a sphere.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 23:51:30 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 15:25:16 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Chen", "Xu", ""], ["Cheng", "Xiuyuan", ""], ["Mallat", "St\u00e9phane", ""]]}, {"id": "1406.2395", "submitter": "Ines Dutra", "authors": "Ezilda Almeida, Pedro Ferreira, Tiago Vinhoza, In\\^es Dutra, Jingwei\n  Li, Yirong Wu, Elizabeth Burnside", "title": "ExpertBayes: Automatically refining manually built Bayesian networks", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian network structures are usually built using only the data and\nstarting from an empty network or from a naive Bayes structure. Very often, in\nsome domains, like medicine, a prior structure knowledge is already known. This\nstructure can be automatically or manually refined in search for better\nperformance models. In this work, we take Bayesian networks built by\nspecialists and show that minor perturbations to this original network can\nyield better classifiers with a very small computational cost, while\nmaintaining most of the intended meaning of the original model.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 00:50:05 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Almeida", "Ezilda", ""], ["Ferreira", "Pedro", ""], ["Vinhoza", "Tiago", ""], ["Dutra", "In\u00eas", ""], ["Li", "Jingwei", ""], ["Wu", "Yirong", ""], ["Burnside", "Elizabeth", ""]]}, {"id": "1406.2419", "submitter": "Hilton Bristow", "authors": "Hilton Bristow, Simon Lucey", "title": "Why do linear SVMs trained on HOG features perform so well?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear Support Vector Machines trained on HOG features are now a de facto\nstandard across many visual perception tasks. Their popularisation can largely\nbe attributed to the step-change in performance they brought to pedestrian\ndetection, and their subsequent successes in deformable parts models. This\npaper explores the interactions that make the HOG-SVM symbiosis perform so\nwell. By connecting the feature extraction and learning processes rather than\ntreating them as disparate plugins, we show that HOG features can be viewed as\ndoing two things: (i) inducing capacity in, and (ii) adding prior to a linear\nSVM trained on pixels. From this perspective, preserving second-order\nstatistics and locality of interactions are key to good performance. We\ndemonstrate surprising accuracy on expression recognition and pedestrian\ndetection tasks, by assuming only the importance of preserving such local\nsecond-order interactions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 04:34:43 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Bristow", "Hilton", ""], ["Lucey", "Simon", ""]]}, {"id": "1406.2431", "submitter": "Oren Anava", "authors": "Oren Anava, Shahar Golan, Nadav Golbandi, Zohar Karnin, Ronny Lempel,\n  Oleg Rokhlenko, Oren Somekh", "title": "Budget-Constrained Item Cold-Start Handling in Collaborative Filtering\n  Recommenders via Optimal Design", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that collaborative filtering (CF) based recommender systems\nprovide better modeling of users and items associated with considerable rating\nhistory. The lack of historical ratings results in the user and the item\ncold-start problems. The latter is the main focus of this work. Most of the\ncurrent literature addresses this problem by integrating content-based\nrecommendation techniques to model the new item. However, in many cases such\ncontent is not available, and the question arises is whether this problem can\nbe mitigated using CF techniques only. We formalize this problem as an\noptimization problem: given a new item, a pool of available users, and a budget\nconstraint, select which users to assign with the task of rating the new item\nin order to minimize the prediction error of our model. We show that the\nobjective function is monotone-supermodular, and propose efficient optimal\ndesign based algorithms that attain an approximation to its optimum. Our\nfindings are verified by an empirical study using the Netflix dataset, where\nthe proposed algorithms outperform several baselines for the problem at hand.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 06:17:23 GMT"}, {"version": "v2", "created": "Wed, 19 Nov 2014 21:10:43 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 09:51:02 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Anava", "Oren", ""], ["Golan", "Shahar", ""], ["Golbandi", "Nadav", ""], ["Karnin", "Zohar", ""], ["Lempel", "Ronny", ""], ["Rokhlenko", "Oleg", ""], ["Somekh", "Oren", ""]]}, {"id": "1406.2504", "submitter": "Bo Xin", "authors": "Bo Xin and David Wipf", "title": "Exploring Algorithmic Limits of Matrix Rank Minimization under Affine\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications require recovering a matrix of minimal rank within an\naffine constraint set, with matrix completion a notable special case. Because\nthe problem is NP-hard in general, it is common to replace the matrix rank with\nthe nuclear norm, which acts as a convenient convex surrogate. While elegant\ntheoretical conditions elucidate when this replacement is likely to be\nsuccessful, they are highly restrictive and convex algorithms fail when the\nambient rank is too high or when the constraint set is poorly structured.\nNon-convex alternatives fare somewhat better when carefully tuned; however,\nconvergence to locally optimal solutions remains a continuing source of\nfailure. Against this backdrop we derive a deceptively simple and\nparameter-free probabilistic PCA-like algorithm that is capable, over a wide\nbattery of empirical tests, of successful recovery even at the theoretical\nlimit where the number of measurements equal the degrees of freedom in the\nunknown low-rank matrix. Somewhat surprisingly, this is possible even when the\naffine constraint set is highly ill-conditioned. While proving general recovery\nguarantees remains evasive for non-convex algorithms, Bayesian-inspired or\notherwise, we nonetheless show conditions whereby the underlying cost function\nhas a unique stationary point located at the global optimum; no existing cost\nfunction we are aware of satisfies this same property. We conclude with a\nsimple computer vision application involving image rectification and a standard\ncollaborative filtering benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 10:53:20 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 07:24:02 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2015 19:05:56 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Xin", "Bo", ""], ["Wipf", "David", ""]]}, {"id": "1406.2538", "submitter": "Guntis Barzdins", "authors": "Guntis Barzdins", "title": "FrameNet CNL: a Knowledge Representation and Information Extraction\n  Language", "comments": "CNL-2014 camera-ready version. The final publication is available at\n  link.springer.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a FrameNet-based information extraction and knowledge\nrepresentation framework, called FrameNet-CNL. The framework is used on natural\nlanguage documents and represents the extracted knowledge in a tailor-made\nFrame-ontology from which unambiguous FrameNet-CNL paraphrase text can be\ngenerated automatically in multiple languages. This approach brings together\nthe fields of information extraction and CNL, because a source text can be\nconsidered belonging to FrameNet-CNL, if information extraction parser produces\nthe correct knowledge representation as a result. We describe a\nstate-of-the-art information extraction parser used by a national news agency\nand speculate that FrameNet-CNL eventually could shape the natural language\nsubset used for writing the newswire articles.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 13:16:36 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Barzdins", "Guntis", ""]]}, {"id": "1406.2541", "submitter": "Matthew W. Hoffman", "authors": "Jos\\'e Miguel Hern\\'andez-Lobato, Matthew W. Hoffman, Zoubin\n  Ghahramani", "title": "Predictive Entropy Search for Efficient Global Optimization of Black-box\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel information-theoretic approach for Bayesian optimization\ncalled Predictive Entropy Search (PES). At each iteration, PES selects the next\nevaluation point that maximizes the expected information gained with respect to\nthe global maximum. PES codifies this intractable acquisition function in terms\nof the expected reduction in the differential entropy of the predictive\ndistribution. This reformulation allows PES to obtain approximations that are\nboth more accurate and efficient than other alternatives such as Entropy Search\n(ES). Furthermore, PES can easily perform a fully Bayesian treatment of the\nmodel hyperparameters while ES cannot. We evaluate PES in both synthetic and\nreal-world applications, including optimization problems in machine learning,\nfinance, biotechnology, and robotics. We show that the increased accuracy of\nPES leads to significant gains in optimization performance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 13:29:09 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Hoffman", "Matthew W.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1406.2572", "submitter": "KyungHyun Cho", "authors": "Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya\n  Ganguli and Yoshua Bengio", "title": "Identifying and attacking the saddle point problem in high-dimensional\n  non-convex optimization", "comments": "The theoretical review and analysis in this article draw heavily from\n  arXiv:1405.4604 [cs.LG]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central challenge to many fields of science and engineering involves\nminimizing non-convex error functions over continuous, high dimensional spaces.\nGradient descent or quasi-Newton methods are almost ubiquitously used to\nperform such minimizations, and it is often thought that a main source of\ndifficulty for these local methods to find the global minimum is the\nproliferation of local minima with much higher error than the global minimum.\nHere we argue, based on results from statistical physics, random matrix theory,\nneural network theory, and empirical evidence, that a deeper and more profound\ndifficulty originates from the proliferation of saddle points, not local\nminima, especially in high dimensional problems of practical interest. Such\nsaddle points are surrounded by high error plateaus that can dramatically slow\ndown learning, and give the illusory impression of the existence of a local\nminimum. Motivated by these arguments, we propose a new approach to\nsecond-order optimization, the saddle-free Newton method, that can rapidly\nescape high dimensional saddle points, unlike gradient descent and quasi-Newton\nmethods. We apply this algorithm to deep or recurrent neural network training,\nand provide numerical evidence for its superior optimization performance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 14:52:14 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Dauphin", "Yann", ""], ["Pascanu", "Razvan", ""], ["Gulcehre", "Caglar", ""], ["Cho", "Kyunghyun", ""], ["Ganguli", "Surya", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1406.2580", "submitter": "L.T. Handoko", "authors": "D. H. Apriyanti, A.A. Arymurthy, L.T. Handoko", "title": "Identification of Orchid Species Using Content-Based Flower Image\n  Retrieval", "comments": "Proceeding of International Conference on Computer, Control,\n  Informatics and its Applications 2013, pp. 53-57", "journal-ref": null, "doi": "10.1109/IC3INA.2013.6819148", "report-no": "KRPURWODADILIPI-13044", "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we developed the system for recognizing the orchid species by\nusing the images of flower. We used MSRM (Maximal Similarity based on Region\nMerging) method for segmenting the flower object from the background and\nextracting the shape feature such as the distance from the edge to the centroid\npoint of the flower, aspect ratio, roundness, moment invariant, fractal\ndimension and also extract color feature. We used HSV color feature with\nignoring the V value. To retrieve the image, we used Support Vector Machine\n(SVM) method. Orchid is a unique flower. It has a part of flower called lip\n(labellum) that distinguishes it from other flowers even from other types of\norchids. Thus, in this paper, we proposed to do feature extraction not only on\nflower region but also on lip (labellum) region. The result shows that our\nproposed method can increase the accuracy value of content based flower image\nretrieval for orchid species up to $\\pm$ 14%. The most dominant feature is\nCentroid Contour Distance, Moment Invariant and HSV Color. The system accuracy\nis 85,33% in validation phase and 79,33% in testing phase.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 15:11:27 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Apriyanti", "D. H.", ""], ["Arymurthy", "A. A.", ""], ["Handoko", "L. T.", ""]]}, {"id": "1406.2582", "submitter": "Michael Schober", "authors": "Michael Schober, David Duvenaud, Philipp Hennig", "title": "Probabilistic ODE Solvers with Runge-Kutta Means", "comments": "18 pages (9 page conference paper, plus supplements); appears in\n  Advances in Neural Information Processing Systems (NIPS), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runge-Kutta methods are the classic family of solvers for ordinary\ndifferential equations (ODEs), and the basis for the state of the art. Like\nmost numerical methods, they return point estimates. We construct a family of\nprobabilistic numerical methods that instead return a Gauss-Markov process\ndefining a probability distribution over the ODE solution. In contrast to prior\nwork, we construct this family such that posterior means match the outputs of\nthe Runge-Kutta family exactly, thus inheriting their proven good properties.\nRemaining degrees of freedom not identified by the match to Runge-Kutta are\nchosen such that the posterior probability measure fits the observed structure\nof the ODE. Our results shed light on the structure of Runge-Kutta solvers from\na new direction, provide a richer, probabilistic output, have low computational\ncost, and raise new research questions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 15:13:24 GMT"}, {"version": "v2", "created": "Fri, 24 Oct 2014 11:45:49 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Schober", "Michael", ""], ["Duvenaud", "David", ""], ["Hennig", "Philipp", ""]]}, {"id": "1406.2602", "submitter": "Ethan Fetaya", "authors": "Ethan Fetaya, Ohad Shamir and Shimon Ullman", "title": "Graph Approximation and Clustering on a Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning from a similarity matrix (such as\nspectral clustering and lowd imensional embedding), when computing pairwise\nsimilarities are costly, and only a limited number of entries can be observed.\nWe provide a theoretical analysis using standard notions of graph\napproximation, significantly generalizing previous results (which focused on\nspectral clustering with two clusters). We also propose a new algorithmic\napproach based on adaptive sampling, which experimentally matches or improves\non previous methods, while being considerably more general and computationally\ncheaper.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 15:49:05 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Fetaya", "Ethan", ""], ["Shamir", "Ohad", ""], ["Ullman", "Shimon", ""]]}, {"id": "1406.2616", "submitter": "Ashesh Jain", "authors": "Ashesh Jain, Debarghya Das, Jayesh K Gupta, Ashutosh Saxena", "title": "PlanIt: A Crowdsourcing Approach for Learning to Plan Paths from Large\n  Scale Preference Feedback", "comments": "PlanIt Camera Ready ICRA'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning user preferences over robot trajectories\nfor environments rich in objects and humans. This is challenging because the\ncriterion defining a good trajectory varies with users, tasks and interactions\nin the environment. We represent trajectory preferences using a cost function\nthat the robot learns and uses it to generate good trajectories in new\nenvironments. We design a crowdsourcing system - PlanIt, where non-expert users\nlabel segments of the robot's trajectory. PlanIt allows us to collect a large\namount of user feedback, and using the weak and noisy labels from PlanIt we\nlearn the parameters of our model. We test our approach on 122 different\nenvironments for robotic navigation and manipulation tasks. Our extensive\nexperiments show that the learned cost function generates preferred\ntrajectories in human environments. Our crowdsourcing system is publicly\navailable for the visualization of the learned costs and for providing\npreference feedback: \\url{http://planit.cs.cornell.edu}\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 16:23:52 GMT"}, {"version": "v2", "created": "Wed, 11 Jun 2014 15:18:04 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2016 05:35:21 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Jain", "Ashesh", ""], ["Das", "Debarghya", ""], ["Gupta", "Jayesh K", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1406.2622", "submitter": "Julien Audiffren", "authors": "Julien Audiffren (CMLA), Hachem Kadri (LIF)", "title": "Equivalence of Learning Algorithms", "comments": "arXiv admin note: substantial text overlap with arXiv:1310.2451", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to introduce a concept of equivalence between\nmachine learning algorithms. We define two notions of algorithmic equivalence,\nnamely, weak and strong equivalence. These notions are of paramount importance\nfor identifying when learning prop erties from one learning algorithm can be\ntransferred to another. Using regularized kernel machines as a case study, we\nillustrate the importance of the introduced equivalence concept by analyzing\nthe relation between kernel ridge regression (KRR) and m-power regularized\nleast squares regression (M-RLSR) algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 16:40:56 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Audiffren", "Julien", "", "CMLA"], ["Kadri", "Hachem", "", "LIF"]]}, {"id": "1406.2639", "submitter": "Holger Roth", "authors": "Holger R. Roth and Le Lu and Ari Seff and Kevin M. Cherry and Joanne\n  Hoffman and Shijun Wang and Jiamin Liu and Evrim Turkbey and Ronald M.\n  Summers", "title": "A New 2.5D Representation for Lymph Node Detection using Random Sets of\n  Deep Convolutional Neural Network Observations", "comments": "This article will be presented at MICCAI (Medical Image Computing and\n  Computer-Assisted Interventions) 2014", "journal-ref": "Medical Image Computing and Computer-Assisted Intervention -\n  MICCAI 2014 Volume 8673 of the series Lecture Notes in Computer Science pp\n  520-527", "doi": "10.1007/978-3-319-10404-1_65", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Automated Lymph Node (LN) detection is an important clinical diagnostic task\nbut very challenging due to the low contrast of surrounding structures in\nComputed Tomography (CT) and to their varying sizes, poses, shapes and sparsely\ndistributed locations. State-of-the-art studies show the performance range of\n52.9% sensitivity at 3.1 false-positives per volume (FP/vol.), or 60.9% at 6.1\nFP/vol. for mediastinal LN, by one-shot boosting on 3D HAAR features. In this\npaper, we first operate a preliminary candidate generation stage, towards 100%\nsensitivity at the cost of high FP levels (40 per patient), to harvest volumes\nof interest (VOI). Our 2.5D approach consequently decomposes any 3D VOI by\nresampling 2D reformatted orthogonal views N times, via scale, random\ntranslations, and rotations with respect to the VOI centroid coordinates. These\nrandom views are then used to train a deep Convolutional Neural Network (CNN)\nclassifier. In testing, the CNN is employed to assign LN probabilities for all\nN random views that can be simply averaged (as a set) to compute the final\nclassification probability per VOI. We validate the approach on two datasets:\n90 CT volumes with 388 mediastinal LNs and 86 patients with 595 abdominal LNs.\nWe achieve sensitivities of 70%/83% at 3 FP/vol. and 84%/90% at 6 FP/vol. in\nmediastinum and abdomen respectively, which drastically improves over the\nprevious state-of-the-art work.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 22:43:42 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Roth", "Holger R.", ""], ["Lu", "Le", ""], ["Seff", "Ari", ""], ["Cherry", "Kevin M.", ""], ["Hoffman", "Joanne", ""], ["Wang", "Shijun", ""], ["Liu", "Jiamin", ""], ["Turkbey", "Evrim", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1406.2646", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J Kir\\'aly, Martin Kreuzer, Louis Theran", "title": "Learning with Cross-Kernels and Ideal PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.AC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe how cross-kernel matrices, that is, kernel matrices between the\ndata and a custom chosen set of `feature spanning points' can be used for\nlearning. The main potential of cross-kernels lies in the fact that (a) only\none side of the matrix scales with the number of data points, and (b)\ncross-kernels, as opposed to the usual kernel matrices, can be used to certify\nfor the data manifold. Our theoretical framework, which is based on a duality\ninvolving the feature space and vanishing ideals, indicates that cross-kernels\nhave the potential to be used for any kind of kernel learning. We present a\nnovel algorithm, Ideal PCA (IPCA), which cross-kernelizes PCA. We demonstrate\non real and synthetic data that IPCA allows to (a) obtain PCA-like features\nfaster and (b) to extract novel and empirically validated features certifying\nfor the data manifold.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 17:48:58 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Kir\u00e1ly", "Franz J", ""], ["Kreuzer", "Martin", ""], ["Theran", "Louis", ""]]}, {"id": "1406.2661", "submitter": "Ian Goodfellow", "authors": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David\n  Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio", "title": "Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for estimating generative models via an\nadversarial process, in which we simultaneously train two models: a generative\nmodel G that captures the data distribution, and a discriminative model D that\nestimates the probability that a sample came from the training data rather than\nG. The training procedure for G is to maximize the probability of D making a\nmistake. This framework corresponds to a minimax two-player game. In the space\nof arbitrary functions G and D, a unique solution exists, with G recovering the\ntraining data distribution and D equal to 1/2 everywhere. In the case where G\nand D are defined by multilayer perceptrons, the entire system can be trained\nwith backpropagation. There is no need for any Markov chains or unrolled\napproximate inference networks during either training or generation of samples.\nExperiments demonstrate the potential of the framework through qualitative and\nquantitative evaluation of the generated samples.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 18:58:17 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Goodfellow", "Ian J.", ""], ["Pouget-Abadie", "Jean", ""], ["Mirza", "Mehdi", ""], ["Xu", "Bing", ""], ["Warde-Farley", "David", ""], ["Ozair", "Sherjil", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1406.2673", "submitter": "Balaji Lakshminarayanan", "authors": "Balaji Lakshminarayanan, Daniel M. Roy and Yee Whye Teh", "title": "Mondrian Forests: Efficient Online Random Forests", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 27 (NIPS), pages\n  3140-3148, 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembles of randomized decision trees, usually referred to as random\nforests, are widely used for classification and regression tasks in machine\nlearning and statistics. Random forests achieve competitive predictive\nperformance and are computationally efficient to train and test, making them\nexcellent candidates for real-world prediction tasks. The most popular random\nforest variants (such as Breiman's random forest and extremely randomized\ntrees) operate on batches of training data. Online methods are now in greater\ndemand. Existing online random forests, however, require more training data\nthan their batch counterpart to achieve comparable predictive performance. In\nthis work, we use Mondrian processes (Roy and Teh, 2009) to construct ensembles\nof random decision trees we call Mondrian forests. Mondrian forests can be\ngrown in an incremental/online fashion and remarkably, the distribution of\nonline Mondrian forests is the same as that of batch Mondrian forests. Mondrian\nforests achieve competitive predictive performance comparable with existing\nonline random forests and periodically re-trained batch random forests, while\nbeing more than an order of magnitude faster, thus representing a better\ncomputation vs accuracy tradeoff.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 19:34:51 GMT"}, {"version": "v2", "created": "Mon, 16 Feb 2015 14:57:52 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Lakshminarayanan", "Balaji", ""], ["Roy", "Daniel M.", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1406.2710", "submitter": "Ryan Kiros", "authors": "Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov", "title": "A Multiplicative Model for Learning Distributed Text-Based Attribute\n  Representations", "comments": "11 pages. An earlier version was accepted to the ICML-2014 Workshop\n  on Knowledge-Powered Deep Learning for Text Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a general framework for learning distributed\nrepresentations of attributes: characteristics of text whose representations\ncan be jointly learned with word embeddings. Attributes can correspond to\ndocument indicators (to learn sentence vectors), language indicators (to learn\ndistributed language representations), meta-data and side information (such as\nthe age, gender and industry of a blogger) or representations of authors. We\ndescribe a third-order model where word context and attribute vectors interact\nmultiplicatively to predict the next word in a sequence. This leads to the\nnotion of conditional word similarity: how meanings of words change when\nconditioned on different attributes. We perform several experimental tasks\nincluding sentiment classification, cross-lingual document classification, and\nblog authorship attribution. We also qualitatively evaluate conditional word\nneighbours and attribute-conditioned text generation.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 20:29:10 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Kiros", "Ryan", ""], ["Zemel", "Richard S.", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1406.2721", "submitter": "Zhaoshi Meng", "authors": "Zhaoshi Meng, Brian Eriksson, Alfred O. Hero III", "title": "Learning Latent Variable Gaussian Graphical Models", "comments": "To appear in The 31st International Conference on Machine Learning\n  (ICML 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical models (GGM) have been widely used in many\nhigh-dimensional applications ranging from biological and financial data to\nrecommender systems. Sparsity in GGM plays a central role both statistically\nand computationally. Unfortunately, real-world data often does not fit well to\nsparse graphical models. In this paper, we focus on a family of latent variable\nGaussian graphical models (LVGGM), where the model is conditionally sparse\ngiven latent variables, but marginally non-sparse. In LVGGM, the inverse\ncovariance matrix has a low-rank plus sparse structure, and can be learned in a\nregularized maximum likelihood framework. We derive novel parameter estimation\nerror bounds for LVGGM under mild conditions in the high-dimensional setting.\nThese results complement the existing theory on the structural learning, and\nopen up new possibilities of using LVGGM for statistical inference.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 21:03:22 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Meng", "Zhaoshi", ""], ["Eriksson", "Brian", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1406.2732", "submitter": "George Papandreou", "authors": "George Papandreou", "title": "Deep Epitomic Convolutional Neural Networks", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have recently proven extremely competitive\nin challenging image recognition tasks. This paper proposes the epitomic\nconvolution as a new building block for deep neural networks. An epitomic\nconvolution layer replaces a pair of consecutive convolution and max-pooling\nlayers found in standard deep convolutional neural networks. The main version\nof the proposed model uses mini-epitomes in place of filters and computes\nresponses invariant to small translations by epitomic search instead of\nmax-pooling over image positions. The topographic version of the proposed model\nuses large epitomes to learn filter maps organized in translational\ntopographies. We show that error back-propagation can successfully learn\nmultiple epitomic layers in a supervised fashion. The effectiveness of the\nproposed method is assessed in image classification tasks on standard\nbenchmarks. Our experiments on Imagenet indicate improved recognition\nperformance compared to standard convolutional neural networks of similar\narchitecture. Our models pre-trained on Imagenet perform excellently on\nCaltech-101. We also obtain competitive image classification results on the\nsmall-image MNIST and CIFAR-10 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 22:07:01 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Papandreou", "George", ""]]}, {"id": "1406.2751", "submitter": "J\\\"org Bornschein", "authors": "J\\\"org Bornschein and Yoshua Bengio", "title": "Reweighted Wake-Sleep", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep directed graphical models with many hidden variables and\nperforming inference remains a major challenge. Helmholtz machines and deep\nbelief networks are such models, and the wake-sleep algorithm has been proposed\nto train them. The wake-sleep algorithm relies on training not just the\ndirected generative model but also a conditional generative model (the\ninference network) that runs backward from visible to latent, estimating the\nposterior distribution of latent given visible. We propose a novel\ninterpretation of the wake-sleep algorithm which suggests that better\nestimators of the gradient can be obtained by sampling latent variables\nmultiple times from the inference network. This view is based on importance\nsampling as an estimator of the likelihood, with the approximate inference\nnetwork as a proposal distribution. This interpretation is confirmed\nexperimentally, showing that better likelihood can be achieved with this\nreweighted wake-sleep procedure. Based on this interpretation, we propose that\na sigmoidal belief network is not sufficiently powerful for the layers of the\ninference network in order to recover a good estimator of the posterior\ndistribution of latent variables. Our experiments show that using a more\npowerful layer model, such as NADE, yields substantially better generative\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 00:44:31 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 23:30:10 GMT"}, {"version": "v3", "created": "Sat, 20 Dec 2014 04:25:43 GMT"}, {"version": "v4", "created": "Thu, 16 Apr 2015 17:22:58 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Bornschein", "J\u00f6rg", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1406.2963", "submitter": "Ce Zhang", "authors": "Shanan E. Peters, Ce Zhang, Miron Livny, Christopher R\\'e", "title": "A machine-compiled macroevolutionary history of Phanerozoic life", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.LG q-bio.PE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Many aspects of macroevolutionary theory and our understanding of biotic\nresponses to global environmental change derive from literature-based\ncompilations of palaeontological data. Existing manually assembled databases\nare, however, incomplete and difficult to assess and enhance. Here, we develop\nand validate the quality of a machine reading system, PaleoDeepDive, that\nautomatically locates and extracts data from heterogeneous text, tables, and\nfigures in publications. PaleoDeepDive performs comparably to humans in complex\ndata extraction and inference tasks and generates congruent synthetic\nmacroevolutionary results. Unlike traditional databases, PaleoDeepDive produces\na probabilistic database that systematically improves as information is added.\nWe also show that the system can readily accommodate sophisticated data types,\nsuch as morphological data in biological illustrations and associated textual\ndescriptions. Our machine reading approach to scientific data integration and\nsynthesis brings within reach many questions that are currently underdetermined\nand does so in ways that may stimulate entirely new modes of inquiry.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 17:02:14 GMT"}, {"version": "v2", "created": "Sat, 19 Jul 2014 16:40:47 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Peters", "Shanan E.", ""], ["Zhang", "Ce", ""], ["Livny", "Miron", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1406.2969", "submitter": "Yilun Wang", "authors": "Yilun Wang and Xinhua Su", "title": "Truncated Nuclear Norm Minimization for Image Restoration Based On\n  Iterative Support Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering a large matrix from limited measurements is a challenging task\narising in many real applications, such as image inpainting, compressive\nsensing and medical imaging, and this kind of problems are mostly formulated as\nlow-rank matrix approximation problems. Due to the rank operator being\nnon-convex and discontinuous, most of the recent theoretical studies use the\nnuclear norm as a convex relaxation and the low-rank matrix recovery problem is\nsolved through minimization of the nuclear norm regularized problem. However, a\nmajor limitation of nuclear norm minimization is that all the singular values\nare simultaneously minimized and the rank may not be well approximated\n\\cite{hu2012fast}. Correspondingly, in this paper, we propose a new multi-stage\nalgorithm, which makes use of the concept of Truncated Nuclear Norm\nRegularization (TNNR) proposed in \\citep{hu2012fast} and Iterative Support\nDetection (ISD) proposed in \\citep{wang2010sparse} to overcome the above\nlimitation. Besides matrix completion problems considered in\n\\citep{hu2012fast}, the proposed method can be also extended to the general\nlow-rank matrix recovery problems. Extensive experiments well validate the\nsuperiority of our new algorithms over other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 17:18:25 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Wang", "Yilun", ""], ["Su", "Xinhua", ""]]}, {"id": "1406.2989", "submitter": "Mathias Berglund", "authors": "Tapani Raiko, Mathias Berglund, Guillaume Alain, Laurent Dinh", "title": "Techniques for Learning Binary Stochastic Feedforward Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic binary hidden units in a multi-layer perceptron (MLP) network give\nat least three potential benefits when compared to deterministic MLP networks.\n(1) They allow to learn one-to-many type of mappings. (2) They can be used in\nstructured prediction problems, where modeling the internal structure of the\noutput is important. (3) Stochasticity has been shown to be an excellent\nregularizer, which makes generalization performance potentially better in\ngeneral. However, training stochastic networks is considerably more difficult.\nWe study training using M samples of hidden activations per input. We show that\nthe case M=1 leads to a fundamentally different behavior where the network\ntries to avoid stochasticity. We propose two new estimators for the training\ngradient and propose benchmark tests for comparing training algorithms. Our\nexperiments confirm that training stochastic networks is difficult and show\nthat the proposed two estimators perform favorably among all the five known\nestimators.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 18:29:27 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 13:37:05 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2015 12:58:06 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Raiko", "Tapani", ""], ["Berglund", "Mathias", ""], ["Alain", "Guillaume", ""], ["Dinh", "Laurent", ""]]}, {"id": "1406.3010", "submitter": "Weiguang Ding", "authors": "Weiguang Ding, Graham W. Taylor", "title": "\"Mental Rotation\" by Optimizing Transforming Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual system is able to recognize objects despite transformations\nthat can drastically alter their appearance. To this end, much effort has been\ndevoted to the invariance properties of recognition systems. Invariance can be\nengineered (e.g. convolutional nets), or learned from data explicitly (e.g.\ntemporal coherence) or implicitly (e.g. by data augmentation). One idea that\nhas not, to date, been explored is the integration of latent variables which\npermit a search over a learned space of transformations. Motivated by evidence\nthat people mentally simulate transformations in space while comparing\nexamples, so-called \"mental rotation\", we propose a transforming distance.\nHere, a trained relational model actively transforms pairs of examples so that\nthey are maximally similar in some feature space yet respect the learned\ntransformational constraints. We apply our method to nearest-neighbour problems\non the Toronto Face Database and NORB.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 19:38:05 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 17:55:09 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Ding", "Weiguang", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1406.3100", "submitter": "Jonathan Tapson", "authors": "Philip de Chazal, Jonathan Tapson and Andr\\'e van Schaik", "title": "Learning ELM network weights using linear discriminant analysis", "comments": "In submission to the ELM 2014 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an alternative to the pseudo-inverse method for determining the\nhidden to output weight values for Extreme Learning Machines performing\nclassification tasks. The method is based on linear discriminant analysis and\nprovides Bayes optimal single point estimates for the weight values.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 02:08:31 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["de Chazal", "Philip", ""], ["Tapson", "Jonathan", ""], ["van Schaik", "Andr\u00e9", ""]]}, {"id": "1406.3149", "submitter": "Christian Napoli", "authors": "Francesco Bonanno, Giacomo Capizzi, Grazia Lo Sciuto, Christian\n  Napoli, Giuseppe Pappalardo, Emiliano Tramontana", "title": "A Cascade Neural Network Architecture investigating Surface Plasmon\n  Polaritons propagation for thin metals in OpenMP", "comments": null, "journal-ref": "International conference on Artificial Intelligence and Soft\n  Computing (ICAISC 2014), Vol I, 22-33 (2014)", "doi": null, "report-no": null, "categories": "cs.NE cond-mat.mes-hall cond-mat.mtrl-sci cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Surface plasmon polaritons (SPPs) confined along metal-dielectric interface\nhave attracted a relevant interest in the area of ultracompact photonic\ncircuits, photovoltaic devices and other applications due to their strong field\nconfinement and enhancement. This paper investigates a novel cascade neural\nnetwork (NN) architecture to find the dependance of metal thickness on the SPP\npropagation. Additionally, a novel training procedure for the proposed cascade\nNN has been developed using an OpenMP-based framework, thus greatly reducing\ntraining time. The performed experiments confirm the effectiveness of the\nproposed NN architecture for the problem at hand.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 08:40:04 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Bonanno", "Francesco", ""], ["Capizzi", "Giacomo", ""], ["Sciuto", "Grazia Lo", ""], ["Napoli", "Christian", ""], ["Pappalardo", "Giuseppe", ""], ["Tramontana", "Emiliano", ""]]}, {"id": "1406.3190", "submitter": "Jie Shen", "authors": "Jie Shen and Huan Xu and Ping Li", "title": "Online Optimization for Large-Scale Max-Norm Regularization", "comments": "A conference version appears in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-norm regularizer has been extensively studied in the last decade as it\npromotes an effective low-rank estimation for the underlying data. However,\nsuch max-norm regularized problems are typically formulated and solved in a\nbatch manner, which prevents it from processing big data due to possible memory\nbudget. In this paper, hence, we propose an online algorithm that is scalable\nto large-scale setting. Particularly, we consider the matrix decomposition\nproblem as an example, although a simple variant of the algorithm and analysis\ncan be adapted to other important problems such as matrix completion. The\ncrucial technique in our implementation is to reformulating the max-norm to an\nequivalent matrix factorization form, where the factors consist of a (possibly\novercomplete) basis component and a coefficients one. In this way, we may\nmaintain the basis component in the memory and optimize over it and the\ncoefficients for each sample alternatively. Since the memory footprint of the\nbasis component is independent of the sample size, our algorithm is appealing\nwhen manipulating a large collection of samples. We prove that the sequence of\nthe solutions (i.e., the basis component) produced by our algorithm converges\nto a stationary point of the expected loss function asymptotically. Numerical\nstudy demonstrates encouraging results for the efficacy and robustness of our\nalgorithm compared to the widely used nuclear norm solvers.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 10:49:50 GMT"}, {"version": "v2", "created": "Sun, 14 Dec 2014 02:29:08 GMT"}, {"version": "v3", "created": "Thu, 7 May 2015 03:59:34 GMT"}, {"version": "v4", "created": "Sat, 14 May 2016 03:53:59 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Shen", "Jie", ""], ["Xu", "Huan", ""], ["Li", "Ping", ""]]}, {"id": "1406.3269", "submitter": "Krzysztof Geras", "authors": "Krzysztof J. Geras and Charles Sutton", "title": "Scheduled denoising autoencoders", "comments": "Published as a conference paper at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a representation learning method that learns features at multiple\ndifferent levels of scale. Working within the unsupervised framework of\ndenoising autoencoders, we observe that when the input is heavily corrupted\nduring training, the network tends to learn coarse-grained features, whereas\nwhen the input is only slightly corrupted, the network tends to learn\nfine-grained features. This motivates the scheduled denoising autoencoder,\nwhich starts with a high level of noise that lowers as training progresses. We\nfind that the resulting representation yields a significant boost on a later\nsupervised task compared to the original input, or to a standard denoising\nautoencoder trained at a single noise level. After supervised fine-tuning our\nbest model achieves the lowest ever reported error on the CIFAR-10 data set\namong permutation-invariant methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 15:40:18 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 20:42:11 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2015 21:05:32 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Geras", "Krzysztof J.", ""], ["Sutton", "Charles", ""]]}, {"id": "1406.3270", "submitter": "Matthieu  Geist", "authors": "Matthieu Geist, Olivier Pietquin", "title": "Kalman Temporal Differences", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 39, pages\n  483-532, 2010", "doi": "10.1613/jair.3077", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because reinforcement learning suffers from a lack of scalability, online\nvalue (and Q-) function approximation has received increasing interest this\nlast decade. This contribution introduces a novel approximation scheme, namely\nthe Kalman Temporal Differences (KTD) framework, that exhibits the following\nfeatures: sample-efficiency, non-linear approximation, non-stationarity\nhandling and uncertainty management. A first KTD-based algorithm is provided\nfor deterministic Markov Decision Processes (MDP) which produces biased\nestimates in the case of stochastic transitions. Than the eXtended KTD\nframework (XKTD), solving stochastic MDP, is described. Convergence is analyzed\nfor special cases for both deterministic and stochastic transitions. Related\nalgorithms are experimented on classical benchmarks. They compare favorably to\nthe state of the art while exhibiting the announced features.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:02:28 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Geist", "Matthieu", ""], ["Pietquin", "Olivier", ""]]}, {"id": "1406.3332", "submitter": "Julien Mairal", "authors": "Julien Mairal (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean\n  Kuntzmann), Piotr Koniusz (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire\n  Jean Kuntzmann), Zaid Harchaoui (INRIA Grenoble Rh\\^one-Alpes / LJK\n  Laboratoire Jean Kuntzmann), Cordelia Schmid (INRIA Grenoble Rh\\^one-Alpes /\n  LJK Laboratoire Jean Kuntzmann)", "title": "Convolutional Kernel Networks", "comments": "appears in Advances in Neural Information Processing Systems (NIPS),\n  Dec 2014, Montreal, Canada, http://nips.cc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal in visual recognition is to devise image representations\nthat are invariant to particular transformations. In this paper, we address\nthis goal with a new type of convolutional neural network (CNN) whose\ninvariance is encoded by a reproducing kernel. Unlike traditional approaches\nwhere neural networks are learned either to represent data or for solving a\nclassification task, our network learns to approximate the kernel feature map\non training data. Such an approach enjoys several benefits over classical ones.\nFirst, by teaching CNNs to be invariant, we obtain simple network architectures\nthat achieve a similar accuracy to more complex ones, while being easy to train\nand robust to overfitting. Second, we bridge a gap between the neural network\nliterature and kernels, which are natural tools to model invariance. We\nevaluate our methodology on visual recognition tasks where CNNs have proven to\nperform well, e.g., digit recognition with the MNIST dataset, and the more\nchallenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive\nwith the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 19:41:03 GMT"}, {"version": "v2", "created": "Fri, 14 Nov 2014 16:58:48 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Mairal", "Julien", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire Jean\n  Kuntzmann"], ["Koniusz", "Piotr", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire\n  Jean Kuntzmann"], ["Harchaoui", "Zaid", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK\n  Laboratoire Jean Kuntzmann"], ["Schmid", "Cordelia", "", "INRIA Grenoble Rh\u00f4ne-Alpes /\n  LJK Laboratoire Jean Kuntzmann"]]}, {"id": "1406.3407", "submitter": "Gang Chen", "authors": "Gang Chen and Sargur H. Srihari", "title": "Restricted Boltzmann Machine for Classification with Hierarchical\n  Correlated Prior", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Restricted Boltzmann machines (RBM) and its variants have become hot research\ntopics recently, and widely applied to many classification problems, such as\ncharacter recognition and document categorization. Often, classification RBM\nignores the interclass relationship or prior knowledge of sharing information\namong classes. In this paper, we are interested in RBM with the hierarchical\nprior over classes. We assume parameters for nearby nodes are correlated in the\nhierarchical tree, and further the parameters at each node of the tree be\northogonal to those at its ancestors. We propose a hierarchical correlated RBM\nfor classification problem, which generalizes the classification RBM with\nsharing information among different classes. In order to reduce the redundancy\nbetween node parameters in the hierarchy, we also introduce orthogonal\nrestrictions to our objective function. We test our method on challenge\ndatasets, and show promising results compared to competitive baselines.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 02:19:26 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2015 18:39:18 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Chen", "Gang", ""], ["Srihari", "Sargur H.", ""]]}, {"id": "1406.3474", "submitter": "Sijin Li", "authors": "Sijin Li, Zhi-Qiang Liu, Antoni B. Chan", "title": "Heterogeneous Multi-task Learning for Human Pose Estimation with Deep\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an heterogeneous multi-task learning framework for human pose\nestimation from monocular image with deep convolutional neural network. In\nparticular, we simultaneously learn a pose-joint regressor and a sliding-window\nbody-part detector in a deep network architecture. We show that including the\nbody-part detection task helps to regularize the network, directing it to\nconverge to a good solution. We report competitive and state-of-art results on\nseveral data sets. We also empirically show that the learned neurons in the\nmiddle layer of our network are tuned to localized body parts.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 10:11:18 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Li", "Sijin", ""], ["Liu", "Zhi-Qiang", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1406.3496", "submitter": "Hadi Fanaee-T", "authors": "Hadi Fanaee-T and Jo\\~ao Gama", "title": "EigenEvent: An Algorithm for Event Detection from Complex Data Streams\n  in Syndromic Surveillance", "comments": "To appear in Intelligent Data Analysis Journal, vol. 19(3), 2015", "journal-ref": "PP. 597-616, Vol. 19, No. 3, June 2015, Intelligent Data Analysis", "doi": "10.3233/IDA-150734", "report-no": null, "categories": "cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syndromic surveillance systems continuously monitor multiple pre-diagnostic\ndaily streams of indicators from different regions with the aim of early\ndetection of disease outbreaks. The main objective of these systems is to\ndetect outbreaks hours or days before the clinical and laboratory confirmation.\nThe type of data that is being generated via these systems is usually\nmultivariate and seasonal with spatial and temporal dimensions. The algorithm\nWhat's Strange About Recent Events (WSARE) is the state-of-the-art method for\nsuch problems. It exhaustively searches for contrast sets in the multivariate\ndata and signals an alarm when find statistically significant rules. This\nbottom-up approach presents a much lower detection delay comparing the existing\ntop-down approaches. However, WSARE is very sensitive to the small-scale\nchanges and subsequently comes with a relatively high rate of false alarms. We\npropose a new approach called EigenEvent that is neither fully top-down nor\nbottom-up. In this method, we instead of top-down or bottom-up search, track\nchanges in data correlation structure via eigenspace techniques. This new\nmethodology enables us to detect both overall changes (via eigenvalue) and\ndimension-level changes (via eigenvectors). Experimental results on hundred\nsets of benchmark data reveals that EigenEvent presents a better overall\nperformance comparing state-of-the-art, in particular in terms of the false\nalarm rate.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 10:38:09 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Fanaee-T", "Hadi", ""], ["Gama", "Jo\u00e3o", ""]]}, {"id": "1406.3497", "submitter": "Matteo Pirotta", "authors": "Matteo Pirotta, Simone Parisi and Marcello Restelli", "title": "Multi-objective Reinforcement Learning with Continuous Pareto Frontier\n  Approximation Supplementary Material", "comments": "AAAI-15 Supplement. Updated upon acceptance at the Twenty-Ninth AAAI\n  Conference on Artificial Intelligence (AAAI-15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document contains supplementary material for the paper \"Multi-objective\nReinforcement Learning with Continuous Pareto Frontier Approximation\",\npublished at the Twenty-Ninth AAAI Conference on Artificial Intelligence\n(AAAI-15). The paper is about learning a continuous approximation of the Pareto\nfrontier in Multi-Objective Markov Decision Problems (MOMDPs). We propose a\npolicy-based approach that exploits gradient information to generate solutions\nclose to the Pareto ones. Differently from previous policy-gradient\nmulti-objective algorithms, where n optimization routines are use to have n\nsolutions, our approach performs a single gradient-ascent run that at each step\ngenerates an improved continuous approximation of the Pareto frontier. The idea\nis to exploit a gradient-based approach to optimize the parameters of a\nfunction that defines a manifold in the policy parameter space so that the\ncorresponding image in the objective space gets as close as possible to the\nPareto frontier. Besides deriving how to compute and estimate such gradient, we\nwill also discuss the non-trivial issue of defining a metric to assess the\nquality of the candidate Pareto frontiers. Finally, the properties of the\nproposed approach are empirically evaluated on two interesting MOMDPs.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 10:49:38 GMT"}, {"version": "v2", "created": "Tue, 18 Nov 2014 21:31:32 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Pirotta", "Matteo", ""], ["Parisi", "Simone", ""], ["Restelli", "Marcello", ""]]}, {"id": "1406.3587", "submitter": "Dongpo Xu", "authors": "Dongpo Xu, Danilo P. Mandic", "title": "Quaternion Gradient and Hessian", "comments": "23 pages", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 2016,\n  27(2):249-261", "doi": "10.1109/TNNLS.2015.2440473", "report-no": null, "categories": "math.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimization of real scalar functions of quaternion variables, such as\nthe mean square error or array output power, underpins many practical\napplications. Solutions often require the calculation of the gradient and\nHessian, however, real functions of quaternion variables are essentially\nnon-analytic. To address this issue, we propose new definitions of quaternion\ngradient and Hessian, based on the novel generalized HR (GHR) calculus, thus\nmaking possible efficient derivation of optimization algorithms directly in the\nquaternion field, rather than transforming the problem to the real domain, as\nis current practice. In addition, unlike the existing quaternion gradients, the\nGHR calculus allows for the product and chain rule, and for a one-to-one\ncorrespondence of the proposed quaternion gradient and Hessian with their real\ncounterparts. Properties of the quaternion gradient and Hessian relevant to\nnumerical applications are elaborated, and the results illuminate the\nusefulness of the GHR calculus in greatly simplifying the derivation of the\nquaternion least mean squares, and in quaternion least square and Newton\nalgorithm. The proposed gradient and Hessian are also shown to enable the same\ngeneric forms as the corresponding real- and complex-valued algorithms, further\nillustrating the advantages in algorithm design and evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 16:33:55 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Xu", "Dongpo", ""], ["Mandic", "Danilo P.", ""]]}, {"id": "1406.3650", "submitter": "Stephan Mandt", "authors": "Stephan Mandt and David Blei", "title": "Smoothed Gradients for Stochastic Variational Inference", "comments": "Appears in Neural Information Processing Systems, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic variational inference (SVI) lets us scale up Bayesian computation\nto massive data. It uses stochastic optimization to fit a variational\ndistribution, following easy-to-compute noisy natural gradients. As with most\ntraditional stochastic optimization methods, SVI takes precautions to use\nunbiased stochastic gradients whose expectations are equal to the true\ngradients. In this paper, we explore the idea of following biased stochastic\ngradients in SVI. Our method replaces the natural gradient with a similarly\nconstructed vector that uses a fixed-window moving average of some of its\nprevious terms. We will demonstrate the many advantages of this technique.\nFirst, its computational cost is the same as for SVI and storage requirements\nonly multiply by a constant factor. Second, it enjoys significant variance\nreduction over the unbiased estimates, smaller bias than averaged gradients,\nand leads to smaller mean-squared error against the full gradient. We test our\nmethod on latent Dirichlet allocation with three large corpora.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 21:19:09 GMT"}, {"version": "v2", "created": "Tue, 18 Nov 2014 03:12:37 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Mandt", "Stephan", ""], ["Blei", "David", ""]]}, {"id": "1406.3692", "submitter": "Prateek Dewan", "authors": "Prateek Dewan and Anand Kashyap and Ponnurangam Kumaraguru", "title": "Analyzing Social and Stylometric Features to Identify Spear phishing\n  Emails", "comments": "Detection of spear phishing using social media features", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spear phishing is a complex targeted attack in which, an attacker harvests\ninformation about the victim prior to the attack. This information is then used\nto create sophisticated, genuine-looking attack vectors, drawing the victim to\ncompromise confidential information. What makes spear phishing different, and\nmore powerful than normal phishing, is this contextual information about the\nvictim. Online social media services can be one such source for gathering vital\ninformation about an individual. In this paper, we characterize and examine a\ntrue positive dataset of spear phishing, spam, and normal phishing emails from\nSymantec's enterprise email scanning service. We then present a model to detect\nspear phishing emails sent to employees of 14 international organizations, by\nusing social features extracted from LinkedIn. Our dataset consists of 4,742\ntargeted attack emails sent to 2,434 victims, and 9,353 non targeted attack\nemails sent to 5,912 non victims; and publicly available information from their\nLinkedIn profiles. We applied various machine learning algorithms to this\nlabeled data, and achieved an overall maximum accuracy of 97.76% in identifying\nspear phishing emails. We used a combination of social features from LinkedIn\nprofiles, and stylometric features extracted from email subjects, bodies, and\nattachments. However, we achieved a slightly better accuracy of 98.28% without\nthe social features. Our analysis revealed that social features extracted from\nLinkedIn do not help in identifying spear phishing emails. To the best of our\nknowledge, this is one of the first attempts to make use of a combination of\nstylometric features extracted from emails, and social features extracted from\nan online social network to detect targeted spear phishing emails.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jun 2014 07:01:03 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Dewan", "Prateek", ""], ["Kashyap", "Anand", ""], ["Kumaraguru", "Ponnurangam", ""]]}, {"id": "1406.3726", "submitter": "Ankur Sahai", "authors": "Ankur Sahai", "title": "Evaluation of Machine Learning Techniques for Green Energy Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate the following Machine Learning techniques for Green Energy (Wind,\nSolar) Prediction: Bayesian Inference, Neural Networks, Support Vector\nMachines, Clustering techniques (PCA). Our objective is to predict green energy\nusing weather forecasts, predict deviations from forecast green energy, find\ncorrelation amongst different weather parameters and green energy availability,\nrecover lost or missing energy (/ weather) data. We use historical weather data\nand weather forecasts for the same.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jun 2014 13:08:30 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Sahai", "Ankur", ""]]}, {"id": "1406.3781", "submitter": "Nishant Mehta", "authors": "Nishant A. Mehta and Robert C. Williamson", "title": "From Stochastic Mixability to Fast Rates", "comments": "21 pages, accepted to NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical risk minimization (ERM) is a fundamental learning rule for\nstatistical learning problems where the data is generated according to some\nunknown distribution $\\mathsf{P}$ and returns a hypothesis $f$ chosen from a\nfixed class $\\mathcal{F}$ with small loss $\\ell$. In the parametric setting,\ndepending upon $(\\ell, \\mathcal{F},\\mathsf{P})$ ERM can have slow\n$(1/\\sqrt{n})$ or fast $(1/n)$ rates of convergence of the excess risk as a\nfunction of the sample size $n$. There exist several results that give\nsufficient conditions for fast rates in terms of joint properties of $\\ell$,\n$\\mathcal{F}$, and $\\mathsf{P}$, such as the margin condition and the Bernstein\ncondition. In the non-statistical prediction with expert advice setting, there\nis an analogous slow and fast rate phenomenon, and it is entirely characterized\nin terms of the mixability of the loss $\\ell$ (there being no role there for\n$\\mathcal{F}$ or $\\mathsf{P}$). The notion of stochastic mixability builds a\nbridge between these two models of learning, reducing to classical mixability\nin a special case. The present paper presents a direct proof of fast rates for\nERM in terms of stochastic mixability of $(\\ell,\\mathcal{F}, \\mathsf{P})$, and\nin so doing provides new insight into the fast-rates phenomenon. The proof\nexploits an old result of Kemperman on the solution to the general moment\nproblem. We also show a partial converse that suggests a characterization of\nfast rates for ERM in terms of stochastic mixability is possible.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jun 2014 23:25:05 GMT"}, {"version": "v2", "created": "Sat, 22 Nov 2014 11:16:28 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Mehta", "Nishant A.", ""], ["Williamson", "Robert C.", ""]]}, {"id": "1406.3792", "submitter": "Tao Xiong", "authors": "Tao Xiong, Yukun Bao, Zhongyi Hu", "title": "Interval Forecasting of Electricity Demand: A Novel Bivariate EMD-based\n  Support Vector Regression Modeling Framework", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijepes.2014.06.010", "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly accurate interval forecasting of electricity demand is fundamental to\nthe success of reducing the risk when making power system planning and\noperational decisions by providing a range rather than point estimation. In\nthis study, a novel modeling framework integrating bivariate empirical mode\ndecomposition (BEMD) and support vector regression (SVR), extended from the\nwell-established empirical mode decomposition (EMD) based time series modeling\nframework in the energy demand forecasting literature, is proposed for interval\nforecasting of electricity demand. The novelty of this study arises from the\nemployment of BEMD, a new extension of classical empirical model decomposition\n(EMD) destined to handle bivariate time series treated as complex-valued time\nseries, as decomposition method instead of classical EMD only capable of\ndecomposing one-dimensional single-valued time series. This proposed modeling\nframework is endowed with BEMD to decompose simultaneously both the lower and\nupper bounds time series, constructed in forms of complex-valued time series,\nof electricity demand on a monthly per hour basis, resulting in capturing the\npotential interrelationship between lower and upper bounds. The proposed\nmodeling framework is justified with monthly interval-valued electricity demand\ndata per hour in Pennsylvania-New Jersey-Maryland Interconnection, indicating\nit as a promising method for interval-valued electricity demand forecasting.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 02:39:37 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Xiong", "Tao", ""], ["Bao", "Yukun", ""], ["Hu", "Zhongyi", ""]]}, {"id": "1406.3816", "submitter": "Francesco Orabona", "authors": "Francesco Orabona", "title": "Simultaneous Model Selection and Optimization through Parameter-free\n  Stochastic Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent algorithms for training linear and kernel\npredictors are gaining more and more importance, thanks to their scalability.\nWhile various methods have been proposed to speed up their convergence, the\nmodel selection phase is often ignored. In fact, in theoretical works most of\nthe time assumptions are made, for example, on the prior knowledge of the norm\nof the optimal solution, while in the practical world validation methods remain\nthe only viable approach. In this paper, we propose a new kernel-based\nstochastic gradient descent algorithm that performs model selection while\ntraining, with no parameters to tune, nor any form of cross-validation. The\nalgorithm builds on recent advancement in online learning theory for\nunconstrained settings, to estimate over time the right regularization in a\ndata-dependent way. Optimal rates of convergence are proved under standard\nsmoothness assumptions on the target function, using the range space of the\nfractional integral operator associated with the kernel.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 13:34:27 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Orabona", "Francesco", ""]]}, {"id": "1406.3830", "submitter": "Misha Denil", "authors": "Misha Denil and Alban Demiraj and Nal Kalchbrenner and Phil Blunsom\n  and Nando de Freitas", "title": "Modelling, Visualising and Summarising Documents with a Single\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing the compositional process which maps the meaning of words to that\nof documents is a central challenge for researchers in Natural Language\nProcessing and Information Retrieval. We introduce a model that is able to\nrepresent the meaning of documents by embedding them in a low dimensional\nvector space, while preserving distinctions of word and sentence order crucial\nfor capturing nuanced semantics. Our model is based on an extended Dynamic\nConvolution Neural Network, which learns convolution filters at both the\nsentence and document level, hierarchically learning to capture and compose low\nlevel lexical features into high level semantic concepts. We demonstrate the\neffectiveness of this model on a range of document modelling tasks, achieving\nstrong results with no feature engineering and with a more compact model.\nInspired by recent advances in visualising deep convolution networks for\ncomputer vision, we present a novel visualisation technique for our document\nnetworks which not only provides insight into their learning process, but also\ncan be interpreted to produce a compelling automatic summarisation system for\ntexts.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 17:15:32 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Denil", "Misha", ""], ["Demiraj", "Alban", ""], ["Kalchbrenner", "Nal", ""], ["Blunsom", "Phil", ""], ["de Freitas", "Nando", ""]]}, {"id": "1406.3837", "submitter": "Thomas Laurent", "authors": "Xavier Bresson, Huiyi Hu, Thomas Laurent, Arthur Szlam, and James von\n  Brecht", "title": "An Incremental Reseeding Strategy for Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a simple and easily parallelizable algorithm for\nmultiway graph partitioning. The algorithm alternates between three basic\ncomponents: diffusing seed vertices over the graph, thresholding the diffused\nseeds, and then randomly reseeding the thresholded clusters. We demonstrate\nexperimentally that the proper combination of these ingredients leads to an\nalgorithm that achieves state-of-the-art performance in terms of cluster purity\non standard benchmarks datasets. Moreover, the algorithm runs an order of\nmagnitude faster than the other algorithms that achieve comparable results in\nterms of accuracy. We also describe a coarsen, cluster and refine approach\nsimilar to GRACLUS and METIS that removes an additional order of magnitude from\nthe runtime of our algorithm while still maintaining competitive accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 18:30:51 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Bresson", "Xavier", ""], ["Hu", "Huiyi", ""], ["Laurent", "Thomas", ""], ["Szlam", "Arthur", ""], ["von Brecht", "James", ""]]}, {"id": "1406.3840", "submitter": "Tor Lattimore", "authors": "Tor Lattimore and Koby Crammer and Csaba Szepesv\\'ari", "title": "Optimal Resource Allocation with Semi-Bandit Feedback", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a sequential resource allocation problem involving a fixed number of\nrecurring jobs. At each time-step the manager should distribute available\nresources among the jobs in order to maximise the expected number of completed\njobs. Allocating more resources to a given job increases the probability that\nit completes, but with a cut-off. Specifically, we assume a linear model where\nthe probability increases linearly until it equals one, after which allocating\nadditional resources is wasteful. We assume the difficulty of each job is\nunknown and present the first algorithm for this problem and prove upper and\nlower bounds on its regret. Despite its apparent simplicity, the problem has a\nrich structure: we show that an appropriate optimistic algorithm can improve\nits learning speed dramatically beyond the results one normally expects for\nsimilar problems as the problem becomes resource-laden.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 18:41:47 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Lattimore", "Tor", ""], ["Crammer", "Koby", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1406.3843", "submitter": "Yichuan Zhang", "authors": "Yichuan Zhang, Charles Sutton", "title": "Semi-Separable Hamiltonian Monte Carlo for Inference in Bayesian\n  Hierarchical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling from hierarchical Bayesian models is often difficult for MCMC\nmethods, because of the strong correlations between the model parameters and\nthe hyperparameters. Recent Riemannian manifold Hamiltonian Monte Carlo (RMHMC)\nmethods have significant potential advantages in this setting, but are\ncomputationally expensive. We introduce a new RMHMC method, which we call\nsemi-separable Hamiltonian Monte Carlo, which uses a specially designed mass\nmatrix that allows the joint Hamiltonian over model parameters and\nhyperparameters to decompose into two simpler Hamiltonians. This structure is\nexploited by a new integrator which we call the alternating blockwise leapfrog\nalgorithm. The resulting method can mix faster than simpler Gibbs sampling\nwhile being simpler and more efficient than previous instances of RMHMC.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 19:03:46 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Zhang", "Yichuan", ""], ["Sutton", "Charles", ""]]}, {"id": "1406.3852", "submitter": "Matthew Blaschko", "authors": "Wacha Bounliphone, Arthur Gretton, Arthur Tenenhaus (E3S), Matthew\n  Blaschko (INRIA Saclay - Ile de France, CVN)", "title": "A low variance consistent test of relative dependency", "comments": "International Conference on Machine Learning, Jul 2015, Lille, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel non-parametric statistical hypothesis test of relative\ndependence between a source variable and two candidate target variables. Such a\ntest enables us to determine whether one source variable is significantly more\ndependent on a first target variable or a second. Dependence is measured via\nthe Hilbert-Schmidt Independence Criterion (HSIC), resulting in a pair of\nempirical dependence measures (source-target 1, source-target 2). We test\nwhether the first dependence measure is significantly larger than the second.\nModeling the covariance between these HSIC statistics leads to a provably more\npowerful test than the construction of independent HSIC statistics by\nsub-sampling. The resulting test is consistent and unbiased, and (being based\non U-statistics) has favorable convergence properties. The test can be computed\nin quadratic time, matching the computational complexity of standard empirical\nHSIC estimators. The effectiveness of the test is demonstrated on several\nreal-world problems: we identify language groups from a multilingual corpus,\nand we prove that tumor location is more dependent on gene expression than\nchromosomal imbalances. Source code is available for download at\nhttps://github.com/wbounliphone/reldep.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 19:23:11 GMT"}, {"version": "v2", "created": "Fri, 11 Jul 2014 17:12:58 GMT"}, {"version": "v3", "created": "Wed, 27 May 2015 08:25:19 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Bounliphone", "Wacha", "", "E3S"], ["Gretton", "Arthur", "", "E3S"], ["Tenenhaus", "Arthur", "", "E3S"], ["Blaschko", "Matthew", "", "INRIA Saclay - Ile de France, CVN"]]}, {"id": "1406.3884", "submitter": "Georgios Evangelopoulos", "authors": "Georgios Evangelopoulos, Stephen Voinea, Chiyuan Zhang, Lorenzo\n  Rosasco, Tomaso Poggio", "title": "Learning An Invariant Speech Representation", "comments": "CBMM Memo No. 022, 5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": "CBMM Memo No. 022", "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of speech, and in particular the ability to generalize and learn\nfrom small sets of labelled examples like humans do, depends on an appropriate\nrepresentation of the acoustic input. We formulate the problem of finding\nrobust speech features for supervised learning with small sample complexity as\na problem of learning representations of the signal that are maximally\ninvariant to intraclass transformations and deformations. We propose an\nextension of a theory for unsupervised learning of invariant visual\nrepresentations to the auditory domain and empirically evaluate its validity\nfor voiced speech sound classification. Our version of the theory requires the\nmemory-based, unsupervised storage of acoustic templates -- such as specific\nphones or words -- together with all the transformations of each that normally\noccur. A quasi-invariant representation for a speech segment can be obtained by\nprojecting it to each template orbit, i.e., the set of transformed signals, and\ncomputing the associated one-dimensional empirical probability distributions.\nThe computations can be performed by modules of filtering and pooling, and\nextended to hierarchical architectures. In this paper, we apply a single-layer,\nmulticomponent representation for phonemes and demonstrate improved accuracy\nand decreased sample complexity for vowel classification compared to standard\nspectral, cepstral and perceptual features.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 02:03:29 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Evangelopoulos", "Georgios", ""], ["Voinea", "Stephen", ""], ["Zhang", "Chiyuan", ""], ["Rosasco", "Lorenzo", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1406.3895", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Weiran Wang and Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "The Laplacian K-modes algorithm for clustering", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition to finding meaningful clusters, centroid-based clustering\nalgorithms such as K-means or mean-shift should ideally find centroids that are\nvalid patterns in the input space, representative of data in their cluster.\nThis is challenging with data having a nonconvex or manifold structure, as with\nimages or text. We introduce a new algorithm, Laplacian K-modes, which\nnaturally combines three powerful ideas in clustering: the explicit use of\nassignment variables (as in K-means); the estimation of cluster centroids which\nare modes of each cluster's density estimate (as in mean-shift); and the\nregularizing effect of the graph Laplacian, which encourages similar\nassignments for nearby points (as in spectral clustering). The optimization\nalgorithm alternates an assignment step, which is a convex quadratic program,\nand a mean-shift step, which separates for each cluster centroid. The algorithm\nfinds meaningful density estimates for each cluster, even with challenging\nproblems where the clusters have manifold structure, are highly nonconvex or in\nhigh dimension. It also provides centroids that are valid patterns, truly\nrepresentative of their cluster (unlike K-means), and an out-of-sample mapping\nthat predicts soft assignments for a new point.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 03:29:48 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Wang", "Weiran", ""], ["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1406.3896", "submitter": "Jasper Snoek", "authors": "Kevin Swersky and Jasper Snoek and Ryan Prescott Adams", "title": "Freeze-Thaw Bayesian Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a dynamic form of Bayesian optimization for machine\nlearning models with the goal of rapidly finding good hyperparameter settings.\nOur method uses the partial information gained during the training of a machine\nlearning model in order to decide whether to pause training and start a new\nmodel, or resume the training of a previously-considered model. We specifically\ntailor our method to machine learning problems by developing a novel\npositive-definite covariance kernel to capture a variety of training curves.\nFurthermore, we develop a Gaussian process prior that scales gracefully with\nadditional temporal observations. Finally, we provide an information-theoretic\nframework to automate the decision process. Experiments on several common\nmachine learning models show that our approach is extremely effective in\npractice.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 03:43:20 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Swersky", "Kevin", ""], ["Snoek", "Jasper", ""], ["Adams", "Ryan Prescott", ""]]}, {"id": "1406.3922", "submitter": "Amr Gerard", "authors": "Yousuf M. Soliman", "title": "Personalized Medical Treatments Using Novel Reinforcement Learning\n  Algorithms", "comments": "This paper has been withdrawn by the author. Some of the work was\n  taken from the work of Dr. Yair Goldberg and Dr. Michael R. Kosorok and they\n  have requested for the paper to be withdrawn. arXiv admin note: v1 had\n  substantial text overlap with arXiv:1202.5130, arXiv:1205.6659; and text\n  overlap with arXiv:1301.2158 by other authors without attribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In both the fields of computer science and medicine there is very strong\ninterest in developing personalized treatment policies for patients who have\nvariable responses to treatments. In particular, I aim to find an optimal\npersonalized treatment policy which is a non-deterministic function of the\npatient specific covariate data that maximizes the expected survival time or\nclinical outcome. I developed an algorithmic framework to solve multistage\ndecision problem with a varying number of stages that are subject to censoring\nin which the \"rewards\" are expected survival times. In specific, I developed a\nnovel Q-learning algorithm that dynamically adjusts for these parameters.\nFurthermore, I found finite upper bounds on the generalized error of the\ntreatment paths constructed by this algorithm. I have also shown that when the\noptimal Q-function is an element of the approximation space, the anticipated\nsurvival times for the treatment regime constructed by the algorithm will\nconverge to the optimal treatment path. I demonstrated the performance of the\nproposed algorithmic framework via simulation studies and through the analysis\nof chronic depression data and a hypothetical clinical trial. The censored\nQ-learning algorithm I developed is more effective than the state of the art\nclinical decision support systems and is able to operate in environments when\nmany covariate parameters may be unobtainable or censored.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 07:14:26 GMT"}, {"version": "v2", "created": "Mon, 30 Jun 2014 08:29:19 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Soliman", "Yousuf M.", ""]]}, {"id": "1406.3926", "submitter": "Yasin Abbasi-Yadkori", "authors": "Yasin Abbasi-Yadkori and Csaba Szepesvari", "title": "Bayesian Optimal Control of Smoothly Parameterized Systems: The Lazy\n  Posterior Sampling Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Bayesian optimal control of a general class of smoothly\nparameterized Markov decision problems. Since computing the optimal control is\ncomputationally expensive, we design an algorithm that trades off performance\nfor computational efficiency. The algorithm is a lazy posterior sampling method\nthat maintains a distribution over the unknown parameter. The algorithm changes\nits policy only when the variance of the distribution is reduced sufficiently.\nImportantly, we analyze the algorithm and show the precise nature of the\nperformance vs. computation tradeoff. Finally, we show the effectiveness of the\nmethod on a web server control application.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 08:04:42 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Abbasi-Yadkori", "Yasin", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1406.4112", "submitter": "Zhenyong Fu", "authors": "Zhen-Yong Fu, Tao Xiang, Shaogang Gong", "title": "Semantic Graph for Zero-Shot Learning", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Zero-shot learning aims to classify visual objects without any training data\nvia knowledge transfer between seen and unseen classes. This is typically\nachieved by exploring a semantic embedding space where the seen and unseen\nclasses can be related. Previous works differ in what embedding space is used\nand how different classes and a test image can be related. In this paper, we\nutilize the annotation-free semantic word space for the former and focus on\nsolving the latter issue of modeling relatedness. Specifically, in contrast to\nprevious work which ignores the semantic relationships between seen classes and\nfocus merely on those between seen and unseen classes, in this paper a novel\napproach based on a semantic graph is proposed to represent the relationships\nbetween all the seen and unseen class in a semantic word space. Based on this\nsemantic graph, we design a special absorbing Markov chain process, in which\neach unseen class is viewed as an absorbing state. After incorporating one test\nimage into the semantic graph, the absorbing probabilities from the test data\nto each unseen class can be effectively computed; and zero-shot classification\ncan be achieved by finding the class label with the highest absorbing\nprobability. The proposed model has a closed-form solution which is linear with\nrespect to the number of test images. We demonstrate the effectiveness and\ncomputational efficiency of the proposed method over the state-of-the-arts on\nthe AwA (animals with attributes) dataset.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 19:40:52 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 09:53:18 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Fu", "Zhen-Yong", ""], ["Xiang", "Tao", ""], ["Gong", "Shaogang", ""]]}, {"id": "1406.4203", "submitter": "Ryan Babbush", "authors": "Ryan Babbush, Vasil Denchev, Nan Ding, Sergei Isakov and Hartmut Neven", "title": "Construction of non-convex polynomial loss functions for training a\n  binary classifier with quantum annealing", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum annealing is a heuristic quantum algorithm which exploits quantum\nresources to minimize an objective function embedded as the energy levels of a\nprogrammable physical system. To take advantage of a potential quantum\nadvantage, one needs to be able to map the problem of interest to the native\nhardware with reasonably low overhead. Because experimental considerations\nconstrain our objective function to take the form of a low degree PUBO\n(polynomial unconstrained binary optimization), we employ non-convex loss\nfunctions which are polynomial functions of the margin. We show that these loss\nfunctions are robust to label noise and provide a clear advantage over convex\nmethods. These loss functions may also be useful for classical approaches as\nthey compile to regularized risk expressions which can be evaluated in constant\ntime with respect to the number of training examples.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 00:53:59 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Babbush", "Ryan", ""], ["Denchev", "Vasil", ""], ["Ding", "Nan", ""], ["Isakov", "Sergei", ""], ["Neven", "Hartmut", ""]]}, {"id": "1406.4296", "submitter": "Adrien Gaidon", "authors": "Adrien Gaidon (Xerox Research Center Europe, France), Gloria Zen\n  (University of Trento, Italy), Jose A. Rodriguez-Serrano (Xerox Research\n  Center Europe, France)", "title": "Self-Learning Camera: Autonomous Adaptation of Object Detectors to\n  Unlabeled Video Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning object detectors requires massive amounts of labeled training\nsamples from the specific data source of interest. This is impractical when\ndealing with many different sources (e.g., in camera networks), or constantly\nchanging ones such as mobile cameras (e.g., in robotics or driving assistant\nsystems). In this paper, we address the problem of self-learning detectors in\nan autonomous manner, i.e. (i) detectors continuously updating themselves to\nefficiently adapt to streaming data sources (contrary to transductive\nalgorithms), (ii) without any labeled data strongly related to the target data\nstream (contrary to self-paced learning), and (iii) without manual intervention\nto set and update hyper-parameters. To that end, we propose an unsupervised,\non-line, and self-tuning learning algorithm to optimize a multi-task learning\nconvex objective. Our method uses confident but laconic oracles (high-precision\nbut low-recall off-the-shelf generic detectors), and exploits the structure of\nthe problem to jointly learn on-line an ensemble of instance-level trackers,\nfrom which we derive an adapted category-level object detector. Our approach is\nvalidated on real-world publicly available video object datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 09:51:18 GMT"}, {"version": "v2", "created": "Wed, 18 Jun 2014 12:33:22 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Gaidon", "Adrien", "", "Xerox Research Center Europe, France"], ["Zen", "Gloria", "", "University of Trento, Italy"], ["Rodriguez-Serrano", "Jose A.", "", "Xerox Research\n  Center Europe, France"]]}, {"id": "1406.4363", "submitter": "Shin Matsushima", "authors": "Shin Matsushima, Hyokun Yun, Xinhua Zhang, S.V.N. Vishwanathan", "title": "Distributed Stochastic Optimization of the Regularized Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning algorithms minimize a regularized risk, and stochastic\noptimization is widely used for this task. When working with massive data, it\nis desirable to perform stochastic optimization in parallel. Unfortunately,\nmany existing stochastic optimization algorithms cannot be parallelized\nefficiently. In this paper we show that one can rewrite the regularized risk\nminimization problem as an equivalent saddle-point problem, and propose an\nefficient distributed stochastic optimization (DSO) algorithm. We prove the\nalgorithm's rate of convergence; remarkably, our analysis shows that the\nalgorithm scales almost linearly with the number of processors. We also verify\nwith empirical evaluations that the proposed algorithm is competitive with\nother parallel, general purpose stochastic and batch optimization algorithms\nfor regularized risk minimization.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 13:38:49 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 09:15:47 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Matsushima", "Shin", ""], ["Yun", "Hyokun", ""], ["Zhang", "Xinhua", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1406.4444", "submitter": "Ziming Zhang", "authors": "Ziming Zhang and Venkatesh Saligrama", "title": "PRISM: Person Re-Identification via Structured Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-id), an emerging problem in visual surveillance,\ndeals with maintaining entities of individuals whilst they traverse various\nlocations surveilled by a camera network. From a visual perspective re-id is\nchallenging due to significant changes in visual appearance of individuals in\ncameras with different pose, illumination and calibration. Globally the\nchallenge arises from the need to maintain structurally consistent matches\namong all the individual entities across different camera views. We propose\nPRISM, a structured matching method to jointly account for these challenges. We\nview the global problem as a weighted graph matching problem and estimate edge\nweights by learning to predict them based on the co-occurrences of visual\npatterns in the training examples. These co-occurrence based scores in turn\naccount for appearance changes by inferring likely and unlikely visual\nco-occurrences appearing in training instances. We implement PRISM on single\nshot and multi-shot scenarios. PRISM uniformly outperforms state-of-the-art in\nterms of matching rate while being computationally efficient.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 20:07:27 GMT"}, {"version": "v2", "created": "Wed, 18 Jun 2014 10:02:26 GMT"}, {"version": "v3", "created": "Tue, 22 Jul 2014 15:04:40 GMT"}, {"version": "v4", "created": "Fri, 8 May 2015 01:55:13 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Zhang", "Ziming", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1406.4445", "submitter": "Venkatesh Saligrama", "authors": "Ziming Zhang and Venkatesh Saligrama", "title": "RAPID: Rapidly Accelerated Proximal Gradient Algorithms for Convex\n  Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new algorithm to speed-up the convergence of\naccelerated proximal gradient (APG) methods. In order to minimize a convex\nfunction $f(\\mathbf{x})$, our algorithm introduces a simple line search step\nafter each proximal gradient step in APG so that a biconvex function\n$f(\\theta\\mathbf{x})$ is minimized over scalar variable $\\theta>0$ while fixing\nvariable $\\mathbf{x}$. We propose two new ways of constructing the auxiliary\nvariables in APG based on the intermediate solutions of the proximal gradient\nand the line search steps. We prove that at arbitrary iteration step $t\n(t\\geq1)$, our algorithm can achieve a smaller upper-bound for the gap between\nthe current and optimal objective values than those in the traditional APG\nmethods such as FISTA, making it converge faster in practice. In fact, our\nalgorithm can be potentially applied to many important convex optimization\nproblems, such as sparse linear regression and kernel SVMs. Our experimental\nresults clearly demonstrate that our algorithm converges faster than APG in all\nof the applications above, even comparable to some sophisticated solvers.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 20:08:58 GMT"}, {"version": "v2", "created": "Wed, 18 Jun 2014 10:05:43 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Zhang", "Ziming", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1406.4465", "submitter": "Yilun Wang", "authors": "Yaru Fan and Yilun Wang", "title": "Multi-stage Multi-task feature learning via adaptive threshold", "comments": "13 pages,12 figures. arXiv admin note: text overlap with\n  arXiv:1210.5806 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task feature learning aims to identity the shared features among tasks\nto improve generalization. It has been shown that by minimizing non-convex\nlearning models, a better solution than the convex alternatives can be\nobtained. Therefore, a non-convex model based on the capped-$\\ell_{1},\\ell_{1}$\nregularization was proposed in \\cite{Gong2013}, and a corresponding efficient\nmulti-stage multi-task feature learning algorithm (MSMTFL) was presented.\nHowever, this algorithm harnesses a prescribed fixed threshold in the\ndefinition of the capped-$\\ell_{1},\\ell_{1}$ regularization and the lack of\nadaptivity might result in suboptimal performance. In this paper we propose to\nemploy an adaptive threshold in the capped-$\\ell_{1},\\ell_{1}$ regularized\nformulation, where the corresponding variant of MSMTFL will incorporate an\nadditional component to adaptively determine the threshold value. This variant\nis expected to achieve a better feature selection performance over the original\nMSMTFL algorithm. In particular, the embedded adaptive threshold component\ncomes from our previously proposed iterative support detection (ISD) method\n\\cite{Wang2010}. Empirical studies on both synthetic and real-world data sets\ndemonstrate the effectiveness of this new variant over the original MSMTFL.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 12:47:37 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 19:47:37 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Fan", "Yaru", ""], ["Wang", "Yilun", ""]]}, {"id": "1406.4469", "submitter": "Santiago Segarra", "authors": "Santiago Segarra, Mark Eisen, Alejandro Ribeiro", "title": "Authorship Attribution through Function Word Adjacency Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2451111", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for authorship attribution based on function word adjacency networks\n(WANs) is introduced. Function words are parts of speech that express\ngrammatical relationships between other words but do not carry lexical meaning\non their own. In the WANs in this paper, nodes are function words and directed\nedges stand in for the likelihood of finding the sink word in the ordered\nvicinity of the source word. WANs of different authors can be interpreted as\ntransition probabilities of a Markov chain and are therefore compared in terms\nof their relative entropies. Optimal selection of WAN parameters is studied and\nattribution accuracy is benchmarked across a diverse pool of authors and\nvarying text lengths. This analysis shows that, since function words are\nindependent of content, their use tends to be specific to an author and that\nthe relational data captured by function WANs is a good summary of stylometric\nfingerprints. Attribution accuracy is observed to exceed the one achieved by\nmethods that rely on word frequencies alone. Further combining WANs with\nmethods that rely on word frequencies alone, results in larger attribution\naccuracy, indicating that both sources of information encode different aspects\nof authorial styles.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 18:32:18 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Segarra", "Santiago", ""], ["Eisen", "Mark", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1406.4472", "submitter": "Giorgio Valentini", "authors": "Giorgio Valentini", "title": "Notes on hierarchical ensemble methods for DAG-structured taxonomies", "comments": "12 pages, 3 figures. Typos corrected. Modified title and abstract.\n  Added references and some changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several real problems ranging from text classification to computational\nbiology are characterized by hierarchical multi-label classification tasks.\nMost of the methods presented in literature focused on tree-structured\ntaxonomies, but only few on taxonomies structured according to a Directed\nAcyclic Graph (DAG). In this contribution novel classification ensemble\nalgorithms for DAG-structured taxonomies are introduced. In particular\nHierarchical Top-Down (HTD-DAG) and True Path Rule (TPR-DAG) for DAGs are\npresented and discussed.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 18:41:19 GMT"}, {"version": "v2", "created": "Wed, 18 Jun 2014 10:38:35 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Valentini", "Giorgio", ""]]}, {"id": "1406.4566", "submitter": "Furong Huang", "authors": "Furong Huang, Niranjan U.N., Ioakeim Perros, Robert Chen, Jimeng Sun,\n  Anima Anandkumar", "title": "Guaranteed Scalable Learning of Latent Tree Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an integrated approach for structure and parameter estimation in\nlatent tree graphical models. Our overall approach follows a\n\"divide-and-conquer\" strategy that learns models over small groups of variables\nand iteratively merges onto a global solution. The structure learning involves\ncombinatorial operations such as minimum spanning tree construction and local\nrecursive grouping; the parameter learning is based on the method of moments\nand on tensor decompositions. Our method is guaranteed to correctly recover the\nunknown tree structure and the model parameters with low sample complexity for\nthe class of linear multivariate latent tree models which includes discrete and\nGaussian distributions, and Gaussian mixtures. Our bulk asynchronous parallel\nalgorithm is implemented in parallel and the parallel computation complexity\nincreases only logarithmically with the number of variables and linearly with\ndimensionality of each variable.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 01:17:27 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 00:22:05 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2015 01:07:13 GMT"}, {"version": "v4", "created": "Tue, 17 Dec 2019 19:49:48 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Huang", "Furong", ""], ["N.", "Niranjan U.", ""], ["Perros", "Ioakeim", ""], ["Chen", "Robert", ""], ["Sun", "Jimeng", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1406.4580", "submitter": "Seunghak Lee", "authors": "Seunghak Lee, Jin Kyu Kim, Xun Zheng, Qirong Ho, Garth A. Gibson, Eric\n  P. Xing", "title": "Primitives for Dynamic Big Model Parallelism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When training large machine learning models with many variables or\nparameters, a single machine is often inadequate since the model may be too\nlarge to fit in memory, while training can take a long time even with\nstochastic updates. A natural recourse is to turn to distributed cluster\ncomputing, in order to harness additional memory and processors. However,\nnaive, unstructured parallelization of ML algorithms can make inefficient use\nof distributed memory, while failing to obtain proportional convergence\nspeedups - or can even result in divergence. We develop a framework of\nprimitives for dynamic model-parallelism, STRADS, in order to explore\npartitioning and update scheduling of model variables in distributed ML\nalgorithms - thus improving their memory efficiency while presenting new\nopportunities to speed up convergence without compromising inference\ncorrectness. We demonstrate the efficacy of model-parallel algorithms\nimplemented in STRADS versus popular implementations for Topic Modeling, Matrix\nFactorization and Lasso.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 03:06:52 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Lee", "Seunghak", ""], ["Kim", "Jin Kyu", ""], ["Zheng", "Xun", ""], ["Ho", "Qirong", ""], ["Gibson", "Garth A.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1406.4619", "submitter": "Alexandre Chotard", "authors": "Alexandre Chotard (INRIA Saclay - Ile de France, LRI), Martin Holena", "title": "A Generalized Markov-Chain Modelling Approach to $(1,\\lambda)$-ES Linear\n  Optimization: Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent publications investigated Markov-chain modelling of linear\noptimization by a $(1,\\lambda)$-ES, considering both unconstrained and linearly\nconstrained optimization, and both constant and varying step size. All of them\nassume normality of the involved random steps, and while this is consistent\nwith a black-box scenario, information on the function to be optimized (e.g.\nseparability) may be exploited by the use of another distribution. The\nobjective of our contribution is to complement previous studies realized with\nnormal steps, and to give sufficient conditions on the distribution of the\nrandom steps for the success of a constant step-size $(1,\\lambda)$-ES on the\nsimple problem of a linear function with a linear constraint. The decomposition\nof a multidimensional distribution into its marginals and the copula combining\nthem is applied to the new distributional assumptions, particular attention\nbeing paid to distributions with Archimedean copulas.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 06:58:38 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Chotard", "Alexandre", "", "INRIA Saclay - Ile de France, LRI"], ["Holena", "Martin", ""]]}, {"id": "1406.4625", "submitter": "Bobak Shahriari", "authors": "Bobak Shahriari and Ziyu Wang and Matthew W. Hoffman and Alexandre\n  Bouchard-C\\^ot\\'e and Nando de Freitas", "title": "An Entropy Search Portfolio for Bayesian Optimization", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization is a sample-efficient method for black-box global\noptimization. How- ever, the performance of a Bayesian optimization method very\nmuch depends on its exploration strategy, i.e. the choice of acquisition\nfunction, and it is not clear a priori which choice will result in superior\nperformance. While portfolio methods provide an effective, principled way of\ncombining a collection of acquisition functions, they are often based on\nmeasures of past performance which can be misleading. To address this issue, we\nintroduce the Entropy Search Portfolio (ESP): a novel approach to portfolio\nconstruction which is motivated by information theoretic considerations. We\nshow that ESP outperforms existing portfolio methods on several real and\nsynthetic problems, including geostatistical datasets and simulated control\ntasks. We not only show that ESP is able to offer performance as good as the\nbest, but unknown, acquisition function, but surprisingly it often gives better\nperformance. Finally, over a wide range of conditions we find that ESP is\nrobust to the inclusion of poor acquisition functions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 07:26:08 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 23:58:14 GMT"}, {"version": "v3", "created": "Thu, 30 Oct 2014 15:54:56 GMT"}, {"version": "v4", "created": "Wed, 4 Mar 2015 21:25:31 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Shahriari", "Bobak", ""], ["Wang", "Ziyu", ""], ["Hoffman", "Matthew W.", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["de Freitas", "Nando", ""]]}, {"id": "1406.4631", "submitter": "Han Zhao", "authors": "Han Zhao, Pascal Poupart", "title": "A Sober Look at Spectral Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral learning recently generated lots of excitement in machine learning,\nlargely because it is the first known method to produce consistent estimates\n(under suitable conditions) for several latent variable models. In contrast,\nmaximum likelihood estimates may get trapped in local optima due to the\nnon-convex nature of the likelihood function of latent variable models. In this\npaper, we do an empirical evaluation of spectral learning (SL) and expectation\nmaximization (EM), which reveals an important gap between the theory and the\npractice. First, SL often leads to negative probabilities. Second, EM often\nyields better estimates than spectral learning and it does not seem to get\nstuck in local optima. We discuss how the rank of the model parameters and the\namount of training data can yield negative probabilities. We also question the\ncommon belief that maximum likelihood estimators are necessarily inconsistent.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 08:25:03 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Zhao", "Han", ""], ["Poupart", "Pascal", ""]]}, {"id": "1406.4682", "submitter": "Xu Sun", "authors": "Xu Sun", "title": "Exact Decoding on Latent Variable Conditional Models is NP-Hard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable conditional models, including the latent conditional random\nfields as a special case, are popular models for many natural language\nprocessing and vision processing tasks. The computational complexity of the\nexact decoding/inference in latent conditional random fields is unclear. In\nthis paper, we try to clarify the computational complexity of the exact\ndecoding. We analyze the complexity and demonstrate that it is an NP-hard\nproblem even on a sequential labeling setting. Furthermore, we propose the\nlatent-dynamic inference (LDI-Naive) method and its bounded version\n(LDI-Bounded), which are able to perform exact-inference or\nalmost-exact-inference by using top-$n$ search and dynamic programming.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 11:17:58 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Sun", "Xu", ""]]}, {"id": "1406.4757", "submitter": "Anthony Bagnall Dr", "authors": "Anthony Bagnall and Jason Lines", "title": "An Experimental Evaluation of Nearest Neighbour Time Series\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": "CMP-C14-01", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining research into time series classification (TSC) has focussed on\nalternative distance measures for nearest neighbour classifiers. It is standard\npractice to use 1-NN with Euclidean or dynamic time warping (DTW) distance as a\nstraw man for comparison. As part of a wider investigation into elastic\ndistance measures for TSC~\\cite{lines14elastic}, we perform a series of\nexperiments to test whether this standard practice is valid.\n  Specifically, we compare 1-NN classifiers with Euclidean and DTW distance to\nstandard classifiers, examine whether the performance of 1-NN Euclidean\napproaches that of 1-NN DTW as the number of cases increases, assess whether\nthere is any benefit of setting $k$ for $k$-NN through cross validation whether\nit is worth setting the warping path for DTW through cross validation and\nfinally is it better to use a window or weighting for DTW. Based on experiments\non 77 problems, we conclude that 1-NN with Euclidean distance is fairly easy to\nbeat but 1-NN with DTW is not, if window size is set through cross validation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 15:09:21 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Bagnall", "Anthony", ""], ["Lines", "Jason", ""]]}, {"id": "1406.4781", "submitter": "Anthony Bagnall Dr", "authors": "Anthony Bagnall and Luke Davis", "title": "Predictive Modelling of Bone Age through Classification and Regression\n  of Bone Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": "CMPC14-02", "categories": "cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bone age assessment is a task performed daily in hospitals worldwide. This\ninvolves a clinician estimating the age of a patient from a radiograph of the\nnon-dominant hand.\n  Our approach to automated bone age assessment is to modularise the algorithm\ninto the following three stages: segment and verify hand outline; segment and\nverify bones; use the bone outlines to construct models of age. In this paper\nwe address the final question: given outlines of bones, can we learn how to\npredict the bone age of the patient? We examine two alternative approaches.\nFirstly, we attempt to train classifiers on individual bones to predict the\nbone stage categories commonly used in bone ageing. Secondly, we construct\nregression models to directly predict patient age.\n  We demonstrate that models built on summary features of the bone outline\nperform better than those built using the one dimensional representation of the\noutline, and also do at least as well as other automated systems. We show that\nmodels constructed on just three bones are as accurate at predicting age as\nexpert human assessors using the standard technique. We also demonstrate the\nutility of the model by quantifying the importance of ethnicity and sex on age\ndevelopment. Our conclusion is that the feature based system of separating the\nimage processing from the age modelling is the best approach for automated bone\nageing, since it offers flexibility and transparency and produces accurate\nestimates.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 16:13:10 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Bagnall", "Anthony", ""], ["Davis", "Luke", ""]]}, {"id": "1406.4784", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "Improved Densification of One Permutation Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing work on densification of one permutation hashing reduces the\nquery processing cost of the $(K,L)$-parameterized Locality Sensitive Hashing\n(LSH) algorithm with minwise hashing, from $O(dKL)$ to merely $O(d + KL)$,\nwhere $d$ is the number of nonzeros of the data vector, $K$ is the number of\nhashes in each hash table, and $L$ is the number of hash tables. While that is\na substantial improvement, our analysis reveals that the existing densification\nscheme is sub-optimal. In particular, there is no enough randomness in that\nprocedure, which affects its accuracy on very sparse datasets.\n  In this paper, we provide a new densification procedure which is provably\nbetter than the existing scheme. This improvement is more significant for very\nsparse datasets which are common over the web. The improved technique has the\nsame cost of $O(d + KL)$ for query processing, thereby making it strictly\npreferable over the existing procedure. Experimental evaluations on public\ndatasets, in the task of hashing based near neighbor search, support our\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 16:16:22 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1406.4802", "submitter": "Charles Soussen", "authors": "Charles Soussen, J\\'er\\^ome Idier, Junbo Duan, David Brie", "title": "Homotopy based algorithms for $\\ell_0$-regularized least-squares", "comments": "38 pages", "journal-ref": "IEEE Transactions on Signal Processing, vol. 63, no. 13, Jul.\n  2015, pp. 3301-3316", "doi": "10.1109/TSP.2015.2421476", "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse signal restoration is usually formulated as the minimization of a\nquadratic cost function $\\|y-Ax\\|_2^2$, where A is a dictionary and x is an\nunknown sparse vector. It is well-known that imposing an $\\ell_0$ constraint\nleads to an NP-hard minimization problem. The convex relaxation approach has\nreceived considerable attention, where the $\\ell_0$-norm is replaced by the\n$\\ell_1$-norm. Among the many efficient $\\ell_1$ solvers, the homotopy\nalgorithm minimizes $\\|y-Ax\\|_2^2+\\lambda\\|x\\|_1$ with respect to x for a\ncontinuum of $\\lambda$'s. It is inspired by the piecewise regularity of the\n$\\ell_1$-regularization path, also referred to as the homotopy path. In this\npaper, we address the minimization problem $\\|y-Ax\\|_2^2+\\lambda\\|x\\|_0$ for a\ncontinuum of $\\lambda$'s and propose two heuristic search algorithms for\n$\\ell_0$-homotopy. Continuation Single Best Replacement is a forward-backward\ngreedy strategy extending the Single Best Replacement algorithm, previously\nproposed for $\\ell_0$-minimization at a given $\\lambda$. The adaptive search of\nthe $\\lambda$-values is inspired by $\\ell_1$-homotopy. $\\ell_0$ Regularization\nPath Descent is a more complex algorithm exploiting the structural properties\nof the $\\ell_0$-regularization path, which is piecewise constant with respect\nto $\\lambda$. Both algorithms are empirically evaluated for difficult inverse\nproblems involving ill-conditioned dictionaries. Finally, we show that they can\nbe easily coupled with usual methods of model order selection.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 22:26:17 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 16:37:16 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Soussen", "Charles", ""], ["Idier", "J\u00e9r\u00f4me", ""], ["Duan", "Junbo", ""], ["Brie", "David", ""]]}, {"id": "1406.4877", "submitter": "David Martins de Matos", "authors": "Francisco Raposo, Ricardo Ribeiro, David Martins de Matos", "title": "On the Application of Generic Summarization Algorithms to Music", "comments": "12 pages, 1 table; Submitted to IEEE Signal Processing Letters", "journal-ref": "IEEE Signal Processing Letters, IEEE, vol. 22, n. 1, January 2015", "doi": "10.1109/LSP.2014.2347582", "report-no": null, "categories": "cs.IR cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several generic summarization algorithms were developed in the past and\nsuccessfully applied in fields such as text and speech summarization. In this\npaper, we review and apply these algorithms to music. To evaluate this\nsummarization's performance, we adopt an extrinsic approach: we compare a Fado\nGenre Classifier's performance using truncated contiguous clips against the\nsummaries extracted with those algorithms on 2 different datasets. We show that\nMaximal Marginal Relevance (MMR), LexRank and Latent Semantic Analysis (LSA)\nall improve classification performance in both datasets used for testing.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 20:10:22 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Raposo", "Francisco", ""], ["Ribeiro", "Ricardo", ""], ["de Matos", "David Martins", ""]]}, {"id": "1406.4905", "submitter": "Roger Frigola", "authors": "Roger Frigola and Yutian Chen and Carl E. Rasmussen", "title": "Variational Gaussian Process State-Space Models", "comments": null, "journal-ref": "R. Frigola, Y. Chen and C. E. Rasmussen. Variational Gaussian\n  Process State-Space Models, in Advances in Neural Information Processing\n  Systems (NIPS), 2014", "doi": null, "report-no": null, "categories": "cs.LG cs.RO cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-space models have been successfully used for more than fifty years in\ndifferent areas of science and engineering. We present a procedure for\nefficient variational Bayesian learning of nonlinear state-space models based\non sparse Gaussian processes. The result of learning is a tractable posterior\nover nonlinear dynamical systems. In comparison to conventional parametric\nmodels, we offer the possibility to straightforwardly trade off model capacity\nand computational cost whilst avoiding overfitting. Our main algorithm uses a\nhybrid inference approach combining variational Bayes and sequential Monte\nCarlo. We also present stochastic variational inference and online learning\napproaches for fast learning with long time series.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 22:16:27 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 08:17:59 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Frigola", "Roger", ""], ["Chen", "Yutian", ""], ["Rasmussen", "Carl E.", ""]]}, {"id": "1406.4951", "submitter": "Sukru Burc Eryilmaz", "authors": "Sukru Burc Eryilmaz, Duygu Kuzum, Rakesh Jeyasingh, SangBum Kim,\n  Matthew BrightSky, Chung Lam and H.-S. Philip Wong", "title": "Brain-like associative learning using a nanoscale non-volatile phase\n  change synaptic device array", "comments": "Original article can be found here:\n  http://journal.frontiersin.org/Journal/10.3389/fnins.2014.00205/abstract", "journal-ref": "Front Neurosci. 8, 205 (2014)", "doi": "10.3389/fnins.2014.00205", "report-no": null, "categories": "cs.NE cond-mat.mtrl-sci cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neuroscience together with nanoscale electronic device\ntechnology have resulted in huge interests in realizing brain-like computing\nhardwares using emerging nanoscale memory devices as synaptic elements.\nAlthough there has been experimental work that demonstrated the operation of\nnanoscale synaptic element at the single device level, network level studies\nhave been limited to simulations. In this work, we demonstrate, using\nexperiments, array level associative learning using phase change synaptic\ndevices connected in a grid like configuration similar to the organization of\nthe biological brain. Implementing Hebbian learning with phase change memory\ncells, the synaptic grid was able to store presented patterns and recall\nmissing patterns in an associative brain-like fashion. We found that the system\nis robust to device variations, and large variations in cell resistance states\ncan be accommodated by increasing the number of training epochs. We illustrated\nthe tradeoff between variation tolerance of the network and the overall energy\nconsumption, and found that energy consumption is decreased significantly for\nlower variation tolerance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 05:49:02 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 16:41:34 GMT"}, {"version": "v3", "created": "Mon, 30 Jun 2014 21:54:45 GMT"}, {"version": "v4", "created": "Mon, 14 Jul 2014 00:12:49 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Eryilmaz", "Sukru Burc", ""], ["Kuzum", "Duygu", ""], ["Jeyasingh", "Rakesh", ""], ["Kim", "SangBum", ""], ["BrightSky", "Matthew", ""], ["Lam", "Chung", ""], ["Wong", "H. -S. Philip", ""]]}, {"id": "1406.4966", "submitter": "Jingdong Wang", "authors": "Chao Du, Jingdong Wang", "title": "Inner Product Similarity Search using Compositional Codes", "comments": "The approach presented in this paper (ECCV14 submission) is closely\n  related to multi-stage vector quantization and residual quantization. Thanks\n  the reviewers (CVPR14 and ECCV14) for pointing out the relationship to the\n  two algorithms. Related paper:\n  http://sites.skoltech.ru/app/data/uploads/sites/2/2013/09/CVPR14.pdf, which\n  also adopts the summation of vectors for vector approximation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the nearest neighbor search problem under inner product\nsimilarity and introduces a compact code-based approach. The idea is to\napproximate a vector using the composition of several elements selected from a\nsource dictionary and to represent this vector by a short code composed of the\nindices of the selected elements. The inner product between a query vector and\na database vector is efficiently estimated from the query vector and the short\ncode of the database vector. We show the superior performance of the proposed\ngroup $M$-selection algorithm that selects $M$ elements from $M$ source\ndictionaries for vector approximation in terms of search accuracy and\nefficiency for compact codes of the same length via theoretical and empirical\nanalysis. Experimental results on large-scale datasets ($1M$ and $1B$ SIFT\nfeatures, $1M$ linear models and Netflix) demonstrate the superiority of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 07:42:05 GMT"}, {"version": "v2", "created": "Fri, 20 Jun 2014 02:13:56 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Du", "Chao", ""], ["Wang", "Jingdong", ""]]}, {"id": "1406.5036", "submitter": "Katja Ried", "authors": "Katja Ried, Megan Agnew, Lydia Vermeyden, Dominik Janzing, Robert W.\n  Spekkens and Kevin J. Resch", "title": "Inferring causal structure: a quantum advantage", "comments": "17 pages, 6 figures. Comments welcome", "journal-ref": "Nat Phys 11, 414-420 (2015)", "doi": "10.1038/nphys3266", "report-no": null, "categories": "quant-ph cs.LG gr-qc stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of using observed correlations to infer causal relations is\nrelevant to a wide variety of scientific disciplines. Yet given correlations\nbetween just two classical variables, it is impossible to determine whether\nthey arose from a causal influence of one on the other or a common cause\ninfluencing both, unless one can implement a randomized intervention. We here\nconsider the problem of causal inference for quantum variables. We introduce\ncausal tomography, which unifies and generalizes conventional quantum\ntomography schemes to provide a complete solution to the causal inference\nproblem using a quantum analogue of a randomized trial. We furthermore show\nthat, in contrast to the classical case, observed quantum correlations alone\ncan sometimes provide a solution. We implement a quantum-optical experiment\nthat allows us to control the causal relation between two optical modes, and\ntwo measurement schemes -- one with and one without randomization -- that\nextract this relation from the observed correlations. Our results show that\nentanglement and coherence, known to be central to quantum information\nprocessing, also provide a quantum advantage for causal inference.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 13:30:12 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Ried", "Katja", ""], ["Agnew", "Megan", ""], ["Vermeyden", "Lydia", ""], ["Janzing", "Dominik", ""], ["Spekkens", "Robert W.", ""], ["Resch", "Kevin J.", ""]]}, {"id": "1406.5143", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "The Sample Complexity of Learning Linear Predictors with the Squared\n  Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note, we provide tight sample complexity bounds for learning\nlinear predictors with respect to the squared loss. Our focus is on an agnostic\nsetting, where no assumptions are made on the data distribution. This contrasts\nwith standard results in the literature, which either make distributional\nassumptions, refer to specific parameter settings, or use other performance\nmeasures.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 18:42:12 GMT"}, {"version": "v2", "created": "Sat, 21 Jun 2014 08:27:42 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1406.5161", "submitter": "Jeyanthi Salem Narasimhan", "authors": "Jeyanthi Narasimhan, Abhinav Vishnu, Lawrence Holder, Adolfy Hoisie", "title": "Fast Support Vector Machines Using Parallel Adaptive Shrinking on\n  Distributed Systems", "comments": "10 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Machines (SVM), a popular machine learning technique, has been\napplied to a wide range of domains such as science, finance, and social\nnetworks for supervised learning. Whether it is identifying high-risk patients\nby health-care professionals, or potential high-school students to enroll in\ncollege by school districts, SVMs can play a major role for social good. This\npaper undertakes the challenge of designing a scalable parallel SVM training\nalgorithm for large scale systems, which includes commodity multi-core\nmachines, tightly connected supercomputers and cloud computing systems.\nIntuitive techniques for improving the time-space complexity including adaptive\nelimination of samples for faster convergence and sparse format representation\nare proposed. Under sample elimination, several heuristics for {\\em earliest\npossible} to {\\em lazy} elimination of non-contributing samples are proposed.\nIn several cases, where an early sample elimination might result in a false\npositive, low overhead mechanisms for reconstruction of key data structures are\nproposed. The algorithm and heuristics are implemented and evaluated on various\npublicly available datasets. Empirical evaluation shows up to 26x speed\nimprovement on some datasets against the sequential baseline, when evaluated on\nmultiple compute nodes, and an improvement in execution time up to 30-60\\% is\nreadily observed on a number of other datasets against our parallel baseline.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 19:22:28 GMT"}], "update_date": "2014-06-20", "authors_parsed": [["Narasimhan", "Jeyanthi", ""], ["Vishnu", "Abhinav", ""], ["Holder", "Lawrence", ""], ["Hoisie", "Adolfy", ""]]}, {"id": "1406.5286", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis, Wing-Kin Ma", "title": "Enhancing Pure-Pixel Identification Performance via Preconditioning", "comments": "25 pages, 3 figures", "journal-ref": "SIAM J. on Imaging Sciences 8 (2), pp. 1161-1186, 2015", "doi": "10.1137/140994915", "report-no": null, "categories": "stat.ML cs.LG math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze different preconditionings designed to enhance\nrobustness of pure-pixel search algorithms, which are used for blind\nhyperspectral unmixing and which are equivalent to near-separable nonnegative\nmatrix factorization algorithms. Our analysis focuses on the successive\nprojection algorithm (SPA), a simple, efficient and provably robust algorithm\nin the pure-pixel algorithm class. Recently, a provably robust preconditioning\nwas proposed by Gillis and Vavasis (arXiv:1310.2273) which requires the\nresolution of a semidefinite program (SDP) to find a data points-enclosing\nminimum volume ellipsoid. Since solving the SDP in high precisions can be time\nconsuming, we generalize the robustness analysis to approximate solutions of\nthe SDP, that is, solutions whose objective function values are some\nmultiplicative factors away from the optimal value. It is shown that a high\naccuracy solution is not crucial for robustness, which paves the way for faster\npreconditionings (e.g., based on first-order optimization methods). This first\ncontribution also allows us to provide a robustness analysis for two other\npreconditionings. The first one is pre-whitening, which can be interpreted as\nan optimal solution of the same SDP with additional constraints. We analyze\nrobustness of pre-whitening which allows us to characterize situations in which\nit performs competitively with the SDP-based preconditioning. The second one is\nbased on SPA itself and can be interpreted as an optimal solution of a\nrelaxation of the SDP. It is extremely fast while competing with the SDP-based\npreconditioning on several synthetic data sets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 06:45:24 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Gillis", "Nicolas", ""], ["Ma", "Wing-Kin", ""]]}, {"id": "1406.5291", "submitter": "Soumyadeep Chatterjee", "authors": "Soumyadeep Chatterjee and Sheng Chen and Arindam Banerjee", "title": "Generalized Dantzig Selector: Application to the k-support norm", "comments": "Updates to bound", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Generalized Dantzig Selector (GDS) for linear models, in which\nany norm encoding the parameter structure can be leveraged for estimation. We\ninvestigate both computational and statistical aspects of the GDS. Based on\nconjugate proximal operator, a flexible inexact ADMM framework is designed for\nsolving GDS, and non-asymptotic high-probability bounds are established on the\nestimation error, which rely on Gaussian width of unit norm ball and suitable\nset encompassing estimation error. Further, we consider a non-trivial example\nof the GDS using $k$-support norm. We derive an efficient method to compute the\nproximal operator for $k$-support norm since existing methods are inapplicable\nin this setting. For statistical analysis, we provide upper bounds for the\nGaussian widths needed in the GDS analysis, yielding the first statistical\nrecovery guarantee for estimation with the $k$-support norm. The experimental\nresults confirm our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 07:11:44 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 14:53:00 GMT"}, {"version": "v3", "created": "Mon, 2 Feb 2015 17:54:14 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Chatterjee", "Soumyadeep", ""], ["Chen", "Sheng", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1406.5295", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas", "title": "Rows vs Columns for Linear Systems of Equations - Randomized Kaczmarz or\n  Coordinate Descent?", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about randomized iterative algorithms for solving a linear\nsystem of equations $X \\beta = y$ in different settings. Recent interest in the\ntopic was reignited when Strohmer and Vershynin (2009) proved the linear\nconvergence rate of a Randomized Kaczmarz (RK) algorithm that works on the rows\nof $X$ (data points). Following that, Leventhal and Lewis (2010) proved the\nlinear convergence of a Randomized Coordinate Descent (RCD) algorithm that\nworks on the columns of $X$ (features). The aim of this paper is to simplify\nour understanding of these two algorithms, establish the direct relationships\nbetween them (though RK is often compared to Stochastic Gradient Descent), and\nexamine the algorithmic commonalities or tradeoffs involved with working on\nrows or columns. We also discuss Kernel Ridge Regression and present a\nKaczmarz-style algorithm that works on data points and having the advantage of\nsolving the problem without ever storing or forming the Gram matrix, one of the\nrecognized problems encountered when scaling kernelized methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 07:21:31 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Ramdas", "Aaditya", ""]]}, {"id": "1406.5298", "submitter": "Diederik P Kingma M.Sc.", "authors": "Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, Max Welling", "title": "Semi-Supervised Learning with Deep Generative Models", "comments": "To appear in the proceedings of Neural Information Processing Systems\n  (NIPS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-increasing size of modern data sets combined with the difficulty of\nobtaining label information has made semi-supervised learning one of the\nproblems of significant practical importance in modern data analysis. We\nrevisit the approach to semi-supervised learning with generative models and\ndevelop new models that allow for effective generalisation from small labelled\ndata sets to large unlabelled ones. Generative approaches have thus far been\neither inflexible, inefficient or non-scalable. We show that deep generative\nmodels and approximate Bayesian inference exploiting recent advances in\nvariational methods can be used to provide significant improvements, making\ngenerative approaches highly competitive for semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 07:52:18 GMT"}, {"version": "v2", "created": "Fri, 31 Oct 2014 22:43:31 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Kingma", "Diederik P.", ""], ["Rezende", "Danilo J.", ""], ["Mohamed", "Shakir", ""], ["Welling", "Max", ""]]}, {"id": "1406.5311", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas and Javier Pe\\~na", "title": "Towards A Deeper Geometric, Analytic and Algorithmic Understanding of\n  Margins", "comments": "18 pages, 3 figures", "journal-ref": "Optimization Methods and Software, Volume 31, Issue 2, Pages\n  377-391, 2016", "doi": "10.1080/10556788.2015.1099652", "report-no": null, "categories": "math.OC cs.AI cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a matrix $A$, a linear feasibility problem (of which linear\nclassification is a special case) aims to find a solution to a primal problem\n$w: A^Tw > \\textbf{0}$ or a certificate for the dual problem which is a\nprobability distribution $p: Ap = \\textbf{0}$. Inspired by the continued\nimportance of \"large-margin classifiers\" in machine learning, this paper\nstudies a condition measure of $A$ called its \\textit{margin} that determines\nthe difficulty of both the above problems. To aid geometrical intuition, we\nfirst establish new characterizations of the margin in terms of relevant balls,\ncones and hulls. Our second contribution is analytical, where we present\ngeneralizations of Gordan's theorem, and variants of Hoffman's theorems, both\nusing margins. We end by proving some new results on a classical iterative\nscheme, the Perceptron, whose convergence rates famously depends on the margin.\nOur results are relevant for a deeper understanding of margin-based learning\nand proving convergence rates of iterative schemes, apart from providing a\nunifying perspective on this vast topic.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 08:35:15 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 05:53:45 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Pe\u00f1a", "Javier", ""]]}, {"id": "1406.5362", "submitter": "Christoph H. Lampert", "authors": "Christoph H. Lampert", "title": "Predicting the Future Behavior of a Time-Varying Probability\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of predicting the future, though only in the\nprobabilistic sense of estimating a future state of a time-varying probability\ndistribution. This is not only an interesting academic problem, but solving\nthis extrapolation problem also has many practical application, e.g. for\ntraining classifiers that have to operate under time-varying conditions. Our\nmain contribution is a method for predicting the next step of the time-varying\ndistribution from a given sequence of sample sets from earlier time steps. For\nthis we rely on two recent machine learning techniques: embedding probability\ndistributions into a reproducing kernel Hilbert space, and learning operators\nby vector-valued regression. We illustrate the working principles and the\npractical usefulness of our method by experiments on synthetic and real data.\nWe also highlight an exemplary application: training a classifier in a domain\nadaptation setting without having access to examples from the test time\ndistribution at training time.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 12:14:45 GMT"}, {"version": "v2", "created": "Thu, 20 Nov 2014 17:21:19 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Lampert", "Christoph H.", ""]]}, {"id": "1406.5370", "submitter": "Alexandre d'Aspremont", "authors": "Fajwel Fogel, Alexandre d'Aspremont, Milan Vojnovic", "title": "Spectral Ranking using Seriation", "comments": "Substantially revised. Accepted by JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a seriation algorithm for ranking a set of items given pairwise\ncomparisons between these items. Intuitively, the algorithm assigns similar\nrankings to items that compare similarly with all others. It does so by\nconstructing a similarity matrix from pairwise comparisons, using seriation\nmethods to reorder this matrix and construct a ranking. We first show that this\nspectral seriation algorithm recovers the true ranking when all pairwise\ncomparisons are observed and consistent with a total order. We then show that\nranking reconstruction is still exact when some pairwise comparisons are\ncorrupted or missing, and that seriation based spectral ranking is more robust\nto noise than classical scoring methods. Finally, we bound the ranking error\nwhen only a random subset of the comparions are observed. An additional benefit\nof the seriation formulation is that it allows us to solve semi-supervised\nranking problems. Experiments on both synthetic and real datasets demonstrate\nthat seriation based spectral ranking achieves competitive and in some cases\nsuperior performance compared to classical ranking methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 12:58:46 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 10:21:50 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2016 18:09:07 GMT"}, {"version": "v4", "created": "Thu, 10 Mar 2016 18:15:19 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Fogel", "Fajwel", ""], ["d'Aspremont", "Alexandre", ""], ["Vojnovic", "Milan", ""]]}, {"id": "1406.5383", "submitter": "Yining Wang", "authors": "Yining Wang, Aarti Singh", "title": "Noise-adaptive Margin-based Active Learning and Lower Bounds under\n  Tsybakov Noise Condition", "comments": "16 pages, 2 figures. An abridged version to appear in Thirtieth AAAI\n  Conference on Artificial Intelligence (AAAI), which is held in Phoenix, AZ\n  USA in 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple noise-robust margin-based active learning algorithm to\nfind homogeneous (passing the origin) linear separators and analyze its error\nconvergence when labels are corrupted by noise. We show that when the imposed\nnoise satisfies the Tsybakov low noise condition (Mammen, Tsybakov, and others\n1999; Tsybakov 2004) the algorithm is able to adapt to unknown level of noise\nand achieves optimal statistical rate up to poly-logarithmic factors. We also\nderive lower bounds for margin based active learning algorithms under Tsybakov\nnoise conditions (TNC) for the membership query synthesis scenario (Angluin\n1988). Our result implies lower bounds for the stream based selective sampling\nscenario (Cohn 1990) under TNC for some fairly simple data distributions. Quite\nsurprisingly, we show that the sample complexity cannot be improved even if the\nunderlying data distribution is as simple as the uniform distribution on the\nunit ball. Our proof involves the construction of a well separated hypothesis\nset on the d-dimensional unit ball along with carefully designed label\ndistributions for the Tsybakov noise condition. Our analysis might provide\ninsights for other forms of lower bounds as well.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 13:42:30 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 20:14:19 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 23:09:47 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Wang", "Yining", ""], ["Singh", "Aarti", ""]]}, {"id": "1406.5388", "submitter": "Luc Le Magoarou", "authors": "Luc Le Magoarou (INRIA - IRISA), R\\'emi Gribonval (INRIA - IRISA)", "title": "Learning computationally efficient dictionaries and their implementation\n  as fast transforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning is a branch of signal processing and machine learning\nthat aims at finding a frame (called dictionary) in which some training data\nadmits a sparse representation. The sparser the representation, the better the\ndictionary. The resulting dictionary is in general a dense matrix, and its\nmanipulation can be computationally costly both at the learning stage and later\nin the usage of this dictionary, for tasks such as sparse coding. Dictionary\nlearning is thus limited to relatively small-scale problems. In this paper,\ninspired by usual fast transforms, we consider a general dictionary structure\nthat allows cheaper manipulation, and propose an algorithm to learn such\ndictionaries --and their fast implementation-- over training data. The approach\nis demonstrated experimentally with the factorization of the Hadamard matrix\nand with synthetic dictionary learning experiments.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 13:52:36 GMT"}, {"version": "v2", "created": "Mon, 30 Jun 2014 18:47:09 GMT"}, {"version": "v3", "created": "Thu, 26 Feb 2015 19:53:03 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Magoarou", "Luc Le", "", "INRIA - IRISA"], ["Gribonval", "R\u00e9mi", "", "INRIA - IRISA"]]}, {"id": "1406.5429", "submitter": "Nikos Komodakis", "authors": "Nikos Komodakis and Jean-Christophe Pesquet", "title": "Playing with Duality: An Overview of Recent Primal-Dual Approaches for\n  Solving Large-Scale Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization methods are at the core of many problems in signal/image\nprocessing, computer vision, and machine learning. For a long time, it has been\nrecognized that looking at the dual of an optimization problem may drastically\nsimplify its solution. Deriving efficient strategies which jointly brings into\nplay the primal and the dual problems is however a more recent idea which has\ngenerated many important new contributions in the last years. These novel\ndevelopments are grounded on recent advances in convex analysis, discrete\noptimization, parallel processing, and non-smooth optimization with emphasis on\nsparsity issues. In this paper, we aim at presenting the principles of\nprimal-dual approaches, while giving an overview of numerical methods which\nhave been proposed in different contexts. We show the benefits which can be\ndrawn from primal-dual algorithms both for solving large-scale convex\noptimization problems and discrete ones, and we provide various application\nexamples to illustrate their usefulness.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 15:33:00 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 20:59:42 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Komodakis", "Nikos", ""], ["Pesquet", "Jean-Christophe", ""]]}, {"id": "1406.5565", "submitter": "Sam  Keene", "authors": "Kenneth D. Morton Jr., Peter Torrione, Leslie Collins, Sam Keene", "title": "An Open Source Pattern Recognition Toolbox for MATLAB", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern recognition and machine learning are becoming integral parts of\nalgorithms in a wide range of applications. Different algorithms and approaches\nfor machine learning include different tradeoffs between performance and\ncomputation, so during algorithm development it is often necessary to explore a\nvariety of different approaches to a given task. A toolbox with a unified\nframework across multiple pattern recognition techniques enables algorithm\ndevelopers the ability to rapidly evaluate different choices prior to\ndeployment. MATLAB is a widely used environment for algorithm development and\nprototyping, and although several MATLAB toolboxes for pattern recognition are\ncurrently available these are either incomplete, expensive, or restrictively\nlicensed. In this work we describe a MATLAB toolbox for pattern recognition and\nmachine learning known as the PRT (Pattern Recognition Toolbox), licensed under\nthe permissive MIT license. The PRT includes many popular techniques for data\npreprocessing, supervised learning, clustering, regression and feature\nselection, as well as a methodology for combining these components using a\nsimple, uniform syntax. The resulting algorithms can be evaluated using\ncross-validation and a variety of scoring metrics to ensure robust performance\nwhen the algorithm is deployed. This paper presents an overview of the PRT as\nwell as an example of usage on Fisher's Iris dataset.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 01:50:54 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Morton", "Kenneth D.", "Jr."], ["Torrione", "Peter", ""], ["Collins", "Leslie", ""], ["Keene", "Sam", ""]]}, {"id": "1406.5600", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk, Ivan Petej, and Valentina Fedorova", "title": "From conformal to probabilistic prediction", "comments": "12 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method of probabilistic prediction, which is based\non conformal prediction. The method is applied to the standard USPS data set\nand gives encouraging results.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 11:47:21 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Vovk", "Vladimir", ""], ["Petej", "Ivan", ""], ["Fedorova", "Valentina", ""]]}, {"id": "1406.5614", "submitter": "Shiliang Sun", "authors": "Shiliang Sun, John Shawe-Taylor, Liang Mao", "title": "PAC-Bayes Analysis of Multi-view Learning", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents eight PAC-Bayes bounds to analyze the generalization\nperformance of multi-view classifiers. These bounds adopt data dependent\nGaussian priors which emphasize classifiers with high view agreements. The\ncenter of the prior for the first two bounds is the origin, while the center of\nthe prior for the third and fourth bounds is given by a data dependent vector.\nAn important technique to obtain these bounds is two derived logarithmic\ndeterminant inequalities whose difference lies in whether the dimensionality of\ndata is involved. The centers of the fifth and sixth bounds are calculated on a\nseparate subset of the training set. The last two bounds use unlabeled data to\nrepresent view agreements and are thus applicable to semi-supervised multi-view\nlearning. We evaluate all the presented multi-view PAC-Bayes bounds on\nbenchmark data and compare them with previous single-view PAC-Bayes bounds. The\nusefulness and performance of the multi-view bounds are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 14:25:35 GMT"}, {"version": "v2", "created": "Sat, 4 Jun 2016 06:55:57 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Sun", "Shiliang", ""], ["Shawe-Taylor", "John", ""], ["Mao", "Liang", ""]]}, {"id": "1406.5647", "submitter": "Arash Ali Amini", "authors": "Arash A. Amini, Elizaveta Levina", "title": "On semidefinite relaxations for the block model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model (SBM) is a popular tool for community detection in\nnetworks, but fitting it by maximum likelihood (MLE) involves a computationally\ninfeasible optimization problem. We propose a new semidefinite programming\n(SDP) solution to the problem of fitting the SBM, derived as a relaxation of\nthe MLE. We put ours and previously proposed SDPs in a unified framework, as\nrelaxations of the MLE over various sub-classes of the SBM, revealing a\nconnection to sparse PCA. Our main relaxation, which we call SDP-1, is tighter\nthan other recently proposed SDP relaxations, and thus previously established\ntheoretical guarantees carry over. However, we show that SDP-1 exactly recovers\ntrue communities over a wider class of SBMs than those covered by current\nresults. In particular, the assumption of strong assortativity of the SBM,\nimplicit in consistency conditions for previously proposed SDPs, can be relaxed\nto weak assortativity for our approach, thus significantly broadening the class\nof SBMs covered by the consistency results. We also show that strong\nassortativity is indeed a necessary condition for exact recovery for previously\nproposed SDP approaches and not an artifact of the proofs. Our analysis of SDPs\nis based on primal-dual witness constructions, which provides some insight into\nthe nature of the solutions of various SDPs. We show how to combine features\nfrom SDP-1 and already available SDPs to achieve the most flexibility in terms\nof both assortativity and block-size constraints, as our relaxation has the\ntendency to produce communities of similar sizes. This tendency makes it the\nideal tool for fitting network histograms, a method gaining popularity in the\ngraphon estimation literature, as we illustrate on an example of a social\nnetworks of dolphins. We also provide empirical evidence that SDPs outperform\nspectral methods for fitting SBMs with a large number of blocks.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 20:08:38 GMT"}, {"version": "v2", "created": "Thu, 11 Dec 2014 17:36:13 GMT"}, {"version": "v3", "created": "Wed, 16 Mar 2016 14:24:05 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Amini", "Arash A.", ""], ["Levina", "Elizaveta", ""]]}, {"id": "1406.5665", "submitter": "Aravindan Vijayaraghavan", "authors": "Konstantin Makarychev, Yury Makarychev, Aravindan Vijayaraghavan", "title": "Constant Factor Approximation for Balanced Cut in the PIE model", "comments": "Full version of the paper at the 46th ACM Symposium on the Theory of\n  Computing (STOC 2014). 32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study a new semi-random semi-adversarial model for Balanced\nCut, a planted model with permutation-invariant random edges (PIE). Our model\nis much more general than planted models considered previously. Consider a set\nof vertices V partitioned into two clusters $L$ and $R$ of equal size. Let $G$\nbe an arbitrary graph on $V$ with no edges between $L$ and $R$. Let\n$E_{random}$ be a set of edges sampled from an arbitrary permutation-invariant\ndistribution (a distribution that is invariant under permutation of vertices in\n$L$ and in $R$). Then we say that $G + E_{random}$ is a graph with\npermutation-invariant random edges.\n  We present an approximation algorithm for the Balanced Cut problem that finds\na balanced cut of cost $O(|E_{random}|) + n \\text{polylog}(n)$ in this model.\nIn the regime when $|E_{random}| = \\Omega(n \\text{polylog}(n))$, this is a\nconstant factor approximation with respect to the cost of the planted cut.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 03:00:32 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Makarychev", "Konstantin", ""], ["Makarychev", "Yury", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1406.5667", "submitter": "Aravindan Vijayaraghavan", "authors": "Konstantin Makarychev, Yury Makarychev, Aravindan Vijayaraghavan", "title": "Correlation Clustering with Noisy Partial Information", "comments": "To appear at Conference on Learning Theory (COLT) 2015. Substantial\n  changes from previous version, including a new section on recovery of the\n  ground truth clustering. 20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and study a semi-random model for the Correlation\nClustering problem on arbitrary graphs G. We give two approximation algorithms\nfor Correlation Clustering instances from this model. The first algorithm finds\na solution of value $(1+ \\delta) optcost + O_{\\delta}(n\\log^3 n)$ with high\nprobability, where $optcost$ is the value of the optimal solution (for every\n$\\delta > 0$). The second algorithm finds the ground truth clustering with an\narbitrarily small classification error $\\eta$ (under some additional\nassumptions on the instance).\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 03:07:55 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 19:33:12 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Makarychev", "Konstantin", ""], ["Makarychev", "Yury", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1406.5675", "submitter": "Shusen Wang", "authors": "Shusen Wang, Luo Luo, Zhihua Zhang", "title": "SPSD Matrix Approximation vis Column Selection: Theories, Algorithms,\n  and Extensions", "comments": "Journal of Machine Learning Research, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric positive semidefinite (SPSD) matrix approximation is an important\nproblem with applications in kernel methods. However, existing SPSD matrix\napproximation methods such as the Nystr\\\"om method only have weak error bounds.\nIn this paper we conduct in-depth studies of an SPSD matrix approximation model\nand establish strong relative-error bounds. We call it the prototype model for\nit has more efficient and effective extensions, and some of its extensions have\nhigh scalability. Though the prototype model itself is not suitable for\nlarge-scale data, it is still useful to study its properties, on which the\nanalysis of its extensions relies.\n  This paper offers novel theoretical analysis, efficient algorithms, and a\nhighly accurate extension. First, we establish a lower error bound for the\nprototype model and improve the error bound of an existing column selection\nalgorithm to match the lower bound. In this way, we obtain the first optimal\ncolumn selection algorithm for the prototype model. We also prove that the\nprototype model is exact under certain conditions. Second, we develop a simple\ncolumn selection algorithm with a provable error bound. Third, we propose a\nso-called spectral shifting model to make the approximation more accurate when\nthe eigenvalues of the matrix decay slowly, and the improvement is\ntheoretically quantified. The spectral shifting method can also be applied to\nimprove other SPSD matrix approximation models.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 05:02:06 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 10:05:36 GMT"}, {"version": "v3", "created": "Sat, 28 Mar 2015 07:43:49 GMT"}, {"version": "v4", "created": "Sun, 11 Oct 2015 08:49:11 GMT"}, {"version": "v5", "created": "Sun, 20 Dec 2015 09:47:37 GMT"}, {"version": "v6", "created": "Fri, 20 May 2016 06:32:12 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Wang", "Shusen", ""], ["Luo", "Luo", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1406.5679", "submitter": "Andrej Karpathy", "authors": "Andrej Karpathy, Armand Joulin and Li Fei-Fei", "title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model for bidirectional retrieval of images and sentences\nthrough a multi-modal embedding of visual and natural language data. Unlike\nprevious models that directly map images or sentences into a common embedding\nspace, our model works on a finer level and embeds fragments of images\n(objects) and fragments of sentences (typed dependency tree relations) into a\ncommon space. In addition to a ranking objective seen in previous work, this\nallows us to add a new fragment alignment objective that learns to directly\nassociate these fragments across modalities. Extensive experimental evaluation\nshows that reasoning on both the global level of images and sentences and the\nfiner level of their respective fragments significantly improves performance on\nimage-sentence retrieval tasks. Additionally, our model provides interpretable\npredictions since the inferred inter-modal fragment alignment is explicit.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 06:22:50 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Karpathy", "Andrej", ""], ["Joulin", "Armand", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1406.5706", "submitter": "Francesca Paola Carli", "authors": "Francesca Paola Carli", "title": "On the Maximum Entropy Property of the First-Order Stable Spline Kernel\n  and its Implications", "comments": "12 pages. In 2014 IEEE Multi-conference on Systems and Control. IEEE,\n  2014", "journal-ref": null, "doi": "10.1109/CCA.2014.6981380", "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new nonparametric approach for system identification has been recently\nproposed where the impulse response is seen as the realization of a zero--mean\nGaussian process whose covariance, the so--called stable spline kernel,\nguarantees that the impulse response is almost surely stable. Maximum entropy\nproperties of the stable spline kernel have been pointed out in the literature.\nIn this paper we provide an independent proof that relies on the theory of\nmatrix extension problems in the graphical model literature and leads to a\nclosed form expression for the inverse of the first order stable spline kernel\nas well as to a new factorization in the form $UWU^\\top$ with $U$ upper\ntriangular and $W$ diagonal. Interestingly, all first--order stable spline\nkernels share the same factor $U$ and $W$ admits a closed form representation\nin terms of the kernel hyperparameter, making the factorization computationally\ninexpensive. Maximum likelihood properties of the stable spline kernel are also\nhighlighted. These results can be applied both to improve the stability and to\nreduce the computational complexity associated with the computation of stable\nspline estimators.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 11:38:59 GMT"}, {"version": "v2", "created": "Sun, 21 Sep 2014 22:03:06 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Carli", "Francesca Paola", ""]]}, {"id": "1406.5736", "submitter": "Chao Ding", "authors": "Chao Ding and Hou-Duo Qi", "title": "Convex Optimization Learning of Faithful Euclidean Distance\n  Representations in Nonlinear Dimensionality Reduction", "comments": "44 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical multidimensional scaling only works well when the noisy distances\nobserved in a high dimensional space can be faithfully represented by Euclidean\ndistances in a low dimensional space. Advanced models such as Maximum Variance\nUnfolding (MVU) and Minimum Volume Embedding (MVE) use Semi-Definite\nProgramming (SDP) to reconstruct such faithful representations. While those SDP\nmodels are capable of producing high quality configuration numerically, they\nsuffer two major drawbacks. One is that there exist no theoretically guaranteed\nbounds on the quality of the configuration. The other is that they are slow in\ncomputation when the data points are beyond moderate size. In this paper, we\npropose a convex optimization model of Euclidean distance matrices. We\nestablish a non-asymptotic error bound for the random graph model with\nsub-Gaussian noise, and prove that our model produces a matrix estimator of\nhigh accuracy when the order of the uniform sample size is roughly the degree\nof freedom of a low-rank matrix up to a logarithmic factor. Our results\npartially explain why MVU and MVE often work well. Moreover, we develop a fast\ninexact accelerated proximal gradient method. Numerical experiments show that\nthe model can produce configurations of high quality on large data points that\nthe SDP approach would struggle to cope with.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 15:34:15 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Ding", "Chao", ""], ["Qi", "Hou-Duo", ""]]}, {"id": "1406.5752", "submitter": "Tianyi Zhou", "authors": "Tianyi Zhou and Jeff Bilmes and Carlos Guestrin", "title": "Divide-and-Conquer Learning by Anchoring a Conical Hull", "comments": "26 pages, long version, in updating", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We reduce a broad class of machine learning problems, usually addressed by EM\nor sampling, to the problem of finding the $k$ extremal rays spanning the\nconical hull of a data point set. These $k$ \"anchors\" lead to a global solution\nand a more interpretable model that can even outperform EM and sampling on\ngeneralization error. To find the $k$ anchors, we propose a novel\ndivide-and-conquer learning scheme \"DCA\" that distributes the problem to\n$\\mathcal O(k\\log k)$ same-type sub-problems on different low-D random\nhyperplanes, each can be solved by any solver. For the 2D sub-problem, we\npresent a non-iterative solver that only needs to compute an array of cosine\nvalues and its max/min entries. DCA also provides a faster subroutine for other\nmethods to check whether a point is covered in a conical hull, which improves\nalgorithm design in multiple dimensions and brings significant speedup to\nlearning. We apply our method to GMM, HMM, LDA, NMF and subspace clustering,\nthen show its competitive performance and scalability over other methods on\nrich datasets.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 19:16:20 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Zhou", "Tianyi", ""], ["Bilmes", "Jeff", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1406.5910", "submitter": "Roman Shapovalov", "authors": "Roman Shapovalov, Dmitry Vetrov, Anton Osokin, Pushmeet Kohli", "title": "Multi-utility Learning: Structured-output Learning with Multiple\n  Annotation-specific Loss Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured-output learning is a challenging problem; particularly so because\nof the difficulty in obtaining large datasets of fully labelled instances for\ntraining. In this paper we try to overcome this difficulty by presenting a\nmulti-utility learning framework for structured prediction that can learn from\ntraining instances with different forms of supervision. We propose a unified\ntechnique for inferring the loss functions most suitable for quantifying the\nconsistency of solutions with the given weak annotation. We demonstrate the\neffectiveness of our framework on the challenging semantic image segmentation\nproblem for which a wide variety of annotations can be used. For instance, the\npopular training datasets for semantic segmentation are composed of images with\nhard-to-generate full pixel labellings, as well as images with easy-to-obtain\nweak annotations, such as bounding boxes around objects, or image-level labels\nthat specify which object categories are present in an image. Experimental\nevaluation shows that the use of annotation-specific loss functions\ndramatically improves segmentation accuracy compared to the baseline system\nwhere only one type of weak annotation is used.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 14:06:24 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Shapovalov", "Roman", ""], ["Vetrov", "Dmitry", ""], ["Osokin", "Anton", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1406.5979", "submitter": "Stephane Ross", "authors": "Stephane Ross, J. Andrew Bagnell", "title": "Reinforcement and Imitation Learning via Interactive No-Regret Learning", "comments": "14 pages. Under review for NIPS 2014 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has demonstrated that problems-- particularly imitation learning\nand structured prediction-- where a learner's predictions influence the\ninput-distribution it is tested on can be naturally addressed by an interactive\napproach and analyzed using no-regret online learning. These approaches to\nimitation learning, however, neither require nor benefit from information about\nthe cost of actions. We extend existing results in two directions: first, we\ndevelop an interactive imitation learning approach that leverages cost\ninformation; second, we extend the technique to address reinforcement learning.\nThe results provide theoretical support to the commonly observed successes of\nonline approximate policy iteration. Our approach suggests a broad new family\nof algorithms and provides a unifying view of existing techniques for imitation\nand reinforcement learning.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 17:00:28 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Ross", "Stephane", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1406.6020", "submitter": "Julien Audiffren", "authors": "Julien Audiffren (CMLA), Liva Ralaivola (LIF)", "title": "Stationary Mixing Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the bandit problem where arms are associated with stationary\nphi-mixing processes and where rewards are therefore dependent: the question\nthat arises from this setting is that of recovering some independence by\nignoring the value of some rewards. As we shall see, the bandit problem we\ntackle requires us to address the exploration/exploitation/independence\ntrade-off. To do so, we provide a UCB strategy together with a general regret\nanalysis for the case where the size of the independence blocks (the ignored\nrewards) is fixed and we go a step beyond by providing an algorithm that is\nable to compute the size of the independence blocks from the data. Finally, we\ngive an analysis of our bandit problem in the restless case, i.e., in the\nsituation where the time counters for all mixing processes simultaneously\nevolve.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 18:48:59 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Audiffren", "Julien", "", "CMLA"], ["Ralaivola", "Liva", "", "LIF"]]}, {"id": "1406.6084", "submitter": "Zhenming Liu", "authors": "Henry Lam and Zhenming Liu", "title": "From Black-Scholes to Online Learning: Dynamic Hedging under Adversarial\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a non-stochastic online learning approach to price financial\noptions by modeling the market dynamic as a repeated game between the nature\n(adversary) and the investor. We demonstrate that such framework yields\nanalogous structure as the Black-Scholes model, the widely popular option\npricing model in stochastic finance, for both European and American options\nwith convex payoffs. In the case of non-convex options, we construct\napproximate pricing algorithms, and demonstrate that their efficiency can be\nanalyzed through the introduction of an artificial probability measure, in\nparallel to the so-called risk-neutral measure in the finance literature, even\nthough our framework is completely adversarial. Continuous-time convergence\nresults and extensions to incorporate price jumps are also presented.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 20:40:14 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Lam", "Henry", ""], ["Liu", "Zhenming", ""]]}, {"id": "1406.6101", "submitter": "Imen Trabelsi", "authors": "Imen Trabelsi, Dorra Ben Ayed, Noureddine Ellouze", "title": "Improved Frame Level Features and SVM Supervectors Approach for the\n  Recogniton of Emotional States from Speech: Application to categorical and\n  dimensional states", "comments": null, "journal-ref": "I.J. Image, Graphics and Signal Processing, 2013, 9, 8-13", "doi": "10.5815/ijigsp.2013.09.02", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The purpose of speech emotion recognition system is to classify speakers\nutterances into different emotional states such as disgust, boredom, sadness,\nneutral and happiness. Speech features that are commonly used in speech emotion\nrecognition rely on global utterance level prosodic features. In our work, we\nevaluate the impact of frame level feature extraction. The speech samples are\nfrom Berlin emotional database and the features extracted from these utterances\nare energy, different variant of mel frequency cepstrum coefficients, velocity\nand acceleration features.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 22:21:17 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Trabelsi", "Imen", ""], ["Ayed", "Dorra Ben", ""], ["Ellouze", "Noureddine", ""]]}, {"id": "1406.6114", "submitter": "Sakthithasan Sripirakas", "authors": "Sakthithasan Sripirakas and Russel Pears", "title": "Mining Recurrent Concepts in Data Streams using the Discrete Fourier\n  Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research we address the problem of capturing recurring concepts in a\ndata stream environment. Recurrence capture enables the re-use of previously\nlearned classifiers without the need for re-learning while providing for better\naccuracy during the concept recurrence interval. We capture concepts by\napplying the Discrete Fourier Transform (DFT) to Decision Tree classifiers to\nobtain highly compressed versions of the trees at concept drift points in the\nstream and store such trees in a repository for future use. Our empirical\nresults on real world and synthetic data exhibiting varying degrees of\nrecurrence show that the Fourier compressed trees are more robust to noise and\nare able to capture recurring concepts with higher precision than a meta\nlearning approach that chooses to re-use classifiers in their originally\noccurring form.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 00:48:23 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Sripirakas", "Sakthithasan", ""], ["Pears", "Russel", ""]]}, {"id": "1406.6130", "submitter": "Mark Reid", "authors": "Mark D. Reid and Rafael M. Frongillo and Robert C. Williamson and\n  Nishant Mehta", "title": "Generalized Mixability via Entropic Duality", "comments": "20 pages, 1 figure. Supersedes the work in arXiv:1403.2433 [cs.LG]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixability is a property of a loss which characterizes when fast convergence\nis possible in the game of prediction with expert advice. We show that a key\nproperty of mixability generalizes, and the exp and log operations present in\nthe usual theory are not as special as one might have thought. In doing this we\nintroduce a more general notion of $\\Phi$-mixability where $\\Phi$ is a general\nentropy (\\ie, any convex function on probabilities). We show how a property\nshared by the convex dual of any such entropy yields a natural algorithm (the\nminimizer of a regret bound) which, analogous to the classical aggregating\nalgorithm, is guaranteed a constant regret when used with $\\Phi$-mixable\nlosses. We characterize precisely which $\\Phi$ have $\\Phi$-mixable losses and\nput forward a number of conjectures about the optimality and relationships\nbetween different choices of entropy.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 03:31:16 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Reid", "Mark D.", ""], ["Frongillo", "Rafael M.", ""], ["Williamson", "Robert C.", ""], ["Mehta", "Nishant", ""]]}, {"id": "1406.6145", "submitter": "Tyler Maunu", "authors": "Gilad Lerman and Tyler Maunu", "title": "Fast, Robust and Non-convex Subspace Recovery", "comments": null, "journal-ref": "Information and Inference: A Journal of the IMA 7 (2018) 277-336", "doi": "10.1093/imaiai/iax012", "report-no": null, "categories": "cs.LG cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a fast and non-convex algorithm for robust subspace\nrecovery. The data sets considered include inliers drawn around a\nlow-dimensional subspace of a higher dimensional ambient space, and a possibly\nlarge portion of outliers that do not lie nearby this subspace. The proposed\nalgorithm, which we refer to as Fast Median Subspace (FMS), is designed to\nrobustly determine the underlying subspace of such data sets, while having\nlower computational complexity than existing methods. We prove convergence of\nthe FMS iterates to a stationary point. Further, under a special model of data,\nFMS converges to a point which is near to the global minimum with overwhelming\nprobability. Under this model, we show that the iteration complexity is\nglobally bounded and locally $r$-linear. The latter theorem holds for any fixed\nfraction of outliers (less than 1) and any fixed positive distance between the\nlimit point and the global minimum. Numerical experiments on synthetic and real\ndata demonstrate its competitive speed and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 06:15:07 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 22:58:10 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Lerman", "Gilad", ""], ["Maunu", "Tyler", ""]]}, {"id": "1406.6176", "submitter": "Muneki Yasuda", "authors": "Muneki Yasuda, Shun Kataoka, Yuji Waizumi, Kazuyuki Tanaka", "title": "Composite Likelihood Estimation for Restricted Boltzmann machines", "comments": null, "journal-ref": "Proceedings of 21st International Conference on Pattern\n  Recognition (ICPR2012), pp. 2234-2237, 2012", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the parameters of graphical models using the maximum likelihood\nestimation is generally hard which requires an approximation. Maximum composite\nlikelihood estimations are statistical approximations of the maximum likelihood\nestimation which are higher-order generalizations of the maximum\npseudo-likelihood estimation. In this paper, we propose a composite likelihood\nmethod and investigate its property. Furthermore, we apply our composite\nlikelihood method to restricted Boltzmann machines.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 09:32:10 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Yasuda", "Muneki", ""], ["Kataoka", "Shun", ""], ["Waizumi", "Yuji", ""], ["Tanaka", "Kazuyuki", ""]]}, {"id": "1406.6200", "submitter": "Thijs van Ommen", "authors": "Thijs van Ommen", "title": "Combining predictions from linear models when training and test inputs\n  differ", "comments": "12 pages, 2 figures. To appear in Proceedings of the 30th Conference\n  on Uncertainty in Artificial Intelligence (UAI2014). This version includes\n  the supplementary material (regularity assumptions, proofs)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for combining predictions from different models in a supervised\nlearning setting must somehow estimate/predict the quality of a model's\npredictions at unknown future inputs. Many of these methods (often implicitly)\nmake the assumption that the test inputs are identical to the training inputs,\nwhich is seldom reasonable. By failing to take into account that prediction\nwill generally be harder for test inputs that did not occur in the training\nset, this leads to the selection of too complex models. Based on a novel,\nunbiased expression for KL divergence, we propose XAIC and its special case\nFAIC as versions of AIC intended for prediction that use different degrees of\nknowledge of the test inputs. Both methods substantially differ from and may\noutperform all the known versions of AIC even when the training and test inputs\nare iid, and are especially useful for deterministic inputs and under covariate\nshift. Our experiments on linear models suggest that if the test and training\ninputs differ substantially, then XAIC and FAIC predictively outperform AIC,\nBIC and several other methods including Bayesian model averaging.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 10:56:13 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["van Ommen", "Thijs", ""]]}, {"id": "1406.6247", "submitter": "Volodymyr Mnih", "authors": "Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu", "title": "Recurrent Models of Visual Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying convolutional neural networks to large images is computationally\nexpensive because the amount of computation scales linearly with the number of\nimage pixels. We present a novel recurrent neural network model that is capable\nof extracting information from an image or video by adaptively selecting a\nsequence of regions or locations and only processing the selected regions at\nhigh resolution. Like convolutional neural networks, the proposed model has a\ndegree of translation invariance built-in, but the amount of computation it\nperforms can be controlled independently of the input image size. While the\nmodel is non-differentiable, it can be trained using reinforcement learning\nmethods to learn task-specific policies. We evaluate our model on several image\nclassification tasks, where it significantly outperforms a convolutional neural\nnetwork baseline on cluttered images, and on a dynamic visual control problem,\nwhere it learns to track a simple object without an explicit training signal\nfor doing so.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 14:16:56 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Mnih", "Volodymyr", ""], ["Heess", "Nicolas", ""], ["Graves", "Alex", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1406.6312", "submitter": "Ahmed El-Kishky", "authors": "Ahmed El-Kishky, Yanglei Song, Chi Wang, Clare Voss, Jiawei Han", "title": "Scalable Topical Phrase Mining from Text Corpora", "comments": null, "journal-ref": "Proceedings of the VLDB Endowment, Vol. 8(3), pp. 305 - 316, 2014", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most topic modeling algorithms model text corpora with unigrams, human\ninterpretation often relies on inherent grouping of terms into phrases. As\nsuch, we consider the problem of discovering topical phrases of mixed lengths.\nExisting work either performs post processing to the inference results of\nunigram-based topic models, or utilizes complex n-gram-discovery topic models.\nThese methods generally produce low-quality topical phrases or suffer from poor\nscalability on even moderately-sized datasets. We propose a different approach\nthat is both computationally efficient and effective. Our solution combines a\nnovel phrase mining framework to segment a document into single and multi-word\nphrases, and a new topic model that operates on the induced document partition.\nOur approach discovers high quality topical phrases with negligible extra cost\nto the bag-of-words topic model in a variety of datasets including research\npublication titles, abstracts, reviews, and news articles.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 17:10:29 GMT"}, {"version": "v2", "created": "Wed, 19 Nov 2014 00:18:06 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["El-Kishky", "Ahmed", ""], ["Song", "Yanglei", ""], ["Wang", "Chi", ""], ["Voss", "Clare", ""], ["Han", "Jiawei", ""]]}, {"id": "1406.6314", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Richard Nock", "title": "Further heuristics for $k$-means: The merge-and-split heuristic and the\n  $(k,l)$-means", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the optimal $k$-means clustering is NP-hard in general and many\nheuristics have been designed for minimizing monotonically the $k$-means\nobjective. We first show how to extend Lloyd's batched relocation heuristic and\nHartigan's single-point relocation heuristic to take into account empty-cluster\nand single-point cluster events, respectively. Those events tend to\nincreasingly occur when $k$ or $d$ increases, or when performing several\nrestarts. First, we show that those special events are a blessing because they\nallow to partially re-seed some cluster centers while further minimizing the\n$k$-means objective function. Second, we describe a novel heuristic,\nmerge-and-split $k$-means, that consists in merging two clusters and splitting\nthis merged cluster again with two new centers provided it improves the\n$k$-means objective. This novel heuristic can improve Hartigan's $k$-means when\nit has converged to a local minimum. We show empirically that this\nmerge-and-split $k$-means improves over the Hartigan's heuristic which is the\n{\\em de facto} method of choice. Finally, we propose the $(k,l)$-means\nobjective that generalizes the $k$-means objective by associating the data\npoints to their $l$ closest cluster centers, and show how to either directly\nconvert or iteratively relax the $(k,l)$-means into a $k$-means in order to\nreach better local minima.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 02:34:34 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Nielsen", "Frank", ""], ["Nock", "Richard", ""]]}, {"id": "1406.6398", "submitter": "Margareta Ackerman Margareta Ackerman", "authors": "Margareta Ackerman and Sanjoy Dasgupta", "title": "Incremental Clustering: The Case for Extra Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosion in the amount of data available for analysis often necessitates\na transition from batch to incremental clustering methods, which process one\nelement at a time and typically store only a small subset of the data. In this\npaper, we initiate the formal analysis of incremental clustering methods\nfocusing on the types of cluster structure that they are able to detect. We\nfind that the incremental setting is strictly weaker than the batch model,\nproving that a fundamental class of cluster structures that can readily be\ndetected in the batch setting is impossible to identify using any incremental\nmethod. Furthermore, we show how the limitations of incremental clustering can\nbe overcome by allowing additional clusters.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 21:41:03 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Ackerman", "Margareta", ""], ["Dasgupta", "Sanjoy", ""]]}, {"id": "1406.6474", "submitter": "Robert Nishihara", "authors": "Robert Nishihara, Stefanie Jegelka, Michael I. Jordan", "title": "On the Convergence Rate of Decomposable Submodular Function Minimization", "comments": "17 pages, 3 figures", "journal-ref": "Neural Information Processing Systems 27, 2014", "doi": null, "report-no": null, "categories": "math.OC cs.DM cs.DS cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular functions describe a variety of discrete problems in machine\nlearning, signal processing, and computer vision. However, minimizing\nsubmodular functions poses a number of algorithmic challenges. Recent work\nintroduced an easy-to-use, parallelizable algorithm for minimizing submodular\nfunctions that decompose as the sum of \"simple\" submodular functions.\nEmpirically, this algorithm performs extremely well, but no theoretical\nanalysis was given. In this paper, we show that the algorithm converges\nlinearly, and we provide upper and lower bounds on the rate of convergence. Our\nproof relies on the geometry of submodular polyhedra and draws on results from\nspectral graph theory.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 06:52:33 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 18:12:03 GMT"}, {"version": "v3", "created": "Wed, 5 Nov 2014 07:19:00 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Nishihara", "Robert", ""], ["Jegelka", "Stefanie", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1406.6507", "submitter": "Hyun Oh Song", "authors": "Hyun Oh Song, Yong Jae Lee, Stefanie Jegelka, Trevor Darrell", "title": "Weakly-supervised Discovery of Visual Pattern Configurations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing prominence of weakly labeled data nurtures a growing demand\nfor object detection methods that can cope with minimal supervision. We propose\nan approach that automatically identifies discriminative configurations of\nvisual patterns that are characteristic of a given object class. We formulate\nthe problem as a constrained submodular optimization problem and demonstrate\nthe benefits of the discovered configurations in remedying mislocalizations and\nfinding informative positive and negative training examples. Together, these\nlead to state-of-the-art weakly-supervised detection results on the challenging\nPASCAL VOC dataset.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 09:35:40 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Song", "Hyun Oh", ""], ["Lee", "Yong Jae", ""], ["Jegelka", "Stefanie", ""], ["Darrell", "Trevor", ""]]}, {"id": "1406.6568", "submitter": "Victor Miller", "authors": "V. A. Miller, S. Erlien, J. Piersol", "title": "Support vector machine classification of dimensionally reduced\n  structural MRI images for dementia", "comments": "technical note", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We classify very-mild to moderate dementia in patients (CDR ranging from 0 to\n2) using a support vector machine classifier acting on dimensionally reduced\nfeature set derived from MRI brain scans of the 416 subjects available in the\nOASIS-Brains dataset. We use image segmentation and principal component\nanalysis to reduce the dimensionality of the data. Our resulting feature set\ncontains 11 features for each subject. Performance of the classifiers is\nevaluated using 10-fold cross-validation. Using linear and (gaussian) kernels,\nwe obtain a training classification accuracy of 86.4% (90.1%), test accuracy of\n85.0% (85.7%), test precision of 68.7% (68.5%), test recall of 68.0% (74.0%),\nand test Matthews correlation coefficient of 0.594 (0.616).\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 13:50:18 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Miller", "V. A.", ""], ["Erlien", "S.", ""], ["Piersol", "J.", ""]]}, {"id": "1406.6603", "submitter": "Marco Prato", "authors": "Silvia Bonettini and Alessandro Chiuso and Marco Prato", "title": "A scaled gradient projection method for Bayesian learning in dynamical\n  systems", "comments": null, "journal-ref": "SIAM Journal on Scientific Computing 37 (2015), A1297-A1318", "doi": "10.1137/140973529", "report-no": null, "categories": "math.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crucial task in system identification problems is the selection of the most\nappropriate model class, and is classically addressed resorting to\ncross-validation or using asymptotic arguments. As recently suggested in the\nliterature, this can be addressed in a Bayesian framework, where model\ncomplexity is regulated by few hyperparameters, which can be estimated via\nmarginal likelihood maximization. It is thus of primary importance to design\neffective optimization methods to solve the corresponding optimization problem.\nIf the unknown impulse response is modeled as a Gaussian process with a\nsuitable kernel, the maximization of the marginal likelihood leads to a\nchallenging nonconvex optimization problem, which requires a stable and\neffective solution strategy. In this paper we address this problem by means of\na scaled gradient projection algorithm, in which the scaling matrix and the\nsteplength parameter play a crucial role to provide a meaning solution in a\ncomputational time comparable with second order methods. In particular, we\npropose both a generalization of the split gradient approach to design the\nscaling matrix in the presence of box constraints, and an effective\nimplementation of the gradient and objective function. The extensive numerical\nexperiments carried out on several test problems show that our method is very\neffective in providing in few tenths of a second solutions of the problems with\naccuracy comparable with state-of-the-art approaches. Moreover, the flexibility\nof the proposed strategy makes it easily adaptable to a wider range of problems\narising in different areas of machine learning, signal processing and system\nidentification.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 15:12:48 GMT"}, {"version": "v2", "created": "Fri, 28 Nov 2014 14:15:56 GMT"}, {"version": "v3", "created": "Mon, 2 Feb 2015 11:25:41 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Bonettini", "Silvia", ""], ["Chiuso", "Alessandro", ""], ["Prato", "Marco", ""]]}, {"id": "1406.6618", "submitter": "Nihar Shah", "authors": "Nihar B. Shah, Sivaraman Balakrishnan, Joseph Bradley, Abhay Parekh,\n  Kannan Ramchandran, Martin Wainwright", "title": "When is it Better to Compare than to Score?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When eliciting judgements from humans for an unknown quantity, one often has\nthe choice of making direct-scoring (cardinal) or comparative (ordinal)\nmeasurements. In this paper we study the relative merits of either choice,\nproviding empirical and theoretical guidelines for the selection of a\nmeasurement scheme. We provide empirical evidence based on experiments on\nAmazon Mechanical Turk that in a variety of tasks, (pairwise-comparative)\nordinal measurements have lower per sample noise and are typically faster to\nelicit than cardinal ones. Ordinal measurements however typically provide less\ninformation. We then consider the popular Thurstone and Bradley-Terry-Luce\n(BTL) models for ordinal measurements and characterize the minimax error rates\nfor estimating the unknown quantity. We compare these minimax error rates to\nthose under cardinal measurement models and quantify for what noise levels\nordinal measurements are better. Finally, we revisit the data collected from\nour experiments and show that fitting these models confirms this prediction:\nfor tasks where the noise in ordinal measurements is sufficiently low, the\nordinal approach results in smaller errors in the estimation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 15:48:41 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Shah", "Nihar B.", ""], ["Balakrishnan", "Sivaraman", ""], ["Bradley", "Joseph", ""], ["Parekh", "Abhay", ""], ["Ramchandran", "Kannan", ""], ["Wainwright", "Martin", ""]]}, {"id": "1406.6633", "submitter": "Maria Florina Balcan", "authors": "Maria-Florina Balcan, Chris Berlind, Avrim Blum, Emma Cohen, Kaushik\n  Patnaik, and Le Song", "title": "Active Learning and Best-Response Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine an important setting for engineered systems in which low-power\ndistributed sensors are each making highly noisy measurements of some unknown\ntarget function. A center wants to accurately learn this function by querying a\nsmall number of sensors, which ordinarily would be impossible due to the high\nnoise rate. The question we address is whether local communication among\nsensors, together with natural best-response dynamics in an\nappropriately-defined game, can denoise the system without destroying the true\nsignal and allow the center to succeed from only a small number of active\nqueries. By using techniques from game theory and empirical processes, we prove\npositive (and negative) results on the denoising power of several natural\ndynamics. We then show experimentally that when combined with recent agnostic\nactive learning algorithms, this process can achieve low error from very few\nqueries, performing substantially better than active or passive learning\nwithout these denoising dynamics as well as passive learning with denoising.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 16:34:35 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Berlind", "Chris", ""], ["Blum", "Avrim", ""], ["Cohen", "Emma", ""], ["Patnaik", "Kaushik", ""], ["Song", "Le", ""]]}, {"id": "1406.6651", "submitter": "Ishanu Chattopadhyay", "authors": "Ishanu Chattopadhyay", "title": "Causality Networks", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While correlation measures are used to discern statistical relationships\nbetween observed variables in almost all branches of data-driven scientific\ninquiry, what we are really interested in is the existence of causal\ndependence. Designing an efficient causality test, that may be carried out in\nthe absence of restrictive pre-suppositions on the underlying dynamical\nstructure of the data at hand, is non-trivial. Nevertheless, ability to\ncomputationally infer statistical prima facie evidence of causal dependence may\nyield a far more discriminative tool for data analysis compared to the\ncalculation of simple correlations. In the present work, we present a new\nnon-parametric test of Granger causality for quantized or symbolic data streams\ngenerated by ergodic stationary sources. In contrast to state-of-art binary\ntests, our approach makes precise and computes the degree of causal dependence\nbetween data streams, without making any restrictive assumptions, linearity or\notherwise. Additionally, without any a priori imposition of specific dynamical\nstructure, we infer explicit generative models of causal cross-dependence,\nwhich may be then used for prediction. These explicit models are represented as\ngeneralized probabilistic automata, referred to crossed automata, and are shown\nto be sufficient to capture a fairly general class of causal dependence. The\nproposed algorithms are computationally efficient in the PAC sense; $i.e.$, we\nfind good models of cross-dependence with high probability, with polynomial\nrun-times and sample complexities. The theoretical results are applied to\nweekly search-frequency data from Google Trends API for a chosen set of\nsocially \"charged\" keywords. The causality network inferred from this dataset\nreveals, quite expectedly, the causal importance of certain keywords. It is\nalso illustrated that correlation analysis fails to gather such insight.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 17:46:32 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Chattopadhyay", "Ishanu", ""]]}, {"id": "1406.6720", "submitter": "Seyed Mostafa Kia", "authors": "Seyed Mostafa Kia", "title": "Mass-Univariate Hypothesis Testing on MEEG Data using Cross-Validation", "comments": "Master thesis, July 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in statistical theory, together with advances in the\ncomputational power of computers, provide alternative methods to do\nmass-univariate hypothesis testing in which a large number of univariate tests,\ncan be properly used to compare MEEG data at a large number of time-frequency\npoints and scalp locations. One of the major problematic aspects of this kind\nof mass-univariate analysis is due to high number of accomplished hypothesis\ntests. Hence procedures that remove or alleviate the increased probability of\nfalse discoveries are crucial for this type of analysis. Here, I propose a new\nmethod for mass-univariate analysis of MEEG data based on cross-validation\nscheme. In this method, I suggest a hierarchical classification procedure under\nk-fold cross-validation to detect which sensors at which time-bin and which\nfrequency-bin contributes in discriminating between two different stimuli or\ntasks. To achieve this goal, a new feature extraction method based on the\ndiscrete cosine transform (DCT) employed to get maximum advantage of all three\ndata dimensions. Employing cross-validation and hierarchy architecture\nalongside the DCT feature space makes this method more reliable and at the same\ntime enough sensitive to detect the narrow effects in brain activities.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 22:01:56 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Kia", "Seyed Mostafa", ""]]}, {"id": "1406.6812", "submitter": "Yasin Abbasi-Yadkori", "authors": "Yasin Abbasi-Yadkori and Gergely Neu", "title": "Online learning in MDPs with side information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online learning of finite Markov decision process (MDP) problems\nwhen a side information vector is available. The problem is motivated by\napplications such as clinical trials, recommendation systems, etc. Such\napplications have an episodic structure, where each episode corresponds to a\npatient/customer. Our objective is to compete with the optimal dynamic policy\nthat can take side information into account.\n  We propose a computationally efficient algorithm and show that its regret is\nat most $O(\\sqrt{T})$, where $T$ is the number of rounds. To best of our\nknowledge, this is the first regret bound for this setting.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 08:57:05 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Abbasi-Yadkori", "Yasin", ""], ["Neu", "Gergely", ""]]}, {"id": "1406.6909", "submitter": "Alexey Dosovitskiy", "authors": "Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin\n  Riedmiller and Thomas Brox", "title": "Discriminative Unsupervised Feature Learning with Exemplar Convolutional\n  Neural Networks", "comments": "PAMI submission. Includes matching experiments as in\n  arXiv:1405.5769v1. Also includes new network architectures, experiments on\n  Caltech-256, experiment on combining Exemplar-CNN with clustering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks have proven to be very successful in learning\ntask specific features that allow for unprecedented performance on various\ncomputer vision tasks. Training of such networks follows mostly the supervised\nlearning paradigm, where sufficiently many input-output pairs are required for\ntraining. Acquisition of large training sets is one of the key challenges, when\napproaching a new task. In this paper, we aim for generic feature learning and\npresent an approach for training a convolutional network using only unlabeled\ndata. To this end, we train the network to discriminate between a set of\nsurrogate classes. Each surrogate class is formed by applying a variety of\ntransformations to a randomly sampled 'seed' image patch. In contrast to\nsupervised network training, the resulting feature representation is not class\nspecific. It rather provides robustness to the transformations that have been\napplied during training. This generic feature representation allows for\nclassification results that outperform the state of the art for unsupervised\nlearning on several popular datasets (STL-10, CIFAR-10, Caltech-101,\nCaltech-256). While such generic features cannot compete with class specific\nfeatures from supervised training on a classification task, we show that they\nare advantageous on geometric matching problems, where they also outperform the\nSIFT descriptor.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 15:07:14 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 11:43:36 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Dosovitskiy", "Alexey", ""], ["Fischer", "Philipp", ""], ["Springenberg", "Jost Tobias", ""], ["Riedmiller", "Martin", ""], ["Brox", "Thomas", ""]]}, {"id": "1406.7002", "submitter": "Alireza Nejati", "authors": "Alireza Nejati, Charles Unsworth", "title": "A Concise Information-Theoretic Derivation of the Baum-Welch algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the Baum-Welch algorithm for hidden Markov models (HMMs) through an\ninformation-theoretical approach using cross-entropy instead of the Lagrange\nmultiplier approach which is universal in machine learning literature. The\nproposed approach provides a more concise derivation of the Baum-Welch method\nand naturally generalizes to multiple observations.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 09:09:29 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Nejati", "Alireza", ""], ["Unsworth", "Charles", ""]]}, {"id": "1406.7157", "submitter": "Shweta Jain", "authors": "Shweta Jain, Sujit Gujar, Satyanath Bhat, Onno Zoeter, Y. Narahari", "title": "An Incentive Compatible Multi-Armed-Bandit Crowdsourcing Mechanism with\n  Quality Assurance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a requester who wishes to crowdsource a series of identical binary\nlabeling tasks to a pool of workers so as to achieve an assured accuracy for\neach task, in a cost optimal way. The workers are heterogeneous with unknown\nbut fixed qualities and their costs are private. The problem is to select for\neach task an optimal subset of workers so that the outcome obtained from the\nselected workers guarantees a target accuracy level. The problem is a\nchallenging one even in a non strategic setting since the accuracy of\naggregated label depends on unknown qualities. We develop a novel multi-armed\nbandit (MAB) mechanism for solving this problem. First, we propose a framework,\nAssured Accuracy Bandit (AAB), which leads to an MAB algorithm, Constrained\nConfidence Bound for a Non Strategic setting (CCB-NS). We derive an upper bound\non the number of time steps the algorithm chooses a sub-optimal set that\ndepends on the target accuracy level and true qualities. A more challenging\nsituation arises when the requester not only has to learn the qualities of the\nworkers but also elicit their true costs. We modify the CCB-NS algorithm to\nobtain an adaptive exploration separated algorithm which we call { \\em\nConstrained Confidence Bound for a Strategic setting (CCB-S)}. CCB-S algorithm\nproduces an ex-post monotone allocation rule and thus can be transformed into\nan ex-post incentive compatible and ex-post individually rational mechanism\nthat learns the qualities of the workers and guarantees a given target accuracy\nlevel in a cost optimal way. We provide a lower bound on the number of times\nany algorithm should select a sub-optimal set and we see that the lower bound\nmatches our upper bound upto a constant factor. We provide insights on the\npractical implementation of this framework through an illustrative example and\nwe show the efficacy of our algorithms through simulations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 11:59:47 GMT"}, {"version": "v2", "created": "Thu, 9 Oct 2014 06:37:12 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2015 15:18:31 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Jain", "Shweta", ""], ["Gujar", "Sujit", ""], ["Bhat", "Satyanath", ""], ["Zoeter", "Onno", ""], ["Narahari", "Y.", ""]]}, {"id": "1406.7250", "submitter": "Shankar Vembu", "authors": "Amit G. Deshwar, Shankar Vembu, Christina K. Yung, Gun Ho Jang,\n  Lincoln Stein, Quaid Morris", "title": "Reconstructing subclonal composition and evolution from whole genome\n  sequencing of tumors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumors often contain multiple subpopulations of cancerous cells defined by\ndistinct somatic mutations. We describe a new method, PhyloWGS, that can be\napplied to WGS data from one or more tumor samples to reconstruct complete\ngenotypes of these subpopulations based on variant allele frequencies (VAFs) of\npoint mutations and population frequencies of structural variations. We\nintroduce a principled phylogenic correction for VAFs in loci affected by copy\nnumber alterations and we show that this correction greatly improves subclonal\nreconstruction compared to existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 18:01:20 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 19:24:52 GMT"}, {"version": "v3", "created": "Tue, 6 Jan 2015 22:05:57 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Deshwar", "Amit G.", ""], ["Vembu", "Shankar", ""], ["Yung", "Christina K.", ""], ["Jang", "Gun Ho", ""], ["Stein", "Lincoln", ""], ["Morris", "Quaid", ""]]}, {"id": "1406.7314", "submitter": "Imen Trabelsi", "authors": "Imen Trabelsi and Dorra Ben Ayed", "title": "On the Use of Different Feature Extraction Methods for Linear and Non\n  Linear kernels", "comments": "8 pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The speech feature extraction has been a key focus in robust speech\nrecognition research; it significantly affects the recognition performance. In\nthis paper, we first study a set of different features extraction methods such\nas linear predictive coding (LPC), mel frequency cepstral coefficient (MFCC)\nand perceptual linear prediction (PLP) with several features normalization\ntechniques like rasta filtering and cepstral mean subtraction (CMS). Based on\nthis, a comparative evaluation of these features is performed on the task of\ntext independent speaker identification using a combination between gaussian\nmixture models (GMM) and linear and non-linear kernels based on support vector\nmachine (SVM).\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 20:56:00 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Trabelsi", "Imen", ""], ["Ayed", "Dorra Ben", ""]]}, {"id": "1406.7330", "submitter": "Zhenming Liu", "authors": "Felix Ming Fai Wong, Zhenming Liu, Mung Chiang", "title": "Stock Market Prediction from WSJ: Text Mining via Sparse Matrix\n  Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of predicting directional movements of stock prices\nbased on news articles: here our algorithm uses daily articles from The Wall\nStreet Journal to predict the closing stock prices on the same day. We propose\na unified latent space model to characterize the \"co-movements\" between stock\nprices and news articles. Unlike many existing approaches, our new model is\nable to simultaneously leverage the correlations: (a) among stock prices, (b)\namong news articles, and (c) between stock prices and news articles. Thus, our\nmodel is able to make daily predictions on more than 500 stocks (most of which\nare not even mentioned in any news article) while having low complexity. We\ncarry out extensive backtesting on trading strategies based on our algorithm.\nThe result shows that our model has substantially better accuracy rate (55.7%)\ncompared to many widely used algorithms. The return (56%) and Sharpe ratio due\nto a trading strategy based on our model are also much higher than baseline\nindices.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 22:34:47 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Wong", "Felix Ming Fai", ""], ["Liu", "Zhenming", ""], ["Chiang", "Mung", ""]]}, {"id": "1406.7362", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho and Yoshua Bengio", "title": "Exponentially Increasing the Capacity-to-Computation Ratio for\n  Conditional Computation in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many state-of-the-art results obtained with deep networks are achieved with\nthe largest models that could be trained, and if more computation power was\navailable, we might be able to exploit much larger datasets in order to improve\ngeneralization ability. Whereas in learning algorithms such as decision trees\nthe ratio of capacity (e.g., the number of parameters) to computation is very\nfavorable (up to exponentially more parameters than computation), the ratio is\nessentially 1 for deep neural networks. Conditional computation has been\nproposed as a way to increase the capacity of a deep neural network without\nincreasing the amount of computation required, by activating some parameters\nand computation \"on-demand\", on a per-example basis. In this note, we propose a\nnovel parametrization of weight matrices in neural networks which has the\npotential to increase up to exponentially the ratio of the number of parameters\nto computation. The proposed approach is based on turning on some parameters\n(weight matrices) when specific bit patterns of hidden unit activations are\nobtained. In order to better control for the overfitting that might result, we\npropose a parametrization that is tree-structured, where each node of the tree\ncorresponds to a prefix of a sequence of sign bits, or gating units, associated\nwith hidden units.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 06:45:51 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1406.7424", "submitter": "Hiroki Sayama", "authors": "Andreas D. Pape, Kenneth J. Kurtz, Hiroki Sayama", "title": "Complexity Measures and Concept Learning", "comments": "27 pages, 7 tables, 1 figure. Accepted for publication in Journal of\n  Mathematical Psychology, in press", "journal-ref": "Journal of Mathematical Psychology, vol. 64-65, pp. 66-75, 2015", "doi": "10.1016/j.jmp.2015.01.001", "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nature of concept learning is a core question in cognitive science.\nTheories must account for the relative difficulty of acquiring different\nconcepts by supervised learners. For a canonical set of six category types, two\ndistinct orderings of classification difficulty have been found. One ordering,\nwhich we call paradigm-specific, occurs when adult human learners classify\nobjects with easily distinguishable characteristics such as size, shape, and\nshading. The general order occurs in all other known cases: when adult humans\nclassify objects with characteristics that are not readily distinguished (e.g.,\nbrightness, saturation, hue); for children and monkeys; and when categorization\ndifficulty is extrapolated from errors in identification learning. The\nparadigm-specific order was found to be predictable mathematically by measuring\nthe logical complexity of tasks, i.e., how concisely the solution can be\nrepresented by logical rules.\n  However, logical complexity explains only the paradigm-specific order but not\nthe general order. Here we propose a new difficulty measurement, information\ncomplexity, that calculates the amount of uncertainty remaining when a subset\nof the dimensions are specified. This measurement is based on Shannon entropy.\nWe show that, when the metric extracts minimal uncertainties, this new\nmeasurement predicts the paradigm-specific order for the canonical six category\ntypes, and when the metric extracts average uncertainties, this new measurement\npredicts the general order. Moreover, for learning category types beyond the\ncanonical six, we find that the minimal-uncertainty formulation correctly\npredicts the paradigm-specific order as well or better than existing metrics\n(Boolean complexity and GIST) in most cases.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 17:58:59 GMT"}, {"version": "v2", "created": "Wed, 19 Nov 2014 13:42:13 GMT"}, {"version": "v3", "created": "Fri, 23 Jan 2015 16:28:08 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Pape", "Andreas D.", ""], ["Kurtz", "Kenneth J.", ""], ["Sayama", "Hiroki", ""]]}, {"id": "1406.7429", "submitter": "Jonathan Katzman", "authors": "Jonathan Katzman and Diane Duros", "title": "Comparison of SVM Optimization Techniques in the Primal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the efficacy of different optimization techniques in a\nprimal formulation of a support vector machine (SVM). Three main techniques are\ncompared. The dataset used to compare all three techniques was the Sentiment\nAnalysis on Movie Reviews dataset, from kaggle.com.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 18:59:44 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Katzman", "Jonathan", ""], ["Duros", "Diane", ""]]}, {"id": "1406.7443", "submitter": "Zheng Wen", "authors": "Zheng Wen, Branislav Kveton, and Azin Ashkan", "title": "Efficient Learning in Large-Scale Combinatorial Semi-Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A stochastic combinatorial semi-bandit is an online learning problem where at\neach step a learning agent chooses a subset of ground items subject to\ncombinatorial constraints, and then observes stochastic weights of these items\nand receives their sum as a payoff. In this paper, we consider efficient\nlearning in large-scale combinatorial semi-bandits with linear generalization,\nand as a solution, propose two learning algorithms called Combinatorial Linear\nThompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB). Both\nalgorithms are computationally efficient as long as the offline version of the\ncombinatorial problem can be solved efficiently. We establish that CombLinTS\nand CombLinUCB are also provably statistically efficient under reasonable\nassumptions, by developing regret bounds that are independent of the problem\nscale (number of items) and sublinear in time. We also evaluate CombLinTS on a\nvariety of problems with thousands of items. Our experiment results demonstrate\nthat CombLinTS is scalable, robust to the choice of algorithm parameters, and\nsignificantly outperforms the best of our baselines.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 21:50:56 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 06:38:56 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2015 06:35:54 GMT"}, {"version": "v4", "created": "Tue, 31 Jan 2017 05:32:13 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Wen", "Zheng", ""], ["Kveton", "Branislav", ""], ["Ashkan", "Azin", ""]]}, {"id": "1406.7444", "submitter": "Christian J. Schuler", "authors": "Christian J. Schuler, Michael Hirsch, Stefan Harmeling, Bernhard\n  Sch\\\"olkopf", "title": "Learning to Deblur", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a learning-based approach to blind image deconvolution. It uses a\ndeep layered architecture, parts of which are borrowed from recent work on\nneural network learning, and parts of which incorporate computations that are\nspecific to image deconvolution. The system is trained end-to-end on a set of\nartificially generated training examples, enabling competitive performance in\nblind deconvolution, both with respect to quality and runtime.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 21:56:31 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Schuler", "Christian J.", ""], ["Hirsch", "Michael", ""], ["Harmeling", "Stefan", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1406.7445", "submitter": "Ni Lao", "authors": "Ni Lao, Jun Zhu", "title": "Contrastive Feature Induction for Efficient Structure Learning of\n  Conditional Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure learning of Conditional Random Fields (CRFs) can be cast into an\nL1-regularized optimization problem. To avoid optimizing over a fully linked\nmodel, gain-based or gradient-based feature selection methods start from an\nempty model and incrementally add top ranked features to it. However, for\nhigh-dimensional problems like statistical relational learning, training time\nof these incremental methods can be dominated by the cost of evaluating the\ngain or gradient of a large collection of candidate features. In this study we\npropose a fast feature evaluation algorithm called Contrastive Feature\nInduction (CFI), which only evaluates a subset of features that involve both\nvariables with high signals (deviation from mean) and variables with high\nerrors (residue). We prove that the gradient of candidate features can be\nrepresented solely as a function of signals and errors, and that CFI is an\nefficient approximation of gradient-based evaluation methods. Experiments on\nsynthetic and real data sets show competitive learning speed and accuracy of\nCFI on pairwise CRFs, compared to state-of-the-art structure learning methods\nsuch as full optimization over all features, and Grafting.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 22:13:52 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Lao", "Ni", ""], ["Zhu", "Jun", ""]]}, {"id": "1406.7447", "submitter": "Richard Combes", "authors": "Richard Combes and Alexandre Proutiere", "title": "Unimodal Bandits without Smoothness", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic bandit problems with a continuous set of arms and\nwhere the expected reward is a continuous and unimodal function of the arm. No\nfurther assumption is made regarding the smoothness and the structure of the\nexpected reward function. For these problems, we propose the Stochastic\nPentachotomy (SP) algorithm, and derive finite-time upper bounds on its regret\nand optimization error. In particular, we show that, for any expected reward\nfunction $\\mu$ that behaves as $\\mu(x)=\\mu(x^\\star)-C|x-x^\\star|^\\xi$ locally\naround its maximizer $x^\\star$ for some $\\xi, C>0$, the SP algorithm is\norder-optimal. Namely its regret and optimization error scale as\n$O(\\sqrt{T\\log(T)})$ and $O(\\sqrt{\\log(T)/T})$, respectively, when the time\nhorizon $T$ grows large. These scalings are achieved without the knowledge of\n$\\xi$ and $C$. Our algorithm is based on asymptotically optimal sequential\nstatistical tests used to successively trim an interval that contains the best\narm with high probability. To our knowledge, the SP algorithm constitutes the\nfirst sequential arm selection rule that achieves a regret and optimization\nerror scaling as $O(\\sqrt{T})$ and $O(1/\\sqrt{T})$, respectively, up to a\nlogarithmic factor for non-smooth expected reward functions, as well as for\nsmooth functions with unknown smoothness.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 23:45:30 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 13:24:33 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Combes", "Richard", ""], ["Proutiere", "Alexandre", ""]]}, {"id": "1406.7498", "submitter": "Aditya Gopalan", "authors": "Aditya Gopalan, Shie Mannor", "title": "Thompson Sampling for Learning Parameterized Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider reinforcement learning in parameterized Markov Decision Processes\n(MDPs), where the parameterization may induce correlation across transition\nprobabilities or rewards. Consequently, observing a particular state transition\nmight yield useful information about other, unobserved, parts of the MDP. We\npresent a version of Thompson sampling for parameterized reinforcement learning\nproblems, and derive a frequentist regret bound for priors over general\nparameter spaces. The result shows that the number of instants where suboptimal\nactions are chosen scales logarithmically with time, with high probability. It\nholds for prior distributions that put significant probability near the true\nmodel, without any additional, specific closed-form structure such as conjugate\nor product-form priors. The constant factor in the logarithmic scaling encodes\nthe information complexity of learning the MDP in terms of the Kullback-Leibler\ngeometry of the parameter space.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jun 2014 12:34:45 GMT"}, {"version": "v2", "created": "Fri, 20 Feb 2015 05:29:21 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2015 07:37:46 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Gopalan", "Aditya", ""], ["Mannor", "Shie", ""]]}, {"id": "1406.7758", "submitter": "Ziyu Wang", "authors": "Ziyu Wang, Nando de Freitas", "title": "Theoretical Analysis of Bayesian Optimisation with Unknown Gaussian\n  Process Hyper-Parameters", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimisation has gained great popularity as a tool for optimising\nthe parameters of machine learning algorithms and models. Somewhat ironically,\nsetting up the hyper-parameters of Bayesian optimisation methods is notoriously\nhard. While reasonable practical solutions have been advanced, they can often\nfail to find the best optima. Surprisingly, there is little theoretical\nanalysis of this crucial problem in the literature. To address this, we derive\na cumulative regret bound for Bayesian optimisation with Gaussian processes and\nunknown kernel hyper-parameters in the stochastic setting. The bound, which\napplies to the expected improvement acquisition function and sub-Gaussian\nobservation noise, provides us with guidelines on how to design hyper-parameter\nestimation methods. A simple simulation demonstrates the importance of\nfollowing these guidelines.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 14:35:58 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Wang", "Ziyu", ""], ["de Freitas", "Nando", ""]]}, {"id": "1406.7806", "submitter": "Andrew Maas", "authors": "Andrew L. Maas, Peng Qi, Ziang Xie, Awni Y. Hannun, Christopher T.\n  Lengerich, Daniel Jurafsky and Andrew Y. Ng", "title": "Building DNN Acoustic Models for Large Vocabulary Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are now a central component of nearly all\nstate-of-the-art speech recognition systems. Building neural network acoustic\nmodels requires several design decisions including network architecture, size,\nand training loss function. This paper offers an empirical investigation on\nwhich aspects of DNN acoustic model design are most important for speech\nrecognition system performance. We report DNN classifier performance and final\nspeech recognizer word error rates, and compare DNNs using several metrics to\nquantify factors influencing differences in task performance. Our first set of\nexperiments use the standard Switchboard benchmark corpus, which contains\napproximately 300 hours of conversational telephone speech. We compare standard\nDNNs to convolutional networks, and present the first experiments using\nlocally-connected, untied neural networks for acoustic modeling. We\nadditionally build systems on a corpus of 2,100 hours of training data by\ncombining the Switchboard and Fisher corpora. This larger corpus allows us to\nmore thoroughly examine performance of large DNN models -- with up to ten times\nmore parameters than those typically used in speech recognition systems. Our\nresults suggest that a relatively simple DNN architecture and optimization\ntechnique produces strong results. These findings, along with previous work,\nhelp establish a set of best practices for building DNN hybrid speech\nrecognition systems with maximum likelihood training. Our experiments in DNN\noptimization additionally serve as a case study for training DNNs with\ndiscriminative loss functions for speech tasks, as well as DNN classifiers more\ngenerally.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 16:42:25 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 07:44:15 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Maas", "Andrew L.", ""], ["Qi", "Peng", ""], ["Xie", "Ziang", ""], ["Hannun", "Awni Y.", ""], ["Lengerich", "Christopher T.", ""], ["Jurafsky", "Daniel", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1406.7842", "submitter": "Xiaowen Dong", "authors": "Xiaowen Dong, Dorina Thanou, Pascal Frossard, Pierre Vandergheynst", "title": "Learning Laplacian Matrix in Smooth Graph Signal Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of a meaningful graph plays a crucial role in the success of\nmany graph-based representations and algorithms for handling structured data,\nespecially in the emerging field of graph signal processing. However, a\nmeaningful graph is not always readily available from the data, nor easy to\ndefine depending on the application domain. In particular, it is often\ndesirable in graph signal processing applications that a graph is chosen such\nthat the data admit certain regularity or smoothness on the graph. In this\npaper, we address the problem of learning graph Laplacians, which is equivalent\nto learning graph topologies, such that the input data form graph signals with\nsmooth variations on the resulting topology. To this end, we adopt a factor\nanalysis model for the graph signals and impose a Gaussian probabilistic prior\non the latent variables that control these signals. We show that the Gaussian\nprior leads to an efficient representation that favors the smoothness property\nof the graph signals. We then propose an algorithm for learning graphs that\nenforces such property and is based on minimizing the variations of the signals\non the learned graph. Experiments on both synthetic and real world data\ndemonstrate that the proposed graph learning framework can efficiently infer\nmeaningful graph topologies from signal observations under the smoothness\nprior.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 18:33:59 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 14:07:03 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2016 22:12:47 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Dong", "Xiaowen", ""], ["Thanou", "Dorina", ""], ["Frossard", "Pascal", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1406.7865", "submitter": "Antonio Sutera", "authors": "Antonio Sutera, Arnaud Joly, Vincent Fran\\c{c}ois-Lavet, Zixiao Aaron\n  Qiu, Gilles Louppe, Damien Ernst and Pierre Geurts", "title": "Simple connectome inference from partial correlation statistics in\n  calcium imaging", "comments": null, "journal-ref": "JMLR: Workshop and Conference Proceedings 46:23-35, 2015", "doi": "10.1007/978-3-319-53070-3_2", "report-no": null, "categories": "stat.ML cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a simple yet effective solution to the problem of\nconnectome inference in calcium imaging data. The proposed algorithm consists\nof two steps. First, processing the raw signals to detect neural peak\nactivities. Second, inferring the degree of association between neurons from\npartial correlation statistics. This paper summarises the methodology that led\nus to win the Connectomics Challenge, proposes a simplified version of our\nmethod, and finally compares our results with respect to other inference\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 19:34:23 GMT"}, {"version": "v2", "created": "Tue, 1 Jul 2014 07:55:27 GMT"}, {"version": "v3", "created": "Tue, 29 Jul 2014 12:36:32 GMT"}, {"version": "v4", "created": "Tue, 18 Nov 2014 14:18:42 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Sutera", "Antonio", ""], ["Joly", "Arnaud", ""], ["Fran\u00e7ois-Lavet", "Vincent", ""], ["Qiu", "Zixiao Aaron", ""], ["Louppe", "Gilles", ""], ["Ernst", "Damien", ""], ["Geurts", "Pierre", ""]]}]