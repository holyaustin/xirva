[{"id": "0704.0671", "submitter": "Maxim Raginsky", "authors": "Maxim Raginsky", "title": "Learning from compressed observations", "comments": "6 pages; submitted to the 2007 IEEE Information Theory Workshop (ITW\n  2007)", "journal-ref": null, "doi": "10.1109/ITW.2007.4313111", "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": null, "abstract": "  The problem of statistical learning is to construct a predictor of a random\nvariable $Y$ as a function of a related random variable $X$ on the basis of an\ni.i.d. training sample from the joint distribution of $(X,Y)$. Allowable\npredictors are drawn from some specified class, and the goal is to approach\nasymptotically the performance (expected loss) of the best predictor in the\nclass. We consider the setting in which one has perfect observation of the\n$X$-part of the sample, while the $Y$-part has to be communicated at some\nfinite bit rate. The encoding of the $Y$-values is allowed to depend on the\n$X$-values. Under suitable regularity conditions on the admissible predictors,\nthe underlying family of probability distributions and the loss function, we\ngive an information-theoretic characterization of achievable predictor\nperformance in terms of conditional distortion-rate functions. The ideas are\nillustrated on the example of nonparametric regression in Gaussian noise.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2007 02:57:15 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Raginsky", "Maxim", ""]]}, {"id": "0704.0954", "submitter": "Jos\\'e M. F. Moura", "authors": "Soummya Kar and Jose M. F. Moura", "title": "Sensor Networks with Random Links: Topology Design for Distributed\n  Consensus", "comments": "Submitted to IEEE Transactions", "journal-ref": null, "doi": "10.1109/TSP.2008.920143", "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": null, "abstract": "  In a sensor network, in practice, the communication among sensors is subject\nto:(1) errors or failures at random times; (3) costs; and(2) constraints since\nsensors and networks operate under scarce resources, such as power, data rate,\nor communication. The signal-to-noise ratio (SNR) is usually a main factor in\ndetermining the probability of error (or of communication failure) in a link.\nThese probabilities are then a proxy for the SNR under which the links operate.\nThe paper studies the problem of designing the topology, i.e., assigning the\nprobabilities of reliable communication among sensors (or of link failures) to\nmaximize the rate of convergence of average consensus, when the link\ncommunication costs are taken into account, and there is an overall\ncommunication budget constraint. To consider this problem, we address a number\nof preliminary issues: (1) model the network as a random topology; (2)\nestablish necessary and sufficient conditions for mean square sense (mss) and\nalmost sure (a.s.) convergence of average consensus when network links fail;\nand, in particular, (3) show that a necessary and sufficient condition for both\nmss and a.s. convergence is for the algebraic connectivity of the mean graph\ndescribing the network topology to be strictly positive. With these results, we\nformulate topology design, subject to random link failures and to a\ncommunication cost constraint, as a constrained convex optimization problem to\nwhich we apply semidefinite programming techniques. We show by an extensive\nnumerical study that the optimal design improves significantly the convergence\nspeed of the consensus algorithm and can achieve the asymptotic performance of\na non-random network at a fraction of the communication cost.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2007 21:58:52 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Kar", "Soummya", ""], ["Moura", "Jose M. F.", ""]]}, {"id": "0704.1020", "submitter": "Gyorgy Ottucsak", "authors": "Andras Gyorgy, Tamas Linder, Gabor Lugosi, Gyorgy Ottucsak", "title": "The on-line shortest path problem under partial monitoring", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SC", "license": null, "abstract": "  The on-line shortest path problem is considered under various models of\npartial monitoring. Given a weighted directed acyclic graph whose edge weights\ncan change in an arbitrary (adversarial) way, a decision maker has to choose in\neach round of a game a path between two distinguished vertices such that the\nloss of the chosen path (defined as the sum of the weights of its composing\nedges) be as small as possible. In a setting generalizing the multi-armed\nbandit problem, after choosing a path, the decision maker learns only the\nweights of those edges that belong to the chosen path. For this problem, an\nalgorithm is given whose average cumulative loss in n rounds exceeds that of\nthe best path, matched off-line to the entire sequence of the edge weights, by\na quantity that is proportional to 1/\\sqrt{n} and depends only polynomially on\nthe number of edges of the graph. The algorithm can be implemented with linear\ncomplexity in the number of rounds n and in the number of edges. An extension\nto the so-called label efficient setting is also given, in which the decision\nmaker is informed about the weights of the edges corresponding to the chosen\npath at a total of m << n time instances. Another extension is shown where the\ndecision maker competes against a time-varying path, a generalization of the\nproblem of tracking the best expert. A version of the multi-armed bandit\nsetting for shortest path is also discussed where the decision maker learns\nonly the total weight of the chosen path but not the weights of the individual\nedges on the path. Applications to routing in packet switched networks along\nwith simulation results are also presented.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2007 10:15:54 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Gyorgy", "Andras", ""], ["Linder", "Tamas", ""], ["Lugosi", "Gabor", ""], ["Ottucsak", "Gyorgy", ""]]}, {"id": "0704.1028", "submitter": "Jianlin Cheng", "authors": "Jianlin Cheng", "title": "A neural network approach to ordinal regression", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": null, "abstract": "  Ordinal regression is an important type of learning, which has properties of\nboth classification and regression. Here we describe a simple and effective\napproach to adapt a traditional neural network to learn ordinal categories. Our\napproach is a generalization of the perceptron method for ordinal regression.\nOn several benchmark datasets, our method (NNRank) outperforms a neural network\nclassification method. Compared with the ordinal regression methods using\nGaussian processes and support vector machines, NNRank achieves comparable\nperformance. Moreover, NNRank has the advantages of traditional neural\nnetworks: learning in both online and batch modes, handling very large training\ndatasets, and making rapid predictions. These features make NNRank a useful and\ncomplementary tool for large-scale data processing tasks such as information\nretrieval, web page ranking, collaborative filtering, and protein ranking in\nBioinformatics.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2007 17:36:00 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Cheng", "Jianlin", ""]]}, {"id": "0704.1274", "submitter": "Dev Rajnarayan", "authors": "David H. Wolpert and Dev G. Rajnarayan", "title": "Parametric Learning and Monte Carlo Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": null, "abstract": "  This paper uncovers and explores the close relationship between Monte Carlo\nOptimization of a parametrized integral (MCO), Parametric machine-Learning\n(PL), and `blackbox' or `oracle'-based optimization (BO). We make four\ncontributions. First, we prove that MCO is mathematically identical to a broad\nclass of PL problems. This identity potentially provides a new application\ndomain for all broadly applicable PL techniques: MCO. Second, we introduce\nimmediate sampling, a new version of the Probability Collectives (PC) algorithm\nfor blackbox optimization. Immediate sampling transforms the original BO\nproblem into an MCO problem. Accordingly, by combining these first two\ncontributions, we can apply all PL techniques to BO. In our third contribution\nwe validate this way of improving BO by demonstrating that cross-validation and\nbagging improve immediate sampling. Finally, conventional MC and MCO procedures\nignore the relationship between the sample point locations and the associated\nvalues of the integrand; only the values of the integrand at those locations\nare considered. We demonstrate that one can exploit the sample location\ninformation using PL techniques, for example by forming a fit of the sample\nlocations to the associated values of the integrand. This provides an\nadditional way to apply PL techniques to improve MCO.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2007 17:01:07 GMT"}], "update_date": "2011-11-09", "authors_parsed": [["Wolpert", "David H.", ""], ["Rajnarayan", "Dev G.", ""]]}, {"id": "0704.1409", "submitter": "Yao Hengshuai", "authors": "Yao HengShuai", "title": "Preconditioned Temporal Difference Learning", "comments": "This paper has been withdrawn by the author. Look at the ICML version\n  instead: http://icml2008.cs.helsinki.fi/papers/111.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": null, "abstract": "  This paper has been withdrawn by the author. This draft is withdrawn for its\npoor quality in english, unfortunately produced by the author when he was just\nstarting his science route. Look at the ICML version instead:\nhttp://icml2008.cs.helsinki.fi/papers/111.pdf\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2007 13:17:01 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2007 03:33:26 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2012 14:08:19 GMT"}], "update_date": "2012-06-11", "authors_parsed": [["HengShuai", "Yao", ""]]}, {"id": "0704.2092", "submitter": "Jinsong Tan", "authors": "Jinsong Tan", "title": "A Note on the Inapproximability of Correlation Clustering", "comments": null, "journal-ref": "Information Processing Letters, 108: 331-335, 2008", "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inapproximability of the correlation clustering problem defined\nas follows: Given a graph $G = (V,E)$ where each edge is labeled either \"+\"\n(similar) or \"-\" (dissimilar), correlation clustering seeks to partition the\nvertices into clusters so that the number of pairs correctly (resp.\nincorrectly) classified with respect to the labels is maximized (resp.\nminimized). The two complementary problems are called MaxAgree and MinDisagree,\nrespectively, and have been studied on complete graphs, where every edge is\nlabeled, and general graphs, where some edge might not have been labeled.\nNatural edge-weighted versions of both problems have been studied as well. Let\nS-MaxAgree denote the weighted problem where all weights are taken from set S,\nwe show that S-MaxAgree with weights bounded by $O(|V|^{1/2-\\delta})$\nessentially belongs to the same hardness class in the following sense: if there\nis a polynomial time algorithm that approximates S-MaxAgree within a factor of\n$\\lambda = O(\\log{|V|})$ with high probability, then for any choice of S',\nS'-MaxAgree can be approximated in polynomial time within a factor of $(\\lambda\n+ \\epsilon)$, where $\\epsilon > 0$ can be arbitrarily small, with high\nprobability. A similar statement also holds for $S-MinDisagree. This result\nimplies it is hard (assuming $NP \\neq RP$) to approximate unweighted MaxAgree\nwithin a factor of $80/79-\\epsilon$, improving upon a previous known factor of\n$116/115-\\epsilon$ by Charikar et. al. \\cite{Chari05}.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2007 03:52:41 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2009 03:22:02 GMT"}], "update_date": "2009-03-23", "authors_parsed": [["Tan", "Jinsong", ""]]}, {"id": "0704.2644", "submitter": "Maxim Raginsky", "authors": "Maxim Raginsky", "title": "Joint universal lossy coding and identification of stationary mixing\n  sources", "comments": "5 pages, 1 eps figure; to appear in Proc. ISIT 2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": null, "abstract": "  The problem of joint universal source coding and modeling, treated in the\ncontext of lossless codes by Rissanen, was recently generalized to fixed-rate\nlossy coding of finitely parametrized continuous-alphabet i.i.d. sources. We\nextend these results to variable-rate lossy block coding of stationary ergodic\nsources and show that, for bounded metric distortion measures, any finitely\nparametrized family of stationary sources satisfying suitable mixing,\nsmoothness and Vapnik-Chervonenkis learnability conditions admits universal\nschemes for joint lossy source coding and identification. We also give several\nexplicit examples of parametric sources satisfying the regularity conditions.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2007 01:25:22 GMT"}], "update_date": "2007-07-13", "authors_parsed": [["Raginsky", "Maxim", ""]]}, {"id": "0704.2668", "submitter": "Alex Smola J", "authors": "Le Song, Alex Smola, Arthur Gretton, Karsten Borgwardt, Justin Bedo", "title": "Supervised Feature Selection via Dependence Estimation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": null, "abstract": "  We introduce a framework for filtering features that employs the\nHilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence\nbetween the features and the labels. The key idea is that good features should\nmaximise such dependence. Feature selection for various supervised learning\nproblems (including classification and regression) is unified under this\nframework, and the solutions can be approximated using a backward-elimination\nalgorithm. We demonstrate the usefulness of our method on both artificial and\nreal world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2007 08:26:29 GMT"}], "update_date": "2007-05-23", "authors_parsed": [["Song", "Le", ""], ["Smola", "Alex", ""], ["Gretton", "Arthur", ""], ["Borgwardt", "Karsten", ""], ["Bedo", "Justin", ""]]}]