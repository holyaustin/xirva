[{"id": "0806.1156", "submitter": "Cnrs : Umr 6057 Laboratoire Parole Et Langage", "authors": "Irina Nesterenko (LPL), St\\'ephane Rauzy (LPL)", "title": "Utilisation des grammaires probabilistes dans les t\\^aches de\n  segmentation et d'annotation prosodique", "comments": null, "journal-ref": "Journ\\'ees d'Etudes sur la Parole, Avignon : France (2008)", "doi": null, "report-no": "3267", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nous pr\\'esentons dans cette contribution une approche \\`a la fois symbolique\net probabiliste permettant d'extraire l'information sur la segmentation du\nsignal de parole \\`a partir d'information prosodique. Nous utilisons pour ce\nfaire des grammaires probabilistes poss\\'edant une structure hi\\'erarchique\nminimale. La phase de construction des grammaires ainsi que leur pouvoir de\npr\\'ediction sont \\'evalu\\'es qualitativement ainsi que quantitativement.\n  -----\n  Methodologically oriented, the present work sketches an approach for prosodic\ninformation retrieval and speech segmentation, based on both symbolic and\nprobabilistic information. We have recourse to probabilistic grammars, within\nwhich we implement a minimal hierarchical structure. Both the stages of\nprobabilistic grammar building and its testing in prediction are explored and\nquantitatively and qualitatively evaluated.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2008 13:33:31 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Nesterenko", "Irina", "", "LPL"], ["Rauzy", "St\u00e9phane", "", "LPL"]]}, {"id": "0806.1199", "submitter": "Lukas Kroc", "authors": "Michael Chertkov, Lukas Kroc, Massimo Vergassola", "title": "Belief Propagation and Beyond for Particle Tracking", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": "LA-UR-08-3645", "categories": "cs.IT cond-mat.stat-mech cs.AI cs.LG math.IT physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel approach to statistical learning from particles tracked\nwhile moving in a random environment. The problem consists in inferring\nproperties of the environment from recorded snapshots. We consider here the\ncase of a fluid seeded with identical passive particles that diffuse and are\nadvected by a flow. Our approach rests on efficient algorithms to estimate the\nweighted number of possible matchings among particles in two consecutive\nsnapshots, the partition function of the underlying graphical model. The\npartition function is then maximized over the model parameters, namely\ndiffusivity and velocity gradient. A Belief Propagation (BP) scheme is the\nbackbone of our algorithm, providing accurate results for the flow parameters\nwe want to learn. The BP estimate is additionally improved by incorporating\nLoop Series (LS) contributions. For the weighted matching problem, LS is\ncompactly expressed as a Cauchy integral, accurately estimated by a saddle\npoint approximation. Numerical experiments show that the quality of our\nimproved BP algorithm is comparable to the one of a fully polynomial randomized\napproximation scheme, based on the Markov Chain Monte Carlo (MCMC) method,\nwhile the BP-based scheme is substantially faster than the MCMC scheme.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2008 16:18:13 GMT"}], "update_date": "2008-06-09", "authors_parsed": [["Chertkov", "Michael", ""], ["Kroc", "Lukas", ""], ["Vergassola", "Massimo", ""]]}, {"id": "0806.2850", "submitter": "Nick Costiris J.", "authors": "N. J. Costiris, E. Mavrommatis, K. A. Gernoth, J. W. Clark", "title": "Decoding Beta-Decay Systematics: A Global Statistical Model for Beta^-\n  Halflives", "comments": "20 pages, 19 figures", "journal-ref": "Phys.Rev.C80:044332,2009", "doi": "10.1103/PhysRevC.80.044332", "report-no": null, "categories": "nucl-th astro-ph cond-mat.dis-nn cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical modeling of nuclear data provides a novel approach to nuclear\nsystematics complementary to established theoretical and phenomenological\napproaches based on quantum theory. Continuing previous studies in which global\nstatistical modeling is pursued within the general framework of machine\nlearning theory, we implement advances in training algorithms designed to\nimproved generalization, in application to the problem of reproducing and\npredicting the halflives of nuclear ground states that decay 100% by the beta^-\nmode. More specifically, fully-connected, multilayer feedforward artificial\nneural network models are developed using the Levenberg-Marquardt optimization\nalgorithm together with Bayesian regularization and cross-validation. The\npredictive performance of models emerging from extensive computer experiments\nis compared with that of traditional microscopic and phenomenological models as\nwell as with the performance of other learning systems, including earlier\nneural network models as well as the support vector machines recently applied\nto the same problem. In discussing the results, emphasis is placed on\npredictions for nuclei that are far from the stability line, and especially\nthose involved in the r-process nucleosynthesis. It is found that the new\nstatistical models can match or even surpass the predictive performance of\nconventional models for beta-decay systematics and accordingly should provide a\nvaluable additional tool for exploring the expanding nuclear landscape.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2008 18:23:15 GMT"}], "update_date": "2009-11-06", "authors_parsed": [["Costiris", "N. J.", ""], ["Mavrommatis", "E.", ""], ["Gernoth", "K. A.", ""], ["Clark", "J. W.", ""]]}, {"id": "0806.2890", "submitter": "Julian McAuley", "authors": "Tiberio S. Caetano, Julian J. McAuley, Li Cheng, Quoc V. Le and Alex\n  J. Smola", "title": "Learning Graph Matching", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a fundamental problem in pattern recognition, graph matching has\napplications in a variety of fields, from computer vision to computational\nbiology. In graph matching, patterns are modeled as graphs and pattern\nrecognition amounts to finding a correspondence between the nodes of different\ngraphs. Many formulations of this problem can be cast in general as a quadratic\nassignment problem, where a linear term in the objective function encodes node\ncompatibility and a quadratic term encodes edge compatibility. The main\nresearch focus in this theme is about designing efficient algorithms for\napproximately solving the quadratic assignment problem, since it is NP-hard. In\nthis paper we turn our attention to a different question: how to estimate\ncompatibility functions such that the solution of the resulting graph matching\nproblem best matches the expected solution that a human would manually provide.\nWe present a method for learning graph matching: the training examples are\npairs of graphs and the `labels' are matches between them. Our experimental\nresults reveal that learning can substantially improve the performance of\nstandard graph matching algorithms. In particular, we find that simple linear\nassignment with such a learning scheme outperforms Graduated Assignment with\nbistochastic normalisation, a state-of-the-art quadratic assignment relaxation\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2008 23:28:08 GMT"}], "update_date": "2008-06-19", "authors_parsed": [["Caetano", "Tiberio S.", ""], ["McAuley", "Julian J.", ""], ["Cheng", "Li", ""], ["Le", "Quoc V.", ""], ["Smola", "Alex J.", ""]]}, {"id": "0806.3537", "submitter": "David Soloveichik", "authors": "David Soloveichik", "title": "Statistical Learning of Arbitrary Computable Classifiers", "comments": "Expanded the section on prior work and added references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical learning theory chiefly studies restricted hypothesis classes,\nparticularly those with finite Vapnik-Chervonenkis (VC) dimension. The\nfundamental quantity of interest is the sample complexity: the number of\nsamples required to learn to a specified level of accuracy. Here we consider\nlearning over the set of all computable labeling functions. Since the\nVC-dimension is infinite and a priori (uniform) bounds on the number of samples\nare impossible, we let the learning algorithm decide when it has seen\nsufficient samples to have learned. We first show that learning in this setting\nis indeed possible, and develop a learning algorithm. We then show, however,\nthat bounding sample complexity independently of the distribution is\nimpossible. Notably, this impossibility is entirely due to the requirement that\nthe learning algorithm be computable, and not due to the statistical nature of\nthe problem.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2008 01:28:14 GMT"}, {"version": "v2", "created": "Thu, 10 Jul 2008 02:51:05 GMT"}], "update_date": "2008-07-10", "authors_parsed": [["Soloveichik", "David", ""]]}, {"id": "0806.4210", "submitter": "Jan Arpe", "authors": "Jan Arpe and Elchanan Mossel", "title": "Agnostically Learning Juntas from Random Walks", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the class of functions g:{-1,+1}^n -> {-1,+1} that only depend\non an unknown subset of k<<n variables (so-called k-juntas) is agnostically\nlearnable from a random walk in time polynomial in n, 2^{k^2}, epsilon^{-k},\nand log(1/delta). In other words, there is an algorithm with the claimed\nrunning time that, given epsilon, delta > 0 and access to a random walk on\n{-1,+1}^n labeled by an arbitrary function f:{-1,+1}^n -> {-1,+1}, finds with\nprobability at least 1-delta a k-junta that is (opt(f)+epsilon)-close to f,\nwhere opt(f) denotes the distance of a closest k-junta to f.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2008 23:18:44 GMT"}], "update_date": "2008-06-27", "authors_parsed": [["Arpe", "Jan", ""], ["Mossel", "Elchanan", ""]]}, {"id": "0806.4341", "submitter": "Vladimir Vyugin", "authors": "Vladimir V. V'yugin", "title": "On Sequences with Non-Learnable Subsequences", "comments": null, "journal-ref": "LNCS 5010, pp. 302-313, 2008", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The remarkable results of Foster and Vohra was a starting point for a series\nof papers which show that any sequence of outcomes can be learned (with no\nprior knowledge) using some universal randomized forecasting algorithm and\nforecast-dependent checking rules. We show that for the class of all\ncomputationally efficient outcome-forecast-based checking rules, this property\nis violated. Moreover, we present a probabilistic algorithm generating with\nprobability close to one a sequence with a subsequence which simultaneously\nmiscalibrates all partially weakly computable randomized forecasting\nalgorithms. %subsequences non-learnable by each randomized algorithm.\n  According to the Dawid's prequential framework we consider partial recursive\nrandomized algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2008 15:21:00 GMT"}], "update_date": "2008-06-27", "authors_parsed": [["V'yugin", "Vladimir V.", ""]]}, {"id": "0806.4391", "submitter": "Vladimir Vyugin", "authors": "Vladimir V. V'yugin", "title": "Prediction with Expert Advice in Games with Unbounded One-Step Gains", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The games of prediction with expert advice are considered in this paper. We\npresent some modification of Kalai and Vempala algorithm of following the\nperturbed leader for the case of unrestrictedly large one-step gains. We show\nthat in general case the cumulative gain of any probabilistic prediction\nalgorithm can be much worse than the gain of some expert of the pool.\nNevertheless, we give the lower bound for this cumulative gain in general case\nand construct a universal algorithm which has the optimal performance; we also\nprove that in case when one-step gains of experts of the pool have ``limited\ndeviations'' the performance of our algorithm is close to the performance of\nthe best expert.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2008 20:21:06 GMT"}], "update_date": "2008-06-30", "authors_parsed": [["V'yugin", "Vladimir V.", ""]]}, {"id": "0806.4422", "submitter": "Ping Li", "authors": "Ping Li", "title": "Computationally Efficient Estimators for Dimension Reductions Using\n  Stable Random Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of stable random projections is a tool for efficiently computing\nthe $l_\\alpha$ distances using low memory, where $0<\\alpha \\leq 2$ is a tuning\nparameter. The method boils down to a statistical estimation task and various\nestimators have been proposed, based on the geometric mean, the harmonic mean,\nand the fractional power etc.\n  This study proposes the optimal quantile estimator, whose main operation is\nselecting, which is considerably less expensive than taking fractional power,\nthe main operation in previous estimators. Our experiments report that the\noptimal quantile estimator is nearly one order of magnitude more\ncomputationally efficient than previous estimators. For large-scale learning\ntasks in which storing and computing pairwise distances is a serious\nbottleneck, this estimator should be desirable.\n  In addition to its computational advantages, the optimal quantile estimator\nexhibits nice theoretical properties. It is more accurate than previous\nestimators when $\\alpha>1$. We derive its theoretical error bounds and\nestablish the explicit (i.e., no hidden constants) sample complexity bound.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2008 05:19:19 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "0806.4423", "submitter": "Ping Li", "authors": "Ping Li", "title": "On Approximating the Lp Distances for p>2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications in machine learning and data mining require computing pairwise\nLp distances in a data matrix A. For massive high-dimensional data, computing\nall pairwise distances of A can be infeasible. In fact, even storing A or all\npairwise distances of A in the memory may be also infeasible. This paper\nproposes a simple method for p = 2, 4, 6, ... We first decompose the l_p (where\np is even) distances into a sum of 2 marginal norms and p-1 ``inner products''\nat different orders. Then we apply normal or sub-Gaussian random projections to\napproximate the resultant ``inner products,'' assuming that the marginal norms\ncan be computed exactly by a linear scan. We propose two strategies for\napplying random projections. The basic projection strategy requires only one\nprojection matrix but it is more difficult to analyze, while the alternative\nprojection strategy requires p-1 projection matrices but its theoretical\nanalysis is much easier. In terms of the accuracy, at least for p=4, the basic\nstrategy is always more accurate than the alternative strategy if the data are\nnon-negative, which is common in reality.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2008 05:36:09 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "0806.4484", "submitter": "Vladimir Vyugin", "authors": "Vladimir V'yugin", "title": "On empirical meaning of randomness with respect to a real parameter", "comments": "14 pages", "journal-ref": "LNCS 4649, pp. 387-396, 2007", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the empirical meaning of randomness with respect to a family of\nprobability distributions $P_\\theta$, where $\\theta$ is a real parameter, using\nalgorithmic randomness theory. In the case when for a computable probability\ndistribution $P_\\theta$ an effectively strongly consistent estimate exists, we\nshow that the Levin's a priory semicomputable semimeasure of the set of all\n$P_\\theta$-random sequences is positive if and only if the parameter $\\theta$\nis a computable real number. The different methods for generating\n``meaningful'' $P_\\theta$-random sequences with noncomputable $\\theta$ are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2008 10:49:33 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2009 20:07:47 GMT"}], "update_date": "2009-06-25", "authors_parsed": [["V'yugin", "Vladimir", ""]]}, {"id": "0806.4686", "submitter": "Tong Zhang", "authors": "John Langford, Lihong Li, Tong Zhang", "title": "Sparse Online Learning via Truncated Gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general method called truncated gradient to induce sparsity in\nthe weights of online learning algorithms with convex loss functions. This\nmethod has several essential properties: The degree of sparsity is continuous\n-- a parameter controls the rate of sparsification from no sparsification to\ntotal sparsification. The approach is theoretically motivated, and an instance\nof it can be regarded as an online counterpart of the popular\n$L_1$-regularization method in the batch setting. We prove that small rates of\nsparsification result in only small additional regret with respect to typical\nonline learning guarantees. The approach works well empirically. We apply the\napproach to several datasets and find that for datasets with large numbers of\nfeatures, substantial sparsity is discoverable.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2008 14:19:50 GMT"}, {"version": "v2", "created": "Fri, 4 Jul 2008 01:58:32 GMT"}], "update_date": "2008-07-04", "authors_parsed": [["Langford", "John", ""], ["Li", "Lihong", ""], ["Zhang", "Tong", ""]]}]