[{"id": "1101.0255", "submitter": "Reza Hosseini", "authors": "Reza Hosseini", "title": "Conditional information and definition of neighbor in categorical random\n  fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the definition of neighbor in Markov random fields as defined by\nBesag (1974) when the joint distribution of the sites is not positive is not\nwell-defined. In a random field with finite number of sites we study the\nconditions under which giving the value at extra sites will change the belief\nof an agent about one site. Also the conditions under which the information\nfrom some sites is equivalent to giving the value at all other sites is\nstudied. These concepts provide an alternative to the concept of neighbor for\ngeneral case where the positivity condition of the joint does not hold.\n", "versions": [{"version": "v1", "created": "Fri, 31 Dec 2010 13:33:14 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Hosseini", "Reza", ""]]}, {"id": "1101.0428", "submitter": "Michael Fairbank Mr", "authors": "Michael Fairbank and Eduardo Alonso", "title": "The Local Optimality of Reinforcement Learning by Value Gradients, and\n  its Relationship to Policy Gradient Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this theoretical paper we are concerned with the problem of learning a\nvalue function by a smooth general function approximator, to solve a\ndeterministic episodic control problem in a large continuous state space. It is\nshown that learning the gradient of the value-function at every point along a\ntrajectory generated by a greedy policy is a sufficient condition for the\ntrajectory to be locally extremal, and often locally optimal, and we argue that\nthis brings greater efficiency to value-function learning. This contrasts to\ntraditional value-function learning in which the value-function must be learnt\nover the whole of state space.\n  It is also proven that policy-gradient learning applied to a greedy policy on\na value-function produces a weight update equivalent to a value-gradient weight\nupdate, which provides a surprising connection between these two alternative\nparadigms of reinforcement learning, and a convergence proof for control\nproblems with a value function represented by a general smooth function\napproximator.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jan 2011 20:20:27 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Fairbank", "Michael", ""], ["Alonso", "Eduardo", ""]]}, {"id": "1101.1057", "submitter": "Sebastien Gerchinovitz", "authors": "S\\'ebastien Gerchinovitz (DMA, INRIA Paris - Rocquencourt)", "title": "Sparsity regret bounds for individual sequences in online linear\n  regression", "comments": "Published in Journal of Machine Learning Research at\n  http://www.jmlr.org/papers/volume14/gerchinovitz13a/gerchinovitz13a.pdf", "journal-ref": "Journal of Machine Learning Research 14 (2011) 729-769", "doi": null, "report-no": "RR-7504", "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online linear regression on arbitrary\ndeterministic sequences when the ambient dimension d can be much larger than\nthe number of time rounds T. We introduce the notion of sparsity regret bound,\nwhich is a deterministic online counterpart of recent risk bounds derived in\nthe stochastic setting under a sparsity scenario. We prove such regret bounds\nfor an online-learning algorithm called SeqSEW and based on exponential\nweighting and data-driven truncation. In a second part we apply a\nparameter-free version of this algorithm to the stochastic setting (regression\nmodel with random design). This yields risk bounds of the same flavor as in\nDalalyan and Tsybakov (2011) but which solve two questions left open therein.\nIn particular our risk bounds are adaptive (up to a logarithmic factor) to the\nunknown variance of the noise if the latter is Gaussian. We also address the\nregression model with fixed design.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jan 2011 19:43:37 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2012 06:52:57 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2013 10:33:39 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Gerchinovitz", "S\u00e9bastien", "", "DMA, INRIA Paris - Rocquencourt"]]}, {"id": "1101.2320", "submitter": "Llu\\'is Belanche-Mu\\~noz", "authors": "L.A. Belanche and F.F. Gonz\\'alez", "title": "Review and Evaluation of Feature Selection Algorithms in Synthetic\n  Problems", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purpose of Feature Subset Selection is to find a reduced subset of\nattributes from a data set described by a feature set. The task of a feature\nselection algorithm (FSA) is to provide with a computational solution motivated\nby a certain definition of relevance or by a reliable evaluation measure. In\nthis paper several fundamental algorithms are studied to assess their\nperformance in a controlled experimental scenario. A measure to evaluate FSAs\nis devised that computes the degree of matching between the output given by a\nFSA and the known optimal solutions. An extensive experimental study on\nsynthetic problems is carried out to assess the behaviour of the algorithms in\nterms of solution accuracy and size as a function of the relevance,\nirrelevance, redundancy and size of the data samples. The controlled\nexperimental conditions facilitate the derivation of better-supported and\nmeaningful conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jan 2011 10:49:51 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Belanche", "L. A.", ""], ["Gonz\u00e1lez", "F. F.", ""]]}, {"id": "1101.2987", "submitter": "Mahesh  Pal Dr.", "authors": "Mahesh Pal", "title": "Support vector machines/relevance vector machine for remote sensing\n  classification: A review", "comments": "19 pages", "journal-ref": "Proceeding of the Workshop on Application of advanced soft\n  computing Techniques in Geo-spatial Data Analysis. Department of Civil\n  Engineering, IIT Bombay, Sept. 22-23,2008, 211-227", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based machine learning algorithms are based on mapping data from the\noriginal input feature space to a kernel feature space of higher dimensionality\nto solve a linear problem in that space. Over the last decade, kernel based\nclassification and regression approaches such as support vector machines have\nwidely been used in remote sensing as well as in various civil engineering\napplications. In spite of their better performance with different datasets,\nsupport vector machines still suffer from shortcomings such as\nvisualization/interpretation of model, choice of kernel and kernel specific\nparameter as well as the regularization parameter. Relevance vector machines\nare another kernel based approach being explored for classification and\nregression with in last few years. The advantages of the relevance vector\nmachines over the support vector machines is the availability of probabilistic\npredictions, using arbitrary kernel functions and not requiring setting of the\nregularization parameter. This paper presents a state-of-the-art review of SVM\nand RVM in remote sensing and provides some details of their use in other civil\nengineering application also.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jan 2011 13:29:12 GMT"}], "update_date": "2011-01-18", "authors_parsed": [["Pal", "Mahesh", ""]]}, {"id": "1101.3594", "submitter": "Donghui Yan", "authors": "Donghui Yan, Peng Gong, Aiyou Chen and Liheng Zhong", "title": "Classification under Data Contamination with Application to Remote\n  Sensing Image Mis-registration", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by the problem of image mis-registration in remote\nsensing and we are interested in determining the resulting loss in the accuracy\nof pattern classification. A statistical formulation is given where we propose\nto use data contamination to model and understand the phenomenon of image\nmis-registration. This model is widely applicable to many other types of errors\nas well, for example, measurement errors and gross errors etc. The impact of\ndata contamination on classification is studied under a statistical learning\ntheoretical framework. A closed-form asymptotic bound is established for the\nresulting loss in classification accuracy, which is less than\n$\\epsilon/(1-\\epsilon)$ for data contamination of an amount of $\\epsilon$. Our\nbound is sharper than similar bounds in the domain adaptation literature and,\nunlike such bounds, it applies to classifiers with an infinite\nVapnik-Chervonekis (VC) dimension. Extensive simulations have been conducted on\nboth synthetic and real datasets under various types of data contamination,\nincluding label flipping, feature swapping and the replacement of feature\nvalues with data generated from a random source such as a Gaussian or Cauchy\ndistribution. Our simulation results show that the bound we derive is fairly\ntight.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jan 2011 00:41:43 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2012 18:04:10 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Yan", "Donghui", ""], ["Gong", "Peng", ""], ["Chen", "Aiyou", ""], ["Zhong", "Liheng", ""]]}, {"id": "1101.4003", "submitter": "Jose Antonio Martin H.", "authors": "Matilde Santos, Jose Antonio Martin H., Victoria Lopez and Guillermo\n  Botella", "title": "Dyna-H: a heuristic planning reinforcement learning algorithm applied to\n  role-playing-game strategy decision systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a Role-Playing Game, finding optimal trajectories is one of the most\nimportant tasks. In fact, the strategy decision system becomes a key component\nof a game engine. Determining the way in which decisions are taken (online,\nbatch or simulated) and the consumed resources in decision making (e.g.\nexecution time, memory) will influence, in mayor degree, the game performance.\nWhen classical search algorithms such as A* can be used, they are the very\nfirst option. Nevertheless, such methods rely on precise and complete models of\nthe search space, and there are many interesting scenarios where their\napplication is not possible. Then, model free methods for sequential decision\nmaking under uncertainty are the best choice. In this paper, we propose a\nheuristic planning strategy to incorporate the ability of heuristic-search in\npath-finding into a Dyna agent. The proposed Dyna-H algorithm, as A* does,\nselects branches more likely to produce outcomes than other branches. Besides,\nit has the advantages of being a model-free online reinforcement learning\nalgorithm. The proposal was evaluated against the one-step Q-Learning and\nDyna-Q algorithms obtaining excellent experimental results: Dyna-H\nsignificantly overcomes both methods in all experiments. We suggest also, a\nfunctional analogy between the proposed sampling from worst trajectories\nheuristic and the role of dreams (e.g. nightmares) in human behavior.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jan 2011 19:51:58 GMT"}, {"version": "v2", "created": "Thu, 10 Feb 2011 18:12:32 GMT"}, {"version": "v3", "created": "Sat, 30 Jul 2011 09:56:22 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Santos", "Matilde", ""], ["H.", "Jose Antonio Martin", ""], ["Lopez", "Victoria", ""], ["Botella", "Guillermo", ""]]}, {"id": "1101.4170", "submitter": "Jean-Marc Lasgouttes", "authors": "Victorin Martin and Jean-Marc Lasgouttes and Cyril Furtlehner", "title": "The Role of Normalization in the Belief Propagation Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important part of problems in statistical physics and computer science can\nbe expressed as the computation of marginal probabilities over a Markov Random\nField. The belief propagation algorithm, which is an exact procedure to compute\nthese marginals when the underlying graph is a tree, has gained its popularity\nas an efficient way to approximate them in the more general case. In this\npaper, we focus on an aspect of the algorithm that did not get that much\nattention in the literature, which is the effect of the normalization of the\nmessages. We show in particular that, for a large class of normalization\nstrategies, it is possible to focus only on belief convergence. Following this,\nwe express the necessary and sufficient conditions for local stability of a\nfixed point in terms of the graph structure and the beliefs values at the fixed\npoint. We also explicit some connexion between the normalization constants and\nthe underlying Bethe Free Energy.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jan 2011 16:11:05 GMT"}], "update_date": "2011-01-24", "authors_parsed": [["Martin", "Victorin", ""], ["Lasgouttes", "Jean-Marc", ""], ["Furtlehner", "Cyril", ""]]}, {"id": "1101.4227", "submitter": "Aram Galstyan", "authors": "Greg Ver Steeg, Aram Galstyan, Armen E. Allahverdyan", "title": "Statistical Mechanics of Semi-Supervised Clustering in Sparse Graphs", "comments": "8 pages, 4 figures", "journal-ref": "J. Stat. Mech. (2011) P08009", "doi": "10.1088/1742-5468/2011/08/P08009", "report-no": null, "categories": "physics.data-an cond-mat.dis-nn cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We theoretically study semi-supervised clustering in sparse graphs in the\npresence of pairwise constraints on the cluster assignments of nodes. We focus\non bi-cluster graphs, and study the impact of semi-supervision for varying\nconstraint density and overlap between the clusters. Recent results for\nunsupervised clustering in sparse graphs indicate that there is a critical\nratio of within-cluster and between-cluster connectivities below which clusters\ncannot be recovered with better than random accuracy. The goal of this paper is\nto examine the impact of pairwise constraints on the clustering accuracy. Our\nresults suggests that the addition of constraints does not provide automatic\nimprovement over the unsupervised case. When the density of the constraints is\nsufficiently small, their only impact is to shift the detection threshold while\npreserving the criticality. Conversely, if the density of (hard) constraints is\nabove the percolation threshold, the criticality is suppressed and the\ndetection threshold disappears.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jan 2011 20:37:31 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2011 18:50:19 GMT"}, {"version": "v3", "created": "Mon, 31 Oct 2011 03:46:11 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""], ["Allahverdyan", "Armen E.", ""]]}, {"id": "1101.4388", "submitter": "Guohui Song", "authors": "Guohui Song, Haizhang Zhang, Fred J. Hickernell", "title": "Reproducing Kernel Banach Spaces with the l1 Norm", "comments": "28 pages, an extra section was added", "journal-ref": "Appl. Comput. Harmon. Anal., 34:96-116, 2013", "doi": "10.1016/j.acha.2012.03.009", "report-no": null, "categories": "stat.ML cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Targeting at sparse learning, we construct Banach spaces B of functions on an\ninput space X with the properties that (1) B possesses an l1 norm in the sense\nthat it is isometrically isomorphic to the Banach space of integrable functions\non X with respect to the counting measure; (2) point evaluations are continuous\nlinear functionals on B and are representable through a bilinear form with a\nkernel function; (3) regularized learning schemes on B satisfy the linear\nrepresenter theorem. Examples of kernel functions admissible for the\nconstruction of such spaces are given.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jan 2011 16:57:03 GMT"}, {"version": "v2", "created": "Wed, 26 Jan 2011 04:56:10 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2012 18:46:47 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Song", "Guohui", ""], ["Zhang", "Haizhang", ""], ["Hickernell", "Fred J.", ""]]}, {"id": "1101.4439", "submitter": "Haizhang Zhang", "authors": "Guohui Song, Haizhang Zhang", "title": "Reproducing Kernel Banach Spaces with the l1 Norm II: Error Analysis for\n  Regularized Least Square Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical approach in estimating the learning rate of a regularized learning\nscheme is to bound the approximation error by the sum of the sampling error,\nthe hypothesis error and the regularization error. Using a reproducing kernel\nspace that satisfies the linear representer theorem brings the advantage of\ndiscarding the hypothesis error from the sum automatically. Following this\ndirection, we illustrate how reproducing kernel Banach spaces with the l1 norm\ncan be applied to improve the learning rate estimate of l1-regularization in\nmachine learning.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jan 2011 03:39:57 GMT"}, {"version": "v2", "created": "Thu, 27 Jan 2011 14:45:29 GMT"}], "update_date": "2011-01-28", "authors_parsed": [["Song", "Guohui", ""], ["Zhang", "Haizhang", ""]]}, {"id": "1101.4681", "submitter": "Zizhuo Wang", "authors": "Zizhuo Wang, Shiming Deng and Yinyu Ye", "title": "Close the Gaps: A Learning-while-Doing Algorithm for a Class of\n  Single-Product Revenue Management Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a retailer selling a single product with limited on-hand\ninventory over a finite selling season. Customer demand arrives according to a\nPoisson process, the rate of which is influenced by a single action taken by\nthe retailer (such as price adjustment, sales commission, advertisement\nintensity, etc.). The relationship between the action and the demand rate is\nnot known in advance. However, the retailer is able to learn the optimal action\n\"on the fly\" as she maximizes her total expected revenue based on the observed\ndemand reactions.\n  Using the pricing problem as an example, we propose a dynamic\n\"learning-while-doing\" algorithm that only involves function value estimation\nto achieve a near-optimal performance. Our algorithm employs a series of\nshrinking price intervals and iteratively tests prices within that interval\nusing a set of carefully chosen parameters. We prove that the convergence rate\nof our algorithm is among the fastest of all possible algorithms in terms of\nasymptotic \"regret\" (the relative loss comparing to the full information\noptimal solution). Our result closes the performance gaps between parametric\nand non-parametric learning and between a post-price mechanism and a\ncustomer-bidding mechanism. Important managerial insight from this research is\nthat the values of information on both the parametric form of the demand\nfunction as well as each customer's exact reservation price are less important\nthan prior literature suggests. Our results also suggest that firms would be\nbetter off to perform dynamic learning and action concurrently rather than\nsequentially.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jan 2011 22:12:37 GMT"}, {"version": "v2", "created": "Wed, 26 Jan 2011 01:03:17 GMT"}, {"version": "v3", "created": "Thu, 27 Jan 2011 19:41:03 GMT"}, {"version": "v4", "created": "Fri, 28 Jan 2011 05:15:22 GMT"}, {"version": "v5", "created": "Mon, 21 Jan 2013 18:19:33 GMT"}, {"version": "v6", "created": "Thu, 27 Jun 2013 00:48:11 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["Wang", "Zizhuo", ""], ["Deng", "Shiming", ""], ["Ye", "Yinyu", ""]]}, {"id": "1101.4749", "submitter": "Osman  G\\\"unay", "authors": "Osman Gunay and Behcet Ugur Toreyin and Kivanc Kose and A. Enis Cetin", "title": "Online Adaptive Decision Fusion Framework Based on Entropic Projections\n  onto Convex Sets with Application to Wildfire Detection in Video", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": "10.1117/1.3595426", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an Entropy functional based online Adaptive Decision Fusion\n(EADF) framework is developed for image analysis and computer vision\napplications. In this framework, it is assumed that the compound algorithm\nconsists of several sub-algorithms each of which yielding its own decision as a\nreal number centered around zero, representing the confidence level of that\nparticular sub-algorithm. Decision values are linearly combined with weights\nwhich are updated online according to an active fusion method based on\nperforming entropic projections onto convex sets describing sub-algorithms. It\nis assumed that there is an oracle, who is usually a human operator, providing\nfeedback to the decision fusion method. A video based wildfire detection system\nis developed to evaluate the performance of the algorithm in handling the\nproblems where data arrives sequentially. In this case, the oracle is the\nsecurity guard of the forest lookout tower verifying the decision of the\ncombined algorithm. Simulation results are presented. The EADF framework is\nalso tested with a standard dataset.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jan 2011 09:11:49 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Gunay", "Osman", ""], ["Toreyin", "Behcet Ugur", ""], ["Kose", "Kivanc", ""], ["Cetin", "A. Enis", ""]]}, {"id": "1101.4752", "submitter": "Matus Telgarsky", "authors": "Matus Telgarsky", "title": "A Primal-Dual Convergence Analysis of Boosting", "comments": "40 pages, 8 figures; the NIPS 2011 submission \"The Fast Convergence\n  of Boosting\" is a brief presentation of the primary results; compared with\n  the JMLR version, this arXiv version has hyperref and some formatting tweaks", "journal-ref": "Journal of Machine Learning Research, 13:561-606, 2012", "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting combines weak learners into a predictor with low empirical risk. Its\ndual constructs a high entropy distribution upon which weak learners and\ntraining labels are uncorrelated. This manuscript studies this primal-dual\nrelationship under a broad family of losses, including the exponential loss of\nAdaBoost and the logistic loss, revealing:\n  - Weak learnability aids the whole loss family: for any {\\epsilon}>0,\nO(ln(1/{\\epsilon})) iterations suffice to produce a predictor with empirical\nrisk {\\epsilon}-close to the infimum;\n  - The circumstances granting the existence of an empirical risk minimizer may\nbe characterized in terms of the primal and dual problems, yielding a new proof\nof the known rate O(ln(1/{\\epsilon}));\n  - Arbitrary instances may be decomposed into the above two, granting rate\nO(1/{\\epsilon}), with a matching lower bound provided for the logistic loss.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jan 2011 09:18:46 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2011 05:45:48 GMT"}, {"version": "v3", "created": "Mon, 2 Apr 2012 22:59:53 GMT"}], "update_date": "2012-04-04", "authors_parsed": [["Telgarsky", "Matus", ""]]}, {"id": "1101.4918", "submitter": "Ridwan Al Iqbal", "authors": "Ridwan Al Iqbal", "title": "Using Feature Weights to Improve Performance of Neural Networks", "comments": "2 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different features have different relevance to a particular learning problem.\nSome features are less relevant; while some very important. Instead of\nselecting the most relevant features using feature selection, an algorithm can\nbe given this knowledge of feature importance based on expert opinion or prior\nlearning. Learning can be faster and more accurate if learners take feature\nimportance into account. Correlation aided Neural Networks (CANN) is presented\nwhich is such an algorithm. CANN treats feature importance as the correlation\ncoefficient between the target attribute and the features. CANN modifies normal\nfeed-forward Neural Network to fit both correlation values and training data.\nEmpirical evaluation shows that CANN is faster and more accurate than applying\nthe two step approach of feature selection and then using normal learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jan 2011 20:24:25 GMT"}], "update_date": "2011-01-26", "authors_parsed": [["Iqbal", "Ridwan Al", ""]]}, {"id": "1101.4924", "submitter": "Ridwan Al Iqbal", "authors": "Ridwan Al Iqbal", "title": "A Generalized Method for Integrating Rule-based Knowledge into Inductive\n  Methods Through Virtual Sample Creation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid learning methods use theoretical knowledge of a domain and a set of\nclassified examples to develop a method for classification. Methods that use\ndomain knowledge have been shown to perform better than inductive learners.\nHowever, there is no general method to include domain knowledge into all\ninductive learning algorithms as all hybrid methods are highly specialized for\na particular algorithm. We present an algorithm that will take domain knowledge\nin the form of propositional rules, generate artificial examples from the rules\nand also remove instances likely to be flawed. This enriched dataset then can\nbe used by any learning algorithm. Experimental results of different scenarios\nare shown that demonstrate this method to be more effective than simple\ninductive learning.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jan 2011 20:42:01 GMT"}], "update_date": "2011-01-26", "authors_parsed": [["Iqbal", "Ridwan Al", ""]]}, {"id": "1101.5039", "submitter": "Alireza Nowroozi", "authors": "Mohammadreza Abolghasemi-Dahaghani, Farzad Didehvar (1), Alireza\n  Nowroozi", "title": "A Novel Template-Based Learning Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a model which is capable of learning and abstracting\nnew concepts based on comparing observations and finding the resemblance\nbetween the observations. In the model, the new observations are compared with\nthe templates which have been derived from the previous experiences. In the\nfirst stage, the objects are first represented through a geometric description\nwhich is used for finding the object boundaries and a descriptor which is\ninspired by the human visual system and then they are fed into the model. Next,\nthe new observations are identified through comparing them with the\npreviously-learned templates and are used for producing new templates. The\ncomparisons are made based on measures like Euclidean or correlation distance.\nThe new template is created by applying onion-pealing algorithm. The algorithm\nconsecutively uses convex hulls which are made by the points representing the\nobjects. If the new observation is remarkably similar to one of the observed\ncategories, it is no longer utilized in creating a new template. The existing\ntemplates are used to provide a description of the new observation. This\ndescription is provided in the templates space. Each template represents a\ndimension of the feature space. The degree of the resemblance each template\nbears to each object indicates the value associated with the object in that\ndimension of the templates space. In this way, the description of the new\nobservation becomes more accurate and detailed as the time passes and the\nexperiences increase. We have used this model for learning and recognizing the\nnew polygons in the polygon space. Representing the polygons was made possible\nthrough employing a geometric method and a method inspired by human visual\nsystem. Various implementations of the model have been compared. The evaluation\nresults of the model prove its efficiency in learning and deriving new\ntemplates.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jan 2011 12:21:13 GMT"}], "update_date": "2011-01-27", "authors_parsed": [["Abolghasemi-Dahaghani", "Mohammadreza", ""], ["Didehvar", "Farzad", ""], ["Nowroozi", "Alireza", ""]]}, {"id": "1101.5097", "submitter": "Morten M{\\o}rup", "authors": "Morten M{\\o}rup, Mikkel N. Schmidt, Lars Kai Hansen", "title": "Infinite Multiple Membership Relational Modeling for Complex Networks", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning latent structure in complex networks has become an important problem\nfueled by many types of networked data originating from practically all fields\nof science. In this paper, we propose a new non-parametric Bayesian\nmultiple-membership latent feature model for networks. Contrary to existing\nmultiple-membership models that scale quadratically in the number of vertices\nthe proposed model scales linearly in the number of links admitting\nmultiple-membership analysis in large scale networks. We demonstrate a\nconnection between the single membership relational model and multiple\nmembership models and show on \"real\" size benchmark network data that\naccounting for multiple memberships improves the learning of latent structure\nas measured by link prediction while explicitly accounting for multiple\nmembership result in a more compact representation of the latent structure of\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jan 2011 16:15:22 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["M\u00f8rup", "Morten", ""], ["Schmidt", "Mikkel N.", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1101.5141", "submitter": "Francisco Aparecido Rodrigues", "authors": "Francisco A. Rodrigues, Guilherme Ferraz de Arruda, Luciano da\n  Fontoura Costa", "title": "A Complex Networks Approach for Data Clustering", "comments": "9 pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many methods have been developed for data clustering, such as k-means,\nexpectation maximization and algorithms based on graph theory. In this latter\ncase, graphs are generally constructed by taking into account the Euclidian\ndistance as a similarity measure, and partitioned using spectral methods.\nHowever, these methods are not accurate when the clusters are not well\nseparated. In addition, it is not possible to automatically determine the\nnumber of clusters. These limitations can be overcome by taking into account\nnetwork community identification algorithms. In this work, we propose a\nmethodology for data clustering based on complex networks theory. We compare\ndifferent metrics for quantifying the similarity between objects and take into\naccount three community finding techniques. This approach is applied to two\nreal-world databases and to two sets of artificially generated data. By\ncomparing our method with traditional clustering approaches, we verify that the\nproximity measures given by the Chebyshev and Manhattan distances are the most\nsuitable metrics to quantify the similarity between objects. In addition, the\ncommunity identification method based on the greedy optimization provides the\nsmallest misclassification rates.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jan 2011 19:58:58 GMT"}], "update_date": "2011-01-27", "authors_parsed": [["Rodrigues", "Francisco A.", ""], ["de Arruda", "Guilherme Ferraz", ""], ["Costa", "Luciano da Fontoura", ""]]}, {"id": "1101.5632", "submitter": "Kian Hsiang Low", "authors": "Kian Hsiang Low, John M. Dolan, and Pradeep Khosla", "title": "Active Markov Information-Theoretic Path Planning for Robotic\n  Environmental Sensing", "comments": "10th International Conference on Autonomous Agents and Multiagent\n  Systems (AAMAS 2011), Extended version with proofs, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in multi-robot exploration and mapping has focused on\nsampling environmental fields, which are typically modeled using the Gaussian\nprocess (GP). Existing information-theoretic exploration strategies for\nlearning GP-based environmental field maps adopt the non-Markovian problem\nstructure and consequently scale poorly with the length of history of\nobservations. Hence, it becomes computationally impractical to use these\nstrategies for in situ, real-time active sampling. To ease this computational\nburden, this paper presents a Markov-based approach to efficient\ninformation-theoretic path planning for active sampling of GP-based fields. We\nanalyze the time complexity of solving the Markov-based path planning problem,\nand demonstrate analytically that it scales better than that of deriving the\nnon-Markovian strategies with increasing length of planning horizon. For a\nclass of exploration tasks called the transect sampling task, we provide\ntheoretical guarantees on the active sampling performance of our Markov-based\npolicy, from which ideal environmental field conditions and sampling task\nsettings can be established to limit its performance degradation due to\nviolation of the Markov assumption. Empirical evaluation on real-world\ntemperature and plankton density field data shows that our Markov-based policy\ncan generally achieve active sampling performance comparable to that of the\nwidely-used non-Markovian greedy policies under less favorable realistic field\nconditions and task settings while enjoying significant computational gain over\nthem.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jan 2011 21:27:31 GMT"}], "update_date": "2011-02-01", "authors_parsed": [["Low", "Kian Hsiang", ""], ["Dolan", "John M.", ""], ["Khosla", "Pradeep", ""]]}, {"id": "1101.5672", "submitter": "John Wright", "authors": "Quan Geng and Huan Wang and John Wright", "title": "On the Local Correctness of L^1 Minimization for Dictionary Learning", "comments": "37 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea that many important classes of signals can be well-represented by\nlinear combinations of a small set of atoms selected from a given dictionary\nhas had dramatic impact on the theory and practice of signal processing. For\npractical problems in which an appropriate sparsifying dictionary is not known\nahead of time, a very popular and successful heuristic is to search for a\ndictionary that minimizes an appropriate sparsity surrogate over a given set of\nsample data. While this idea is appealing, the behavior of these algorithms is\nlargely a mystery; although there is a body of empirical evidence suggesting\nthey do learn very effective representations, there is little theory to\nguarantee when they will behave correctly, or when the learned dictionary can\nbe expected to generalize. In this paper, we take a step towards such a theory.\nWe show that under mild hypotheses, the dictionary learning problem is locally\nwell-posed: the desired solution is indeed a local minimum of the $\\ell^1$\nnorm. Namely, if $\\mb A \\in \\Re^{m \\times n}$ is an incoherent (and possibly\novercomplete) dictionary, and the coefficients $\\mb X \\in \\Re^{n \\times p}$\nfollow a random sparse model, then with high probability $(\\mb A,\\mb X)$ is a\nlocal minimum of the $\\ell^1$ norm over the manifold of factorizations $(\\mb\nA',\\mb X')$ satisfying $\\mb A' \\mb X' = \\mb Y$, provided the number of samples\n$p = \\Omega(n^3 k)$. For overcomplete $\\mb A$, this is the first result showing\nthat the dictionary learning problem is locally solvable. Our analysis draws on\ntools developed for the problem of completing a low-rank matrix from a small\nsubset of its entries, which allow us to overcome a number of technical\nobstacles; in particular, the absence of the restricted isometry property.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jan 2011 06:06:56 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Geng", "Quan", ""], ["Wang", "Huan", ""], ["Wright", "John", ""]]}, {"id": "1101.5785", "submitter": "Guoshen Yu", "authors": "Guoshen Yu and Guillermo Sapiro", "title": "Statistical Compressed Sensing of Gaussian Mixture Models", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2011.2168521", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A novel framework of compressed sensing, namely statistical compressed\nsensing (SCS), that aims at efficiently sampling a collection of signals that\nfollow a statistical distribution, and achieving accurate reconstruction on\naverage, is introduced. SCS based on Gaussian models is investigated in depth.\nFor signals that follow a single Gaussian model, with Gaussian or Bernoulli\nsensing matrices of O(k) measurements, considerably smaller than the O(k\nlog(N/k)) required by conventional CS based on sparse models, where N is the\nsignal dimension, and with an optimal decoder implemented via linear filtering,\nsignificantly faster than the pursuit decoders applied in conventional CS, the\nerror of SCS is shown tightly upper bounded by a constant times the best k-term\napproximation error, with overwhelming probability. The failure probability is\nalso significantly smaller than that of conventional sparsity-oriented CS.\nStronger yet simpler results further show that for any sensing matrix, the\nerror of Gaussian SCS is upper bounded by a constant times the best k-term\napproximation with probability one, and the bound constant can be efficiently\ncalculated. For Gaussian mixture models (GMMs), that assume multiple Gaussian\ndistributions and that each signal follows one of them with an unknown index, a\npiecewise linear estimator is introduced to decode SCS. The accuracy of model\nselection, at the heart of the piecewise linear decoder, is analyzed in terms\nof the properties of the Gaussian distributions and the number of sensing\nmeasurements. A maximum a posteriori expectation-maximization algorithm that\niteratively estimates the Gaussian models parameters, the signals model\nselection, and decodes the signals, is presented for GMM-based SCS. In real\nimage sensing applications, GMM-based SCS is shown to lead to improved results\ncompared to conventional CS, at a considerably lower computational cost.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jan 2011 17:16:55 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Yu", "Guoshen", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1101.5805", "submitter": "Matteo Riondato", "authors": "Matteo Riondato, Mert Akdere, Ugur Cetintemel, Stanley B. Zdonik, Eli\n  Upfal", "title": "The VC-Dimension of Queries and Selectivity Estimation Through Sampling", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel method, based on the statistical concept of the\nVapnik-Chervonenkis dimension, to evaluate the selectivity (output cardinality)\nof SQL queries - a crucial step in optimizing the execution of large scale\ndatabase and data-mining operations. The major theoretical contribution of this\nwork, which is of independent interest, is an explicit bound to the\nVC-dimension of a range space defined by all possible outcomes of a collection\n(class) of queries. We prove that the VC-dimension is a function of the maximum\nnumber of Boolean operations in the selection predicate and of the maximum\nnumber of select and join operations in any individual query in the collection,\nbut it is neither a function of the number of queries in the collection nor of\nthe size (number of tuples) of the database. We leverage on this result and\ndevelop a method that, given a class of queries, builds a concise random sample\nof a database, such that with high probability the execution of any query in\nthe class on the sample provides an accurate estimate for the selectivity of\nthe query on the original large database. The error probability holds\nsimultaneously for the selectivity estimates of all queries in the collection,\nthus the same sample can be used to evaluate the selectivity of multiple\nqueries, and the sample needs to be refreshed only following major changes in\nthe database. The sample representation computed by our method is typically\nsufficiently small to be stored in main memory. We present extensive\nexperimental results, validating our theoretical analysis and demonstrating the\nadvantage of our technique when compared to complex selectivity estimation\ntechniques used in PostgreSQL and the Microsoft SQL Server.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jan 2011 19:13:43 GMT"}, {"version": "v2", "created": "Wed, 2 Feb 2011 15:57:00 GMT"}, {"version": "v3", "created": "Thu, 11 Aug 2011 20:56:03 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Riondato", "Matteo", ""], ["Akdere", "Mert", ""], ["Cetintemel", "Ugur", ""], ["Zdonik", "Stanley B.", ""], ["Upfal", "Eli", ""]]}]