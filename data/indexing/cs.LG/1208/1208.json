[{"id": "1208.0129", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal, Peter L. Bartlett, John C. Duchi", "title": "Oracle inequalities for computationally adaptive model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze general model selection procedures using penalized empirical loss\nminimization under computational constraints. While classical model selection\napproaches do not consider computational aspects of performing model selection,\nwe argue that any practical model selection procedure must not only trade off\nestimation and approximation error, but also the computational effort required\nto compute empirical minimizers for different function classes. We provide a\nframework for analyzing such problems, and we give algorithms for model\nselection under a computational budget. These algorithms satisfy oracle\ninequalities that show that the risk of the selected model is not much worse\nthan if we had devoted all of our omputational budget to the optimal function\nclass.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 07:57:53 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Agarwal", "Alekh", ""], ["Bartlett", "Peter L.", ""], ["Duchi", "John C.", ""]]}, {"id": "1208.0378", "submitter": "Charless Fowlkes", "authors": "Julian Yarkony, Alexander T. Ihler, Charless C. Fowlkes", "title": "Fast Planar Correlation Clustering for Image Segmentation", "comments": "This is the extended version of a paper to appear at the 12th\n  European Conference on Computer Vision (ECCV 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new optimization scheme for finding high-quality correlation\nclusterings in planar graphs that uses weighted perfect matching as a\nsubroutine. Our method provides lower-bounds on the energy of the optimal\ncorrelation clustering that are typically fast to compute and tight in\npractice. We demonstrate our algorithm on the problem of image segmentation\nwhere this approach outperforms existing global optimization techniques in\nminimizing the objective and is competitive with the state of the art in\nproducing high-quality segmentations.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2012 00:54:02 GMT"}], "update_date": "2012-08-03", "authors_parsed": [["Yarkony", "Julian", ""], ["Ihler", "Alexander T.", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1208.0402", "submitter": "Yun Jiang", "authors": "Yun Jiang, Marcus Lim and Ashutosh Saxena", "title": "Multidimensional Membership Mixture Models", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the multidimensional membership mixture (M3) models where every\ndimension of the membership represents an independent mixture model and each\ndata point is generated from the selected mixture components jointly. This is\nhelpful when the data has a certain shared structure. For example, three unique\nmeans and three unique variances can effectively form a Gaussian mixture model\nwith nine components, while requiring only six parameters to fully describe it.\nIn this paper, we present three instantiations of M3 models (together with the\nlearning and inference algorithms): infinite, finite, and hybrid, depending on\nwhether the number of mixtures is fixed or not. They are built upon Dirichlet\nprocess mixture models, latent Dirichlet allocation, and a combination\nrespectively. We then consider two applications: topic modeling and learning 3D\nobject arrangements. Our experiments show that our M3 models achieve better\nperformance using fewer topics than many classic topic models. We also observe\nthat topics from the different dimensions of M3 models are meaningful and\northogonal to each other.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2012 05:20:01 GMT"}], "update_date": "2012-08-03", "authors_parsed": [["Jiang", "Yun", ""], ["Lim", "Marcus", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1208.0432", "submitter": "Ju Sun", "authors": "Ju Sun and Yuqian Zhang and John Wright", "title": "Efficient Point-to-Subspace Query in $\\ell^1$ with Application to Robust\n  Object Instance Recognition", "comments": "Revised based on reviewers' feedback; one new experiment on\n  synthesized data added; one section discussing the speed up added", "journal-ref": "SIAM Journal on Imaging Sciences, 7(4):2105 - 2138, 2014", "doi": "10.1137/130936166", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by vision tasks such as robust face and object recognition, we\nconsider the following general problem: given a collection of low-dimensional\nlinear subspaces in a high-dimensional ambient (image) space, and a query point\n(image), efficiently determine the nearest subspace to the query in $\\ell^1$\ndistance. In contrast to the naive exhaustive search which entails large-scale\nlinear programs, we show that the computational burden can be cut down\nsignificantly by a simple two-stage algorithm: (1) projecting the query and\ndata-base subspaces into lower-dimensional space by random Cauchy matrix, and\nsolving small-scale distance evaluations (linear programs) in the projection\nspace to locate candidate nearest; (2) with few candidates upon independent\nrepetition of (1), getting back to the high-dimensional space and performing\nexhaustive search. To preserve the identity of the nearest subspace with\nnontrivial probability, the projection dimension typically is low-order\npolynomial of the subspace dimension multiplied by logarithm of number of the\nsubspaces (Theorem 2.1). The reduced dimensionality and hence complexity\nrenders the proposed algorithm particularly relevant to vision application such\nas robust face and object instance recognition that we investigate empirically.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2012 08:43:45 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2013 19:59:11 GMT"}, {"version": "v3", "created": "Thu, 6 Mar 2014 06:12:11 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Sun", "Ju", ""], ["Zhang", "Yuqian", ""], ["Wright", "John", ""]]}, {"id": "1208.0564", "submitter": "Lena Chekina", "authors": "L. Chekina, D. Mimran, L. Rokach, Y. Elovici, B. Shapira", "title": "Detection of Deviations in Mobile Applications Network Behavior", "comments": "Length of 10 pages, submitted to Annual Computer Security\n  Applications Conference, ACSAC'2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a novel system for detecting meaningful deviations in a mobile\napplication's network behavior is proposed. The main goal of the proposed\nsystem is to protect mobile device users and cellular infrastructure companies\nfrom malicious applications. The new system is capable of: (1) identifying\nmalicious attacks or masquerading applications installed on a mobile device,\nand (2) identifying republishing of popular applications injected with a\nmalicious code. The detection is performed based on the application's network\ntraffic patterns only. For each application two types of models are learned.\nThe first model, local, represents the personal traffic pattern for each user\nusing an application and is learned on the device. The second model,\ncollaborative, represents traffic patterns of numerous users using an\napplication and is learned on the system server. Machine-learning methods are\nused for learning and detection purposes. This paper focuses on methods\nutilized for local (i.e., on mobile device) learning and detection of\ndeviations from the normal application's behavior. These methods were\nimplemented and evaluated on Android devices. The evaluation experiments\ndemonstrate that: (1) various applications have specific network traffic\npatterns and certain application categories can be distinguishable by their\nnetwork patterns, (2) different levels of deviations from normal behavior can\nbe detected accurately, and (3) local learning is feasible and has a low\nperformance overhead on mobile devices.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2012 21:39:21 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2012 09:31:22 GMT"}], "update_date": "2012-08-07", "authors_parsed": [["Chekina", "L.", ""], ["Mimran", "D.", ""], ["Rokach", "L.", ""], ["Elovici", "Y.", ""], ["Shapira", "B.", ""]]}, {"id": "1208.0645", "submitter": "Zhi-Hua Zhou", "authors": "Wei Gao and Zhi-Hua Zhou", "title": "On the Consistency of AUC Pairwise Optimization", "comments": null, "journal-ref": "IJCAI 2015", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AUC (area under ROC curve) is an important evaluation criterion, which has\nbeen popularly used in many learning tasks such as class-imbalance learning,\ncost-sensitive learning, learning to rank, etc. Many learning approaches try to\noptimize AUC, while owing to the non-convexity and discontinuousness of AUC,\nalmost all approaches work with surrogate loss functions. Thus, the consistency\nof AUC is crucial; however, it has been almost untouched before. In this paper,\nwe provide a sufficient condition for the asymptotic consistency of learning\napproaches based on surrogate loss functions. Based on this result, we prove\nthat exponential loss and logistic loss are consistent with AUC, but hinge loss\nis inconsistent. Then, we derive the $q$-norm hinge loss and general hinge loss\nthat are consistent with AUC. We also derive the consistent bounds for\nexponential loss and logistic loss, and obtain the consistent bounds for many\nsurrogate loss functions under the non-noise setting. Further, we disclose an\nequivalence between the exponential surrogate loss of AUC and exponential\nsurrogate loss of accuracy, and one straightforward consequence of such finding\nis that AdaBoost and RankBoost are equivalent.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2012 02:37:44 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2012 08:35:28 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2012 07:00:09 GMT"}, {"version": "v4", "created": "Wed, 2 Jul 2014 14:46:59 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Gao", "Wei", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1208.0782", "submitter": "Shang Shang", "authors": "Shang Shang, Pan Hui, Sanjeev R. Kulkarni and Paul W. Cuff", "title": "Wisdom of the Crowd: Incorporating Social Influence in Recommendation\n  Models", "comments": "HotPost 2011, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Recommendation systems have received considerable attention recently.\nHowever, most research has been focused on improving the performance of\ncollaborative filtering (CF) techniques. Social networks, indispensably,\nprovide us extra information on people's preferences, and should be considered\nand deployed to improve the quality of recommendations. In this paper, we\npropose two recommendation models, for individuals and for groups respectively,\nbased on social contagion and social influence network theory. In the\nrecommendation model for individuals, we improve the result of collaborative\nfiltering prediction with social contagion outcome, which simulates the result\nof information cascade in the decision-making process. In the recommendation\nmodel for groups, we apply social influence network theory to take\ninterpersonal influence into account to form a settled pattern of disagreement,\nand then aggregate opinions of group members. By introducing the concept of\nsusceptibility and interpersonal influence, the settled rating results are\nflexible, and inclined to members whose ratings are \"essential\".\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2012 16:00:35 GMT"}, {"version": "v2", "created": "Fri, 17 May 2013 21:55:30 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Shang", "Shang", ""], ["Hui", "Pan", ""], ["Kulkarni", "Sanjeev R.", ""], ["Cuff", "Paul W.", ""]]}, {"id": "1208.0787", "submitter": "Shang Shang", "authors": "Shang Shang, Sanjeev R. Kulkarni, Paul W. Cuff and Pan Hui", "title": "A Random Walk Based Model Incorporating Social Information for\n  Recommendations", "comments": "2012 IEEE Machine Learning for Signal Processing Workshop (MLSP), 6\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Collaborative filtering (CF) is one of the most popular approaches to build a\nrecommendation system. In this paper, we propose a hybrid collaborative\nfiltering model based on a Makovian random walk to address the data sparsity\nand cold start problems in recommendation systems. More precisely, we construct\na directed graph whose nodes consist of items and users, together with item\ncontent, user profile and social network information. We incorporate user's\nratings into edge settings in the graph model. The model provides personalized\nrecommendations and predictions to individuals and groups. The proposed\nalgorithms are evaluated on MovieLens and Epinions datasets. Experimental\nresults show that the proposed methods perform well compared with other\ngraph-based methods, especially in the cold start case.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2012 16:15:10 GMT"}, {"version": "v2", "created": "Fri, 17 May 2013 21:57:26 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Shang", "Shang", ""], ["Kulkarni", "Sanjeev R.", ""], ["Cuff", "Paul W.", ""], ["Hui", "Pan", ""]]}, {"id": "1208.0806", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk", "title": "Cross-conformal predictors", "comments": "10 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note introduces the method of cross-conformal prediction, which is a\nhybrid of the methods of inductive conformal prediction and cross-validation,\nand studies its validity and predictive efficiency empirically.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2012 18:01:52 GMT"}], "update_date": "2012-08-06", "authors_parsed": [["Vovk", "Vladimir", ""]]}, {"id": "1208.0848", "submitter": "Qiang Wu", "authors": "Ting Hu, Jun Fan, Qiang Wu, Ding-Xuan Zhou", "title": "Learning Theory Approach to Minimum Error Entropy Criterion", "comments": null, "journal-ref": "JMLR 2013", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimum error entropy (MEE) criterion and an empirical risk\nminimization learning algorithm in a regression setting. A learning theory\napproach is presented for this MEE algorithm and explicit error bounds are\nprovided in terms of the approximation ability and capacity of the involved\nhypothesis space when the MEE scaling parameter is large. Novel asymptotic\nanalysis is conducted for the generalization error associated with Renyi's\nentropy and a Parzen window function, to overcome technical difficulties arisen\nfrom the essential differences between the classical least squares problems and\nthe MEE setting. A semi-norm and the involved symmetrized least squares error\nare introduced, which is related to some ranking algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2012 21:15:19 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2013 21:09:57 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Hu", "Ting", ""], ["Fan", "Jun", ""], ["Wu", "Qiang", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1208.0864", "submitter": "Anil Aswani", "authors": "Anil Aswani, Humberto Gonzalez, S. Shankar Sastry, Claire Tomlin", "title": "Statistical Results on Filtering and Epi-convergence for Learning-Based\n  Model Predictive Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based model predictive control (LBMPC) is a technique that provides\ndeterministic guarantees on robustness, while statistical identification tools\nare used to identify richer models of the system in order to improve\nperformance. This technical note provides proofs that elucidate the reasons for\nour choice of measurement model, as well as giving proofs concerning the\nstochastic convergence of LBMPC. The first part of this note discusses\nsimultaneous state estimation and statistical identification (or learning) of\nunmodeled dynamics, for dynamical systems that can be described by ordinary\ndifferential equations (ODE's). The second part provides proofs concerning the\nepi-convergence of different statistical estimators that can be used with the\nlearning-based model predictive control (LBMPC) technique. In particular, we\nprove results on the statistical properties of a nonparametric estimator that\nwe have designed to have the correct deterministic and stochastic properties\nfor numerical implementation when used in conjunction with LBMPC.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2012 22:56:36 GMT"}], "update_date": "2012-08-07", "authors_parsed": [["Aswani", "Anil", ""], ["Gonzalez", "Humberto", ""], ["Sastry", "S. Shankar", ""], ["Tomlin", "Claire", ""]]}, {"id": "1208.0959", "submitter": "Misha Denil", "authors": "Misha Denil and Nando de Freitas", "title": "Recklessly Approximate Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been observed that certain extremely simple feature encoding\ntechniques are able to achieve state of the art performance on several standard\nimage classification benchmarks including deep belief networks, convolutional\nnets, factored RBMs, mcRBMs, convolutional RBMs, sparse autoencoders and\nseveral others. Moreover, these \"triangle\" or \"soft threshold\" encodings are\nex- tremely efficient to compute. Several intuitive arguments have been put\nforward to explain this remarkable performance, yet no mathematical\njustification has been offered.\n  The main result of this report is to show that these features are realized as\nan approximate solution to the a non-negative sparse coding problem. Using this\nconnection we describe several variants of the soft threshold features and\ndemonstrate their effectiveness on two image classification benchmark tasks.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2012 21:48:52 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2013 19:00:48 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Denil", "Misha", ""], ["de Freitas", "Nando", ""]]}, {"id": "1208.0984", "submitter": "Marc Schoenauer", "authors": "Riad Akrour (INRIA Saclay - Ile de France, LRI), Marc Schoenauer\n  (INRIA Saclay - Ile de France, LRI), Mich\\`ele Sebag (LRI)", "title": "APRIL: Active Preference-learning based Reinforcement Learning", "comments": null, "journal-ref": "ECML PKDD 2012 7524 (2012) 116-131", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on reinforcement learning (RL) with limited prior\nknowledge. In the domain of swarm robotics for instance, the expert can hardly\ndesign a reward function or demonstrate the target behavior, forbidding the use\nof both standard RL and inverse reinforcement learning. Although with a limited\nexpertise, the human expert is still often able to emit preferences and rank\nthe agent demonstrations. Earlier work has presented an iterative\npreference-based RL framework: expert preferences are exploited to learn an\napproximate policy return, thus enabling the agent to achieve direct policy\nsearch. Iteratively, the agent selects a new candidate policy and demonstrates\nit; the expert ranks the new demonstration comparatively to the previous best\none; the expert's ranking feedback enables the agent to refine the approximate\npolicy return, and the process is iterated. In this paper, preference-based\nreinforcement learning is combined with active ranking in order to decrease the\nnumber of ranking queries to the expert needed to yield a satisfactory policy.\nExperiments on the mountain car and the cancer treatment testbeds witness that\na couple of dozen rankings enable to learn a competent policy.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2012 06:34:44 GMT"}], "update_date": "2012-08-07", "authors_parsed": [["Akrour", "Riad", "", "INRIA Saclay - Ile de France, LRI"], ["Schoenauer", "Marc", "", "INRIA Saclay - Ile de France, LRI"], ["Sebag", "Mich\u00e8le", "", "LRI"]]}, {"id": "1208.1056", "submitter": "Xinjia Chen", "authors": "Xinjia Chen", "title": "Sequential Estimation Methods from Inclusion Principle", "comments": "28 pages, no figure; in proceedings of SPIE conference, Baltimore,\n  Maryland, April 24-27, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose new sequential estimation methods based on\ninclusion principle. The main idea is to reformulate the estimation problems as\nconstructing sequential random intervals and use confidence sequences to\ncontrol the associated coverage probabilities. In contrast to existing\nasymptotic sequential methods, our estimation procedures rigorously guarantee\nthe pre-specified levels of confidence.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2012 22:02:13 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Chen", "Xinjia", ""]]}, {"id": "1208.1237", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis and Stephen A. Vavasis", "title": "Fast and Robust Recursive Algorithms for Separable Nonnegative Matrix\n  Factorization", "comments": "30 pages, 2 figures, 7 tables. Main change: Improvement of the bound\n  of the main theorem (Th. 3), replacing r with sqrt(r)", "journal-ref": "IEEE Trans. on Pattern Analysis and Machine Intelligence 36 (4),\n  pp. 698-714, 2014", "doi": "10.1109/TPAMI.2013.226", "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the nonnegative matrix factorization problem under\nthe separability assumption (that is, there exists a cone spanned by a small\nsubset of the columns of the input nonnegative data matrix containing all\ncolumns), which is equivalent to the hyperspectral unmixing problem under the\nlinear mixing model and the pure-pixel assumption. We present a family of fast\nrecursive algorithms, and prove they are robust under any small perturbations\nof the input data matrix. This family generalizes several existing\nhyperspectral unmixing algorithms and hence provides for the first time a\ntheoretical justification of their better practical performance.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2012 18:49:07 GMT"}, {"version": "v2", "created": "Fri, 10 Aug 2012 20:57:52 GMT"}, {"version": "v3", "created": "Mon, 7 Oct 2013 14:15:01 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Gillis", "Nicolas", ""], ["Vavasis", "Stephen A.", ""]]}, {"id": "1208.1259", "submitter": "Ping Li", "authors": "Ping Li and Art Owen and Cun-Hui Zhang", "title": "One Permutation Hashing for Efficient Search and Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the method of b-bit minwise hashing has been applied to large-scale\nlinear learning and sublinear time near-neighbor search. The major drawback of\nminwise hashing is the expensive preprocessing cost, as the method requires\napplying (e.g.,) k=200 to 500 permutations on the data. The testing time can\nalso be expensive if a new data point (e.g., a new document or image) has not\nbeen processed, which might be a significant issue in user-facing applications.\n  We develop a very simple solution based on one permutation hashing.\nConceptually, given a massive binary data matrix, we permute the columns only\nonce and divide the permuted columns evenly into k bins; and we simply store,\nfor each data vector, the smallest nonzero location in each bin. The\ninteresting probability analysis (which is validated by experiments) reveals\nthat our one permutation scheme should perform very similarly to the original\n(k-permutation) minwise hashing. In fact, the one permutation scheme can be\neven slightly more accurate, due to the \"sample-without-replacement\" effect.\n  Our experiments with training linear SVM and logistic regression on the\nwebspam dataset demonstrate that this one permutation hashing scheme can\nachieve the same (or even slightly better) accuracies compared to the original\nk-permutation scheme. To test the robustness of our method, we also experiment\nwith the small news20 dataset which is very sparse and has merely on average\n500 nonzeros in each data vector. Interestingly, our one permutation scheme\nnoticeably outperforms the k-permutation scheme when k is not too small on the\nnews20 dataset. In summary, our method can achieve at least the same accuracy\nas the original k-permutation scheme, at merely 1/k of the original\npreprocessing cost.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2012 12:28:06 GMT"}], "update_date": "2012-08-08", "authors_parsed": [["Li", "Ping", ""], ["Owen", "Art", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1208.1315", "submitter": "Ehsan Saboori Mr.", "authors": "Shafigh Parsazad, Ehsan Saboori and Amin Allahyar", "title": "Data Selection for Semi-Supervised Learning", "comments": "6 Pages", "journal-ref": "International Journal of Computer Science Issues, Vol. 9, Issue 2,\n  No 3, pp. 195-200, March 2012", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The real challenge in pattern recognition task and machine learning process\nis to train a discriminator using labeled data and use it to distinguish\nbetween future data as accurate as possible. However, most of the problems in\nthe real world have numerous data, which labeling them is a cumbersome or even\nan impossible matter. Semi-supervised learning is one approach to overcome\nthese types of problems. It uses only a small set of labeled with the company\nof huge remain and unlabeled data to train the discriminator. In\nsemi-supervised learning, it is very essential that which data is labeled and\ndepend on position of data it effectiveness changes. In this paper, we proposed\nan evolutionary approach called Artificial Immune System (AIS) to determine\nwhich data is better to be labeled to get the high quality data. The\nexperimental results represent the effectiveness of this algorithm in finding\nthese data points.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2012 01:31:32 GMT"}], "update_date": "2012-08-08", "authors_parsed": [["Parsazad", "Shafigh", ""], ["Saboori", "Ehsan", ""], ["Allahyar", "Amin", ""]]}, {"id": "1208.1544", "submitter": "Andrea Montanari", "authors": "Amy Zhang, Nadia Fawaz, Stratis Ioannidis and Andrea Montanari", "title": "Guess Who Rated This Movie: Identifying Users Through Subspace\n  Clustering", "comments": "10 pages", "journal-ref": "28th Conference on Uncertainty in Artificial Intelligence (UAI\n  2012)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often the case that, within an online recommender system, multiple\nusers share a common account. Can such shared accounts be identified solely on\nthe basis of the user- provided ratings? Once a shared account is identified,\ncan the different users sharing it be identified as well? Whenever such user\nidentification is feasible, it opens the way to possible improvements in\npersonalized recommendations, but also raises privacy concerns. We develop a\nmodel for composite accounts based on unions of linear subspaces, and use\nsubspace clustering for carrying out the identification task. We show that a\nsignificant fraction of such accounts is identifiable in a reliable manner, and\nillustrate potential uses for personalized recommendation.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2012 23:21:31 GMT"}], "update_date": "2012-08-09", "authors_parsed": [["Zhang", "Amy", ""], ["Fawaz", "Nadia", ""], ["Ioannidis", "Stratis", ""], ["Montanari", "Andrea", ""]]}, {"id": "1208.1819", "submitter": "Peter Sarlin", "authors": "Peter Sarlin", "title": "Self-Organizing Time Map: An Abstraction of Temporal Multivariate\n  Patterns", "comments": null, "journal-ref": "Neurocomputing 99(1) (2013), pp. 496-508", "doi": "10.1016/j.neucom.2012.07.011", "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper adopts and adapts Kohonen's standard Self-Organizing Map (SOM) for\nexploratory temporal structure analysis. The Self-Organizing Time Map (SOTM)\nimplements SOM-type learning to one-dimensional arrays for individual time\nunits, preserves the orientation with short-term memory and arranges the arrays\nin an ascending order of time. The two-dimensional representation of the SOTM\nattempts thus twofold topology preservation, where the horizontal direction\npreserves time topology and the vertical direction data topology. This enables\ndiscovering the occurrence and exploring the properties of temporal structural\nchanges in data. For representing qualities and properties of SOTMs, we adapt\nmeasures and visualizations from the standard SOM paradigm, as well as\nintroduce a measure of temporal structural changes. The functioning of the\nSOTM, and its visualizations and quality and property measures, are illustrated\non artificial toy data. The usefulness of the SOTM in a real-world setting is\nshown on poverty, welfare and development indicators.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2012 06:14:19 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Sarlin", "Peter", ""]]}, {"id": "1208.1829", "submitter": "Qiang Qian", "authors": "Qiang Qian and Songcan Chen", "title": "Metric Learning across Heterogeneous Domains by Respectively Aligning\n  Both Priors and Posteriors", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we attempts to learn a single metric across two heterogeneous\ndomains where source domain is fully labeled and has many samples while target\ndomain has only a few labeled samples but abundant unlabeled samples. To the\nbest of our knowledge, this task is seldom touched. The proposed learning model\nhas a simple underlying motivation: all the samples in both the source and the\ntarget domains are mapped into a common space, where both their priors\nP(sample)s and their posteriors P(label|sample)s are forced to be respectively\naligned as much as possible. We show that the two mappings, from both the\nsource domain and the target domain to the common space, can be reparameterized\ninto a single positive semi-definite(PSD) matrix. Then we develop an efficient\nBregman Projection algorithm to optimize the PDS matrix over which a LogDet\nfunction is used to regularize. Furthermore, we also show that this model can\nbe easily kernelized and verify its effectiveness in crosslanguage retrieval\ntask and cross-domain object recognition task.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2012 07:14:37 GMT"}], "update_date": "2012-08-10", "authors_parsed": [["Qian", "Qiang", ""], ["Chen", "Songcan", ""]]}, {"id": "1208.1846", "submitter": "Qiang Qian", "authors": "Guangxu Guo and Songcan Chen", "title": "Margin Distribution Controlled Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schapire's margin theory provides a theoretical explanation to the success of\nboosting-type methods and manifests that a good margin distribution (MD) of\ntraining samples is essential for generalization. However the statement that a\nMD is good is vague, consequently, many recently developed algorithms try to\ngenerate a MD in their goodness senses for boosting generalization. Unlike\ntheir indirect control over MD, in this paper, we propose an alternative\nboosting algorithm termed Margin distribution Controlled Boosting (MCBoost)\nwhich directly controls the MD by introducing and optimizing a key adjustable\nmargin parameter. MCBoost's optimization implementation adopts the column\ngeneration technique to ensure fast convergence and small number of weak\nclassifiers involved in the final MCBooster. We empirically demonstrate: 1)\nAdaBoost is actually also a MD controlled algorithm and its iteration number\nacts as a parameter controlling the distribution and 2) the generalization\nperformance of MCBoost evaluated on UCI benchmark datasets is validated better\nthan those of AdaBoost, L2Boost, LPBoost, AdaBoost-CG and MDBoost.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2012 08:53:11 GMT"}], "update_date": "2012-08-10", "authors_parsed": [["Guo", "Guangxu", ""], ["Chen", "Songcan", ""]]}, {"id": "1208.1860", "submitter": "Benjamin Rubinstein", "authors": "Sahand Negahban, Benjamin I. P. Rubinstein and Jim Gemmell", "title": "Scaling Multiple-Source Entity Resolution using Statistically Efficient\n  Transfer Learning", "comments": "Short version to appear in CIKM'2012; 10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a serious, previously-unexplored challenge facing almost all\napproaches to scaling up entity resolution (ER) to multiple data sources: the\nprohibitive cost of labeling training data for supervised learning of\nsimilarity scores for each pair of sources. While there exists a rich\nliterature describing almost all aspects of pairwise ER, this new challenge is\narising now due to the unprecedented ability to acquire and store data from\nonline sources, features driven by ER such as enriched search verticals, and\nthe uniqueness of noisy and missing data characteristics for each source. We\nshow on real-world and synthetic data that for state-of-the-art techniques, the\nreality of heterogeneous sources means that the number of labeled training data\nmust scale quadratically in the number of sources, just to maintain constant\nprecision/recall. We address this challenge with a brand new transfer learning\nalgorithm which requires far less training data (or equivalently, achieves\nsuperior accuracy with the same data) and is trained using fast convex\noptimization. The intuition behind our approach is to adaptively share\nstructure learned about one scoring problem with all other scoring problems\nsharing a data source in common. We demonstrate that our theoretically\nmotivated approach incurs no runtime cost while it can maintain constant\nprecision/recall with the cost of labeling increasing only linearly with the\nnumber of sources.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2012 10:02:35 GMT"}], "update_date": "2012-08-10", "authors_parsed": [["Negahban", "Sahand", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Gemmell", "Jim", ""]]}, {"id": "1208.2015", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Paris - Rocquencourt, LIENS)", "title": "Sharp analysis of low-rank kernel matrix approximations", "comments": null, "journal-ref": "International Conference on Learning Theory (COLT), \\'Etats-Unis\n  (2013)", "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider supervised learning problems within the positive-definite kernel\nframework, such as kernel ridge regression, kernel logistic regression or the\nsupport vector machine. With kernels leading to infinite-dimensional feature\nspaces, a common practical limiting difficulty is the necessity of computing\nthe kernel matrix, which most frequently leads to algorithms with running time\nat least quadratic in the number of observations n, i.e., O(n^2). Low-rank\napproximations of the kernel matrix are often considered as they allow the\nreduction of running time complexities to O(p^2 n), where p is the rank of the\napproximation. The practicality of such methods thus depends on the required\nrank p. In this paper, we show that in the context of kernel ridge regression,\nfor approximations based on a random subset of columns of the original kernel\nmatrix, the rank p may be chosen to be linear in the degrees of freedom\nassociated with the problem, a quantity which is classically used in the\nstatistical analysis of such methods, and is often seen as the implicit number\nof parameters of non-parametric estimators. This result enables simple\nalgorithms that have sub-quadratic running time complexity, but provably\nexhibit the same predictive performance than existing algorithms, for any given\nproblem instance, and not only for worst-case situations.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2012 19:31:22 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2013 11:11:19 GMT"}, {"version": "v3", "created": "Wed, 22 May 2013 11:10:14 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Bach", "Francis", "", "INRIA Paris - Rocquencourt, LIENS"]]}, {"id": "1208.2112", "submitter": "Qifeng Qiao", "authors": "Qifeng Qiao and Peter A. Beling", "title": "Inverse Reinforcement Learning with Gaussian Process", "comments": "conferencel American Control Conference 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new algorithms for inverse reinforcement learning (IRL, or inverse\noptimal control) in convex optimization settings. We argue that finite-space\nIRL can be posed as a convex quadratic program under a Bayesian inference\nframework with the objective of maximum a posterior estimation. To deal with\nproblems in large or even infinite state space, we propose a Gaussian process\nmodel and use preference graphs to represent observations of decision\ntrajectories. Our method is distinguished from other approaches to IRL in that\nit makes no assumptions about the form of the reward function and yet it\nretains the promise of computationally manageable implementations for potential\nreal-world applications. In comparison with an establish algorithm on\nsmall-scale numerical problems, our method demonstrated better accuracy in\napprenticeship learning and a more robust dependence on the number of\nobservations.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2012 08:36:49 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2013 08:12:56 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Qiao", "Qifeng", ""], ["Beling", "Peter A.", ""]]}, {"id": "1208.2128", "submitter": "V.p.gladis Pushparathi", "authors": "V. P. Gladis Pushpa Rathi and S. Palani", "title": "Brain tumor MRI image classification with feature selection and\n  extraction using linear discriminant analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction is a method of capturing visual content of an image. The\nfeature extraction is the process to represent raw image in its reduced form to\nfacilitate decision making such as pattern classification. We have tried to\naddress the problem of classification MRI brain images by creating a robust and\nmore accurate classifier which can act as an expert assistant to medical\npractitioners. The objective of this paper is to present a novel method of\nfeature selection and extraction. This approach combines the Intensity,\nTexture, shape based features and classifies the tumor as white matter, Gray\nmatter, CSF, abnormal and normal area. The experiment is performed on 140 tumor\ncontained brain MR images from the Internet Brain Segmentation Repository. The\nproposed technique has been carried out over a larger database as compare to\nany previous work and is more robust and effective. PCA and Linear Discriminant\nAnalysis (LDA) were applied on the training sets. The Support Vector Machine\n(SVM) classifier served as a comparison of nonlinear techniques Vs linear ones.\nPCA and LDA methods are used to reduce the number of features used. The feature\nselection using the proposed technique is more beneficial as it analyses the\ndata according to grouping class variable and gives reduced feature set with\nhigh classification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2012 09:33:37 GMT"}], "update_date": "2012-08-13", "authors_parsed": [["Rathi", "V. P. Gladis Pushpa", ""], ["Palani", "S.", ""]]}, {"id": "1208.2294", "submitter": "Grigory Yaroslavtsev", "authors": "Sofya Raskhodnikova and Grigory Yaroslavtsev", "title": "Learning pseudo-Boolean k-DNF and Submodular Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that any submodular function f: {0,1}^n -> {0,1,...,k} can be\nrepresented as a pseudo-Boolean 2k-DNF formula. Pseudo-Boolean DNFs are a\nnatural generalization of DNF representation for functions with integer range.\nEach term in such a formula has an associated integral constant. We show that\nan analog of Hastad's switching lemma holds for pseudo-Boolean k-DNFs if all\nconstants associated with the terms of the formula are bounded.\n  This allows us to generalize Mansour's PAC-learning algorithm for k-DNFs to\npseudo-Boolean k-DNFs, and hence gives a PAC-learning algorithm with membership\nqueries under the uniform distribution for submodular functions of the form\nf:{0,1}^n -> {0,1,...,k}. Our algorithm runs in time polynomial in n, k^{O(k\n\\log k / \\epsilon)}, 1/\\epsilon and log(1/\\delta) and works even in the\nagnostic setting. The line of previous work on learning submodular functions\n[Balcan, Harvey (STOC '11), Gupta, Hardt, Roth, Ullman (STOC '11), Cheraghchi,\nKlivans, Kothari, Lee (SODA '12)] implies only n^{O(k)} query complexity for\nlearning submodular functions in this setting, for fixed epsilon and delta.\n  Our learning algorithm implies a property tester for submodularity of\nfunctions f:{0,1}^n -> {0, ..., k} with query complexity polynomial in n for\nk=O((\\log n/ \\loglog n)^{1/2}) and constant proximity parameter \\epsilon.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2012 22:22:14 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Raskhodnikova", "Sofya", ""], ["Yaroslavtsev", "Grigory", ""]]}, {"id": "1208.2417", "submitter": "Assaf Hallak", "authors": "Assaf Hallak and Shie Mannor", "title": "How to sample if you must: on optimal functional sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine a fundamental problem that models various active sampling setups,\nsuch as network tomography. We analyze sampling of a multivariate normal\ndistribution with an unknown expectation that needs to be estimated: in our\nsetup it is possible to sample the distribution from a given set of linear\nfunctionals, and the difficulty addressed is how to optimally select the\ncombinations to achieve low estimation error. Although this problem is in the\nheart of the field of optimal design, no efficient solutions for the case with\nmany functionals exist. We present some bounds and an efficient sub-optimal\nsolution for this problem for more structured sets such as binary functionals\nthat are induced by graph walks.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2012 10:12:48 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Hallak", "Assaf", ""], ["Mannor", "Shie", ""]]}, {"id": "1208.2523", "submitter": "Konrad Rawlik", "authors": "Konrad Rawlik and Marc Toussaint and Sethu Vijayakumar", "title": "Path Integral Control by Reproducing Kernel Hilbert Space Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an embedding of stochastic optimal control problems, of the so\ncalled path integral form, into reproducing kernel Hilbert spaces. Using\nconsistent, sample based estimates of the embedding leads to a model free,\nnon-parametric approach for calculation of an approximate solution to the\ncontrol problem. This formulation admits a decomposition of the problem into an\ninvariant and task dependent component. Consequently, we make much more\nefficient use of the sample data compared to previous sample based approaches\nin this domain, e.g., by allowing sample re-use across tasks. Numerical\nexamples on test problems, which illustrate the sample efficiency, are\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 08:30:14 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Rawlik", "Konrad", ""], ["Toussaint", "Marc", ""], ["Vijayakumar", "Sethu", ""]]}, {"id": "1208.2572", "submitter": "Silvia Villa", "authors": "Lorenzo Rosasco, Silvia Villa, Sofia Mosci, Matteo Santoro, Alessandro\n  verri", "title": "Nonparametric sparsity and regularization", "comments": "45 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we are interested in the problems of supervised learning and\nvariable selection when the input-output dependence is described by a nonlinear\nfunction depending on a few variables. Our goal is to consider a sparse\nnonparametric model, hence avoiding linear or additive models. The key idea is\nto measure the importance of each variable in the model by making use of\npartial derivatives. Based on this intuition we propose a new notion of\nnonparametric sparsity and a corresponding least squares regularization scheme.\nUsing concepts and results from the theory of reproducing kernel Hilbert spaces\nand proximal methods, we show that the proposed learning algorithm corresponds\nto a minimization problem which can be provably solved by an iterative\nprocedure. The consistency properties of the obtained estimator are studied\nboth in terms of prediction and selection performance. An extensive empirical\nanalysis shows that the proposed method performs favorably with respect to the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 13:02:33 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Rosasco", "Lorenzo", ""], ["Villa", "Silvia", ""], ["Mosci", "Sofia", ""], ["Santoro", "Matteo", ""], ["verri", "Alessandro", ""]]}, {"id": "1208.2808", "submitter": "Sudarshan Nandy", "authors": "Sudarshan Nandy, Partha Pratim Sarkar and Achintya Das", "title": "Analysis of a Statistical Hypothesis Based Learning Mechanism for Faster\n  crawling", "comments": "14 Pages, 7 Figure", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA), Vol.3, No.4, July 2012, 117-130", "doi": "10.5121/ijaia.2012.3409", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of world-wide-web (WWW) spreads its wings from an intangible\nquantities of web-pages to a gigantic hub of web information which gradually\nincreases the complexity of crawling process in a search engine. A search\nengine handles a lot of queries from various parts of this world, and the\nanswers of it solely depend on the knowledge that it gathers by means of\ncrawling. The information sharing becomes a most common habit of the society,\nand it is done by means of publishing structured, semi-structured and\nunstructured resources on the web. This social practice leads to an exponential\ngrowth of web-resource, and hence it became essential to crawl for continuous\nupdating of web-knowledge and modification of several existing resources in any\nsituation. In this paper one statistical hypothesis based learning mechanism is\nincorporated for learning the behavior of crawling speed in different\nenvironment of network, and for intelligently control of the speed of crawler.\nThe scaling technique is used to compare the performance proposed method with\nthe standard crawler. The high speed performance is observed after scaling, and\nthe retrieval of relevant web-resource in such a high speed is analyzed.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2012 08:36:49 GMT"}], "update_date": "2012-08-15", "authors_parsed": [["Nandy", "Sudarshan", ""], ["Sarkar", "Partha Pratim", ""], ["Das", "Achintya", ""]]}, {"id": "1208.2873", "submitter": "Vasileios Lampos", "authors": "Vasileios Lampos", "title": "Detecting Events and Patterns in Large-Scale User Generated Textual\n  Streams with Statistical Learning Methods", "comments": "PhD thesis, 238 pages, 9 chapters, 2 appendices, 58 figures, 49\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.SI stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A vast amount of textual web streams is influenced by events or phenomena\nemerging in the real world. The social web forms an excellent modern paradigm,\nwhere unstructured user generated content is published on a regular basis and\nin most occasions is freely distributed. The present Ph.D. Thesis deals with\nthe problem of inferring information - or patterns in general - about events\nemerging in real life based on the contents of this textual stream. We show\nthat it is possible to extract valuable information about social phenomena,\nsuch as an epidemic or even rainfall rates, by automatic analysis of the\ncontent published in Social Media, and in particular Twitter, using Statistical\nMachine Learning methods. An important intermediate task regards the formation\nand identification of features which characterise a target event; we select and\nuse those textual features in several linear, non-linear and hybrid inference\napproaches achieving a significantly good performance in terms of the applied\nloss function. By examining further this rich data set, we also propose methods\nfor extracting various types of mood signals revealing how affective norms - at\nleast within the social web's population - evolve during the day and how\nsignificant events emerging in the real world are influencing them. Lastly, we\npresent some preliminary findings showing several spatiotemporal\ncharacteristics of this textual information as well as the potential of using\nit to tackle tasks such as the prediction of voting intentions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 18:59:54 GMT"}], "update_date": "2012-08-15", "authors_parsed": [["Lampos", "Vasileios", ""]]}, {"id": "1208.2925", "submitter": "Alvin Cheung", "authors": "Alvin Cheung, Armando Solar-Lezama, Samuel Madden", "title": "Using Program Synthesis for Social Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIT-CSAIL-TR-2012-025", "categories": "cs.LG cs.DB cs.PL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach to select events of interest to a user in\na social media setting where events are generated by the activities of the\nuser's friends through their mobile devices. We argue that given the unique\nrequirements of the social media setting, the problem is best viewed as an\ninductive learning problem, where the goal is to first generalize from the\nusers' expressed \"likes\" and \"dislikes\" of specific events, then to produce a\nprogram that can be manipulated by the system and distributed to the collection\ndevices to collect only data of interest. The key contribution of this paper is\na new algorithm that combines existing machine learning techniques with new\nprogram synthesis technology to learn users' preferences. We show that when\ncompared with the more standard approaches, our new algorithm provides up to\norder-of-magnitude reductions in model training time, and significantly higher\nprediction accuracies for our target application. The approach also improves on\nstandard machine learning techniques in that it produces clear programs that\ncan be manipulated to optimize data collection and filtering.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2012 17:04:19 GMT"}], "update_date": "2012-08-15", "authors_parsed": [["Cheung", "Alvin", ""], ["Solar-Lezama", "Armando", ""], ["Madden", "Samuel", ""]]}, {"id": "1208.3030", "submitter": "Dacheng Tao", "authors": "Wei Bian and Dacheng Tao", "title": "Asymptotic Generalization Bound of Fisher's Linear Discriminant Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher's linear discriminant analysis (FLDA) is an important dimension\nreduction method in statistical pattern recognition. It has been shown that\nFLDA is asymptotically Bayes optimal under the homoscedastic Gaussian\nassumption. However, this classical result has the following two major\nlimitations: 1) it holds only for a fixed dimensionality $D$, and thus does not\napply when $D$ and the training sample size $N$ are proportionally large; 2) it\ndoes not provide a quantitative description on how the generalization ability\nof FLDA is affected by $D$ and $N$. In this paper, we present an asymptotic\ngeneralization analysis of FLDA based on random matrix theory, in a setting\nwhere both $D$ and $N$ increase and $D/N\\longrightarrow\\gamma\\in[0,1)$. The\nobtained lower bound of the generalization discrimination power overcomes both\nlimitations of the classical result, i.e., it is applicable when $D$ and $N$\nare proportionally large and provides a quantitative description of the\ngeneralization ability of FLDA in terms of the ratio $\\gamma=D/N$ and the\npopulation discrimination power. Besides, the discrimination power bound also\nleads to an upper bound on the generalization error of binary-classification\nwith FLDA.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2012 05:35:36 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2013 04:12:22 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Bian", "Wei", ""], ["Tao", "Dacheng", ""]]}, {"id": "1208.3145", "submitter": "Stijn van Dongen", "authors": "Stijn van Dongen and Anton J. Enright", "title": "Metric distances derived from cosine similarity and Pearson and Spearman\n  correlations", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate two classes of transformations of cosine similarity and\nPearson and Spearman correlations into metric distances, utilising the simple\ntool of metric-preserving functions. The first class puts anti-correlated\nobjects maximally far apart. Previously known transforms fall within this\nclass. The second class collates correlated and anti-correlated objects. An\nexample of such a transformation that yields a metric distance is the sine\nfunction when applied to centered data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2012 11:08:53 GMT"}], "update_date": "2012-08-16", "authors_parsed": [["van Dongen", "Stijn", ""], ["Enright", "Anton J.", ""]]}, {"id": "1208.3279", "submitter": "David Weiss", "authors": "David Weiss, Benjamin Sapp, Ben Taskar", "title": "Structured Prediction Cascades", "comments": "32 pages, in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured prediction tasks pose a fundamental trade-off between the need for\nmodel complexity to increase predictive power and the limited computational\nresources for inference in the exponentially-sized output spaces such models\nrequire. We formulate and develop the Structured Prediction Cascade\narchitecture: a sequence of increasingly complex models that progressively\nfilter the space of possible outputs. The key principle of our approach is that\neach model in the cascade is optimized to accurately filter and refine the\nstructured output state space of the next model, speeding up both learning and\ninference in the next layer of the cascade. We learn cascades by optimizing a\nnovel convex loss function that controls the trade-off between the filtering\nefficiency and the accuracy of the cascade, and provide generalization bounds\nfor both accuracy and efficiency. We also extend our approach to intractable\nmodels using tree-decomposition ensembles, and provide algorithms and theory\nfor this setting. We evaluate our approach on several large-scale problems,\nachieving state-of-the-art performance in handwriting recognition and human\npose recognition. We find that structured prediction cascades allow tremendous\nspeedups and the use of previously intractable features and models in both\nsettings.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2012 16:20:23 GMT"}], "update_date": "2012-08-17", "authors_parsed": [["Weiss", "David", ""], ["Sapp", "Benjamin", ""], ["Taskar", "Ben", ""]]}, {"id": "1208.3422", "submitter": "Zhixiang Eddie Xu", "authors": "Zhixiang Xu, Kilian Q. Weinberger, Olivier Chapelle", "title": "Distance Metric Learning for Kernel Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in metric learning has significantly improved the\nstate-of-the-art in k-nearest neighbor classification. Support vector machines\n(SVM), particularly with RBF kernels, are amongst the most popular\nclassification algorithms that uses distance metrics to compare examples. This\npaper provides an empirical analysis of the efficacy of three of the most\npopular Mahalanobis metric learning algorithms as pre-processing for SVM\ntraining. We show that none of these algorithms generate metrics that lead to\nparticularly satisfying improvements for SVM-RBF classification. As a remedy we\nintroduce support vector metric learning (SVML), a novel algorithm that\nseamlessly combines the learning of a Mahalanobis metric with the training of\nthe RBF-SVM parameters. We demonstrate the capabilities of SVML on nine\nbenchmark data sets of varying sizes and difficulties. In our study, SVML\noutperforms all alternative state-of-the-art metric learning algorithms in\nterms of accuracy and establishes itself as a serious alternative to the\nstandard Euclidean metric with model selection by cross validation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2012 17:16:18 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2013 20:26:55 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Xu", "Zhixiang", ""], ["Weinberger", "Kilian Q.", ""], ["Chapelle", "Olivier", ""]]}, {"id": "1208.3561", "submitter": "Alon Gonen", "authors": "Alon Gonen, Sivan Sabato, Shai Shalev-Shwartz", "title": "Efficient Active Learning of Halfspaces: an Aggressive Approach", "comments": "Full version of: Gonen, Sabato and Shalev-Shwartz, \"Efficient Active\n  Learning of Halfspaces: an Aggressive Approach\", ICML 2013", "journal-ref": "Journal of Machine Learning Research, 14(Sep):2487-2519, 2013", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study pool-based active learning of half-spaces. We revisit the aggressive\napproach for active learning in the realizable case, and show that it can be\nmade efficient and practical, while also having theoretical guarantees under\nreasonable assumptions. We further show, both theoretically and experimentally,\nthat it can be preferable to mellow approaches. Our efficient aggressive active\nlearner of half-spaces has formal approximation guarantees that hold when the\npool is separable with a margin. While our analysis is focused on the\nrealizable setting, we show that a simple heuristic allows using the same\nalgorithm successfully for pools with low error as well. We further compare the\naggressive approach to the mellow approach, and prove that there are cases in\nwhich the aggressive approach results in significantly better label complexity\ncompared to the mellow approach. We demonstrate experimentally that substantial\nimprovements in label complexity can be achieved using the aggressive approach,\nfor both realizable and low-error settings.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2012 09:49:31 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2012 15:38:37 GMT"}, {"version": "v3", "created": "Sat, 25 May 2013 19:23:14 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Gonen", "Alon", ""], ["Sabato", "Sivan", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1208.3689", "submitter": "Waad Bouaguel", "authors": "Waad Bouaguel and Ghazi Bel Mufti", "title": "An improvement direction for filter selection techniques using\n  information theory measures and quadratic optimization", "comments": "4 pages, 2 tables, (IJARAI) International Journal of Advanced\n  Research in Artificial Intelligence", "journal-ref": "International Journal of Advanced Research in Artificial\n  Intelligence, 1:7-11, 8 2012", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filter selection techniques are known for their simplicity and efficiency.\nHowever this kind of methods doesn't take into consideration the features\ninter-redundancy. Consequently the un-removed redundant features remain in the\nfinal classification model, giving lower generalization performance. In this\npaper we propose to use a mathematical optimization method that reduces\ninter-features redundancy and maximize relevance between each feature and the\ntarget variable.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2012 20:58:20 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Bouaguel", "Waad", ""], ["Mufti", "Ghazi Bel", ""]]}, {"id": "1208.3719", "submitter": "Chris Thornton", "authors": "Chris Thornton and Frank Hutter and Holger H. Hoos and Kevin\n  Leyton-Brown", "title": "Auto-WEKA: Combined Selection and Hyperparameter Optimization of\n  Classification Algorithms", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": "Technical Report TR-2012-05", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many different machine learning algorithms exist; taking into account each\nalgorithm's hyperparameters, there is a staggeringly large number of possible\nalternatives overall. We consider the problem of simultaneously selecting a\nlearning algorithm and setting its hyperparameters, going beyond previous work\nthat addresses these issues in isolation. We show that this problem can be\naddressed by a fully automated approach, leveraging recent innovations in\nBayesian optimization. Specifically, we consider a wide range of feature\nselection techniques (combining 3 search and 8 evaluator methods) and all\nclassification approaches implemented in WEKA, spanning 2 ensemble methods, 10\nmeta-methods, 27 base classifiers, and hyperparameter settings for each\nclassifier. On each of 21 popular datasets from the UCI repository, the KDD Cup\n09, variants of the MNIST dataset and CIFAR-10, we show classification\nperformance often much better than using standard selection/hyperparameter\noptimization methods. We hope that our approach will help non-expert users to\nmore effectively identify machine learning algorithms and hyperparameter\nsettings appropriate to their applications, and hence to achieve improved\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2012 02:14:47 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2013 23:27:04 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Thornton", "Chris", ""], ["Hutter", "Frank", ""], ["Hoos", "Holger H.", ""], ["Leyton-Brown", "Kevin", ""]]}, {"id": "1208.3728", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin and Karthik Sridharan", "title": "Online Learning with Predictable Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present methods for online linear optimization that take advantage of\nbenign (as opposed to worst-case) sequences. Specifically if the sequence\nencountered by the learner is described well by a known \"predictable process\",\nthe algorithms presented enjoy tighter bounds as compared to the typical worst\ncase bounds. Additionally, the methods achieve the usual worst-case regret\nbounds if the sequence is not benign. Our approach can be seen as a way of\nadding prior knowledge about the sequence within the paradigm of online\nlearning. The setting is shown to encompass partial and side information.\nVariance and path-length bounds can be seen as particular examples of online\nlearning with simple predictable sequences.\n  We further extend our methods and results to include competing with a set of\npossible predictable processes (models), that is \"learning\" the predictable\nprocess itself concurrently with using it to obtain better regret guarantees.\nWe show that such model selection is possible under various assumptions on the\navailable feedback. Our results suggest a promising direction of further\nresearch with potential applications to stock market and time series\nprediction.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2012 06:27:31 GMT"}, {"version": "v2", "created": "Sat, 24 May 2014 10:56:17 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1208.3779", "submitter": "Jim Jing-Yan Wang", "authors": "Jim Jing-Yan Wang, Halima Bensmail and Xin Gao", "title": "Multiple graph regularized protein domain ranking", "comments": "21 pages", "journal-ref": "Jim Jing-Yan Wang, Halima Bensmail and Xin Gao: Multiple graph\n  regularized protein domain ranking, BMC Bioinformatics (2012), 13:307", "doi": "10.1186/1471-2105-13-307", "report-no": null, "categories": "cs.LG cs.CE cs.IR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background Protein domain ranking is a fundamental task in structural\nbiology. Most protein domain ranking methods rely on the pairwise comparison of\nprotein domains while neglecting the global manifold structure of the protein\ndomain database. Recently, graph regularized ranking that exploits the global\nstructure of the graph defined by the pairwise similarities has been proposed.\nHowever, the existing graph regularized ranking methods are very sensitive to\nthe choice of the graph model and parameters, and this remains a difficult\nproblem for most of the protein domain ranking methods.\n  Results To tackle this problem, we have developed the Multiple Graph\nregularized Ranking algorithm, MultiG- Rank. Instead of using a single graph to\nregularize the ranking scores, MultiG-Rank approximates the intrinsic manifold\nof protein domain distribution by combining multiple initial graphs for the\nregularization. Graph weights are learned with ranking scores jointly and\nautomatically, by alternately minimizing an ob- jective function in an\niterative algorithm. Experimental results on a subset of the ASTRAL SCOP\nprotein domain database demonstrate that MultiG-Rank achieves a better ranking\nperformance than single graph regularized ranking methods and pairwise\nsimilarity based ranking methods.\n  Conclusion The problem of graph model and parameter selection in graph\nregularized protein domain ranking can be solved effectively by combining\nmultiple graphs. This aspect of generalization introduces a new frontier in\napplying multiple graphs to solving protein domain ranking applications.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2012 19:32:20 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2013 14:19:56 GMT"}, {"version": "v3", "created": "Sun, 21 Apr 2013 06:21:36 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Wang", "Jim Jing-Yan", ""], ["Bensmail", "Halima", ""], ["Gao", "Xin", ""]]}, {"id": "1208.3839", "submitter": "Jing-Yan Wang", "authors": "Jing-Yan Wang", "title": "Discriminative Sparse Coding on Multi-Manifold for Data Representation\n  and Classification", "comments": "This paper has been withdrawn by the author due to the terrible\n  writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding has been popularly used as an effective data representation\nmethod in various applications, such as computer vision, medical imaging and\nbioinformatics, etc. However, the conventional sparse coding algorithms and its\nmanifold regularized variants (graph sparse coding and Laplacian sparse\ncoding), learn the codebook and codes in a unsupervised manner and neglect the\nclass information available in the training set. To address this problem, in\nthis paper we propose a novel discriminative sparse coding method based on\nmulti-manifold, by learning discriminative class-conditional codebooks and\nsparse codes from both data feature space and class labels. First, the entire\ntraining set is partitioned into multiple manifolds according to the class\nlabels. Then, we formulate the sparse coding as a manifold-manifold matching\nproblem and learn class-conditional codebooks and codes to maximize the\nmanifold margins of different classes. Lastly, we present a data point-manifold\nmatching error based strategy to classify the unlabeled data point.\nExperimental results on somatic mutations identification and breast tumors\nclassification in ultrasonic images tasks demonstrate the efficacy of the\nproposed data representation-classification approach.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2012 14:49:27 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2013 14:21:40 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Wang", "Jing-Yan", ""]]}, {"id": "1208.3845", "submitter": "Jing-Yan Wang", "authors": "Jing-Yan Wang and Mustafa AbdulJabbar", "title": "Adaptive Graph via Multiple Kernel Learning for Nonnegative Matrix\n  Factorization", "comments": "This paper has been withdrawn by the author due to the terrible\n  writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Matrix Factorization (NMF) has been continuously evolving in\nseveral areas like pattern recognition and information retrieval methods. It\nfactorizes a matrix into a product of 2 low-rank non-negative matrices that\nwill define parts-based, and linear representation of nonnegative data.\nRecently, Graph regularized NMF (GrNMF) is proposed to find a compact\nrepresentation,which uncovers the hidden semantics and simultaneously respects\nthe intrinsic geometric structure. In GNMF, an affinity graph is constructed\nfrom the original data space to encode the geometrical information. In this\npaper, we propose a novel idea which engages a Multiple Kernel Learning\napproach into refining the graph structure that reflects the factorization of\nthe matrix and the new data space. The GrNMF is improved by utilizing the graph\nrefined by the kernel learning, and then a novel kernel learning method is\nintroduced under the GrNMF framework. Our approach shows encouraging results of\nthe proposed algorithm in comparison to the state-of-the-art clustering\nalgorithms like NMF, GrNMF, SVD etc.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2012 15:21:09 GMT"}, {"version": "v2", "created": "Sun, 26 Aug 2012 07:24:22 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2013 14:21:50 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Wang", "Jing-Yan", ""], ["AbdulJabbar", "Mustafa", ""]]}, {"id": "1208.3943", "submitter": "Jay Gholap B.Tech.(Computer Engineering)", "authors": "Jay Gholap", "title": "Performance Tuning Of J48 Algorithm For Prediction Of Soil Fertility", "comments": "5 Pages", "journal-ref": "Published in Asian Journal of Computer Science and Information\n  Technology,Vol 2,No. 8 (2012)", "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining involves the systematic analysis of large data sets, and data\nmining in agricultural soil datasets is exciting and modern research area. The\nproductive capacity of a soil depends on soil fertility. Achieving and\nmaintaining appropriate levels of soil fertility, is of utmost importance if\nagricultural land is to remain capable of nourishing crop production. In this\nresearch, Steps for building a predictive model of soil fertility have been\nexplained.\n  This paper aims at predicting soil fertility class using decision tree\nalgorithms in data mining . Further, it focuses on performance tuning of J48\ndecision tree algorithm with the help of meta-techniques such as attribute\nselection and boosting.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2012 08:48:40 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Gholap", "Jay", ""]]}, {"id": "1208.4138", "submitter": "Zahoor Khan", "authors": "Ashraf Mohammed Iqbal, Abidalrahman Moh'd, Zahoor Khan", "title": "Semi-supervised Clustering Ensemble by Voting", "comments": "The International Conference on Information and Communication Systems\n  (ICICS 2009), Amman, Jordan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering ensemble is one of the most recent advances in unsupervised\nlearning. It aims to combine the clustering results obtained using different\nalgorithms or from different runs of the same clustering algorithm for the same\ndata set, this is accomplished using on a consensus function, the efficiency\nand accuracy of this method has been proven in many works in literature. In the\nfirst part of this paper we make a comparison among current approaches to\nclustering ensemble in literature. All of these approaches consist of two main\nsteps: the ensemble generation and consensus function. In the second part of\nthe paper, we suggest engaging supervision in the clustering ensemble procedure\nto get more enhancements on the clustering results. Supervision can be applied\nin two places: either by using semi-supervised algorithms in the clustering\nensemble generation step or in the form of a feedback used by the consensus\nfunction stage. Also, we introduce a flexible two parameter weighting\nmechanism, the first parameter describes the compatibility between the datasets\nunder study and the semi-supervised clustering algorithms used to generate the\nbase partitions, the second parameter is used to provide the user feedback on\nthe these partitions. The two parameters are engaged in a \"relabeling and\nvoting\" based consensus function to produce the final clustering.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2012 23:21:10 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["Iqbal", "Ashraf Mohammed", ""], ["Moh'd", "Abidalrahman", ""], ["Khan", "Zahoor", ""]]}, {"id": "1208.4147", "submitter": "Yingzhen Li", "authors": "Yingzhen Li and Ye Zhang", "title": "Generating ordered list of Recommended Items: a Hybrid Recommender\n  System of Microblog", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise recommendation of followers helps in improving the user experience\nand maintaining the prosperity of twitter and microblog platforms. In this\npaper, we design a hybrid recommender system of microblog as a solution of KDD\nCup 2012, track 1 task, which requires predicting users a user might follow in\nTencent Microblog. We describe the background of the problem and present the\nalgorithm consisting of keyword analysis, user taxonomy, (potential)interests\nextraction and item recommendation. Experimental result shows the high\nperformance of our algorithm. Some possible improvements are discussed, which\nleads to further study.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 00:28:32 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2012 06:37:49 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2015 22:55:51 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Li", "Yingzhen", ""], ["Zhang", "Ye", ""]]}, {"id": "1208.4290", "submitter": "Pol  Blasco Moreno", "authors": "Pol Blasco, Deniz G\\\"und\\\"uz and Mischa Dohler", "title": "A Learning Theoretic Approach to Energy Harvesting Communication System\n  Optimization", "comments": null, "journal-ref": "IEEE TRANSACTIONS ON WIRELESS COMM UNICATIONS, VOL. 12, NO. 4,\n  APRIL 2013", "doi": "10.1109/TWC.2013.030413.121120", "report-no": null, "categories": "cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A point-to-point wireless communication system in which the transmitter is\nequipped with an energy harvesting device and a rechargeable battery, is\nstudied. Both the energy and the data arrivals at the transmitter are modeled\nas Markov processes. Delay-limited communication is considered assuming that\nthe underlying channel is block fading with memory, and the instantaneous\nchannel state information is available at both the transmitter and the\nreceiver. The expected total transmitted data during the transmitter's\nactivation time is maximized under three different sets of assumptions\nregarding the information available at the transmitter about the underlying\nstochastic processes. A learning theoretic approach is introduced, which does\nnot assume any a priori information on the Markov processes governing the\ncommunication system. In addition, online and offline optimization problems are\nstudied for the same setting. Full statistical knowledge and causal information\non the realizations of the underlying stochastic processes are assumed in the\nonline optimization problem, while the offline optimization problem assumes\nnon-causal knowledge of the realizations in advance. Comparing the optimal\nsolutions in all three frameworks, the performance loss due to the lack of the\ntransmitter's information regarding the behaviors of the underlying Markov\nprocesses is quantified.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 15:35:31 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2012 12:22:40 GMT"}], "update_date": "2013-05-31", "authors_parsed": [["Blasco", "Pol", ""], ["G\u00fcnd\u00fcz", "Deniz", ""], ["Dohler", "Mischa", ""]]}, {"id": "1208.4773", "submitter": "Tobias Jung", "authors": "Tobias Jung, Louis Wehenkel, Damien Ernst, Francis Maes", "title": "Optimized Look-Ahead Tree Policies: A Bridge Between Look-Ahead Tree\n  Policies and Direct Policy Search", "comments": "In Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct policy search (DPS) and look-ahead tree (LT) policies are two widely\nused classes of techniques to produce high performance policies for sequential\ndecision-making problems. To make DPS approaches work well, one crucial issue\nis to select an appropriate space of parameterized policies with respect to the\ntargeted problem. A fundamental issue in LT approaches is that, to take good\ndecisions, such policies must develop very large look-ahead trees which may\nrequire excessive online computational resources. In this paper, we propose a\nnew hybrid policy learning scheme that lies at the intersection of DPS and LT,\nin which the policy is an algorithm that develops a small look-ahead tree in a\ndirected way, guided by a node scoring function that is learned through DPS.\nThe LT-based representation is shown to be a versatile way of representing\npolicies in a DPS scheme, while at the same time, DPS enables to significantly\nreduce the size of the look-ahead trees that are required to take high-quality\ndecisions.\n  We experimentally compare our method with two other state-of-the-art DPS\ntechniques and four common LT policies on four benchmark domains and show that\nit combines the advantages of the two techniques from which it originates. In\nparticular, we show that our method: (1) produces overall better performing\npolicies than both pure DPS and pure LT policies, (2) requires a substantially\nsmaller number of policy evaluations than other DPS techniques, (3) is easy to\ntune and (4) results in policies that are quite robust with respect to\nperturbations of the initial conditions.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2012 14:48:52 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Jung", "Tobias", ""], ["Wehenkel", "Louis", ""], ["Ernst", "Damien", ""], ["Maes", "Francis", ""]]}, {"id": "1208.5003", "submitter": "Paul Vitanyi", "authors": "Paul M. B. Vitanyi (CWI and University of Amsterdam) and Nick Chater\n  (Behavioural Science Group, Warwick Business School, University of Warwick)", "title": "Identification of Probabilities of Languages", "comments": "23 pages LaTeX, no pictures 1311.7385 This paper has been withdrawn\n  by the auther due to crucial errors. The same subject is attacked more\n  succesfully with reduced claims in ArXiV 1311.7385", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inferring the probability distribution associated\nwith a language, given data consisting of an infinite sequence of elements of\nthe languge. We do this under two assumptions on the algorithms concerned: (i)\nlike a real-life algorothm it has round-off errors, and (ii) it has no\nround-off errors. Assuming (i) we (a) consider a probability mass function of\nthe elements of the language if the data are drawn independent identically\ndistributed (i.i.d.), provided the probability mass function is computable and\nhas a finite expectation. We give an effective procedure to almost surely\nidentify in the limit the target probability mass function using the Strong Law\nof Large Numbers. Second (b) we treat the case of possibly incomputable\nprobabilistic mass functions in the above setting. In this case we can only\npointswize converge to the target probability mass function almost surely.\nThird (c) we consider the case where the data are dependent assuming they are\ntypical for at least one computable measure and the language is finite. There\nis an effective procedure to identify by infinite recurrence a nonempty subset\nof the computable measures according to which the data is typical. Here we use\nthe theory of Kolmogorov complexity. Assuming (ii) we obtain the weaker result\nfor (a) that the target distribution is identified by infinite recurrence\nalmost surely; (b) stays the same as under assumption (i). We consider the\nassociated predictions.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2012 16:29:48 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2012 15:07:24 GMT"}, {"version": "v3", "created": "Tue, 15 Jul 2014 15:33:38 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Vitanyi", "Paul M. B.", "", "CWI and University of Amsterdam"], ["Chater", "Nick", "", "Behavioural Science Group, Warwick Business School, University of Warwick"]]}, {"id": "1208.5062", "submitter": "Yao Xie", "authors": "Yao Xie, Jiaji Huang, Rebecca Willett", "title": "Changepoint detection for high-dimensional time series with missing data", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2012.2234082", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel approach to change-point detection when the\nobserved high-dimensional data may have missing elements. The performance of\nclassical methods for change-point detection typically scales poorly with the\ndimensionality of the data, so that a large number of observations are\ncollected after the true change-point before it can be reliably detected.\nFurthermore, missing components in the observed data handicap conventional\napproaches. The proposed method addresses these challenges by modeling the\ndynamic distribution underlying the data as lying close to a time-varying\nlow-dimensional submanifold embedded within the ambient observation space.\nSpecifically, streaming data is used to track a submanifold approximation,\nmeasure deviations from this approximation, and calculate a series of\nstatistics of the deviations for detecting when the underlying manifold has\nchanged in a sharp or unexpected manner. The approach described in this paper\nleverages several recent results in the field of high-dimensional data\nanalysis, including subspace tracking with missing data, multiscale analysis\ntechniques for point clouds, online optimization, and change-point detection\nperformance analysis. Simulations and experiments highlight the robustness and\nefficacy of the proposed approach in detecting an abrupt change in an otherwise\nslowly varying low-dimensional manifold.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2012 20:36:36 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2012 20:27:36 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2012 20:30:49 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Xie", "Yao", ""], ["Huang", "Jiaji", ""], ["Willett", "Rebecca", ""]]}, {"id": "1208.5801", "submitter": "Nivan Ferreira Jr", "authors": "Nivan Ferreira, James T. Klosowski, Carlos Scheidegger, Claudio Silva", "title": "Vector Field k-Means: Clustering Trajectories by Fitting Multiple Vector\n  Fields", "comments": "30 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists study trajectory data to understand trends in movement patterns,\nsuch as human mobility for traffic analysis and urban planning. There is a\npressing need for scalable and efficient techniques for analyzing this data and\ndiscovering the underlying patterns. In this paper, we introduce a novel\ntechnique which we call vector-field $k$-means.\n  The central idea of our approach is to use vector fields to induce a\nsimilarity notion between trajectories. Other clustering algorithms seek a\nrepresentative trajectory that best describes each cluster, much like $k$-means\nidentifies a representative \"center\" for each cluster. Vector-field $k$-means,\non the other hand, recognizes that in all but the simplest examples, no single\ntrajectory adequately describes a cluster. Our approach is based on the premise\nthat movement trends in trajectory data can be modeled as flows within multiple\nvector fields, and the vector field itself is what defines each of the\nclusters. We also show how vector-field $k$-means connects techniques for\nscalar field design on meshes and $k$-means clustering.\n  We present an algorithm that finds a locally optimal clustering of\ntrajectories into vector fields, and demonstrate how vector-field $k$-means can\nbe used to mine patterns from trajectory data. We present experimental evidence\nof its effectiveness and efficiency using several datasets, including\nhistorical hurricane data, GPS tracks of people and vehicles, and anonymous\ncall records from a large phone company. We compare our results to previous\ntrajectory clustering techniques, and find that our algorithm performs faster\nin practice than the current state-of-the-art in trajectory clustering, in some\nexamples by a large margin.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2012 21:51:36 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2012 18:17:40 GMT"}], "update_date": "2012-09-03", "authors_parsed": [["Ferreira", "Nivan", ""], ["Klosowski", "James T.", ""], ["Scheidegger", "Carlos", ""], ["Silva", "Claudio", ""]]}, {"id": "1208.6231", "submitter": "Beyza Ermis Ms", "authors": "Beyza Ermi\\c{s} and Evrim Acar and A. Taylan Cemgil", "title": "Link Prediction via Generalized Coupled Tensor Factorisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study deals with the missing link prediction problem: the problem of\npredicting the existence of missing connections between entities of interest.\nWe address link prediction using coupled analysis of relational datasets\nrepresented as heterogeneous data, i.e., datasets in the form of matrices and\nhigher-order tensors. We propose to use an approach based on probabilistic\ninterpretation of tensor factorisation models, i.e., Generalised Coupled Tensor\nFactorisation, which can simultaneously fit a large class of tensor models to\nhigher-order tensors/matrices with com- mon latent factors using different loss\nfunctions. Numerical experiments demonstrate that joint analysis of data from\nmultiple sources via coupled factorisation improves the link prediction\nperformance and the selection of right loss function and tensor model is\ncrucial for accurately predicting missing links.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2012 16:48:05 GMT"}], "update_date": "2012-08-31", "authors_parsed": [["Ermi\u015f", "Beyza", ""], ["Acar", "Evrim", ""], ["Cemgil", "A. Taylan", ""]]}, {"id": "1208.6310", "submitter": "Irina Topalova", "authors": "Irina Topalova", "title": "Automated Marble Plate Classification System Based On Different Neural\n  Network Input Training Sets and PLC Implementation", "comments": "7 pages, 11 figures, 1 table, (IJARAI) International Journal of\n  Advanced Research in Artificial Intelligence, Vol. 1, No. 2, 2012;\n  ISSN:2165-4069(Online), ISSN:2165-4050 (Print)", "journal-ref": "(IJARAI) International Journal of Advanced Research in Artificial\n  Intelligence, Vol. 1, No. 2, 2012, 50-56", "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The process of sorting marble plates according to their surface texture is an\nimportant task in the automated marble plate production. Nowadays some\ninspection systems in marble industry that automate the classification tasks\nare too expensive and are compatible only with specific technological equipment\nin the plant. In this paper a new approach to the design of an Automated Marble\nPlate Classification System (AMPCS),based on different neural network input\ntraining sets is proposed, aiming at high classification accuracy using simple\nprocessing and application of only standard devices. It is based on training a\nclassification MLP neural network with three different input training sets:\nextracted texture histograms, Discrete Cosine and Wavelet Transform over the\nhistograms. The algorithm is implemented in a PLC for real-time operation. The\nperformance of the system is assessed with each one of the input training sets.\nThe experimental test results regarding classification accuracy and quick\noperation are represented and discussed.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2012 12:14:46 GMT"}], "update_date": "2012-09-03", "authors_parsed": [["Topalova", "Irina", ""]]}, {"id": "1208.6335", "submitter": "Aman Chadha Mr.", "authors": "Aman Chadha, Sushmit Mallik and Ravdeep Johar", "title": "Comparative Study and Optimization of Feature-Extraction Techniques for\n  Content based Image Retrieval", "comments": "8 pages, 16 figures, 11 tables", "journal-ref": "International Journal of Computer Applications 52(20):35-42, 2012", "doi": "10.5120/8320-1959", "report-no": "Volume 52, Number 20, 2012", "categories": "cs.CV cs.AI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of a Content-Based Image Retrieval (CBIR) system, also known as Query\nby Image Content (QBIC), is to help users to retrieve relevant images based on\ntheir contents. CBIR technologies provide a method to find images in large\ndatabases by using unique descriptors from a trained image. The image\ndescriptors include texture, color, intensity and shape of the object inside an\nimage. Several feature-extraction techniques viz., Average RGB, Color Moments,\nCo-occurrence, Local Color Histogram, Global Color Histogram and Geometric\nMoment have been critically compared in this paper. However, individually these\ntechniques result in poor performance. So, combinations of these techniques\nhave also been evaluated and results for the most efficient combination of\ntechniques have been presented and optimized for each class of image query. We\nalso propose an improvement in image retrieval performance by introducing the\nidea of Query modification through image cropping. It enables the user to\nidentify a region of interest and modify the initial query to refine and\npersonalize the image retrieval results.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2012 23:50:06 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 01:34:05 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Chadha", "Aman", ""], ["Mallik", "Sushmit", ""], ["Johar", "Ravdeep", ""]]}, {"id": "1208.6338", "submitter": "Sumio Watanabe", "authors": "Sumio Watanabe", "title": "A Widely Applicable Bayesian Information Criterion", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical model or a learning machine is called regular if the map taking\na parameter to a probability distribution is one-to-one and if its Fisher\ninformation matrix is always positive definite. If otherwise, it is called\nsingular. In regular statistical models, the Bayes free energy, which is\ndefined by the minus logarithm of Bayes marginal likelihood, can be\nasymptotically approximated by the Schwarz Bayes information criterion (BIC),\nwhereas in singular models such approximation does not hold.\n  Recently, it was proved that the Bayes free energy of a singular model is\nasymptotically given by a generalized formula using a birational invariant, the\nreal log canonical threshold (RLCT), instead of half the number of parameters\nin BIC. Theoretical values of RLCTs in several statistical models are now being\ndiscovered based on algebraic geometrical methodology. However, it has been\ndifficult to estimate the Bayes free energy using only training samples,\nbecause an RLCT depends on an unknown true distribution.\n  In the present paper, we define a widely applicable Bayesian information\ncriterion (WBIC) by the average log likelihood function over the posterior\ndistribution with the inverse temperature $1/\\log n$, where $n$ is the number\nof training samples. We mathematically prove that WBIC has the same asymptotic\nexpansion as the Bayes free energy, even if a statistical model is singular for\nand unrealizable by a statistical model. Since WBIC can be numerically\ncalculated without any information about a true distribution, it is a\ngeneralized version of BIC onto singular statistical models.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2012 00:31:34 GMT"}], "update_date": "2012-09-03", "authors_parsed": [["Watanabe", "Sumio", ""]]}]