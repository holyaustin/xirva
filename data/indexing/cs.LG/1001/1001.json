[{"id": "1001.0405", "submitter": "Hanna Mazzawi", "authors": "Nader H. Bshouty and Hanna Mazzawi", "title": "Optimal Query Complexity for Reconstructing Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of reconstructing a hidden weighted\nhypergraph of constant rank using additive queries. We prove the following: Let\n$G$ be a weighted hidden hypergraph of constant rank with n vertices and $m$\nhyperedges. For any $m$ there exists a non-adaptive algorithm that finds the\nedges of the graph and their weights using $$ O(\\frac{m\\log n}{\\log m}) $$\nadditive queries. This solves the open problem in [S. Choi, J. H. Kim. Optimal\nQuery Complexity Bounds for Finding Graphs. {\\em STOC}, 749--758,~2008].\n  When the weights of the hypergraph are integers that are less than\n$O(poly(n^d/m))$ where $d$ is the rank of the hypergraph (and therefore for\nunweighted hypergraphs) there exists a non-adaptive algorithm that finds the\nedges of the graph and their weights using $$ O(\\frac{m\\log \\frac{n^d}{m}}{\\log\nm}). $$ additive queries.\n  Using the information theoretic bound the above query complexities are tight.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2010 19:54:40 GMT"}], "update_date": "2010-01-05", "authors_parsed": [["Bshouty", "Nader H.", ""], ["Mazzawi", "Hanna", ""]]}, {"id": "1001.0591", "submitter": "Jeff M Phillips", "authors": "Sarang Joshi, Raj Varma Kommaraju, Jeff M. Phillips, and Suresh\n  Venkatasubramanian", "title": "Comparing Distributions and Shapes using the Kernel Distance", "comments": "20 pages. In Proceedings 27th Symposium on Computational Geometry,\n  2011. See also (http://arxiv.org/abs/1103.1625) for more background on the\n  kernel distance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting with a similarity function between objects, it is possible to define\na distance metric on pairs of objects, and more generally on probability\ndistributions over them. These distance metrics have a deep basis in functional\nanalysis, measure theory and geometric measure theory, and have a rich\nstructure that includes an isometric embedding into a (possibly infinite\ndimensional) Hilbert space. They have recently been applied to numerous\nproblems in machine learning and shape analysis.\n  In this paper, we provide the first algorithmic analysis of these distance\nmetrics. Our main contributions are as follows: (i) We present fast\napproximation algorithms for computing the kernel distance between two point\nsets P and Q that runs in near-linear time in the size of (P cup Q) (note that\nan explicit calculation would take quadratic time). (ii) We present\npolynomial-time algorithms for approximately minimizing the kernel distance\nunder rigid transformation; they run in time O(n + poly(1/epsilon, log n)).\n(iii) We provide several general techniques for reducing complex objects to\nconvenient sparse representations (specifically to point sets or sets of points\nsets) which approximately preserve the kernel distance. In particular, this\nallows us to reduce problems of computing the kernel distance between various\ntypes of objects such as curves, surfaces, and distributions to computing the\nkernel distance between point sets. These take advantage of the reproducing\nkernel Hilbert space and a new relation linking binary range spaces to\ncontinuous range spaces with bounded fat-shattering dimension.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2010 22:21:08 GMT"}, {"version": "v2", "created": "Sun, 13 Mar 2011 22:40:00 GMT"}], "update_date": "2011-03-15", "authors_parsed": [["Joshi", "Sarang", ""], ["Kommaraju", "Raj Varma", ""], ["Phillips", "Jeff M.", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1001.0597", "submitter": "XuanLong Nguyen", "authors": "XuanLong Nguyen", "title": "Inference of global clusters from locally distributed data", "comments": "27 pages, 12 figures", "journal-ref": "Published in Bayesian Analysis, 5(4), 817--846, 2010", "doi": null, "report-no": "Technical report 504, Department of Statistics, University of\n  Michigan", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of analyzing the heterogeneity of clustering\ndistributions for multiple groups of observed data, each of which is indexed by\na covariate value, and inferring global clusters arising from observations\naggregated over the covariate domain. We propose a novel Bayesian nonparametric\nmethod reposing on the formalism of spatial modeling and a nested hierarchy of\nDirichlet processes. We provide an analysis of the model properties, relating\nand contrasting the notions of local and global clusters. We also provide an\nefficient inference algorithm, and demonstrate the utility of our method in\nseveral data examples, including the problem of object tracking and a global\nclustering analysis of functional data where the functional identity\ninformation is not available.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2010 22:47:31 GMT"}, {"version": "v2", "created": "Fri, 21 Jan 2011 15:42:15 GMT"}], "update_date": "2012-12-06", "authors_parsed": [["Nguyen", "XuanLong", ""]]}, {"id": "1001.0700", "submitter": "Amit Belani", "authors": "Amit Belani", "title": "Vandalism Detection in Wikipedia: a Bag-of-Words Classifier Approach", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bag-of-words based probabilistic classifier is trained using regularized\nlogistic regression to detect vandalism in the English Wikipedia. Isotonic\nregression is used to calibrate the class membership probabilities. Learning\ncurve, reliability, ROC, and cost analysis are performed.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2010 13:06:21 GMT"}], "update_date": "2010-01-06", "authors_parsed": [["Belani", "Amit", ""]]}, {"id": "1001.0879", "submitter": "Fedor Zhdanov", "authors": "Fedor Zhdanov and Yuri Kalnishkan", "title": "Linear Probability Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-class classification is one of the most important tasks in machine\nlearning. In this paper we consider two online multi-class classification\nproblems: classification by a linear model and by a kernelized model. The\nquality of predictions is measured by the Brier loss function. We suggest two\ncomputationally efficient algorithms to work with these problems and prove\ntheoretical guarantees on their losses. We kernelize one of the algorithms and\nprove theoretical guarantees on its loss. We perform experiments and compare\nour algorithms with logistic regression.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2010 12:40:13 GMT"}], "update_date": "2010-01-07", "authors_parsed": [["Zhdanov", "Fedor", ""], ["Kalnishkan", "Yuri", ""]]}, {"id": "1001.1009", "submitter": "Frederic Thouin", "authors": "Frederic Thouin (1), Mark Coates (1), Michael Rabbat (1) ((1) McGill\n  University, Montreal, Canada)", "title": "Multi-path Probabilistic Available Bandwidth Estimation through Bayesian\n  Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing the largest rate at which data can be sent on an end-to-end path such\nthat the egress rate is equal to the ingress rate with high probability can be\nvery practical when choosing transmission rates in video streaming or selecting\npeers in peer-to-peer applications. We introduce probabilistic available\nbandwidth, which is defined in terms of ingress rates and egress rates of\ntraffic on a path, rather than in terms of capacity and utilization of the\nconstituent links of the path like the standard available bandwidth metric. In\nthis paper, we describe a distributed algorithm, based on a probabilistic\ngraphical model and Bayesian active learning, for simultaneously estimating the\nprobabilistic available bandwidth of multiple paths through a network. Our\nprocedure exploits the fact that each packet train provides information not\nonly about the path it traverses, but also about any path that shares a link\nwith the monitored path. Simulations and PlanetLab experiments indicate that\nthis process can dramatically reduce the number of probes required to generate\naccurate estimates.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2010 23:33:49 GMT"}], "update_date": "2010-01-08", "authors_parsed": [["Thouin", "Frederic", ""], ["Coates", "Mark", ""], ["Rabbat", "Michael", ""]]}, {"id": "1001.1020", "submitter": "Ping Li", "authors": "Ping Li", "title": "An Empirical Evaluation of Four Algorithms for Multi-Class\n  Classification: Mart, ABC-Mart, Robust LogitBoost, and ABC-LogitBoost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This empirical study is mainly devoted to comparing four tree-based boosting\nalgorithms: mart, abc-mart, robust logitboost, and abc-logitboost, for\nmulti-class classification on a variety of publicly available datasets. Some of\nthose datasets have been thoroughly tested in prior studies using a broad range\nof classification algorithms including SVM, neural nets, and deep learning.\n  In terms of the empirical classification errors, our experiment results\ndemonstrate:\n  1. Abc-mart considerably improves mart. 2. Abc-logitboost considerably\nimproves (robust) logitboost. 3. Robust) logitboost} considerably improves mart\non most datasets. 4. Abc-logitboost considerably improves abc-mart on most\ndatasets. 5. These four boosting algorithms (especially abc-logitboost)\noutperform SVM on many datasets. 6. Compared to the best deep learning methods,\nthese four boosting algorithms (especially abc-logitboost) are competitive.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2010 06:34:21 GMT"}], "update_date": "2010-01-08", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1001.1027", "submitter": "Jascha Sohl-Dickstein", "authors": "Jascha Sohl-Dickstein, Ching Ming Wang, Bruno A. Olshausen", "title": "An Unsupervised Algorithm For Learning Lie Group Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present several theoretical contributions which allow Lie groups to be fit\nto high dimensional datasets. Transformation operators are represented in their\neigen-basis, reducing the computational complexity of parameter estimation to\nthat of training a linear transformation model. A transformation specific\n\"blurring\" operator is introduced that allows inference to escape local minima\nvia a smoothing of the transformation space. A penalty on traversed manifold\ndistance is added which encourages the discovery of sparse, minimal distance,\ntransformations between states. Both learning and inference are demonstrated\nusing these methods for the full set of affine transformations on natural image\npatches. Transformation operators are then trained on natural video sequences.\nIt is shown that the learned video transformations provide a better description\nof inter-frame differences than the standard motion model based on rigid\ntranslation.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2010 06:22:56 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2010 07:18:39 GMT"}, {"version": "v3", "created": "Wed, 30 Nov 2011 04:35:48 GMT"}, {"version": "v4", "created": "Thu, 24 Jul 2014 23:34:43 GMT"}, {"version": "v5", "created": "Wed, 7 Jun 2017 17:05:16 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Sohl-Dickstein", "Jascha", ""], ["Wang", "Ching Ming", ""], ["Olshausen", "Bruno A.", ""]]}, {"id": "1001.1079", "submitter": "Ricardo Silva", "authors": "Ricardo Silva", "title": "Measuring Latent Causal Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering latent representations of the observed world has become\nincreasingly more relevant in data analysis. Much of the effort concentrates on\nbuilding latent variables which can be used in prediction problems, such as\nclassification and regression. A related goal of learning latent structure from\ndata is that of identifying which hidden common causes generate the\nobservations, such as in applications that require predicting the effect of\npolicies. This will be the main problem tackled in our contribution: given a\ndataset of indicators assumed to be generated by unknown and unmeasured common\ncauses, we wish to discover which hidden common causes are those, and how they\ngenerate our data. This is possible under the assumption that observed\nvariables are linear functions of the latent causes with additive noise.\nPrevious results in the literature present solutions for the case where each\nobserved variable is a noisy function of a single latent variable. We show how\nto extend the existing results for some cases where observed variables measure\nmore than one latent variable.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2010 14:41:21 GMT"}], "update_date": "2010-01-08", "authors_parsed": [["Silva", "Ricardo", ""]]}, {"id": "1001.2605", "submitter": "Bo Zhang", "authors": "Hong Qiao, Peng Zhang, Di Wang, Bo Zhang", "title": "An Explicit Nonlinear Mapping for Manifold Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold learning is a hot research topic in the field of computer science\nand has many applications in the real world. A main drawback of manifold\nlearning methods is, however, that there is no explicit mappings from the input\ndata manifold to the output embedding. This prohibits the application of\nmanifold learning methods in many practical problems such as classification and\ntarget detection. Previously, in order to provide explicit mappings for\nmanifold learning methods, many methods have been proposed to get an\napproximate explicit representation mapping with the assumption that there\nexists a linear projection between the high-dimensional data samples and their\nlow-dimensional embedding. However, this linearity assumption may be too\nrestrictive. In this paper, an explicit nonlinear mapping is proposed for\nmanifold learning, based on the assumption that there exists a polynomial\nmapping between the high-dimensional data samples and their low-dimensional\nrepresentations. As far as we know, this is the first time that an explicit\nnonlinear mapping for manifold learning is given. In particular, we apply this\nto the method of Locally Linear Embedding (LLE) and derive an explicit\nnonlinear manifold learning algorithm, named Neighborhood Preserving Polynomial\nEmbedding (NPPE). Experimental results on both synthetic and real-world data\nshow that the proposed mapping is much more effective in preserving the local\nneighborhood information and the nonlinear geometry of the high-dimensional\ndata samples than previous work.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2010 03:03:24 GMT"}], "update_date": "2010-01-18", "authors_parsed": [["Qiao", "Hong", ""], ["Zhang", "Peng", ""], ["Wang", "Di", ""], ["Zhang", "Bo", ""]]}, {"id": "1001.2709", "submitter": "Francesco Dinuzzo", "authors": "Francesco Dinuzzo", "title": "Kernel machines with two layers and multiple kernel learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the framework of kernel machines with two layers is\nintroduced, generalizing classical kernel methods. The new learning methodology\nprovide a formal connection between computational architectures with multiple\nlayers and the theme of kernel learning in standard regularization methods.\nFirst, a representer theorem for two-layer networks is presented, showing that\nfinite linear combinations of kernels on each layer are optimal architectures\nwhenever the corresponding functions solve suitable variational problems in\nreproducing kernel Hilbert spaces (RKHS). The input-output map expressed by\nthese architectures turns out to be equivalent to a suitable single-layer\nkernel machines in which the kernel function is also learned from the data.\nRecently, the so-called multiple kernel learning methods have attracted\nconsiderable attention in the machine learning literature. In this paper,\nmultiple kernel learning methods are shown to be specific cases of kernel\nmachines with two layers in which the second layer is linear. Finally, a simple\nand effective multiple kernel learning method called RLS2 (regularized least\nsquares with two layers) is introduced, and his performances on several\nlearning problems are extensively analyzed. An open source MATLAB toolbox to\ntrain and validate RLS2 models with a Graphic User Interface is available.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2010 15:10:39 GMT"}], "update_date": "2010-01-18", "authors_parsed": [["Dinuzzo", "Francesco", ""]]}, {"id": "1001.2813", "submitter": "Anthony Di Franco", "authors": "Anthony Di Franco", "title": "A Monte Carlo Algorithm for Universally Optimal Bayesian Sequence\n  Prediction and Planning", "comments": "Submitted to MDPI Algorithms Special Issue \"Algorithmic Complexity in\n  Physics & Embedded Artificial Intelligences\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO cond-mat.dis-nn cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is to address the question of whether we can in\nprinciple design rational decision-making agents or artificial intelligences\nembedded in computable physics such that their decisions are optimal in\nreasonable mathematical senses. Recent developments in rare event probability\nestimation, recursive bayesian inference, neural networks, and probabilistic\nplanning are sufficient to explicitly approximate reinforcement learners of the\nAIXI style with non-trivial model classes (here, the class of resource-bounded\nTuring machines). Consideration of the effects of resource limitations in a\nconcrete implementation leads to insights about possible architectures for\nlearning systems using optimal decision makers as components.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2010 01:10:17 GMT"}], "update_date": "2010-01-19", "authors_parsed": [["Di Franco", "Anthony", ""]]}, {"id": "1001.2957", "submitter": "Sumio Watanabe", "authors": "Sumio Watanabe", "title": "Asymptotic Learning Curve and Renormalizable Condition in Statistical\n  Learning Theory", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/233/1/012014", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayes statistics and statistical physics have the common mathematical\nstructure, where the log likelihood function corresponds to the random\nHamiltonian. Recently, it was discovered that the asymptotic learning curves in\nBayes estimation are subject to a universal law, even if the log likelihood\nfunction can not be approximated by any quadratic form. However, it is left\nunknown what mathematical property ensures such a universal law. In this paper,\nwe define a renormalizable condition of the statistical estimation problem, and\nshow that, under such a condition, the asymptotic learning curves are ensured\nto be subject to the universal law, even if the true distribution is\nunrealizable and singular for a statistical model. Also we study a\nnonrenormalizable case, in which the learning curves have the different\nasymptotic behaviors from the universal law.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2010 05:34:09 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2010 04:47:17 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Watanabe", "Sumio", ""]]}, {"id": "1001.3090", "submitter": "Dayu Huang", "authors": "Dayu Huang, Sean Meyn", "title": "Feature Extraction for Universal Hypothesis Testing via Rank-constrained\n  Optimization", "comments": "5 pages, 4 figures, submitted to ISIT 2010", "journal-ref": null, "doi": "10.1109/ISIT.2010.5513384", "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the construction of tests for universal hypothesis\ntesting problems, in which the alternate hypothesis is poorly modeled and the\nobservation space is large. The mismatched universal test is a feature-based\ntechnique for this purpose. In prior work it is shown that its\nfinite-observation performance can be much better than the (optimal) Hoeffding\ntest, and good performance depends crucially on the choice of features. The\ncontributions of this paper include: 1) We obtain bounds on the number of\n\\epsilon distinguishable distributions in an exponential family. 2) This\nmotivates a new framework for feature extraction, cast as a rank-constrained\noptimization problem. 3) We obtain a gradient-based algorithm to solve the\nrank-constrained optimization problem and prove its local convergence.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2010 17:07:03 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2010 19:18:47 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Huang", "Dayu", ""], ["Meyn", "Sean", ""]]}, {"id": "1001.3448", "submitter": "Mohsen Bayati", "authors": "Mohsen Bayati and Andrea Montanari", "title": "The dynamics of message passing on dense graphs, with applications to\n  compressed sensing", "comments": "41 pages", "journal-ref": "IEEE Transactions on Information Theory, Vol 57, Issue 2 pp.\n  764-785, 2011", "doi": "10.1109/TIT.2010.2094817", "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate message passing algorithms proved to be extremely effective in\nreconstructing sparse signals from a small number of incoherent linear\nmeasurements. Extensive numerical experiments further showed that their\ndynamics is accurately tracked by a simple one-dimensional iteration termed\nstate evolution. In this paper we provide the first rigorous foundation to\nstate evolution. We prove that indeed it holds asymptotically in the large\nsystem limit for sensing matrices with independent and identically distributed\ngaussian entries.\n  While our focus is on message passing algorithms for compressed sensing, the\nanalysis extends beyond this setting, to a general class of algorithms on dense\ngraphs. In this context, state evolution plays the role that density evolution\nhas for sparse graphs.\n  The proof technique is fundamentally different from the standard approach to\ndensity evolution, in that it copes with large number of short loops in the\nunderlying factor graph. It relies instead on a conditioning technique recently\ndeveloped by Erwin Bolthausen in the context of spin glass theory.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2010 02:57:15 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2010 18:49:24 GMT"}, {"version": "v3", "created": "Wed, 27 Oct 2010 07:58:49 GMT"}, {"version": "v4", "created": "Thu, 27 Jan 2011 18:55:05 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Bayati", "Mohsen", ""], ["Montanari", "Andrea", ""]]}, {"id": "1001.3478", "submitter": "William Jackson", "authors": "S.Kannan, R.Bhaskaran", "title": "Role of Interestingness Measures in CAR Rule Ordering for Associative\n  Classifier: An Empirical Approach", "comments": null, "journal-ref": "Journal of Computing, Vol. 2, Issue 1, January 2010", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Associative Classifier is a novel technique which is the integration of\nAssociation Rule Mining and Classification. The difficult task in building\nAssociative Classifier model is the selection of relevant rules from a large\nnumber of class association rules (CARs). A very popular method of ordering\nrules for selection is based on confidence, support and antecedent size (CSA).\nOther methods are based on hybrid orderings in which CSA method is combined\nwith other measures. In the present work, we study the effect of using\ndifferent interestingness measures of Association rules in CAR rule ordering\nand selection for associative classifier.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2010 07:30:02 GMT"}], "update_date": "2010-03-25", "authors_parsed": [["Kannan", "S.", ""], ["Bhaskaran", "R.", ""]]}, {"id": "1001.4140", "submitter": "Dakshina Ranjan Kisku", "authors": "Dakshina Ranjan Kisku, Hunny Mehrotra, Jamuna Kanta Sing, Phalguni\n  Gupta", "title": "SVM-based Multiview Face Recognition by Generalization of Discriminant\n  Analysis", "comments": "6 pages, 3 figures", "journal-ref": "International Journal of Computer Systems Science and Engineering\n  (formerly International Journal of Intelligent Systems and Technologies),\n  vol. 3, no. 3, pp. 174--179, 2008", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identity verification of authentic persons by their multiview faces is a real\nvalued problem in machine vision. Multiview faces are having difficulties due\nto non-linear representation in the feature space. This paper illustrates the\nusability of the generalization of LDA in the form of canonical covariate for\nface recognition to multiview faces. In the proposed work, the Gabor filter\nbank is used to extract facial features that characterized by spatial\nfrequency, spatial locality and orientation. Gabor face representation captures\nsubstantial amount of variations of the face instances that often occurs due to\nillumination, pose and facial expression changes. Convolution of Gabor filter\nbank to face images of rotated profile views produce Gabor faces with high\ndimensional features vectors. Canonical covariate is then used to Gabor faces\nto reduce the high dimensional feature spaces into low dimensional subspaces.\nFinally, support vector machines are trained with canonical sub-spaces that\ncontain reduced set of features and perform recognition task. The proposed\nsystem is evaluated with UMIST face database. The experiment results\ndemonstrate the efficiency and robustness of the proposed system with high\nrecognition rates.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2010 08:53:49 GMT"}], "update_date": "2010-01-26", "authors_parsed": [["Kisku", "Dakshina Ranjan", ""], ["Mehrotra", "Hunny", ""], ["Sing", "Jamuna Kanta", ""], ["Gupta", "Phalguni", ""]]}, {"id": "1001.4301", "submitter": "Marko Jankovic", "authors": "Marko V. Jankovic", "title": "Probabilistic Approach to Neural Networks Computation Based on Quantum\n  Probability Model Probabilistic Principal Subspace Analysis Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce elements of probabilistic model that is suitable\nfor modeling of learning algorithms in biologically plausible artificial neural\nnetworks framework. Model is based on two of the main concepts in quantum\nphysics - a density matrix and the Born rule. As an example, we will show that\nproposed probabilistic interpretation is suitable for modeling of on-line\nlearning algorithms for PSA, which are preferably realized by a parallel\nhardware based on very simple computational units. Proposed concept (model) can\nbe used in the context of improving algorithm convergence speed, learning\nfactor choice, or input signal scale robustness. We are going to see how the\nBorn rule and the Hebbian learning rule are connected\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2010 02:09:42 GMT"}], "update_date": "2010-01-26", "authors_parsed": [["Jankovic", "Marko V.", ""]]}, {"id": "1001.4475", "submitter": "Gilles Stoltz", "authors": "S\\'ebastien Bubeck (INRIA Futurs), R\\'emi Munos (INRIA Lille - Nord\n  Europe), Gilles Stoltz (DMA, GREGH, INRIA Paris - Rocquencourt), Csaba\n  Szepesvari", "title": "X-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a generalization of stochastic bandits where the set of arms,\n$\\cX$, is allowed to be a generic measurable space and the mean-payoff function\nis \"locally Lipschitz\" with respect to a dissimilarity function that is known\nto the decision maker. Under this condition we construct an arm selection\npolicy, called HOO (hierarchical optimistic optimization), with improved regret\nbounds compared to previous results for a large class of problems. In\nparticular, our results imply that if $\\cX$ is the unit hypercube in a\nEuclidean space and the mean-payoff function has a finite number of global\nmaxima around which the behavior of the function is locally continuous with a\nknown smoothness degree, then the expected regret of HOO is bounded up to a\nlogarithmic factor by $\\sqrt{n}$, i.e., the rate of growth of the regret is\nindependent of the dimension of the space. We also prove the minimax optimality\nof our algorithm when the dissimilarity is a metric. Our basic strategy has\nquadratic computational complexity as a function of the number of time steps\nand does not rely on the doubling trick. We also introduce a modified strategy,\nwhich relies on the doubling trick but runs in linearithmic time. Both results\nare improvements with respect to previous approaches.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2010 16:30:15 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2011 07:03:48 GMT"}], "update_date": "2011-04-15", "authors_parsed": [["Bubeck", "S\u00e9bastien", "", "INRIA Futurs"], ["Munos", "R\u00e9mi", "", "INRIA Lille - Nord\n  Europe"], ["Stoltz", "Gilles", "", "DMA, GREGH, INRIA Paris - Rocquencourt"], ["Szepesvari", "Csaba", ""]]}, {"id": "1001.5007", "submitter": "Maxime Gariel", "authors": "Maxime Gariel, Ashok N. Srivastava, Eric Feron", "title": "Trajectory Clustering and an Application to Airspace Monitoring", "comments": "15 pages, 20 figures", "journal-ref": null, "doi": "10.1016/j.eij.2011.02.007", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework aimed at monitoring the behavior of aircraft\nin a given airspace. Nominal trajectories are determined and learned using data\ndriven methods. Standard procedures are used by air traffic controllers (ATC)\nto guide aircraft, ensure the safety of the airspace, and to maximize the\nrunway occupancy. Even though standard procedures are used by ATC, the control\nof the aircraft remains with the pilots, leading to a large variability in the\nflight patterns observed. Two methods to identify typical operations and their\nvariability from recorded radar tracks are presented. This knowledge base is\nthen used to monitor the conformance of current operations against operations\npreviously identified as standard. A tool called AirTrajectoryMiner is\npresented, aiming at monitoring the instantaneous health of the airspace, in\nreal time. The airspace is \"healthy\" when all aircraft are flying according to\nthe nominal procedures. A measure of complexity is introduced, measuring the\nconformance of current flight to nominal flight patterns. When an aircraft does\nnot conform, the complexity increases as more attention from ATC is required to\nensure a safe separation between aircraft.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2010 19:24:33 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2010 21:23:03 GMT"}], "update_date": "2011-03-30", "authors_parsed": [["Gariel", "Maxime", ""], ["Srivastava", "Ashok N.", ""], ["Feron", "Eric", ""]]}, {"id": "1001.5348", "submitter": "Csn  InterJRI", "authors": "Suresh Chandra Satapathy, Gunanidhi Pradhan, Sabyasachi Pattnaik,\n  J.V.R. Murthy, P.V.G.D. Prasad Reddy", "title": "Performance Comparisons of PSO based Clustering", "comments": "6 pages 2 figures", "journal-ref": "InterJRI Computer Science and Networking, Volume 1, pp18-23, 2009", "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we have investigated the performance of PSO Particle Swarm\nOptimization based clustering on few real world data sets and one artificial\ndata set. The performances are measured by two metric namely quantization error\nand inter-cluster distance. The K means clustering algorithm is first\nimplemented for all data sets, the results of which form the basis of\ncomparison of PSO based approaches. We have explored different variants of PSO\nsuch as gbest, lbest ring, lbest vonneumann and Hybrid PSO for comparison\npurposes. The results reveal that PSO based clustering algorithms perform\nbetter compared to K means in all data sets.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2010 08:10:26 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Satapathy", "Suresh Chandra", ""], ["Pradhan", "Gunanidhi", ""], ["Pattnaik", "Sabyasachi", ""], ["Murthy", "J. V. R.", ""], ["Reddy", "P. V. G. D. Prasad", ""]]}]