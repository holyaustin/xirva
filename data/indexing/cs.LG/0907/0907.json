[{"id": "0907.0229", "submitter": "Sergey Polikarpov", "authors": "S. V. Polikarpov, V. S. Dergachev, K. E. Rumyantsev, D. M. Golubchikov", "title": "A new model of artificial neuron: cyberneuron and its use", "comments": "23 pages, 23 figures, in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a new type of artificial neuron, called the authors\n\"cyberneuron\". Unlike classical models of artificial neurons, this type of\nneuron used table substitution instead of the operation of multiplication of\ninput values for the weights. This allowed to significantly increase the\ninformation capacity of a single neuron, but also greatly simplify the process\nof learning. Considered an example of the use of \"cyberneuron\" with the task of\ndetecting computer viruses.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2009 19:54:39 GMT"}], "update_date": "2009-07-02", "authors_parsed": [["Polikarpov", "S. V.", ""], ["Dergachev", "V. S.", ""], ["Rumyantsev", "K. E.", ""], ["Golubchikov", "D. M.", ""]]}, {"id": "0907.0453", "submitter": "Leonid (Aryeh) Kontorovich", "authors": "Leonid Aryeh Kontorovich", "title": "Random DFAs are Efficiently PAC Learnable", "comments": "withdrawn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has been withdrawn due to an error found by Dana Angluin and Lev\nReyzin.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2009 17:54:45 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2009 08:59:45 GMT"}], "update_date": "2009-07-20", "authors_parsed": [["Kontorovich", "Leonid Aryeh", ""]]}, {"id": "0907.0746", "submitter": "Marcus Hutter", "authors": "Marcus Hutter", "title": "Open Problems in Universal Induction & Intelligence", "comments": "32 LaTeX pages", "journal-ref": "Algorithms, 3:2 (2009) pages 879-906", "doi": null, "report-no": null, "categories": "cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specialized intelligent systems can be found everywhere: finger print,\nhandwriting, speech, and face recognition, spam filtering, chess and other game\nprograms, robots, et al. This decade the first presumably complete mathematical\ntheory of artificial intelligence based on universal\ninduction-prediction-decision-action has been proposed. This\ninformation-theoretic approach solidifies the foundations of inductive\ninference and artificial intelligence. Getting the foundations right usually\nmarks a significant progress and maturing of a field. The theory provides a\ngold standard and guidance for researchers working on intelligent algorithms.\nThe roots of universal induction have been laid exactly half-a-century ago and\nthe roots of universal intelligence exactly one decade ago. So it is timely to\ntake stock of what has been achieved and what remains to be done. Since there\nare already good recent surveys, I describe the state-of-the-art only in\npassing and refer the reader to the literature. This article concentrates on\nthe open problems in universal induction and its extension to universal\nintelligence.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2009 08:45:22 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Hutter", "Marcus", ""]]}, {"id": "0907.0783", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III", "title": "Bayesian Multitask Learning with Latent Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We learn multiple hypotheses for related tasks under a latent hierarchical\nrelationship between tasks. We exploit the intuition that for domain\nadaptation, we wish to share classifier structure, but for multitask learning,\nwe wish to share covariance structure. Our hierarchical model is seen to\nsubsume several previously proposed multitask learning models and performs well\non three distinct real-world data sets.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2009 18:35:52 GMT"}], "update_date": "2009-07-07", "authors_parsed": [["Daum\u00e9", "Hal", "III"]]}, {"id": "0907.0784", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III", "title": "Cross-Task Knowledge-Constrained Self Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithmic framework for learning multiple related tasks. Our\nframework exploits a form of prior knowledge that relates the output spaces of\nthese tasks. We present PAC learning results that analyze the conditions under\nwhich such learning is possible. We present results on learning a shallow\nparser and named-entity recognition system that exploits our framework, showing\nconsistent improvements over baseline methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2009 18:42:01 GMT"}], "update_date": "2009-07-07", "authors_parsed": [["Daum\u00e9", "Hal", "III"]]}, {"id": "0907.0786", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III and John Langford and Daniel Marcu", "title": "Search-based Structured Prediction", "comments": null, "journal-ref": "Machine Learning Journal 2009", "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Searn, an algorithm for integrating search and learning to solve\ncomplex structured prediction problems such as those that occur in natural\nlanguage, speech, computational biology, and vision. Searn is a meta-algorithm\nthat transforms these complex problems into simple classification problems to\nwhich any binary classifier may be applied. Unlike current algorithms for\nstructured learning that require decomposition of both the loss function and\nthe feature functions over the predicted structure, Searn is able to learn\nprediction functions for any loss function and any class of features. Moreover,\nSearn comes with a strong, natural theoretical guarantee: good performance on\nthe derived classification problems implies good performance on the structured\nprediction problem.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2009 18:48:34 GMT"}], "update_date": "2009-07-07", "authors_parsed": [["Daum\u00e9", "Hal", "III"], ["Langford", "John", ""], ["Marcu", "Daniel", ""]]}, {"id": "0907.0808", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III and Daniel Marcu", "title": "A Bayesian Model for Supervised Clustering with the Dirichlet Process\n  Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian framework for tackling the supervised clustering\nproblem, the generic problem encountered in tasks such as reference matching,\ncoreference resolution, identity uncertainty and record linkage. Our clustering\nmodel is based on the Dirichlet process prior, which enables us to define\ndistributions over the countably infinite sets that naturally arise in this\nproblem. We add supervision to our model by positing the existence of a set of\nunobserved random variables (we call these \"reference types\") that are generic\nacross all clusters. Inference in our framework, which requires integrating\nover infinitely many parameters, is solved using Markov chain Monte Carlo\ntechniques. We present algorithms for both conjugate and non-conjugate priors.\nWe present a simple--but general--parameterization of our model based on a\nGaussian assumption. We evaluate this model on one artificial task and three\nreal-world tasks, comparing it against both unsupervised and state-of-the-art\nsupervised algorithms. Our results show that our model is able to outperform\nother models across a variety of tasks and performance metrics.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2009 22:32:58 GMT"}], "update_date": "2009-07-07", "authors_parsed": [["Daum\u00e9", "Hal", "III"], ["Marcu", "Daniel", ""]]}, {"id": "0907.0809", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III and Daniel Marcu", "title": "Learning as Search Optimization: Approximate Large Margin Methods for\n  Structured Prediction", "comments": null, "journal-ref": "ICML 2005", "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mappings to structured output spaces (strings, trees, partitions, etc.) are\ntypically learned using extensions of classification algorithms to simple\ngraphical structures (eg., linear chains) in which search and parameter\nestimation can be performed exactly. Unfortunately, in many complex problems,\nit is rare that exact search or parameter estimation is tractable. Instead of\nlearning exact models and searching via heuristic means, we embrace this\ndifficulty and treat the structured output problem in terms of approximate\nsearch. We present a framework for learning as search optimization, and two\nparameter updates with convergence theorems and bounds. Empirical evidence\nshows that our integrated approach to learning and decoding can outperform\nexact models at smaller computational cost.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2009 22:34:25 GMT"}], "update_date": "2009-07-07", "authors_parsed": [["Daum\u00e9", "Hal", "III"], ["Marcu", "Daniel", ""]]}, {"id": "0907.1054", "submitter": "Kaushik Sinha", "authors": "Mikhail Belkin and Kaushik Sinha", "title": "Learning Gaussian Mixtures with Arbitrary Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a method for learning the parameters of a mixture of\n$k$ identical spherical Gaussians in $n$-dimensional space with an arbitrarily\nsmall separation between the components. Our algorithm is polynomial in all\nparameters other than $k$. The algorithm is based on an appropriate grid search\nover the space of parameters. The theoretical analysis of the algorithm hinges\non a reduction of the problem to 1 dimension and showing that two 1-dimensional\nmixtures whose densities are close in the $L^2$ norm must have similar means\nand mixing coefficients. To produce such a lower bound for the $L^2$ norm in\nterms of the distances between the corresponding means, we analyze the behavior\nof the Fourier transform of a mixture of Gaussians in 1 dimension around the\norigin, which turns out to be closely related to the properties of the\nVandermonde matrix obtained from the component means. Analysis of this matrix\ntogether with basic function approximation results allows us to provide a lower\nbound for the norm of the mixture in the Fourier domain.\n  In recent years much research has been aimed at understanding the\ncomputational aspects of learning parameters of Gaussians mixture distributions\nin high dimension. To the best of our knowledge all existing work on learning\nparameters of Gaussian mixtures assumes minimum separation between components\nof the mixture which is an increasing function of either the dimension of the\nspace $n$ or the number of components $k$. In our paper we prove the first\nresult showing that parameters of a $n$-dimensional Gaussian mixture model with\narbitrarily small component separation can be learned in time polynomial in\n$n$.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2009 17:41:57 GMT"}, {"version": "v2", "created": "Thu, 13 May 2010 19:20:36 GMT"}], "update_date": "2010-05-14", "authors_parsed": [["Belkin", "Mikhail", ""], ["Sinha", "Kaushik", ""]]}, {"id": "0907.1413", "submitter": "Anand Sarwate", "authors": "Kamalika Chaudhuri, Anand D. Sarwate", "title": "Privacy constraints in regularized convex optimization", "comments": "This paper has been withdrawn by the authors due to some errors.\n  Corrections have been included in arXiv:0912.0071v4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is withdrawn due to some errors, which are corrected in\narXiv:0912.0071v4 [cs.LG].\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2009 06:51:54 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2010 20:36:43 GMT"}, {"version": "v3", "created": "Tue, 21 Jun 2011 17:05:53 GMT"}], "update_date": "2011-06-22", "authors_parsed": [["Chaudhuri", "Kamalika", ""], ["Sarwate", "Anand D.", ""]]}, {"id": "0907.1812", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III", "title": "Fast search for Dirichlet process mixture models", "comments": null, "journal-ref": "AIStats 2007", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dirichlet process (DP) mixture models provide a flexible Bayesian framework\nfor density estimation. Unfortunately, their flexibility comes at a cost:\ninference in DP mixture models is computationally expensive, even when\nconjugate distributions are used. In the common case when one seeks only a\nmaximum a posteriori assignment of data points to clusters, we show that search\nalgorithms provide a practical alternative to expensive MCMC and variational\ntechniques. When a true posterior sample is desired, the solution found by\nsearch can serve as a good initializer for MCMC. Experimental results show that\nusing these techniques is it possible to apply DP mixture models to very large\ndata sets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2009 13:23:37 GMT"}], "update_date": "2009-07-13", "authors_parsed": [["Daum\u00e9", "Hal", "III"]]}, {"id": "0907.1814", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III", "title": "Bayesian Query-Focused Summarization", "comments": null, "journal-ref": "ACL 2006", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present BayeSum (for ``Bayesian summarization''), a model for sentence\nextraction in query-focused summarization. BayeSum leverages the common case in\nwhich multiple documents are relevant to a single query. Using these documents\nas reinforcement for query terms, BayeSum is not afflicted by the paucity of\ninformation in short queries. We show that approximate inference in BayeSum is\npossible on large data sets and results in a state-of-the-art summarization\nsystem. Furthermore, we show how BayeSum can be understood as a justified query\nexpansion technique in the language modeling for IR framework.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2009 13:24:55 GMT"}], "update_date": "2009-07-13", "authors_parsed": [["Daum\u00e9", "Hal", "III"]]}, {"id": "0907.1815", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III", "title": "Frustratingly Easy Domain Adaptation", "comments": null, "journal-ref": "ACL 2007", "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an approach to domain adaptation that is appropriate exactly in\nthe case when one has enough ``target'' data to do slightly better than just\nusing only ``source'' data. Our approach is incredibly simple, easy to\nimplement as a preprocessing step (10 lines of Perl!) and outperforms\nstate-of-the-art approaches on a range of datasets. Moreover, it is trivially\nextended to a multi-domain adaptation problem, where one has data from a\nvariety of different domains.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2009 13:25:48 GMT"}], "update_date": "2009-07-13", "authors_parsed": [["Daum\u00e9", "Hal", "III"]]}, {"id": "0907.1916", "submitter": "Johanne Cohen", "authors": "Olivier Bournez and Johanne Cohen", "title": "Learning Equilibria in Games by Stochastic Distributed Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of fully stochastic and fully distributed algorithms,\nthat we prove to learn equilibria in games.\n  Indeed, we consider a family of stochastic distributed dynamics that we prove\nto converge weakly (in the sense of weak convergence for probabilistic\nprocesses) towards their mean-field limit, i.e an ordinary differential\nequation (ODE) in the general case. We focus then on a class of stochastic\ndynamics where this ODE turns out to be related to multipopulation replicator\ndynamics.\n  Using facts known about convergence of this ODE, we discuss the convergence\nof the initial stochastic dynamics: For general games, there might be\nnon-convergence, but when convergence of the ODE holds, considered stochastic\nalgorithms converge towards Nash equilibria. For games admitting Lyapunov\nfunctions, that we call Lyapunov games, the stochastic dynamics converge. We\nprove that any ordinal potential game, and hence any potential game is a\nLyapunov game, with a multiaffine Lyapunov function. For Lyapunov games with a\nmultiaffine Lyapunov function, we prove that this Lyapunov function is a\nsuper-martingale over the stochastic dynamics. This leads a way to provide\nbounds on their time of convergence by martingale arguments. This applies in\nparticular for many classes of games that have been considered in literature,\nincluding several load balancing game scenarios and congestion games.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2009 21:26:54 GMT"}], "update_date": "2009-07-14", "authors_parsed": [["Bournez", "Olivier", ""], ["Cohen", "Johanne", ""]]}, {"id": "0907.2079", "submitter": "Yong Zhang", "authors": "Zhaosong Lu and Yong Zhang", "title": "An Augmented Lagrangian Approach for Sparse Principal Component Analysis", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a widely used technique for data\nanalysis and dimension reduction with numerous applications in science and\nengineering. However, the standard PCA suffers from the fact that the principal\ncomponents (PCs) are usually linear combinations of all the original variables,\nand it is thus often difficult to interpret the PCs. To alleviate this\ndrawback, various sparse PCA approaches were proposed in literature [15, 6, 17,\n28, 8, 25, 18, 7, 16]. Despite success in achieving sparsity, some important\nproperties enjoyed by the standard PCA are lost in these methods such as\nuncorrelation of PCs and orthogonality of loading vectors. Also, the total\nexplained variance that they attempt to maximize can be too optimistic. In this\npaper we propose a new formulation for sparse PCA, aiming at finding sparse and\nnearly uncorrelated PCs with orthogonal loading vectors while explaining as\nmuch of the total variance as possible. We also develop a novel augmented\nLagrangian method for solving a class of nonsmooth constrained optimization\nproblems, which is well suited for our formulation of sparse PCA. We show that\nit converges to a feasible point, and moreover under some regularity\nassumptions, it converges to a stationary point. Additionally, we propose two\nnonmonotone gradient methods for solving the augmented Lagrangian subproblems,\nand establish their global and local convergence. Finally, we compare our\nsparse PCA approach with several existing methods on synthetic, random, and\nreal data, respectively. The computational results demonstrate that the sparse\nPCs produced by our approach substantially outperform those by other methods in\nterms of total explained variance, correlation of PCs, and orthogonality of\nloading vectors.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2009 00:45:51 GMT"}], "update_date": "2009-07-14", "authors_parsed": [["Lu", "Zhaosong", ""], ["Zhang", "Yong", ""]]}, {"id": "0907.2222", "submitter": "Dilip Krishnaswamy", "authors": "Dilip Krishnaswamy, Shanyu Zhao", "title": "Network-aware Adaptation with Real-Time Channel Statistics for Wireless\n  LAN Multimedia Transmissions in the Digital Home", "comments": "6 pages, 12 figures", "journal-ref": "IEEE COMSWARE 2008, Jan 6-10 2008, Pages 714-719", "doi": "10.1109/COMSWA.2008.4554505", "report-no": null, "categories": "cs.NI cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper suggests the use of intelligent network-aware processing agents in\nwireless local area network drivers to generate metrics for bandwidth\nestimation based on real-time channel statistics to enable wireless multimedia\napplication adaptation. Various configurations in the wireless digital home are\nstudied and the experimental results with performance variations are presented.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2009 18:18:28 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Krishnaswamy", "Dilip", ""], ["Zhao", "Shanyu", ""]]}, {"id": "0907.3342", "submitter": "Gerard Bloch", "authors": "Mustapha Ouladsine (LSIS), G\\'erard Bloch (CRAN), Xavier Dovifaaz\n  (CRAN)", "title": "Neural Modeling and Control of Diesel Engine with Pollution Constraints", "comments": "15 pages", "journal-ref": "Journal of Intelligent and Robotic Systems 41, 2-3 (2005) 157-171", "doi": "10.1007/s10846-005-3806-y", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes a neural approach for modelling and control of a\nturbocharged Diesel engine. A neural model, whose structure is mainly based on\nsome physical equations describing the engine behaviour, is built for the\nrotation speed and the exhaust gas opacity. The model is composed of three\ninterconnected neural submodels, each of them constituting a nonlinear\nmulti-input single-output error model. The structural identification and the\nparameter estimation from data gathered on a real engine are described. The\nneural direct model is then used to determine a neural controller of the\nengine, in a specialized training scheme minimising a multivariable criterion.\nSimulations show the effect of the pollution constraint weighting on a\ntrajectory tracking of the engine speed. Neural networks, which are flexible\nand parsimonious nonlinear black-box models, with universal approximation\ncapabilities, can accurately describe or control complex nonlinear systems,\nwith little a priori theoretical knowledge. The presented work extends optimal\nneuro-control to the multivariable case and shows the flexibility of neural\noptimisers. Considering the preliminary results, it appears that neural\nnetworks can be used as embedded models for engine control, to satisfy the more\nand more restricting pollutant emission legislation. Particularly, they are\nable to model nonlinear dynamics and outperform during transients the control\nschemes based on static mappings.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2009 05:58:24 GMT"}], "update_date": "2019-08-24", "authors_parsed": [["Ouladsine", "Mustapha", "", "LSIS"], ["Bloch", "G\u00e9rard", "", "CRAN"], ["Dovifaaz", "Xavier", "", "CRAN"]]}, {"id": "0907.3986", "submitter": "Aleksandrs Slivkins", "authors": "Aleksandrs Slivkins", "title": "Contextual Bandits with Similarity Information", "comments": "This is the full version of a conference paper in COLT 2011, to\n  appear in JMLR in 2014. A preliminary version of this manuscript (with all\n  the results) has been posted to arXiv in February 2011. An earlier version on\n  arXiv, which does not include the results in Section 6, dates back to July\n  2009. The present revision addresses various presentation issues pointed out\n  by journal referees", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a multi-armed bandit (MAB) problem, an online algorithm makes a sequence\nof choices. In each round it chooses from a time-invariant set of alternatives\nand receives the payoff associated with this alternative. While the case of\nsmall strategy sets is by now well-understood, a lot of recent work has focused\non MAB problems with exponentially or infinitely large strategy sets, where one\nneeds to assume extra structure in order to make the problem tractable. In\nparticular, recent literature considered information on similarity between\narms.\n  We consider similarity information in the setting of \"contextual bandits\", a\nnatural extension of the basic MAB problem where before each round an algorithm\nis given the \"context\" -- a hint about the payoffs in this round. Contextual\nbandits are directly motivated by placing advertisements on webpages, one of\nthe crucial problems in sponsored search. A particularly simple way to\nrepresent similarity information in the contextual bandit setting is via a\n\"similarity distance\" between the context-arm pairs which gives an upper bound\non the difference between the respective expected payoffs.\n  Prior work on contextual bandits with similarity uses \"uniform\" partitions of\nthe similarity space, which is potentially wasteful. We design more efficient\nalgorithms that are based on adaptive partitions adjusted to \"popular\" context\nand \"high-payoff\" arms.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2009 06:41:33 GMT"}, {"version": "v2", "created": "Wed, 16 Feb 2011 00:49:41 GMT"}, {"version": "v3", "created": "Thu, 2 Jun 2011 17:32:29 GMT"}, {"version": "v4", "created": "Mon, 13 Jan 2014 17:30:07 GMT"}, {"version": "v5", "created": "Tue, 20 May 2014 03:52:46 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Slivkins", "Aleksandrs", ""]]}]