[{"id": "0711.0189", "submitter": "Ulrike von Luxburg", "authors": "Ulrike von Luxburg", "title": "A Tutorial on Spectral Clustering", "comments": null, "journal-ref": "Statistics and Computing 17(4), 2007", "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": null, "abstract": "  In recent years, spectral clustering has become one of the most popular\nmodern clustering algorithms. It is simple to implement, can be solved\nefficiently by standard linear algebra software, and very often outperforms\ntraditional clustering algorithms such as the k-means algorithm. On the first\nglance spectral clustering appears slightly mysterious, and it is not obvious\nto see why it works at all and what it really does. The goal of this tutorial\nis to give some intuition on those questions. We describe different graph\nLaplacians and their basic properties, present the most common spectral\nclustering algorithms, and derive those algorithms from scratch by several\ndifferent approaches. Advantages and disadvantages of the different spectral\nclustering algorithms are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2007 19:04:43 GMT"}], "update_date": "2007-11-02", "authors_parsed": [["von Luxburg", "Ulrike", ""]]}, {"id": "0711.1814", "submitter": "Francesca A. Lisi", "authors": "Francesca A. Lisi", "title": "Building Rules on Top of Ontologies for the Semantic Web with Inductive\n  Logic Programming", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": null, "abstract": "  Building rules on top of ontologies is the ultimate goal of the logical layer\nof the Semantic Web. To this aim an ad-hoc mark-up language for this layer is\ncurrently under discussion. It is intended to follow the tradition of hybrid\nknowledge representation and reasoning systems such as $\\mathcal{AL}$-log that\nintegrates the description logic $\\mathcal{ALC}$ and the function-free Horn\nclausal language \\textsc{Datalog}. In this paper we consider the problem of\nautomating the acquisition of these rules for the Semantic Web. We propose a\ngeneral framework for rule induction that adopts the methodological apparatus\nof Inductive Logic Programming and relies on the expressive and deductive power\nof $\\mathcal{AL}$-log. The framework is valid whatever the scope of induction\n(description vs. prediction) is. Yet, for illustrative purposes, we also\ndiscuss an instantiation of the framework which aims at description and turns\nout to be useful in Ontology Refinement.\n  Keywords: Inductive Logic Programming, Hybrid Knowledge Representation and\nReasoning Systems, Ontologies, Semantic Web.\n  Note: To appear in Theory and Practice of Logic Programming (TPLP)\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2007 17:15:34 GMT"}], "update_date": "2007-11-13", "authors_parsed": [["Lisi", "Francesca A.", ""]]}, {"id": "0711.2023", "submitter": "Peter Turney", "authors": "Peter D. Turney (National Research Council of Canada)", "title": "Empirical Evaluation of Four Tensor Decomposition Algorithms", "comments": "related work available at http://purl.org/peter.turney/", "journal-ref": null, "doi": null, "report-no": "ERB-1152, NRC-49877", "categories": "cs.LG cs.CL cs.IR", "license": null, "abstract": "  Higher-order tensor decompositions are analogous to the familiar Singular\nValue Decomposition (SVD), but they transcend the limitations of matrices\n(second-order tensors). SVD is a powerful tool that has achieved impressive\nresults in information retrieval, collaborative filtering, computational\nlinguistics, computational vision, and other fields. However, SVD is limited to\ntwo-dimensional arrays of data (two modes), and many potential applications\nhave three or more modes, which require higher-order tensor decompositions.\nThis paper evaluates four algorithms for higher-order tensor decomposition:\nHigher-Order Singular Value Decomposition (HO-SVD), Higher-Order Orthogonal\nIteration (HOOI), Slice Projection (SP), and Multislice Projection (MP). We\nmeasure the time (elapsed run time), space (RAM and disk space requirements),\nand fit (tensor reconstruction accuracy) of the four algorithms, under a\nvariety of conditions. We find that standard implementations of HO-SVD and HOOI\ndo not scale up to larger tensors, due to increasing RAM requirements. We\nrecommend HOOI for tensors that are small enough for the available RAM and MP\nfor larger tensors.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2007 16:28:47 GMT"}], "update_date": "2007-11-14", "authors_parsed": [["Turney", "Peter D.", "", "National Research Council of Canada"]]}, {"id": "0711.2801", "submitter": "Xinjia Chen", "authors": "Xinjia Chen", "title": "Inverse Sampling for Nonasymptotic Sequential Estimation of Bounded\n  Variable Means", "comments": "31 pages, 4 figures, added proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.TH", "license": null, "abstract": "  In this paper, we consider the nonasymptotic sequential estimation of means\nof random variables bounded in between zero and one. We have rigorously\ndemonstrated that, in order to guarantee prescribed relative precision and\nconfidence level, it suffices to continue sampling until the sample sum is no\nless than a certain bound and then take the average of samples as an estimate\nfor the mean of the bounded random variable. We have developed an explicit\nformula and a bisection search method for the determination of such bound of\nsample sum, without any knowledge of the bounded variable. Moreover, we have\nderived bounds for the distribution of sample size. In the special case of\nBernoulli random variables, we have established analytical and numerical\nmethods to further reduce the bound of sample sum and thus improve the\nefficiency of sampling. Furthermore, the fallacy of existing results are\ndetected and analyzed.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2007 17:28:23 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2007 21:59:44 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Chen", "Xinjia", ""]]}, {"id": "0711.2914", "submitter": "Tshilidzi Marwala", "authors": "Gidudu Anthony, Hulley Gregg and Marwala Tshilidzi", "title": "Image Classification Using SVMs: One-against-One Vs One-against-All", "comments": "Proccedings of the 28th Asian Conference on Remote Sensing, 2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": null, "abstract": "  Support Vector Machines (SVMs) are a relatively new supervised classification\ntechnique to the land cover mapping community. They have their roots in\nStatistical Learning Theory and have gained prominence because they are robust,\naccurate and are effective even when using a small training sample. By their\nnature SVMs are essentially binary classifiers, however, they can be adopted to\nhandle the multiple classification tasks common in remote sensing studies. The\ntwo approaches commonly used are the One-Against-One (1A1) and One-Against-All\n(1AA) techniques. In this paper, these approaches are evaluated in as far as\ntheir impact and implication for land cover mapping. The main finding from this\nresearch is that whereas the 1AA technique is more predisposed to yielding\nunclassified and mixed pixels, the resulting classification accuracy is not\nsignificantly different from 1A1 approach. It is the authors conclusion\ntherefore that ultimately the choice of technique adopted boils down to\npersonal preference and the uniqueness of the dataset at hand.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2007 12:25:00 GMT"}], "update_date": "2007-11-20", "authors_parsed": [["Anthony", "Gidudu", ""], ["Gregg", "Hulley", ""], ["Tshilidzi", "Marwala", ""]]}, {"id": "0711.3594", "submitter": "Chunjing Xu", "authors": "Chunjing Xu, Jianzhuang Liu, Xiaoou Tang", "title": "Clustering with Transitive Distance and K-Means Duality", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": null, "abstract": "  Recent spectral clustering methods are a propular and powerful technique for\ndata clustering. These methods need to solve the eigenproblem whose\ncomputational complexity is $O(n^3)$, where $n$ is the number of data samples.\nIn this paper, a non-eigenproblem based clustering method is proposed to deal\nwith the clustering problem. Its performance is comparable to the spectral\nclustering algorithms but it is more efficient with computational complexity\n$O(n^2)$. We show that with a transitive distance and an observed property,\ncalled K-means duality, our algorithm can be used to handle data sets with\ncomplex cluster shapes, multi-scale clusters, and noise. Moreover, no\nparameters except the number of clusters need to be set in our algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2007 15:05:35 GMT"}], "update_date": "2007-11-26", "authors_parsed": [["Xu", "Chunjing", ""], ["Liu", "Jianzhuang", ""], ["Tang", "Xiaoou", ""]]}, {"id": "0711.3675", "submitter": "Yong Wang", "authors": "Yong Wang, Bao-Gang Hu", "title": "Derivations of Normalized Mutual Information in Binary Classifications", "comments": "8 pages, 8 figures, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": null, "abstract": "  This correspondence studies the basic problem of classifications - how to\nevaluate different classifiers. Although the conventional performance indexes,\nsuch as accuracy, are commonly used in classifier selection or evaluation,\ninformation-based criteria, such as mutual information, are becoming popular in\nfeature/model selections. In this work, we propose to assess classifiers in\nterms of normalized mutual information (NI), which is novel and well defined in\na compact range for classifier evaluation. We derive close-form relations of\nnormalized mutual information with respect to accuracy, precision, and recall\nin binary classifications. By exploring the relations among them, we reveal\nthat NI is actually a set of nonlinear functions, with a concordant\npower-exponent form, to each performance index. The relations can also be\nexpressed with respect to precision and recall, or to false alarm and hitting\nrate (recall).\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2007 07:45:52 GMT"}], "update_date": "2007-11-26", "authors_parsed": [["Wang", "Yong", ""], ["Hu", "Bao-Gang", ""]]}, {"id": "0711.4452", "submitter": "Hirotaka Niitsuma", "authors": "Hirotaka Niitsuma and Takashi Okada", "title": "Covariance and PCA for Categorical Variables", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": null, "abstract": "  Covariances from categorical variables are defined using a regular simplex\nexpression for categories. The method follows the variance definition by Gini,\nand it gives the covariance as a solution of simultaneous equations. The\ncalculated results give reasonable values for test data. A method of principal\ncomponent analysis (RS-PCA) is also proposed using regular simplex expressions,\nwhich allows easy interpretation of the principal components. The proposed\nmethods apply to variable selection problem of categorical data USCensus1990\ndata. The proposed methods give appropriate criterion for the variable\nselection problem of categorical\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2007 12:05:47 GMT"}], "update_date": "2007-11-29", "authors_parsed": [["Niitsuma", "Hirotaka", ""], ["Okada", "Takashi", ""]]}]