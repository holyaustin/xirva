[{"id": "0901.0252", "submitter": "Amir Leshem", "authors": "Amir Leshem and Jacob Goldberger", "title": "MIMO decoding based on stochastic reconstruction from multiple\n  projections", "comments": "To appear in Proc. of ICASSP 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Least squares (LS) fitting is one of the most fundamental techniques in\nscience and engineering. It is used to estimate parameters from multiple noisy\nobservations. In many problems the parameters are known a-priori to be bounded\ninteger valued, or they come from a finite set of values on an arbitrary finite\nlattice. In this case finding the closest vector becomes NP-Hard problem. In\nthis paper we propose a novel algorithm, the Tomographic Least Squares Decoder\n(TLSD), that not only solves the ILS problem, better than other sub-optimal\ntechniques, but also is capable of providing the a-posteriori probability\ndistribution for each element in the solution vector. The algorithm is based on\nreconstruction of the vector from multiple two-dimensional projections. The\nprojections are carefully chosen to provide low computational complexity.\nUnlike other iterative techniques, such as the belief propagation, the proposed\nalgorithm has ensured convergence. We also provide simulated experiments\ncomparing the algorithm to other sub-optimal algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2009 16:46:05 GMT"}], "update_date": "2009-01-05", "authors_parsed": [["Leshem", "Amir", ""], ["Goldberger", "Jacob", ""]]}, {"id": "0901.0753", "submitter": "Sung-eok Jeon", "authors": "Sung-eok Jeon and Chuanyi Ji", "title": "Distributed Preemption Decisions: Probabilistic Graphical Model,\n  Algorithm and Near-Optimality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperative decision making is a vision of future network management and\ncontrol. Distributed connection preemption is an important example where nodes\ncan make intelligent decisions on allocating resources and controlling traffic\nflows for multi-class service networks. A challenge is that nodal decisions are\nspatially dependent as traffic flows trespass multiple nodes in a network.\nHence the performance-complexity trade-off becomes important, i.e., how\naccurate decisions are versus how much information is exchanged among nodes.\nConnection preemption is known to be NP-complete. Centralized preemption is\noptimal but computationally intractable. Decentralized preemption is\ncomputationally efficient but may result in a poor performance. This work\ninvestigates distributed preemption where nodes decide whether and which flows\nto preempt using only local information exchange with neighbors. We develop,\nbased on the probabilistic graphical models, a near-optimal distributed\nalgorithm. The algorithm is used by each node to make collectively near-optimal\npreemption decisions. We study trade-offs between near-optimal performance and\ncomplexity that corresponds to the amount of information-exchange of the\ndistributed algorithm. The algorithm is validated by both analysis and\nsimulation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2009 04:36:58 GMT"}], "update_date": "2009-01-08", "authors_parsed": [["Jeon", "Sung-eok", ""], ["Ji", "Chuanyi", ""]]}, {"id": "0901.0760", "submitter": "Marco Duarte", "authors": "Mark A. Davenport, Chinmay Hegde, Marco F. Duarte, and Richard G.\n  Baraniuk", "title": "A Theoretical Analysis of Joint Manifolds", "comments": "24 pages, 4 figures. Corrected typo on grant number in\n  acknowledgements, page 1", "journal-ref": null, "doi": null, "report-no": "TREE0901, Department of Electrical and Computer Engineering, Rice\n  University", "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of low-cost sensor architectures for diverse modalities has\nmade it possible to deploy sensor arrays that capture a single event from a\nlarge number of vantage points and using multiple modalities. In many\nscenarios, these sensors acquire very high-dimensional data such as audio\nsignals, images, and video. To cope with such high-dimensional data, we\ntypically rely on low-dimensional models. Manifold models provide a\nparticularly powerful model that captures the structure of high-dimensional\ndata when it is governed by a low-dimensional set of parameters. However, these\nmodels do not typically take into account dependencies among multiple sensors.\nWe thus propose a new joint manifold framework for data ensembles that exploits\nsuch dependencies. We show that simple algorithms can exploit the joint\nmanifold structure to improve their performance on standard signal processing\napplications. Additionally, recent results concerning dimensionality reduction\nfor manifolds enable us to formulate a network-scalable data compression scheme\nthat uses random projections of the sensed data. This scheme efficiently fuses\nthe data from all sensors through the addition of such projections, regardless\nof the data modalities and dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2009 06:47:47 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2009 07:18:30 GMT"}], "update_date": "2009-12-09", "authors_parsed": [["Davenport", "Mark A.", ""], ["Hegde", "Chinmay", ""], ["Duarte", "Marco F.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "0901.1904", "submitter": "Maxim Raginsky", "authors": "Maxim Raginsky", "title": "Joint universal lossy coding and identification of stationary mixing\n  sources with general alphabets", "comments": "16 pages, 1 figure; accepted to IEEE Transactions on Information\n  Theory", "journal-ref": null, "doi": "10.1109/TIT.2009.2015987", "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of joint universal variable-rate lossy coding and\nidentification for parametric classes of stationary $\\beta$-mixing sources with\ngeneral (Polish) alphabets. Compression performance is measured in terms of\nLagrangians, while identification performance is measured by the variational\ndistance between the true source and the estimated source. Provided that the\nsources are mixing at a sufficiently fast rate and satisfy certain smoothness\nand Vapnik-Chervonenkis learnability conditions, it is shown that, for bounded\nmetric distortions, there exist universal schemes for joint lossy compression\nand identification whose Lagrangian redundancies converge to zero as $\\sqrt{V_n\n\\log n /n}$ as the block length $n$ tends to infinity, where $V_n$ is the\nVapnik-Chervonenkis dimension of a certain class of decision regions defined by\nthe $n$-dimensional marginal distributions of the sources; furthermore, for\neach $n$, the decoder can identify $n$-dimensional marginal of the active\nsource up to a ball of radius $O(\\sqrt{V_n\\log n/n})$ in variational distance,\neventually with probability one. The results are supplemented by several\nexamples of parametric sources satisfying the regularity conditions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2009 22:55:52 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Raginsky", "Maxim", ""]]}, {"id": "0901.1905", "submitter": "Maxim Raginsky", "authors": "Maxim Raginsky", "title": "Achievability results for statistical learning under communication\n  constraints", "comments": "5 pages; to appear in Proc. ISIT 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of statistical learning is to construct an accurate predictor of\na random variable as a function of a correlated random variable on the basis of\nan i.i.d. training sample from their joint distribution. Allowable predictors\nare constrained to lie in some specified class, and the goal is to approach\nasymptotically the performance of the best predictor in the class. We consider\ntwo settings in which the learning agent only has access to rate-limited\ndescriptions of the training data, and present information-theoretic bounds on\nthe predictor performance achievable in the presence of these communication\nconstraints. Our proofs do not assume any separation structure between\ncompression and learning and rely on a new class of operational criteria\nspecifically tailored to joint design of encoders and learning algorithms in\nrate-constrained settings.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2009 23:03:26 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2009 15:31:14 GMT"}], "update_date": "2009-04-30", "authors_parsed": [["Raginsky", "Maxim", ""]]}, {"id": "0901.2376", "submitter": "Sumio Watanabe", "authors": "Sumio Watanabe", "title": "A Limit Theorem in Singular Regression Problem", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In statistical problems, a set of parameterized probability distributions is\nused to estimate the true probability distribution. If Fisher information\nmatrix at the true distribution is singular, then it has been left unknown what\nwe can estimate about the true distribution from random samples. In this paper,\nwe study a singular regression problem and prove a limit theorem which shows\nthe relation between the singular regression problem and two birational\ninvariants, a real log canonical threshold and a singular fluctuation. The\nobtained theorem has an important application to statistics, because it enables\nus to estimate the generalization error from the training error without any\nknowledge of the true probability distribution.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2009 01:00:39 GMT"}], "update_date": "2009-01-19", "authors_parsed": [["Watanabe", "Sumio", ""]]}, {"id": "0901.3150", "submitter": "Sewoong Oh", "authors": "Raghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh", "title": "Matrix Completion from a Few Entries", "comments": "30 pages, 1 figure, journal version (v1, v2: Conference version ISIT\n  2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let M be a random (alpha n) x n matrix of rank r<<n, and assume that a\nuniformly random subset E of its entries is observed. We describe an efficient\nalgorithm that reconstructs M from |E| = O(rn) observed entries with relative\nroot mean square error RMSE <= C(rn/|E|)^0.5 . Further, if r=O(1), M can be\nreconstructed exactly from |E| = O(n log(n)) entries. These results apply\nbeyond random matrices to general low-rank incoherent matrices.\n  This settles (in the case of bounded rank) a question left open by Candes and\nRecht and improves over the guarantees for their reconstruction algorithm. The\ncomplexity of our algorithm is O(|E|r log(n)), which opens the way to its use\nfor massive data sets. In the process of proving these statements, we obtain a\ngeneralization of a celebrated result by Friedman-Kahn-Szemeredi and Feige-Ofek\non the spectrum of sparse random matrices.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2009 21:32:57 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2009 07:00:15 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2009 03:27:35 GMT"}, {"version": "v4", "created": "Thu, 17 Sep 2009 09:26:46 GMT"}], "update_date": "2009-09-17", "authors_parsed": [["Keshavan", "Raghunandan H.", ""], ["Montanari", "Andrea", ""], ["Oh", "Sewoong", ""]]}, {"id": "0901.3202", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Rocquencourt)", "title": "Model-Consistent Sparse Estimation through the Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the least-square linear regression problem with regularization by\nthe $\\ell^1$-norm, a problem usually referred to as the Lasso. In this paper,\nwe first present a detailed asymptotic analysis of model consistency of the\nLasso in low-dimensional settings. For various decays of the regularization\nparameter, we compute asymptotic equivalents of the probability of correct\nmodel selection. For a specific rate decay, we show that the Lasso selects all\nthe variables that should enter the model with probability tending to one\nexponentially fast, while it selects all other variables with strictly positive\nprobability. We show that this property implies that if we run the Lasso for\nseveral bootstrapped replications of a given sample, then intersecting the\nsupports of the Lasso bootstrap estimates leads to consistent model selection.\nThis novel variable selection procedure, referred to as the Bolasso, is\nextended to high-dimensional settings by a provably consistent two-step\nprocedure.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2009 08:05:19 GMT"}], "update_date": "2009-01-22", "authors_parsed": [["Bach", "Francis", "", "INRIA Rocquencourt"]]}, {"id": "0901.3590", "submitter": "Chunhua Shen", "authors": "Chunhua Shen and Hanxi Li", "title": "On the Dual Formulation of Boosting Algorithms", "comments": "16 pages. To publish/Published in IEEE Transactions on Pattern\n  Analysis and Machine Intelligence, 2010", "journal-ref": null, "doi": "10.1109/TPAMI.2010.47", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study boosting algorithms from a new perspective. We show that the\nLagrange dual problems of AdaBoost, LogitBoost and soft-margin LPBoost with\ngeneralized hinge loss are all entropy maximization problems. By looking at the\ndual problems of these boosting algorithms, we show that the success of\nboosting algorithms can be understood in terms of maintaining a better margin\ndistribution by maximizing margins and at the same time controlling the margin\nvariance.We also theoretically prove that, approximately, AdaBoost maximizes\nthe average margin, instead of the minimum margin. The duality formulation also\nenables us to develop column generation based optimization algorithms, which\nare totally corrective. We show that they exhibit almost identical\nclassification results to that of standard stage-wise additive boosting\nalgorithms but with much faster convergence rates. Therefore fewer weak\nclassifiers are needed to build the ensemble using our proposed optimization\ntechnique.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2009 02:14:42 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2009 04:02:54 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2009 04:54:15 GMT"}, {"version": "v4", "created": "Mon, 28 Dec 2009 02:31:35 GMT"}], "update_date": "2014-02-27", "authors_parsed": [["Shen", "Chunhua", ""], ["Li", "Hanxi", ""]]}, {"id": "0901.4012", "submitter": "Jos\\'e Fontanari", "authors": "Jos\\'e F. Fontanari and Angelo Cangelosi", "title": "Cross-situational and supervised learning in the emergence of\n  communication", "comments": null, "journal-ref": "Interaction Studies: Social Behaviour and Communication in\n  Biological and Artificial Systems, 12, 119-133 (2011)", "doi": "10.1075/is.12.1.05fon", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scenarios for the emergence or bootstrap of a lexicon involve the repeated\ninteraction between at least two agents who must reach a consensus on how to\nname N objects using H words. Here we consider minimal models of two types of\nlearning algorithms: cross-situational learning, in which the individuals\ndetermine the meaning of a word by looking for something in common across all\nobserved uses of that word, and supervised operant conditioning learning, in\nwhich there is strong feedback between individuals about the intended meaning\nof the words. Despite the stark differences between these learning schemes, we\nshow that they yield the same communication accuracy in the realistic limits of\nlarge N and H, which coincides with the result of the classical occupancy\nproblem of randomly assigning N objects to H words.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2009 15:12:13 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2009 22:54:17 GMT"}, {"version": "v3", "created": "Sat, 28 Nov 2009 20:11:11 GMT"}], "update_date": "2011-04-14", "authors_parsed": [["Fontanari", "Jos\u00e9 F.", ""], ["Cangelosi", "Angelo", ""]]}, {"id": "0901.4137", "submitter": "Marcus Hutter", "authors": "Marcus Hutter", "title": "Practical Robust Estimators for the Imprecise Dirichlet Model", "comments": "22 pages, 2 figures", "journal-ref": "International Journal of Approximate Reasoning, 50:2 (2009) pages\n  231-242", "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Walley's Imprecise Dirichlet Model (IDM) for categorical i.i.d. data extends\nthe classical Dirichlet model to a set of priors. It overcomes several\nfundamental problems which other approaches to uncertainty suffer from. Yet, to\nbe useful in practice, one needs efficient ways for computing the\nimprecise=robust sets or intervals. The main objective of this work is to\nderive exact, conservative, and approximate, robust and credible interval\nestimates under the IDM for a large class of statistical estimators, including\nthe entropy and mutual information.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2009 23:05:06 GMT"}], "update_date": "2009-12-30", "authors_parsed": [["Hutter", "Marcus", ""]]}, {"id": "0901.4192", "submitter": "Danny Bickson", "authors": "Jason K. Johnson, Danny Bickson and Danny Dolev", "title": "Fixing Convergence of Gaussian Belief Propagation", "comments": "In the IEEE International Symposium on Information Theory (ISIT)\n  2009, Seoul, South Korea, July 2009", "journal-ref": null, "doi": "10.1109/ISIT.2009.5205777", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian belief propagation (GaBP) is an iterative message-passing algorithm\nfor inference in Gaussian graphical models. It is known that when GaBP\nconverges it converges to the correct MAP estimate of the Gaussian random\nvector and simple sufficient conditions for its convergence have been\nestablished. In this paper we develop a double-loop algorithm for forcing\nconvergence of GaBP. Our method computes the correct MAP estimate even in cases\nwhere standard GaBP would not have converged. We further extend this\nconstruction to compute least-squares solutions of over-constrained linear\nsystems. We believe that our construction has numerous applications, since the\nGaBP algorithm is linked to solution of linear systems of equations, which is a\nfundamental problem in computer science and engineering. As a case study, we\ndiscuss the linear detection problem. We show that using our new construction,\nwe are able to force convergence of Montanari's linear detection algorithm, in\ncases where it would originally fail. As a consequence, we are able to increase\nsignificantly the number of users that can transmit concurrently.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2009 08:24:57 GMT"}, {"version": "v2", "created": "Sat, 9 May 2009 07:23:49 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2009 03:25:13 GMT"}], "update_date": "2010-03-23", "authors_parsed": [["Johnson", "Jason K.", ""], ["Bickson", "Danny", ""], ["Dolev", "Danny", ""]]}, {"id": "0901.4876", "submitter": "Robert Brijder", "authors": "Hendrik Blockeel, Robert Brijder", "title": "Non-Confluent NLC Graph Grammar Inference by Compressing Disjoint\n  Subgraphs", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grammar inference deals with determining (preferable simple) models/grammars\nconsistent with a set of observations. There is a large body of research on\ngrammar inference within the theory of formal languages. However, there is\nsurprisingly little known on grammar inference for graph grammars. In this\npaper we take a further step in this direction and work within the framework of\nnode label controlled (NLC) graph grammars. Specifically, we characterize,\ngiven a set of disjoint and isomorphic subgraphs of a graph $G$, whether or not\nthere is a NLC graph grammar rule which can generate these subgraphs to obtain\n$G$. This generalizes previous results by assuming that the set of isomorphic\nsubgraphs is disjoint instead of non-touching. This leads naturally to consider\nthe more involved ``non-confluent'' graph grammar rules.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2009 12:44:29 GMT"}], "update_date": "2009-02-02", "authors_parsed": [["Blockeel", "Hendrik", ""], ["Brijder", "Robert", ""]]}]