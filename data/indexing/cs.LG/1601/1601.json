[{"id": "1601.00024", "submitter": "Ashish Sabharwal", "authors": "Ashish Sabharwal, Horst Samulowitz, Gerald Tesauro", "title": "Selecting Near-Optimal Learners via Incremental Data Allocation", "comments": "AAAI-2016: The Thirtieth AAAI Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a novel machine learning (ML) problem setting of sequentially\nallocating small subsets of training data amongst a large set of classifiers.\nThe goal is to select a classifier that will give near-optimal accuracy when\ntrained on all data, while also minimizing the cost of misallocated samples.\nThis is motivated by large modern datasets and ML toolkits with many\ncombinations of learning algorithms and hyper-parameters. Inspired by the\nprinciple of \"optimism under uncertainty,\" we propose an innovative strategy,\nData Allocation using Upper Bounds (DAUB), which robustly achieves these\nobjectives across a variety of real-world datasets.\n  We further develop substantial theoretical support for DAUB in an idealized\nsetting where the expected accuracy of a classifier trained on $n$ samples can\nbe known exactly. Under these conditions we establish a rigorous sub-linear\nbound on the regret of the approach (in terms of misallocated data), as well as\na rigorous bound on suboptimality of the selected classifier. Our accuracy\nestimates using real-world datasets only entail mild violations of the\ntheoretical scenario, suggesting that the practical behavior of DAUB is likely\nto approach the idealized behavior.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 22:19:09 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Sabharwal", "Ashish", ""], ["Samulowitz", "Horst", ""], ["Tesauro", "Gerald", ""]]}, {"id": "1601.00025", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny, Ahmed Elgammal, Babak Saleh", "title": "Write a Classifier: Predicting Visual Classifiers from Unstructured Text", "comments": "(TPAMI) Transactions on Pattern Analysis and Machine Intelligence\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People typically learn through exposure to visual concepts associated with\nlinguistic descriptions. For instance, teaching visual object categories to\nchildren is often accompanied by descriptions in text or speech. In a machine\nlearning context, these observations motivates us to ask whether this learning\nprocess could be computationally modeled to learn visual classifiers. More\nspecifically, the main question of this work is how to utilize purely textual\ndescription of visual classes with no training images, to learn explicit visual\nclassifiers for them. We propose and investigate two baseline formulations,\nbased on regression and domain transfer, that predict a linear classifier.\nThen, we propose a new constrained optimization formulation that combines a\nregression function and a knowledge transfer function with additional\nconstraints to predict the parameters of a linear classifier. We also propose a\ngeneric kernelized models where a kernel classifier is predicted in the form\ndefined by the representer theorem. The kernelized models allow defining and\nutilizing any two RKHS (Reproducing Kernel Hilbert Space) kernel functions in\nthe visual space and text space, respectively. We finally propose a kernel\nfunction between unstructured text descriptions that builds on distributional\nsemantics, which shows an advantage in our setting and could be useful for\nother applications. We applied all the studied models to predict visual\nclassifiers on two fine-grained and challenging categorization datasets (CU\nBirds and Flower Datasets), and the results indicate successful predictions of\nour final model over several baselines that we designed.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 22:23:34 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 02:13:59 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Elgammal", "Ahmed", ""], ["Saleh", "Babak", ""]]}, {"id": "1601.00034", "submitter": "Siamak Ravanbakhsh", "authors": "Siamak Ravanbakhsh, Barnabas Poczos, Jeff Schneider, Dale Schuurmans,\n  Russell Greiner", "title": "Stochastic Neural Networks with Monotonic Activation Functions", "comments": "AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Laplace approximation that creates a stochastic unit from any\nsmooth monotonic activation function, using only Gaussian noise. This paper\ninvestigates the application of this stochastic approximation in training a\nfamily of Restricted Boltzmann Machines (RBM) that are closely linked to\nBregman divergences. This family, that we call exponential family RBM\n(Exp-RBM), is a subset of the exponential family Harmoniums that expresses\nfamily members through a choice of smooth monotonic non-linearity for each\nneuron. Using contrastive divergence along with our Gaussian approximation, we\nshow that Exp-RBM can learn useful representations using novel stochastic\nunits.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2016 00:47:29 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 15:38:53 GMT"}, {"version": "v3", "created": "Thu, 21 Jul 2016 15:52:02 GMT"}, {"version": "v4", "created": "Fri, 22 Jul 2016 17:38:18 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Ravanbakhsh", "Siamak", ""], ["Poczos", "Barnabas", ""], ["Schneider", "Jeff", ""], ["Schuurmans", "Dale", ""], ["Greiner", "Russell", ""]]}, {"id": "1601.00062", "submitter": "Hao-Jun Shi", "authors": "Jerry Luo, Kayla Shapiro, Hao-Jun Michael Shi, Qi Yang, and Kan Zhu", "title": "Practical Algorithms for Learning Near-Isometric Linear Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two practical non-convex approaches for learning near-isometric,\nlinear embeddings of finite sets of data points. Given a set of training points\n$\\mathcal{X}$, we consider the secant set $S(\\mathcal{X})$ that consists of all\npairwise difference vectors of $\\mathcal{X}$, normalized to lie on the unit\nsphere. The problem can be formulated as finding a symmetric and positive\nsemi-definite matrix $\\boldsymbol{\\Psi}$ that preserves the norms of all the\nvectors in $S(\\mathcal{X})$ up to a distortion parameter $\\delta$. Motivated by\nnon-negative matrix factorization, we reformulate our problem into a Frobenius\nnorm minimization problem, which is solved by the Alternating Direction Method\nof Multipliers (ADMM) and develop an algorithm, FroMax. Another method solves\nfor a projection matrix $\\boldsymbol{\\Psi}$ by minimizing the restricted\nisometry property (RIP) directly over the set of symmetric, postive\nsemi-definite matrices. Applying ADMM and a Moreau decomposition on a proximal\nmapping, we develop another algorithm, NILE-Pro, for dimensionality reduction.\nFroMax is shown to converge faster for smaller $\\delta$ while NILE-Pro\nconverges faster for larger $\\delta$. Both non-convex approaches are then\nempirically demonstrated to be more computationally efficient than prior convex\napproaches for a number of applications in machine learning and signal\nprocessing.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2016 09:06:11 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 21:47:56 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Luo", "Jerry", ""], ["Shapiro", "Kayla", ""], ["Shi", "Hao-Jun Michael", ""], ["Yang", "Qi", ""], ["Zhu", "Kan", ""]]}, {"id": "1601.00236", "submitter": "Chetan Tonde", "authors": "Praneeth Vepakomma and Chetan Tonde and Ahmed Elgammal", "title": "Supervised Dimensionality Reduction via Distance Correlation\n  Maximization", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our work, we propose a novel formulation for supervised dimensionality\nreduction based on a nonlinear dependency criterion called Statistical Distance\nCorrelation, Szekely et. al. (2007). We propose an objective which is free of\ndistributional assumptions on regression variables and regression model\nassumptions. Our proposed formulation is based on learning a low-dimensional\nfeature representation $\\mathbf{z}$, which maximizes the squared sum of\nDistance Correlations between low dimensional features $\\mathbf{z}$ and\nresponse $y$, and also between features $\\mathbf{z}$ and covariates\n$\\mathbf{x}$. We propose a novel algorithm to optimize our proposed objective\nusing the Generalized Minimization Maximizaiton method of \\Parizi et. al.\n(2015). We show superior empirical results on multiple datasets proving the\neffectiveness of our proposed approach over several relevant state-of-the-art\nsupervised dimensionality reduction methods.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 00:14:23 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Vepakomma", "Praneeth", ""], ["Tonde", "Chetan", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1601.00238", "submitter": "Dacheng Tao", "authors": "Tongliang Liu, Dacheng Tao, and Dong Xu", "title": "Dimensionality-Dependent Generalization Bounds for $k$-Dimensional\n  Coding Schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-dimensional coding schemes refer to a collection of methods that\nattempt to represent data using a set of representative $k$-dimensional\nvectors, and include non-negative matrix factorization, dictionary learning,\nsparse coding, $k$-means clustering and vector quantization as special cases.\nPrevious generalization bounds for the reconstruction error of the\n$k$-dimensional coding schemes are mainly dimensionality independent. A major\nadvantage of these bounds is that they can be used to analyze the\ngeneralization error when data is mapped into an infinite- or high-dimensional\nfeature space. However, many applications use finite-dimensional data features.\nCan we obtain dimensionality-dependent generalization bounds for\n$k$-dimensional coding schemes that are tighter than dimensionality-independent\nbounds when data is in a finite-dimensional feature space? The answer is\npositive. In this paper, we address this problem and derive a\ndimensionality-dependent generalization bound for $k$-dimensional coding\nschemes by bounding the covering number of the loss function class induced by\nthe reconstruction error. The bound is of order\n$\\mathcal{O}\\left(\\left(mk\\ln(mkn)/n\\right)^{\\lambda_n}\\right)$, where $m$ is\nthe dimension of features, $k$ is the number of the columns in the linear\nimplementation of coding schemes, $n$ is the size of sample, $\\lambda_n>0.5$\nwhen $n$ is finite and $\\lambda_n=0.5$ when $n$ is infinite. We show that our\nbound can be tighter than previous results, because it avoids inducing the\nworst-case upper bound on $k$ of the loss function and converges faster. The\nproposed generalization bound is also applied to some specific coding schemes\nto demonstrate that the dimensionality-dependent bound is an indispensable\ncomplement to these dimensionality-independent generalization bounds.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 01:17:04 GMT"}, {"version": "v2", "created": "Sat, 23 Apr 2016 08:13:29 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""], ["Xu", "Dong", ""]]}, {"id": "1601.00318", "submitter": "Han Zhao", "authors": "Han Zhao, Pascal Poupart, Geoff Gordon", "title": "A Unified Approach for Learning the Parameters of Sum-Product Networks", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified approach for learning the parameters of Sum-Product\nnetworks (SPNs). We prove that any complete and decomposable SPN is equivalent\nto a mixture of trees where each tree corresponds to a product of univariate\ndistributions. Based on the mixture model perspective, we characterize the\nobjective function when learning SPNs based on the maximum likelihood\nestimation (MLE) principle and show that the optimization problem can be\nformulated as a signomial program. We construct two parameter learning\nalgorithms for SPNs by using sequential monomial approximations (SMA) and the\nconcave-convex procedure (CCCP), respectively. The two proposed methods\nnaturally admit multiplicative updates, hence effectively avoiding the\nprojection operation. With the help of the unified framework, we also show\nthat, in the case of SPNs, CCCP leads to the same algorithm as Expectation\nMaximization (EM) despite the fact that they are different in general.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 18:11:14 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 06:49:49 GMT"}, {"version": "v3", "created": "Sat, 30 Jan 2016 19:32:33 GMT"}, {"version": "v4", "created": "Fri, 26 Aug 2016 18:10:50 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Zhao", "Han", ""], ["Poupart", "Pascal", ""], ["Gordon", "Geoff", ""]]}, {"id": "1601.00350", "submitter": "Mehdi Korki", "authors": "Hadi Zayyani, Mehdi Korki, Farrokh Marvasti", "title": "Sparse Diffusion Steepest-Descent for One Bit Compressed Sensing in\n  Wireless Sensor Networks", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter proposes a sparse diffusion steepest-descent algorithm for one\nbit compressed sensing in wireless sensor networks. The approach exploits the\ndiffusion strategy from distributed learning in the one bit compressed sensing\nframework. To estimate a common sparse vector cooperatively from only the sign\nof measurements, steepest-descent is used to minimize the suitable global and\nlocal convex cost functions. A diffusion strategy is suggested for distributive\nlearning of the sparse vector. Simulation results show the effectiveness of the\nproposed distributed algorithm compared to the state-of-the-art non\ndistributive algorithms in the one bit compressed sensing framework.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 23:03:09 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Zayyani", "Hadi", ""], ["Korki", "Mehdi", ""], ["Marvasti", "Farrokh", ""]]}, {"id": "1601.00393", "submitter": "Hao Zhang", "authors": "Jincheng Mei, Hao Zhang, Bao-Liang Lu", "title": "On the Reducibility of Submodular Functions", "comments": "To appear in AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scalability of submodular optimization methods is critical for their\nusability in practice. In this paper, we study the reducibility of submodular\nfunctions, a property that enables us to reduce the solution space of\nsubmodular optimization problems without performance loss. We introduce the\nconcept of reducibility using marginal gains. Then we show that by adding\nperturbation, we can endow irreducible functions with reducibility, based on\nwhich we propose the perturbation-reduction optimization framework. Our\ntheoretical analysis proves that given the perturbation scales, the\nreducibility gain could be computed, and the performance loss has additive\nupper bounds. We further conduct empirical studies and the results demonstrate\nthat our proposed framework significantly accelerates existing optimization\nmethods for irreducible submodular functions with a cost of only small\nperformance losses.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 07:16:35 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Mei", "Jincheng", ""], ["Zhang", "Hao", ""], ["Lu", "Bao-Liang", ""]]}, {"id": "1601.00449", "submitter": "Massimiliano Pontil", "authors": "Andrew M. McDonald, Massimiliano Pontil, Dimitris Stamos", "title": "Fitting Spectral Decay with the $k$-Support Norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spectral $k$-support norm enjoys good estimation properties in low rank\nmatrix learning problems, empirically outperforming the trace norm. Its unit\nball is the convex hull of rank $k$ matrices with unit Frobenius norm. In this\npaper we generalize the norm to the spectral $(k,p)$-support norm, whose\nadditional parameter $p$ can be used to tailor the norm to the decay of the\nspectrum of the underlying model. We characterize the unit ball and we\nexplicitly compute the norm. We further provide a conditional gradient method\nto solve regularization problems with the norm, and we derive an efficient\nalgorithm to compute the Euclidean projection on the unit ball in the case\n$p=\\infty$. In numerical experiments, we show that allowing $p$ to vary\nsignificantly improves performance over the spectral $k$-support norm on\nvarious matrix completion benchmarks, and better captures the spectral decay of\nthe underlying model.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 10:48:29 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["McDonald", "Andrew M.", ""], ["Pontil", "Massimiliano", ""], ["Stamos", "Dimitris", ""]]}, {"id": "1601.00543", "submitter": "Xiangming Meng", "authors": "Xiangming Meng and Sheng Wu and Linling Kuang and Defeng (David) Huang\n  and Jianhua Lu", "title": "Approximate Message Passing with Nearest Neighbor Sparsity Pattern\n  Learning", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering clustered sparse signals with no prior\nknowledge of the sparsity pattern. Beyond simple sparsity, signals of interest\noften exhibits an underlying sparsity pattern which, if leveraged, can improve\nthe reconstruction performance. However, the sparsity pattern is usually\nunknown a priori. Inspired by the idea of k-nearest neighbor (k-NN) algorithm,\nwe propose an efficient algorithm termed approximate message passing with\nnearest neighbor sparsity pattern learning (AMP-NNSPL), which learns the\nsparsity pattern adaptively. AMP-NNSPL specifies a flexible spike and slab\nprior on the unknown signal and, after each AMP iteration, sets the sparse\nratios as the average of the nearest neighbor estimates via expectation\nmaximization (EM). Experimental results on both synthetic and real data\ndemonstrate the superiority of our proposed algorithm both in terms of\nreconstruction performance and computational complexity.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 15:43:49 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Meng", "Xiangming", "", "David"], ["Wu", "Sheng", "", "David"], ["Kuang", "Linling", "", "David"], ["Defeng", "", "", "David"], ["Huang", "", ""], ["Lu", "Jianhua", ""]]}, {"id": "1601.00574", "submitter": "Roman Lutz", "authors": "Brendan Teich, Roman Lutz, Valentin Kassarnig", "title": "NFL Play Prediction", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on NFL game data we try to predict the outcome of a play in multiple\ndifferent ways. An application of this is the following: by plugging in various\nplay options one could determine the best play for a given situation in real\ntime. While the outcome of a play can be described in many ways we had the most\npromising results with a newly defined measure that we call \"progress\". We see\nthis work as a first step to include predictive analysis into NFL playcalling.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 17:30:07 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Teich", "Brendan", ""], ["Lutz", "Roman", ""], ["Kassarnig", "Valentin", ""]]}, {"id": "1601.00595", "submitter": "George Papageorgiou", "authors": "George Papageorgiou, Pantelis Bouboulis and Sergios Theodoridis", "title": "Robust Non-linear Regression: A Greedy Approach Employing Kernels with\n  Application to Image Denoising", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2708029", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of robust non-linear regression in the presence of both\ninlier noise and outliers. Assuming that the unknown non-linear function\nbelongs to a Reproducing Kernel Hilbert Space (RKHS), our goal is to estimate\nthe set of the associated unknown parameters. Due to the presence of outliers,\ncommon techniques such as the Kernel Ridge Regression (KRR) or the Support\nVector Regression (SVR) turn out to be inadequate. Instead, we employ sparse\nmodeling arguments to explicitly model and estimate the outliers, adopting a\ngreedy approach. The proposed robust scheme, i.e., Kernel Greedy Algorithm for\nRobust Denoising (KGARD), is inspired by the classical Orthogonal Matching\nPursuit (OMP) algorithm. Specifically, the proposed method alternates between a\nKRR task and an OMP-like selection step. Theoretical results concerning the\nidentification of the outliers are provided. Moreover, KGARD is compared\nagainst other cutting edge methods, where its performance is evaluated via a\nset of experiments with various types of noise. Finally, the proposed robust\nestimation framework is applied to the task of image denoising, and its\nenhanced performance in the presence of outliers is demonstrated.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 18:11:45 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 06:29:42 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Papageorgiou", "George", ""], ["Bouboulis", "Pantelis", ""], ["Theodoridis", "Sergios", ""]]}, {"id": "1601.00626", "submitter": "Tim Weninger PhD", "authors": "Baoxu Shi and Tim Weninger", "title": "Scalable Models for Computing Hierarchies in Information Networks", "comments": "Preprint for \"Knowledge and Information Systems\" paper, in press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Information hierarchies are organizational structures that often used to\norganize and present large and complex information as well as provide a\nmechanism for effective human navigation. Fortunately, many statistical and\ncomputational models exist that automatically generate hierarchies; however,\nthe existing approaches do not consider linkages in information {\\em networks}\nthat are increasingly common in real-world scenarios. Current approaches also\ntend to present topics as an abstract probably distribution over words, etc\nrather than as tangible nodes from the original network. Furthermore, the\nstatistical techniques present in many previous works are not yet capable of\nprocessing data at Web-scale. In this paper we present the Hierarchical\nDocument Topic Model (HDTM), which uses a distributed vertex-programming\nprocess to calculate a nonparametric Bayesian generative model. Experiments on\nthree medium size data sets and the entire Wikipedia dataset show that HDTM can\ninfer accurate hierarchies even over large information networks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 20:05:19 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Shi", "Baoxu", ""], ["Weninger", "Tim", ""]]}, {"id": "1601.00670", "submitter": "Alp Kucukelbir", "authors": "David M. Blei, Alp Kucukelbir, Jon D. McAuliffe", "title": "Variational Inference: A Review for Statisticians", "comments": null, "journal-ref": "Journal of the American Statistical Association, Vol. 112 , Iss.\n  518, 2017", "doi": "10.1080/01621459.2017.1285773", "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the core problems of modern statistics is to approximate\ndifficult-to-compute probability densities. This problem is especially\nimportant in Bayesian statistics, which frames all inference about unknown\nquantities as a calculation involving the posterior density. In this paper, we\nreview variational inference (VI), a method from machine learning that\napproximates probability densities through optimization. VI has been used in\nmany applications and tends to be faster than classical methods, such as Markov\nchain Monte Carlo sampling. The idea behind VI is to first posit a family of\ndensities and then to find the member of that family which is close to the\ntarget. Closeness is measured by Kullback-Leibler divergence. We review the\nideas behind mean-field variational inference, discuss the special case of VI\napplied to exponential family models, present a full example with a Bayesian\nmixture of Gaussians, and derive a variant that uses stochastic optimization to\nscale up to massive data. We discuss modern research in VI and highlight\nimportant open problems. VI is powerful, but it is not yet well understood. Our\nhope in writing this paper is to catalyze statistical research on this class of\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 21:28:04 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 20:33:40 GMT"}, {"version": "v3", "created": "Mon, 8 Aug 2016 15:57:26 GMT"}, {"version": "v4", "created": "Wed, 2 Nov 2016 17:58:48 GMT"}, {"version": "v5", "created": "Wed, 14 Jun 2017 13:44:33 GMT"}, {"version": "v6", "created": "Wed, 15 Nov 2017 20:13:02 GMT"}, {"version": "v7", "created": "Mon, 4 Dec 2017 23:07:00 GMT"}, {"version": "v8", "created": "Mon, 26 Mar 2018 01:40:27 GMT"}, {"version": "v9", "created": "Wed, 9 May 2018 20:52:28 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Blei", "David M.", ""], ["Kucukelbir", "Alp", ""], ["McAuliffe", "Jon D.", ""]]}, {"id": "1601.00701", "submitter": "Carlos Stein Naves De Brito", "authors": "Carlos S. N. Brito, Wulfram Gerstner", "title": "Nonlinear Hebbian learning as a unifying principle in receptive field\n  formation", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1005070", "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of sensory receptive fields has been modeled in the past by a\nvariety of models including normative models such as sparse coding or\nindependent component analysis and bottom-up models such as spike-timing\ndependent plasticity or the Bienenstock-Cooper-Munro model of synaptic\nplasticity. Here we show that the above variety of approaches can all be\nunified into a single common principle, namely Nonlinear Hebbian Learning. When\nNonlinear Hebbian Learning is applied to natural images, receptive field shapes\nwere strongly constrained by the input statistics and preprocessing, but\nexhibited only modest variation across different choices of nonlinearities in\nneuron models or synaptic plasticity rules. Neither overcompleteness nor sparse\nnetwork activity are necessary for the development of localized receptive\nfields. The analysis of alternative sensory modalities such as auditory models\nor V2 development lead to the same conclusions. In all examples, receptive\nfields can be predicted a priori by reformulating an abstract model as\nnonlinear Hebbian learning. Thus nonlinear Hebbian learning and natural\nstatistics can account for many aspects of receptive field formation across\nmodels and sensory modalities.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 23:35:41 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Brito", "Carlos S. N.", ""], ["Gerstner", "Wulfram", ""]]}, {"id": "1601.00706", "submitter": "Jimei Yang", "authors": "Jimei Yang, Scott Reed, Ming-Hsuan Yang, Honglak Lee", "title": "Weakly-supervised Disentangling with Recurrent Transformations for 3D\n  View Synthesis", "comments": "This was published in NIPS 2015 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem for both graphics and vision is to synthesize novel\nviews of a 3D object from a single image. This is particularly challenging due\nto the partial observability inherent in projecting a 3D object onto the image\nspace, and the ill-posedness of inferring object shape and pose. However, we\ncan train a neural network to address the problem if we restrict our attention\nto specific object categories (in our case faces and chairs) for which we can\ngather ample training data. In this paper, we propose a novel recurrent\nconvolutional encoder-decoder network that is trained end-to-end on the task of\nrendering rotated objects starting from a single image. The recurrent structure\nallows our model to capture long-term dependencies along a sequence of\ntransformations. We demonstrate the quality of its predictions for human faces\non the Multi-PIE dataset and for a dataset of 3D chair models, and also show\nits ability to disentangle latent factors of variation (e.g., identity and\npose) without using full supervision.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 00:08:09 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Yang", "Jimei", ""], ["Reed", "Scott", ""], ["Yang", "Ming-Hsuan", ""], ["Lee", "Honglak", ""]]}, {"id": "1601.00732", "submitter": "Junbin Gao Professor", "authors": "Stephen Tierney, Junbin Gao, Yi Guo and Zhengwu Zhang", "title": "Low-Rank Representation over the Manifold of Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning it is common to interpret each data point as a vector in\nEuclidean space. However the data may actually be functional i.e.\\ each data\npoint is a function of some variable such as time and the function is\ndiscretely sampled. The naive treatment of functional data as traditional\nmultivariate data can lead to poor performance since the algorithms are\nignoring the correlation in the curvature of each function. In this paper we\npropose a method to analyse subspace structure of the functional data by using\nthe state of the art Low-Rank Representation (LRR). Experimental evaluation on\nsynthetic and real data reveals that this method massively outperforms\nconventional LRR in tasks concerning functional data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 04:21:45 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 09:50:20 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Tierney", "Stephen", ""], ["Gao", "Junbin", ""], ["Guo", "Yi", ""], ["Zhang", "Zhengwu", ""]]}, {"id": "1601.00740", "submitter": "Ashesh Jain", "authors": "Ashesh Jain, Hema S Koppula, Shane Soh, Bharad Raghavan, Avi Singh,\n  Ashutosh Saxena", "title": "Brain4Cars: Car That Knows Before You Do via Sensory-Fusion Deep\n  Learning Architecture", "comments": "Journal Version (ICCV and ICRA combination with more system details)\n  http://brain4cars.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced Driver Assistance Systems (ADAS) have made driving safer over the\nlast decade. They prepare vehicles for unsafe road conditions and alert drivers\nif they perform a dangerous maneuver. However, many accidents are unavoidable\nbecause by the time drivers are alerted, it is already too late. Anticipating\nmaneuvers beforehand can alert drivers before they perform the maneuver and\nalso give ADAS more time to avoid or prepare for the danger.\n  In this work we propose a vehicular sensor-rich platform and learning\nalgorithms for maneuver anticipation. For this purpose we equip a car with\ncameras, Global Positioning System (GPS), and a computing device to capture the\ndriving context from both inside and outside of the car. In order to anticipate\nmaneuvers, we propose a sensory-fusion deep learning architecture which jointly\nlearns to anticipate and fuse multiple sensory streams. Our architecture\nconsists of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory\n(LSTM) units to capture long temporal dependencies. We propose a novel training\nprocedure which allows the network to predict the future given only a partial\ntemporal context. We introduce a diverse data set with 1180 miles of natural\nfreeway and city driving, and show that we can anticipate maneuvers 3.5 seconds\nbefore they occur in real-time with a precision and recall of 90.5\\% and 87.4\\%\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 05:25:14 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Jain", "Ashesh", ""], ["Koppula", "Hema S", ""], ["Soh", "Shane", ""], ["Raghavan", "Bharad", ""], ["Singh", "Avi", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1601.00741", "submitter": "Ashesh Jain", "authors": "Ashesh Jain, Shikhar Sharma, Thorsten Joachims, Ashutosh Saxena", "title": "Learning Preferences for Manipulation Tasks from Online Coactive\n  Feedback", "comments": "IJRR accepted (Learning preferences over trajectories from coactive\n  feedback)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning preferences over trajectories for mobile\nmanipulators such as personal robots and assembly line robots. The preferences\nwe learn are more intricate than simple geometric constraints on trajectories;\nthey are rather governed by the surrounding context of various objects and\nhuman interactions in the environment. We propose a coactive online learning\nframework for teaching preferences in contextually rich environments. The key\nnovelty of our approach lies in the type of feedback expected from the user:\nthe human user does not need to demonstrate optimal trajectories as training\ndata, but merely needs to iteratively provide trajectories that slightly\nimprove over the trajectory currently proposed by the system. We argue that\nthis coactive preference feedback can be more easily elicited than\ndemonstrations of optimal trajectories. Nevertheless, theoretical regret bounds\nof our algorithm match the asymptotic rates of optimal trajectory algorithms.\n  We implement our algorithm on two high degree-of-freedom robots, PR2 and\nBaxter, and present three intuitive mechanisms for providing such incremental\nfeedback. In our experimental evaluation we consider two context rich settings\n-- household chores and grocery store checkout -- and show that users are able\nto train the robot with just a few feedbacks (taking only a few\nminutes).\\footnote{Parts of this work has been published at NIPS and ISRR\nconferences~\\citep{Jain13,Jain13b}. This journal submission presents a\nconsistent full paper, and also includes the proof of regret bounds, more\ndetails of the robotic system, and a thorough related work.}\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 05:47:09 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Jain", "Ashesh", ""], ["Sharma", "Shikhar", ""], ["Joachims", "Thorsten", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1601.00770", "submitter": "Makoto Miwa", "authors": "Makoto Miwa and Mohit Bansal", "title": "End-to-End Relation Extraction using LSTMs on Sequences and Tree\n  Structures", "comments": "Accepted for publication at the Association for Computational\n  Linguistics (ACL), 2016. 13 pages, 1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel end-to-end neural model to extract entities and relations\nbetween them. Our recurrent neural network based model captures both word\nsequence and dependency tree substructure information by stacking bidirectional\ntree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows\nour model to jointly represent both entities and relations with shared\nparameters in a single model. We further encourage detection of entities during\ntraining and use of entity information in relation extraction via entity\npretraining and scheduled sampling. Our model improves over the\nstate-of-the-art feature-based model on end-to-end relation extraction,\nachieving 12.1% and 5.7% relative error reductions in F1-score on ACE2005 and\nACE2004, respectively. We also show that our LSTM-RNN based model compares\nfavorably to the state-of-the-art CNN based model (in F1-score) on nominal\nrelation classification (SemEval-2010 Task 8). Finally, we present an extensive\nablation analysis of several model components.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 08:53:05 GMT"}, {"version": "v2", "created": "Sat, 19 Mar 2016 02:23:01 GMT"}, {"version": "v3", "created": "Wed, 8 Jun 2016 01:08:08 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Miwa", "Makoto", ""], ["Bansal", "Mohit", ""]]}, {"id": "1601.00816", "submitter": "Pierre-Yves Oudeyer", "authors": "Pierre-Yves Oudeyer (Flowers)", "title": "Open challenges in understanding development and evolution of speech\n  forms: The roles of embodied self-organization, motivation and active\n  exploration", "comments": null, "journal-ref": "Journal of Phonetics, Elsevier, 2015, 53, pp.5", "doi": "10.1016/j.wocn.2015.09.001", "report-no": null, "categories": "cs.AI cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article discusses open scientific challenges for understanding\ndevelopment and evolution of speech forms, as a commentary to Moulin-Frier et\nal. (Moulin-Frier et al., 2015). Based on the analysis of mathematical models\nof the origins of speech forms, with a focus on their assumptions , we study\nthe fundamental question of how speech can be formed out of non--speech, at\nboth developmental and evolutionary scales. In particular, we emphasize the\nimportance of embodied self-organization , as well as the role of mechanisms of\nmotivation and active curiosity-driven exploration in speech formation. Finally\n, we discuss an evolutionary-developmental perspective of the origins of\nspeech.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 12:50:14 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Oudeyer", "Pierre-Yves", "", "Flowers"]]}, {"id": "1601.00917", "submitter": "Jie Fu", "authors": "Jie Fu, Hongyin Luo, Jiashi Feng, Kian Hsiang Low, Tat-Seng Chua", "title": "DrMAD: Distilling Reverse-Mode Automatic Differentiation for Optimizing\n  Hyperparameters of Deep Neural Networks", "comments": "International Joint Conference on Artificial Intelligence, IJCAI,\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of deep neural networks is well-known to be sensitive to the\nsetting of their hyperparameters. Recent advances in reverse-mode automatic\ndifferentiation allow for optimizing hyperparameters with gradients. The\nstandard way of computing these gradients involves a forward and backward pass\nof computations. However, the backward pass usually needs to consume\nunaffordable memory to store all the intermediate variables to exactly reverse\nthe forward training procedure. In this work we propose a simple but effective\nmethod, DrMAD, to distill the knowledge of the forward pass into a shortcut\npath, through which we approximately reverse the training trajectory.\nExperiments on several image benchmark datasets show that DrMAD is at least 45\ntimes faster and consumes 100 times less memory compared to state-of-the-art\nmethods for optimizing hyperparameters with minimal compromise to its\neffectiveness. To the best of our knowledge, DrMAD is the first research\nattempt to make it practical to automatically tune thousands of hyperparameters\nof deep neural networks. The code can be downloaded from\nhttps://github.com/bigaidream-projects/drmad\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 17:43:15 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 05:57:51 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2016 11:43:31 GMT"}, {"version": "v4", "created": "Fri, 5 Feb 2016 05:45:35 GMT"}, {"version": "v5", "created": "Wed, 6 Apr 2016 15:55:19 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Fu", "Jie", ""], ["Luo", "Hongyin", ""], ["Feng", "Jiashi", ""], ["Low", "Kian Hsiang", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1601.00925", "submitter": "Tim Vor Der Br\\\"uck", "authors": "Tim vor der Br\\\"uck, Steffen Eger, Alexander Mehler", "title": "Complex Decomposition of the Negative Distance kernel", "comments": "Proceedings of the IEEE International Conference on Machine Learning\n  an Applications, Miami, Florida, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Support Vector Machine (SVM) has become a very popular machine learning\nmethod for text classification. One reason for this relates to the range of\nexisting kernels which allow for classifying data that is not linearly\nseparable. The linear, polynomial and RBF (Gaussian Radial Basis Function)\nkernel are commonly used and serve as a basis of comparison in our study. We\nshow how to derive the primal form of the quadratic Power Kernel (PK) -- also\ncalled the Negative Euclidean Distance Kernel (NDK) -- by means of complex\nnumbers. We exemplify the NDK in the framework of text categorization using the\nDewey Document Classification (DDC) as the target scheme. Our evaluation shows\nthat the power kernel produces F-scores that are comparable to the reference\nkernels, but is -- except for the linear kernel -- faster to compute. Finally,\nwe show how to extend the NDK-approach by including the Mahalanobis distance.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 18:16:07 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["der Br\u00fcck", "Tim vor", ""], ["Eger", "Steffen", ""], ["Mehler", "Alexander", ""]]}, {"id": "1601.00955", "submitter": "Feng Nan", "authors": "Feng Nan, Joseph Wang, Venkatesh Saligrama", "title": "Optimally Pruning Decision Tree Ensembles With Feature Cost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning decision rules for prediction with\nfeature budget constraint. In particular, we are interested in pruning an\nensemble of decision trees to reduce expected feature cost while maintaining\nhigh prediction accuracy for any test example. We propose a novel 0-1 integer\nprogram formulation for ensemble pruning. Our pruning formulation is general -\nit takes any ensemble of decision trees as input. By explicitly accounting for\nfeature-sharing across trees together with accuracy/cost trade-off, our method\nis able to significantly reduce feature cost by pruning subtrees that introduce\nmore loss in terms of feature cost than benefit in terms of prediction accuracy\ngain. Theoretically, we prove that a linear programming relaxation produces the\nexact solution of the original integer program. This allows us to use efficient\nconvex optimization tools to obtain an optimally pruned ensemble for any given\nbudget. Empirically, we see that our pruning algorithm significantly improves\nthe performance of the state of the art ensemble method BudgetRF.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 20:38:35 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Nan", "Feng", ""], ["Wang", "Joseph", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1601.01102", "submitter": "Rose Yu", "authors": "Rose Yu, Huida Qiu, Zhen Wen, Ching-Yung Lin, Yan Liu", "title": "A Survey on Social Media Anomaly Detection", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media anomaly detection is of critical importance to prevent malicious\nactivities such as bullying, terrorist attack planning, and fraud information\ndissemination. With the recent popularity of social media, new types of\nanomalous behaviors arise, causing concerns from various parties. While a large\namount of work have been dedicated to traditional anomaly detection problems,\nwe observe a surge of research interests in the new realm of social media\nanomaly detection. In this paper, we present a survey on existing approaches to\naddress this problem. We focus on the new type of anomalous phenomena in the\nsocial media and review the recent developed techniques to detect those special\ntypes of anomalies. We provide a general overview of the problem domain, common\nformulations, existing methodologies and potential directions. With this work,\nwe hope to call out the attention from the research community on this\nchallenging problem and open up new directions that we can contribute in the\nfuture.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 07:42:24 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 21:34:31 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Yu", "Rose", ""], ["Qiu", "Huida", ""], ["Wen", "Zhen", ""], ["Lin", "Ching-Yung", ""], ["Liu", "Yan", ""]]}, {"id": "1601.01121", "submitter": "Alexander Gepperth", "authors": "Thomas Kopinski, St\\'ephane Magand (ENSTA ParisTech U2IS/RV), Uwe\n  Handmann, Alexander Gepperth (Flowers, ENSTA ParisTech U2IS/RV)", "title": "A pragmatic approach to multi-class classification", "comments": "European Symposium on artificial neural networks (ESANN), Apr 2015,\n  Bruges, Belgium. 2015", "journal-ref": null, "doi": "10.1109/IJCNN.2015.7280768", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel hierarchical approach to multi-class classification which\nis generic in that it can be applied to different classification models (e.g.,\nsupport vector machines, perceptrons), and makes no explicit assumptions about\nthe probabilistic structure of the problem as it is usually done in multi-class\nclassification. By adding a cascade of additional classifiers, each of which\nreceives the previous classifier's output in addition to regular input data,\nthe approach harnesses unused information that manifests itself in the form of,\ne.g., correlations between predicted classes. Using multilayer perceptrons as a\nclassification model, we demonstrate the validity of this approach by testing\nit on a complex ten-class 3D gesture recognition task.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 09:55:17 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Kopinski", "Thomas", "", "ENSTA ParisTech U2IS/RV"], ["Magand", "St\u00e9phane", "", "ENSTA ParisTech U2IS/RV"], ["Handmann", "Uwe", "", "Flowers, ENSTA ParisTech U2IS/RV"], ["Gepperth", "Alexander", "", "Flowers, ENSTA ParisTech U2IS/RV"]]}, {"id": "1601.01142", "submitter": "Yang Gao", "authors": "Yang Gao, Jianfei Chen, Jun Zhu", "title": "Streaming Gibbs Sampling for LDA Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming variational Bayes (SVB) is successful in learning LDA models in an\nonline manner. However previous attempts toward developing online Monte-Carlo\nmethods for LDA have little success, often by having much worse perplexity than\ntheir batch counterparts. We present a streaming Gibbs sampling (SGS) method,\nan online extension of the collapsed Gibbs sampling (CGS). Our empirical study\nshows that SGS can reach similar perplexity as CGS, much better than SVB. Our\ndistributed version of SGS, DSGS, is much more scalable than SVB mainly because\nthe updates' communication complexity is small.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 11:15:45 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Gao", "Yang", ""], ["Chen", "Jianfei", ""], ["Zhu", "Jun", ""]]}, {"id": "1601.01157", "submitter": "Alexander Gepperth", "authors": "Thomas Kopinski, Alexander Gepperth (ENSTA ParisTech U2IS/RV,\n  Flowers), Uwe Handmann", "title": "A simple technique for improving multi-class classification with neural\n  networks", "comments": "European Symposium on artificial neural networks (ESANN), Jun 2015,\n  Bruges, Belgium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to perform multi-class pattern classification with\nneural networks and test it on a challenging 3D hand gesture recognition\nproblem. Our method consists of a standard one-against-all (OAA)\nclassification, followed by another network layer classifying the resulting\nclass scores, possibly augmented by the original raw input vector. This allows\nthe network to disambiguate hard-to-separate classes as the distribution of\nclass scores carries considerable information as well, and is in fact often\nused for assessing the confidence of a decision. We show that by this approach\nwe are able to significantly boost our results, overall as well as for\nparticular difficult cases, on the hard 10-class gesture classification task.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 12:33:35 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Kopinski", "Thomas", "", "ENSTA ParisTech U2IS/RV,\n  Flowers"], ["Gepperth", "Alexander", "", "ENSTA ParisTech U2IS/RV,\n  Flowers"], ["Handmann", "Uwe", ""]]}, {"id": "1601.01218", "submitter": "Dariush Kari", "authors": "Dariush Kari and Nuri Denizcan Vanli and Suleyman Serdar Kozat", "title": "Adaptive and Efficient Nonlinear Channel Equalization for Underwater\n  Acoustic Communication", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT cs.SD math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate underwater acoustic (UWA) channel equalization and introduce\nhierarchical and adaptive nonlinear channel equalization algorithms that are\nhighly efficient and provide significantly improved bit error rate (BER)\nperformance. Due to the high complexity of nonlinear equalizers and poor\nperformance of linear ones, to equalize highly difficult underwater acoustic\nchannels, we employ piecewise linear equalizers. However, in order to achieve\nthe performance of the best piecewise linear model, we use a tree structure to\nhierarchically partition the space of the received signal. Furthermore, the\nequalization algorithm should be completely adaptive, since due to the highly\nnon-stationary nature of the underwater medium, the optimal MSE equalizer as\nwell as the best piecewise linear equalizer changes in time. To this end, we\nintroduce an adaptive piecewise linear equalization algorithm that not only\nadapts the linear equalizer at each region but also learns the complete\nhierarchical structure with a computational complexity only polynomial in the\nnumber of nodes of the tree. Furthermore, our algorithm is constructed to\ndirectly minimize the final squared error without introducing any ad-hoc\nparameters. We demonstrate the performance of our algorithms through highly\nrealistic experiments performed on accurately simulated underwater acoustic\nchannels.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 15:51:54 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Kari", "Dariush", ""], ["Vanli", "Nuri Denizcan", ""], ["Kozat", "Suleyman Serdar", ""]]}, {"id": "1601.01297", "submitter": "Lars Roemheld", "authors": "Imanol Arrieta Ibarra, Bernardo Ramos, Lars Roemheld", "title": "Angrier Birds: Bayesian reinforcement learning", "comments": "Stanford University CS221 Final Project", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train a reinforcement learner to play a simplified version of the game\nAngry Birds. The learner is provided with a game state in a manner similar to\nthe output that could be produced by computer vision algorithms. We improve on\nthe efficiency of regular {\\epsilon}-greedy Q-Learning with linear function\napproximation through more systematic exploration in Randomized Least Squares\nValue Iteration (RLSVI), an algorithm that samples its policy from a posterior\ndistribution on optimal policies. With larger state-action spaces, efficient\nexploration becomes increasingly important, as evidenced by the faster learning\nin RLSVI.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 20:22:22 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 01:28:34 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Ibarra", "Imanol Arrieta", ""], ["Ramos", "Bernardo", ""], ["Roemheld", "Lars", ""]]}, {"id": "1601.01356", "submitter": "Makbule Gulcin Ozsoy", "authors": "Makbule Gulcin Ozsoy", "title": "From Word Embeddings to Item Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network platforms can use the data produced by their users to serve\nthem better. One of the services these platforms provide is recommendation\nservice. Recommendation systems can predict the future preferences of users\nusing their past preferences. In the recommendation systems literature there\nare various techniques, such as neighborhood based methods, machine-learning\nbased methods and matrix-factorization based methods. In this work, a set of\nwell known methods from natural language processing domain, namely Word2Vec, is\napplied to recommendation systems domain. Unlike previous works that use\nWord2Vec for recommendation, this work uses non-textual features, the\ncheck-ins, and it recommends venues to visit/check-in to the target users. For\nthe experiments, a Foursquare check-in dataset is used. The results show that\nuse of continuous vector space representations of items modeled by techniques\nof Word2Vec is promising for making recommendations.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 00:09:37 GMT"}, {"version": "v2", "created": "Sun, 6 Mar 2016 16:09:10 GMT"}, {"version": "v3", "created": "Wed, 15 Jun 2016 08:07:36 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Ozsoy", "Makbule Gulcin", ""]]}, {"id": "1601.01411", "submitter": "Chetan Tonde", "authors": "Chetan Tonde and Ahmed Elgammal", "title": "Learning Kernels for Structured Prediction using Polynomial Kernel\n  Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": "21 pages, 10 figures", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the kernel functions used in kernel methods has been a vastly\nexplored area in machine learning. It is now widely accepted that to obtain\n'good' performance, learning a kernel function is the key challenge. In this\nwork we focus on learning kernel representations for structured regression. We\npropose use of polynomials expansion of kernels, referred to as Schoenberg\ntransforms and Gegenbaur transforms, which arise from the seminal result of\nSchoenberg (1938). These kernels can be thought of as polynomial combination of\ninput features in a high dimensional reproducing kernel Hilbert space (RKHS).\nWe learn kernels over input and output for structured data, such that,\ndependency between kernel features is maximized. We use Hilbert-Schmidt\nIndependence Criterion (HSIC) to measure this. We also give an efficient,\nmatrix decomposition-based algorithm to learn these kernel transformations, and\ndemonstrate state-of-the-art results on several real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 06:37:48 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Tonde", "Chetan", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1601.01507", "submitter": "Antti Airola", "authors": "Antti Airola, Tapio Pahikkala", "title": "Fast Kronecker product kernel methods via generalized vec trick", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kronecker product kernel provides the standard approach in the kernel methods\nliterature for learning from graph data, where edges are labeled and both start\nand end vertices have their own feature representations. The methods allow\ngeneralization to such new edges, whose start and end vertices do not appear in\nthe training data, a setting known as zero-shot or zero-data learning. Such a\nsetting occurs in numerous applications, including drug-target interaction\nprediction, collaborative filtering and information retrieval. Efficient\ntraining algorithms based on the so-called vec trick, that makes use of the\nspecial structure of the Kronecker product, are known for the case where the\ntraining data is a complete bipartite graph. In this work we generalize these\nresults to non-complete training graphs. This allows us to derive a general\nframework for training Kronecker product kernel methods, as specific examples\nwe implement Kronecker ridge regression and support vector machine algorithms.\nExperimental results demonstrate that the proposed approach leads to accurate\nmodels, while allowing order of magnitude improvements in training and\nprediction time.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 12:25:53 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 10:18:12 GMT"}, {"version": "v3", "created": "Wed, 19 Apr 2017 07:31:24 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Airola", "Antti", ""], ["Pahikkala", "Tapio", ""]]}, {"id": "1601.01544", "submitter": "Alessio Benavoli", "authors": "Alessio Benavoli and Marco Zaffalon", "title": "State Space representation of non-stationary Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state space (SS) representation of Gaussian processes (GP) has recently\ngained a lot of interest. The main reason is that it allows to compute GPs\nbased inferences in O(n), where $n$ is the number of observations. This\nimplementation makes GPs suitable for Big Data. For this reason, it is\nimportant to provide a SS representation of the most important kernels used in\nmachine learning. The aim of this paper is to show how to exploit the transient\nbehaviour of SS models to map non-stationary kernels to SS models.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 14:25:07 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Benavoli", "Alessio", ""], ["Zaffalon", "Marco", ""]]}, {"id": "1601.01660", "submitter": "Daniel Neider", "authors": "Daniel Neider, Ufuk Topcu", "title": "An Automaton Learning Approach to Solving Safety Games over Infinite\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to construct finite-state reactive controllers for\nsystems whose interactions with their adversarial environment are modeled by\ninfinite-duration two-player games over (possibly) infinite graphs. The\nproposed method targets safety games with infinitely many states or with such a\nlarge number of states that it would be impractical---if not impossible---for\nconventional synthesis techniques that work on the entire state space. We\nresort to constructing finite-state controllers for such systems through an\nautomata learning approach, utilizing a symbolic representation of the\nunderlying game that is based on finite automata. Throughout the learning\nprocess, the learner maintains an approximation of the winning region\n(represented as a finite automaton) and refines it using different types of\ncounterexamples provided by the teacher until a satisfactory controller can be\nderived (if one exists). We present a symbolic representation of safety games\n(inspired by regular model checking), propose implementations of the learner\nand teacher, and evaluate their performance on examples motivated by robotic\nmotion planning in dynamic environments.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 20:42:19 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Neider", "Daniel", ""], ["Topcu", "Ufuk", ""]]}, {"id": "1601.01675", "submitter": "Denis Sidorov", "authors": "Alexei Zhukov, Victor Kurbatsky, Nikita Tomin, Denis Sidorov, Daniil\n  Panasetsky and Aoife Foley", "title": "Ensemble Methods of Classification for Power Systems Security Assessment", "comments": "6 pages, 4 figures, 4 tables. Submitted to PSSC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most promising approaches for complex technical systems analysis\nemploys ensemble methods of classification. Ensemble methods enable to build a\nreliable decision rules for feature space classification in the presence of\nmany possible states of the system. In this paper, novel techniques based on\ndecision trees are used for evaluation of the reliability of the regime of\nelectric power systems. We proposed hybrid approach based on random forests\nmodels and boosting models. Such techniques can be applied to predict the\ninteraction of increasing renewable power, storage devices and swiching of\nsmart loads from intelligent domestic appliances, heaters and air-conditioning\nunits and electric vehicles with grid for enhanced decision making. The\nensemble classification methods were tested on the modified 118-bus IEEE power\nsystem showing that proposed technique can be employed to examine whether the\npower system is secured under steady-state operating conditions.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 13:31:41 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Zhukov", "Alexei", ""], ["Kurbatsky", "Victor", ""], ["Tomin", "Nikita", ""], ["Sidorov", "Denis", ""], ["Panasetsky", "Daniil", ""], ["Foley", "Aoife", ""]]}, {"id": "1601.01799", "submitter": "Romain Tavenard", "authors": "Adeline Bailly (LETG - Costel, OBELIX), Simon Malinowski (LinkMedia),\n  Romain Tavenard (LETG - Costel, OBELIX), Thomas Guyet (DREAM), Laetitia\n  Chapel (OBELIX)", "title": "Dense Bag-of-Temporal-SIFT-Words for Time Series Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series classification is an application of particular interest with the\nincrease of data to monitor. Classical techniques for time series\nclassification rely on point-to-point distances. Recently, Bag-of-Words\napproaches have been used in this context. Words are quantized versions of\nsimple features extracted from sliding windows. The SIFT framework has proved\nefficient for image classification. In this paper, we design a time series\nclassification scheme that builds on the SIFT framework adapted to time series\nto feed a Bag-of-Words. We then refine our method by studying the impact of\nnormalized Bag-of-Words, as well as densely extract point descriptors. Proposed\nadjustements achieve better performance. The evaluation shows that our method\noutperforms classical techniques in terms of classification.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 09:06:44 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 08:12:54 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Bailly", "Adeline", "", "LETG - Costel, OBELIX"], ["Malinowski", "Simon", "", "LinkMedia"], ["Tavenard", "Romain", "", "LETG - Costel, OBELIX"], ["Guyet", "Thomas", "", "DREAM"], ["Chapel", "Laetitia", "", "OBELIX"]]}, {"id": "1601.01892", "submitter": "Kirell Benzi", "authors": "Kirell Benzi, Vassilis Kalofolias, Xavier Bresson, Pierre\n  Vandergheynst", "title": "Song Recommendation with Non-Negative Matrix Factorization and Graph\n  Total Variation", "comments": "Code available at: https://github.com/kikohs/recog", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work formulates a novel song recommender system as a matrix completion\nproblem that benefits from collaborative filtering through Non-negative Matrix\nFactorization (NMF) and content-based filtering via total variation (TV) on\ngraphs. The graphs encode both playlist proximity information and song\nsimilarity, using a rich combination of audio, meta-data and social features.\nAs we demonstrate, our hybrid recommendation system is very versatile and\nincorporates several well-known methods while outperforming them. Particularly,\nwe show on real-world data that our model overcomes w.r.t. two evaluation\nmetrics the recommendation of models solely based on low-rank information,\ngraph-based information or a combination of both.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 14:59:34 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 17:24:33 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Benzi", "Kirell", ""], ["Kalofolias", "Vassilis", ""], ["Bresson", "Xavier", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1601.01944", "submitter": "Martha White", "authors": "Shantanu Jain, Martha White, Michael W. Trosset, Predrag Radivojac", "title": "Nonparametric semi-supervised learning of class proportions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of developing binary classifiers from positive and unlabeled data\nis often encountered in machine learning. A common requirement in this setting\nis to approximate posterior probabilities of positive and negative classes for\na previously unseen data point. This problem can be decomposed into two steps:\n(i) the development of accurate predictors that discriminate between positive\nand unlabeled data, and (ii) the accurate estimation of the prior probabilities\nof positive and negative examples. In this work we primarily focus on the\nlatter subproblem. We study nonparametric class prior estimation and formulate\nthis problem as an estimation of mixing proportions in two-component mixture\nmodels, given a sample from one of the components and another sample from the\nmixture itself. We show that estimation of mixing proportions is generally\nill-defined and propose a canonical form to obtain identifiability while\nmaintaining the flexibility to model any distribution. We use insights from\nthis theory to elucidate the optimization surface of the class priors and\npropose an algorithm for estimating them. To address the problems of\nhigh-dimensional density estimation, we provide practical transformations to\nlow-dimensional spaces that preserve class priors. Finally, we demonstrate the\nefficacy of our method on univariate and multivariate data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 16:56:55 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Jain", "Shantanu", ""], ["White", "Martha", ""], ["Trosset", "Michael W.", ""], ["Radivojac", "Predrag", ""]]}, {"id": "1601.01974", "submitter": "David Pal", "authors": "Francesco Orabona and D\\'avid P\\'al", "title": "Scale-Free Online Learning", "comments": "arXiv admin note: text overlap with arXiv:1502.05744", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and analyze algorithms for online linear optimization that have\noptimal regret and at the same time do not need to know any upper or lower\nbounds on the norm of the loss vectors. Our algorithms are instances of the\nFollow the Regularized Leader (FTRL) and Mirror Descent (MD) meta-algorithms.\nWe achieve adaptiveness to the norms of the loss vectors by scale invariance,\ni.e., our algorithms make exactly the same decisions if the sequence of loss\nvectors is multiplied by any positive constant. The algorithm based on FTRL\nworks for any decision set, bounded or unbounded. For unbounded decisions sets,\nthis is the first adaptive algorithm for online linear optimization with a\nnon-vacuous regret bound. In contrast, we show lower bounds on scale-free\nalgorithms based on MD on unbounded domains.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 18:47:18 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 18:32:39 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Orabona", "Francesco", ""], ["P\u00e1l", "D\u00e1vid", ""]]}, {"id": "1601.02049", "submitter": "Rados{\\l}aw Adamczak", "authors": "Rados{\\l}aw Adamczak", "title": "A note on the sample complexity of the Er-SpUD algorithm by Spielman,\n  Wang and Wright for exact recovery of sparsely used dictionaries", "comments": "Minor typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering an invertible $n \\times n$ matrix $A$\nand a sparse $n \\times p$ random matrix $X$ based on the observation of $Y =\nAX$ (up to a scaling and permutation of columns of $A$ and rows of $X$). Using\nonly elementary tools from the theory of empirical processes we show that a\nversion of the Er-SpUD algorithm by Spielman, Wang and Wright with high\nprobability recovers $A$ and $X$ exactly, provided that $p \\ge Cn\\log n$, which\nis optimal up to the constant $C$.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 23:00:40 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 23:29:07 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Adamczak", "Rados\u0142aw", ""]]}, {"id": "1601.02068", "submitter": "Yining Wang", "authors": "Yining Wang and Adams Wei Yu and Aarti Singh", "title": "On Computationally Tractable Selection of Experiments in\n  Measurement-Constrained Regression Models", "comments": "41 pages. Accepted for publication in Journal of Machine Learning\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive computationally tractable methods to select a small subset of\nexperiment settings from a large pool of given design points. The primary focus\nis on linear regression models, while the technique extends to generalized\nlinear models and Delta's method (estimating functions of linear regression\nmodels) as well. The algorithms are based on a continuous relaxation of an\notherwise intractable combinatorial optimization problem, with sampling or\ngreedy procedures as post-processing steps. Formal approximation guarantees are\nestablished for both algorithms, and numerical results on both synthetic and\nreal-world data confirm the effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2016 03:05:31 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 21:05:35 GMT"}, {"version": "v3", "created": "Tue, 5 Jul 2016 21:29:33 GMT"}, {"version": "v4", "created": "Fri, 2 Dec 2016 20:07:28 GMT"}, {"version": "v5", "created": "Fri, 24 Mar 2017 00:57:10 GMT"}, {"version": "v6", "created": "Wed, 20 Dec 2017 06:52:49 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Wang", "Yining", ""], ["Yu", "Adams Wei", ""], ["Singh", "Aarti", ""]]}, {"id": "1601.02213", "submitter": "Michael Berthold", "authors": "Michael R. Berthold and Frank H\\\"oppner", "title": "On Clustering Time Series Using Euclidean Distance and Pearson\n  Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For time series comparisons, it has often been observed that z-score\nnormalized Euclidean distances far outperform the unnormalized variant. In this\npaper we show that a z-score normalized, squared Euclidean Distance is, in\nfact, equal to a distance based on Pearson Correlation. This has profound\nimpact on many distance-based classification or clustering methods. In addition\nto this theoretically sound result we also show that the often used k-Means\nalgorithm formally needs a mod ification to keep the interpretation as Pearson\ncorrelation strictly valid. Experimental results demonstrate that in many cases\nthe standard k-Means algorithm generally produces the same results.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2016 13:17:46 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Berthold", "Michael R.", ""], ["H\u00f6ppner", "Frank", ""]]}, {"id": "1601.02257", "submitter": "Robert Finn", "authors": "Robert Finn and Brian Kulis", "title": "A Sufficient Statistics Construction of Bayesian Nonparametric\n  Exponential Family Conjugate Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conjugate pairs of distributions over infinite dimensional spaces are\nprominent in statistical learning theory, particularly due to the widespread\nadoption of Bayesian nonparametric methodologies for a host of models and\napplications. Much of the existing literature in the learning community focuses\non processes possessing some form of computationally tractable conjugacy as is\nthe case for the beta and gamma processes (and, via normalization, the\nDirichlet process). For these processes, proofs of conjugacy and requisite\nderivation of explicit computational formulae for posterior density parameters\nare idiosyncratic to the stochastic process in question. As such, Bayesian\nNonparametric models are currently available for a limited number of conjugate\npairs, e.g. the Dirichlet-multinomial and beta-Bernoulli process pairs. In each\nof these above cases the likelihood process belongs to the class of discrete\nexponential family distributions. The exclusion of continuous likelihood\ndistributions from the known cases of Bayesian Nonparametric Conjugate models\nstands as a disparity in the researcher's toolbox.\n  In this paper we first address the problem of obtaining a general\nconstruction of prior distributions over infinite dimensional spaces possessing\ndistributional properties amenable to conjugacy. Second, we bridge the divide\nbetween the discrete and continuous likelihoods by illustrating a canonical\nconstruction for stochastic processes whose Levy measure densities are from\npositive exponential families, and then demonstrate that these processes in\nfact form the prior, likelihood, and posterior in a conjugate family. Our\ncanonical construction subsumes known computational formulae for posterior\ndensity parameters in the cases where the likelihood is from a discrete\ndistribution belonging to an exponential family.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2016 19:23:27 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Finn", "Robert", ""], ["Kulis", "Brian", ""]]}, {"id": "1601.02300", "submitter": "Marian-Andrei Rizoiu", "authors": "Young-Min Kim, Julien Velcin, St\\'ephane Bonnevay, Marian-Andrei\n  Rizoiu", "title": "Temporal Multinomial Mixture for Instance-Oriented Evolutionary\n  Clustering", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-16354-3_66", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary clustering aims at capturing the temporal evolution of clusters.\nThis issue is particularly important in the context of social media data that\nare naturally temporally driven. In this paper, we propose a new probabilistic\nmodel-based evolutionary clustering technique. The Temporal Multinomial Mixture\n(TMM) is an extension of classical mixture model that optimizes feature\nco-occurrences in the trade-off with temporal smoothness. Our model is\nevaluated for two recent case studies on opinion aggregation over time. We\ncompare four different probabilistic clustering models and we show the\nsuperiority of our proposal in the task of instance-oriented clustering.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 02:06:36 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Kim", "Young-Min", ""], ["Velcin", "Julien", ""], ["Bonnevay", "St\u00e9phane", ""], ["Rizoiu", "Marian-Andrei", ""]]}, {"id": "1601.02327", "submitter": "Guangneng Hu", "authors": "Guang-Neng Hu, Xin-Yu Dai, Yunya Song, Shu-Jian Huang, Jia-Jun Chen", "title": "A Synthetic Approach for Recommendation: Combining Ratings, Social\n  Relations, and Reviews", "comments": "7 pages, 8 figures", "journal-ref": "24th IJCAI,2015,1756-1762", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recommender systems (RSs) provide an effective way of alleviating the\ninformation overload problem by selecting personalized choices. Online social\nnetworks and user-generated content provide diverse sources for recommendation\nbeyond ratings, which present opportunities as well as challenges for\ntraditional RSs. Although social matrix factorization (Social MF) can integrate\nratings with social relations and topic matrix factorization can integrate\nratings with item reviews, both of them ignore some useful information. In this\npaper, we investigate the effective data fusion by combining the two\napproaches, in two steps. First, we extend Social MF to exploit the graph\nstructure of neighbors. Second, we propose a novel framework MR3 to jointly\nmodel these three types of information effectively for rating prediction by\naligning latent factors and hidden topics. We achieve more accurate rating\nprediction on two real-life datasets. Furthermore, we measure the contribution\nof each data source to the proposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 05:41:39 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Hu", "Guang-Neng", ""], ["Dai", "Xin-Yu", ""], ["Song", "Yunya", ""], ["Huang", "Shu-Jian", ""], ["Chen", "Jia-Jun", ""]]}, {"id": "1601.02376", "submitter": "Weinan Zhang", "authors": "Weinan Zhang, Tianming Du, Jun Wang", "title": "Deep Learning over Multi-field Categorical Data: A Case Study on User\n  Response Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting user responses, such as click-through rate and conversion rate,\nare critical in many web applications including web search, personalised\nrecommendation, and online advertising. Different from continuous raw features\nthat we usually found in the image and audio domains, the input features in web\nspace are always of multi-field and are mostly discrete and categorical while\ntheir dependencies are little known. Major user response prediction models have\nto either limit themselves to linear models or require manually building up\nhigh-order combination features. The former loses the ability of exploring\nfeature interactions, while the latter results in a heavy computation in the\nlarge feature space. To tackle the issue, we propose two novel models using\ndeep neural networks (DNNs) to automatically learn effective patterns from\ncategorical feature interactions and make predictions of users' ad clicks. To\nget our DNNs efficiently work, we propose to leverage three feature\ntransformation methods, i.e., factorisation machines (FMs), restricted\nBoltzmann machines (RBMs) and denoising auto-encoders (DAEs). This paper\npresents the structure of our models and their efficient training algorithms.\nThe large-scale experiments with real-world data demonstrate that our methods\nwork better than major state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 10:04:40 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Zhang", "Weinan", ""], ["Du", "Tianming", ""], ["Wang", "Jun", ""]]}, {"id": "1601.02377", "submitter": "Weinan Zhang", "authors": "Weinan Zhang, Lingxi Chen, Jun Wang", "title": "Implicit Look-alike Modelling in Display Ads: Transfer Collaborative\n  Filtering to CTR Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  User behaviour targeting is essential in online advertising. Compared with\nsponsored search keyword targeting and contextual advertising page content\ntargeting, user behaviour targeting builds users' interest profiles via\ntracking their online behaviour and then delivers the relevant ads according to\neach user's interest, which leads to higher targeting accuracy and thus more\nimproved advertising performance. The current user profiling methods include\nbuilding keywords and topic tags or mapping users onto a hierarchical taxonomy.\nHowever, to our knowledge, there is no previous work that explicitly\ninvestigates the user online visits similarity and incorporates such similarity\ninto their ad response prediction. In this work, we propose a general framework\nwhich learns the user profiles based on their online browsing behaviour, and\ntransfers the learned knowledge onto prediction of their ad response.\nTechnically, we propose a transfer learning model based on the probabilistic\nlatent factor graphic models, where the users' ad response profiles are\ngenerated from their online browsing profiles. The large-scale experiments\nbased on real-world data demonstrate significant improvement of our solution\nover some strong baselines.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 10:12:17 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Zhang", "Weinan", ""], ["Chen", "Lingxi", ""], ["Wang", "Jun", ""]]}, {"id": "1601.02513", "submitter": "Vassilis Kalofolias", "authors": "Vassilis Kalofolias", "title": "How to learn a graph from smooth signals", "comments": "8 pages + supplementary material. Accepted in AISTATS 2016, Cadiz,\n  Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework that learns the graph structure underlying a set of\nsmooth signals. Given $X\\in\\mathbb{R}^{m\\times n}$ whose rows reside on the\nvertices of an unknown graph, we learn the edge weights\n$w\\in\\mathbb{R}_+^{m(m-1)/2}$ under the smoothness assumption that\n$\\text{tr}{X^\\top LX}$ is small. We show that the problem is a weighted\n$\\ell$-1 minimization that leads to naturally sparse solutions. We point out\nhow known graph learning or construction techniques fall within our framework\nand propose a new model that performs better than the state of the art in many\nsettings. We present efficient, scalable primal-dual based algorithms for both\nour model and the previous state of the art, and evaluate their performance on\nartificial and real data.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 16:23:30 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Kalofolias", "Vassilis", ""]]}, {"id": "1601.02603", "submitter": "Marian-Andrei Rizoiu", "authors": "Marian-Andrei Rizoiu, Julien Velcin, St\\'ephane Lallich", "title": "How to Use Temporal-Driven Constrained Clustering to Detect Typical\n  Evolutions", "comments": null, "journal-ref": "Int. J. Artif. Intell. Tools 23, 1460013 (2014) [26 pages]", "doi": "10.1142/S0218213014600136", "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new time-aware dissimilarity measure that takes\ninto account the temporal dimension. Observations that are close in the\ndescription space, but distant in time are considered as dissimilar. We also\npropose a method to enforce the segmentation contiguity, by introducing, in the\nobjective function, a penalty term inspired from the Normal Distribution\nFunction. We combine the two propositions into a novel time-driven constrained\nclustering algorithm, called TDCK-Means, which creates a partition of coherent\nclusters, both in the multidimensional space and in the temporal space. This\nalgorithm uses soft semi-supervised constraints, to encourage adjacent\nobservations belonging to the same entity to be assigned to the same cluster.\nWe apply our algorithm to a Political Studies dataset in order to detect\ntypical evolution phases. We adapt the Shannon entropy in order to measure the\nentity contiguity, and we show that our proposition consistently improves\ntemporal cohesion of clusters, without any significant loss in the\nmultidimensional variance.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 01:20:26 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Rizoiu", "Marian-Andrei", ""], ["Velcin", "Julien", ""], ["Lallich", "St\u00e9phane", ""]]}, {"id": "1601.02680", "submitter": "Thiago Marzag\\~ao", "authors": "Thiago Marzag\\~ao", "title": "Using SVM to pre-classify government purchases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Brazilian government often misclassifies the goods it buys. That makes it\nhard to audit government expenditures. We cannot know whether the price paid\nfor a ballpoint pen (code #7510) was reasonable if the pen was misclassified as\na technical drawing pen (code #6675) or as any other good. This paper shows how\nwe can use machine learning to reduce misclassification. I trained a support\nvector machine (SVM) classifier that takes a product description as input and\nreturns the most likely category codes as output. I trained the classifier\nusing 20 million goods purchased by the Brazilian government between 1999-04-01\nand 2015-04-02. In 83.3% of the cases the correct category code was one of the\nthree most likely category codes identified by the classifier. I used the\ntrained classifier to develop a web app that might help the government reduce\nmisclassification. I open sourced the code on GitHub; anyone can use and modify\nit.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 02:34:43 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Marzag\u00e3o", "Thiago", ""]]}, {"id": "1601.02705", "submitter": "Jaeyong Sung", "authors": "Jaeyong Sung, Seok Hyun Jin, Ian Lenz, Ashutosh Saxena", "title": "Robobarista: Learning to Manipulate Novel Objects via Deep Multimodal\n  Embedding", "comments": "Journal Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a large variety of objects and appliances in human environments,\nsuch as stoves, coffee dispensers, juice extractors, and so on. It is\nchallenging for a roboticist to program a robot for each of these object types\nand for each of their instantiations. In this work, we present a novel approach\nto manipulation planning based on the idea that many household objects share\nsimilarly-operated object parts. We formulate the manipulation planning as a\nstructured prediction problem and learn to transfer manipulation strategy\nacross different objects by embedding point-cloud, natural language, and\nmanipulation trajectory data into a shared embedding space using a deep neural\nnetwork. In order to learn semantically meaningful spaces throughout our\nnetwork, we introduce a method for pre-training its lower layers for multimodal\nfeature embedding and a method for fine-tuning this embedding space using a\nloss-based margin. In order to collect a large number of manipulation\ndemonstrations for different objects, we develop a new crowd-sourcing platform\ncalled Robobarista. We test our model on our dataset consisting of 116 objects\nand appliances with 249 parts along with 250 language instructions, for which\nthere are 1225 crowd-sourced manipulation demonstrations. We further show that\nour robot with our model can even prepare a cup of a latte with appliances it\nhas never seen before.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 00:56:30 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Sung", "Jaeyong", ""], ["Jin", "Seok Hyun", ""], ["Lenz", "Ian", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1601.02733", "submitter": "Ehsan Hosseini-Asl", "authors": "Ehsan Hosseini-Asl, Jacek M. Zurada, Olfa Nasraoui", "title": "Deep Learning of Part-based Representation of Data Using Sparse\n  Autoencoders with Nonnegativity Constraints", "comments": "Accepted for publication in IEEE Transactions of Neural Networks and\n  Learning Systems", "journal-ref": null, "doi": "10.1109/TNNLS.2015.2479223", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a new deep learning autoencoder network, trained by a\nnonnegativity constraint algorithm (NCAE), that learns features which show\npart-based representation of data. The learning algorithm is based on\nconstraining negative weights. The performance of the algorithm is assessed\nbased on decomposing data into parts and its prediction performance is tested\non three standard image data sets and one text dataset. The results indicate\nthat the nonnegativity constraint forces the autoencoder to learn features that\namount to a part-based representation of data, while improving sparsity and\nreconstruction quality in comparison with the traditional sparse autoencoder\nand Nonnegative Matrix Factorization. It is also shown that this newly acquired\nrepresentation improves the prediction performance of a deep neural network.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 05:33:03 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Hosseini-Asl", "Ehsan", ""], ["Zurada", "Jacek M.", ""], ["Nasraoui", "Olfa", ""]]}, {"id": "1601.02828", "submitter": "Pawel Swietojanski", "authors": "Pawel Swietojanski and Jinyu Li and Steve Renals", "title": "Learning Hidden Unit Contributions for Unsupervised Acoustic Model\n  Adaptation", "comments": "14 pages, 9 Tables, 11 Figues in IEEE/ACM Transactions on Audio,\n  Speech and Language Processing, Vol. 24, Num. 8, 2016", "journal-ref": null, "doi": "10.1109/TASLP.2016.2560534", "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a broad study on the adaptation of neural network acoustic\nmodels by means of learning hidden unit contributions (LHUC) -- a method that\nlinearly re-combines hidden units in a speaker- or environment-dependent manner\nusing small amounts of unsupervised adaptation data. We also extend LHUC to a\nspeaker adaptive training (SAT) framework that leads to a more adaptable DNN\nacoustic model, working both in a speaker-dependent and a speaker-independent\nmanner, without the requirements to maintain auxiliary speaker-dependent\nfeature extractors or to introduce significant speaker-dependent changes to the\nDNN structure. Through a series of experiments on four different speech\nrecognition benchmarks (TED talks, Switchboard, AMI meetings, and Aurora4)\ncomprising 270 test speakers, we show that LHUC in both its test-only and SAT\nvariants results in consistent word error rate reductions ranging from 5% to\n23% relative depending on the task and the degree of mismatch between training\nand test data. In addition, we have investigated the effect of the amount of\nadaptation data per speaker, the quality of unsupervised adaptation targets,\nthe complementarity to other adaptation techniques, one-shot adaptation, and an\nextension to adapting DNNs trained in a sequence discriminative manner.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 12:33:56 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 17:47:07 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Swietojanski", "Pawel", ""], ["Li", "Jinyu", ""], ["Renals", "Steve", ""]]}, {"id": "1601.02947", "submitter": "Peter Radecki", "authors": "Peter Radecki and Brandon Hencey", "title": "Online Model Estimation for Predictive Thermal Control of Buildings", "comments": "14 pages, 15 figures, 2 tables, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes a general, scalable method to learn control-oriented\nthermal models of buildings that could enable wide-scale deployment of\ncost-effective predictive controls. An Unscented Kalman Filter augmented for\nparameter and disturbance estimation is shown to accurately learn and predict a\nbuilding's thermal response. Recent studies of heating, ventilating, and air\nconditioning (HVAC) systems have shown significant energy savings with advanced\nmodel predictive control (MPC). A scalable cost-effective method to readily\nacquire accurate, robust models of individual buildings' unique thermal\nenvelopes has historically been elusive and hindered the widespread deployment\nof prediction-based control systems. Continuous commissioning and lifetime\nperformance of these thermal models requires deployment of on-line data-driven\nsystem identification and parameter estimation routines. We propose a novel\ngray-box approach using an Unscented Kalman Filter based on a multi-zone\nthermal network and validate it with EnergyPlus simulation data. The filter\nquickly learns parameters of a thermal network during periods of known or\nconstrained loads and then characterizes unknown loads in order to provide\naccurate 24+ hour energy predictions. This study extends our initial\ninvestigation by formalizing parameter and disturbance estimation routines and\ndemonstrating results across a year-long study.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2015 01:48:33 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Radecki", "Peter", ""], ["Hencey", "Brandon", ""]]}, {"id": "1601.03073", "submitter": "Gautam Reddy", "authors": "Gautam Reddy, Antonio Celani and Massimo Vergassola", "title": "Infomax strategies for an optimal balance between exploration and\n  exploitation", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": "10.1007/s10955-016-1521-0", "report-no": null, "categories": "cs.LG cs.IT math.IT physics.data-an q-bio.PE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proper balance between exploitation and exploration is what makes good\ndecisions, which achieve high rewards like payoff or evolutionary fitness. The\nInfomax principle postulates that maximization of information directs the\nfunction of diverse systems, from living systems to artificial neural networks.\nWhile specific applications are successful, the validity of information as a\nproxy for reward remains unclear. Here, we consider the multi-armed bandit\ndecision problem, which features arms (slot-machines) of unknown probabilities\nof success and a player trying to maximize cumulative payoff by choosing the\nsequence of arms to play. We show that an Infomax strategy (Info-p) which\noptimally gathers information on the highest mean reward among the arms\nsaturates known optimal bounds and compares favorably to existing policies. The\nhighest mean reward considered by Info-p is not the quantity actually needed\nfor the choice of the arm to play, yet it allows for optimal tradeoffs between\nexploration and exploitation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 21:50:03 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Reddy", "Gautam", ""], ["Celani", "Antonio", ""], ["Vergassola", "Massimo", ""]]}, {"id": "1601.03124", "submitter": "Guangyong Chen", "authors": "Guangyong Chen, Fengyuan Zhu, Pheng Ann Heng", "title": "Online Prediction of Dyadic Data with Heterogeneous Matrix Factorization", "comments": "26 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dyadic Data Prediction (DDP) is an important problem in many research areas.\nThis paper develops a novel fully Bayesian nonparametric framework which\nintegrates two popular and complementary approaches, discrete mixed membership\nmodeling and continuous latent factor modeling into a unified Heterogeneous\nMatrix Factorization~(HeMF) model, which can predict the unobserved dyadics\naccurately. The HeMF can determine the number of communities automatically and\nexploit the latent linear structure for each bicluster efficiently. We propose\na Variational Bayesian method to estimate the parameters and missing data. We\nfurther develop a novel online learning approach for Variational inference and\nuse it for the online learning of HeMF, which can efficiently cope with the\nimportant large-scale DDP problem. We evaluate the performance of our method on\nthe EachMoive, MovieLens and Netflix Prize collaborative filtering datasets.\nThe experiment shows that, our model outperforms state-of-the-art methods on\nall benchmarks. Compared with Stochastic Gradient Method (SGD), our online\nlearning approach achieves significant improvement on the estimation accuracy\nand robustness.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 04:20:09 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Chen", "Guangyong", ""], ["Zhu", "Fengyuan", ""], ["Heng", "Pheng Ann", ""]]}, {"id": "1601.03466", "submitter": "Tao Zhang", "authors": "Tao Zhang, Quanyan Zhu", "title": "Dynamic Privacy For Distributed Machine Learning Over Network", "comments": "15 pages, 5 figures Corrected typos. Revised argument in section 3,\n  4, and Appendix, results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy-preserving distributed machine learning becomes increasingly\nimportant due to the recent rapid growth of data. This paper focuses on a class\nof regularized empirical risk minimization (ERM) machine learning problems, and\ndevelops two methods to provide differential privacy to distributed learning\nalgorithms over a network. We first decentralize the learning algorithm using\nthe alternating direction method of multipliers (ADMM), and propose the methods\nof dual variable perturbation and primal variable perturbation to provide\ndynamic differential privacy. The two mechanisms lead to algorithms that can\nprovide privacy guarantees under mild conditions of the convexity and\ndifferentiability of the loss function and the regularizer. We study the\nperformance of the algorithms, and show that the dual variable perturbation\noutperforms its primal counterpart. To design an optimal privacy mechanisms, we\nanalyze the fundamental tradeoff between privacy and accuracy, and provide\nguidelines to choose privacy parameters. Numerical experiments using customer\ninformation database are performed to corroborate the results on privacy and\nutility tradeoffs and design.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 02:20:46 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2016 05:20:29 GMT"}, {"version": "v3", "created": "Wed, 9 Mar 2016 21:51:12 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Zhang", "Tao", ""], ["Zhu", "Quanyan", ""]]}, {"id": "1601.03478", "submitter": "Afroze Ibrahim Baqapuri", "authors": "Afroze Ibrahim Baqapuri", "title": "Deep Learning Applied to Image and Text Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to describe images with natural language sentences is the\nhallmark for image and language understanding. Such a system has wide ranging\napplications such as annotating images and using natural sentences to search\nfor images.In this project we focus on the task of bidirectional image\nretrieval: such asystem is capable of retrieving an image based on a sentence\n(image search) andretrieve sentence based on an image query (image annotation).\nWe present asystem based on a global ranking objective function which uses a\ncombinationof convolutional neural networks (CNN) and multi layer perceptrons\n(MLP).It takes a pair of image and sentence and processes them in different\nchannels,finally embedding it into a common multimodal vector space. These\nembeddingsencode abstract semantic information about the two inputs and can be\ncomparedusing traditional information retrieval approaches. For each such pair,\nthe modelreturns a score which is interpretted as a similarity metric. If this\nscore is high,the image and sentence are likely to convey similar meaning, and\nif the score is low then they are likely not to.\n  The visual input is modeled via deep convolutional neural network. On\ntheother hand we explore three models for the textual module. The first one\nisbag of words with an MLP. The second one uses n-grams (bigram, trigrams,and a\ncombination of trigram & skip-grams) with an MLP. The third is morespecialized\ndeep network specific for modeling variable length sequences (SSE).We report\ncomparable performance to recent work in the field, even though ouroverall\nmodel is simpler. We also show that the training time choice of how wecan\ngenerate our negative samples has a significant impact on performance, and can\nbe used to specialize the bi-directional system in one particular task.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 17:19:33 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Baqapuri", "Afroze Ibrahim", ""]]}, {"id": "1601.03483", "submitter": "Renato Cordeiro De Amorim", "authors": "Renato Cordeiro de Amorim", "title": "A survey on feature weighting based K-Means algorithms", "comments": "Journal of Classification (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a real-world data set there is always the possibility, rather high in our\nopinion, that different features may have different degrees of relevance. Most\nmachine learning algorithms deal with this fact by either selecting or\ndeselecting features in the data preprocessing phase. However, we maintain that\neven among relevant features there may be different degrees of relevance, and\nthis should be taken into account during the clustering process. With over 50\nyears of history, K-Means is arguably the most popular partitional clustering\nalgorithm there is. The first K-Means based clustering algorithm to compute\nfeature weights was designed just over 30 years ago. Various such algorithms\nhave been designed since but there has not been, to our knowledge, a survey\nintegrating empirical evidence of cluster recovery ability, common flaws, and\npossible directions for future research. This paper elaborates on the concept\nof feature weighting and addresses these issues by critically analysing some of\nthe most popular, or innovative, feature weighting mechanisms based in K-Means.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 08:46:39 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["de Amorim", "Renato Cordeiro", ""]]}, {"id": "1601.03642", "submitter": "Martin Thoma", "authors": "Martin Thoma", "title": "Creativity in Machine Learning", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent machine learning techniques can be modified to produce creative\nresults. Those results did not exist before; it is not a trivial combination of\nthe data which was fed into the machine learning system. The obtained results\ncome in multiple forms: As images, as text and as audio.\n  This paper gives a high level overview of how they are created and gives some\nexamples. It is meant to be a summary of the current work and give people who\nare new to machine learning some starting points.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 23:28:07 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Thoma", "Martin", ""]]}, {"id": "1601.03651", "submitter": "Lili Mou", "authors": "Yan Xu, Ran Jia, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, Zhi Jin", "title": "Improved Relation Classification by Deep Recurrent Neural Networks with\n  Data Augmentation", "comments": "Accepted by COLING-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, neural networks play an important role in the task of relation\nclassification. By designing different neural architectures, researchers have\nimproved the performance to a large extent in comparison with traditional\nmethods. However, existing neural networks for relation classification are\nusually of shallow architectures (e.g., one-layer convolutional neural networks\nor recurrent networks). They may fail to explore the potential representation\nspace in different abstraction levels. In this paper, we propose deep recurrent\nneural networks (DRNNs) for relation classification to tackle this challenge.\nFurther, we propose a data augmentation method by leveraging the directionality\nof relations. We evaluated our DRNNs on the SemEval-2010 Task~8, and achieve an\nF1-score of 86.1%, outperforming previous state-of-the-art recorded results.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 16:30:41 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 07:11:46 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Xu", "Yan", ""], ["Jia", "Ran", ""], ["Mou", "Lili", ""], ["Li", "Ge", ""], ["Chen", "Yunchuan", ""], ["Lu", "Yangyang", ""], ["Jin", "Zhi", ""]]}, {"id": "1601.03754", "submitter": "Ryan Curtin", "authors": "Ryan R. Curtin", "title": "Dual-tree $k$-means with bounded iteration runtime", "comments": "supplementary material included; submitted to ICML '16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  k-means is a widely used clustering algorithm, but for $k$ clusters and a\ndataset size of $N$, each iteration of Lloyd's algorithm costs $O(kN)$ time.\nAlthough there are existing techniques to accelerate single Lloyd iterations,\nnone of these are tailored to the case of large $k$, which is increasingly\ncommon as dataset sizes grow. We propose a dual-tree algorithm that gives the\nexact same results as standard $k$-means; when using cover trees, we use\nadaptive analysis techniques to, under some assumptions, bound the\nsingle-iteration runtime of the algorithm as $O(N + k log k)$. To our knowledge\nthese are the first sub-$O(kN)$ bounds for exact Lloyd iterations. We then show\nthat this theoretically favorable algorithm performs competitively in practice,\nespecially for large $N$ and $k$ in low dimensions. Further, the algorithm is\ntree-independent, so any type of tree may be used.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 21:18:06 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Curtin", "Ryan R.", ""]]}, {"id": "1601.03764", "submitter": "Yingyu Liang", "authors": "Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, Andrej Risteski", "title": "Linear Algebraic Structure of Word Senses, with Applications to Polysemy", "comments": "Appear in the Transactions of the Association for Computational\n  Linguistics 2018, link:\n  https://transacl.org/ojs/index.php/tacl/article/view/1346", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are ubiquitous in NLP and information retrieval, but it is\nunclear what they represent when the word is polysemous. Here it is shown that\nmultiple word senses reside in linear superposition within the word embedding\nand simple sparse coding can recover vectors that approximately capture the\nsenses. The success of our approach, which applies to several embedding\nmethods, is mathematically explained using a variant of the random walk on\ndiscourses model (Arora et al., 2016). A novel aspect of our technique is that\neach extracted word sense is accompanied by one of about 2000 \"discourse atoms\"\nthat gives a succinct description of which other words co-occur with that word\nsense. Discourse atoms can be of independent interest, and make the method\npotentially more useful. Empirical tests are used to verify and support the\ntheory.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 22:02:18 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 15:22:43 GMT"}, {"version": "v3", "created": "Sun, 15 Jul 2018 08:08:39 GMT"}, {"version": "v4", "created": "Thu, 19 Jul 2018 00:06:35 GMT"}, {"version": "v5", "created": "Fri, 20 Jul 2018 15:26:24 GMT"}, {"version": "v6", "created": "Fri, 7 Dec 2018 17:30:03 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Arora", "Sanjeev", ""], ["Li", "Yuanzhi", ""], ["Liang", "Yingyu", ""], ["Ma", "Tengyu", ""], ["Risteski", "Andrej", ""]]}, {"id": "1601.03769", "submitter": "Kyle Johnston", "authors": "Kyle B Johnston and Hakeem M Oluseyi", "title": "Generation of a Supervised Classification Algorithm for Time-Series\n  Variable Stars with an Application to the LINEAR Dataset", "comments": null, "journal-ref": null, "doi": "10.1016/j.newast.2016.10.004", "report-no": null, "categories": "astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of digital astronomy, new benefits and new problems have been\npresented to the modern day astronomer. While data can be captured in a more\nefficient and accurate manor using digital means, the efficiency of data\nretrieval has led to an overload of scientific data for processing and storage.\nThis paper will focus on the construction and application of a supervised\npattern classification algorithm for the identification of variable stars.\nGiven the reduction of a survey of stars into a standard feature space, the\nproblem of using prior patterns to identify new observed patterns can be\nreduced to time tested classification methodologies and algorithms. Such\nsupervised methods, so called because the user trains the algorithms prior to\napplication using patterns with known classes or labels, provide a means to\nprobabilistically determine the estimated class type of new observations. This\npaper will demonstrate the construction and application of a supervised\nclassification algorithm on variable star data. The classifier is applied to a\nset of 192,744 LINEAR data points. Of the original samples, 34,451 unique stars\nwere classified with high confidence (high level of probability of being the\ntrue class).\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 22:15:32 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Johnston", "Kyle B", ""], ["Oluseyi", "Hakeem M", ""]]}, {"id": "1601.03778", "submitter": "Baichuan Zhang", "authors": "Baichuan Zhang, Sutanay Choudhury, Mohammad Al Hasan, Xia Ning,\n  Khushbu Agarwal, Sumit Purohit, Paola Pesntez Cabrera", "title": "Trust from the past: Bayesian Personalized Ranking based Link Prediction\n  in Knowledge Graphs", "comments": "SDM Workshop on Mining Networks and Graphs (MNG 2016), Miami, FL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction, or predicting the likelihood of a link in a knowledge graph\nbased on its existing state is a key research task. It differs from a\ntraditional link prediction task in that the links in a knowledge graph are\ncategorized into different predicates and the link prediction performance of\ndifferent predicates in a knowledge graph generally varies widely. In this\nwork, we propose a latent feature embedding based link prediction model which\nconsiders the prediction task for each predicate disjointly. To learn the model\nparameters it utilizes a Bayesian personalized ranking based optimization\ntechnique. Experimental results on large-scale knowledge bases such as YAGO2\nshow that our link prediction approach achieves substantially higher\nperformance than several state-of-art approaches. We also show that for a given\npredicate the topological properties of the knowledge graph induced by the\ngiven predicate edges are key indicators of the link prediction performance of\nthat predicate in the knowledge graph.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 23:13:00 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2016 05:12:32 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Zhang", "Baichuan", ""], ["Choudhury", "Sutanay", ""], ["Hasan", "Mohammad Al", ""], ["Ning", "Xia", ""], ["Agarwal", "Khushbu", ""], ["Purohit", "Sumit", ""], ["Cabrera", "Paola Pesntez", ""]]}, {"id": "1601.03797", "submitter": "Sanjay Krishnan", "authors": "Sanjay Krishnan, Jiannan Wang, Eugene Wu, Michael J. Franklin, Ken\n  Goldberg", "title": "ActiveClean: Interactive Data Cleaning While Learning Convex Loss Models", "comments": "Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data cleaning is often an important step to ensure that predictive models,\nsuch as regression and classification, are not affected by systematic errors\nsuch as inconsistent, out-of-date, or outlier data. Identifying dirty data is\noften a manual and iterative process, and can be challenging on large datasets.\nHowever, many data cleaning workflows can introduce subtle biases into the\ntraining processes due to violation of independence assumptions. We propose\nActiveClean, a progressive cleaning approach where the model is updated\nincrementally instead of re-training and can guarantee accuracy on partially\ncleaned data. ActiveClean supports a popular class of models called convex loss\nmodels (e.g., linear regression and SVMs). ActiveClean also leverages the\nstructure of a user's model to prioritize cleaning those records likely to\naffect the results. We evaluate ActiveClean on five real-world datasets UCI\nAdult, UCI EEG, MNIST, Dollars For Docs, and WorldBank with both real and\nsynthetic errors. Our results suggest that our proposed optimizations can\nimprove model accuracy by up-to 2.5x for the same amount of data cleaned.\nFurthermore for a fixed cleaning budget and on all real dirty datasets,\nActiveClean returns more accurate models than uniform sampling and Active\nLearning.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 02:02:00 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Krishnan", "Sanjay", ""], ["Wang", "Jiannan", ""], ["Wu", "Eugene", ""], ["Franklin", "Michael J.", ""], ["Goldberg", "Ken", ""]]}, {"id": "1601.03805", "submitter": "Yi Guo", "authors": "Junbin Gao and Yi Guo and Zhiyong Wang", "title": "Matrix Neural Networks", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional neural networks assume vectorial inputs as the network is\narranged as layers of single line of computing units called neurons. This\nspecial structure requires the non-vectorial inputs such as matrices to be\nconverted into vectors. This process can be problematic. Firstly, the spatial\ninformation among elements of the data may be lost during vectorisation.\nSecondly, the solution space becomes very large which demands very special\ntreatments to the network parameters and high computational cost. To address\nthese issues, we propose matrix neural networks (MatNet), which takes matrices\ndirectly as inputs. Each neuron senses summarised information through bilinear\nmapping from lower layer units in exactly the same way as the classic feed\nforward neural networks. Under this structure, back prorogation and gradient\ndescent combination can be utilised to obtain network parameters efficiently.\nFurthermore, it can be conveniently extended for multimodal inputs. We apply\nMatNet to MNIST handwritten digits classification and image super resolution\ntasks to show its effectiveness. Without too much tweaking MatNet achieves\ncomparable performance as the state-of-the-art methods in both tasks with\nconsiderably reduced complexity.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 03:33:35 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 01:47:07 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Gao", "Junbin", ""], ["Guo", "Yi", ""], ["Wang", "Zhiyong", ""]]}, {"id": "1601.03822", "submitter": "Hossein Keshavarz", "authors": "Hossein Keshavarz, Clayton Scott, XuanLong Nguyen", "title": "On the consistency of inversion-free parameter estimation for Gaussian\n  random fields", "comments": "41 pages, 2 Figures", "journal-ref": "Journal of Multivariate Analysis (2016), pp. 245-266", "doi": "10.1016/j.jmva.2016.06.003", "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian random fields are a powerful tool for modeling environmental\nprocesses. For high dimensional samples, classical approaches for estimating\nthe covariance parameters require highly challenging and massive computations,\nsuch as the evaluation of the Cholesky factorization or solving linear systems.\nRecently, Anitescu, Chen and Stein \\cite{M.Anitescu} proposed a fast and\nscalable algorithm which does not need such burdensome computations. The main\nfocus of this article is to study the asymptotic behavior of the algorithm of\nAnitescu et al. (ACS) for regular and irregular grids in the increasing domain\nsetting. Consistency, minimax optimality and asymptotic normality of this\nalgorithm are proved under mild differentiability conditions on the covariance\nfunction. Despite the fact that ACS's method entails a non-concave\nmaximization, our results hold for any stationary point of the objective\nfunction. A numerical study is presented to evaluate the efficiency of this\nalgorithm for large data sets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 05:47:29 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 04:26:08 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Keshavarz", "Hossein", ""], ["Scott", "Clayton", ""], ["Nguyen", "XuanLong", ""]]}, {"id": "1601.03855", "submitter": "Pratik Gajane", "authors": "Pratik Gajane, Tanguy Urvoy, Fabrice Cl\\'erot (FT R and D)", "title": "A Relative Exponential Weighing Algorithm for Adversarial Utility-based\n  Dueling Bandits", "comments": null, "journal-ref": "The 32nd International Conference on Machine Learning, Jul 2015,\n  Lille, France. 37, pp.218-227, Proceedings of The 32nd International\n  Conference on Machine Learning", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the K-armed dueling bandit problem which is a variation of the\nclassical Multi-Armed Bandit (MAB) problem in which the learner receives only\nrelative feedback about the selected pairs of arms. We propose a new algorithm\ncalled Relative Exponential-weight algorithm for Exploration and Exploitation\n(REX3) to handle the adversarial utility-based formulation of this problem.\nThis algorithm is a non-trivial extension of the Exponential-weight algorithm\nfor Exploration and Exploitation (EXP3) algorithm. We prove a finite time\nexpected regret upper bound of order O(sqrt(K ln(K)T)) for this algorithm and a\ngeneral lower bound of order omega(sqrt(KT)). At the end, we provide\nexperimental results using real data from information retrieval applications.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 09:50:07 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Gajane", "Pratik", "", "FT R and D"], ["Urvoy", "Tanguy", "", "FT R and D"], ["Cl\u00e9rot", "Fabrice", "", "FT R and D"]]}, {"id": "1601.03945", "submitter": "Alberto N. Escalante-B.", "authors": "Alberto N. Escalante-B. and Laurenz Wiskott", "title": "Improved graph-based SFA: Information preservation complements the\n  slowness principle", "comments": "40 pages, 9 figures, 9 tables, submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow feature analysis (SFA) is an unsupervised-learning algorithm that\nextracts slowly varying features from a multi-dimensional time series. A\nsupervised extension to SFA for classification and regression is graph-based\nSFA (GSFA). GSFA is based on the preservation of similarities, which are\nspecified by a graph structure derived from the labels. It has been shown that\nhierarchical GSFA (HGSFA) allows learning from images and other\nhigh-dimensional data. The feature space spanned by HGSFA is complex due to the\ncomposition of the nonlinearities of the nodes in the network. However, we show\nthat the network discards useful information prematurely before it reaches\nhigher nodes, resulting in suboptimal global slowness and an under-exploited\nfeature space.\n  To counteract these problems, we propose an extension called hierarchical\ninformation-preserving GSFA (HiGSFA), where information preservation\ncomplements the slowness-maximization goal. We build a 10-layer HiGSFA network\nto estimate human age from facial photographs of the MORPH-II database,\nachieving a mean absolute error of 3.50 years, improving the state-of-the-art\nperformance. HiGSFA and HGSFA support multiple-labels and offer a rich feature\nspace, feed-forward training, and linear complexity in the number of samples\nand dimensions. Furthermore, HiGSFA outperforms HGSFA in terms of feature\nslowness, estimation accuracy and input reconstruction, giving rise to a\npromising hierarchical supervised-learning approach.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 15:00:20 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Escalante-B.", "Alberto N.", ""], ["Wiskott", "Laurenz", ""]]}, {"id": "1601.04011", "submitter": "Alon Gonen", "authors": "Alon Gonen, Shai Shalev-Shwartz", "title": "Average Stability is Invariant to Data Preconditioning. Implications to\n  Exp-concave Empirical Risk Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the average stability notion introduced by\n\\cite{kearns1999algorithmic, bousquet2002stability} is invariant to data\npreconditioning, for a wide class of generalized linear models that includes\nmost of the known exp-concave losses. In other words, when analyzing the\nstability rate of a given algorithm, we may assume the optimal preconditioning\nof the data. This implies that, at least from a statistical perspective,\nexplicit regularization is not required in order to compensate for\nill-conditioned data, which stands in contrast to a widely common approach that\nincludes a regularization for analyzing the sample complexity of generalized\nlinear models. Several important implications of our findings include: a) We\ndemonstrate that the excess risk of empirical risk minimization (ERM) is\ncontrolled by the preconditioned stability rate. This immediately yields a\nrelatively short and elegant proof for the fast rates attained by ERM in our\ncontext. b) We strengthen the recent bounds of \\cite{hardt2015train} on the\nstability rate of the Stochastic Gradient Descent algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 17:32:44 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 11:46:18 GMT"}, {"version": "v3", "created": "Tue, 11 Oct 2016 12:29:09 GMT"}, {"version": "v4", "created": "Sun, 16 Apr 2017 12:15:33 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Gonen", "Alon", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1601.04033", "submitter": "Augustus Odena", "authors": "Augustus Odena", "title": "Faster Asynchronous SGD", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous distributed stochastic gradient descent methods have trouble\nconverging because of stale gradients. A gradient update sent to a parameter\nserver by a client is stale if the parameters used to calculate that gradient\nhave since been updated on the server. Approaches have been proposed to\ncircumvent this problem that quantify staleness in terms of the number of\nelapsed updates. In this work, we propose a novel method that quantifies\nstaleness in terms of moving averages of gradient statistics. We show that this\nmethod outperforms previous methods with respect to convergence speed and\nscalability to many clients. We also discuss how an extension to this method\ncan be used to dramatically reduce bandwidth costs in a distributed training\ncontext. In particular, our method allows reduction of total bandwidth usage by\na factor of 5 with little impact on cost convergence. We also describe (and\nlink to) a software library that we have used to simulate these algorithms\ndeterministically on a single machine.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 19:03:47 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Odena", "Augustus", ""]]}, {"id": "1601.04114", "submitter": "Hossein Mobahi", "authors": "Hossein Mobahi", "title": "Training Recurrent Neural Networks by Diffusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a new algorithm for training recurrent neural networks\n(although ideas are applicable to feedforward networks as well). The algorithm\nis derived from a theory in nonconvex optimization related to the diffusion\nequation. The contributions made in this work are two fold. First, we show how\nsome seemingly disconnected mechanisms used in deep learning such as smart\ninitialization, annealed learning rate, layerwise pretraining, and noise\ninjection (as done in dropout and SGD) arise naturally and automatically from\nthis framework, without manually crafting them into the algorithms. Second, we\npresent some preliminary results on comparing the proposed method against SGD.\nIt turns out that the new algorithm can achieve similar level of generalization\naccuracy of SGD in much fewer number of epochs.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 02:24:17 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2016 23:22:52 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Mobahi", "Hossein", ""]]}, {"id": "1601.04126", "submitter": "Kush Varshney", "authors": "Kush R. Varshney", "title": "Engineering Safety in Machine Learning", "comments": "2016 Information Theory and Applications Workshop, La Jolla,\n  California", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms are increasingly influencing our decisions and\ninteracting with us in all parts of our daily lives. Therefore, just like for\npower plants, highways, and myriad other engineered sociotechnical systems, we\nmust consider the safety of systems involving machine learning. In this paper,\nwe first discuss the definition of safety in terms of risk, epistemic\nuncertainty, and the harm incurred by unwanted outcomes. Then we examine\ndimensions, such as the choice of cost function and the appropriateness of\nminimizing the empirical average training cost, along which certain real-world\napplications may not be completely amenable to the foundational principle of\nmodern statistical machine learning: empirical risk minimization. In\nparticular, we note an emerging dichotomy of applications: ones in which safety\nis important and risk minimization is not the complete story (we name these\nType A applications), and ones in which safety is not so critical and risk\nminimization is sufficient (we name these Type B applications). Finally, we\ndiscuss how four different strategies for achieving safety in engineering\n(inherently safe design, safety reserves, safe fail, and procedural safeguards)\ncan be mapped to the machine learning context through interpretability and\ncausality of predictive models, objectives beyond expected prediction accuracy,\nhuman involvement for labeling difficult or rare examples, and user experience\ndesign of software.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 05:46:57 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Varshney", "Kush R.", ""]]}, {"id": "1601.04149", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Ding Liu, Shiyu Chang, Qing Ling, Yingzhen Yang, and\n  Thomas S. Huang", "title": "$\\mathbf{D^3}$: Deep Dual-Domain Based Fast Restoration of\n  JPEG-Compressed Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design a Deep Dual-Domain ($\\mathbf{D^3}$) based fast\nrestoration model to remove artifacts of JPEG compressed images. It leverages\nthe large learning capacity of deep networks, as well as the problem-specific\nexpertise that was hardly incorporated in the past design of deep\narchitectures. For the latter, we take into consideration both the prior\nknowledge of the JPEG compression scheme, and the successful practice of the\nsparsity-based dual-domain approach. We further design the One-Step Sparse\nInference (1-SI) module, as an efficient and light-weighted feed-forward\napproximation of sparse coding. Extensive experiments verify the superiority of\nthe proposed $D^3$ model over several state-of-the-art methods. Specifically,\nour best model is capable of outperforming the latest deep model for around 1\ndB in PSNR, and is 30 times faster.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 10:38:43 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 03:19:10 GMT"}, {"version": "v3", "created": "Sat, 9 Apr 2016 19:25:08 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Wang", "Zhangyang", ""], ["Liu", "Ding", ""], ["Chang", "Shiyu", ""], ["Ling", "Qing", ""], ["Yang", "Yingzhen", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1601.04153", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Shiyu Chang, Yingzhen Yang, Ding Liu, and Thomas S.\n  Huang", "title": "Studying Very Low Resolution Recognition Using Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual recognition research often assumes a sufficient resolution of the\nregion of interest (ROI). That is usually violated in practice, inspiring us to\nexplore the Very Low Resolution Recognition (VLRR) problem. Typically, the ROI\nin a VLRR problem can be smaller than $16 \\times 16$ pixels, and is challenging\nto be recognized even by human experts. We attempt to solve the VLRR problem\nusing deep learning methods. Taking advantage of techniques primarily in super\nresolution, domain adaptation and robust regression, we formulate a dedicated\ndeep learning method and demonstrate how these techniques are incorporated step\nby step. Any extra complexity, when introduced, is fully justified by both\nanalysis and simulation results. The resulting \\textit{Robust Partially Coupled\nNetworks} achieves feature enhancement and recognition simultaneously. It\nallows for both the flexibility to combat the LR-HR domain mismatch, and the\nrobustness to outliers. Finally, the effectiveness of the proposed models is\nevaluated on three different VLRR tasks, including face identification, digit\nrecognition and font recognition, all of which obtain very impressive\nperformances.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 10:54:33 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 03:21:40 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Wang", "Zhangyang", ""], ["Chang", "Shiyu", ""], ["Yang", "Yingzhen", ""], ["Liu", "Ding", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1601.04155", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Shiyu Chang, Florin Dolcos, Diane Beck, Ding Liu, and\n  Thomas S. Huang", "title": "Brain-Inspired Deep Networks for Image Aesthetics Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image aesthetics assessment has been challenging due to its subjective\nnature. Inspired by the scientific advances in the human visual perception and\nneuroaesthetics, we design Brain-Inspired Deep Networks (BDN) for this task.\nBDN first learns attributes through the parallel supervised pathways, on a\nvariety of selected feature dimensions. A high-level synthesis network is\ntrained to associate and transform those attributes into the overall aesthetics\nrating. We then extend BDN to predicting the distribution of human ratings,\nsince aesthetics ratings are often subjective. Another highlight is our\nfirst-of-its-kind study of label-preserving transformations in the context of\naesthetics assessment, which leads to an effective data augmentation approach.\nExperimental results on the AVA dataset show that our biological inspired and\ntask-specific BDN model gains significantly performance improvement, compared\nto other state-of-the-art models with the same or higher parameter capacity.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 10:59:40 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 03:46:27 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Wang", "Zhangyang", ""], ["Chang", "Shiyu", ""], ["Dolcos", "Florin", ""], ["Beck", "Diane", ""], ["Liu", "Ding", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1601.04251", "submitter": "Giulia Prando", "authors": "Diego Romeres, Giulia Prando, Gianluigi Pillonetto, Alessandro Chiuso", "title": "On-line Bayesian System Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an on-line system identification setting, in which new data\nbecome available at given time steps. In order to meet real-time estimation\nrequirements, we propose a tailored Bayesian system identification procedure,\nin which the hyper-parameters are still updated through Marginal Likelihood\nmaximization, but after only one iteration of a suitable iterative optimization\nalgorithm. Both gradient methods and the EM algorithm are considered for the\nMarginal Likelihood optimization. We compare this \"1-step\" procedure with the\nstandard one, in which the optimization method is run until convergence to a\nlocal minimum. The experiments we perform confirm the effectiveness of the\napproach we propose.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2016 05:20:19 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Romeres", "Diego", ""], ["Prando", "Giulia", ""], ["Pillonetto", "Gianluigi", ""], ["Chiuso", "Alessandro", ""]]}, {"id": "1601.04366", "submitter": "Martin Stra\\v{z}ar Martin Stra\\v{z}ar", "authors": "Martin Stra\\v{z}ar, Toma\\v{z} Curk", "title": "Learning the kernel matrix via predictive low-rank approximations", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.neucom.2019.02.030", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and accurate low-rank approximations of multiple data sources are\nessential in the era of big data. The scaling of kernel-based learning\nalgorithms to large datasets is limited by the O(n^2) computation and storage\ncomplexity of the full kernel matrix, which is required by most of the recent\nkernel learning algorithms.\n  We present the Mklaren algorithm to approximate multiple kernel matrices\nlearn a regression model, which is entirely based on geometrical concepts. The\nalgorithm does not require access to full kernel matrices yet it accounts for\nthe correlations between all kernels. It uses Incomplete Cholesky\ndecomposition, where pivot selection is based on least-angle regression in the\ncombined, low-dimensional feature space. The algorithm has linear complexity in\nthe number of data points and kernels. When explicit feature space induced by\nthe kernel can be constructed, a mapping from the dual to the primal Ridge\nregression weights is used for model interpretation.\n  The Mklaren algorithm was tested on eight standard regression datasets. It\noutperforms contemporary kernel matrix approximation approaches when learning\nwith multiple kernels. It identifies relevant kernels, achieving highest\nexplained variance than other multiple kernel learning methods for the same\nnumber of iterations. Test accuracy, equivalent to the one using full kernel\nmatrices, was achieved with at significantly lower approximation ranks. A\ndifference in run times of two orders of magnitude was observed when either the\nnumber of samples or kernels exceeds 3000.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2016 23:31:37 GMT"}, {"version": "v2", "created": "Mon, 9 May 2016 16:05:42 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Stra\u017ear", "Martin", ""], ["Curk", "Toma\u017e", ""]]}, {"id": "1601.04451", "submitter": "Robert Duin", "authors": "Robert P.W. Duin, Elzbieta Pekalska", "title": "Zero-error dissimilarity based classifiers", "comments": "5 pages. Paper originally written in 2003. Although it may proof an\n  obvious fact, it is significant for understanding the essential conditions it\n  is based on", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider general non-Euclidean distance measures between real world\nobjects that need to be classified. It is assumed that objects are represented\nby distances to other objects only. Conditions for zero-error dissimilarity\nbased classifiers are derived. Additional conditions are given under which the\nzero-error decision boundary is a continues function of the distances to a\nfinite set of training samples. These conditions affect the objects as well as\nthe distance measure used. It is argued that they can be met in practice.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 10:12:15 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Duin", "Robert P. W.", ""], ["Pekalska", "Elzbieta", ""]]}, {"id": "1601.04468", "submitter": "Stefan Riezler", "authors": "Artem Sokolov and Stefan Riezler and Tanguy Urvoy", "title": "Bandit Structured Prediction for Learning from Partial Feedback in\n  Statistical Machine Translation", "comments": "In Proceedings of MT Summit XV, 2015. Miami, FL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to structured prediction from bandit feedback, called\nBandit Structured Prediction, where only the value of a task loss function at a\nsingle predicted point, instead of a correct structure, is observed in\nlearning. We present an application to discriminative reranking in Statistical\nMachine Translation (SMT) where the learning algorithm only has access to a\n1-BLEU loss evaluation of a predicted translation instead of obtaining a gold\nstandard reference translation. In our experiment bandit feedback is obtained\nby evaluating BLEU on reference translations without revealing them to the\nalgorithm. This can be thought of as a simulation of interactive machine\ntranslation where an SMT system is personalized by a user who provides single\npoint feedback to predicted translations. Our experiments show that our\napproach improves translation quality and is comparable to approaches that\nemploy more informative feedback in learning.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 11:09:02 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Sokolov", "Artem", ""], ["Riezler", "Stefan", ""], ["Urvoy", "Tanguy", ""]]}, {"id": "1601.04530", "submitter": "Robert Duin", "authors": "Robert P.W. Duin, Elzbieta Pekalska", "title": "Domain based classification", "comments": "8 pages, unpublished paper written in 2005, discussing a significant,\n  still almost not studied problem. missing reference links corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of traditional classification ru les minimizing the expected\nprobability of error (0-1 loss) are inappropriate if the class probability\ndistributions are ill-defined or impossible to estimate. We argue that in such\ncases class domains should be used instead of class distributions or densities\nto construct a reliable decision function. Proposals are presented for some\nevaluation criteria and classifier learning schemes, illustrated by an example.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 14:31:12 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 16:30:35 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Duin", "Robert P. W.", ""], ["Pekalska", "Elzbieta", ""]]}, {"id": "1601.04549", "submitter": "Raffaello Camoriano", "authors": "Raffaello Camoriano, Silvio Traversaro, Lorenzo Rosasco, Giorgio Metta\n  and Francesco Nori", "title": "Incremental Semiparametric Inverse Dynamics Learning", "comments": null, "journal-ref": null, "doi": "10.1109/ICRA.2016.7487177", "report-no": null, "categories": "stat.ML cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for incremental semiparametric inverse\ndynamics learning. In particular, we consider the mixture of two approaches:\nParametric modeling based on rigid body dynamics equations and nonparametric\nmodeling based on incremental kernel methods, with no prior information on the\nmechanical properties of the system. This yields to an incremental\nsemiparametric approach, leveraging the advantages of both the parametric and\nnonparametric models. We validate the proposed technique learning the dynamics\nof one arm of the iCub humanoid robot.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 14:54:37 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Camoriano", "Raffaello", ""], ["Traversaro", "Silvio", ""], ["Rosasco", "Lorenzo", ""], ["Metta", "Giorgio", ""], ["Nori", "Francesco", ""]]}, {"id": "1601.04574", "submitter": "Heriberto Cuay\\'ahuitl", "authors": "Heriberto Cuay\\'ahuitl", "title": "SimpleDS: A Simple Deep Reinforcement Learning Dialogue System", "comments": "International Workshop on Spoken Dialogue Systems (IWSDS), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents 'SimpleDS', a simple and publicly available dialogue\nsystem trained with deep reinforcement learning. In contrast to previous\nreinforcement learning dialogue systems, this system avoids manual feature\nengineering by performing action selection directly from raw text of the last\nsystem and (noisy) user responses. Our initial results, in the restaurant\ndomain, show that it is indeed possible to induce reasonable dialogue behaviour\nwith an approach that aims for high levels of automation in dialogue control\nfor intelligent interactive agents.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 15:37:22 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Cuay\u00e1huitl", "Heriberto", ""]]}, {"id": "1601.04580", "submitter": "Jacob Eisenstein", "authors": "Vinodh Krishnan and Jacob Eisenstein", "title": "Nonparametric Bayesian Storyline Detection from Microtexts", "comments": "Appeared at the Workshop on Computing News Storylines at the 2016\n  Conference on Empirical Methods in Natural Language Processing (EMNLP 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News events and social media are composed of evolving storylines, which\ncapture public attention for a limited period of time. Identifying storylines\nrequires integrating temporal and linguistic information, and prior work takes\na largely heuristic approach. We present a novel online non-parametric Bayesian\nframework for storyline detection, using the distance-dependent Chinese\nRestaurant Process (dd-CRP). To ensure efficient linear-time inference, we\nemploy a fixed-lag Gibbs sampling procedure, which is novel for the dd-CRP. We\nevaluate on the TREC Twitter Timeline Generation (TTG), obtaining encouraging\nresults: despite using a weak baseline retrieval model, the dd-CRP story\nclustering method is competitive with the best entries in the 2014 TTG task.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 15:46:00 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 20:27:47 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Krishnan", "Vinodh", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1601.04586", "submitter": "Binhuan Wang", "authors": "Binhuan Wang, Yilong Zhang, Will Wei Sun, Yixin Fang", "title": "Sparse Convex Clustering", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2017.1377081", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex clustering, a convex relaxation of k-means clustering and hierarchical\nclustering, has drawn recent attentions since it nicely addresses the\ninstability issue of traditional nonconvex clustering methods. Although its\ncomputational and statistical properties have been recently studied, the\nperformance of convex clustering has not yet been investigated in the\nhigh-dimensional clustering scenario, where the data contains a large number of\nfeatures and many of them carry no information about the clustering structure.\nIn this paper, we demonstrate that the performance of convex clustering could\nbe distorted when the uninformative features are included in the clustering. To\novercome it, we introduce a new clustering method, referred to as Sparse Convex\nClustering, to simultaneously cluster observations and conduct feature\nselection. The key idea is to formulate convex clustering in a form of\nregularization, with an adaptive group-lasso penalty term on cluster centers.\nIn order to optimally balance the tradeoff between the cluster fitting and\nsparsity, a tuning criterion based on clustering stability is developed. In\ntheory, we provide an unbiased estimator for the degrees of freedom of the\nproposed sparse convex clustering method. Finally, the effectiveness of the\nsparse convex clustering is examined through a variety of numerical experiments\nand a real data application.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 16:03:35 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 03:26:26 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2016 14:38:43 GMT"}, {"version": "v4", "created": "Fri, 10 Feb 2017 16:51:07 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Wang", "Binhuan", ""], ["Zhang", "Yilong", ""], ["Sun", "Will Wei", ""], ["Fang", "Yixin", ""]]}, {"id": "1601.04692", "submitter": "Jean Gallier", "authors": "Jean Gallier", "title": "Spectral Theory of Unsigned and Signed Graphs. Applications to Graph\n  Clustering: a Survey", "comments": "122 pages. arXiv admin note: substantial text overlap with\n  arXiv:1311.2492", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a survey of the method of graph cuts and its applications to graph\nclustering of weighted unsigned and signed graphs. I provide a fairly thorough\ntreatment of the method of normalized graph cuts, a deeply original method due\nto Shi and Malik, including complete proofs. The main thrust of this paper is\nthe method of normalized cuts. I give a detailed account for K = 2 clusters,\nand also for K > 2 clusters, based on the work of Yu and Shi. I also show how\nboth graph drawing and normalized cut K-clustering can be easily generalized to\nhandle signed graphs, which are weighted graphs in which the weight matrix W\nmay have negative coefficients. Intuitively, negative coefficients indicate\ndistance or dissimilarity. The solution is to replace the degree matrix by the\nmatrix in which absolute values of the weights are used, and to replace the\nLaplacian by the Laplacian with the new degree matrix of absolute values. As\nfar as I know, the generalization of K-way normalized clustering to signed\ngraphs is new. Finally, I show how the method of ratio cuts, in which a cut is\nnormalized by the size of the cluster rather than its volume, is just a special\ncase of normalized cuts.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 20:57:41 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Gallier", "Jean", ""]]}, {"id": "1601.04737", "submitter": "Farbod Roosta-Khorasani", "authors": "Farbod Roosta-Khorasani and Michael W. Mahoney", "title": "Sub-Sampled Newton Methods I: Globally Convergent Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale optimization problems are ubiquitous in machine learning and data\nanalysis and there is a plethora of algorithms for solving such problems. Many\nof these algorithms employ sub-sampling, as a way to either speed up the\ncomputations and/or to implicitly implement a form of statistical\nregularization. In this paper, we consider second-order iterative optimization\nalgorithms and we provide bounds on the convergence of the variants of Newton's\nmethod that incorporate uniform sub-sampling as a means to estimate the\ngradient and/or Hessian. Our bounds are non-asymptotic and quantitative. Our\nalgorithms are global and are guaranteed to converge from any initial iterate.\n  Using random matrix concentration inequalities, one can sub-sample the\nHessian to preserve the curvature information. Our first algorithm incorporates\nHessian sub-sampling while using the full gradient. We also give additional\nconvergence results for when the sub-sampled Hessian is regularized by\nmodifying its spectrum or ridge-type regularization. Next, in addition to\nHessian sub-sampling, we also consider sub-sampling the gradient as a way to\nfurther reduce the computational complexity per iteration. We use approximate\nmatrix multiplication results from randomized numerical linear algebra to\nobtain the proper sampling strategy. In all these algorithms, computing the\nupdate boils down to solving a large scale linear system, which can be\ncomputationally expensive. As a remedy, for all of our algorithms, we also give\nglobal convergence results for the case of inexact updates where such linear\nsystem is solved only approximately.\n  This paper has a more advanced companion paper, [42], in which we demonstrate\nthat, by doing a finer-grained analysis, we can get problem-independent bounds\nfor local convergence of these algorithms and explore trade-offs to improve\nupon the basic results of the present paper.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 21:59:21 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 01:16:32 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2016 04:04:24 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Roosta-Khorasani", "Farbod", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1601.04738", "submitter": "Farbod Roosta-Khorasani", "authors": "Farbod Roosta-Khorasani and Michael W. Mahoney", "title": "Sub-Sampled Newton Methods II: Local Convergence Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data-fitting applications require the solution of an optimization\nproblem involving a sum of large number of functions of high dimensional\nparameter. Here, we consider the problem of minimizing a sum of $n$ functions\nover a convex constraint set $\\mathcal{X} \\subseteq \\mathbb{R}^{p}$ where both\n$n$ and $p$ are large. In such problems, sub-sampling as a way to reduce $n$\ncan offer great amount of computational efficiency.\n  Within the context of second order methods, we first give quantitative local\nconvergence results for variants of Newton's method where the Hessian is\nuniformly sub-sampled. Using random matrix concentration inequalities, one can\nsub-sample in a way that the curvature information is preserved. Using such\nsub-sampling strategy, we establish locally Q-linear and Q-superlinear\nconvergence rates. We also give additional convergence results for when the\nsub-sampled Hessian is regularized by modifying its spectrum or Levenberg-type\nregularization.\n  Finally, in addition to Hessian sub-sampling, we consider sub-sampling the\ngradient as way to further reduce the computational complexity per iteration.\nWe use approximate matrix multiplication results from randomized numerical\nlinear algebra (RandNLA) to obtain the proper sampling strategy and we\nestablish locally R-linear convergence rates. In such a setting, we also show\nthat a very aggressive sample size increase results in a R-superlinearly\nconvergent algorithm.\n  While the sample size depends on the condition number of the problem, our\nconvergence rates are problem-independent, i.e., they do not depend on the\nquantities related to the problem. Hence, our analysis here can be used to\ncomplement the results of our basic framework from the companion paper, [38],\nby exploring algorithmic trade-offs that are important in practice.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 22:04:32 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 01:19:22 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2016 04:06:39 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Roosta-Khorasani", "Farbod", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1601.04756", "submitter": "Jaderick Pabico", "authors": "Maureen Lyndel C. Lauron, Jaderick P. Pabico", "title": "Improved Sampling Techniques for Learning an Imbalanced Data Set", "comments": "7 pages, 10 figures, 16th Philippine Computing Science Congress (PCSC\n  2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents the performance of a classifier built using the stackingC\nalgorithm in nine different data sets. Each data set is generated using a\nsampling technique applied on the original imbalanced data set. Five new\nsampling techniques are proposed in this paper (i.e., SMOTERandRep, Lax Random\nOversampling, Lax Random Undersampling, Combined-Lax Random Oversampling\nUndersampling, and Combined-Lax Random Undersampling Oversampling) that were\nbased on the three sampling techniques (i.e., Random Undersampling, Random\nOversampling, and Synthetic Minority Oversampling Technique) usually used as\nsolutions in imbalance learning. The metrics used to evaluate the classifier's\nperformance were F-measure and G-mean. F-measure determines the performance of\nthe classifier for every class, while G-mean measures the overall performance\nof the classifier. The results using F-measure showed that for the data without\na sampling technique, the classifier's performance is good only for the\nmajority class. It also showed that among the eight sampling techniques, RU and\nLRU have the worst performance while other techniques (i.e., RO, C-LRUO and\nC-LROU) performed well only on some classes. The best performing techniques in\nall data sets were SMOTE, SMOTERandRep, and LRO having the lowest F-measure\nvalues between 0.5 and 0.65. The results using G-mean showed that the\noversampling technique that attained the highest G-mean value is LRO (0.86),\nnext is C-LROU (0.85), then SMOTE (0.84) and finally is SMOTERandRep (0.83).\nCombining the result of the two metrics (F-measure and G-mean), only the three\nsampling techniques are considered as good performing (i.e., LRO, SMOTE, and\nSMOTERandRep).\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 23:31:12 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Lauron", "Maureen Lyndel C.", ""], ["Pabico", "Jaderick P.", ""]]}, {"id": "1601.04800", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng", "title": "Top-N Recommender System via Matrix Completion", "comments": "AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-N recommender systems have been investigated widely both in industry and\nacademia. However, the recommendation quality is far from satisfactory. In this\npaper, we propose a simple yet promising algorithm. We fill the user-item\nmatrix based on a low-rank assumption and simultaneously keep the original\ninformation. To do that, a nonconvex rank relaxation rather than the nuclear\nnorm is adopted to provide a better rank approximation and an efficient\noptimization strategy is designed. A comprehensive set of experiments on real\ndatasets demonstrates that our method pushes the accuracy of Top-N\nrecommendation to a new level.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 04:48:42 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""]]}, {"id": "1601.04920", "submitter": "St\\'ephane Mallat", "authors": "St\\'ephane Mallat", "title": "Understanding Deep Convolutional Networks", "comments": "17 pages, 4 Figures", "journal-ref": null, "doi": "10.1098/rsta.2015.0203", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks provide state of the art classifications and\nregressions results over many high-dimensional problems. We review their\narchitecture, which scatters data with a cascade of linear filter weights and\nnon-linearities. A mathematical framework is introduced to analyze their\nproperties. Computations of invariants involve multiscale contractions, the\nlinearization of hierarchical symmetries, and sparse separations. Applications\nare discussed.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 13:40:47 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Mallat", "St\u00e9phane", ""]]}, {"id": "1601.05116", "submitter": "Hossein Mobahi", "authors": "Hossein Mobahi, Stefano Soatto", "title": "A Theory of Local Matching: SIFT and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why has SIFT been so successful? Why its extension, DSP-SIFT, can further\nimprove SIFT? Is there a theory that can explain both? How can such theory\nbenefit real applications? Can it suggest new algorithms with reduced\ncomputational complexity or new descriptors with better accuracy for matching?\nWe construct a general theory of local descriptors for visual matching. Our\ntheory relies on concepts in energy minimization and heat diffusion. We show\nthat SIFT and DSP-SIFT approximate the solution the theory suggests. In\nparticular, DSP-SIFT gives a better approximation to the theoretical solution;\njustifying why DSP-SIFT outperforms SIFT. Using the developed theory, we derive\nnew descriptors that have fewer parameters and are potentially better in\nhandling affine deformations.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 22:15:48 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Mobahi", "Hossein", ""], ["Soatto", "Stefano", ""]]}, {"id": "1601.05141", "submitter": "Pranav Agrawal", "authors": "Mengfan Tang, Pranav Agrawal, Ramesh Jain", "title": "Habits vs Environment: What really causes asthma?", "comments": "Presented at ACM WebSci 2015, Oxford UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite considerable number of studies on risk factors for asthma onset, very\nlittle is known about their relative importance. To have a full picture of\nthese factors, both categories, personal and environmental data, have to be\ntaken into account simultaneously, which is missing in previous studies. We\npropose a framework to rank the risk factors from heterogeneous data sources of\nthe two categories. Established on top of EventShop and Personal EventShop,\nthis framework extracts about 400 features, and analyzes them by employing a\ngradient boosting tree. The features come from sources including personal\nprofile and life-event data, and environmental data on air pollution, weather\nand PM2.5 emission sources. The top ranked risk factors derived from our\nframework agree well with the general medical consensus. Thus, our framework is\na reliable approach, and the discovered rankings of relative importance of risk\nfactors can provide insights for the prevention of asthma.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 00:37:11 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Tang", "Mengfan", ""], ["Agrawal", "Pranav", ""], ["Jain", "Ramesh", ""]]}, {"id": "1601.05495", "submitter": "Sewoong Oh", "authors": "Ashish Khetan and Sewoong Oh", "title": "Data-driven Rank Breaking for Efficient Rank Aggregation", "comments": "46 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank aggregation systems collect ordinal preferences from individuals to\nproduce a global ranking that represents the social preference. Rank-breaking\nis a common practice to reduce the computational complexity of learning the\nglobal ranking. The individual preferences are broken into pairwise comparisons\nand applied to efficient algorithms tailored for independent paired\ncomparisons. However, due to the ignored dependencies in the data, naive\nrank-breaking approaches can result in inconsistent estimates. The key idea to\nproduce accurate and consistent estimates is to treat the pairwise comparisons\nunequally, depending on the topology of the collected data. In this paper, we\nprovide the optimal rank-breaking estimator, which not only achieves\nconsistency but also achieves the best error bound. This allows us to\ncharacterize the fundamental tradeoff between accuracy and complexity. Further,\nthe analysis identifies how the accuracy depends on the spectral gap of a\ncorresponding comparison graph.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 02:39:39 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 14:28:42 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Khetan", "Ashish", ""], ["Oh", "Sewoong", ""]]}, {"id": "1601.05675", "submitter": "Daniele Calandriello", "authors": "Daniele Calandriello, Alessandro Lazaric, Michal Valko and Ioannis\n  Koutis", "title": "Incremental Spectral Sparsification for Large-Scale Graph-Based\n  Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the harmonic function solution performs well in many semi-supervised\nlearning (SSL) tasks, it is known to scale poorly with the number of samples.\nRecent successful and scalable methods, such as the eigenfunction method focus\non efficiently approximating the whole spectrum of the graph Laplacian\nconstructed from the data. This is in contrast to various subsampling and\nquantization methods proposed in the past, which may fail in preserving the\ngraph spectra. However, the impact of the approximation of the spectrum on the\nfinal generalization error is either unknown, or requires strong assumptions on\nthe data. In this paper, we introduce Sparse-HFS, an efficient\nedge-sparsification algorithm for SSL. By constructing an edge-sparse and\nspectrally similar graph, we are able to leverage the approximation guarantees\nof spectral sparsification methods to bound the generalization error of\nSparse-HFS. As a result, we obtain a theoretically-grounded approximation\nscheme for graph-based SSL that also empirically matches the performance of\nknown large-scale methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 15:31:35 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Calandriello", "Daniele", ""], ["Lazaric", "Alessandro", ""], ["Valko", "Michal", ""], ["Koutis", "Ioannis", ""]]}, {"id": "1601.05764", "submitter": "\\'Ad\\'am D. Lelkes", "authors": "Benjamin Fish, Jeremy Kun, \\'Ad\\'am D. Lelkes", "title": "A Confidence-Based Approach for Balancing Fairness and Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study three classical machine learning algorithms in the context of\nalgorithmic fairness: adaptive boosting, support vector machines, and logistic\nregression. Our goal is to maintain the high accuracy of these learning\nalgorithms while reducing the degree to which they discriminate against\nindividuals because of their membership in a protected group.\n  Our first contribution is a method for achieving fairness by shifting the\ndecision boundary for the protected group. The method is based on the theory of\nmargins for boosting. Our method performs comparably to or outperforms previous\nalgorithms in the fairness literature in terms of accuracy and low\ndiscrimination, while simultaneously allowing for a fast and transparent\nquantification of the trade-off between bias and error.\n  Our second contribution addresses the shortcomings of the bias-error\ntrade-off studied in most of the algorithmic fairness literature. We\ndemonstrate that even hopelessly naive modifications of a biased algorithm,\nwhich cannot be reasonably said to be fair, can still achieve low bias and high\naccuracy. To help to distinguish between these naive algorithms and more\nsensible algorithms we propose a new measure of fairness, called resilience to\nrandom bias (RRB). We demonstrate that RRB distinguishes well between our naive\nand sensible fairness algorithms. RRB together with bias and accuracy provides\na more complete picture of the fairness of an algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 19:48:07 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Fish", "Benjamin", ""], ["Kun", "Jeremy", ""], ["Lelkes", "\u00c1d\u00e1m D.", ""]]}, {"id": "1601.05775", "submitter": "Twan van Laarhoven", "authors": "Twan van Laarhoven, Elena Marchiori", "title": "Local Network Community Detection with Continuous Optimization of\n  Conductance and Weighted Kernel K-Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local network community detection is the task of finding a single community\nof nodes concentrated around few given seed nodes in a localized way.\nConductance is a popular objective function used in many algorithms for local\ncommunity detection. This paper studies a continuous relaxation of conductance.\nWe show that continuous optimization of this objective still leads to discrete\ncommunities. We investigate the relation of conductance with weighted kernel\nk-means for a single community, which leads to the introduction of a new\nobjective function, $\\sigma$-conductance. Conductance is obtained by setting\n$\\sigma$ to $0$. Two algorithms, EMc and PGDc, are proposed to locally optimize\n$\\sigma$-conductance and automatically tune the parameter $\\sigma$. They are\nbased on expectation maximization and projected gradient descent, respectively.\nWe prove locality and give performance guarantees for EMc and PGDc for a class\nof dense and well separated communities centered around the seeds. Experiments\nare conducted on networks with ground-truth communities, comparing to\nstate-of-the-art graph diffusion algorithms for conductance optimization. On\nlarge graphs, results indicate that EMc and PGDc stay localized and produce\ncommunities most similar to the ground, while graph diffusion algorithms\ngenerate large communities of lower quality.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 20:36:59 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 18:03:01 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["van Laarhoven", "Twan", ""], ["Marchiori", "Elena", ""]]}, {"id": "1601.05900", "submitter": "Jarrod Moore", "authors": "Margareta Ackerman and Jarrod Moore", "title": "When is Clustering Perturbation Robust?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a fundamental data mining tool that aims to divide data into\ngroups of similar items. Generally, intuition about clustering reflects the\nideal case -- exact data sets endowed with flawless dissimilarity between\nindividual instances.\n  In practice however, these cases are in the minority, and clustering\napplications are typically characterized by noisy data sets with approximate\npairwise dissimilarities. As such, the efficacy of clustering methods in\npractical applications necessitates robustness to perturbations.\n  In this paper, we perform a formal analysis of perturbation robustness,\nrevealing that the extent to which algorithms can exhibit this desirable\ncharacteristic is inherently limited, and identifying the types of structures\nthat allow popular clustering paradigms to discover meaningful clusters in\nspite of faulty data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 08:01:58 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Ackerman", "Margareta", ""], ["Moore", "Jarrod", ""]]}, {"id": "1601.05936", "submitter": "Pranay Dighe", "authors": "Pranay Dighe, Gil Luyet, Afsaneh Asaei and Herve Bourlard", "title": "Exploiting Low-dimensional Structures to Enhance DNN Based Acoustic\n  Modeling in Speech Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2016.7472767", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to model the acoustic space of deep neural network (DNN)\nclass-conditional posterior probabilities as a union of low-dimensional\nsubspaces. To that end, the training posteriors are used for dictionary\nlearning and sparse coding. Sparse representation of the test posteriors using\nthis dictionary enables projection to the space of training data. Relying on\nthe fact that the intrinsic dimensions of the posterior subspaces are indeed\nvery small and the matrix of all posteriors belonging to a class has a very low\nrank, we demonstrate how low-dimensional structures enable further enhancement\nof the posteriors and rectify the spurious errors due to mismatch conditions.\nThe enhanced acoustic modeling method leads to improvements in continuous\nspeech recognition task using hybrid DNN-HMM (hidden Markov model) framework in\nboth clean and noisy conditions, where upto 15.4% relative reduction in word\nerror rate (WER) is achieved.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 10:02:47 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Dighe", "Pranay", ""], ["Luyet", "Gil", ""], ["Asaei", "Afsaneh", ""], ["Bourlard", "Herve", ""]]}, {"id": "1601.06035", "submitter": "Cyril Stark", "authors": "Cyril Stark", "title": "Recommender systems inspired by the structure of quantum theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physicists use quantum models to describe the behavior of physical systems.\nQuantum models owe their success to their interpretability, to their relation\nto probabilistic models (quantization of classical models) and to their high\npredictive power. Beyond physics, these properties are valuable in general data\nscience. This motivates the use of quantum models to analyze general\nnonphysical datasets. Here we provide both empirical and theoretical insights\ninto the application of quantum models in data science. In the theoretical part\nof this paper, we firstly show that quantum models can be exponentially more\nefficient than probabilistic models because there exist datasets that admit\nlow-dimensional quantum models and only exponentially high-dimensional\nprobabilistic models. Secondly, we explain in what sense quantum models realize\na useful relaxation of compressed probabilistic models. Thirdly, we show that\nsparse datasets admit low-dimensional quantum models and finally, we introduce\na method to compute hierarchical orderings of properties of users (e.g.,\npersonality traits) and items (e.g., genres of movies). In the empirical part\nof the paper, we evaluate quantum models in item recommendation and observe\nthat the predictive power of quantum-inspired recommender systems can compete\nwith state-of-the-art recommender systems like SVD++ and PureSVD. Furthermore,\nwe make use of the interpretability of quantum models by computing hierarchical\norderings of properties of users and items. This work establishes a connection\nbetween data science (item recommendation), information theory (communication\ncomplexity), mathematical programming (positive semidefinite factorizations)\nand physics (quantum models).\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 15:09:18 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Stark", "Cyril", ""]]}, {"id": "1601.06071", "submitter": "Minje Kim", "authors": "Minje Kim and Paris Smaragdis", "title": "Bitwise Neural Networks", "comments": "This paper was presented at the International Conference on Machine\n  Learning (ICML) Workshop on Resource-Efficient Machine Learning, Lille,\n  France, Jul. 6-11, 2015", "journal-ref": "International Conference on Machine Learning (ICML) Workshop on\n  Resource-Efficient Machine Learning, Lille, France, Jul. 6-11, 2015", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the assumption that there exists a neural network that efficiently\nrepresents a set of Boolean functions between all binary inputs and outputs, we\npropose a process for developing and deploying neural networks whose weight\nparameters, bias terms, input, and intermediate hidden layer output signals,\nare all binary-valued, and require only basic bit logic for the feedforward\npass. The proposed Bitwise Neural Network (BNN) is especially suitable for\nresource-constrained environments, since it replaces either floating or\nfixed-point arithmetic with significantly more efficient bitwise operations.\nHence, the BNN requires for less spatial complexity, less memory bandwidth, and\nless power consumption in hardware. In order to design such networks, we\npropose to add a few training schemes, such as weight compression and noisy\nbackpropagation, which result in a bitwise network that performs almost as well\nas its corresponding real-valued network. We test the proposed network on the\nMNIST dataset, represented using binary features, and show that BNNs result in\ncompetitive performance while offering dramatic computational savings.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 16:59:01 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Kim", "Minje", ""], ["Smaragdis", "Paris", ""]]}, {"id": "1601.06105", "submitter": "Venkatesh Saligrama", "authors": "Jonathan Root, Venkatesh Saligrama, Jing Qian", "title": "Learning Minimum Volume Sets and Anomaly Detectors from KNN Graphs", "comments": "arXiv admin note: substantial text overlap with arXiv:1502.01783,\n  arXiv:1405.0530", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a non-parametric anomaly detection algorithm for high dimensional\ndata. We first rank scores derived from nearest neighbor graphs on $n$-point\nnominal training data. We then train limited complexity models to imitate these\nscores based on the max-margin learning-to-rank framework. A test-point is\ndeclared as an anomaly at $\\alpha$-false alarm level if the predicted score is\nin the $\\alpha$-percentile. The resulting anomaly detector is shown to be\nasymptotically optimal in that for any false alarm rate $\\alpha$, its decision\nregion converges to the $\\alpha$-percentile minimum volume level set of the\nunknown underlying density. In addition, we test both the statistical\nperformance and computational efficiency of our algorithm on a number of\nsynthetic and real-data experiments. Our results demonstrate the superiority of\nour algorithm over existing $K$-NN based anomaly detection algorithms, with\nsignificant computational savings.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 19:10:31 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Root", "Jonathan", ""], ["Saligrama", "Venkatesh", ""], ["Qian", "Jing", ""]]}, {"id": "1601.06116", "submitter": "James Mnatzaganian", "authors": "James Mnatzaganian, Ernest Fokou\\'e, and Dhireesha Kudithipudi", "title": "A Mathematical Formalization of Hierarchical Temporal Memory's Spatial\n  Pooler", "comments": "This work was submitted for publication and is currently under\n  review. For associated code, see https://github.com/tehtechguy/mHTM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical temporal memory (HTM) is an emerging machine learning algorithm,\nwith the potential to provide a means to perform predictions on spatiotemporal\ndata. The algorithm, inspired by the neocortex, currently does not have a\ncomprehensive mathematical framework. This work brings together all aspects of\nthe spatial pooler (SP), a critical learning component in HTM, under a single\nunifying framework. The primary learning mechanism is explored, where a maximum\nlikelihood estimator for determining the degree of permanence update is\nproposed. The boosting mechanisms are studied and found to be only relevant\nduring the initial few iterations of the network. Observations are made\nrelating HTM to well-known algorithms such as competitive learning and\nattribute bagging. Methods are provided for using the SP for classification as\nwell as dimensionality reduction. Empirical evidence verifies that given the\nproper parameterizations, the SP may be used for feature learning.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 19:26:16 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 02:26:49 GMT"}, {"version": "v3", "created": "Thu, 8 Sep 2016 20:15:01 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Mnatzaganian", "James", ""], ["Fokou\u00e9", "Ernest", ""], ["Kudithipudi", "Dhireesha", ""]]}, {"id": "1601.06180", "submitter": "Robert Peharz", "authors": "Robert Peharz, Robert Gens, Franz Pernkopf, Pedro Domingos", "title": "On the Latent Variable Interpretation in Sum-Product Networks", "comments": "Revised version, accepted for publication in IEEE Transactions on\n  Machine Intelligence and Pattern Analysis (TPAMI). Shortened and revised\n  Section 4: Thanks to our reviewers, pointing out that Theorem 2 holds for\n  selective SPNs. Added paragraph in Section 2.1, relating sizes of\n  original/augmented SPNs. Fixed typos, rephrased sentences, revised references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the central themes in Sum-Product networks (SPNs) is the\ninterpretation of sum nodes as marginalized latent variables (LVs). This\ninterpretation yields an increased syntactic or semantic structure, allows the\napplication of the EM algorithm and to efficiently perform MPE inference. In\nliterature, the LV interpretation was justified by explicitly introducing the\nindicator variables corresponding to the LVs' states. However, as pointed out\nin this paper, this approach is in conflict with the completeness condition in\nSPNs and does not fully specify the probabilistic model. We propose a remedy\nfor this problem by modifying the original approach for introducing the LVs,\nwhich we call SPN augmentation. We discuss conditional independencies in\naugmented SPNs, formally establish the probabilistic interpretation of the\nsum-weights and give an interpretation of augmented SPNs as Bayesian networks.\nBased on these results, we find a sound derivation of the EM algorithm for\nSPNs. Furthermore, the Viterbi-style algorithm for MPE proposed in literature\nwas never proven to be correct. We show that this is indeed a correct\nalgorithm, when applied to selective SPNs, and in particular when applied to\naugmented SPNs. Our theoretical results are confirmed in experiments on\nsynthetic data and 103 real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 21:40:33 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 07:54:35 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Peharz", "Robert", ""], ["Gens", "Robert", ""], ["Pernkopf", "Franz", ""], ["Domingos", "Pedro", ""]]}, {"id": "1601.06201", "submitter": "Prashant Khanduri", "authors": "Prashant Khanduri, Bhavya Kailkhura, Jayaraman J. Thiagarajan, Pramod\n  K. Varshney", "title": "Universal Collaboration Strategies for Signal Detection: A Sparse\n  Learning Approach", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2016.2601911", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of high dimensional signal detection in a\nlarge distributed network whose nodes can collaborate with their one-hop\nneighboring nodes (spatial collaboration). We assume that only a small subset\nof nodes communicate with the Fusion Center (FC). We design optimal\ncollaboration strategies which are universal for a class of deterministic\nsignals. By establishing the equivalence between the collaboration strategy\ndesign problem and sparse PCA, we solve the problem efficiently and evaluate\nthe impact of collaboration on detection performance.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 23:15:42 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 21:35:46 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Khanduri", "Prashant", ""], ["Kailkhura", "Bhavya", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1601.06207", "submitter": "Alican Nalci", "authors": "Alican Nalci, Igor Fedorov, Maher Al-Shoukairi, Thomas T. Liu, and\n  Bhaskar D. Rao", "title": "Rectified Gaussian Scale Mixtures and the Sparse Non-Negative Least\n  Squares Problem", "comments": "Under Review by IEEE Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a Bayesian evidence maximization framework to solve\nthe sparse non-negative least squares (S-NNLS) problem. We introduce a family\nof probability densities referred to as the Rectified Gaussian Scale Mixture\n(R- GSM) to model the sparsity enforcing prior distribution for the solution.\nThe R-GSM prior encompasses a variety of heavy-tailed densities such as the\nrectified Laplacian and rectified Student- t distributions with a proper choice\nof the mixing density. We utilize the hierarchical representation induced by\nthe R-GSM prior and develop an evidence maximization framework based on the\nExpectation-Maximization (EM) algorithm. Using the EM based method, we estimate\nthe hyper-parameters and obtain a point estimate for the solution. We refer to\nthe proposed method as rectified sparse Bayesian learning (R-SBL). We provide\nfour R- SBL variants that offer a range of options for computational complexity\nand the quality of the E-step computation. These methods include the Markov\nchain Monte Carlo EM, linear minimum mean-square-error estimation, approximate\nmessage passing and a diagonal approximation. Using numerical experiments, we\nshow that the proposed R-SBL method outperforms existing S-NNLS solvers in\nterms of both signal and support recovery performance, and is also very robust\nagainst the structure of the design matrix.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 23:47:36 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 10:14:35 GMT"}, {"version": "v3", "created": "Wed, 9 Mar 2016 02:38:23 GMT"}, {"version": "v4", "created": "Sun, 19 Mar 2017 00:03:43 GMT"}, {"version": "v5", "created": "Fri, 16 Feb 2018 21:27:34 GMT"}, {"version": "v6", "created": "Tue, 27 Mar 2018 18:36:04 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Nalci", "Alican", ""], ["Fedorov", "Igor", ""], ["Al-Shoukairi", "Maher", ""], ["Liu", "Thomas T.", ""], ["Rao", "Bhaskar D.", ""]]}, {"id": "1601.06239", "submitter": "Yao Wang", "authors": "Xiangyu Chang, Shaobo Lin and Yao Wang", "title": "Divide and Conquer Local Average Regression", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The divide and conquer strategy, which breaks a massive data set into a se-\nries of manageable data blocks, and then combines the independent results of\ndata blocks to obtain a final decision, has been recognized as a\nstate-of-the-art method to overcome challenges of massive data analysis. In\nthis paper, we merge the divide and conquer strategy with local average\nregression methods to infer the regressive relationship of input-output pairs\nfrom a massive data set. After theoretically analyzing the pros and cons, we\nfind that although the divide and conquer local average regression can reach\nthe optimal learning rate, the restric- tion to the number of data blocks is a\nbit strong, which makes it only feasible for small number of data blocks. We\nthen propose two variants to lessen (or remove) this restriction. Our results\nshow that these variants can achieve the optimal learning rate with much milder\nrestriction (or without such restriction). Extensive experimental studies are\ncarried out to verify our theoretical assertions.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 06:17:03 GMT"}, {"version": "v2", "created": "Sun, 13 Mar 2016 18:00:50 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Chang", "Xiangyu", ""], ["Lin", "Shaobo", ""], ["Wang", "Yao", ""]]}, {"id": "1601.06248", "submitter": "Takuya Koumura", "authors": "Takuya Koumura and Kazuo Okanoya", "title": "Automatic recognition of element classes and boundaries in the birdsong\n  with variable sequences", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0159188", "report-no": null, "categories": "q-bio.NC cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Researches on sequential vocalization often require analysis of vocalizations\nin long continuous sounds. In such studies as developmental ones or studies\nacross generations in which days or months of vocalizations must be analyzed,\nmethods for automatic recognition would be strongly desired. Although methods\nfor automatic speech recognition for application purposes have been intensively\nstudied, blindly applying them for biological purposes may not be an optimal\nsolution. This is because, unlike human speech recognition, analysis of\nsequential vocalizations often requires accurate extraction of timing\ninformation. In the present study we propose automated systems suitable for\nrecognizing birdsong, one of the most intensively investigated sequential\nvocalizations, focusing on the three properties of the birdsong. First, a song\nis a sequence of vocal elements, called notes, which can be grouped into\ncategories. Second, temporal structure of birdsong is precisely controlled,\nmeaning that temporal information is important in song analysis. Finally, notes\nare produced according to certain probabilistic rules, which may facilitate the\naccurate song recognition. We divided the procedure of song recognition into\nthree sub-steps: local classification, boundary detection, and global\nsequencing, each of which corresponds to each of the three properties of\nbirdsong. We compared the performances of several different ways to arrange\nthese three steps. As results, we demonstrated a hybrid model of a deep neural\nnetwork and a hidden Markov model is effective in recognizing birdsong with\nvariable note sequences. We propose suitable arrangements of methods according\nto whether accurate boundary detection is needed. Also we designed the new\nmeasure to jointly evaluate the accuracy of note classification and boundary\ndetection. Our methods should be applicable, with small modification and\ntuning, to the songs in other species that hold the three properties of the\nsequential vocalization.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 07:57:56 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Koumura", "Takuya", ""], ["Okanoya", "Kazuo", ""]]}, {"id": "1601.06259", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, David Isenberg, Aarti Singh, Larry Wasserman", "title": "Minimax Lower Bounds for Linear Independence Testing", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear independence testing is a fundamental information-theoretic and\nstatistical problem that can be posed as follows: given $n$ points\n$\\{(X_i,Y_i)\\}^n_{i=1}$ from a $p+q$ dimensional multivariate distribution\nwhere $X_i \\in \\mathbb{R}^p$ and $Y_i \\in\\mathbb{R}^q$, determine whether $a^T\nX$ and $b^T Y$ are uncorrelated for every $a \\in \\mathbb{R}^p, b\\in\n\\mathbb{R}^q$ or not. We give minimax lower bound for this problem (when $p+q,n\n\\to \\infty$, $(p+q)/n \\leq \\kappa < \\infty$, without sparsity assumptions). In\nsummary, our results imply that $n$ must be at least as large as $\\sqrt\n{pq}/\\|\\Sigma_{XY}\\|_F^2$ for any procedure (test) to have non-trivial power,\nwhere $\\Sigma_{XY}$ is the cross-covariance matrix of $X,Y$. We also provide\nsome evidence that the lower bound is tight, by connections to two-sample\ntesting and regression in specific settings.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 10:20:58 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Isenberg", "David", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1601.06342", "submitter": "Sung-Hsien Hsieh", "authors": "Sung-Hsien Hsieh, Chun-Shien Lu, Soo-Chang Pei", "title": "Fast Binary Embedding via Circulant Downsampled Matrix -- A\n  Data-Independent Approach", "comments": "8 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary embedding of high-dimensional data aims to produce low-dimensional\nbinary codes while preserving discriminative power. State-of-the-art methods\noften suffer from high computation and storage costs. We present a simple and\nfast embedding scheme by first downsampling N-dimensional data into\nM-dimensional data and then multiplying the data with an MxM circulant matrix.\nOur method requires O(N +M log M) computation and O(N) storage costs. We prove\nif data have sparsity, our scheme can achieve similarity-preserving well.\nExperiments further demonstrate that though our method is cost-effective and\nfast, it still achieves comparable performance in image applications.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2016 03:32:22 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Hsieh", "Sung-Hsien", ""], ["Lu", "Chun-Shien", ""], ["Pei", "Soo-Chang", ""]]}, {"id": "1601.06440", "submitter": "Amandianeze Nwana", "authors": "Amandianeze O. Nwana and Tsuhan Chen", "title": "QUOTE: \"Querying\" Users as Oracles in Tag Engines - A Semi-Supervised\n  Learning Approach to Personalized Image Tagging", "comments": null, "journal-ref": null, "doi": "10.1109/ISM.2016.0016", "report-no": null, "categories": "cs.IR cs.LG cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One common trend in image tagging research is to focus on visually relevant\ntags, and this tends to ignore the personal and social aspect of tags,\nespecially on photoblogging websites such as Flickr. Previous work has\ncorrectly identified that many of the tags that users provide on images are not\nvisually relevant (i.e. representative of the salient content in the image) and\nthey go on to treat such tags as noise, ignoring that the users chose to\nprovide those tags over others that could have been more visually relevant.\nAnother common assumption about user generated tags for images is that the\norder of these tags provides no useful information for the prediction of tags\non future images. This assumption also tends to define usefulness in terms of\nwhat is visually relevant to the image. For general tagging or labeling\napplications that focus on providing visual information about image content,\nthese assumptions are reasonable, but when considering personalized image\ntagging applications, these assumptions are at best too rigid, ignoring user\nchoice and preferences.\n  We challenge the aforementioned assumptions, and provide a machine learning\napproach to the problem of personalized image tagging with the following\ncontributions: 1.) We reformulate the personalized image tagging problem as a\nsearch/retrieval ranking problem, 2.) We leverage the order of tags, which does\nnot always reflect visual relevance, provided by the user in the past as a cue\nto their tag preferences, similar to click data, 3.) We propose a technique to\naugment sparse user tag data (semi-supervision), and 4.) We demonstrate the\nefficacy of our method on a subset of Flickr images, showing improvement over\nprevious state-of-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 21:07:16 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Nwana", "Amandianeze O.", ""], ["Chen", "Tsuhan", ""]]}, {"id": "1601.06476", "submitter": "Gregory Puleo", "authors": "Jack P. Hou, Amin Emad, Gregory J. Puleo, Jian Ma, Olgica Milenkovic", "title": "A new correlation clustering method for cancer mutation analysis", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancer genomes exhibit a large number of different alterations that affect\nmany genes in a diverse manner. It is widely believed that these alterations\nfollow combinatorial patterns that have a strong connection with the underlying\nmolecular interaction networks and functional pathways. A better understanding\nof the generative mechanisms behind the mutation rules and their influence on\ngene communities is of great importance for the process of driver mutations\ndiscovery and for identification of network modules related to cancer\ndevelopment and progression. We developed a new method for cancer mutation\npattern analysis based on a constrained form of correlation clustering.\nCorrelation clustering is an agnostic learning method that can be used for\ngeneral community detection problems in which the number of communities or\ntheir structure is not known beforehand. The resulting algorithm, named $C^3$,\nleverages mutual exclusivity of mutations, patient coverage, and driver network\nconcentration principles; it accepts as its input a user determined combination\nof heterogeneous patient data, such as that available from TCGA (including\nmutation, copy number, and gene expression information), and creates a large\nnumber of clusters containing mutually exclusive mutated genes in a particular\ntype of cancer. The cluster sizes may be required to obey some useful soft size\nconstraints, without impacting the computational complexity of the algorithm.\nTo test $C^3$, we performed a detailed analysis on TCGA breast cancer and\nglioblastoma data and showed that our algorithm outperforms the\nstate-of-the-art CoMEt method in terms of discovering mutually exclusive gene\nmodules and identifying driver genes. Our $C^3$ method represents a unique tool\nfor efficient and reliable identification of mutation patterns and driver\npathways in large-scale cancer genomics studies.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 04:02:52 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Hou", "Jack P.", ""], ["Emad", "Amin", ""], ["Puleo", "Gregory J.", ""], ["Ma", "Jian", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1601.06551", "submitter": "Tian Lin", "authors": "Wei Chen, Tian Lin, Zihan Tan, Mingfei Zhao, Xuren Zhou", "title": "Robust Influence Maximization", "comments": "12 pages, 4 figures, Technical Report, contains proofs for the paper\n  appeared in KDD'2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the important issue of uncertainty in the edge\ninfluence probability estimates for the well studied influence maximization\nproblem --- the task of finding $k$ seed nodes in a social network to maximize\nthe influence spread. We propose the problem of robust influence maximization,\nwhich maximizes the worst-case ratio between the influence spread of the chosen\nseed set and the optimal seed set, given the uncertainty of the parameter\ninput. We design an algorithm that solves this problem with a\nsolution-dependent bound. We further study uniform sampling and adaptive\nsampling methods to effectively reduce the uncertainty on parameters and\nimprove the robustness of the influence maximization task. Our empirical\nresults show that parameter uncertainty may greatly affect influence\nmaximization performance and prior studies that learned influence probabilities\ncould lead to poor performance in robust influence maximization due to\nrelatively large uncertainty in parameter estimates, and information cascade\nbased adaptive sampling method may be an effective way to improve the\nrobustness of influence maximization.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 10:36:47 GMT"}, {"version": "v2", "created": "Sun, 12 Jun 2016 06:24:13 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Chen", "Wei", ""], ["Lin", "Tian", ""], ["Tan", "Zihan", ""], ["Zhao", "Mingfei", ""], ["Zhou", "Xuren", ""]]}, {"id": "1601.06581", "submitter": "Kyuyeon Hwang", "authors": "Kyuyeon Hwang, Wonyong Sung", "title": "Character-Level Incremental Speech Recognition with Recurrent Neural\n  Networks", "comments": "To appear in ICASSP 2016", "journal-ref": null, "doi": "10.1109/ICASSP.2016.7472696", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-time speech recognition applications, the latency is an important\nissue. We have developed a character-level incremental speech recognition (ISR)\nsystem that responds quickly even during the speech, where the hypotheses are\ngradually improved while the speaking proceeds. The algorithm employs a\nspeech-to-character unidirectional recurrent neural network (RNN), which is\nend-to-end trained with connectionist temporal classification (CTC), and an\nRNN-based character-level language model (LM). The output values of the\nCTC-trained RNN are character-level probabilities, which are processed by beam\nsearch decoding. The RNN LM augments the decoding by providing long-term\ndependency information. We propose tree-based online beam search with\nadditional depth-pruning, which enables the system to process infinitely long\ninput speech with low latency. This system not only responds quickly on speech\nbut also can dictate out-of-vocabulary (OOV) words according to pronunciation.\nThe proposed model achieves the word error rate (WER) of 8.90% on the Wall\nStreet Journal (WSJ) Nov'92 20K evaluation set when trained on the WSJ SI-284\ntraining set.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 12:51:46 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 11:03:05 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Hwang", "Kyuyeon", ""], ["Sung", "Wonyong", ""]]}, {"id": "1601.06602", "submitter": "Markus Schneider", "authors": "Markus Schneider and Wolfgang Ertel and Fabio Ramos", "title": "Expected Similarity Estimation for Large-Scale Batch and Streaming\n  Anomaly Detection", "comments": null, "journal-ref": null, "doi": "10.1007/s10994-016-5567-7", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm for anomaly detection on very large datasets and\ndata streams. The method, named EXPected Similarity Estimation (EXPoSE), is\nkernel-based and able to efficiently compute the similarity between new data\npoints and the distribution of regular data. The estimator is formulated as an\ninner product with a reproducing kernel Hilbert space embedding and makes no\nassumption about the type or shape of the underlying data distribution. We show\nthat offline (batch) learning with EXPoSE can be done in linear time and online\n(incremental) learning takes constant time per instance and model update.\nFurthermore, EXPoSE can make predictions in constant time, while it requires\nonly constant memory. In addition, we propose different methodologies for\nconcept drift adaptation on evolving data streams. On several real datasets we\ndemonstrate that our approach can compete with state of the art algorithms for\nanomaly detection while being an order of magnitude faster than most other\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 13:56:59 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2016 12:37:33 GMT"}, {"version": "v3", "created": "Mon, 6 Jun 2016 13:48:17 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Schneider", "Markus", ""], ["Ertel", "Wolfgang", ""], ["Ramos", "Fabio", ""]]}, {"id": "1601.06615", "submitter": "Suraj Srinivas", "authors": "Suraj Srinivas, Ravi Kiran Sarvadevabhatla, Konda Reddy Mopuri, Nikita\n  Prabhu, Srinivas S S Kruthiventi and R. Venkatesh Babu", "title": "A Taxonomy of Deep Convolutional Neural Nets for Computer Vision", "comments": "Published in Frontiers in Robotics and AI (http://goo.gl/6691Bm)", "journal-ref": "Frontiers in Robotics and AI 2(36), January 2016", "doi": "10.3389/frobt.2015.00036", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional architectures for solving computer vision problems and the degree\nof success they enjoyed have been heavily reliant on hand-crafted features.\nHowever, of late, deep learning techniques have offered a compelling\nalternative -- that of automatically learning problem-specific features. With\nthis new paradigm, every problem in computer vision is now being re-examined\nfrom a deep learning perspective. Therefore, it has become important to\nunderstand what kind of deep networks are suitable for a given problem.\nAlthough general surveys of this fast-moving paradigm (i.e. deep-networks)\nexist, a survey specific to computer vision is missing. We specifically\nconsider one form of deep networks widely used in computer vision -\nconvolutional neural networks (CNNs). We start with \"AlexNet\" as our base CNN\nand then examine the broad variations proposed over time to suit different\napplications. We hope that our recipe-style survey will serve as a guide,\nparticularly for novice practitioners intending to use deep-learning techniques\nfor computer vision.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 14:25:07 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Srinivas", "Suraj", ""], ["Sarvadevabhatla", "Ravi Kiran", ""], ["Mopuri", "Konda Reddy", ""], ["Prabhu", "Nikita", ""], ["Kruthiventi", "Srinivas S S", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1601.06650", "submitter": "Ilija Bogunovic Ilija Bogunovic", "authors": "Ilija Bogunovic, Jonathan Scarlett, Volkan Cevher", "title": "Time-Varying Gaussian Process Bandit Optimization", "comments": "To appear in AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the sequential Bayesian optimization problem with bandit\nfeedback, adopting a formulation that allows for the reward function to vary\nwith time. We model the reward function using a Gaussian process whose\nevolution obeys a simple Markov model. We introduce two natural extensions of\nthe classical Gaussian process upper confidence bound (GP-UCB) algorithm. The\nfirst, R-GP-UCB, resets GP-UCB at regular intervals. The second, TV-GP-UCB,\ninstead forgets about old data in a smooth fashion. Our main contribution\ncomprises of novel regret bounds for these algorithms, providing an explicit\ncharacterization of the trade-off between the time horizon and the rate at\nwhich the function varies. We illustrate the performance of the algorithms on\nboth synthetic and real data, and we find the gradual forgetting of TV-GP-UCB\nto perform favorably compared to the sharp resetting of R-GP-UCB. Moreover,\nboth algorithms significantly outperform classical GP-UCB, since it treats\nstale and fresh data equally.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 16:02:50 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Bogunovic", "Ilija", ""], ["Scarlett", "Jonathan", ""], ["Cevher", "Volkan", ""]]}, {"id": "1601.06680", "submitter": "Jos\\'e A. R. Fonollosa", "authors": "Jos\\'e A. R. Fonollosa", "title": "Conditional distribution variability measures for causality detection", "comments": "NIPS 2013 workshop on causality", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive variability measures for the conditional probability\ndistributions of a pair of random variables, and we study its application in\nthe inference of causal-effect relationships. We also study the combination of\nthe proposed measures with standard statistical measures in the the framework\nof the ChaLearn cause-effect pair challenge. The developed model obtains an AUC\nscore of 0.82 on the final test database and ranked second in the challenge.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 17:14:31 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Fonollosa", "Jos\u00e9 A. R.", ""]]}, {"id": "1601.06683", "submitter": "Alaa Saade", "authors": "Alaa Saade, Marc Lelarge, Florent Krzakala and Lenka Zdeborov\\'a", "title": "Clustering from Sparse Pairwise Measurements", "comments": null, "journal-ref": "Proceedings of the 2016 IEEE International Symposium on\n  Information Theory (ISIT) Pages: 780 - 784", "doi": "10.1109/ISIT.2016.7541405", "report-no": null, "categories": "cs.SI cond-mat.dis-nn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of grouping items into clusters based on few random\npairwise comparisons between the items. We introduce three closely related\nalgorithms for this task: a belief propagation algorithm approximating the\nBayes optimal solution, and two spectral algorithms based on the\nnon-backtracking and Bethe Hessian operators. For the case of two symmetric\nclusters, we conjecture that these algorithms are asymptotically optimal in\nthat they detect the clusters as soon as it is information theoretically\npossible to do so. We substantiate this claim for one of the spectral\napproaches we introduce.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 17:19:48 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 06:14:01 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Saade", "Alaa", ""], ["Lelarge", "Marc", ""], ["Krzakala", "Florent", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1601.06750", "submitter": "Divya Padmanabhan", "authors": "Divya Padmanabhan, Satyanath Bhat, Dinesh Garg, Shirish Shevade, Y.\n  Narahari", "title": "A Robust UCB Scheme for Active Learning in Regression from Strategic\n  Crowds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of training an accurate linear regression model by\nprocuring labels from multiple noisy crowd annotators, under a budget\nconstraint. We propose a Bayesian model for linear regression in crowdsourcing\nand use variational inference for parameter estimation. To minimize the number\nof labels crowdsourced from the annotators, we adopt an active learning\napproach. In this specific context, we prove the equivalence of well-studied\ncriteria of active learning like entropy minimization and expected error\nreduction. Interestingly, we observe that we can decouple the problems of\nidentifying an optimal unlabeled instance and identifying an annotator to label\nit. We observe a useful connection between the multi-armed bandit framework and\nthe annotator selection in active learning. Due to the nature of the\ndistribution of the rewards on the arms, we use the Robust Upper Confidence\nBound (UCB) scheme with truncated empirical mean estimator to solve the\nannotator selection problem. This yields provable guarantees on the regret. We\nfurther apply our model to the scenario where annotators are strategic and\ndesign suitable incentives to induce them to put in their best efforts.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 20:14:22 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 09:20:39 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Padmanabhan", "Divya", ""], ["Bhat", "Satyanath", ""], ["Garg", "Dinesh", ""], ["Shevade", "Shirish", ""], ["Narahari", "Y.", ""]]}, {"id": "1601.06759", "submitter": "A\\\"aron van den Oord", "authors": "Aaron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu", "title": "Pixel Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the distribution of natural images is a landmark problem in\nunsupervised learning. This task requires an image model that is at once\nexpressive, tractable and scalable. We present a deep neural network that\nsequentially predicts the pixels in an image along the two spatial dimensions.\nOur method models the discrete probability of the raw pixel values and encodes\nthe complete set of dependencies in the image. Architectural novelties include\nfast two-dimensional recurrent layers and an effective use of residual\nconnections in deep recurrent networks. We achieve log-likelihood scores on\nnatural images that are considerably better than the previous state of the art.\nOur main results also provide benchmarks on the diverse ImageNet dataset.\nSamples generated from the model appear crisp, varied and globally coherent.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 20:34:24 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 15:32:16 GMT"}, {"version": "v3", "created": "Fri, 19 Aug 2016 14:10:16 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Oord", "Aaron van den", ""], ["Kalchbrenner", "Nal", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1601.06815", "submitter": "Tyler Highlander", "authors": "Tyler Highlander and Andres Rodriguez", "title": "Very Efficient Training of Convolutional Neural Networks using Fast\n  Fourier Transform and Overlap-and-Add", "comments": "British Machine Vision Conference 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are currently state-of-the-art for\nvarious classification tasks, but are computationally expensive. Propagating\nthrough the convolutional layers is very slow, as each kernel in each layer\nmust sequentially calculate many dot products for a single forward and backward\npropagation which equates to $\\mathcal{O}(N^{2}n^{2})$ per kernel per layer\nwhere the inputs are $N \\times N$ arrays and the kernels are $n \\times n$\narrays. Convolution can be efficiently performed as a Hadamard product in the\nfrequency domain. The bottleneck is the transformation which has a cost of\n$\\mathcal{O}(N^{2}\\log_2 N)$ using the fast Fourier transform (FFT). However,\nthe increase in efficiency is less significant when $N\\gg n$ as is the case in\nCNNs. We mitigate this by using the \"overlap-and-add\" technique reducing the\ncomputational complexity to $\\mathcal{O}(N^2\\log_2 n)$ per kernel. This method\nincreases the algorithm's efficiency in both the forward and backward\npropagation, reducing the training and testing time for CNNs. Our empirical\nresults show our method reduces computational time by a factor of up to 16.3\ntimes the traditional convolution implementation for a 8 $\\times$ 8 kernel and\na 224 $\\times$ 224 image.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 21:29:11 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Highlander", "Tyler", ""], ["Rodriguez", "Andres", ""]]}, {"id": "1601.06823", "submitter": "Feng Wang", "authors": "Feng Wang, David M.J. Tax", "title": "Survey on the attention based RNN model and its applications in computer\n  vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recurrent neural networks (RNN) can be used to solve the sequence to\nsequence problem, where both the input and the output have sequential\nstructures. Usually there are some implicit relations between the structures.\nHowever, it is hard for the common RNN model to fully explore the relations\nbetween the sequences. In this survey, we introduce some attention based RNN\nmodels which can focus on different parts of the input for each output item, in\norder to explore and take advantage of the implicit relations between the input\nand the output items. The different attention mechanisms are described in\ndetail. We then introduce some applications in computer vision which apply the\nattention based RNN models. The superiority of the attention based RNN model is\nshown by the experimental results. At last some future research directions are\ngiven.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 21:54:02 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Wang", "Feng", ""], ["Tax", "David M. J.", ""]]}, {"id": "1601.06933", "submitter": "Mitra Montazeri", "authors": "Mohadeseh Montazeri, Hamid Reza Naji, Mitra Montazeri, Ahmad Faraahi", "title": "A Novel Memetic Feature Selection Algorithm", "comments": "M., Montazeri, H. R. Naji, M. Montazeri, A. Faraahi. A novel memetic\n  feature selection algorithm. In Information and Knowledge Technology (IKT),\n  2013 5th Conference on. 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is a problem of finding efficient features among all\nfeatures in which the final feature set can improve accuracy and reduce\ncomplexity. In feature selection algorithms search strategies are key aspects.\nSince feature selection is an NP-Hard problem; therefore heuristic algorithms\nhave been studied to solve this problem. In this paper, we have proposed a\nmethod based on memetic algorithm to find an efficient feature subset for a\nclassification problem. It incorporates a filter method in the genetic\nalgorithm to improve classification performance and accelerates the search in\nidentifying core feature subsets. Particularly, the method adds or deletes a\nfeature from a candidate feature subset based on the multivariate feature\ninformation. Empirical study on commonly data sets of the university of\nCalifornia, Irvine shows that the proposed method outperforms existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 09:07:08 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Montazeri", "Mohadeseh", ""], ["Naji", "Hamid Reza", ""], ["Montazeri", "Mitra", ""], ["Faraahi", "Ahmad", ""]]}, {"id": "1601.07213", "submitter": "Alexander Ororbia II", "authors": "Alexander G. Ororbia II, C. Lee Giles, and Daniel Kifer", "title": "Unifying Adversarial Training Algorithms with Flexible Deep Data\n  Gradient Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many previous proposals for adversarial training of deep neural nets have\nincluded di- rectly modifying the gradient, training on a mix of original and\nadversarial examples, using contractive penalties, and approximately optimizing\nconstrained adversarial ob- jective functions. In this paper, we show these\nproposals are actually all instances of optimizing a general, regularized\nobjective we call DataGrad. Our proposed DataGrad framework, which can be\nviewed as a deep extension of the layerwise contractive au- toencoder penalty,\ncleanly simplifies prior work and easily allows extensions such as adversarial\ntraining with multi-task cues. In our experiments, we find that the deep gra-\ndient regularization of DataGrad (which also has L1 and L2 flavors of\nregularization) outperforms alternative forms of regularization, including\nclassical L1, L2, and multi- task, both on the original dataset as well as on\nadversarial sets. Furthermore, we find that combining multi-task optimization\nwith DataGrad adversarial training results in the most robust performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 22:41:13 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 20:40:13 GMT"}, {"version": "v3", "created": "Fri, 29 Jul 2016 15:36:19 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Ororbia", "Alexander G.", "II"], ["Giles", "C. Lee", ""], ["Kifer", "Daniel", ""]]}, {"id": "1601.07233", "submitter": "Andrew Schaumberg", "authors": "Andrew Schaumberg, Angela Yu, Tatsuhiro Koshi, Xiaochan Zong,\n  Santoshkalyan Rayadhurgam", "title": "Predicting Drug Interactions and Mutagenicity with Ensemble Classifiers\n  on Subgraphs of Molecules", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this study, we intend to solve a mutual information problem in interacting\nmolecules of any type, such as proteins, nucleic acids, and small molecules.\nUsing machine learning techniques, we accurately predict pairwise interactions,\nwhich can be of medical and biological importance. Graphs are are useful in\nthis problem for their generality to all types of molecules, due to the\ninherent association of atoms through atomic bonds. Subgraphs can represent\ndifferent molecular domains. These domains can be biologically significant as\nmost molecules only have portions that are of functional significance and can\ninteract with other domains. Thus, we use subgraphs as features in different\nmachine learning algorithms to predict if two drugs interact and predict\npotential single molecule effects.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 00:31:02 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Schaumberg", "Andrew", ""], ["Yu", "Angela", ""], ["Koshi", "Tatsuhiro", ""], ["Zong", "Xiaochan", ""], ["Rayadhurgam", "Santoshkalyan", ""]]}, {"id": "1601.07243", "submitter": "Jean Honorio", "authors": "Jean Honorio", "title": "On the Sample Complexity of Learning Graphical Games", "comments": null, "journal-ref": "IEEE Allerton Conference on Communication, Control and Computing,\n  2017", "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the sample complexity of learning graphical games from purely\nbehavioral data. We assume that we can only observe the players' joint actions\nand not their payoffs. We analyze the sufficient and necessary number of\nsamples for the correct recovery of the set of pure-strategy Nash equilibria\n(PSNE) of the true game. Our analysis focuses on directed graphs with $n$ nodes\nand at most $k$ parents per node. Sparse graphs correspond to ${k \\in O(1)}$\nwith respect to $n$, while dense graphs correspond to ${k \\in O(n)}$. By using\nVC dimension arguments, we show that if the number of samples is greater than\n${O(k n \\log^2{n})}$ for sparse graphs or ${O(n^2 \\log{n})}$ for dense graphs,\nthen maximum likelihood estimation correctly recovers the PSNE with high\nprobability. By using information-theoretic arguments, we show that if the\nnumber of samples is less than ${\\Omega(k n \\log^2{n})}$ for sparse graphs or\n${\\Omega(n^2 \\log{n})}$ for dense graphs, then any conceivable method fails to\nrecover the PSNE with arbitrary probability.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 01:49:50 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 17:52:18 GMT"}, {"version": "v3", "created": "Mon, 20 Feb 2017 20:45:46 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Honorio", "Jean", ""]]}, {"id": "1601.07267", "submitter": "Ioannis Avramopoulos", "authors": "Ioannis Avramopoulos", "title": "Evolutionary stability implies asymptotic stability under multiplicative\n  weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that evolutionarily stable states in general (nonlinear) population\ngames (which can be viewed as continuous vector fields constrained on a\npolytope) are asymptotically stable under a multiplicative weights dynamic\n(under appropriate choices of a parameter called the learning rate or step\nsize, which we demonstrate to be crucial to achieve convergence, as otherwise\neven chaotic behavior is possible to manifest). Our result implies that\nevolutionary theories based on multiplicative weights are compatible (in\nprinciple, more general) with those based on the notion of evolutionary\nstability. However, our result further establishes multiplicative weights as a\nnonlinear programming primitive (on par with standard nonlinear programming\nmethods) since various nonlinear optimization problems, such as finding\nNash/Wardrop equilibria in nonatomic congestion games, which are well-known to\nbe equipped with a convex potential function, and finding strict local maxima\nof quadratic programming problems, are special cases of the problem of\ncomputing evolutionarily stable states in nonlinear population games.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 05:30:22 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2016 06:08:17 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Avramopoulos", "Ioannis", ""]]}, {"id": "1601.07358", "submitter": "Jens Clausen", "authors": "Jens Clausen, Hans J. Briegel", "title": "Quantum machine learning with glow for episodic tasks and decision games", "comments": "20 pages, 14 figures", "journal-ref": "Phys. Rev. A 97, 022303 (2018)", "doi": "10.1103/PhysRevA.97.022303", "report-no": null, "categories": "quant-ph cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a general class of models, where a reinforcement learning (RL)\nagent learns from cyclic interactions with an external environment via\nclassical signals. Perceptual inputs are encoded as quantum states, which are\nsubsequently transformed by a quantum channel representing the agent's memory,\nwhile the outcomes of measurements performed at the channel's output determine\nthe agent's actions. The learning takes place via stepwise modifications of the\nchannel properties. They are described by an update rule that is inspired by\nthe projective simulation (PS) model and equipped with a glow mechanism that\nallows for a backpropagation of policy changes, analogous to the eligibility\ntraces in RL and edge glow in PS. In this way, the model combines features of\nPS with the ability for generalization, offered by its physical embodiment as a\nquantum system. We apply the agent to various setups of an invasion game and a\ngrid world, which serve as elementary model tasks allowing a direct comparison\nwith a basic classical PS agent.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 13:31:38 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Clausen", "Jens", ""], ["Briegel", "Hans J.", ""]]}, {"id": "1601.07381", "submitter": "Lorenzo Livi", "authors": "Filippo Maria Bianchi and Lorenzo Livi and Cesare Alippi", "title": "Investigating echo state networks dynamics by means of recurrence\n  analysis", "comments": "Revised version. 24 pages; 12 figures", "journal-ref": null, "doi": "10.1109/TNNLS.2016.2630802", "report-no": null, "categories": "physics.data-an cs.LG nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we elaborate over the well-known interpretability issue in\necho state networks. The idea is to investigate the dynamics of reservoir\nneurons with time-series analysis techniques taken from research on complex\nsystems. Notably, we analyze time-series of neuron activations with Recurrence\nPlots (RPs) and Recurrence Quantification Analysis (RQA), which permit to\nvisualize and characterize high-dimensional dynamical systems. We show that\nthis approach is useful in a number of ways. First, the two-dimensional\nrepresentation offered by RPs provides a way for visualizing the\nhigh-dimensional dynamics of a reservoir. Our results suggest that, if the\nnetwork is stable, reservoir and input denote similar line patterns in the\nrespective RPs. Conversely, the more unstable the ESN, the more the RP of the\nreservoir presents instability patterns. As a second result, we show that the\n$\\mathrm{L_{max}}$ measure is highly correlated with the well-established\nmaximal local Lyapunov exponent. This suggests that complexity measures based\non RP diagonal lines distribution provide a valuable tool to quantify the\ndegree of network stability. Finally, our analysis shows that all RQA measures\nfluctuate on the proximity of the so-called edge of stability, where an ESN\ntypically achieves maximum computational capability. We verify that the\ndetermination of the edge of stability provided by such RQA measures is more\naccurate than two well-known criteria based on the Jacobian matrix of the\nreservoir. Therefore, we claim that RPs and RQA-based analyses can be used as\nvaluable tools to design an effective network given a specific problem.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 17:34:36 GMT"}, {"version": "v2", "created": "Sun, 24 Apr 2016 13:15:40 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Bianchi", "Filippo Maria", ""], ["Livi", "Lorenzo", ""], ["Alippi", "Cesare", ""]]}, {"id": "1601.07460", "submitter": "Asish Ghoshal", "authors": "Asish Ghoshal and Jean Honorio", "title": "Information-theoretic limits of Bayesian network structure learning", "comments": "Accepted to AISTATS 2017, Florida", "journal-ref": "International Conference on Artificial Intelligence and Statistics\n  (AISTATS), 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the information-theoretic limits of learning the\nstructure of Bayesian networks (BNs), on discrete as well as continuous random\nvariables, from a finite number of samples. We show that the minimum number of\nsamples required by any procedure to recover the correct structure grows as\n$\\Omega(m)$ and $\\Omega(k \\log m + (k^2/m))$ for non-sparse and sparse BNs\nrespectively, where $m$ is the number of variables and $k$ is the maximum\nnumber of parents per node. We provide a simple recipe, based on an extension\nof the Fano's inequality, to obtain information-theoretic limits of structure\nrecovery for any exponential family BN. We instantiate our result for specific\nconditional distributions in the exponential family to characterize the\nfundamental limits of learning various commonly used BNs, such as conditional\nprobability table based networks, gaussian BNs, noisy-OR networks, and logistic\nregression networks. En route to obtaining our main results, we obtain tight\nbounds on the number of sparse and non-sparse essential-DAGs. Finally, as a\nbyproduct, we recover the information-theoretic limits of sparse variable\nselection for logistic regression.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 17:41:05 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 04:27:52 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2016 01:11:16 GMT"}, {"version": "v4", "created": "Fri, 3 Mar 2017 05:57:15 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Ghoshal", "Asish", ""], ["Honorio", "Jean", ""]]}, {"id": "1601.07482", "submitter": "Cory Merkel", "authors": "Cory Merkel and Dhireesha Kudithipudi", "title": "Unsupervised Learning in Neuromemristive Systems", "comments": "To appear in the proceedings of the National Aerospace & Electronics\n  Conference & Ohio Innovation Summit (NAECON-OIS'15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromemristive systems (NMSs) currently represent the most promising\nplatform to achieve energy efficient neuro-inspired computation. However, since\nthe research field is less than a decade old, there are still countless\nalgorithms and design paradigms to be explored within these systems. One\nparticular domain that remains to be fully investigated within NMSs is\nunsupervised learning. In this work, we explore the design of an NMS for\nunsupervised clustering, which is a critical element of several machine\nlearning algorithms. Using a simple memristor crossbar architecture and\nlearning rule, we are able to achieve performance which is on par with MATLAB's\nk-means clustering.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 18:19:32 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Merkel", "Cory", ""], ["Kudithipudi", "Dhireesha", ""]]}, {"id": "1601.07621", "submitter": "Evan Racah Mr.", "authors": "Evan Racah, Seyoon Ko, Peter Sadowski, Wahid Bhimji, Craig Tull,\n  Sang-Yun Oh, Pierre Baldi, Prabhat", "title": "Revealing Fundamental Physics from the Daya Bay Neutrino Experiment\n  using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/ICMLA.2016.0160", "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experiments in particle physics produce enormous quantities of data that must\nbe analyzed and interpreted by teams of physicists. This analysis is often\nexploratory, where scientists are unable to enumerate the possible types of\nsignal prior to performing the experiment. Thus, tools for summarizing,\nclustering, visualizing and classifying high-dimensional data are essential. In\nthis work, we show that meaningful physical content can be revealed by\ntransforming the raw data into a learned high-level representation using deep\nneural networks, with measurements taken at the Daya Bay Neutrino Experiment as\na case study. We further show how convolutional deep neural networks can\nprovide an effective classification filter with greater than 97% accuracy\nacross different classes of physics events, significantly better than other\nmachine learning approaches.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 01:53:13 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2016 01:42:12 GMT"}, {"version": "v3", "created": "Tue, 6 Dec 2016 21:50:26 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Racah", "Evan", ""], ["Ko", "Seyoon", ""], ["Sadowski", "Peter", ""], ["Bhimji", "Wahid", ""], ["Tull", "Craig", ""], ["Oh", "Sang-Yun", ""], ["Baldi", "Pierre", ""], ["Prabhat", "", ""]]}, {"id": "1601.07714", "submitter": "Thomas Ketseoglou", "authors": "Brian Mohtashemi, Thomas Ketseoglou", "title": "Log-Normal Matrix Completion for Large Scale Link Prediction", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquitous proliferation of online social networks has led to the\nwidescale emergence of relational graphs expressing unique patterns in link\nformation and descriptive user node features. Matrix Factorization and\nCompletion have become popular methods for Link Prediction due to the low rank\nnature of mutual node friendship information, and the availability of parallel\ncomputer architectures for rapid matrix processing. Current Link Prediction\nliterature has demonstrated vast performance improvement through the\nutilization of sparsity in addition to the low rank matrix assumption. However,\nthe majority of research has introduced sparsity through the limited L1 or\nFrobenius norms, instead of considering the more detailed distributions which\nled to the graph formation and relationship evolution. In particular, social\nnetworks have been found to express either Pareto, or more recently discovered,\nLog Normal distributions. Employing the convexity-inducing Lovasz Extension, we\ndemonstrate how incorporating specific degree distribution information can lead\nto large scale improvements in Matrix Completion based Link prediction. We\nintroduce Log-Normal Matrix Completion (LNMC), and solve the complex\noptimization problem by employing Alternating Direction Method of Multipliers.\nUsing data from three popular social networks, our experiments yield up to 5%\nAUC increase over top-performing non-structured sparsity based methods.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 10:30:19 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Mohtashemi", "Brian", ""], ["Ketseoglou", "Thomas", ""]]}, {"id": "1601.07721", "submitter": "Peilin Zhong", "authors": "David P. Woodruff, Peilin Zhong", "title": "Distributed Low Rank Approximation of Implicit Functions of a Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed low rank approximation in which the matrix to be\napproximated is only implicitly represented across the different servers. For\nexample, each of $s$ servers may have an $n \\times d$ matrix $A^t$, and we may\nbe interested in computing a low rank approximation to $A = f(\\sum_{t=1}^s\nA^t)$, where $f$ is a function which is applied entrywise to the matrix\n$\\sum_{t=1}^s A^t$. We show for a wide class of functions $f$ it is possible to\nefficiently compute a $d \\times d$ rank-$k$ projection matrix $P$ for which\n$\\|A - AP\\|_F^2 \\leq \\|A - [A]_k\\|_F^2 + \\varepsilon \\|A\\|_F^2$, where $AP$\ndenotes the projection of $A$ onto the row span of $P$, and $[A]_k$ denotes the\nbest rank-$k$ approximation to $A$ given by the singular value decomposition.\nThe communication cost of our protocols is $d \\cdot (sk/\\varepsilon)^{O(1)}$,\nand they succeed with high probability. Our framework allows us to efficiently\ncompute a low rank approximation to an entry-wise softmax, to a Gaussian kernel\nexpansion, and to $M$-Estimators applied entrywise (i.e., forms of robust low\nrank approximation). We also show that our additive error approximation is best\npossible, in the sense that any protocol achieving relative error for these\nproblems requires significantly more communication. Finally, we experimentally\nvalidate our algorithms on real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 10:58:27 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Woodruff", "David P.", ""], ["Zhong", "Peilin", ""]]}, {"id": "1601.07795", "submitter": "Setareh Maghsudi", "authors": "Setareh Maghsudi and Ekram Hossain", "title": "Distributed User Association in Energy Harvesting Small Cell Networks: A\n  Probabilistic Model", "comments": "27 Pages, Single-Column", "journal-ref": null, "doi": "10.1109/TWC.2017.2647946", "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a distributed downlink user association problem in a small cell\nnetwork, where small cells obtain the required energy for providing wireless\nservices to users through ambient energy harvesting. Since energy harvesting is\nopportunistic in nature, the amount of harvested energy is a random variable,\nwithout any a priori known characteristics. Moreover, since users arrive in the\nnetwork randomly and require different wireless services, the energy\nconsumption is a random variable as well. In this paper, we propose a\nprobabilistic framework to mathematically model and analyze the random behavior\nof energy harvesting and energy consumption in dense small cell networks.\nFurthermore, as acquiring (even statistical) channel and network knowledge is\nvery costly in a distributed dense network, we develop a bandit-theoretical\nformulation for distributed user association when no information is available\nat users\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 17:14:44 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Maghsudi", "Setareh", ""], ["Hossain", "Ekram", ""]]}, {"id": "1601.07804", "submitter": "Xin Ding", "authors": "Xin Ding, Wei Chen and Ian J. Wassell", "title": "Joint Sensing Matrix and Sparsifying Dictionary Optimization for Tensor\n  Compressive Sensing", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2699639", "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor Compressive Sensing (TCS) is a multidimensional framework of\nCompressive Sensing (CS), and it is advantageous in terms of reducing the\namount of storage, easing hardware implementations and preserving\nmultidimensional structures of signals in comparison to a conventional CS\nsystem. In a TCS system, instead of using a random sensing matrix and a\npredefined dictionary, the average-case performance can be further improved by\nemploying an optimized multidimensional sensing matrix and a learned\nmultilinear sparsifying dictionary. In this paper, we propose a joint\noptimization approach of the sensing matrix and dictionary for a TCS system.\nFor the sensing matrix design in TCS, an extended separable approach with a\nclosed form solution and a novel iterative non-separable method are proposed\nwhen the multilinear dictionary is fixed. In addition, a multidimensional\ndictionary learning method that takes advantages of the multidimensional\nstructure is derived, and the influence of sensing matrices is taken into\naccount in the learning process. A joint optimization is achieved via\nalternately iterating the optimization of the sensing matrix and dictionary.\nNumerical experiments using both synthetic data and real images demonstrate the\nsuperiority of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 15:35:34 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Ding", "Xin", ""], ["Chen", "Wei", ""], ["Wassell", "Ian J.", ""]]}, {"id": "1601.07913", "submitter": "Daniel Whiteson", "authors": "Pierre Baldi, Kyle Cranmer, Taylor Faucett, Peter Sadowski, Daniel\n  Whiteson", "title": "Parameterized Machine Learning for High-Energy Physics", "comments": "For submission to PRD", "journal-ref": null, "doi": "10.1140/epjc/s10052-016-4099-4", "report-no": null, "categories": "hep-ex cs.LG hep-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a new structure for machine learning classifiers applied to\nproblems in high-energy physics by expanding the inputs to include not only\nmeasured features but also physics parameters. The physics parameters represent\na smoothly varying learning task, and the resulting parameterized classifier\ncan smoothly interpolate between them and replace sets of classifiers trained\nat individual values. This simplifies the training process and gives improved\nperformance at intermediate values, even for complex problems requiring deep\nlearning. Applications include tools parameterized in terms of theoretical\nmodel parameters, such as the mass of a particle, which allow for a single\nnetwork to provide improved discrimination across a range of masses. This\nconcept is simple to implement and allows for optimized interpolatable results.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 21:01:05 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Baldi", "Pierre", ""], ["Cranmer", "Kyle", ""], ["Faucett", "Taylor", ""], ["Sadowski", "Peter", ""], ["Whiteson", "Daniel", ""]]}, {"id": "1601.07925", "submitter": "Randal Olson", "authors": "Randal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A.\n  Lavender, La Creis Kidd, Jason H. Moore", "title": "Automating biomedical data science through tree-based pipeline\n  optimization", "comments": "16 pages, 5 figures, to appear in EvoBIO 2016 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, data science and machine learning has grown from a\nmysterious art form to a staple tool across a variety of fields in academia,\nbusiness, and government. In this paper, we introduce the concept of tree-based\npipeline optimization for automating one of the most tedious parts of machine\nlearning---pipeline design. We implement a Tree-based Pipeline Optimization\nTool (TPOT) and demonstrate its effectiveness on a series of simulated and\nreal-world genetic data sets. In particular, we show that TPOT can build\nmachine learning pipelines that achieve competitive classification accuracy and\ndiscover novel pipeline operators---such as synthetic feature\nconstructors---that significantly improve classification accuracy on these data\nsets. We also highlight the current challenges to pipeline optimization, such\nas the tendency to produce pipelines that overfit the data, and suggest future\nresearch paths to overcome these challenges. As such, this work represents an\nearly step toward fully automating machine learning pipeline design.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 21:45:55 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Olson", "Randal S.", ""], ["Urbanowicz", "Ryan J.", ""], ["Andrews", "Peter C.", ""], ["Lavender", "Nicole A.", ""], ["Kidd", "La Creis", ""], ["Moore", "Jason H.", ""]]}, {"id": "1601.07932", "submitter": "Keehwan Park", "authors": "Keehwan Park and Jean Honorio", "title": "Information-Theoretic Lower Bounds for Recovery of Diffusion Network\n  Structures", "comments": "ISIT'16", "journal-ref": "International Symposium on Information Theory (ISIT) 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the information-theoretic lower bound of the sample complexity of\nthe correct recovery of diffusion network structures. We introduce a\ndiscrete-time diffusion model based on the Independent Cascade model for which\nwe obtain a lower bound of order $\\Omega(k \\log p)$, for directed graphs of $p$\nnodes, and at most $k$ parents per node. Next, we introduce a continuous-time\ndiffusion model, for which a similar lower bound of order $\\Omega(k \\log p)$ is\nobtained. Our results show that the algorithm of Pouget-Abadie et al. is\nstatistically optimal for the discrete-time regime. Our work also opens the\nquestion of whether it is possible to devise an optimal algorithm for the\ncontinuous-time regime.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 22:12:06 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 23:29:19 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Park", "Keehwan", ""], ["Honorio", "Jean", ""]]}, {"id": "1601.07947", "submitter": "Dimitrios Berberidis", "authors": "Fatemeh Sheikholeslami, Dimitris Berberidis, Georgios B.Giannakis", "title": "Large-scale Kernel-based Feature Extraction via Budgeted Nonlinear\n  Subspace Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based methods enjoy powerful generalization capabilities in handling a\nvariety of learning tasks. When such methods are provided with sufficient\ntraining data, broadly-applicable classes of nonlinear functions can be\napproximated with desired accuracy. Nevertheless, inherent to the nonparametric\nnature of kernel-based estimators are computational and memory requirements\nthat become prohibitive with large-scale datasets. In response to this\nformidable challenge, the present work puts forward a low-rank, kernel-based,\nfeature extraction approach that is particularly tailored for online operation,\nwhere data streams need not be stored in memory. A novel generative model is\nintroduced to approximate high-dimensional (possibly infinite) features via a\nlow-rank nonlinear subspace, the learning of which leads to a direct kernel\nfunction approximation. Offline and online solvers are developed for the\nsubspace learning task, along with affordable versions, in which the number of\nstored data vectors is confined to a predefined budget. Analytical results\nprovide performance bounds on how well the kernel matrix as well as\nkernel-based classification and regression tasks can be approximated by\nleveraging budgeted online subspace learning and feature extraction schemes.\nTests on synthetic and real datasets demonstrate and benchmark the efficiency\nof the proposed method when linear classification and regression is applied to\nthe extracted features.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 23:38:44 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 21:45:26 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Sheikholeslami", "Fatemeh", ""], ["Berberidis", "Dimitris", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1601.07996", "submitter": "Jundong Li", "authors": "Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert P.\n  Trevino, Jiliang Tang, Huan Liu", "title": "Feature Selection: A Data Perspective", "comments": null, "journal-ref": "ACM Computing Surveys (CSUR), 50(6): 94:1-94:45, 2017", "doi": "10.1145/3136625", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection, as a data preprocessing strategy, has been proven to be\neffective and efficient in preparing data (especially high-dimensional data)\nfor various data mining and machine learning problems. The objectives of\nfeature selection include: building simpler and more comprehensible models,\nimproving data mining performance, and preparing clean, understandable data.\nThe recent proliferation of big data has presented some substantial challenges\nand opportunities to feature selection. In this survey, we provide a\ncomprehensive and structured overview of recent advances in feature selection\nresearch. Motivated by current challenges and opportunities in the era of big\ndata, we revisit feature selection research from a data perspective and review\nrepresentative feature selection algorithms for conventional data, structured\ndata, heterogeneous data and streaming data. Methodologically, to emphasize the\ndifferences and similarities of most existing feature selection algorithms for\nconventional data, we categorize them into four main groups: similarity based,\ninformation theoretical based, sparse learning based and statistical based\nmethods. To facilitate and promote the research in this community, we also\npresent an open-source feature selection repository that consists of most of\nthe popular feature selection algorithms\n(\\url{http://featureselection.asu.edu/}). Also, we use it as an example to show\nhow to evaluate feature selection algorithms. At the end of the survey, we\npresent a discussion about some open problems and challenges that require more\nattention in future research.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 08:32:10 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2016 00:29:42 GMT"}, {"version": "v3", "created": "Fri, 4 Mar 2016 22:24:56 GMT"}, {"version": "v4", "created": "Mon, 26 Sep 2016 10:32:12 GMT"}, {"version": "v5", "created": "Sun, 26 Aug 2018 20:43:57 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Li", "Jundong", ""], ["Cheng", "Kewei", ""], ["Wang", "Suhang", ""], ["Morstatter", "Fred", ""], ["Trevino", "Robert P.", ""], ["Tang", "Jiliang", ""], ["Liu", "Huan", ""]]}, {"id": "1601.08068", "submitter": "Hildo Bijl", "authors": "Hildo Bijl, Thomas B. Sch\\\"on, Jan-Willem van Wingerden, Michel\n  Verhaegen", "title": "System Identification through Online Sparse Gaussian Process Regression\n  with Input Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a growing interest in using non-parametric regression methods\nlike Gaussian Process (GP) regression for system identification. GP regression\ndoes traditionally have three important downsides: (1) it is computationally\nintensive, (2) it cannot efficiently implement newly obtained measurements\nonline, and (3) it cannot deal with stochastic (noisy) input points. In this\npaper we present an algorithm tackling all these three issues simultaneously.\nThe resulting Sparse Online Noisy Input GP (SONIG) regression algorithm can\nincorporate new noisy measurements in constant runtime. A comparison has shown\nthat it is more accurate than similar existing regression algorithms. When\napplied to non-linear black-box system modeling, its performance is competitive\nwith existing non-linear ARX models.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 11:55:26 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 18:09:28 GMT"}, {"version": "v3", "created": "Tue, 15 Aug 2017 22:08:11 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Bijl", "Hildo", ""], ["Sch\u00f6n", "Thomas B.", ""], ["van Wingerden", "Jan-Willem", ""], ["Verhaegen", "Michel", ""]]}, {"id": "1601.08169", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J Kir\\'aly, Harald Oberhauser", "title": "Kernels for sequentially ordered data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DM cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework for kernel learning with sequential data of any\nkind, such as time series, sequences of graphs, or strings. Our approach is\nbased on signature features which can be seen as an ordered variant of sample\n(cross-)moments; it allows to obtain a \"sequentialized\" version of any static\nkernel. The sequential kernels are efficiently computable for discrete\nsequences and are shown to approximate a continuous moment form in a sampling\nsense.\n  A number of known kernels for sequences arise as \"sequentializations\" of\nsuitable static kernels: string kernels may be obtained as a special case, and\nalignment kernels are closely related up to a modification that resolves their\nopen non-definiteness issue. Our experiments indicate that our signature-based\nsequential kernel framework may be a promising approach to learning with\nsequential data, such as time series, that allows to avoid extensive manual\npre-processing.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 16:06:36 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Kir\u00e1ly", "Franz J", ""], ["Oberhauser", "Harald", ""]]}]