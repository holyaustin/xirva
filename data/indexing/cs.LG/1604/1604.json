[{"id": "1604.00092", "submitter": "Paul Vernaza", "authors": "Paul Vernaza", "title": "Variational reaction-diffusion systems for semantic segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel global energy model for multi-class semantic image segmentation is\nproposed that admits very efficient exact inference and derivative calculations\nfor learning. Inference in this model is equivalent to MAP inference in a\nparticular kind of vector-valued Gaussian Markov random field, and ultimately\nreduces to solving a linear system of linear PDEs known as a reaction-diffusion\nsystem. Solving this system can be achieved in time scaling near-linearly in\nthe number of image pixels by reducing it to sequential FFTs, after a linear\nchange of basis. The efficiency and differentiability of the model make it\nespecially well-suited for integration with convolutional neural networks, even\nallowing it to be used in interior, feature-generating layers and stacked\nmultiple times. Experimental results are shown demonstrating that the model can\nbe employed profitably in conjunction with different convolutional net\narchitectures, and that doing so compares favorably to joint training of a\nfully-connected CRF with a convolutional net.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 01:04:31 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Vernaza", "Paul", ""]]}, {"id": "1604.00119", "submitter": "Krish Perumal", "authors": "Krish Perumal", "title": "Semi-supervised and Unsupervised Methods for Categorizing Posts in Web\n  Discussion Forums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web discussion forums are used by millions of people worldwide to share\ninformation belonging to a variety of domains such as automotive vehicles,\npets, sports, etc. They typically contain posts that fall into different\ncategories such as problem, solution, feedback, spam, etc. Automatic\nidentification of these categories can aid information retrieval that is\ntailored for specific user requirements. Previously, a number of supervised\nmethods have attempted to solve this problem; however, these depend on the\navailability of abundant training data. A few existing unsupervised and\nsemi-supervised approaches are either focused on identifying a single category\nor do not report category-specific performance. In contrast, this work proposes\nunsupervised and semi-supervised methods that require no or minimal training\ndata to achieve this objective without compromising on performance. A\nfine-grained analysis is also carried out to discuss their limitations. The\nproposed methods are based on sequence models (specifically, Hidden Markov\nModels) that can model language for each category using word and part-of-speech\nprobability distributions, and manually specified features. Empirical\nevaluations across domains demonstrate that the proposed methods are better\nsuited for this task than existing ones.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 03:32:03 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 16:06:13 GMT"}, {"version": "v3", "created": "Sun, 24 Apr 2016 21:27:17 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Perumal", "Krish", ""]]}, {"id": "1604.00126", "submitter": "Ardavan Saeedi", "authors": "Kayhan Batmanghelich, Ardavan Saeedi, Karthik Narasimhan, Sam Gershman", "title": "Nonparametric Spherical Topic Modeling with Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional topic models do not account for semantic regularities in\nlanguage. Recent distributional representations of words exhibit semantic\nconsistency over directional metrics such as cosine similarity. However,\nneither categorical nor Gaussian observational distributions used in existing\ntopic models are appropriate to leverage such correlations. In this paper, we\npropose to use the von Mises-Fisher distribution to model the density of words\nover a unit sphere. Such a representation is well-suited for directional data.\nWe use a Hierarchical Dirichlet Process for our base topic model and propose an\nefficient inference algorithm based on Stochastic Variational Inference. This\nmodel enables us to naturally exploit the semantic structures of word\nembeddings while flexibly discovering the number of topics. Experiments\ndemonstrate that our method outperforms competitive approaches in terms of\ntopic coherence on two different text corpora while offering efficient\ninference.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 04:36:58 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Batmanghelich", "Kayhan", ""], ["Saeedi", "Ardavan", ""], ["Narasimhan", "Karthik", ""], ["Gershman", "Sam", ""]]}, {"id": "1604.00279", "submitter": "Moritz August", "authors": "Moritz August, Xiaotong Ni", "title": "Using Recurrent Neural Networks to Optimize Dynamical Decoupling for\n  Quantum Memory", "comments": "18 pages, comments are welcome", "journal-ref": "Phys. Rev. A 95, 012335 (2017)", "doi": "10.1103/PhysRevA.95.012335", "report-no": null, "categories": "quant-ph cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We utilize machine learning models which are based on recurrent neural\nnetworks to optimize dynamical decoupling (DD) sequences. DD is a relatively\nsimple technique for suppressing the errors in quantum memory for certain noise\nmodels. In numerical simulations, we show that with minimum use of prior\nknowledge and starting from random sequences, the models are able to improve\nover time and eventually output DD-sequences with performance better than that\nof the well known DD-families. Furthermore, our algorithm is easy to implement\nin experiments to find solutions tailored to the specific hardware, as it\ntreats the figure of merit as a black box.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 15:24:27 GMT"}, {"version": "v2", "created": "Sat, 17 Sep 2016 14:44:36 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["August", "Moritz", ""], ["Ni", "Xiaotong", ""]]}, {"id": "1604.00289", "submitter": "Brenden Lake", "authors": "Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, Samuel J.\n  Gershman", "title": "Building Machines That Learn and Think Like People", "comments": "In press at Behavioral and Brain Sciences. Open call for commentary\n  proposals (until Nov. 22, 2016).\n  https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/information/calls-for-commentary/open-calls-for-commentary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in artificial intelligence (AI) has renewed interest in\nbuilding systems that learn and think like people. Many advances have come from\nusing deep neural networks trained end-to-end in tasks such as object\nrecognition, video games, and board games, achieving performance that equals or\neven beats humans in some respects. Despite their biological inspiration and\nperformance achievements, these systems differ from human intelligence in\ncrucial ways. We review progress in cognitive science suggesting that truly\nhuman-like learning and thinking machines will have to reach beyond current\nengineering trends in both what they learn, and how they learn it.\nSpecifically, we argue that these machines should (a) build causal models of\nthe world that support explanation and understanding, rather than merely\nsolving pattern recognition problems; (b) ground learning in intuitive theories\nof physics and psychology, to support and enrich the knowledge that is learned;\nand (c) harness compositionality and learning-to-learn to rapidly acquire and\ngeneralize knowledge to new tasks and situations. We suggest concrete\nchallenges and promising routes towards these goals that can combine the\nstrengths of recent neural network advances with more structured cognitive\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 15:37:57 GMT"}, {"version": "v2", "created": "Sat, 7 May 2016 18:03:53 GMT"}, {"version": "v3", "created": "Wed, 2 Nov 2016 17:26:50 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Lake", "Brenden M.", ""], ["Ullman", "Tomer D.", ""], ["Tenenbaum", "Joshua B.", ""], ["Gershman", "Samuel J.", ""]]}, {"id": "1604.00317", "submitter": "Ehud Ben-Reuven", "authors": "Ehud Ben-Reuven and Jacob Goldberger", "title": "A Semisupervised Approach for Language Identification based on Ladder\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we address the problem of training a neuralnetwork for language\nidentification using both labeled and unlabeled speech samples in the form of\ni-vectors. We propose a neural network architecture that can also handle\nout-of-set languages. We utilize a modified version of the recently proposed\nLadder Network semisupervised training procedure that optimizes the\nreconstruction costs of a stack of denoising autoencoders. We show that this\napproach can be successfully applied to the case where the training dataset is\ncomposed of both labeled and unlabeled acoustic data. The results show enhanced\nlanguage identification on the NIST 2015 language identification dataset.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 16:26:57 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Ben-Reuven", "Ehud", ""], ["Goldberger", "Jacob", ""]]}, {"id": "1604.00461", "submitter": "Mo Yu", "authors": "Mo Yu, Mark Dredze, Raman Arora, Matthew Gormley", "title": "Embedding Lexical Features via Low-Rank Tensors", "comments": "Accepted by NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern NLP models rely heavily on engineered features, which often combine\nword and contextual information into complex lexical features. Such combination\nresults in large numbers of features, which can lead to over-fitting. We\npresent a new model that represents complex lexical features---comprised of\nparts for words, contextual information and labels---in a tensor that captures\nconjunction information among these parts. We apply low-rank tensor\napproximations to the corresponding parameter tensors to reduce the parameter\nspace and improve prediction speed. Furthermore, we investigate two methods for\nhandling features that include $n$-grams of mixed lengths. Our model achieves\nstate-of-the-art results on tasks in relation extraction, PP-attachment, and\npreposition disambiguation.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 04:59:21 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Yu", "Mo", ""], ["Dredze", "Mark", ""], ["Arora", "Raman", ""], ["Gormley", "Matthew", ""]]}, {"id": "1604.00557", "submitter": "Mukhtiar Ali Unar", "authors": "Muhammad Saleh Shah, Asim Imdad Wagan, Mukhtiar Ali Unar", "title": "SAM: Support Vector Machine Based Active Queue Management", "comments": "8 pages, Mehran University Research Journal of Engineering and\n  Technology, Vol 33, No.1, January 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have seen an increasing interest in the design of AQM (Active\nQueue Management) controllers. The purpose of these controllers is to manage\nthe network congestion under varying loads, link delays and bandwidth. In this\npaper, a new AQM controller is proposed which is trained by using the SVM\n(Support Vector Machine) with the RBF (Radial Basis Function) kernal. The\nproposed controller is called the support vector based AQM (SAM) controller.\nThe performance of the proposed controller has been compared with three\nconventional AQM controllers, namely the Random Early Detection, Blue and\nProportional Plus Integral Controller. The preliminary simulation studies show\nthat the performance of the proposed controller is comparable to the\nconventional controllers. However, the proposed controller is more efficient in\ncontrolling the queue size than the conventional controllers.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2016 20:47:51 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Shah", "Muhammad Saleh", ""], ["Wagan", "Asim Imdad", ""], ["Unar", "Mukhtiar Ali", ""]]}, {"id": "1604.00647", "submitter": "Ernesto Diaz-Aviles", "authors": "Lucas Drumond, Ernesto Diaz-Aviles, and Lars Schmidt-Thieme", "title": "Multi-Relational Learning at Scale with ADMM", "comments": "Keywords: Multi-Relational Learning, Distributed Learning,\n  Factorization Models, ADMM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from multiple-relational data which contains noise, ambiguities, or\nduplicate entities is essential to a wide range of applications such as\nstatistical inference based on Web Linked Data, recommender systems,\ncomputational biology, and natural language processing. These tasks usually\nrequire working with very large and complex datasets - e.g., the Web graph -\nhowever, current approaches to multi-relational learning are not practical for\nsuch scenarios due to their high computational complexity and poor scalability\non large data.\n  In this paper, we propose a novel and scalable approach for multi-relational\nfactorization based on consensus optimization. Our model, called ConsMRF, is\nbased on the Alternating Direction Method of Multipliers (ADMM) framework,\nwhich enables us to optimize each target relation using a smaller set of\nparameters than the state-of-the-art competitors in this task.\n  Due to ADMM's nature, ConsMRF can be easily parallelized which makes it\nsuitable for large multi-relational data. Experiments on large Web datasets -\nderived from DBpedia, Wikipedia and YAGO - show the efficiency and performance\nimprovement of ConsMRF over strong competitors. In addition, ConsMRF\nnear-linear scalability indicates great potential to tackle Web-scale problem\nsizes.\n", "versions": [{"version": "v1", "created": "Sun, 3 Apr 2016 15:42:36 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Drumond", "Lucas", ""], ["Diaz-Aviles", "Ernesto", ""], ["Schmidt-Thieme", "Lars", ""]]}, {"id": "1604.00653", "submitter": "Weiwei Pan", "authors": "W. Pan, F. Doshi-Velez", "title": "A Characterization of the Non-Uniqueness of Nonnegative Matrix\n  Factorizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) is a popular dimension reduction\ntechnique that produces interpretable decomposition of the data into parts.\nHowever, this decompostion is not generally identifiable (even up to\npermutation and scaling). While other studies have provide criteria under which\nNMF is identifiable, we present the first (to our knowledge) characterization\nof the non-identifiability of NMF. We describe exactly when and how\nnon-uniqueness can occur, which has important implications for algorithms to\nefficiently discover alternate solutions, if they exist.\n", "versions": [{"version": "v1", "created": "Sun, 3 Apr 2016 16:32:01 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Pan", "W.", ""], ["Doshi-Velez", "F.", ""]]}, {"id": "1604.00727", "submitter": "David Golub", "authors": "David Golub, Xiaodong He", "title": "Character-Level Question Answering with Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a character-level encoder-decoder framework can be successfully\napplied to question answering with a structured knowledge base. We use our\nmodel for single-relation question answering and demonstrate the effectiveness\nof our approach on the SimpleQuestions dataset (Bordes et al., 2015), where we\nimprove state-of-the-art accuracy from 63.9% to 70.9%, without use of\nensembles. Importantly, our character-level model has 16x fewer parameters than\nan equivalent word-level model, can be learned with significantly less data\ncompared to previous work, which relies on data augmentation, and is robust to\nnew entities in testing.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 02:43:23 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 23:09:31 GMT"}, {"version": "v3", "created": "Fri, 8 Apr 2016 21:12:47 GMT"}, {"version": "v4", "created": "Sun, 5 Jun 2016 02:02:10 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Golub", "David", ""], ["He", "Xiaodong", ""]]}, {"id": "1604.00772", "submitter": "Nikolaus Hansen", "authors": "Nikolaus Hansen (Inria)", "title": "The CMA Evolution Strategy: A Tutorial", "comments": "ArXiv e-prints, arXiv:1604.xxxxx", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial introduces the CMA Evolution Strategy (ES), where CMA stands\nfor Covariance Matrix Adaptation. The CMA-ES is a stochastic, or randomized,\nmethod for real-parameter (continuous domain) optimization of non-linear,\nnon-convex functions. We try to motivate and derive the algorithm from\nintuitive concepts and from requirements of non-linear, non-convex search in\ncontinuous domain.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 08:16:12 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Hansen", "Nikolaus", "", "Inria"]]}, {"id": "1604.00783", "submitter": "Divya Padmanabhan", "authors": "Divya Padmanabhan, Satyanath Bhat, Shirish Shevade, Y. Narahari", "title": "Topic Model Based Multi-Label Classification from the Crowd", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification is a common supervised machine learning problem\nwhere each instance is associated with multiple classes. The key challenge in\nthis problem is learning the correlations between the classes. An additional\nchallenge arises when the labels of the training instances are provided by\nnoisy, heterogeneous crowdworkers with unknown qualities. We first assume\nlabels from a perfect source and propose a novel topic model where the present\nas well as the absent classes generate the latent topics and hence the words.\nWe non-trivially extend our topic model to the scenario where the labels are\nprovided by noisy crowdworkers. Extensive experimentation on real world\ndatasets reveals the superior performance of the proposed model. The proposed\nmodel learns the qualities of the annotators as well, even with minimal\ntraining data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 09:24:12 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Padmanabhan", "Divya", ""], ["Bhat", "Satyanath", ""], ["Shevade", "Shirish", ""], ["Narahari", "Y.", ""]]}, {"id": "1604.00788", "submitter": "Minh-Thang Luong", "authors": "Minh-Thang Luong, Christopher D. Manning", "title": "Achieving Open Vocabulary Neural Machine Translation with Hybrid\n  Word-Character Models", "comments": "11pages, 4 figures. ACL 2016 camera-ready version. SOTA WMT'15\n  English-Czech 20.7 BLEU (+2.1-11.4 points)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearly all previous work on neural machine translation (NMT) has used quite\nrestricted vocabularies, perhaps with a subsequent method to patch in unknown\nwords. This paper presents a novel word-character solution to achieving open\nvocabulary NMT. We build hybrid systems that translate mostly at the word level\nand consult the character components for rare words. Our character-level\nrecurrent neural networks compute source word representations and recover\nunknown target words when needed. The twofold advantage of such a hybrid\napproach is that it is much faster and easier to train than character-based\nones; at the same time, it never produces unknown words as in the case of\nword-based models. On the WMT'15 English to Czech translation task, this hybrid\napproach offers an addition boost of +2.1-11.4 BLEU points over models that\nalready handle unknown words. Our best system achieves a new state-of-the-art\nresult with 20.7 BLEU score. We demonstrate that our character models can\nsuccessfully learn to not only generate well-formed words for Czech, a\nhighly-inflected language with a very complex vocabulary, but also build\ncorrect representations for English source words.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 09:30:54 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 00:50:19 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Luong", "Minh-Thang", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1604.00861", "submitter": "Heikki Huttunen", "authors": "Giambattista Parascandolo, Heikki Huttunen, Tuomas Virtanen", "title": "Recurrent Neural Networks for Polyphonic Sound Event Detection in Real\n  Life Recordings", "comments": "To appean in Proceedings of IEEE ICASSP 2016", "journal-ref": null, "doi": "10.1109/ICASSP.2016.7472917", "report-no": null, "categories": "cs.SD cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an approach to polyphonic sound event detection in\nreal life recordings based on bi-directional long short term memory (BLSTM)\nrecurrent neural networks (RNNs). A single multilabel BLSTM RNN is trained to\nmap acoustic features of a mixture signal consisting of sounds from multiple\nclasses, to binary activity indicators of each event class. Our method is\ntested on a large database of real-life recordings, with 61 classes (e.g.\nmusic, car, speech) from 10 different everyday contexts. The proposed method\noutperforms previous approaches by a large margin, and the results are further\nimproved using data augmentation techniques. Overall, our system reports an\naverage F1-score of 65.5% on 1 second blocks and 64.7% on single frames, a\nrelative improvement over previous state-of-the-art approach of 6.8% and 15.1%\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 13:54:09 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Parascandolo", "Giambattista", ""], ["Huttunen", "Heikki", ""], ["Virtanen", "Tuomas", ""]]}, {"id": "1604.00923", "submitter": "Philip Thomas", "authors": "Philip S. Thomas and Emma Brunskill", "title": "Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new way of predicting the performance of a\nreinforcement learning policy given historical data that may have been\ngenerated by a different policy. The ability to evaluate a policy from\nhistorical data is important for applications where the deployment of a bad\npolicy can be dangerous or costly. We show empirically that our algorithm\nproduces estimates that often have orders of magnitude lower mean squared error\nthan existing methods---it makes more efficient use of the available data. Our\nnew estimator is based on two advances: an extension of the doubly robust\nestimator (Jiang and Li, 2015), and a new way to mix between model based\nestimates and importance sampling based estimates.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 15:56:52 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Thomas", "Philip S.", ""], ["Brunskill", "Emma", ""]]}, {"id": "1604.00981", "submitter": "Jianmin Chen", "authors": "Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio and Rafal\n  Jozefowicz", "title": "Revisiting Distributed Synchronous SGD", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed training of deep learning models on large-scale training data is\ntypically conducted with asynchronous stochastic optimization to maximize the\nrate of updates, at the cost of additional noise introduced from asynchrony. In\ncontrast, the synchronous approach is often thought to be impractical due to\nidle time wasted on waiting for straggling workers. We revisit these\nconventional beliefs in this paper, and examine the weaknesses of both\napproaches. We demonstrate that a third approach, synchronous optimization with\nbackup workers, can avoid asynchronous noise while mitigating for the worst\nstragglers. Our approach is empirically validated and shown to converge faster\nand to better test accuracies.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 18:40:05 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2016 18:49:12 GMT"}, {"version": "v3", "created": "Tue, 21 Mar 2017 07:44:39 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Chen", "Jianmin", ""], ["Pan", "Xinghao", ""], ["Monga", "Rajat", ""], ["Bengio", "Samy", ""], ["Jozefowicz", "Rafal", ""]]}, {"id": "1604.01170", "submitter": "Antonia Godoy-Lorite", "authors": "Antonia Godoy-Lorite, Roger Guimera, Cristopher Moore, Marta\n  Sales-Pardo", "title": "Accurate and scalable social recommendation using mixed-membership\n  stochastic block models", "comments": "9 pages, 4 figures", "journal-ref": "Proc. Natl. Acad. Sci. USA 113 (50) , 14207 -14212 (2016)", "doi": "10.1073/pnas.1606316113", "report-no": null, "categories": "cs.SI cs.IR cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With ever-increasing amounts of online information available, modeling and\npredicting individual preferences-for books or articles, for example-is\nbecoming more and more important. Good predictions enable us to improve advice\nto users, and obtain a better understanding of the socio-psychological\nprocesses that determine those preferences. We have developed a collaborative\nfiltering model, with an associated scalable algorithm, that makes accurate\npredictions of individuals' preferences. Our approach is based on the explicit\nassumption that there are groups of individuals and of items, and that the\npreferences of an individual for an item are determined only by their group\nmemberships. Importantly, we allow each individual and each item to belong\nsimultaneously to mixtures of different groups and, unlike many popular\napproaches, such as matrix factorization, we do not assume implicitly or\nexplicitly that individuals in each group prefer items in a single group of\nitems. The resulting overlapping groups and the predicted preferences can be\ninferred with a expectation-maximization algorithm whose running time scales\nlinearly (per iteration). Our approach enables us to predict individual\npreferences in large datasets, and is considerably more accurate than the\ncurrent algorithms for such large datasets.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 08:28:08 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 07:55:35 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Godoy-Lorite", "Antonia", ""], ["Guimera", "Roger", ""], ["Moore", "Cristopher", ""], ["Sales-Pardo", "Marta", ""]]}, {"id": "1604.01272", "submitter": "Despoina Christou", "authors": "Despoina Christou", "title": "Feature extraction using Latent Dirichlet Allocation and Neural\n  Networks: A case study on movie synopses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction has gained increasing attention in the field of machine\nlearning, as in order to detect patterns, extract information, or predict\nfuture observations from big data, the urge of informative features is crucial.\nThe process of extracting features is highly linked to dimensionality reduction\nas it implies the transformation of the data from a sparse high-dimensional\nspace, to higher level meaningful abstractions. This dissertation employs\nNeural Networks for distributed paragraph representations, and Latent Dirichlet\nAllocation to capture higher level features of paragraph vectors. Although\nNeural Networks for distributed paragraph representations are considered the\nstate of the art for extracting paragraph vectors, we show that a quick topic\nanalysis model such as Latent Dirichlet Allocation can provide meaningful\nfeatures too. We evaluate the two methods on the CMU Movie Summary Corpus, a\ncollection of 25,203 movie plot summaries extracted from Wikipedia. Finally,\nfor both approaches, we use K-Nearest Neighbors to discover similar movies, and\nplot the projected representations using T-Distributed Stochastic Neighbor\nEmbedding to depict the context similarities. These similarities, expressed as\nmovie distances, can be used for movies recommendation. The recommended movies\nof this approach are compared with the recommended movies from IMDB, which use\na collaborative filtering recommendation approach, to show that our two models\ncould constitute either an alternative or a supplementary recommendation\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 14:32:48 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Christou", "Despoina", ""]]}, {"id": "1604.01304", "submitter": "Li  Li", "authors": "Li Li and Houfeng Wang", "title": "Towards Label Imbalance in Multi-label Classification with Many Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-label classification, an instance may be associated with a set of\nlabels simultaneously. Recently, the research on multi-label classification has\nlargely shifted its focus to the other end of the spectrum where the number of\nlabels is assumed to be extremely large. The existing works focus on how to\ndesign scalable algorithms that offer fast training procedures and have a small\nmemory footprint. However they ignore and even compound another challenge - the\nlabel imbalance problem. To address this drawback, we propose a novel\nRepresentation-based Multi-label Learning with Sampling (RMLS) approach. To the\nbest of our knowledge, we are the first to tackle the imbalance problem in\nmulti-label classification with many labels. Our experimentations with\nreal-world datasets demonstrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 15:44:33 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Li", "Li", ""], ["Wang", "Houfeng", ""]]}, {"id": "1604.01348", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi, Leslie Pack Kaelbling, Tom\\'as Lozano-P\\'erez", "title": "Bayesian Optimization with Exponential Convergence", "comments": "In NIPS 2015 (Advances in Neural Information Processing Systems 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Bayesian optimization method with exponential\nconvergence without the need of auxiliary optimization and without the\ndelta-cover sampling. Most Bayesian optimization methods require auxiliary\noptimization: an additional non-convex global optimization problem, which can\nbe time-consuming and hard to implement in practice. Also, the existing\nBayesian optimization method with exponential convergence requires access to\nthe delta-cover sampling, which was considered to be impractical. Our approach\neliminates both requirements and achieves an exponential convergence rate.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 17:53:59 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Kaelbling", "Leslie Pack", ""], ["Lozano-P\u00e9rez", "Tom\u00e1s", ""]]}, {"id": "1604.01350", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi", "title": "Bounded Optimal Exploration in MDP", "comments": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence\n  (AAAI), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the framework of probably approximately correct Markov decision\nprocesses (PAC-MDP), much theoretical work has focused on methods to attain\nnear optimality after a relatively long period of learning and exploration.\nHowever, practical concerns require the attainment of satisfactory behavior\nwithin a short period of time. In this paper, we relax the PAC-MDP conditions\nto reconcile theoretically driven exploration methods and practical needs. We\npropose simple algorithms for discrete and continuous state spaces, and\nillustrate the benefits of our proposed relaxation via theoretical analyses and\nnumerical examples. Our algorithms also maintain anytime error bounds and\naverage loss bounds. Our approach accommodates both Bayesian and non-Bayesian\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 18:00:02 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Kawaguchi", "Kenji", ""]]}, {"id": "1604.01357", "submitter": "Jelani Nelson", "authors": "Kasper Green Larsen, Jelani Nelson, Huy L. Nguyen, Mikkel Thorup", "title": "Heavy hitters via cluster-preserving clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In turnstile $\\ell_p$ $\\varepsilon$-heavy hitters, one maintains a\nhigh-dimensional $x\\in\\mathbb{R}^n$ subject to $\\texttt{update}(i,\\Delta)$\ncausing $x_i\\leftarrow x_i + \\Delta$, where $i\\in[n]$, $\\Delta\\in\\mathbb{R}$.\nUpon receiving a query, the goal is to report a small list $L\\subset[n]$, $|L|\n= O(1/\\varepsilon^p)$, containing every \"heavy hitter\" $i\\in[n]$ with $|x_i|\n\\ge \\varepsilon \\|x_{\\overline{1/\\varepsilon^p}}\\|_p$, where $x_{\\overline{k}}$\ndenotes the vector obtained by zeroing out the largest $k$ entries of $x$ in\nmagnitude.\n  For any $p\\in(0,2]$ the CountSketch solves $\\ell_p$ heavy hitters using\n$O(\\varepsilon^{-p}\\log n)$ words of space with $O(\\log n)$ update time,\n$O(n\\log n)$ query time to output $L$, and whose output after any query is\ncorrect with high probability (whp) $1 - 1/poly(n)$. Unfortunately the query\ntime is very slow. To remedy this, the work [CM05] proposed for $p=1$ in the\nstrict turnstile model, a whp correct algorithm achieving suboptimal space\n$O(\\varepsilon^{-1}\\log^2 n)$, worse update time $O(\\log^2 n)$, but much better\nquery time $O(\\varepsilon^{-1}poly(\\log n))$.\n  We show this tradeoff between space and update time versus query time is\nunnecessary. We provide a new algorithm, ExpanderSketch, which in the most\ngeneral turnstile model achieves optimal $O(\\varepsilon^{-p}\\log n)$ space,\n$O(\\log n)$ update time, and fast $O(\\varepsilon^{-p}poly(\\log n))$ query time,\nand whp correctness. Our main innovation is an efficient reduction from the\nheavy hitters to a clustering problem in which each heavy hitter is encoded as\nsome form of noisy spectral cluster in a much bigger graph, and the goal is to\nidentify every cluster. Since every heavy hitter must be found, correctness\nrequires that every cluster be found. We then develop a \"cluster-preserving\nclustering\" algorithm, partitioning the graph into clusters without destroying\nany original cluster.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 18:33:08 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Larsen", "Kasper Green", ""], ["Nelson", "Jelani", ""], ["Nguyen", "Huy L.", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1604.01376", "submitter": "Valentina Zantedeschi", "authors": "Valentina Zantedeschi, R\\'emi Emonet, Marc Sebban", "title": "Lipschitz Continuity of Mahalanobis Distances and Bilinear Forms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many theoretical results in the machine learning domain stand only for\nfunctions that are Lipschitz continuous. Lipschitz continuity is a strong form\nof continuity that linearly bounds the variations of a function. In this paper,\nwe derive tight Lipschitz constants for two families of metrics: Mahalanobis\ndistances and bounded-space bilinear forms. To our knowledge, this is the first\ntime the Mahalanobis distance is formally proved to be Lipschitz continuous and\nthat such tight Lipschitz constants are derived.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 12:39:26 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Zantedeschi", "Valentina", ""], ["Emonet", "R\u00e9mi", ""], ["Sebban", "Marc", ""]]}, {"id": "1604.01474", "submitter": "Changsheng Li", "authors": "Changsheng Li, Junchi Yan, Fan Wei, Weishan Dong, Qingshan Liu,\n  Hongyuan Zha", "title": "Self-Paced Multi-Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel multi-task learning (MTL) framework, called\nSelf-Paced Multi-Task Learning (SPMTL). Different from previous works treating\nall tasks and instances equally when training, SPMTL attempts to jointly learn\nthe tasks by taking into consideration the complexities of both tasks and\ninstances. This is inspired by the cognitive process of human brain that often\nlearns from the easy to the hard. We construct a compact SPMTL formulation by\nproposing a new task-oriented regularizer that can jointly prioritize the tasks\nand the instances. Thus it can be interpreted as a self-paced learner for MTL.\nA simple yet effective algorithm is designed for optimizing the proposed\nobjective function. An error bound for a simplified formulation is also\nanalyzed theoretically. Experimental results on toy and real-world datasets\ndemonstrate the effectiveness of the proposed approach, compared to the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 03:44:03 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 02:28:32 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Li", "Changsheng", ""], ["Yan", "Junchi", ""], ["Wei", "Fan", ""], ["Dong", "Weishan", ""], ["Liu", "Qingshan", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1604.01475", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Yingzhen Yang, Shiyu Chang, Qing Ling, Thomas S. Huang", "title": "Learning A Deep $\\ell_\\infty$ Encoder for Hashing", "comments": "To be presented at IJCAI'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the $\\ell_\\infty$-constrained representation which\ndemonstrates robustness to quantization errors, utilizing the tool of deep\nlearning. Based on the Alternating Direction Method of Multipliers (ADMM), we\nformulate the original convex minimization problem as a feed-forward neural\nnetwork, named \\textit{Deep $\\ell_\\infty$ Encoder}, by introducing the novel\nBounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers as\nnetwork biases. Such a structural prior acts as an effective network\nregularization, and facilitates the model initialization. We then investigate\nthe effective use of the proposed model in the application of hashing, by\ncoupling the proposed encoders under a supervised pairwise loss, to develop a\n\\textit{Deep Siamese $\\ell_\\infty$ Network}, which can be optimized from end to\nend. Extensive experiments demonstrate the impressive performances of the\nproposed model. We also provide an in-depth analysis of its behaviors against\nthe competitors.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 03:54:33 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Wang", "Zhangyang", ""], ["Yang", "Yingzhen", ""], ["Chang", "Shiyu", ""], ["Ling", "Qing", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1604.01518", "submitter": "Xinxing Xu", "authors": "Xinxing Xu, Joey Tianyi Zhou, IvorW. Tsang, Zheng Qin, Rick Siow Mong\n  Goh and Yong Liu", "title": "Simple and Efficient Learning using Privileged Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Support Vector Machine using Privileged Information (SVM+) has been\nproposed to train a classifier to utilize the additional privileged information\nthat is only available in the training phase but not available in the test\nphase. In this work, we propose an efficient solution for SVM+ by simply\nutilizing the squared hinge loss instead of the hinge loss as in the existing\nSVM+ formulation, which interestingly leads to a dual form with less variables\nand in the same form with the dual of the standard SVM. The proposed algorithm\nis utilized to leverage the additional web knowledge that is only available\nduring training for the image categorization tasks. The extensive experimental\nresults on both Caltech101 andWebQueries datasets show that our proposed method\ncan achieve a factor of up to hundred times speedup with the comparable\naccuracy when compared with the existing SVM+ method.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 07:33:55 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Xu", "Xinxing", ""], ["Zhou", "Joey Tianyi", ""], ["Tsang", "IvorW.", ""], ["Qin", "Zheng", ""], ["Goh", "Rick Siow Mong", ""], ["Liu", "Yong", ""]]}, {"id": "1604.01662", "submitter": "Hao Wang", "authors": "Hao Wang and Dit-Yan Yeung", "title": "A Survey on Bayesian Deep Learning", "comments": "Published in ACM Computing Surveys (CSUR) 2020. Constantly updating\n  project page at https://github.com/js05212/BayesianDeepLearning-Survey", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive artificial intelligence system needs to not only perceive the\nenvironment with different `senses' (e.g., seeing and hearing) but also infer\nthe world's conditional (or even causal) relations and corresponding\nuncertainty. The past decade has seen major advances in many perception tasks\nsuch as visual object recognition and speech recognition using deep learning\nmodels. For higher-level inference, however, probabilistic graphical models\nwith their Bayesian nature are still more powerful and flexible. In recent\nyears, Bayesian deep learning has emerged as a unified probabilistic framework\nto tightly integrate deep learning and Bayesian models. In this general\nframework, the perception of text or images using deep learning can boost the\nperformance of higher-level inference and in turn, the feedback from the\ninference process is able to enhance the perception of text or images. This\nsurvey provides a comprehensive introduction to Bayesian deep learning and\nreviews its recent applications on recommender systems, topic models, control,\netc. Besides, we also discuss the relationship and differences between Bayesian\ndeep learning and other related topics such as Bayesian treatment of neural\nnetworks. For a constantly updating project page, please refer to\nhttps://github.com/js05212/BayesianDeepLearning-Survey.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 15:35:08 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 06:17:44 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 03:52:57 GMT"}, {"version": "v4", "created": "Wed, 6 Jan 2021 03:14:13 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Wang", "Hao", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1604.01686", "submitter": "Shehroz Khan", "authors": "Shehroz S. Khan, Amir Ahmad", "title": "Relationship between Variants of One-Class Nearest Neighbours and\n  Creating their Accurate Ensembles", "comments": "14 pages, 9 figures, 8 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In one-class classification problems, only the data for the target class is\navailable, whereas the data for the non-target class may be completely absent.\nIn this paper, we study one-class nearest neighbour (OCNN) classifiers and\ntheir different variants. We present a theoretical analysis to show the\nrelationships among different variants of OCNN that may use different\nneighbours or thresholds to identify unseen examples of the non-target class.\nWe also present a method based on inter-quartile range for optimising\nparameters used in OCNN in the absence of non-target data during training.\nThen, we propose two ensemble approaches based on random subspace and random\nprojection methods to create accurate OCNN ensembles. We tested the proposed\nmethods on 15 benchmark and real world domain-specific datasets and show that\nrandom-projection ensembles of OCNN perform best.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 16:36:41 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 18:00:42 GMT"}, {"version": "v3", "created": "Fri, 24 Feb 2017 21:30:42 GMT"}, {"version": "v4", "created": "Wed, 22 Mar 2017 20:56:34 GMT"}, {"version": "v5", "created": "Thu, 28 Dec 2017 08:32:46 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Khan", "Shehroz S.", ""], ["Ahmad", "Amir", ""]]}, {"id": "1604.01785", "submitter": "Peter Gr\\\"unwald", "authors": "Peter Gr\\\"unwald", "title": "Safe Probability", "comments": "Submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize the idea of probability distributions that lead to reliable\npredictions about some, but not all aspects of a domain. The resulting notion\nof `safety' provides a fresh perspective on foundational issues in statistics,\nproviding a middle ground between imprecise probability and multiple-prior\nmodels on the one hand and strictly Bayesian approaches on the other. It also\nallows us to formalize fiducial distributions in terms of the set of random\nvariables that they can safely predict, thus taking some of the sting out of\nthe fiducial idea. By restricting probabilistic inference to safe uses, one\nalso automatically avoids paradoxes such as the Monty Hall problem. Safety\ncomes in a variety of degrees, such as \"validity\" (the strongest notion),\n\"calibration\", \"confidence safety\" and \"unbiasedness\" (almost the weakest\nnotion).\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 20:01:28 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Gr\u00fcnwald", "Peter", ""]]}, {"id": "1604.01792", "submitter": "Tom Sercu", "authors": "Tom Sercu, Vaibhava Goel", "title": "Advances in Very Deep Convolutional Neural Networks for LVCSR", "comments": "Proc. Interspeech 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very deep CNNs with small 3x3 kernels have recently been shown to achieve\nvery strong performance as acoustic models in hybrid NN-HMM speech recognition\nsystems. In this paper we investigate how to efficiently scale these models to\nlarger datasets. Specifically, we address the design choice of pooling and\npadding along the time dimension which renders convolutional evaluation of\nsequences highly inefficient. We propose a new CNN design without timepadding\nand without timepooling, which is slightly suboptimal for accuracy, but has two\nsignificant advantages: it enables sequence training and deployment by allowing\nefficient convolutional evaluation of full utterances, and, it allows for batch\nnormalization to be straightforwardly adopted to CNNs on sequence data. Through\nbatch normalization, we recover the lost peformance from removing the\ntime-pooling, while keeping the benefit of efficient convolutional evaluation.\nWe demonstrate the performance of our models both on larger scale data than\nbefore, and after sequence training. Our very deep CNN model sequence trained\non the 2000h switchboard dataset obtains 9.4 word error rate on the Hub5\ntest-set, matching with a single model the performance of the 2015 IBM system\ncombination, which was the previous best published result.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 20:07:52 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2016 00:27:19 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Sercu", "Tom", ""], ["Goel", "Vaibhava", ""]]}, {"id": "1604.01802", "submitter": "David Held", "authors": "David Held, Sebastian Thrun, Silvio Savarese", "title": "Learning to Track at 100 FPS with Deep Regression Networks", "comments": "To appear in European Conference on Computer Vision (ECCV) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques are often used in computer vision due to their\nability to leverage large amounts of training data to improve performance.\nUnfortunately, most generic object trackers are still trained from scratch\nonline and do not benefit from the large number of videos that are readily\navailable for offline training. We propose a method for offline training of\nneural networks that can track novel objects at test-time at 100 fps. Our\ntracker is significantly faster than previous methods that use neural networks\nfor tracking, which are typically very slow to run and not practical for\nreal-time applications. Our tracker uses a simple feed-forward network with no\nonline training required. The tracker learns a generic relationship between\nobject motion and appearance and can be used to track novel objects that do not\nappear in the training set. We test our network on a standard tracking\nbenchmark to demonstrate our tracker's state-of-the-art performance. Further,\nour performance improves as we add more videos to our offline training set. To\nthe best of our knowledge, our tracker is the first neural-network tracker that\nlearns to track generic objects at 100 fps.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 20:39:34 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 02:34:48 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Held", "David", ""], ["Thrun", "Sebastian", ""], ["Savarese", "Silvio", ""]]}, {"id": "1604.01806", "submitter": "Srikanth Cherla", "authors": "Srikanth Cherla and Son N Tran and Tillman Weyde and Artur d'Avila\n  Garcez", "title": "Generalising the Discriminative Restricted Boltzmann Machine", "comments": "Submitted to ECML 2016 conference track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel theoretical result that generalises the Discriminative\nRestricted Boltzmann Machine (DRBM). While originally the DRBM was defined\nassuming the {0, 1}-Bernoulli distribution in each of its hidden units, this\nresult makes it possible to derive cost functions for variants of the DRBM that\nutilise other distributions, including some that are often encountered in the\nliterature. This is illustrated with the Binomial and {-1, +1}-Bernoulli\ndistributions here. We evaluate these two DRBM variants and compare them with\nthe original one on three benchmark datasets, namely the MNIST and USPS digit\nclassification datasets, and the 20 Newsgroups document classification dataset.\nResults show that each of the three compared models outperforms the remaining\ntwo in one of the three datasets, thus indicating that the proposed theoretical\ngeneralisation of the DRBM may be valuable in practice.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 21:01:35 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Cherla", "Srikanth", ""], ["Tran", "Son N", ""], ["Weyde", "Tillman", ""], ["Garcez", "Artur d'Avila", ""]]}, {"id": "1604.01828", "submitter": "Adithya M Devraj", "authors": "Adithya M. Devraj, Sean P. Meyn", "title": "Differential TD Learning for Value Function Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value functions arise as a component of algorithms as well as performance\nmetrics in statistics and engineering applications. Computation of the\nassociated Bellman equations is numerically challenging in all but a few\nspecial cases. A popular approximation technique is known as Temporal\nDifference (TD) learning. The algorithm introduced in this paper is intended to\nresolve two well-known problems with this approach: In the discounted-cost\nsetting, the variance of the algorithm diverges as the discount factor\napproaches unity. Second, for the average cost setting, unbiased algorithms\nexist only in special cases. It is shown that the gradient of any of these\nvalue functions admits a representation that lends itself to algorithm design.\nBased on this result, the new differential TD method is obtained for Markovian\nmodels on Euclidean space with smooth dynamics. Numerical examples show\nremarkable improvements in performance. In application to speed scaling,\nvariance is reduced by two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 23:09:08 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 23:43:37 GMT"}, {"version": "v3", "created": "Sun, 23 Dec 2018 13:52:32 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Devraj", "Adithya M.", ""], ["Meyn", "Sean P.", ""]]}, {"id": "1604.01839", "submitter": "Barna Saha", "authors": "Arya Mazumdar, Barna Saha", "title": "Clustering Via Crowdsourcing", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, crowdsourcing, aka human aided computation has emerged as an\neffective platform for solving problems that are considered complex for\nmachines alone. Using human is time-consuming and costly due to monetary\ncompensations. Therefore, a crowd based algorithm must judiciously use any\ninformation computed through an automated process, and ask minimum number of\nquestions to the crowd adaptively.\n  One such problem which has received significant attention is {\\em entity\nresolution}. Formally, we are given a graph $G=(V,E)$ with unknown edge set $E$\nwhere $G$ is a union of $k$ (again unknown, but typically large $O(n^\\alpha)$,\nfor $\\alpha>0$) disjoint cliques $G_i(V_i, E_i)$, $i =1, \\dots, k$. The goal is\nto retrieve the sets $V_i$s by making minimum number of pair-wise queries $V\n\\times V\\to\\{\\pm1\\}$ to an oracle (the crowd). When the answer to each query is\ncorrect, e.g. via resampling, then this reduces to finding connected components\nin a graph. On the other hand, when crowd answers may be incorrect, it\ncorresponds to clustering over minimum number of noisy inputs. Even, with\nperfect answers, a simple lower and upper bound of $\\Theta(nk)$ on query\ncomplexity can be shown. A major contribution of this paper is to reduce the\nquery complexity to linear or even sublinear in $n$ when mild side information\nis provided by a machine, and even in presence of crowd errors which are not\ncorrectable via resampling. We develop new information theoretic lower bounds\non the query complexity of clustering with side information and errors, and our\nupper bounds closely match with them. Our algorithms are naturally\nparallelizable, and also give near-optimal bounds on the number of adaptive\nrounds required to match the query complexity.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 00:58:23 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Mazumdar", "Arya", ""], ["Saha", "Barna", ""]]}, {"id": "1604.01854", "submitter": "Tim Leathart", "authors": "Tim Leathart, Bernhard Pfahringer and Eibe Frank", "title": "Building Ensembles of Adaptive Nested Dichotomies with Random-Pair\n  Selection", "comments": "ECMLPKDD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A system of nested dichotomies is a method of decomposing a multi-class\nproblem into a collection of binary problems. Such a system recursively splits\nthe set of classes into two subsets, and trains a binary classifier to\ndistinguish between each subset. Even though ensembles of nested dichotomies\nwith random structure have been shown to perform well in practice, using a more\nsophisticated class subset selection method can be used to improve\nclassification accuracy. We investigate an approach to this problem called\nrandom-pair selection, and evaluate its effectiveness compared to other\npublished methods of subset selection. We show that our method outperforms\nother methods in many cases when forming ensembles of nested dichotomies, and\nis at least on par in all other cases.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 02:56:19 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 04:25:25 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Leathart", "Tim", ""], ["Pfahringer", "Bernhard", ""], ["Frank", "Eibe", ""]]}, {"id": "1604.01870", "submitter": "Weiran Wang", "authors": "Weiran Wang, Jialei Wang, Dan Garber, Nathan Srebro", "title": "Efficient Globally Convergent Stochastic Optimization for Canonical\n  Correlation Analysis", "comments": "Accepted by NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stochastic optimization of canonical correlation analysis (CCA),\nwhose objective is nonconvex and does not decouple over training samples.\nAlthough several stochastic gradient based optimization algorithms have been\nrecently proposed to solve this problem, no global convergence guarantee was\nprovided by any of them. Inspired by the alternating least squares/power\niterations formulation of CCA, and the shift-and-invert preconditioning method\nfor PCA, we propose two globally convergent meta-algorithms for CCA, both of\nwhich transform the original problem into sequences of least squares problems\nthat need only be solved approximately. We instantiate the meta-algorithms with\nstate-of-the-art SGD methods and obtain time complexities that significantly\nimprove upon that of previous work. Experimental results demonstrate their\nsuperior performance.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 04:14:54 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2016 17:58:52 GMT"}, {"version": "v3", "created": "Fri, 20 May 2016 03:09:29 GMT"}, {"version": "v4", "created": "Mon, 14 Nov 2016 18:11:15 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Wang", "Weiran", ""], ["Wang", "Jialei", ""], ["Garber", "Dan", ""], ["Srebro", "Nathan", ""]]}, {"id": "1604.01871", "submitter": "Adam Smith", "authors": "Audra McMillan and Adam Smith", "title": "When is Nontrivial Estimation Possible for Graphons and Stochastic Block\n  Models?", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Block graphons (also called stochastic block models) are an important and\nwidely-studied class of models for random networks. We provide a lower bound on\nthe accuracy of estimators for block graphons with a large number of blocks. We\nshow that, given only the number $k$ of blocks and an upper bound $\\rho$ on the\nvalues (connection probabilities) of the graphon, every estimator incurs error\nat least on the order of $\\min(\\rho, \\sqrt{\\rho k^2/n^2})$ in the $\\delta_2$\nmetric with constant probability, in the worst case over graphons. In\nparticular, our bound rules out any nontrivial estimation (that is, with\n$\\delta_2$ error substantially less than $\\rho$) when $k\\geq n\\sqrt{\\rho}$.\nCombined with previous upper and lower bounds, our results characterize, up to\nlogarithmic terms, the minimax accuracy of graphon estimation in the $\\delta_2$\nmetric. A similar lower bound to ours was obtained independently by Klopp,\nTsybakov and Verzelen (2016).\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 04:19:11 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["McMillan", "Audra", ""], ["Smith", "Adam", ""]]}, {"id": "1604.01946", "submitter": "Phil Blunsom", "authors": "Jeremy Appleyard, Tomas Kocisky, Phil Blunsom", "title": "Optimizing Performance of Recurrent Neural Networks on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As recurrent neural networks become larger and deeper, training times for\nsingle networks are rising into weeks or even months. As such there is a\nsignificant incentive to improve the performance and scalability of these\nnetworks. While GPUs have become the hardware of choice for training and\ndeploying recurrent models, the implementations employed often make use of only\nbasic optimizations for these architectures. In this article we demonstrate\nthat by exposing parallelism between operations within the network, an order of\nmagnitude speedup across a range of network sizes can be achieved over a naive\nimplementation. We describe three stages of optimization that have been\nincorporated into the fifth release of NVIDIA's cuDNN: firstly optimizing a\nsingle cell, secondly a single layer, and thirdly the entire network.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 10:31:01 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Appleyard", "Jeremy", ""], ["Kocisky", "Tomas", ""], ["Blunsom", "Phil", ""]]}, {"id": "1604.01952", "submitter": "David Balduzzi", "authors": "David Balduzzi", "title": "Deep Online Convex Optimization with Gated Games", "comments": "13 pages. This paper renders arXiv:1509.01851 obsolete. It contains\n  the same basic results, with major changes to exposition and minor changes to\n  terminology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods from convex optimization are widely used as building blocks for deep\nlearning algorithms. However, the reasons for their empirical success are\nunclear, since modern convolutional networks (convnets), incorporating\nrectifier units and max-pooling, are neither smooth nor convex. Standard\nguarantees therefore do not apply. This paper provides the first convergence\nrates for gradient descent on rectifier convnets. The proof utilizes the\nparticular structure of rectifier networks which consists in binary\nactive/inactive gates applied on top of an underlying linear network. The\napproach generalizes to max-pooling, dropout and maxout. In other words, to\nprecisely the neural networks that perform best empirically. The key step is to\nintroduce gated games, an extension of convex games with similar convergence\nproperties that capture the gating function of rectifiers. The main result is\nthat rectifier convnets converge to a critical point at a rate controlled by\nthe gated-regret of the units in the network. Corollaries of the main result\ninclude: (i) a game-theoretic description of the representations learned by a\nneural network; (ii) a logarithmic-regret algorithm for training neural nets;\nand (iii) a formal setting for analyzing conditional computation in neural nets\nthat can be applied to recently developed models of attention.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 10:46:54 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Balduzzi", "David", ""]]}, {"id": "1604.01999", "submitter": "Varun Kanade", "authors": "Vincent Cohen-Addad, Varun Kanade", "title": "Online Optimization of Smoothed Piecewise Constant Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online optimization of smoothed piecewise constant functions over\nthe domain [0, 1). This is motivated by the problem of adaptively picking\nparameters of learning algorithms as in the recently introduced framework by\nGupta and Roughgarden (2016). Majority of the machine learning literature has\nfocused on Lipschitz-continuous functions or functions with bounded gradients.\n1 This is with good reason---any learning algorithm suffers linear regret even\nagainst piecewise constant functions that are chosen adversarially, arguably\nthe simplest of non-Lipschitz continuous functions. The smoothed setting we\nconsider is inspired by the seminal work of Spielman and Teng (2004) and the\nrecent work of Gupta and Roughgarden---in this setting, the sequence of\nfunctions may be chosen by an adversary, however, with some uncertainty in the\nlocation of discontinuities. We give algorithms that achieve sublinear regret\nin the full information and bandit settings.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 13:52:47 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 12:05:37 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["Kanade", "Varun", ""]]}, {"id": "1604.02027", "submitter": "Ke Jiang", "authors": "Ke Jiang and Suvrit Sra and Brian Kulis", "title": "Combinatorial Topic Models using Small-Variance Asymptotics", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models have emerged as fundamental tools in unsupervised machine\nlearning. Most modern topic modeling algorithms take a probabilistic view and\nderive inference algorithms based on Latent Dirichlet Allocation (LDA) or its\nvariants. In contrast, we study topic modeling as a combinatorial optimization\nproblem, and propose a new objective function derived from LDA by passing to\nthe small-variance limit. We minimize the derived objective by using ideas from\ncombinatorial optimization, which results in a new, fast, and high-quality\ntopic modeling algorithm. In particular, we show that our results are\ncompetitive with popular LDA-based topic modeling approaches, and also discuss\nthe (dis)similarities between our approach and its probabilistic counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 15:04:16 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 03:11:02 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Jiang", "Ke", ""], ["Sra", "Suvrit", ""], ["Kulis", "Brian", ""]]}, {"id": "1604.02038", "submitter": "Fei Tian", "authors": "Fei Tian, Bin Gao, Di He, Tie-Yan Liu", "title": "Sentence Level Recurrent Topic Model: Letting Topics Speak for\n  Themselves", "comments": "The submitted version was done in Feb.2016. Still in improvement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Sentence Level Recurrent Topic Model (SLRTM), a new topic model\nthat assumes the generation of each word within a sentence to depend on both\nthe topic of the sentence and the whole history of its preceding words in the\nsentence. Different from conventional topic models that largely ignore the\nsequential order of words or their topic coherence, SLRTM gives full\ncharacterization to them by using a Recurrent Neural Networks (RNN) based\nframework. Experimental results have shown that SLRTM outperforms several\nstrong baselines on various tasks. Furthermore, SLRTM can automatically\ngenerate sentences given a topic (i.e., topics to sentences), which is a key\ntechnology for real world applications such as personalized short text\nconversation.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 15:29:45 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 05:45:44 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Tian", "Fei", ""], ["Gao", "Bin", ""], ["He", "Di", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1604.02123", "submitter": "Talayeh Razzaghi", "authors": "Talayeh Razzaghi, Oleg Roderick, Ilya Safro, Nicholas Marko", "title": "Multilevel Weighted Support Vector Machine for Classification on\n  Healthcare Data with Missing Values", "comments": "arXiv admin note: substantial text overlap with arXiv:1503.06250", "journal-ref": null, "doi": "10.1371/journal.pone.0155119", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by the needs of predictive analytics on healthcare\ndata as represented by Electronic Medical Records. Such data is invariably\nproblematic: noisy, with missing entries, with imbalance in classes of\ninterests, leading to serious bias in predictive modeling. Since standard data\nmining methods often produce poor performance measures, we argue for\ndevelopment of specialized techniques of data-preprocessing and classification.\nIn this paper, we propose a new method to simultaneously classify large\ndatasets and reduce the effects of missing values. It is based on a multilevel\nframework of the cost-sensitive SVM and the expected maximization imputation\nmethod for missing values, which relies on iterated regression analyses. We\ncompare classification results of multilevel SVM-based algorithms on public\nbenchmark datasets with imbalanced classes and missing values as well as real\ndata in health applications, and show that our multilevel SVM-based method\nproduces fast, and more accurate and robust classification results.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 19:19:52 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Razzaghi", "Talayeh", ""], ["Roderick", "Oleg", ""], ["Safro", "Ilya", ""], ["Marko", "Nicholas", ""]]}, {"id": "1604.02125", "submitter": "Gordon Christie", "authors": "Gordon Christie, Ankit Laddha, Aishwarya Agrawal, Stanislaw Antol,\n  Yash Goyal, Kevin Kochersberger, Dhruv Batra", "title": "Resolving Language and Vision Ambiguities Together: Joint Segmentation &\n  Prepositional Attachment Resolution in Captioned Scenes", "comments": "*The first two authors contributed equally. Conference on Empirical\n  Methods in Natural Language Processing (EMNLP) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to simultaneously perform semantic segmentation and\nprepositional phrase attachment resolution for captioned images. Some\nambiguities in language cannot be resolved without simultaneously reasoning\nabout an associated image. If we consider the sentence \"I shot an elephant in\nmy pajamas\", looking at language alone (and not using common sense), it is\nunclear if it is the person or the elephant wearing the pajamas or both. Our\napproach produces a diverse set of plausible hypotheses for both semantic\nsegmentation and prepositional phrase attachment resolution that are then\njointly reranked to select the most consistent pair. We show that our semantic\nsegmentation and prepositional phrase attachment resolution modules have\ncomplementary strengths, and that joint reasoning produces more accurate\nresults than any module operating in isolation. Multiple hypotheses are also\nshown to be crucial to improved multiple-module reasoning. Our vision and\nlanguage approach significantly outperforms the Stanford Parser (De Marneffe et\nal., 2006) by 17.91% (28.69% relative) and 12.83% (25.28% relative) in two\ndifferent experiments. We also make small improvements over DeepLab-CRF (Chen\net al., 2015).\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 19:26:56 GMT"}, {"version": "v2", "created": "Thu, 5 May 2016 19:05:27 GMT"}, {"version": "v3", "created": "Fri, 2 Sep 2016 01:47:35 GMT"}, {"version": "v4", "created": "Mon, 26 Sep 2016 04:08:19 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Christie", "Gordon", ""], ["Laddha", "Ankit", ""], ["Agrawal", "Aishwarya", ""], ["Antol", "Stanislaw", ""], ["Goyal", "Yash", ""], ["Kochersberger", "Kevin", ""], ["Batra", "Dhruv", ""]]}, {"id": "1604.02218", "submitter": "Hao Yu", "authors": "Hao Yu and Michael J. Neely", "title": "A Low Complexity Algorithm with $O(\\sqrt{T})$ Regret and $O(1)$\n  Constraint Violations for Online Convex Optimization with Long Term\n  Constraints", "comments": "This paper is published in JMLR. The title is changed to emphasize\n  that constraint violations attained by our algorithm is independent of the\n  number of rounds $T$. In this version, we also analyze the regret and\n  constraint violations for our algorithm without requiring the Slater\n  condition", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers online convex optimization over a complicated constraint\nset, which typically consists of multiple functional constraints and a set\nconstraint. The conventional online projection algorithm (Zinkevich, 2003) can\nbe difficult to implement due to the potentially high computation complexity of\nthe projection operation. In this paper, we relax the functional constraints by\nallowing them to be violated at each round but still requiring them to be\nsatisfied in the long term. This type of relaxed online convex optimization\n(with long term constraints) was first considered in Mahdavi et al. (2012).\nThat prior work proposes an algorithm to achieve $O(\\sqrt{T})$ regret and\n$O(T^{3/4})$ constraint violations for general problems and another algorithm\nto achieve an $O(T^{2/3})$ bound for both regret and constraint violations when\nthe constraint set can be described by a finite number of linear constraints. A\nrecent extension in \\citet{Jenatton16ICML} can achieve\n$O(T^{\\max\\{\\theta,1-\\theta\\}})$ regret and $O(T^{1-\\theta/2})$ constraint\nviolations where $\\theta\\in (0,1)$. The current paper proposes a new simple\nalgorithm that yields improved performance in comparison to prior works. The\nnew algorithm achieves an $O(\\sqrt{T})$ regret bound with $O(1)$ constraint\nviolations.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 03:37:52 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 00:31:37 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 06:31:30 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Yu", "Hao", ""], ["Neely", "Michael J.", ""]]}, {"id": "1604.02264", "submitter": "Frank-Michael Schleif", "authors": "Frank-Michael Schleif and Andrej Gisbrecht and Peter Tino", "title": "Probabilistic classifiers with low rank indefinite kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indefinite similarity measures can be frequently found in bio-informatics by\nmeans of alignment scores, but are also common in other fields like shape\nmeasures in image retrieval. Lacking an underlying vector space, the data are\ngiven as pairwise similarities only. The few algorithms available for such data\ndo not scale to larger datasets. Focusing on probabilistic batch classifiers,\nthe Indefinite Kernel Fisher Discriminant (iKFD) and the Probabilistic\nClassification Vector Machine (PCVM) are both effective algorithms for this\ntype of data but, with cubic complexity. Here we propose an extension of iKFD\nand PCVM such that linear runtime and memory complexity is achieved for low\nrank indefinite kernels. Employing the Nystr\\\"om approximation for indefinite\nkernels, we also propose a new almost parameter free approach to identify the\nlandmarks, restricted to a supervised learning problem. Evaluations at several\nlarger similarity data from various domains show that the proposed methods\nprovides similar generalization capabilities while being easier to parametrize\nand substantially faster for large scale data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 07:58:36 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Schleif", "Frank-Michael", ""], ["Gisbrecht", "Andrej", ""], ["Tino", "Peter", ""]]}, {"id": "1604.02270", "submitter": "Mikhail Kolmogorov", "authors": "Mikhail Kolmogorov, Eamonn Kennedy, Zhuxin Dong, Gregory Timp and\n  Pavel Pevzner", "title": "Single-Molecule Protein Identification by Sub-Nanopore Sensors", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1005356", "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in top-down mass spectrometry enabled identification of\nintact proteins, but this technology still faces challenges. For example,\ntop-down mass spectrometry suffers from a lack of sensitivity since the ion\ncounts for a single fragmentation event are often low. In contrast, nanopore\ntechnology is exquisitely sensitive to single intact molecules, but it has only\nbeen successfully applied to DNA sequencing, so far. Here, we explore the\npotential of sub-nanopores for single-molecule protein identification (SMPI)\nand describe an algorithm for identification of the electrical current blockade\nsignal (nanospectrum) resulting from the translocation of a denaturated,\nlinearly charged protein through a sub-nanopore. The analysis of identification\np-values suggests that the current technology is already sufficient for\nmatching nanospectra against small protein databases, e.g., protein\nidentification in bacterial proteomes.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 08:16:41 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 23:24:59 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Kolmogorov", "Mikhail", ""], ["Kennedy", "Eamonn", ""], ["Dong", "Zhuxin", ""], ["Timp", "Gregory", ""], ["Pevzner", "Pavel", ""]]}, {"id": "1604.02275", "submitter": "Rocco De Rosa rd", "authors": "Rocco De Rosa, Thomas Mensink and Barbara Caputo", "title": "Online Open World Recognition", "comments": "keywords{Open world recognition, Open set, Incremental Learning,\n  Metric Learning, Nonparametric methods, Classification confidence}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we enter into the big data age and an avalanche of images have become\nreadily available, recognition systems face the need to move from close, lab\nsettings where the number of classes and training data are fixed, to dynamic\nscenarios where the number of categories to be recognized grows continuously\nover time, as well as new data providing useful information to update the\nsystem. Recent attempts, like the open world recognition framework, tried to\ninject dynamics into the system by detecting new unknown classes and adding\nthem incrementally, while at the same time continuously updating the models for\nthe known classes. incrementally adding new classes and detecting instances\nfrom unknown classes, while at the same time continuously updating the models\nfor the known classes. In this paper we argue that to properly capture the\nintrinsic dynamic of open world recognition, it is necessary to add to these\naspects (a) the incremental learning of the underlying metric, (b) the\nincremental estimate of confidence thresholds for the unknown classes, and (c)\nthe use of local learning to precisely describe the space of classes. We extend\nthree existing metric learning algorithms towards these goals by using online\nmetric learning. Experimentally we validate our approach on two large-scale\ndatasets in different learning scenarios. For all these scenarios our proposed\nmethods outperform their non-online counterparts. We conclude that local and\nonline learning is important to capture the full dynamics of open world\nrecognition.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 08:43:15 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["De Rosa", "Rocco", ""], ["Mensink", "Thomas", ""], ["Caputo", "Barbara", ""]]}, {"id": "1604.02336", "submitter": "Kevin Wilson", "authors": "Kevin H. Wilson, Yan Karklin, Bojian Han, and Chaitanya Ekanadham", "title": "Back to the Basics: Bayesian extensions of IRT outperform neural\n  networks for proficiency estimation", "comments": "6 pages, 2 figures, Educational Data Mining 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating student proficiency is an important task for computer based\nlearning systems. We compare a family of IRT-based proficiency estimation\nmethods to Deep Knowledge Tracing (DKT), a recently proposed recurrent neural\nnetwork model with promising initial results. We evaluate how well each model\npredicts a student's future response given previous responses using two\npublicly available and one proprietary data set. We find that IRT-based methods\nconsistently matched or outperformed DKT across all data sets at the finest\nlevel of content granularity that was tractable for them to be trained on. A\nhierarchical extension of IRT that captured item grouping structure performed\nbest overall. When data sets included non-trivial autocorrelations in student\nresponse patterns, a temporal extension of IRT improved performance over\nstandard IRT while the RNN-based method did not. We conclude that IRT-based\nmodels provide a simpler, better-performing alternative to existing RNN-based\nmodels of student interaction data while also affording more interpretability\nand guarantees due to their formulation as Bayesian probabilistic models.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 12:54:18 GMT"}, {"version": "v2", "created": "Sat, 21 May 2016 18:26:21 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Wilson", "Kevin H.", ""], ["Karklin", "Yan", ""], ["Han", "Bojian", ""], ["Ekanadham", "Chaitanya", ""]]}, {"id": "1604.02354", "submitter": "Dong Wang", "authors": "Dong Wang, Xiaoyang Tan", "title": "Bayesian Neighbourhood Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a good distance metric in feature space potentially improves the\nperformance of the KNN classifier and is useful in many real-world\napplications. Many metric learning algorithms are however based on the point\nestimation of a quadratic optimization problem, which is time-consuming,\nsusceptible to overfitting, and lack a natural mechanism to reason with\nparameter uncertainty, an important property useful especially when the\ntraining set is small and/or noisy. To deal with these issues, we present a\nnovel Bayesian metric learning method, called Bayesian NCA, based on the\nwell-known Neighbourhood Component Analysis method, in which the metric\nposterior is characterized by the local label consistency constraints of\nobservations, encoded with a similarity graph instead of independent pairwise\nconstraints. For efficient Bayesian optimization, we explore the variational\nlower bound over the log-likelihood of the original NCA objective. Experiments\non several publicly available datasets demonstrate that the proposed method is\nable to learn robust metric measures from small size dataset and/or from\nchallenging training set with labels contaminated by errors. The proposed\nmethod is also shown to outperform a previous pairwise constrained Bayesian\nmetric learning method.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 13:35:03 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Wang", "Dong", ""], ["Tan", "Xiaoyang", ""]]}, {"id": "1604.02376", "submitter": "Jyothi Korra", "authors": "Jyothi Korra", "title": "Finding Optimal Combination of Kernels using Genetic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Computer Vision, problem of identifying or classifying the objects present\nin an image is called Object Categorization. It is a challenging problem,\nespecially when the images have clutter background, occlusions or different\nlighting conditions. Many vision features have been proposed which aid object\ncategorization even in such adverse conditions. Past research has shown that,\nemploying multiple features rather than any single features leads to better\nrecognition. Multiple Kernel Learning (MKL) framework has been developed for\nlearning an optimal combination of features for object categorization. Existing\nMKL methods use linear combination of base kernels which may not be optimal for\nobject categorization. Real-world object categorization may need to consider\ncomplex combination of kernels(non-linear) and not only linear combination.\nEvolving non-linear functions of base kernels using Genetic Programming is\nproposed in this report. Experiment results show that non-kernel generated\nusing genetic programming gives good accuracy as compared to linear combination\nof kernels.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 15:33:30 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 23:16:20 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Korra", "Jyothi", ""]]}, {"id": "1604.02477", "submitter": "Lorenzo Livi", "authors": "Lorenzo Livi, Cesare Alippi", "title": "One-class classifiers based on entropic spanning graphs", "comments": "Extended and revised version of the paper \"One-Class Classification\n  Through Mutual Information Minimization\" presented at the 2016 IEEE IJCNN,\n  Vancouver, Canada", "journal-ref": null, "doi": "10.1109/TNNLS.2016.2608983", "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-class classifiers offer valuable tools to assess the presence of outliers\nin data. In this paper, we propose a design methodology for one-class\nclassifiers based on entropic spanning graphs. Our approach takes into account\nthe possibility to process also non-numeric data by means of an embedding\nprocedure. The spanning graph is learned on the embedded input data and the\noutcoming partition of vertices defines the classifier. The final partition is\nderived by exploiting a criterion based on mutual information minimization.\nHere, we compute the mutual information by using a convenient formulation\nprovided in terms of the $\\alpha$-Jensen difference. Once training is\ncompleted, in order to associate a confidence level with the classifier\ndecision, a graph-based fuzzy model is constructed. The fuzzification process\nis based only on topological information of the vertices of the entropic\nspanning graph. As such, the proposed one-class classifier is suitable also for\ndata characterized by complex geometric structures. We provide experiments on\nwell-known benchmarks containing both feature vectors and labeled graphs. In\naddition, we apply the method to the protein solubility recognition problem by\nconsidering several representations for the input samples. Experimental results\ndemonstrate the effectiveness and versatility of the proposed method with\nrespect to other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 20:41:54 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 15:54:05 GMT"}, {"version": "v3", "created": "Wed, 13 Jul 2016 17:16:33 GMT"}, {"version": "v4", "created": "Fri, 12 Aug 2016 15:49:01 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Livi", "Lorenzo", ""], ["Alippi", "Cesare", ""]]}, {"id": "1604.02492", "submitter": "Samuel Elder", "authors": "Sam Elder", "title": "Challenges in Bayesian Adaptive Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional statistical analysis requires that the analysis process and data\nare independent. By contrast, the new field of adaptive data analysis hopes to\nunderstand and provide algorithms and accuracy guarantees for research as it is\ncommonly performed in practice, as an iterative process of interacting\nrepeatedly with the same data set, such as repeated tests against a holdout\nset. Previous work has defined a model with a rather strong lower bound on\nsample complexity in terms of the number of queries, $n\\sim\\sqrt q$, arguing\nthat adaptive data analysis is much harder than static data analysis, where\n$n\\sim\\log q$ is possible. Instead, we argue that those strong lower bounds\npoint to a limitation of the previous model in that it must consider wildly\nasymmetric scenarios which do not hold in typical applications.\n  To better understand other difficulties of adaptivity, we propose a new\nBayesian version of the problem that mandates symmetry. Since the other lower\nbound techniques are ruled out, we can more effectively see difficulties that\nmight otherwise be overshadowed. As a first contribution to this model, we\nproduce a new problem using error-correcting codes on which a large family of\nmethods, including all previously proposed algorithms, require roughly\n$n\\sim\\sqrt[4]q$. These early results illustrate new difficulties in adaptive\ndata analysis regarding slightly correlated queries on problems with\nconcentrated uncertainty.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 21:56:24 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 19:15:56 GMT"}, {"version": "v3", "created": "Fri, 4 Nov 2016 21:05:14 GMT"}, {"version": "v4", "created": "Mon, 21 Nov 2016 22:20:12 GMT"}, {"version": "v5", "created": "Mon, 20 Mar 2017 20:00:41 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Elder", "Sam", ""]]}, {"id": "1604.02506", "submitter": "Antonio Jose Jimeno Yepes", "authors": "Antonio Jimeno Yepes", "title": "Word embeddings and recurrent neural networks based on Long-Short Term\n  Memory nodes in supervised biomedical word sense disambiguation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word sense disambiguation helps identifying the proper sense of ambiguous\nwords in text. With large terminologies such as the UMLS Metathesaurus\nambiguities appear and highly effective disambiguation methods are required.\nSupervised learning algorithm methods are used as one of the approaches to\nperform disambiguation. Features extracted from the context of an ambiguous\nword are used to identify the proper sense of such a word. The type of features\nhave an impact on machine learning methods, thus affect disambiguation\nperformance. In this work, we have evaluated several types of features derived\nfrom the context of the ambiguous word and we have explored as well more global\nfeatures derived from MEDLINE using word embeddings. Results show that word\nembeddings improve the performance of more traditional features and allow as\nwell using recurrent neural network classifiers based on Long-Short Term Memory\n(LSTM) nodes. The combination of unigrams and word embeddings with an SVM sets\na new state of the art performance with a macro accuracy of 95.97 in the MSH\nWSD data set.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 01:14:05 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 12:04:10 GMT"}, {"version": "v3", "created": "Mon, 19 Dec 2016 19:44:52 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Yepes", "Antonio Jimeno", ""]]}, {"id": "1604.02594", "submitter": "Zhiyun Lu", "authors": "Zhiyun Lu, Vikas Sindhwani, Tara N. Sainath", "title": "Learning Compact Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs), including long short-term memory (LSTM)\nRNNs, have produced state-of-the-art results on a variety of speech recognition\ntasks. However, these models are often too large in size for deployment on\nmobile devices with memory and latency constraints. In this work, we study\nmechanisms for learning compact RNNs and LSTMs via low-rank factorizations and\nparameter sharing schemes. Our goal is to investigate redundancies in recurrent\narchitectures where compression can be admitted without losing performance. A\nhybrid strategy of using structured matrices in the bottom layers and shared\nlow-rank factors on the top layers is found to be particularly effective,\nreducing the parameters of a standard LSTM by 75%, at a small cost of 0.3%\nincrease in WER, on a 2,000-hr English Voice Search task.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 19:09:22 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Lu", "Zhiyun", ""], ["Sindhwani", "Vikas", ""], ["Sainath", "Tara N.", ""]]}, {"id": "1604.02606", "submitter": "Bo Li", "authors": "Bo Li, Yevgeniy Vorobeychik, Xinyun Chen", "title": "A General Retraining Framework for Scalable Adversarial Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional classification algorithms assume that training and test data come\nfrom similar distributions. This assumption is violated in adversarial\nsettings, where malicious actors modify instances to evade detection. A number\nof custom methods have been developed for both adversarial evasion attacks and\nrobust learning. We propose the first systematic and general-purpose retraining\nframework which can: a) boost robustness of an \\emph{arbitrary} learning\nalgorithm, in the face of b) a broader class of adversarial models than any\nprior methods. We show that, under natural conditions, the retraining framework\nminimizes an upper bound on optimal adversarial risk, and show how to extend\nthis result to account for approximations of evasion attacks. Extensive\nexperimental evaluation demonstrates that our retraining methods are nearly\nindistinguishable from state-of-the-art algorithms for optimizing adversarial\nrisk, but are more general and far more scalable. The experiments also confirm\nthat without retraining, our adversarial framework dramatically reduces the\neffectiveness of learning. In contrast, retraining significantly boosts\nrobustness to evasion attacks without significantly compromising overall\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 20:14:36 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 09:31:57 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Li", "Bo", ""], ["Vorobeychik", "Yevgeniy", ""], ["Chen", "Xinyun", ""]]}, {"id": "1604.02634", "submitter": "Renbo Zhao", "authors": "Renbo Zhao and Vincent Y. F. Tan", "title": "Online Nonnegative Matrix Factorization with Outliers", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2620967", "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified and systematic framework for performing online\nnonnegative matrix factorization in the presence of outliers. Our framework is\nparticularly suited to large-scale data. We propose two solvers based on\nprojected gradient descent and the alternating direction method of multipliers.\nWe prove that the sequence of objective values converges almost surely by\nappealing to the quasi-martingale convergence theorem. We also show the\nsequence of learned dictionaries converges to the set of stationary points of\nthe expected loss function almost surely. In addition, we extend our basic\nproblem formulation to various settings with different constraints and\nregularizers. We also adapt the solvers and analyses to each setting. We\nperform extensive experiments on both synthetic and real datasets. These\nexperiments demonstrate the computational efficiency and efficacy of our\nalgorithms on tasks such as (parts-based) basis learning, image denoising,\nshadow removal and foreground-background separation.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 04:02:57 GMT"}, {"version": "v2", "created": "Sat, 15 Oct 2016 12:01:30 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Zhao", "Renbo", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "1604.02646", "submitter": "Biswajit Paria", "authors": "Biswajit Paria, Vikas Reddy, Anirban Santara, Pabitra Mitra", "title": "Visualization Regularizers for Neural Network based Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep neural networks is mostly due their ability to learn\nmeaningful features from the data. Features learned in the hidden layers of\ndeep neural networks trained in computer vision tasks have been shown to be\nsimilar to mid-level vision features. We leverage this fact in this work and\npropose the visualization regularizer for image tasks. The proposed\nregularization technique enforces smoothness of the features learned by hidden\nnodes and turns out to be a special case of Tikhonov regularization. We achieve\nhigher classification accuracy as compared to existing regularizers such as the\nL2 norm regularizer and dropout, on benchmark datasets without changing the\ntraining computational complexity.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 07:02:40 GMT"}, {"version": "v2", "created": "Sun, 15 May 2016 14:38:38 GMT"}, {"version": "v3", "created": "Tue, 3 Jan 2017 10:07:22 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Paria", "Biswajit", ""], ["Reddy", "Vikas", ""], ["Santara", "Anirban", ""], ["Mitra", "Pabitra", ""]]}, {"id": "1604.02752", "submitter": "Junan Zhu", "authors": "Junan Zhu, Ahmad Beirami, Dror Baron", "title": "Performance Trade-Offs in Multi-Processor Approximate Message Passing", "comments": "5 pages, 2 figures, to appear at 2016 IEEE International Symposium on\n  Information Theory (ISIT2016)", "journal-ref": null, "doi": "10.1109/ISIT.2016.7541385", "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider large-scale linear inverse problems in Bayesian settings. Our\ngeneral approach follows a recent line of work that applies the approximate\nmessage passing (AMP) framework in multi-processor (MP) computational systems\nby storing and processing a subset of rows of the measurement matrix along with\ncorresponding measurements at each MP node. In each MP-AMP iteration, nodes of\nthe MP system and its fusion center exchange lossily compressed messages\npertaining to their estimates of the input. There is a trade-off between the\nphysical costs of the reconstruction process including computation time,\ncommunication loads, and the reconstruction quality, and it is impossible to\nsimultaneously minimize all the costs. We pose this minimization as a\nmulti-objective optimization problem (MOP), and study the properties of the\nbest trade-offs (Pareto optimality) in this MOP. We prove that the achievable\nregion of this MOP is convex, and conjecture how the combined cost of\ncomputation and communication scales with the desired mean squared error. These\nproperties are verified numerically.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2016 22:28:10 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Zhu", "Junan", ""], ["Beirami", "Ahmad", ""], ["Baron", "Dror", ""]]}, {"id": "1604.02855", "submitter": "Rocco De Rosa rd", "authors": "Rocco De Rosa, Ilaria Gori, Fabio Cuzzolin, Barbara Caputo and\n  Nicol\\`o Cesa-Bianchi", "title": "Active Learning for Online Recognition of Human Activities from\n  Streaming Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognising human activities from streaming videos poses unique challenges to\nlearning algorithms: predictive models need to be scalable, incrementally\ntrainable, and must remain bounded in size even when the data stream is\narbitrarily long. Furthermore, as parameter tuning is problematic in a\nstreaming setting, suitable approaches should be parameterless, and make no\nassumptions on what class labels may occur in the stream. We present here an\napproach to the recognition of human actions from streaming data which meets\nall these requirements by: (1) incrementally learning a model which adaptively\ncovers the feature space with simple local classifiers; (2) employing an active\nlearning strategy to reduce annotation requests; (3) achieving promising\naccuracy within a fixed model size. Extensive experiments on standard\nbenchmarks show that our approach is competitive with state-of-the-art\nnon-incremental methods, and outperforms the existing active incremental\nbaselines.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 09:32:51 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["De Rosa", "Rocco", ""], ["Gori", "Ilaria", ""], ["Cuzzolin", "Fabio", ""], ["Caputo", "Barbara", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""]]}, {"id": "1604.02917", "submitter": "Stefanos Eleftheriadis", "authors": "Stefanos Eleftheriadis and Ognjen Rudovic and Marc P. Deisenroth and\n  Maja Pantic", "title": "Gaussian Process Domain Experts for Model Adaptation in Facial Behavior\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for supervised domain adaptation that is based\nupon the probabilistic framework of Gaussian processes (GPs). Specifically, we\nintroduce domain-specific GPs as local experts for facial expression\nclassification from face images. The adaptation of the classifier is\nfacilitated in probabilistic fashion by conditioning the target expert on\nmultiple source experts. Furthermore, in contrast to existing adaptation\napproaches, we also learn a target expert from available target data solely.\nThen, a single and confident classifier is obtained by combining the\npredictions from multiple experts based on their confidence. Learning of the\nmodel is efficient and requires no retraining/reweighting of the source\nclassifiers. We evaluate the proposed approach on two publicly available\ndatasets for multi-class (MultiPIE) and multi-label (DISFA) facial expression\nclassification. To this end, we perform adaptation of two contextual factors:\n'where' (view) and 'who' (subject). We show in our experiments that the\nproposed approach consistently outperforms both source and target classifiers,\nwhile using as few as 30 target examples. It also outperforms the\nstate-of-the-art approaches for supervised domain adaptation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 12:37:36 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 18:54:08 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Eleftheriadis", "Stefanos", ""], ["Rudovic", "Ognjen", ""], ["Deisenroth", "Marc P.", ""], ["Pantic", "Maja", ""]]}, {"id": "1604.03006", "submitter": "Sewoong Oh", "authors": "Weihao Gao, Sewoong Oh, Pramod Viswanath", "title": "Demystifying Fixed k-Nearest Neighbor Information Estimators", "comments": "55 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating mutual information from i.i.d. samples drawn from an unknown joint\ndensity function is a basic statistical problem of broad interest with\nmultitudinous applications. The most popular estimator is one proposed by\nKraskov and St\\\"ogbauer and Grassberger (KSG) in 2004, and is nonparametric and\nbased on the distances of each sample to its $k^{\\rm th}$ nearest neighboring\nsample, where $k$ is a fixed small integer. Despite its widespread use (part of\nscientific software packages), theoretical properties of this estimator have\nbeen largely unexplored. In this paper we demonstrate that the estimator is\nconsistent and also identify an upper bound on the rate of convergence of the\nbias as a function of number of samples. We argue that the superior performance\nbenefits of the KSG estimator stems from a curious \"correlation boosting\"\neffect and build on this intuition to modify the KSG estimator in novel ways to\nconstruct a superior estimator. As a byproduct of our investigations, we obtain\nnearly tight rates of convergence of the $\\ell_2$ error of the well known fixed\n$k$ nearest neighbor estimator of differential entropy by Kozachenko and\nLeonenko.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 15:47:05 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 16:45:42 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Gao", "Weihao", ""], ["Oh", "Sewoong", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1604.03010", "submitter": "Xin Du", "authors": "Xin Du", "title": "Semi-supervised learning of local structured output predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of semi-supervised structured output\nprediction, which aims to learn predictors for structured outputs, such as\nsequences, tree nodes, vectors, etc., from a set of data points of both\ninput-output pairs and single inputs without outputs. The traditional methods\nto solve this problem usually learns one single predictor for all the data\npoints, and ignores the variety of the different data points. Different parts\nof the data set may have different local distributions, and requires different\noptimal local predictors. To overcome this disadvantage of existing methods, we\npropose to learn different local predictors for neighborhoods of different data\npoints, and the missing structured outputs simultaneously. In the neighborhood\nof each data point, we proposed to learn a linear predictor by minimizing both\nthe complexity of the predictor and the upper bound of the structured\nprediction loss. The minimization is conducted by gradient descent algorithms.\nExperiments over four benchmark data sets, including DDSM mammography medical\nimages, SUN natural image data set, Cora research paper data set, and Spanish\nnews wire article sentence data set, show the advantages of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 15:53:04 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Du", "Xin", ""]]}, {"id": "1604.03034", "submitter": "Dezhi Fang", "authors": "Dezhi Fang, Duen Horng Chau", "title": "M3: Scaling Up Machine Learning via Memory Mapping", "comments": "2 pages, 1 figure, 1 table", "journal-ref": null, "doi": "10.1145/1235", "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To process data that do not fit in RAM, conventional wisdom would suggest\nusing distributed approaches. However, recent research has demonstrated virtual\nmemory's strong potential in scaling up graph mining algorithms on a single\nmachine. We propose to use a similar approach for general machine learning. We\ncontribute: (1) our latest finding that memory mapping is also a feasible\ntechnique for scaling up general machine learning algorithms like logistic\nregression and k-means, when data fits in or exceeds RAM (we tested datasets up\nto 190GB); (2) an approach, called M3, that enables existing machine learning\nalgorithms to work with out-of-core datasets through memory mapping, achieving\na speed that is significantly faster than a 4-instance Spark cluster, and\ncomparable to an 8-instance cluster.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 17:12:14 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Fang", "Dezhi", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1604.03058", "submitter": "Xundong Wu", "authors": "Xundong Wu, Yong Wu and Yong Zhao", "title": "Binarized Neural Networks on the ImageNet Classification Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We trained Binarized Neural Networks (BNNs) on the high resolution ImageNet\nILSVRC-2102 dataset classification task and achieved a good performance. With a\nmoderate size network of 13 layers, we obtained top-5 classification accuracy\nrate of 84.1 % on validation set through network distillation, much better than\nprevious published results of 73.2% on XNOR network and 69.1% on binarized\nGoogleNET. We expect networks of better performance can be obtained by\nfollowing our current strategies. We provide a detailed discussion and\npreliminary analysis on strategies used in the network training.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 18:39:33 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 03:22:37 GMT"}, {"version": "v3", "created": "Mon, 24 Oct 2016 15:25:06 GMT"}, {"version": "v4", "created": "Tue, 8 Nov 2016 00:38:03 GMT"}, {"version": "v5", "created": "Sat, 19 Nov 2016 01:37:40 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Wu", "Xundong", ""], ["Wu", "Yong", ""], ["Zhao", "Yong", ""]]}, {"id": "1604.03073", "submitter": "Ashley Prater", "authors": "Ashley Prater", "title": "Reservoir computing for spatiotemporal signal classification without\n  trained output weights", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir computing is a recently introduced machine learning paradigm that\nhas been shown to be well-suited for the processing of spatiotemporal data.\nRather than training the network node connections and weights via\nbackpropagation in traditional recurrent neural networks, reservoirs instead\nhave fixed connections and weights among the `hidden layer' nodes, and\ntraditionally only the weights to the output layer of neurons are trained using\nlinear regression. We claim that for signal classification tasks one may forgo\nthe weight training step entirely and instead use a simple supervised\nclustering method based upon principal components of norms of reservoir states.\nThe proposed method is mathematically analyzed and explored through numerical\nexperiments on real-world data. The examples demonstrate that the proposed may\noutperform the traditional trained output weight approach in terms of\nclassification accuracy and sensitivity to reservoir parameters.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 19:14:05 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 13:28:17 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Prater", "Ashley", ""]]}, {"id": "1604.03099", "submitter": "Carlos Leandro", "authors": "Carlos Leandro", "title": "Symbolic Knowledge Extraction using {\\L}ukasiewicz Logics", "comments": "15 pages. arXiv admin note: substantial text overlap with\n  arXiv:1604.02780, arXiv:1604.02774", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes a methodology that combines logic-based systems and\nconnectionist systems. Our approach uses finite truth-valued {\\L}ukasiewicz\nlogic, wherein every connective can be defined by a neuron in an artificial\nnetwork. This allowed the injection of first-order formulas into a network\narchitecture, and also simplified symbolic rule extraction. For that we trained\na neural networks using the Levenderg-Marquardt algorithm, where we restricted\nthe knowledge dissemination in the network structure. This procedure reduces\nneural network plasticity without drastically damaging the learning\nperformance, thus making the descriptive power of produced neural networks\nsimilar to the descriptive power of {\\L}ukasiewicz logic language and\nsimplifying the translation between symbolic and connectionist structures. We\nused this method for reverse engineering truth table and in extraction of\nformulas from real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 05:17:09 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Leandro", "Carlos", ""]]}, {"id": "1604.03171", "submitter": "Jamie Morgenstern", "authors": "Jamie Morgenstern, Tim Roughgarden", "title": "Learning Simple Auctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework for proving polynomial sample complexity\nbounds for the problem of learning from samples the best auction in a class of\n\"simple\" auctions. Our framework captures all of the most prominent examples of\n\"simple\" auctions, including anonymous and non-anonymous item and bundle\npricings, with either a single or multiple buyers. The technique we propose is\nto break the analysis of auctions into two natural pieces. First, one shows\nthat the set of allocation rules have large amounts of structure; second,\nfixing an allocation on a sample, one shows that the set of auctions agreeing\nwith this allocation on that sample have revenue functions with low\ndimensionality. Our results effectively imply that whenever it's possible to\ncompute a near-optimal simple auction with a known prior, it is also possible\nto compute such an auction with an unknown prior (given a polynomial number of\nsamples).\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 22:51:04 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Morgenstern", "Jamie", ""], ["Roughgarden", "Tim", ""]]}, {"id": "1604.03200", "submitter": "Ilias Flaounas", "authors": "Ricardo \\~Nanculef, Ilias Flaounas, Nello Cristianini", "title": "Efficient Classification of Multi-Labelled Text Streams by Clashing", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2014.02.017", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for the classification of multi-labelled text documents\nexplicitly designed for data stream applications that require to process a\nvirtually infinite sequence of data using constant memory and constant\nprocessing time. Our method is composed of an online procedure used to\nefficiently map text into a low-dimensional feature space and a partition of\nthis space into a set of regions for which the system extracts and keeps\nstatistics used to predict multi-label text annotations. Documents are fed into\nthe system as a sequence of words, mapped to a region of the partition, and\nannotated using the statistics computed from the labelled instances colliding\nin the same region. This approach is referred to as clashing. We illustrate the\nmethod in real-world text data, comparing the results with those obtained using\nother text classifiers. In addition, we provide an analysis about the effect of\nthe representation space dimensionality on the predictive performance of the\nsystem. Our results show that the online embedding indeed approximates the\ngeometry of the full corpus-wise TF and TF-IDF space. The model obtains\ncompetitive F measures with respect to the most accurate methods, using\nsignificantly fewer computational resources. In addition, the method achieves a\nhigher macro-averaged F measure than methods with similar running time.\nFurthermore, the system is able to learn faster than the other methods from\npartially labelled streams.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 01:52:38 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["\u00d1anculef", "Ricardo", ""], ["Flaounas", "Ilias", ""], ["Cristianini", "Nello", ""]]}, {"id": "1604.03221", "submitter": "Alireza Hajibagheri", "authors": "Alireza Hajibagheri, Gita Sukthankar, Kiran Lakkaraju", "title": "Leveraging Network Dynamics for Improved Link Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of link prediction is to forecast connections that are most likely to\noccur in the future, based on examples of previously observed links. A key\ninsight is that it is useful to explicitly model network dynamics, how\nfrequently links are created or destroyed when doing link prediction. In this\npaper, we introduce a new supervised link prediction framework, RPM (Rate\nPrediction Model). In addition to network similarity measures, RPM uses the\npredicted rate of link modifications, modeled using time series data; it is\nimplemented in Spark-ML and trained with the original link distribution, rather\nthan a small balanced subset. We compare the use of this network dynamics model\nto directly creating time series of network similarity measures. Our\nexperiments show that RPM, which leverages predicted rates, outperforms the use\nof network similarity measures, either individually or within a time series.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 20:46:13 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Hajibagheri", "Alireza", ""], ["Sukthankar", "Gita", ""], ["Lakkaraju", "Kiran", ""]]}, {"id": "1604.03227", "submitter": "Jason Kuen", "authors": "Jason Kuen, Zhenhua Wang, Gang Wang", "title": "Recurrent Attentional Networks for Saliency Detection", "comments": "CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional-deconvolution networks can be adopted to perform end-to-end\nsaliency detection. But, they do not work well with objects of multiple scales.\nTo overcome such a limitation, in this work, we propose a recurrent attentional\nconvolutional-deconvolution network (RACDNN). Using spatial transformer and\nrecurrent network units, RACDNN is able to iteratively attend to selected image\nsub-regions to perform saliency refinement progressively. Besides tackling the\nscale problem, RACDNN can also learn context-aware features from past\niterations to enhance saliency refinement in future iterations. Experiments on\nseveral challenging saliency detection datasets validate the effectiveness of\nRACDNN, and show that RACDNN outperforms state-of-the-art saliency detection\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 03:03:04 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Kuen", "Jason", ""], ["Wang", "Zhenhua", ""], ["Wang", "Gang", ""]]}, {"id": "1604.03247", "submitter": "Dinesh Govindaraj", "authors": "Dinesh Govindaraj", "title": "Thesis: Multiple Kernel Learning for Object Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object Categorization is a challenging problem, especially when the images\nhave clutter background, occlusions or different lighting conditions. In the\npast, many descriptors have been proposed which aid object categorization even\nin such adverse conditions. Each descriptor has its own merits and de-merits.\nSome descriptors are invariant to transformations while the others are more\ndiscriminative. Past research has shown that, employing multiple descriptors\nrather than any single descriptor leads to better recognition. The problem of\nlearning the optimal combination of the available descriptors for a particular\nclassification task is studied. Multiple Kernel Learning (MKL) framework has\nbeen developed for learning an optimal combination of descriptors for object\ncategorization. Existing MKL formulations often employ block l-1 norm\nregularization which is equivalent to selecting a single kernel from a library\nof kernels. Since essentially a single descriptor is selected, the existing\nformulations maybe sub- optimal for object categorization. A MKL formulation\nbased on block l-infinity norm regularization has been developed, which chooses\nan optimal combination of kernels as opposed to selecting a single kernel. A\nComposite Multiple Kernel Learning(CKL) formulation based on mixed l-infinity\nand l-1 norm regularization has been developed. These formulations end in\nSecond Order Cone Programs(SOCP). Other efficient alter- native algorithms for\nthese formulation have been implemented. Empirical results on benchmark\ndatasets show significant improvement using these new MKL formulations.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 04:56:24 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Govindaraj", "Dinesh", ""]]}, {"id": "1604.03248", "submitter": "Natasha Markuzon", "authors": "Mallory Sheth, Roy Welsch, Natasha Markuzon", "title": "The Univariate Flagging Algorithm (UFA): a Fully-Automated Approach for\n  Identifying Optimal Thresholds in Data", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many data classification problems, there is no linear relationship between\nan explanatory and the dependent variables. Instead, there may be ranges of the\ninput variable for which the observed outcome is signficantly more or less\nlikely. This paper describes an algorithm for automatic detection of such\nthresholds, called the Univariate Flagging Algorithm (UFA). The algorithm\nsearches for a separation that optimizes the difference between separated areas\nwhile providing the maximum support. We evaluate its performance using three\nexamples and demonstrate that thresholds identified by the algorithm align well\nwith visual inspection and subject matter expertise. We also introduce two\nclassification approaches that use UFA and show that the performance attained\non unseen test data is equal to or better than that of more traditional\nclassifiers. We demonstrate that the proposed algorithm is robust against\nmissing data and noise, is scalable, and is easy to interpret and visualize. It\nis also well suited for problems where incidence of the target is low.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 05:04:04 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Sheth", "Mallory", ""], ["Welsch", "Roy", ""], ["Markuzon", "Natasha", ""]]}, {"id": "1604.03266", "submitter": "Guy Uziel", "authors": "Guy Uziel and Ran El-Yaniv", "title": "Online Learning of Portfolio Ensembles with Sector Exposure\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online learning of ensembles of portfolio selection algorithms\nand aim to regularize risk by encouraging diversification with respect to a\npredefined risk-driven grouping of stocks. Our procedure uses online convex\noptimization to control capital allocation to underlying investment algorithms\nwhile encouraging non-sparsity over the given grouping. We prove a logarithmic\nregret for this procedure with respect to the best-in-hindsight ensemble. We\napplied the procedure with known mean-reversion portfolio selection algorithms\nusing the standard GICS industry sector grouping. Empirical Experimental\nresults showed an impressive percentage increase of risk-adjusted return\n(Sharpe ratio).\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 07:11:09 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Uziel", "Guy", ""], ["El-Yaniv", "Ran", ""]]}, {"id": "1604.03278", "submitter": "Rocco De Rosa rd", "authors": "Rocco De Rosa", "title": "Confidence Decision Trees via Online and Active Learning for Streaming\n  (BIG) Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision tree classifiers are a widely used tool in data stream mining. The\nuse of confidence intervals to estimate the gain associated with each split\nleads to very effective methods, like the popular Hoeffding tree algorithm.\nFrom a statistical viewpoint, the analysis of decision tree classifiers in a\nstreaming setting requires knowing when enough new information has been\ncollected to justify splitting a leaf. Although some of the issues in the\nstatistical analysis of Hoeffding trees have been already clarified, a general\nand rigorous study of confidence intervals for splitting criteria is missing.\nWe fill this gap by deriving accurate confidence intervals to estimate the\nsplitting gain in decision tree learning with respect to three criteria:\nentropy, Gini index, and a third index proposed by Kearns and Mansour. Our\nconfidence intervals depend in a more detailed way on the tree parameters. We\nalso extend our confidence analysis to a selective sampling setting, in which\nthe decision tree learner adaptively decides which labels to query in the\nstream. We furnish theoretical guarantee bounding the probability that the\nclassification is non-optimal learning the decision tree via our selective\nsampling strategy. Experiments on real and synthetic data in a streaming\nsetting show that our trees are indeed more accurate than trees with the same\nnumber of leaves generated by other techniques and our active learning module\npermits to save labeling cost. In addition, comparing our labeling strategy\nwith recent methods, we show that our approach is more robust and consistent\nrespect all the other techniques applied to incremental decision trees.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 07:59:55 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["De Rosa", "Rocco", ""]]}, {"id": "1604.03336", "submitter": "Raef Bassily", "authors": "Raef Bassily and Yoav Freund", "title": "Typical Stability", "comments": "New sections, extended discussions, and complete proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a notion of algorithmic stability called typical\nstability. When our goal is to release real-valued queries (statistics)\ncomputed over a dataset, this notion does not require the queries to be of\nbounded sensitivity -- a condition that is generally assumed under differential\nprivacy [DMNS06, Dwork06] when used as a notion of algorithmic stability\n[DFHPRR15a, DFHPRR15b, BNSSSU16] -- nor does it require the samples in the\ndataset to be independent -- a condition that is usually assumed when\ngeneralization-error guarantees are sought. Instead, typical stability requires\nthe output of the query, when computed on a dataset drawn from the underlying\ndistribution, to be concentrated around its expected value with respect to that\ndistribution.\n  We discuss the implications of typical stability on the generalization error\n(i.e., the difference between the value of the query computed on the dataset\nand the expected value of the query with respect to the true data\ndistribution). We show that typical stability can control generalization error\nin adaptive data analysis even when the samples in the dataset are not\nnecessarily independent and when queries to be computed are not necessarily of\nbounded-sensitivity as long as the results of the queries over the dataset\n(i.e., the computed statistics) follow a distribution with a \"light\" tail.\nExamples of such queries include, but not limited to, subgaussian and\nsubexponential queries.\n  We also discuss the composition guarantees of typical stability and prove\ncomposition theorems that characterize the degradation of the parameters of\ntypical stability under $k$-fold adaptive composition. We also give simple\nnoise-addition algorithms that achieve this notion. These algorithms are\nsimilar to their differentially private counterparts, however, the added noise\nis calibrated differently.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 10:52:06 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 00:06:06 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Bassily", "Raef", ""], ["Freund", "Yoav", ""]]}, {"id": "1604.03343", "submitter": "Jan Leike", "authors": "Daniel Filan, Marcus Hutter, Jan Leike", "title": "Loss Bounds and Time Complexity for Speed Priors", "comments": "AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes for the first time the predictive performance of speed\npriors and their computational complexity. A speed prior is essentially a\nprobability distribution that puts low probability on strings that are not\nefficiently computable. We propose a variant to the original speed prior\n(Schmidhuber, 2002), and show that our prior can predict sequences drawn from\nprobability measures that are estimable in polynomial time. Our speed prior is\ncomputable in doubly-exponential time, but not in polynomial time. On a\npolynomial time computable sequence our speed prior is computable in\nexponential time. We show better upper complexity bounds for Schmidhuber's\nspeed prior under the same conditions, and that it predicts deterministic\nsequences that are computable in polynomial time; however, we also show that it\nis not computable in polynomial time, and the question of its predictive\nproperties for stochastic sequences remains open.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 11:26:12 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Filan", "Daniel", ""], ["Hutter", "Marcus", ""], ["Leike", "Jan", ""]]}, {"id": "1604.03346", "submitter": "Moacir Antonelli Ponti", "authors": "Moacir Ponti and Mateus Riva", "title": "An incremental linear-time learning algorithm for the Optimum-Path\n  Forest classifier", "comments": "submitted to IPL Journal for consideration in Nov/2016", "journal-ref": null, "doi": "10.1016/j.ipl.2017.05.004", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a classification method with incremental capabilities based on the\nOptimum-Path Forest classifier (OPF). The OPF considers instances as nodes of a\nfully-connected training graph, arc weights represent distances between two\nfeature vectors. Our algorithm includes new instances in an OPF in linear-time,\nwhile keeping similar accuracies when compared with the original quadratic-time\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 11:31:23 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 15:47:21 GMT"}, {"version": "v3", "created": "Fri, 29 Jul 2016 15:54:04 GMT"}, {"version": "v4", "created": "Fri, 12 Aug 2016 13:14:10 GMT"}, {"version": "v5", "created": "Wed, 23 Nov 2016 12:08:23 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Ponti", "Moacir", ""], ["Riva", "Mateus", ""]]}, {"id": "1604.03348", "submitter": "Zhi-Hua Zhou", "authors": "Teng Zhang and Zhi-Hua Zhou", "title": "Optimal Margin Distribution Machine", "comments": "arXiv admin note: substantial text overlap with arXiv:1311.0989", "journal-ref": "IJCAI 2018", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machine (SVM) has been one of the most popular learning\nalgorithms, with the central idea of maximizing the minimum margin, i.e., the\nsmallest distance from the instances to the classification boundary. Recent\ntheoretical results, however, disclosed that maximizing the minimum margin does\nnot necessarily lead to better generalization performances, and instead, the\nmargin distribution has been proven to be more crucial. Based on this idea, we\npropose a new method, named Optimal margin Distribution Machine (ODM), which\ntries to achieve a better generalization performance by optimizing the margin\ndistribution. We characterize the margin distribution by the first- and\nsecond-order statistics, i.e., the margin mean and variance. The proposed\nmethod is a general learning approach which can be used in any place where SVM\ncan be applied, and their superiority is verified both theoretically and\nempirically in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 11:39:16 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhang", "Teng", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1604.03373", "submitter": "Jiaqian Yu", "authors": "Jiaqian Yu (CVC, GALEN), Matthew Blaschko", "title": "A Convex Surrogate Operator for General Non-Modular Loss Functions", "comments": "in The 19th International Conference on Artificial Intelligence and\n  Statistics, May 2016, Cadiz, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical risk minimization frequently employs convex surrogates to\nunderlying discrete loss functions in order to achieve computational\ntractability during optimization. However, classical convex surrogates can only\ntightly bound modular loss functions, sub-modular functions or supermodular\nfunctions separately while maintaining polynomial time computation. In this\nwork, a novel generic convex surrogate for general non-modular loss functions\nis introduced, which provides for the first time a tractable solution for loss\nfunctions that are neither super-modular nor submodular. This convex surro-gate\nis based on a submodular-supermodular decomposition for which the existence and\nuniqueness is proven in this paper. It takes the sum of two convex surrogates\nthat separately bound the supermodular component and the submodular component\nusing slack-rescaling and the Lov{\\'a}sz hinge, respectively. It is further\nproven that this surrogate is convex , piecewise linear, an extension of the\nloss function, and for which subgradient computation is polynomial time.\nEmpirical results are reported on a non-submodular loss based on the\nS{{\\o}}rensen-Dice difference function, and a real-world face track dataset\nwith tens of thousands of frames, demonstrating the improved performance,\nefficiency, and scalabil-ity of the novel convex surrogate.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 12:31:59 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Yu", "Jiaqian", "", "CVC, GALEN"], ["Blaschko", "Matthew", ""]]}, {"id": "1604.03390", "submitter": "Marc Bola\\~nos", "authors": "\\'Alvaro Peris, Marc Bola\\~nos, Petia Radeva and Francisco Casacuberta", "title": "Video Description using Bidirectional Recurrent Neural Networks", "comments": "8 pages, 3 figures, 1 table, Submitted to International Conference on\n  Artificial Neural Networks (ICANN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although traditionally used in the machine translation field, the\nencoder-decoder framework has been recently applied for the generation of video\nand image descriptions. The combination of Convolutional and Recurrent Neural\nNetworks in these models has proven to outperform the previous state of the\nart, obtaining more accurate video descriptions. In this work we propose\npushing further this model by introducing two contributions into the encoding\nstage. First, producing richer image representations by combining object and\nlocation information from Convolutional Neural Networks and second, introducing\nBidirectional Recurrent Neural Networks for capturing both forward and backward\ntemporal relationships in the input frames.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 13:09:01 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 12:28:07 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Peris", "\u00c1lvaro", ""], ["Bola\u00f1os", "Marc", ""], ["Radeva", "Petia", ""], ["Casacuberta", "Francisco", ""]]}, {"id": "1604.03506", "submitter": "Liangjie Hong", "authors": "Liangjie Hong, Adnan Boz", "title": "An Unbiased Data Collection and Content Exploitation/Exploration\n  Strategy for Personalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of missions for personalization systems and recommender systems is to\nshow content items according to users' personal interests. In order to achieve\nsuch goal, these systems are learning user interests over time and trying to\npresent content items tailoring to user profiles. Recommending items according\nto users' preferences has been investigated extensively in the past few years,\nmainly thanks for the popularity of Netflix competition. In a real setting,\nusers may be attracted by a subset of those items and interact with them, only\nleaving partial feedbacks to the system to learn in the next cycle, which leads\nto significant biases into systems and hence results in a situation where user\nengagement metrics cannot be improved over time. The problem is not just for\none component of the system. The data collected from users is usually used in\nmany different tasks, including learning ranking functions, building user\nprofiles and constructing content classifiers. Once the data is biased, all\nthese downstream use cases would be impacted as well. Therefore, it would be\nbeneficial to gather unbiased data through user interactions. Traditionally,\nunbiased data collection is done through showing items uniformly sampling from\nthe content pool. However, this simple scheme is not feasible as it risks user\nengagement metrics and it takes long time to gather user feedbacks. In this\npaper, we introduce a user-friendly unbiased data collection framework, by\nutilizing methods developed in the exploitation and exploration literature. We\ndiscuss how the framework is different from normal multi-armed bandit problems\nand why such method is needed. We layout a novel Thompson sampling for\nBernoulli ranked-list to effectively balance user experiences and data\ncollection. The proposed method is validated from a real bucket test and we\nshow strong results comparing to old algorithms\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 18:32:43 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Hong", "Liangjie", ""], ["Boz", "Adnan", ""]]}, {"id": "1604.03519", "submitter": "Hyungtae Lee", "authors": "Hyungtae Lee and Heesung Kwon", "title": "Going Deeper with Contextual CNN for Hyperspectral Image Classification", "comments": "14 pages", "journal-ref": null, "doi": "10.1109/TIP.2017.2725580", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a novel deep convolutional neural network (CNN)\nthat is deeper and wider than other existing deep networks for hyperspectral\nimage classification. Unlike current state-of-the-art approaches in CNN-based\nhyperspectral image classification, the proposed network, called contextual\ndeep CNN, can optimally explore local contextual interactions by jointly\nexploiting local spatio-spectral relationships of neighboring individual pixel\nvectors. The joint exploitation of the spatio-spectral information is achieved\nby a multi-scale convolutional filter bank used as an initial component of the\nproposed CNN pipeline. The initial spatial and spectral feature maps obtained\nfrom the multi-scale filter bank are then combined together to form a joint\nspatio-spectral feature map. The joint feature map representing rich spectral\nand spatial properties of the hyperspectral image is then fed through a fully\nconvolutional network that eventually predicts the corresponding label of each\npixel vector. The proposed approach is tested on three benchmark datasets: the\nIndian Pines dataset, the Salinas dataset and the University of Pavia dataset.\nPerformance comparison shows enhanced classification performance of the\nproposed approach over the current state-of-the-art on the three datasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 18:44:34 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 19:39:52 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 14:21:21 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Lee", "Hyungtae", ""], ["Kwon", "Heesung", ""]]}, {"id": "1604.03539", "submitter": "Ishan Misra", "authors": "Ishan Misra and Abhinav Shrivastava and Abhinav Gupta and Martial\n  Hebert", "title": "Cross-stitch Networks for Multi-task Learning", "comments": "To appear in CVPR 2016 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning in Convolutional Networks has displayed remarkable\nsuccess in the field of recognition. This success can be largely attributed to\nlearning shared representations from multiple supervisory tasks. However,\nexisting multi-task approaches rely on enumerating multiple network\narchitectures specific to the tasks at hand, that do not generalize. In this\npaper, we propose a principled approach to learn shared representations in\nConvNets using multi-task learning. Specifically, we propose a new sharing\nunit: \"cross-stitch\" unit. These units combine the activations from multiple\nnetworks and can be trained end-to-end. A network with cross-stitch units can\nlearn an optimal combination of shared and task-specific representations. Our\nproposed method generalizes across multiple tasks and shows dramatically\nimproved performance over baseline methods for categories with few training\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 19:43:25 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Misra", "Ishan", ""], ["Shrivastava", "Abhinav", ""], ["Gupta", "Abhinav", ""], ["Hebert", "Martial", ""]]}, {"id": "1604.03540", "submitter": "Abhinav Shrivastava", "authors": "Abhinav Shrivastava, Abhinav Gupta, Ross Girshick", "title": "Training Region-based Object Detectors with Online Hard Example Mining", "comments": "To appear in Proceedings of IEEE Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2016. (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of object detection has made significant advances riding on the\nwave of region-based ConvNets, but their training procedure still includes many\nheuristics and hyperparameters that are costly to tune. We present a simple yet\nsurprisingly effective online hard example mining (OHEM) algorithm for training\nregion-based ConvNet detectors. Our motivation is the same as it has always\nbeen -- detection datasets contain an overwhelming number of easy examples and\na small number of hard examples. Automatic selection of these hard examples can\nmake training more effective and efficient. OHEM is a simple and intuitive\nalgorithm that eliminates several heuristics and hyperparameters in common use.\nBut more importantly, it yields consistent and significant boosts in detection\nperformance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness\nincreases as datasets become larger and more difficult, as demonstrated by the\nresults on the MS COCO dataset. Moreover, combined with complementary advances\nin the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP on\nPASCAL VOC 2007 and 2012 respectively.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 19:44:13 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Shrivastava", "Abhinav", ""], ["Gupta", "Abhinav", ""], ["Girshick", "Ross", ""]]}, {"id": "1604.03584", "submitter": "Zhouyuan Huo", "authors": "Zhouyuan Huo, Heng Huang", "title": "Asynchronous Stochastic Gradient Descent with Variance Reduction for\n  Non-Convex Optimization", "comments": "V1,v2,v3 have been withdrawn due to reference issue, because arXiv\n  policy, we can't delete them. Please refer the newest version v4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first theoretical analysis on the convergence rate of the\nasynchronous stochastic variance reduced gradient (SVRG) descent algorithm on\nnon-convex optimization. Recent studies have shown that the asynchronous\nstochastic gradient descent (SGD) based algorithms with variance reduction\nconverge with a linear convergent rate on convex problems. However, there is no\nwork to analyze asynchronous SGD with variance reduction technique on\nnon-convex problem. In this paper, we study two asynchronous parallel\nimplementations of SVRG: one is on a distributed memory system and the other is\non a shared memory system. We provide the theoretical analysis that both\nalgorithms can obtain a convergence rate of $O(1/T)$, and linear speed up is\nachievable if the number of workers is upper bounded. V1,v2,v3 have been\nwithdrawn due to reference issue, please refer the newest version v4.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 21:02:38 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 02:54:36 GMT"}, {"version": "v3", "created": "Thu, 19 May 2016 16:58:17 GMT"}, {"version": "v4", "created": "Tue, 20 Dec 2016 05:44:39 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Huo", "Zhouyuan", ""], ["Huang", "Heng", ""]]}, {"id": "1604.03628", "submitter": "Jianwei Yang", "authors": "Jianwei Yang, Devi Parikh, Dhruv Batra", "title": "Joint Unsupervised Learning of Deep Representations and Image Clusters", "comments": "19 pages, 11 figures, 14 tables, 2016 IEEE Conference on Computer\n  Vision and Pattern Recognition (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a recurrent framework for Joint Unsupervised\nLEarning (JULE) of deep representations and image clusters. In our framework,\nsuccessive operations in a clustering algorithm are expressed as steps in a\nrecurrent process, stacked on top of representations output by a Convolutional\nNeural Network (CNN). During training, image clusters and representations are\nupdated jointly: image clustering is conducted in the forward pass, while\nrepresentation learning in the backward pass. Our key idea behind this\nframework is that good representations are beneficial to image clustering and\nclustering results provide supervisory signals to representation learning. By\nintegrating two processes into a single model with a unified weighted triplet\nloss and optimizing it end-to-end, we can obtain not only more powerful\nrepresentations, but also more precise image clusters. Extensive experiments\nshow that our method outperforms the state-of-the-art on image clustering\nacross a variety of image datasets. Moreover, the learned representations\ngeneralize well when transferred to other tasks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 01:24:59 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 19:45:59 GMT"}, {"version": "v3", "created": "Mon, 20 Jun 2016 19:56:16 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Yang", "Jianwei", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1604.03640", "submitter": "Qianli Liao", "authors": "Qianli Liao, Tomaso Poggio", "title": "Bridging the Gaps Between Residual Learning, Recurrent Neural Networks\n  and Visual Cortex", "comments": "This version was written in Sept. 2016. For April 2016 version see v1\n  below", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We discuss relations between Residual Networks (ResNet), Recurrent Neural\nNetworks (RNNs) and the primate visual cortex. We begin with the observation\nthat a special type of shallow RNN is exactly equivalent to a very deep ResNet\nwith weight sharing among the layers. A direct implementation of such a RNN,\nalthough having orders of magnitude fewer parameters, leads to a performance\nsimilar to the corresponding ResNet. We propose 1) a generalization of both RNN\nand ResNet architectures and 2) the conjecture that a class of moderately deep\nRNNs is a biologically-plausible model of the ventral stream in visual cortex.\nWe demonstrate the effectiveness of the architectures by testing them on the\nCIFAR-10 and ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 02:59:34 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 21:10:49 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Liao", "Qianli", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1604.03692", "submitter": "Tianmin Shu", "authors": "Tianmin Shu, M. S. Ryoo and Song-Chun Zhu", "title": "Learning Social Affordance for Human-Robot Interaction", "comments": "International Joint Conference on Artificial Intelligence (IJCAI),\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an approach for robot learning of social affordance\nfrom human activity videos. We consider the problem in the context of\nhuman-robot interaction: Our approach learns structural representations of\nhuman-human (and human-object-human) interactions, describing how body-parts of\neach agent move with respect to each other and what spatial relations they\nshould maintain to complete each sub-event (i.e., sub-goal). This enables the\nrobot to infer its own movement in reaction to the human body motion, allowing\nit to naturally replicate such interactions.\n  We introduce the representation of social affordance and propose a generative\nmodel for its weakly supervised learning from human demonstration videos. Our\napproach discovers critical steps (i.e., latent sub-events) in an interaction\nand the typical motion associated with them, learning what body-parts should be\ninvolved and how. The experimental results demonstrate that our Markov Chain\nMonte Carlo (MCMC) based learning algorithm automatically discovers\nsemantically meaningful interactive affordance from RGB-D videos, which allows\nus to generate appropriate full body motion for an agent.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 08:40:06 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2016 21:02:02 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Shu", "Tianmin", ""], ["Ryoo", "M. S.", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1604.03736", "submitter": "Sebastian Urban", "authors": "Wiebke K\\\"opp, Patrick van der Smagt, Sebastian Urban", "title": "A Differentiable Transition Between Additive and Multiplicative Neurons", "comments": "ICLR 2016 extended abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to combine both additive and multiplicative neural units\neither use a fixed assignment of operations or require discrete optimization to\ndetermine what function a neuron should perform. However, this leads to an\nextensive increase in the computational complexity of the training procedure.\n  We present a novel, parameterizable transfer function based on the\nmathematical concept of non-integer functional iteration that allows the\noperation each neuron performs to be smoothly and, most importantly,\ndifferentiablely adjusted between addition and multiplication. This allows the\ndecision between addition and multiplication to be integrated into the standard\nbackpropagation training procedure.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 12:38:27 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["K\u00f6pp", "Wiebke", ""], ["van der Smagt", "Patrick", ""], ["Urban", "Sebastian", ""]]}, {"id": "1604.03763", "submitter": "Shun Zheng", "authors": "Shun Zheng, Jialei Wang, Fen Xia, Wei Xu, Tong Zhang", "title": "A General Distributed Dual Coordinate Optimization Framework for\n  Regularized Loss Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern large-scale machine learning applications, the training data are\noften partitioned and stored on multiple machines. It is customary to employ\nthe \"data parallelism\" approach, where the aggregated training loss is\nminimized without moving data across machines. In this paper, we introduce a\nnovel distributed dual formulation for regularized loss minimization problems\nthat can directly handle data parallelism in the distributed setting. This\nformulation allows us to systematically derive dual coordinate optimization\nprocedures, which we refer to as Distributed Alternating Dual Maximization\n(DADM). The framework extends earlier studies described in (Boyd et al., 2011;\nMa et al., 2015a; Jaggi et al., 2014; Yang, 2013) and has rigorous theoretical\nanalyses. Moreover with the help of the new formulation, we develop the\naccelerated version of DADM (Acc-DADM) by generalizing the acceleration\ntechnique from (Shalev-Shwartz and Zhang, 2014) to the distributed setting. We\nalso provide theoretical results for the proposed accelerated version and the\nnew result improves previous ones (Yang, 2013; Ma et al., 2015a) whose runtimes\ngrow linearly on the condition number. Our empirical studies validate our\ntheory and show that our accelerated approach significantly improves the\nprevious state-of-the-art distributed dual coordinate optimization algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 13:33:32 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 15:00:24 GMT"}, {"version": "v3", "created": "Fri, 25 Aug 2017 02:42:32 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Zheng", "Shun", ""], ["Wang", "Jialei", ""], ["Xia", "Fen", ""], ["Xu", "Wei", ""], ["Zhang", "Tong", ""]]}, {"id": "1604.03829", "submitter": "Tarun Choubisa", "authors": "Raviteja Upadrashta, Tarun Choubisa, A. Praneeth, Tony G., Aswath V.\n  S., P. Vijay Kumar, Sripad Kowshik, Hari Prasad Gokul R, T. V. Prabhakar", "title": "Animation and Chirplet-Based Development of a PIR Sensor Array for\n  Intruder Classification in an Outdoor Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the development of a passive infra-red sensor tower\nplatform along with a classification algorithm to distinguish between human\nintrusion, animal intrusion and clutter arising from wind-blown vegetative\nmovement in an outdoor environment. The research was aimed at exploring the\npotential use of wireless sensor networks as an early-warning system to help\nmitigate human-wildlife conflicts occurring at the edge of a forest. There are\nthree important features to the development. Firstly, the sensor platform\nemploys multiple sensors arranged in the form of a two-dimensional array to\ngive it a key spatial-resolution capability that aids in classification.\nSecondly, given the challenges of collecting data involving animal intrusion,\nan Animation-based Simulation tool for Passive Infra-Red sEnsor (ASPIRE) was\ndeveloped that simulates signals corresponding to human and animal intrusion\nand some limited models of vegetative clutter. This speeded up the process of\nalgorithm development by allowing us to test different hypotheses in a\ntime-efficient manner. Finally, a chirplet-based model for intruder signal was\ndeveloped that significantly helped boost classification accuracy despite\ndrawing data from a smaller number of sensors. An SVM-based classifier was used\nwhich made use of chirplet, energy and signal cross-correlation-based features.\nThe average accuracy obtained for intruder detection and classification on\nreal-world and simulated data sets was in excess of 97%.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 15:26:32 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Upadrashta", "Raviteja", ""], ["Choubisa", "Tarun", ""], ["Praneeth", "A.", ""], ["G.", "Tony", ""], ["S.", "Aswath V.", ""], ["Kumar", "P. Vijay", ""], ["Kowshik", "Sripad", ""], ["R", "Hari Prasad Gokul", ""], ["Prabhakar", "T. V.", ""]]}, {"id": "1604.03853", "submitter": "Mehmet Basbug", "authors": "Mehmet E. Basbug, Barbara E. Engelhardt", "title": "Hierarchical Compound Poisson Factorization", "comments": "Will appear on Proceedings of the 33 rd International Conference on\n  Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Non-negative matrix factorization models based on a hierarchical\nGamma-Poisson structure capture user and item behavior effectively in extremely\nsparse data sets, making them the ideal choice for collaborative filtering\napplications. Hierarchical Poisson factorization (HPF) in particular has proved\nsuccessful for scalable recommendation systems with extreme sparsity. HPF,\nhowever, suffers from a tight coupling of sparsity model (absence of a rating)\nand response model (the value of the rating), which limits the expressiveness\nof the latter. Here, we introduce hierarchical compound Poisson factorization\n(HCPF) that has the favorable Gamma-Poisson structure and scalability of HPF to\nhigh-dimensional extremely sparse matrices. More importantly, HCPF decouples\nthe sparsity model from the response model, allowing us to choose the most\nsuitable distribution for the response. HCPF can capture binary, non-negative\ndiscrete, non-negative continuous, and zero-inflated continuous responses. We\ncompare HCPF with HPF on nine discrete and three continuous data sets and\nconclude that HCPF captures the relationship between sparsity and response\nbetter than HPF.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 16:12:01 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 11:09:19 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Basbug", "Mehmet E.", ""], ["Engelhardt", "Barbara E.", ""]]}, {"id": "1604.03912", "submitter": "Michael Herman", "authors": "Michael Herman, Tobias Gindele, J\\\"org Wagner, Felix Schmitt, Wolfram\n  Burgard", "title": "Inverse Reinforcement Learning with Simultaneous Estimation of Rewards\n  and Dynamics", "comments": "accepted to appear in AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse Reinforcement Learning (IRL) describes the problem of learning an\nunknown reward function of a Markov Decision Process (MDP) from observed\nbehavior of an agent. Since the agent's behavior originates in its policy and\nMDP policies depend on both the stochastic system dynamics as well as the\nreward function, the solution of the inverse problem is significantly\ninfluenced by both. Current IRL approaches assume that if the transition model\nis unknown, additional samples from the system's dynamics are accessible, or\nthe observed behavior provides enough samples of the system's dynamics to solve\nthe inverse problem accurately. These assumptions are often not satisfied. To\novercome this, we present a gradient-based IRL approach that simultaneously\nestimates the system's dynamics. By solving the combined optimization problem,\nour approach takes into account the bias of the demonstrations, which stems\nfrom the generating policy. The evaluation on a synthetic MDP and a transfer\nlearning task shows improvements regarding the sample efficiency as well as the\naccuracy of the estimated reward functions and transition models.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 19:06:41 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Herman", "Michael", ""], ["Gindele", "Tobias", ""], ["Wagner", "J\u00f6rg", ""], ["Schmitt", "Felix", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1604.03915", "submitter": "Jialei Wang", "authors": "Jialei Wang, Peder A. Olsen, Andrew R. Conn, Aurelie C. Lozano", "title": "Removing Clouds and Recovering Ground Observations in Satellite Image\n  Sequences via Temporally Contiguous Robust Matrix Completion", "comments": "To Appear In Conference on Computer Vision and Pattern Recognition\n  (CVPR 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of removing and replacing clouds in satellite image\nsequences, which has a wide range of applications in remote sensing. Our\napproach first detects and removes the cloud-contaminated part of the image\nsequences. It then recovers the missing scenes from the clean parts using the\nproposed \"TECROMAC\" (TEmporally Contiguous RObust MAtrix Completion) objective.\nThe objective function balances temporal smoothness with a low rank solution\nwhile staying close to the original observations. The matrix whose the rows are\npixels and columnsare days corresponding to the image, has low-rank because the\npixels reflect land-types such as vegetation, roads and lakes and there are\nrelatively few variations as a result. We provide efficient optimization\nalgorithms for TECROMAC, so we can exploit images containing millions of\npixels. Empirical results on real satellite image sequences, as well as\nsimulated data, demonstrate that our approach is able to recover underlying\nimages from heavily cloud-contaminated observations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 19:13:17 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Wang", "Jialei", ""], ["Olsen", "Peder A.", ""], ["Conn", "Andrew R.", ""], ["Lozano", "Aurelie C.", ""]]}, {"id": "1604.03924", "submitter": "Ryan Rogers", "authors": "Ryan Rogers, Aaron Roth, Adam Smith, Om Thakkar", "title": "Max-Information, Differential Privacy, and Post-Selection Hypothesis\n  Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we initiate a principled study of how the generalization\nproperties of approximate differential privacy can be used to perform adaptive\nhypothesis testing, while giving statistically valid $p$-value corrections. We\ndo this by observing that the guarantees of algorithms with bounded approximate\nmax-information are sufficient to correct the $p$-values of adaptively chosen\nhypotheses, and then by proving that algorithms that satisfy\n$(\\epsilon,\\delta)$-differential privacy have bounded approximate max\ninformation when their inputs are drawn from a product distribution. This\nsubstantially extends the known connection between differential privacy and\nmax-information, which previously was only known to hold for (pure)\n$(\\epsilon,0)$-differential privacy. It also extends our understanding of\nmax-information as a partially unifying measure controlling the generalization\nproperties of adaptive data analyses. We also show a lower bound, proving that\n(despite the strong composition properties of max-information), when data is\ndrawn from a product distribution, $(\\epsilon,\\delta)$-differentially private\nalgorithms can come first in a composition with other algorithms satisfying\nmax-information bounds, but not necessarily second if the composition is\nrequired to itself satisfy a nontrivial max-information bound. This, in\nparticular, implies that the connection between\n$(\\epsilon,\\delta)$-differential privacy and max-information holds only for\ninputs drawn from product distributions, unlike the connection between\n$(\\epsilon,0)$-differential privacy and max-information.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 19:44:04 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 15:00:56 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Rogers", "Ryan", ""], ["Roth", "Aaron", ""], ["Smith", "Adam", ""], ["Thakkar", "Om", ""]]}, {"id": "1604.03930", "submitter": "Chi Jin", "authors": "Rong Ge, Chi Jin, Sham M. Kakade, Praneeth Netrapalli, Aaron Sidford", "title": "Efficient Algorithms for Large-scale Generalized Eigenvector Computation\n  and Canonical Correlation Analysis", "comments": "International Conference on Machine Learning (ICML) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of canonical-correlation analysis (CCA)\n(Hotelling, 1936) and, more broadly, the generalized eigenvector problem for a\npair of symmetric matrices. These are two fundamental problems in data analysis\nand scientific computing with numerous applications in machine learning and\nstatistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009).\n  We provide simple iterative algorithms, with improved runtimes, for solving\nthese problems that are globally linearly convergent with moderate dependencies\non the condition numbers and eigenvalue gaps of the matrices involved.\n  We obtain our results by reducing CCA to the top-$k$ generalized eigenvector\nproblem. We solve this problem through a general framework that simply requires\nblack box access to an approximate linear system solver. Instantiating this\nframework with accelerated gradient descent we obtain a running time of\n$O(\\frac{z k \\sqrt{\\kappa}}{\\rho} \\log(1/\\epsilon) \\log\n\\left(k\\kappa/\\rho\\right))$ where $z$ is the total number of nonzero entries,\n$\\kappa$ is the condition number and $\\rho$ is the relative eigenvalue gap of\nthe appropriate matrices.\n  Our algorithm is linear in the input size and the number of components $k$ up\nto a $\\log(k)$ factor. This is essential for handling large-scale matrices that\nappear in practice. To the best of our knowledge this is the first such\nalgorithm with global linear convergence. We hope that our results prompt\nfurther research and ultimately improve the practical running time for\nperforming these important data analysis procedures on large data sets.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 19:57:46 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 18:03:11 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Ge", "Rong", ""], ["Jin", "Chi", ""], ["Kakade", "Sham M.", ""], ["Netrapalli", "Praneeth", ""], ["Sidford", "Aaron", ""]]}, {"id": "1604.03986", "submitter": "Yusen Zhan", "authors": "Yusen Zhan, Haitham Bou Ammar, Matthew E. taylor", "title": "Theoretically-Grounded Policy Advice from Multiple Teachers in\n  Reinforcement Learning Settings with Applications to Negative Transfer", "comments": "10 pages, 6 figures, IJCAI 2016 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy advice is a transfer learning method where a student agent is able to\nlearn faster via advice from a teacher. However, both this and other\nreinforcement learning transfer methods have little theoretical analysis. This\npaper formally defines a setting where multiple teacher agents can provide\nadvice to a student and introduces an algorithm to leverage both autonomous\nexploration and teacher's advice. Our regret bounds justify the intuition that\ngood teachers help while bad teachers hurt. Using our formalization, we are\nalso able to quantify, for the first time, when negative transfer can occur\nwithin such a reinforcement learning setting.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 22:13:52 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Zhan", "Yusen", ""], ["Ammar", "Haitham Bou", ""], ["taylor", "Matthew E.", ""]]}, {"id": "1604.04026", "submitter": "Nguyen Duy Khuong", "authors": "Duy Khuong Nguyen, Tu Bao Ho", "title": "Fast Parallel Randomized Algorithm for Nonnegative Matrix Factorization\n  with KL Divergence for Large Sparse Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Matrix Factorization (NMF) with Kullback-Leibler Divergence\n(NMF-KL) is one of the most significant NMF problems and equivalent to\nProbabilistic Latent Semantic Indexing (PLSI), which has been successfully\napplied in many applications. For sparse count data, a Poisson distribution and\nKL divergence provide sparse models and sparse representation, which describe\nthe random variation better than a normal distribution and Frobenius norm.\nSpecially, sparse models provide more concise understanding of the appearance\nof attributes over latent components, while sparse representation provides\nconcise interpretability of the contribution of latent components over\ninstances. However, minimizing NMF with KL divergence is much more difficult\nthan minimizing NMF with Frobenius norm; and sparse models, sparse\nrepresentation and fast algorithms for large sparse datasets are still\nchallenges for NMF with KL divergence. In this paper, we propose a fast\nparallel randomized coordinate descent algorithm having fast convergence for\nlarge sparse datasets to archive sparse models and sparse representation. The\nproposed algorithm's experimental results overperform the current studies' ones\nin this problem.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 03:40:35 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Nguyen", "Duy Khuong", ""], ["Ho", "Tu Bao", ""]]}, {"id": "1604.04029", "submitter": "Weixiang Shao", "authors": "Weixiang Shao and Jiawei Zhang and Lifang He and Philip S. Yu", "title": "Multi-Source Multi-View Clustering via Discrepancy Penalty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advance of technology, entities can be observed in multiple views.\nMultiple views containing different types of features can be used for\nclustering. Although multi-view clustering has been successfully applied in\nmany applications, the previous methods usually assume the complete instance\nmapping between different views. In many real-world applications, information\ncan be gathered from multiple sources, while each source can contain multiple\nviews, which are more cohesive for learning. The views under the same source\nare usually fully mapped, but they can be very heterogeneous. Moreover, the\nmappings between different sources are usually incomplete and partially\nobserved, which makes it more difficult to integrate all the views across\ndifferent sources. In this paper, we propose MMC (Multi-source Multi-view\nClustering), which is a framework based on collective spectral clustering with\na discrepancy penalty across sources, to tackle these challenges. MMC has\nseveral advantages compared with other existing methods. First, MMC can deal\nwith incomplete mapping between sources. Second, it considers the disagreements\nbetween sources while treating views in the same source as a cohesive set.\nThird, MMC also tries to infer the instance similarities across sources to\nenhance the clustering performance. Extensive experiments conducted on\nreal-world data demonstrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 04:02:47 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2016 03:00:30 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Shao", "Weixiang", ""], ["Zhang", "Jiawei", ""], ["He", "Lifang", ""], ["Yu", "Philip S.", ""]]}, {"id": "1604.04125", "submitter": "Farahnaz Ahmed Wick", "authors": "Farahnaz Ahmed Wick, Michael L. Wick, Marc Pomplun", "title": "Filling in the details: Perceiving from low fidelity images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans perceive their surroundings in great detail even though most of our\nvisual field is reduced to low-fidelity color-deprived (e.g. dichromatic) input\nby the retina. In contrast, most deep learning architectures are\ncomputationally wasteful in that they consider every part of the input when\nperforming an image processing task. Yet, the human visual system is able to\nperform visual reasoning despite having only a small fovea of high visual\nacuity. With this in mind, we wish to understand the extent to which\nconnectionist architectures are able to learn from and reason with low acuity,\ndistorted inputs. Specifically, we train autoencoders to generate full-detail\nimages from low-detail \"foveations\" of those images and then measure their\nability to reconstruct the full-detail images from the foveated versions. By\nvarying the type of foveation, we can study how well the architectures can cope\nwith various types of distortion. We find that the autoencoder compensates for\nlower detail by learning increasingly global feature functions. In many cases,\nthe learnt features are suitable for reconstructing the original full-detail\nimage. For example, we find that the networks accurately perceive color in the\nperiphery, even when 75\\% of the input is achromatic.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 12:10:23 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Wick", "Farahnaz Ahmed", ""], ["Wick", "Michael L.", ""], ["Pomplun", "Marc", ""]]}, {"id": "1604.04144", "submitter": "Jason Kuen", "authors": "Jason Kuen, Kian Ming Lim, Chin Poo Lee", "title": "Self-taught learning of a deep invariant representation for visual\n  tracking via temporal slowness principle", "comments": "Pattern Recognition (Elsevier), 2015", "journal-ref": null, "doi": "10.1016/j.patcog.2015.02.012", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual representation is crucial for a visual tracking method's performances.\nConventionally, visual representations adopted in visual tracking rely on\nhand-crafted computer vision descriptors. These descriptors were developed\ngenerically without considering tracking-specific information. In this paper,\nwe propose to learn complex-valued invariant representations from tracked\nsequential image patches, via strong temporal slowness constraint and stacked\nconvolutional autoencoders. The deep slow local representations are learned\noffline on unlabeled data and transferred to the observational model of our\nproposed tracker. The proposed observational model retains old training samples\nto alleviate drift, and collect negative samples which are coherent with\ntarget's motion pattern for better discriminative tracking. With the learned\nrepresentation and online training samples, a logistic regression classifier is\nadopted to distinguish target from background, and retrained online to adapt to\nappearance changes. Subsequently, the observational model is integrated into a\nparticle filter framework to peform visual tracking. Experimental results on\nvarious challenging benchmark sequences demonstrate that the proposed tracker\nperforms favourably against several state-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 13:12:07 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Kuen", "Jason", ""], ["Lim", "Kian Ming", ""], ["Lee", "Chin Poo", ""]]}, {"id": "1604.04182", "submitter": "Garrett Bernstein", "authors": "Garrett Bernstein and Daniel Sheldon", "title": "Consistently Estimating Markov Chains with Noisy Aggregate Data", "comments": "AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating the parameters of a time-homogeneous\nMarkov chain given only noisy, aggregate data. This arises when a population of\nindividuals behave independently according to a Markov chain, but individual\nsample paths cannot be observed due to limitations of the observation process\nor the need to protect privacy. Instead, only population-level counts of the\nnumber of individuals in each state at each time step are available. When these\ncounts are exact, a conditional least squares (CLS) estimator is known to be\nconsistent and asymptotically normal. We initiate the study of method of\nmoments estimators for this problem to handle the more realistic case when\nobservations are additionally corrupted by noise. We show that CLS can be\ninterpreted as a simple \"plug-in\" method of moments estimator. However, when\nobservations are noisy, it is not consistent because it fails to account for\nadditional variance introduced by the noise. We develop a new, simpler method\nof moments estimator that bypasses this problem and is consistent under noisy\nobservations.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 15:13:06 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Bernstein", "Garrett", ""], ["Sheldon", "Daniel", ""]]}, {"id": "1604.04213", "submitter": "Farshad Rassaei", "authors": "Marco Pellegrini and Farshad Rassaei", "title": "Modeling Electrical Daily Demand in Presence of PHEVs in Smart Grids\n  with Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replacing a portion of current light duty vehicles (LDV) with plug-in hybrid\nelectric vehicles (PHEVs) offers the possibility to reduce the dependence on\npetroleum fuels together with environmental and economic benefits. The charging\nactivity of PHEVs will certainly introduce new load to the power grid. In the\nframework of the development of a smarter grid, the primary focus of the\npresent study is to propose a model for the electrical daily demand in presence\nof PHEVs charging. Expected PHEV demand is modeled by the PHEV charging time\nand the starting time of charge according to real world data. A normal\ndistribution for starting time of charge is assumed. Several distributions for\ncharging time are considered: uniform distribution, Gaussian with positive\nsupport, Rician distribution and a non-uniform distribution coming from driving\npatterns in real-world data. We generate daily demand profiles by using\nreal-world residential profiles throughout 2014 in the presence of different\nexpected PHEV demand models. Support vector machines (SVMs), a set of\nsupervised machine learning models, are employed in order to find the best\nmodel to fit the data. SVMs with radial basis function (RBF) and polynomial\nkernels were tested. Model performances are evaluated by means of mean squared\nerror (MSE) and mean absolute percentage error (MAPE). Best results are\nobtained with RBF kernel: maximum (worst) values for MSE and MAPE were about\n2.89 10-8 and 0.023, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 16:38:22 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Pellegrini", "Marco", ""], ["Rassaei", "Farshad", ""]]}, {"id": "1604.04326", "submitter": "Stephan Zheng", "authors": "Stephan Zheng, Yang Song, Thomas Leung, Ian Goodfellow", "title": "Improving the Robustness of Deep Neural Networks via Stability Training", "comments": "Published in CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the issue of output instability of deep neural\nnetworks: small perturbations in the visual input can significantly distort the\nfeature embeddings and output of a neural network. Such instability affects\nmany deep architectures with state-of-the-art performance on a wide range of\ncomputer vision tasks. We present a general stability training method to\nstabilize deep networks against small input distortions that result from\nvarious types of common image processing, such as compression, rescaling, and\ncropping. We validate our method by stabilizing the state-of-the-art Inception\narchitecture against these types of distortions. In addition, we demonstrate\nthat our stabilized model gives robust state-of-the-art performance on\nlarge-scale near-duplicate detection, similar-image ranking, and classification\non noisy datasets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 01:15:18 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Zheng", "Stephan", ""], ["Song", "Yang", ""], ["Leung", "Thomas", ""], ["Goodfellow", "Ian", ""]]}, {"id": "1604.04348", "submitter": "Fei Wen", "authors": "Fei Wen, Yuan Yang, Peilin Liu, and Robert C. Qiu", "title": "Positive Definite Estimation of Large Covariance Matrix Using\n  Generalized Nonconvex Penalties", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the issue of large covariance matrix estimation in\nhigh-dimensional statistical analysis. Recently, improved iterative algorithms\nwith positive-definite guarantee have been developed. However, these algorithms\ncannot be directly extended to use a nonconvex penalty for sparsity inducing.\nGenerally, a nonconvex penalty has the capability of ameliorating the bias\nproblem of the popular convex lasso penalty, and thus is more advantageous. In\nthis work, we propose a class of positive-definite covariance estimators using\ngeneralized nonconvex penalties. We develop a first-order algorithm based on\nthe alternating direction method framework to solve the nonconvex optimization\nproblem efficiently. The convergence of this algorithm has been proved.\nFurther, the statistical properties of the new estimators have been analyzed\nfor generalized nonconvex penalties. Moreover, extension of this algorithm to\ncovariance estimation from sketched measurements has been considered. The\nperformances of the new estimators have been demonstrated by both a simulation\nstudy and a gene clustering example for tumor tissues. Code for the proposed\nestimators is available at https://github.com/FWen/Nonconvex-PDLCE.git.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 03:50:57 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 03:58:42 GMT"}, {"version": "v3", "created": "Thu, 28 Jul 2016 13:23:57 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Wen", "Fei", ""], ["Yang", "Yuan", ""], ["Liu", "Peilin", ""], ["Qiu", "Robert C.", ""]]}, {"id": "1604.04378", "submitter": "Shengxian Wan", "authors": "Shengxian Wan, Yanyan Lan, Jun Xu, Jiafeng Guo, Liang Pang, Xueqi\n  Cheng", "title": "Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN", "comments": "Accepted by IJCAI-2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic matching, which aims to determine the matching degree between two\ntexts, is a fundamental problem for many NLP applications. Recently, deep\nlearning approach has been applied to this problem and significant improvements\nhave been achieved. In this paper, we propose to view the generation of the\nglobal interaction between two texts as a recursive process: i.e. the\ninteraction of two texts at each position is a composition of the interactions\nbetween their prefixes as well as the word level interaction at the current\nposition. Based on this idea, we propose a novel deep architecture, namely\nMatch-SRNN, to model the recursive matching structure. Firstly, a tensor is\nconstructed to capture the word level interactions. Then a spatial RNN is\napplied to integrate the local interactions recursively, with importance\ndetermined by four types of gates. Finally, the matching score is calculated\nbased on the global interaction. We show that, after degenerated to the exact\nmatching scenario, Match-SRNN can approximate the dynamic programming process\nof longest common subsequence. Thus, there exists a clear interpretation for\nMatch-SRNN. Our experiments on two semantic matching tasks showed the\neffectiveness of Match-SRNN, and its ability of visualizing the learned\nmatching structure.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 07:23:53 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Wan", "Shengxian", ""], ["Lan", "Yanyan", ""], ["Xu", "Jun", ""], ["Guo", "Jiafeng", ""], ["Pang", "Liang", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1604.04428", "submitter": "Harm Berntsen", "authors": "Harm Berntsen, Wouter Kuijper and Tom Heskes", "title": "The Artificial Mind's Eye: Resisting Adversarials for Convolutional\n  Neural Networks using Internal Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel artificial neural network architecture that integrates\nrobustness to adversarial input in the network structure. The main idea of our\napproach is to force the network to make predictions on what the given instance\nof the class under consideration would look like and subsequently test those\npredictions. By forcing the network to redraw the relevant parts of the image\nand subsequently comparing this new image to the original, we are having the\nnetwork give a \"proof\" of the presence of the object.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 11:07:45 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 15:18:56 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Berntsen", "Harm", ""], ["Kuijper", "Wouter", ""], ["Heskes", "Tom", ""]]}, {"id": "1604.04434", "submitter": "Chaobing Song", "authors": "Chaobing Song, Shu-Tao Xia", "title": "Bayesian linear regression with Student-t assumptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an automatic method of determining model complexity using the training\ndata alone, Bayesian linear regression provides us a principled way to select\nhyperparameters. But one often needs approximation inference if distribution\nassumption is beyond Gaussian distribution. In this paper, we propose a\nBayesian linear regression model with Student-t assumptions (BLRS), which can\nbe inferred exactly. In this framework, both conjugate prior and expectation\nmaximization (EM) algorithm are generalized. Meanwhile, we prove that the\nmaximum likelihood solution is equivalent to the standard Bayesian linear\nregression with Gaussian assumptions (BLRG). The $q$-EM algorithm for BLRS is\nnearly identical to the EM algorithm for BLRG. It is showed that $q$-EM for\nBLRS can converge faster than EM for BLRG for the task of predicting online\nnews popularity.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 11:21:27 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Song", "Chaobing", ""], ["Xia", "Shu-Tao", ""]]}, {"id": "1604.04451", "submitter": "Josef Kittler", "authors": "Josef Kittler and Cemre Zor", "title": "Delta divergence: A novel decision cognizant measure of classifier\n  incongruence", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disagreement between two classifiers regarding the class membership of an\nobservation in pattern recognition can be indicative of an anomaly and its\nnuance. As in general classifiers base their decision on class aposteriori\nprobabilities, the most natural approach to detecting classifier incongruence\nis to use divergence. However, existing divergences are not particularly\nsuitable to gauge classifier incongruence. In this paper, we postulate the\nproperties that a divergence measure should satisfy and propose a novel\ndivergence measure, referred to as Delta divergence. In contrast to existing\nmeasures, it is decision cognizant. The focus in Delta divergence on the\ndominant hypotheses has a clutter reducing property, the significance of which\ngrows with increasing number of classes. The proposed measure satisfies other\nimportant properties such as symmetry, and independence of classifier\nconfidence. The relationship of the proposed divergence to some baseline\nmeasures is demonstrated experimentally, showing its superiority.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 12:06:48 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 13:18:52 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Kittler", "Josef", ""], ["Zor", "Cemre", ""]]}, {"id": "1604.04505", "submitter": "Andreas Christmann", "authors": "Andreas Christmann, Florian Dumpert, and Dao-Hong Xiang", "title": "A short note on extension theorems and their connection to universal\n  consistency in machine learning", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical machine learning plays an important role in modern statistics and\ncomputer science. One main goal of statistical machine learning is to provide\nuniversally consistent algorithms, i.e., the estimator converges in probability\nor in some stronger sense to the Bayes risk or to the Bayes decision function.\nKernel methods based on minimizing the regularized risk over a reproducing\nkernel Hilbert space (RKHS) belong to these statistical machine learning\nmethods. It is in general unknown which kernel yields optimal results for a\nparticular data set or for the unknown probability measure. Hence various\nkernel learning methods were proposed to choose the kernel and therefore also\nits RKHS in a data adaptive manner. Nevertheless, many practitioners often use\nthe classical Gaussian RBF kernel or certain Sobolev kernels with good success.\nThe goal of this short note is to offer one possible theoretical explanation\nfor this empirical fact.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 13:51:57 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Christmann", "Andreas", ""], ["Dumpert", "Florian", ""], ["Xiang", "Dao-Hong", ""]]}, {"id": "1604.04528", "submitter": "Youngbin Park", "authors": "Youngbin Park, Sungphill Moon and Il Hong Suh", "title": "Tracking Human-like Natural Motion Using Deep Recurrent Neural Networks", "comments": "submitted to ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kinect skeleton tracker is able to achieve considerable human body tracking\nperformance in convenient and a low-cost manner. However, The tracker often\ncaptures unnatural human poses such as discontinuous and vibrated motions when\nself-occlusions occur. A majority of approaches tackle this problem by using\nmultiple Kinect sensors in a workspace. Combination of the measurements from\ndifferent sensors is then conducted in Kalman filter framework or optimization\nproblem is formulated for sensor fusion. However, these methods usually require\nheuristics to measure reliability of measurements observed from each Kinect\nsensor. In this paper, we developed a method to improve Kinect skeleton using\nsingle Kinect sensor, in which supervised learning technique was employed to\ncorrect unnatural tracking motions. Specifically, deep recurrent neural\nnetworks were used for improving joint positions and velocities of Kinect\nskeleton, and three methods were proposed to integrate the refined positions\nand velocities for further enhancement. Moreover, we suggested a novel measure\nto evaluate naturalness of captured motions. We evaluated the proposed approach\nby comparison with the ground truth obtained using a commercial optical\nmaker-based motion capture system.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 14:55:27 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Park", "Youngbin", ""], ["Moon", "Sungphill", ""], ["Suh", "Il Hong", ""]]}, {"id": "1604.04558", "submitter": "Jyothi Korra", "authors": "Jinju Joby and Jyothi Korra", "title": "Accessing accurate documents by mining auxiliary document information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Earlier techniques of text mining included algorithms like k-means, Naive\nBayes, SVM which classify and cluster the text document for mining relevant\ninformation about the documents. The need for improving the mining techniques\nhas us searching for techniques using the available algorithms. This paper\nproposes one technique which uses the auxiliary information that is present\ninside the text documents to improve the mining. This auxiliary information can\nbe a description to the content. This information can be either useful or\ncompletely useless for mining. The user should assess the worth of the\nauxiliary information before considering this technique for text mining. In\nthis paper, a combination of classical clustering algorithms is used to mine\nthe datasets. The algorithm runs in two stages which carry out mining at\ndifferent levels of abstraction. The clustered documents would then be\nclassified based on the necessary groups. The proposed technique is aimed at\nimproved results of document clustering.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 16:27:38 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Joby", "Jinju", ""], ["Korra", "Jyothi", ""]]}, {"id": "1604.04573", "submitter": "Jiang Wang Mr.", "authors": "Jiang Wang, Yi Yang, Junhua Mao, Zhiheng Huang, Chang Huang, Wei Xu", "title": "CNN-RNN: A Unified Framework for Multi-label Image Classification", "comments": "CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep convolutional neural networks (CNNs) have shown a great success in\nsingle-label image classification, it is important to note that real world\nimages generally contain multiple labels, which could correspond to different\nobjects, scenes, actions and attributes in an image. Traditional approaches to\nmulti-label image classification learn independent classifiers for each\ncategory and employ ranking or thresholding on the classification results.\nThese techniques, although working well, fail to explicitly exploit the label\ndependencies in an image. In this paper, we utilize recurrent neural networks\n(RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNN\nframework learns a joint image-label embedding to characterize the semantic\nlabel dependency as well as the image-label relevance, and it can be trained\nend-to-end from scratch to integrate both information in a unified framework.\nExperimental results on public benchmark datasets demonstrate that the proposed\narchitecture achieves better performance than the state-of-the-art multi-label\nclassification model\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 17:10:54 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Wang", "Jiang", ""], ["Yang", "Yi", ""], ["Mao", "Junhua", ""], ["Huang", "Zhiheng", ""], ["Huang", "Chang", ""], ["Xu", "Wei", ""]]}, {"id": "1604.04618", "submitter": "Thomas Steinke", "authors": "Mark Bun, Thomas Steinke, Jonathan Ullman", "title": "Make Up Your Mind: The Price of Online Queries in Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of answering queries about a sensitive dataset\nsubject to differential privacy. The queries may be chosen adversarially from a\nlarger set Q of allowable queries in one of three ways, which we list in order\nfrom easiest to hardest to answer:\n  Offline: The queries are chosen all at once and the differentially private\nmechanism answers the queries in a single batch.\n  Online: The queries are chosen all at once, but the mechanism only receives\nthe queries in a streaming fashion and must answer each query before seeing the\nnext query.\n  Adaptive: The queries are chosen one at a time and the mechanism must answer\neach query before the next query is chosen. In particular, each query may\ndepend on the answers given to previous queries.\n  Many differentially private mechanisms are just as efficient in the adaptive\nmodel as they are in the offline model. Meanwhile, most lower bounds for\ndifferential privacy hold in the offline setting. This suggests that the three\nmodels may be equivalent.\n  We prove that these models are all, in fact, distinct. Specifically, we show\nthat there is a family of statistical queries such that exponentially more\nqueries from this family can be answered in the offline model than in the\nonline model. We also exhibit a family of search queries such that\nexponentially more queries from this family can be answered in the online model\nthan in the adaptive model. We also investigate whether such separations might\nhold for simple queries like threshold queries over the real line.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 19:55:26 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Bun", "Mark", ""], ["Steinke", "Thomas", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1604.04639", "submitter": "Dylan Hutchison", "authors": "Dylan Hutchison", "title": "ModelWizard: Toward Interactive Model Construction", "comments": "Master's Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data scientists engage in model construction to discover machine learning\nmodels that well explain a dataset, in terms of predictiveness,\nunderstandability and generalization across domains. Questions such as \"what if\nwe model common cause Z\" and \"what if Y's dependence on X reverses\" inspire\nmany candidate models to consider and compare, yet current tools emphasize\nconstructing a final model all at once.\n  To more naturally reflect exploration when debating numerous models, we\npropose an interactive model construction framework grounded in composable\noperations. Primitive operations capture core steps refining data and model\nthat, when verified, form an inductive basis to prove model validity. Derived,\ncomposite operations enable advanced model families, both generic and\nspecialized, abstracted away from low-level details.\n  We prototype our envisioned framework in ModelWizard, a domain-specific\nlanguage embedded in F# to construct Tabular models. We enumerate language\ndesign and demonstrate its use through several applications, emphasizing how\nlanguage may facilitate creation of complex models. To future engineers\ndesigning data science languages and tools, we offer ModelWizard's design as a\nnew model construction paradigm, speeding discovery of our universe's\nstructure.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 20:43:20 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Hutchison", "Dylan", ""]]}, {"id": "1604.04696", "submitter": "Joachim Mathiesen", "authors": "Bjarke M{\\o}nsted, Anders Mollgaard, Joachim Mathiesen", "title": "Phone-based Metric as a Predictor for Basic Personality Traits", "comments": null, "journal-ref": null, "doi": "10.1016/j.jrp.2017.12.004", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basic personality traits are typically assessed through questionnaires. Here\nwe consider phone-based metrics as a way to asses personality traits. We use\ndata from smartphones with custom data-collection software distributed to 730\nindividuals. The data includes information about location, physical motion,\nface-to-face contacts, online social network friends, text messages and calls.\nThe data is further complemented by questionnaire-based data on basic\npersonality traits. From the phone-based metrics, we define a set of behavioral\nvariables, which we use in a prediction of basic personality traits. We find\nthat predominantly, the Big Five personality traits extraversion and, to some\ndegree, neuroticism are strongly expressed in our data. As an alternative to\nthe Big Five, we investigate whether other linear combinations of the 44\nquestions underlying the Big Five Inventory are more predictable. In a tertile\nclassification problem, basic dimensionality reduction techniques, such as\nindependent component analysis, increase the predictability relative to the\nbaseline from $11\\%$ to $23\\%$. Finally, from a supervised linear classifier,\nwe were able to further improve this predictability to $33\\%$. In all cases,\nthe most predictable projections had an overweight of the questions related to\nextraversion and neuroticism. In addition, our findings indicate that the score\nsystem underlying the Big Five Inventory disregards a part of the information\navailable in the 44 questions.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 05:44:47 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["M\u00f8nsted", "Bjarke", ""], ["Mollgaard", "Anders", ""], ["Mathiesen", "Joachim", ""]]}, {"id": "1604.04706", "submitter": "Parameswaran Raman", "authors": "Parameswaran Raman, Sriram Srinivasan, Shin Matsushima, Xinhua Zhang,\n  Hyokun Yun, S.V.N. Vishwanathan", "title": "DS-MLR: Exploiting Double Separability for Scaling up Distributed\n  Multinomial Logistic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling multinomial logistic regression to datasets with very large number of\ndata points and classes is challenging. This is primarily because one needs to\ncompute the log-partition function on every data point. This makes distributing\nthe computation hard. In this paper, we present a distributed stochastic\ngradient descent based optimization method (DS-MLR) for scaling up multinomial\nlogistic regression problems to massive scale datasets without hitting any\nstorage constraints on the data and model parameters. Our algorithm exploits\ndouble-separability, an attractive property that allows us to achieve both data\nas well as model parallelism simultaneously. In addition, we introduce a\nnon-blocking and asynchronous variant of our algorithm that avoids\nbulk-synchronization. We demonstrate the versatility of DS-MLR to various\nscenarios in data and model parallelism, through an extensive empirical study\nusing several real-world datasets. In particular, we demonstrate the\nscalability of DS-MLR by solving an extreme multi-class classification problem\non the Reddit dataset (159 GB data, 358 GB parameters) where, to the best of\nour knowledge, no other existing methods apply.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 07:26:58 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 18:45:59 GMT"}, {"version": "v3", "created": "Tue, 23 May 2017 08:06:02 GMT"}, {"version": "v4", "created": "Thu, 15 Feb 2018 01:02:54 GMT"}, {"version": "v5", "created": "Wed, 18 Apr 2018 01:15:04 GMT"}, {"version": "v6", "created": "Mon, 21 May 2018 23:44:36 GMT"}, {"version": "v7", "created": "Fri, 3 Aug 2018 22:13:06 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Raman", "Parameswaran", ""], ["Srinivasan", "Sriram", ""], ["Matsushima", "Shin", ""], ["Zhang", "Xinhua", ""], ["Yun", "Hyokun", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1604.04767", "submitter": "Markus Thom", "authors": "Markus Thom, Matthias Rapp, G\\\"unther Palm", "title": "Efficient Dictionary Learning with Sparseness-Enforcing Projections", "comments": "The final publication is available at Springer via\n  http://dx.doi.org/10.1007/s11263-015-0799-8", "journal-ref": "International Journal of Computer Vision, vol. 114, no. 2, pp.\n  168-194, 2015", "doi": "10.1007/s11263-015-0799-8", "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning dictionaries suitable for sparse coding instead of using engineered\nbases has proven effective in a variety of image processing tasks. This paper\nstudies the optimization of dictionaries on image data where the representation\nis enforced to be explicitly sparse with respect to a smooth, normalized\nsparseness measure. This involves the computation of Euclidean projections onto\nlevel sets of the sparseness measure. While previous algorithms for this\noptimization problem had at least quasi-linear time complexity, here the first\nalgorithm with linear time complexity and constant space complexity is\nproposed. The key for this is the mathematically rigorous derivation of a\ncharacterization of the projection's result based on a soft-shrinkage function.\nThis theory is applied in an original algorithm called Easy Dictionary Learning\n(EZDL), which learns dictionaries with a simple and fast-to-compute\nHebbian-like learning rule. The new algorithm is efficient, expressive and\nparticularly simple to implement. It is demonstrated that despite its\nsimplicity, the proposed learning algorithm is able to generate a rich variety\nof dictionaries, in particular a topographic organization of atoms or separable\natoms. Further, the dictionaries are as expressive as those of benchmark\nlearning algorithms in terms of the reproduction quality on entire images, and\nresult in an equivalent denoising performance. EZDL learns approximately 30 %\nfaster than the already very efficient Online Dictionary Learning algorithm,\nand is therefore eligible for rapid data set analysis and problems with vast\nquantities of learning samples.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 15:42:12 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Thom", "Markus", ""], ["Rapp", "Matthias", ""], ["Palm", "G\u00fcnther", ""]]}, {"id": "1604.04802", "submitter": "Nazneen Fatema Rajani", "authors": "Nazneen Fatema Rajani and Raymond J. Mooney", "title": "Supervised and Unsupervised Ensembling for Knowledge Base Population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present results on combining supervised and unsupervised methods to\nensemble multiple systems for two popular Knowledge Base Population (KBP)\ntasks, Cold Start Slot Filling (CSSF) and Tri-lingual Entity Discovery and\nLinking (TEDL). We demonstrate that our combined system along with auxiliary\nfeatures outperforms the best performing system for both tasks in the 2015\ncompetition, several ensembling baselines, as well as the state-of-the-art\nstacking approach to ensembling KBP systems. The success of our technique on\ntwo different and challenging problems demonstrates the power and generality of\nour combined approach to ensembling.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 21:18:14 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Rajani", "Nazneen Fatema", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "1604.04812", "submitter": "Ehsan Hosseini-Asl", "authors": "Ehsan Hosseini-Asl", "title": "Structured Sparse Convolutional Autoencoder", "comments": "The paper need some improvements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to improve the feature learning in Convolutional Networks\n(Convnet) by capturing the structure of objects. A new sparsity function is\nimposed on the extracted featuremap to capture the structure and shape of the\nlearned object, extracting interpretable features to improve the prediction\nperformance. The proposed algorithm is based on organizing the activation\nwithin and across featuremap by constraining the node activities through\n$\\ell_{2}$ and $\\ell_{1}$ normalization in a structured form.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 00:26:57 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 17:40:29 GMT"}, {"version": "v3", "created": "Mon, 2 Jan 2017 18:33:43 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Hosseini-Asl", "Ehsan", ""]]}, {"id": "1604.04835", "submitter": "Han Xiao Bookman", "authors": "Han Xiao, Minlie Huang, Xiaoyan Zhu", "title": "SSP: Semantic Space Projection for Knowledge Graph Embedding with Text\n  Descriptions", "comments": "Submitted to AAAI.2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge representation is an important, long-history topic in AI, and there\nhave been a large amount of work for knowledge graph embedding which projects\nsymbolic entities and relations into low-dimensional, real-valued vector space.\nHowever, most embedding methods merely concentrate on data fitting and ignore\nthe explicit semantic expression, leading to uninterpretable representations.\nThus, traditional embedding methods have limited potentials for many\napplications such as question answering, and entity classification. To this\nend, this paper proposes a semantic representation method for knowledge graph\n\\textbf{(KSR)}, which imposes a two-level hierarchical generative process that\nglobally extracts many aspects and then locally assigns a specific category in\neach aspect for every triple. Since both aspects and categories are\nsemantics-relevant, the collection of categories in each aspect is treated as\nthe semantic representation of this triple. Extensive experiments justify our\nmodel outperforms other state-of-the-art baselines substantially.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 07:15:33 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 02:39:15 GMT"}, {"version": "v3", "created": "Sat, 17 Jun 2017 04:33:41 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Xiao", "Han", ""], ["Huang", "Minlie", ""], ["Zhu", "Xiaoyan", ""]]}, {"id": "1604.04879", "submitter": "Jorge Luis Rivero Jlrivero", "authors": "Jorge Luis Rivero Perez, Bernardete Ribeiro, Carlos Morell Perez", "title": "Mahalanobis Distance Metric Learning Algorithm for Instance-based Data\n  Stream Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the massive data challenges nowadays and the rapid growing of\ntechnology, stream mining has recently received considerable attention. To\naddress the large number of scenarios in which this phenomenon manifests itself\nsuitable tools are required in various research fields. Instance-based data\nstream algorithms generally employ the Euclidean distance for the\nclassification task underlying this problem. A novel way to look into this\nissue is to take advantage of a more flexible metric due to the increased\nrequirements imposed by the data stream scenario. In this paper we present a\nnew algorithm that learns a Mahalanobis metric using similarity and\ndissimilarity constraints in an online manner. This approach hybridizes a\nMahalanobis distance metric learning algorithm and a k-NN data stream\nclassification algorithm with concept drift detection. First, some basic\naspects of Mahalanobis distance metric learning are described taking into\naccount key properties as well as online distance metric learning algorithms.\nSecond, we implement specific evaluation methodologies and comparative metrics\nsuch as Q statistic for data stream classification algorithms. Finally, our\nalgorithm is evaluated on different datasets by comparing its results with one\nof the best instance-based data stream classification algorithm of the state of\nthe art. The results demonstrate that our proposal is better\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 15:01:51 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Perez", "Jorge Luis Rivero", ""], ["Ribeiro", "Bernardete", ""], ["Perez", "Carlos Morell", ""]]}, {"id": "1604.04893", "submitter": "Fouad Khan", "authors": "Fouad Khan", "title": "An Initial Seed Selection Algorithm for K-means Clustering of\n  Georeferenced Data to Improve Replicability of Cluster Assignments for\n  Mapping Application", "comments": "Applied Soft Computing 12 (2012)", "journal-ref": null, "doi": "10.1016/j.asoc.2012.07.021", "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-means is one of the most widely used clustering algorithms in various\ndisciplines, especially for large datasets. However the method is known to be\nhighly sensitive to initial seed selection of cluster centers. K-means++ has\nbeen proposed to overcome this problem and has been shown to have better\naccuracy and computational efficiency than k-means. In many clustering problems\nthough -such as when classifying georeferenced data for mapping applications-\nstandardization of clustering methodology, specifically, the ability to arrive\nat the same cluster assignment for every run of the method i.e. replicability\nof the methodology, may be of greater significance than any perceived measure\nof accuracy, especially when the solution is known to be non-unique, as in the\ncase of k-means clustering. Here we propose a simple initial seed selection\nalgorithm for k-means clustering along one attribute that draws initial cluster\nboundaries along the 'deepest valleys' or greatest gaps in dataset. Thus, it\nincorporates a measure to maximize distance between consecutive cluster centers\nwhich augments the conventional k-means optimization for minimum distance\nbetween cluster center and cluster members. Unlike existing initialization\nmethods, no additional parameters or degrees of freedom are introduced to the\nclustering algorithm. This improves the replicability of cluster assignments by\nas much as 100% over k-means and k-means++, virtually reducing the variance\nover different runs to zero, without introducing any additional parameters to\nthe clustering process. Further, the proposed method is more computationally\nefficient than k-means++ and in some cases, more accurate.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 16:25:15 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Khan", "Fouad", ""]]}, {"id": "1604.04939", "submitter": "Andreas Damianou Dr", "authors": "Andreas Damianou, Neil D. Lawrence, Carl Henrik Ek", "title": "Multi-view Learning as a Nonparametric Nonlinear Inter-Battery Factor\n  Analysis", "comments": "49 pages including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor analysis aims to determine latent factors, or traits, which summarize\na given data set. Inter-battery factor analysis extends this notion to multiple\nviews of the data. In this paper we show how a nonlinear, nonparametric version\nof these models can be recovered through the Gaussian process latent variable\nmodel. This gives us a flexible formalism for multi-view learning where the\nlatent variables can be used both for exploratory purposes and for learning\nrepresentations that enable efficient inference for ambiguous estimation tasks.\nLearning is performed in a Bayesian manner through the formulation of a\nvariational compression scheme which gives a rigorous lower bound on the log\nlikelihood. Our Bayesian framework provides strong regularization during\ntraining, allowing the structure of the latent space to be determined\nefficiently and automatically. We demonstrate this by producing the first (to\nour knowledge) published results of learning from dozens of views, even when\ndata is scarce. We further show experimental results on several different types\nof multi-view data sets and for different kinds of tasks, including exploratory\ndata analysis, generation, ambiguity modelling through latent priors and\nclassification.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 23:13:50 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Damianou", "Andreas", ""], ["Lawrence", "Neil D.", ""], ["Ek", "Carl Henrik", ""]]}, {"id": "1604.04942", "submitter": "Martha White", "authors": "Lei Le and Martha White", "title": "Identifying global optimality for dictionary learning", "comments": "Updates to previous version include a small modification to\n  Proposition 2, to only use normed regularizers, and a modification to the\n  main theorem (previously Theorem 13) to focus on the overcomplete, full rank\n  setting and to better characterize non-differentiable induced regularizers.\n  The theory has been significantly modified since version 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning new representations of input observations in machine learning is\noften tackled using a factorization of the data. For many such problems,\nincluding sparse coding and matrix completion, learning these factorizations\ncan be difficult, in terms of efficiency and to guarantee that the solution is\na global minimum. Recently, a general class of objectives have been\nintroduced-which we term induced dictionary learning models (DLMs)-that have an\ninduced convex form that enables global optimization. Though attractive\ntheoretically, this induced form is impractical, particularly for large or\ngrowing datasets. In this work, we investigate the use of practical alternating\nminimization algorithms for induced DLMs, that ensure convergence to global\noptima. We characterize the stationary points of these models, and, using these\ninsights, highlight practical choices for the objectives. We then provide\ntheoretical and empirical evidence that alternating minimization, from a random\ninitialization, converges to global minima for a large subclass of induced\nDLMs. In particular, we take advantage of the existence of the (potentially\nunknown) convex induced form, to identify when stationary points are global\nminima for the dictionary learning objective. We then provide an empirical\ninvestigation into practical optimization choices for using alternating\nminimization for induced DLMs, for both batch and stochastic gradient descent.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2016 23:46:04 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 01:13:04 GMT"}, {"version": "v3", "created": "Sun, 19 Feb 2017 16:10:17 GMT"}, {"version": "v4", "created": "Mon, 7 Aug 2017 02:04:52 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Le", "Lei", ""], ["White", "Martha", ""]]}, {"id": "1604.04960", "submitter": "Seungjin Choi", "authors": "Suwon Suh and Seungjin Choi", "title": "Gaussian Copula Variational Autoencoders for Mixed Data", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variational autoencoder (VAE) is a generative model with continuous\nlatent variables where a pair of probabilistic encoder (bottom-up) and decoder\n(top-down) is jointly learned by stochastic gradient variational Bayes. We\nfirst elaborate Gaussian VAE, approximating the local covariance matrix of the\ndecoder as an outer product of the principal direction at a position determined\nby a sample drawn from Gaussian distribution. We show that this model, referred\nto as VAE-ROC, better captures the data manifold, compared to the standard\nGaussian VAE where independent multivariate Gaussian was used to model the\ndecoder. Then we extend the VAE-ROC to handle mixed categorical and continuous\ndata. To this end, we employ Gaussian copula to model the local dependency in\nmixed categorical and continuous data, leading to {\\em Gaussian copula\nvariational autoencoder} (GCVAE). As in VAE-ROC, we use the rank-one\napproximation for the covariance in the Gaussian copula, to capture the local\ndependency structure in the mixed data. Experiments on various datasets\ndemonstrate the useful behaviour of VAE-ROC and GCVAE, compared to the standard\nVAE.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 02:14:07 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Suh", "Suwon", ""], ["Choi", "Seungjin", ""]]}, {"id": "1604.04970", "submitter": "Yueying Kao", "authors": "Yueying Kao, Ran He, Kaiqi Huang", "title": "Deep Aesthetic Quality Assessment with Semantic Information", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TIP.2017.2651399", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human beings often assess the aesthetic quality of an image coupled with the\nidentification of the image's semantic content. This paper addresses the\ncorrelation issue between automatic aesthetic quality assessment and semantic\nrecognition. We cast the assessment problem as the main task among a multi-task\ndeep model, and argue that semantic recognition task offers the key to address\nthis problem. Based on convolutional neural networks, we employ a single and\nsimple multi-task framework to efficiently utilize the supervision of aesthetic\nand semantic labels. A correlation item between these two tasks is further\nintroduced to the framework by incorporating the inter-task relationship\nlearning. This item not only provides some useful insight about the correlation\nbut also improves assessment accuracy of the aesthetic task. Particularly, an\neffective strategy is developed to keep a balance between the two tasks, which\nfacilitates to optimize the parameters of the framework. Extensive experiments\non the challenging AVA dataset and Photo.net dataset validate the importance of\nsemantic recognition in aesthetic quality assessment, and demonstrate that\nmulti-task deep models can discover an effective aesthetic representation to\nachieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 03:16:56 GMT"}, {"version": "v2", "created": "Sat, 20 Aug 2016 14:09:48 GMT"}, {"version": "v3", "created": "Fri, 21 Oct 2016 07:46:54 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Kao", "Yueying", ""], ["He", "Ran", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1604.05024", "submitter": "Ziqiang Shi", "authors": "Ziqiang Shi and Rujie Liu", "title": "Empirical study of PROXTONE and PROXTONE$^+$ for Fast Learning of Large\n  Scale Sparse Models", "comments": "arXiv admin note: text overlap with arXiv:1311.2115 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PROXTONE is a novel and fast method for optimization of large scale\nnon-smooth convex problem \\cite{shi2015large}. In this work, we try to use\nPROXTONE method in solving large scale \\emph{non-smooth non-convex} problems,\nfor example training of sparse deep neural network (sparse DNN) or sparse\nconvolutional neural network (sparse CNN) for embedded or mobile device.\nPROXTONE converges much faster than first order methods, while first order\nmethod is easy in deriving and controlling the sparseness of the solutions.\nThus in some applications, in order to train sparse models fast, we propose to\ncombine the merits of both methods, that is we use PROXTONE in the first\nseveral epochs to reach the neighborhood of an optimal solution, and then use\nthe first order method to explore the possibility of sparsity in the following\ntraining. We call such method PROXTONE plus (PROXTONE$^+$). Both PROXTONE and\nPROXTONE$^+$ are tested in our experiments, and which demonstrate both methods\nimproved convergence speed twice as fast at least on diverse sparse model\nlearning problems, and at the same time reduce the size to 0.5\\% for DNN\nmodels. The source of all the algorithms is available upon request.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 08:01:02 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Shi", "Ziqiang", ""], ["Liu", "Rujie", ""]]}, {"id": "1604.05085", "submitter": "Wojciech Ja\\'skowski", "authors": "Wojciech Ja\\'skowski", "title": "Mastering 2048 with Delayed Temporal Coherence Learning, Multi-Stage\n  Weight Promotion, Redundant Encoding and Carousel Shaping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  2048 is an engaging single-player, nondeterministic video puzzle game, which,\nthanks to the simple rules and hard-to-master gameplay, has gained massive\npopularity in recent years. As 2048 can be conveniently embedded into the\ndiscrete-state Markov decision processes framework, we treat it as a testbed\nfor evaluating existing and new methods in reinforcement learning. With the aim\nto develop a strong 2048 playing program, we employ temporal difference\nlearning with systematic n-tuple networks. We show that this basic method can\nbe significantly improved with temporal coherence learning, multi-stage\nfunction approximator with weight promotion, carousel shaping, and redundant\nencoding. In addition, we demonstrate how to take advantage of the\ncharacteristics of the n-tuple network, to improve the algorithmic\neffectiveness of the learning process by i) delaying the (decayed) update and\napplying lock-free optimistic parallelism to effortlessly make advantage of\nmultiple CPU cores. This way, we were able to develop the best known 2048\nplaying program to date, which confirms the effectiveness of the introduced\nmethods for discrete-state Markov decision problems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 11:06:32 GMT"}, {"version": "v2", "created": "Sun, 6 Nov 2016 13:18:39 GMT"}, {"version": "v3", "created": "Mon, 12 Dec 2016 12:54:36 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Ja\u015bkowski", "Wojciech", ""]]}, {"id": "1604.05091", "submitter": "Peter Ondruska", "authors": "Peter Ondruska, Julie Dequaire, Dominic Zeng Wang, Ingmar Posner", "title": "End-to-End Tracking and Semantic Segmentation Using Recurrent Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a novel end-to-end framework for tracking and\nclassifying a robot's surroundings in complex, dynamic and only partially\nobservable real-world environments. The approach deploys a recurrent neural\nnetwork to filter an input stream of raw laser measurements in order to\ndirectly infer object locations, along with their identity in both visible and\noccluded areas. To achieve this we first train the network using unsupervised\nDeep Tracking, a recently proposed theoretical framework for end-to-end space\noccupancy prediction. We show that by learning to track on a large amount of\nunsupervised data, the network creates a rich internal representation of its\nenvironment which we in turn exploit through the principle of inductive\ntransfer of knowledge to perform the task of it's semantic classification. As a\nresult, we show that only a small amount of labelled data suffices to steer the\nnetwork towards mastering this additional task. Furthermore we propose a novel\nrecurrent neural network architecture specifically tailored to tracking and\nsemantic classification in real-world robotics applications. We demonstrate the\ntracking and classification performance of the method on real-world data\ncollected at a busy road junction. Our evaluation shows that the proposed\nend-to-end framework compares favourably to a state-of-the-art, model-free\ntracking solution and that it outperforms a conventional one-shot training\nscheme for semantic classification.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 11:15:56 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2016 14:09:26 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Ondruska", "Peter", ""], ["Dequaire", "Julie", ""], ["Wang", "Dominic Zeng", ""], ["Posner", "Ingmar", ""]]}, {"id": "1604.05198", "submitter": "Baogang Hu", "authors": "Linlin Cao, Ran He, Bao-Gang Hu", "title": "Locally Imposing Function for Generalized Constraint Neural Networks - A\n  Study on Equality Constraints", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is a further study on the Generalized Constraint Neural Network\n(GCNN) model [1], [2]. Two challenges are encountered in the study, that is, to\nembed any type of prior information and to select its imposing schemes. The\nwork focuses on the second challenge and studies a new constraint imposing\nscheme for equality constraints. A new method called locally imposing function\n(LIF) is proposed to provide a local correction to the GCNN prediction\nfunction, which therefore falls within Locally Imposing Scheme (LIS). In\ncomparison, the conventional Lagrange multiplier method is considered as\nGlobally Imposing Scheme (GIS) because its added constraint term exhibits a\nglobal impact to its objective function. Two advantages are gained from LIS\nover GIS. First, LIS enables constraints to fire locally and explicitly in the\ndomain only where they need on the prediction function. Second, constraints can\nbe implemented within a network setting directly. We attempt to interpret\nseveral constraint methods graphically from a viewpoint of the locality\nprinciple. Numerical examples confirm the advantages of the proposed method. In\nsolving boundary value problems with Dirichlet and Neumann constraints, the\nGCNN model with LIF is possible to achieve an exact satisfaction of the\nconstraints.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 15:11:13 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Cao", "Linlin", ""], ["He", "Ran", ""], ["Hu", "Bao-Gang", ""]]}, {"id": "1604.05242", "submitter": "Dinesh Govindaraj", "authors": "Dinesh Govindaraj", "title": "Can Boosting with SVM as Week Learners Help?", "comments": "Work done in 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition in images involves identifying objects with partial\nocclusions, viewpoint changes, varying illumination, cluttered backgrounds.\nRecent work in object recognition uses machine learning techniques SVM-KNN,\nLocal Ensemble Kernel Learning, Multiple Kernel Learning. In this paper, we\nwant to utilize SVM as week learners in AdaBoost. Experiments are done with\nclassifiers like near- est neighbor, k-nearest neighbor, Support vector\nmachines, Local learning(SVM- KNN) and AdaBoost. Models use Scale-Invariant\ndescriptors and Pyramid his- togram of gradient descriptors. AdaBoost is\ntrained with set of week classifier as SVMs, each with kernel distance function\non different descriptors. Results shows AdaBoost with SVM outperform other\nmethods for Object Categorization dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 17:05:00 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 23:03:27 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Govindaraj", "Dinesh", ""]]}, {"id": "1604.05257", "submitter": "Sattar Vakili", "authors": "Sattar Vakili, Qing Zhao", "title": "Risk-Averse Multi-Armed Bandit Problems under Mean-Variance Measure", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2016.2592622", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-armed bandit problems have been studied mainly under the measure of\nexpected total reward accrued over a horizon of length $T$. In this paper, we\naddress the issue of risk in multi-armed bandit problems and develop parallel\nresults under the measure of mean-variance, a commonly adopted risk measure in\neconomics and mathematical finance. We show that the model-specific regret and\nthe model-independent regret in terms of the mean-variance of the reward\nprocess are lower bounded by $\\Omega(\\log T)$ and $\\Omega(T^{2/3})$,\nrespectively. We then show that variations of the UCB policy and the DSEE\npolicy developed for the classic risk-neutral MAB achieve these lower bounds.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 17:28:41 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 11:35:36 GMT"}, {"version": "v3", "created": "Mon, 14 Aug 2017 21:10:14 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Vakili", "Sattar", ""], ["Zhao", "Qing", ""]]}, {"id": "1604.05263", "submitter": "Alan Saul D", "authors": "Alan D. Saul, James Hensman, Aki Vehtari, Neil D. Lawrence", "title": "Chained Gaussian Processes", "comments": "Appearing in Proceedings of the 19th International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2016, Cadiz, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process models are flexible, Bayesian non-parametric approaches to\nregression. Properties of multivariate Gaussians mean that they can be combined\nlinearly in the manner of additive models and via a link function (like in\ngeneralized linear models) to handle non-Gaussian data. However, the link\nfunction formalism is restrictive, link functions are always invertible and\nmust convert a parameter of interest to a linear combination of the underlying\nprocesses. There are many likelihoods and models where a non-linear combination\nis more appropriate. We term these more general models Chained Gaussian\nProcesses: the transformation of the GPs to the likelihood parameters will not\ngenerally be invertible, and that implies that linearisation would only be\npossible with multiple (localized) links, i.e. a chain. We develop an\napproximate inference procedure for Chained GPs that is scalable and applicable\nto any factorized likelihood. We demonstrate the approximation on a range of\nlikelihood functions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 17:46:23 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Saul", "Alan D.", ""], ["Hensman", "James", ""], ["Vehtari", "Aki", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1604.05280", "submitter": "Scott Garrabrant", "authors": "Scott Garrabrant, Nate Soares, Jessica Taylor", "title": "Asymptotic Convergence in Online Learning with Unbounded Delays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of predicting the results of computations that are too\nexpensive to run, via the observation of the results of smaller computations.\nWe model this as an online learning problem with delayed feedback, where the\nlength of the delay is unbounded, which we study mainly in a stochastic\nsetting. We show that in this setting, consistency is not possible in general,\nand that optimal forecasters might not have average regret going to zero.\nHowever, it is still possible to give algorithms that converge asymptotically\nto Bayes-optimal predictions, by evaluating forecasters on specific sparse\nindependent subsequences of their predictions. We give an algorithm that does\nthis, which converges asymptotically on good behavior, and give very weak\nbounds on how long it takes to converge. We then relate our results back to the\nproblem of predicting large computations in a deterministic setting.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 19:04:59 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 02:10:26 GMT"}, {"version": "v3", "created": "Fri, 2 Sep 2016 02:21:21 GMT"}, {"version": "v4", "created": "Wed, 7 Sep 2016 18:43:24 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Garrabrant", "Scott", ""], ["Soares", "Nate", ""], ["Taylor", "Jessica", ""]]}, {"id": "1604.05288", "submitter": "Scott Garrabrant", "authors": "Scott Garrabrant, Benya Fallenstein, Abram Demski, Nate Soares", "title": "Inductive Coherence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While probability theory is normally applied to external environments, there\nhas been some recent interest in probabilistic modeling of the outputs of\ncomputations that are too expensive to run. Since mathematical logic is a\npowerful tool for reasoning about computer programs, we consider this problem\nfrom the perspective of integrating probability and logic. Recent work on\nassigning probabilities to mathematical statements has used the concept of\ncoherent distributions, which satisfy logical constraints such as the\nprobability of a sentence and its negation summing to one. Although there are\nalgorithms which converge to a coherent probability distribution in the limit,\nthis yields only weak guarantees about finite approximations of these\ndistributions. In our setting, this is a significant limitation: Coherent\ndistributions assign probability one to all statements provable in a specific\nlogical theory, such as Peano Arithmetic, which can prove what the output of\nany terminating computation is; thus, a coherent distribution must assign\nprobability one to the output of any terminating computation. To model\nuncertainty about computations, we propose to work with approximations to\ncoherent distributions. We introduce inductive coherence, a strengthening of\ncoherence that provides appropriate constraints on finite approximations, and\npropose an algorithm which satisfies this criterion.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 19:37:46 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 00:14:04 GMT"}, {"version": "v3", "created": "Fri, 7 Oct 2016 17:00:38 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Garrabrant", "Scott", ""], ["Fallenstein", "Benya", ""], ["Demski", "Abram", ""], ["Soares", "Nate", ""]]}, {"id": "1604.05307", "submitter": "Hemant Tyagi", "authors": "Hemant Tyagi, Anastasios Kyrillidis, Bernd G\\\"artner, Andreas Krause", "title": "Learning Sparse Additive Models with Interactions in High Dimensions", "comments": "23 pages, to appear in Proceedings of the 19th International\n  Conference on Artificial Intelligence and Statistics (AISTATS) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is referred to as a\nSparse Additive Model (SPAM), if it is of the form $f(\\mathbf{x}) = \\sum_{l \\in\n\\mathcal{S}}\\phi_{l}(x_l)$, where $\\mathcal{S} \\subset [d]$, $|\\mathcal{S}| \\ll\nd$. Assuming $\\phi_l$'s and $\\mathcal{S}$ to be unknown, the problem of\nestimating $f$ from its samples has been studied extensively. In this work, we\nconsider a generalized SPAM, allowing for second order interaction terms. For\nsome $\\mathcal{S}_1 \\subset [d], \\mathcal{S}_2 \\subset {[d] \\choose 2}$, the\nfunction $f$ is assumed to be of the form: $$f(\\mathbf{x}) = \\sum_{p \\in\n\\mathcal{S}_1}\\phi_{p} (x_p) + \\sum_{(l,l^{\\prime}) \\in\n\\mathcal{S}_2}\\phi_{(l,l^{\\prime})} (x_{l},x_{l^{\\prime}}).$$ Assuming\n$\\phi_{p},\\phi_{(l,l^{\\prime})}$, $\\mathcal{S}_1$ and, $\\mathcal{S}_2$ to be\nunknown, we provide a randomized algorithm that queries $f$ and exactly\nrecovers $\\mathcal{S}_1,\\mathcal{S}_2$. Consequently, this also enables us to\nestimate the underlying $\\phi_p, \\phi_{(l,l^{\\prime})}$. We derive sample\ncomplexity bounds for our scheme and also extend our analysis to include the\nsituation where the queries are corrupted with noise -- either stochastic, or\narbitrary but bounded. Lastly, we provide simulation results on synthetic data,\nthat validate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 17:09:48 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Tyagi", "Hemant", ""], ["Kyrillidis", "Anastasios", ""], ["G\u00e4rtner", "Bernd", ""], ["Krause", "Andreas", ""]]}, {"id": "1604.05377", "submitter": "Artit Wangperawong", "authors": "Artit Wangperawong, Cyrille Brun, Olav Laudy, Rujikorn Pavasuthipaisit", "title": "Churn analysis using deep convolutional neural networks and autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer temporal behavioral data was represented as images in order to\nperform churn prediction by leveraging deep learning architectures prominent in\nimage classification. Supervised learning was performed on labeled data of over\n6 million customers using deep convolutional neural networks, which achieved an\nAUC of 0.743 on the test dataset using no more than 12 temporal features for\neach customer. Unsupervised learning was conducted using autoencoders to better\nunderstand the reasons for customer churn. Images that maximally activate the\nhidden units of an autoencoder trained with churned customers reveal ample\nopportunities for action to be taken to prevent churn among strong data, no\nvoice users.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 23:18:23 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Wangperawong", "Artit", ""], ["Brun", "Cyrille", ""], ["Laudy", "Olav", ""], ["Pavasuthipaisit", "Rujikorn", ""]]}, {"id": "1604.05393", "submitter": "Fouad Khan", "authors": "Fouad Khan", "title": "An Adaptive Learning Mechanism for Selection of Increasingly More\n  Complex Systems", "comments": "(IJACSA), 6(6), 2015", "journal-ref": null, "doi": "10.14569/IJACSA.2015.060632", "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently it has been demonstrated that causal entropic forces can lead to the\nemergence of complex phenomena associated with human cognitive niche such as\ntool use and social cooperation. Here I show that even more fundamental traits\nassociated with human cognition such as 'self-awareness' can easily be\ndemonstrated to be arising out of merely a selection for 'better regulators';\ni.e. systems which respond comparatively better to threats to their existence\nwhich are internal to themselves. A simple model demonstrates how indeed the\naverage self-awareness for a universe of systems continues to rise as less\nself-aware systems are eliminated. The model also demonstrates however that the\nmaximum attainable self-awareness for any system is limited by the plasticity\nand energy availability for that typology of systems. I argue that this rise in\nself-awareness may be the reason why systems tend towards greater complexity.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 00:59:54 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Khan", "Fouad", ""]]}, {"id": "1604.05417", "submitter": "Swami Sankaranarayanan", "authors": "Swami Sankaranarayanan, Azadeh Alavi, Carlos Castillo, Rama Chellappa", "title": "Triplet Probabilistic Embedding for Face Verification and Clustering", "comments": "Oral Paper in BTAS 2016; NVIDIA Best paper Award\n  (http://ieee-biometrics.org/btas2016/awards.html)", "journal-ref": null, "doi": "10.1109/BTAS.2016.7791205", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress made over the past twenty five years,\nunconstrained face verification remains a challenging problem. This paper\nproposes an approach that couples a deep CNN-based approach with a\nlow-dimensional discriminative embedding learned using triplet probability\nconstraints to solve the unconstrained face verification problem. Aside from\nyielding performance improvements, this embedding provides significant\nadvantages in terms of memory and for post-processing operations like subject\nspecific clustering. Experiments on the challenging IJB-A dataset show that the\nproposed algorithm performs comparably or better than the state of the art\nmethods in verification and identification metrics, while requiring much less\ntraining data and training time. The superior performance of the proposed\nmethod on the CFP dataset shows that the representation learned by our deep CNN\nis robust to extreme pose variation. Furthermore, we demonstrate the robustness\nof the deep features to challenges including age, pose, blur and clutter by\nperforming simple clustering experiments on both IJB-A and LFW datasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 03:29:56 GMT"}, {"version": "v2", "created": "Sun, 8 May 2016 16:04:02 GMT"}, {"version": "v3", "created": "Wed, 18 Jan 2017 03:10:44 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Sankaranarayanan", "Swami", ""], ["Alavi", "Azadeh", ""], ["Castillo", "Carlos", ""], ["Chellappa", "Rama", ""]]}, {"id": "1604.05429", "submitter": "Nadia Kanwal", "authors": "Nadia Kanwal and Erkan Bostanci", "title": "Comparative Study of Instance Based Learning and Back Propagation for\n  Classification Problems", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a comparative study of the performance of Back Propagation\nand Instance Based Learning Algorithm for classification tasks. The study is\ncarried out by a series of experiments will all possible combinations of\nparameter values for the algorithms under evaluation. The algorithm's\nclassification accuracy is compared over a range of datasets and measurements\nlike Cross Validation, Kappa Statistics, Root Mean Squared Value and True\nPositive vs False Positive rate have been used to evaluate their performance.\nAlong with performance comparison, techniques of handling missing values have\nalso been compared that include Mean or Mode replacement and Multiple\nImputation. The results showed that parameter adjustment plays vital role in\nimproving an algorithm's accuracy and therefore, Back Propagation has shown\nbetter results as compared to Instance Based Learning. Furthermore, the problem\nof missing values was better handled by Multiple imputation method, however,\nnot suitable for less amount of data.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 04:31:55 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Kanwal", "Nadia", ""], ["Bostanci", "Erkan", ""]]}, {"id": "1604.05449", "submitter": "Dacheng Tao", "authors": "Shan You, Chang Xu, Yunhe Wang, Chao Xu and Dacheng Tao", "title": "Streaming Label Learning for Modeling Labels on the Fly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging to handle a large volume of labels in multi-label learning.\nHowever, existing approaches explicitly or implicitly assume that all the\nlabels in the learning process are given, which could be easily violated in\nchanging environments. In this paper, we define and study streaming label\nlearning (SLL), i.e., labels are arrived on the fly, to model newly arrived\nlabels with the help of the knowledge learned from past labels. The core of SLL\nis to explore and exploit the relationships between new labels and past labels\nand then inherit the relationship into hypotheses of labels to boost the\nperformance of new classifiers. In specific, we use the label\nself-representation to model the label relationship, and SLL will be divided\ninto two steps: a regression problem and a empirical risk minimization (ERM)\nproblem. Both problems are simple and can be efficiently solved. We further\nshow that SLL can generate a tighter generalization error bound for new labels\nthan the general ERM framework with trace norm or Frobenius norm\nregularization. Finally, we implement extensive experiments on various\nbenchmark datasets to validate the new setting. And results show that SLL can\neffectively handle the constantly emerging new labels and provides excellent\nclassification performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 07:12:29 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["You", "Shan", ""], ["Xu", "Chang", ""], ["Wang", "Yunhe", ""], ["Xu", "Chao", ""], ["Tao", "Dacheng", ""]]}, {"id": "1604.05468", "submitter": "Rahul Kamath", "authors": "Rahul Kamath, Masanao Ochi, Yutaka Matsuo", "title": "Understanding Rating Behaviour and Predicting Ratings by Identifying\n  Representative Users", "comments": "The 29th Pacific Asia Conference on Language, Information and\n  Computation (PACLIC-29)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online user reviews describing various products and services are now abundant\non the web. While the information conveyed through review texts and ratings is\neasily comprehensible, there is a wealth of hidden information in them that is\nnot immediately obvious. In this study, we unlock this hidden value behind user\nreviews to understand the various dimensions along which users rate products.\nWe learn a set of users that represent each of these dimensions and use their\nratings to predict product ratings. Specifically, we work with restaurant\nreviews to identify users whose ratings are influenced by dimensions like\n'Service', 'Atmosphere' etc. in order to predict restaurant ratings and\nunderstand the variation in rating behaviour across different cuisines. While\nprevious approaches to obtaining product ratings require either a large number\nof user ratings or a few review texts, we show that it is possible to predict\nratings with few user ratings and no review text. Our experiments show that our\napproach outperforms other conventional methods by 16-27% in terms of RMSE.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 08:31:23 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Kamath", "Rahul", ""], ["Ochi", "Masanao", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "1604.05590", "submitter": "Uri Stemmer", "authors": "Kobbi Nissim, Uri Stemmer, Salil Vadhan", "title": "Locating a Small Cluster Privately", "comments": null, "journal-ref": null, "doi": "10.1145/2902251.2902296", "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for locating a small cluster of points with\ndifferential privacy [Dwork, McSherry, Nissim, and Smith, 2006]. Our algorithm\nhas implications to private data exploration, clustering, and removal of\noutliers. Furthermore, we use it to significantly relax the requirements of the\nsample and aggregate technique [Nissim, Raskhodnikova, and Smith, 2007], which\nallows compiling of \"off the shelf\" (non-private) analyses into analyses that\npreserve differential privacy.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 14:27:32 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 15:51:54 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Nissim", "Kobbi", ""], ["Stemmer", "Uri", ""], ["Vadhan", "Salil", ""]]}, {"id": "1604.05753", "submitter": "Kunal Talwar", "authors": "Amit Daniely and Nevena Lazic and Yoram Singer and Kunal Talwar", "title": "Sketching and Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional sparse data present computational and statistical challenges\nfor supervised learning. We propose compact linear sketches for reducing the\ndimensionality of the input, followed by a single layer neural network. We show\nthat any sparse polynomial function can be computed, on nearly all sparse\nbinary vectors, by a single layer neural network that takes a compact sketch of\nthe vector as input. Consequently, when a set of sparse binary vectors is\napproximately separable using a sparse polynomial, there exists a single-layer\nneural network that takes a short sketch as input and correctly classifies\nnearly all the points. Previous work has proposed using sketches to reduce\ndimensionality while preserving the hypothesis class. However, the sketch size\nhas an exponential dependence on the degree in the case of polynomial\nclassifiers. In stark contrast, our approach of using improper learning, using\na larger hypothesis class allows the sketch size to have a logarithmic\ndependence on the degree. Even in the linear case, our approach allows us to\nimprove on the pesky $O({1}/{{\\gamma}^2})$ dependence of random projections, on\nthe margin $\\gamma$. We empirically show that our approach leads to more\ncompact neural networks than related methods such as feature hashing at equal\nor better performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 21:22:29 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Daniely", "Amit", ""], ["Lazic", "Nevena", ""], ["Singer", "Yoram", ""], ["Talwar", "Kunal", ""]]}, {"id": "1604.05819", "submitter": "Suchi Saria", "authors": "Daniel P. Robinson and Suchi Saria", "title": "Trading-Off Cost of Deployment Versus Accuracy in Learning Predictive\n  Models", "comments": "Authors contributed equally to this work. To appear in IJCAI 2016,\n  Twenty-Fifth International Joint Conference on Artificial Intelligence, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive models are finding an increasing number of applications in many\nindustries. As a result, a practical means for trading-off the cost of\ndeploying a model versus its effectiveness is needed. Our work is motivated by\nrisk prediction problems in healthcare. Cost-structures in domains such as\nhealthcare are quite complex, posing a significant challenge to existing\napproaches. We propose a novel framework for designing cost-sensitive\nstructured regularizers that is suitable for problems with complex cost\ndependencies. We draw upon a surprising connection to boolean circuits. In\nparticular, we represent the problem costs as a multi-layer boolean circuit,\nand then use properties of boolean circuits to define an extended feature\nvector and a group regularizer that exactly captures the underlying cost\nstructure. The resulting regularizer may then be combined with a fidelity\nfunction to perform model prediction, for example. For the challenging\nreal-world application of risk prediction for sepsis in intensive care units,\nthe use of our regularizer leads to models that are in harmony with the\nunderlying cost structure and thus provide an excellent prediction accuracy\nversus cost tradeoff.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 04:59:08 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Robinson", "Daniel P.", ""], ["Saria", "Suchi", ""]]}, {"id": "1604.05993", "submitter": "Lin Xu", "authors": "Lin Xu, Shaobo Lin, Jinshan Zeng, Xia Liu, Zongben Xu", "title": "Greedy Criterion in Orthogonal Greedy Learning", "comments": "12 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:1411.3553", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthogonal greedy learning (OGL) is a stepwise learning scheme that starts\nwith selecting a new atom from a specified dictionary via the steepest gradient\ndescent (SGD) and then builds the estimator through orthogonal projection. In\nthis paper, we find that SGD is not the unique greedy criterion and introduce a\nnew greedy criterion, called \"$\\delta$-greedy threshold\" for learning. Based on\nthe new greedy criterion, we derive an adaptive termination rule for OGL. Our\ntheoretical study shows that the new learning scheme can achieve the existing\n(almost) optimal learning rate of OGL. Plenty of numerical experiments are\nprovided to support that the new scheme can achieve almost optimal\ngeneralization performance, while requiring less computation than OGL.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 15:07:00 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Xu", "Lin", ""], ["Lin", "Shaobo", ""], ["Zeng", "Jinshan", ""], ["Liu", "Xia", ""], ["Xu", "Zongben", ""]]}, {"id": "1604.06020", "submitter": "Stefano Teso", "authors": "Stefano Teso, Andrea Passerini, Paolo Viappiani", "title": "Constructive Preference Elicitation by Setwise Max-margin Learning", "comments": "7 pages. A conference version of this work is accepted by the 25th\n  International Joint Conference on Artificial Intelligence (IJCAI-16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an approach to preference elicitation that is\nsuitable to large configuration spaces beyond the reach of existing\nstate-of-the-art approaches. Our setwise max-margin method can be viewed as a\ngeneralization of max-margin learning to sets, and can produce a set of\n\"diverse\" items that can be used to ask informative queries to the user.\nMoreover, the approach can encourage sparsity in the parameter space, in order\nto favor the assessment of utility towards combinations of weights that\nconcentrate on just few features. We present a mixed integer linear programming\nformulation and show how our approach compares favourably with Bayesian\npreference elicitation alternatives and easily scales to realistic datasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 16:22:01 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Teso", "Stefano", ""], ["Passerini", "Andrea", ""], ["Viappiani", "Paolo", ""]]}, {"id": "1604.06057", "submitter": "Karthik Narasimhan", "authors": "Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, Joshua B.\n  Tenenbaum", "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal\n  Abstraction and Intrinsic Motivation", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning goal-directed behavior in environments with sparse feedback is a\nmajor challenge for reinforcement learning algorithms. The primary difficulty\narises due to insufficient exploration, resulting in an agent being unable to\nlearn robust value functions. Intrinsically motivated agents can explore new\nbehavior for its own sake rather than to directly solve problems. Such\nintrinsic behaviors could eventually help the agent solve tasks posed by the\nenvironment. We present hierarchical-DQN (h-DQN), a framework to integrate\nhierarchical value functions, operating at different temporal scales, with\nintrinsically motivated deep reinforcement learning. A top-level value function\nlearns a policy over intrinsic goals, and a lower-level function learns a\npolicy over atomic actions to satisfy the given goals. h-DQN allows for\nflexible goal specifications, such as functions over entities and relations.\nThis provides an efficient space for exploration in complicated environments.\nWe demonstrate the strength of our approach on two problems with very sparse,\ndelayed feedback: (1) a complex discrete stochastic decision process, and (2)\nthe classic ATARI game `Montezuma's Revenge'.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 18:47:48 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 14:45:58 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Kulkarni", "Tejas D.", ""], ["Narasimhan", "Karthik R.", ""], ["Saeedi", "Ardavan", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1604.06133", "submitter": "Miron Kursa", "authors": "Miron Bartosz Kursa", "title": "Embedded all relevant feature selection with Random Ferns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning methods can produce variable importance scores\nexpressing the usability of each feature in context of the produced model;\nthose scores on their own are yet not sufficient to generate feature selection,\nespecially when an all relevant selection is required. Although there are\nwrapper methods aiming to solve this problem, they introduce a substantial\nincrease in the required computational effort.\n  In this paper I investigate an idea of incorporating all relevant selection\nwithin the training process by producing importance for implicitly generated\nshadows, attributes irrelevant by design. I propose and evaluate such a method\nin context of random ferns classifier. Experiment results confirm the\neffectiveness of such approach, although show that fully stochastic nature of\nrandom ferns limits its applicability either to small dimensions or as a part\nof a broader feature selection procedure.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 22:16:34 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Kursa", "Miron Bartosz", ""]]}, {"id": "1604.06153", "submitter": "Chaobing Song", "authors": "Chaobing Song, Shu-Tao Xia", "title": "Nonextensive information theoretical machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new discriminative model named \\emph{nonextensive\ninformation theoretical machine (NITM)} based on nonextensive generalization of\nShannon information theory. In NITM, weight parameters are treated as random\nvariables. Tsallis divergence is used to regularize the distribution of weight\nparameters and maximum unnormalized Tsallis entropy distribution is used to\nevaluate fitting effect. On the one hand, it is showed that some well-known\nmargin-based loss functions such as $\\ell_{0/1}$ loss, hinge loss, squared\nhinge loss and exponential loss can be unified by unnormalized Tsallis entropy.\nOn the other hand, Gaussian prior regularization is generalized to Student-t\nprior regularization with similar computational complexity. The model can be\nsolved efficiently by gradient-based convex optimization and its performance is\nillustrated on standard datasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 01:29:56 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Song", "Chaobing", ""], ["Xia", "Shu-Tao", ""]]}, {"id": "1604.06154", "submitter": "Xichuan Zhou", "authors": "Xichuan Zhou, Shengli Li, Kai Qin, Kunping Li, Fang Tang, Shengdong\n  Hu, Shujun Liu, Zhi Lin", "title": "Deep Adaptive Network: An Efficient Deep Neural Network with Sparse\n  Binary Connections", "comments": "10 pages, extended and submitted to IEEE Transactions of Systems,\n  Man, and Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are state-of-the-art models for understanding the\ncontent of images, video and raw input data. However, implementing a deep\nneural network in embedded systems is a challenging task, because a typical\ndeep neural network, such as a Deep Belief Network using 128x128 images as\ninput, could exhaust Giga bytes of memory and result in bandwidth and computing\nbottleneck. To address this challenge, this paper presents a hardware-oriented\ndeep learning algorithm, named as the Deep Adaptive Network, which attempts to\nexploit the sparsity in the neural connections. The proposed method adaptively\nreduces the weights associated with negligible features to zero, leading to\nsparse feedforward network architecture. Furthermore, since the small\nproportion of important weights are significantly larger than zero, they can be\nrobustly thresholded and represented using single-bit integers (-1 and +1),\nleading to implementations of deep neural networks with sparse and binary\nconnections. Our experiments showed that, for the application of recognizing\nMNIST handwritten digits, the features extracted by a two-layer Deep Adaptive\nNetwork with about 25% reserved important connections achieved 97.2%\nclassification accuracy, which was almost the same with the standard Deep\nBelief Network (97.3%). Furthermore, for efficient hardware implementations,\nthe sparse-and-binary-weighted deep neural network could save about 99.3%\nmemory and 99.9% computation units without significant loss of classification\naccuracy for pattern recognition applications.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 01:47:33 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Zhou", "Xichuan", ""], ["Li", "Shengli", ""], ["Qin", "Kai", ""], ["Li", "Kunping", ""], ["Tang", "Fang", ""], ["Hu", "Shengdong", ""], ["Liu", "Shujun", ""], ["Lin", "Zhi", ""]]}, {"id": "1604.06162", "submitter": "Chicheng Zhang", "authors": "Chicheng Zhang and Kamalika Chaudhuri", "title": "The Extended Littlestone's Dimension for Learning with Mistakes and\n  Abstentions", "comments": "29 pages, 7 figures; COLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies classification with an abstention option in the online\nsetting. In this setting, examples arrive sequentially, the learner is given a\nhypothesis class $\\mathcal H$, and the goal of the learner is to either predict\na label on each example or abstain, while ensuring that it does not make more\nthan a pre-specified number of mistakes when it does predict a label.\n  Previous work on this problem has left open two main challenges. First, not\nmuch is known about the optimality of algorithms, and in particular, about what\nan optimal algorithmic strategy is for any individual hypothesis class. Second,\nwhile the realizable case has been studied, the more realistic non-realizable\nscenario is not well-understood. In this paper, we address both challenges.\nFirst, we provide a novel measure, called the Extended Littlestone's Dimension,\nwhich captures the number of abstentions needed to ensure a certain number of\nmistakes. Second, we explore the non-realizable case, and provide upper and\nlower bounds on the number of abstentions required by an algorithm to guarantee\na specified number of mistakes.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 02:39:22 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 22:53:09 GMT"}, {"version": "v3", "created": "Wed, 28 Sep 2016 05:19:33 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Zhang", "Chicheng", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1604.06174", "submitter": "Tianqi Chen", "authors": "Tianqi Chen and Bing Xu and Chiyuan Zhang and Carlos Guestrin", "title": "Training Deep Nets with Sublinear Memory Cost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a systematic approach to reduce the memory consumption of deep\nneural network training. Specifically, we design an algorithm that costs\nO(sqrt(n)) memory to train a n layer network, with only the computational cost\nof an extra forward pass per mini-batch. As many of the state-of-the-art models\nhit the upper bound of the GPU memory, our algorithm allows deeper and more\ncomplex models to be explored, and helps advance the innovations in deep\nlearning research. We focus on reducing the memory cost to store the\nintermediate feature maps and gradients during training. Computation graph\nanalysis is used for automatic in-place operation and memory sharing\noptimizations. We show that it is possible to trade computation for memory -\ngiving a more memory efficient training algorithm with a little extra\ncomputation cost. In the extreme case, our analysis also shows that the memory\nconsumption can be reduced to O(log n) with as little as O(n log n) extra cost\nfor forward computation. Our experiments show that we can reduce the memory\ncost of a 1,000-layer deep residual network from 48G to 7G with only 30 percent\nadditional running time cost on ImageNet problems. Similarly, significant\nmemory cost reduction is observed in training complex recurrent neural networks\non very long sequences.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 04:15:27 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 19:21:36 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Chen", "Tianqi", ""], ["Xu", "Bing", ""], ["Zhang", "Chiyuan", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1604.06338", "submitter": "Huy Phan", "authors": "Huy Phan, Lars Hertel, Marco Maass, Alfred Mertins", "title": "Robust Audio Event Recognition with 1-Max Pooling Convolutional Neural\n  Networks", "comments": "To appear in Proceedings of Interspeech 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a simple, yet efficient convolutional neural network\n(CNN) architecture for robust audio event recognition. Opposing to deep CNN\narchitectures with multiple convolutional and pooling layers topped up with\nmultiple fully connected layers, the proposed network consists of only three\nlayers: convolutional, pooling, and softmax layer. Two further features\ndistinguish it from the deep architectures that have been proposed for the\ntask: varying-size convolutional filters at the convolutional layer and 1-max\npooling scheme at the pooling layer. In intuition, the network tends to select\nthe most discriminative features from the whole audio signals for recognition.\nOur proposed CNN not only shows state-of-the-art performance on the standard\ntask of robust audio event recognition but also outperforms other deep\narchitectures up to 4.5% in terms of recognition accuracy, which is equivalent\nto 76.3% relative error reduction.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 14:51:43 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 13:01:16 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Phan", "Huy", ""], ["Hertel", "Lars", ""], ["Maass", "Marco", ""], ["Mertins", "Alfred", ""]]}, {"id": "1604.06443", "submitter": "Gautam Kamath", "authors": "Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur\n  Moitra, Alistair Stewart", "title": "Robust Estimators in High Dimensions without the Computational\n  Intractability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study high-dimensional distribution learning in an agnostic setting where\nan adversary is allowed to arbitrarily corrupt an $\\varepsilon$-fraction of the\nsamples. Such questions have a rich history spanning statistics, machine\nlearning and theoretical computer science. Even in the most basic settings, the\nonly known approaches are either computationally inefficient or lose\ndimension-dependent factors in their error guarantees. This raises the\nfollowing question:Is high-dimensional agnostic distribution learning even\npossible, algorithmically?\n  In this work, we obtain the first computationally efficient algorithms with\ndimension-independent error guarantees for agnostically learning several\nfundamental classes of high-dimensional distributions: (1) a single Gaussian,\n(2) a product distribution on the hypercube, (3) mixtures of two product\ndistributions (under a natural balancedness condition), and (4) mixtures of\nspherical Gaussians. Our algorithms achieve error that is independent of the\ndimension, and in many cases scales nearly-linearly with the fraction of\nadversarially corrupted samples. Moreover, we develop a general recipe for\ndetecting and correcting corruptions in high-dimensions, that may be applicable\nto many other problems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 19:54:24 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 02:31:22 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kamath", "Gautam", ""], ["Kane", "Daniel", ""], ["Li", "Jerry", ""], ["Moitra", "Ankur", ""], ["Stewart", "Alistair", ""]]}, {"id": "1604.06474", "submitter": "Tengyuan Liang", "authors": "T. Tony Cai, Tengyuan Liang and Alexander Rakhlin", "title": "On Detection and Structural Reconstruction of Small-World Random\n  Networks", "comments": "22 pages, 3 figures", "journal-ref": "IEEE Transactions on Network Science and Engineering 4 (2017)\n  165-176", "doi": "10.1109/TNSE.2017.2703102", "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study detection and fast reconstruction of the celebrated\nWatts-Strogatz (WS) small-world random graph model \\citep{watts1998collective}\nwhich aims to describe real-world complex networks that exhibit both high\nclustering and short average length properties. The WS model with neighborhood\nsize $k$ and rewiring probability probability $\\beta$ can be viewed as a\ncontinuous interpolation between a deterministic ring lattice graph and the\nErd\\H{o}s-R\\'{e}nyi random graph. We study both the computational and\nstatistical aspects of detecting the deterministic ring lattice structure (or\nlocal geographical links, strong ties) in the presence of random connections\n(or long range links, weak ties), and for its recovery. The phase diagram in\nterms of $(k,\\beta)$ is partitioned into several regions according to the\ndifficulty of the problem. We propose distinct methods for the various regions.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 20:13:54 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Cai", "T. Tony", ""], ["Liang", "Tengyuan", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "1604.06498", "submitter": "Yuting Ma", "authors": "Yuting Ma, Tian Zheng", "title": "Stabilized Sparse Online Learning for Sparse Data", "comments": "45 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is commonly used for optimization in\nlarge-scale machine learning problems. Langford et al. (2009) introduce a\nsparse online learning method to induce sparsity via truncated gradient. With\nhigh-dimensional sparse data, however, the method suffers from slow convergence\nand high variance due to the heterogeneity in feature sparsity. To mitigate\nthis issue, we introduce a stabilized truncated stochastic gradient descent\nalgorithm. We employ a soft-thresholding scheme on the weight vector where the\nimposed shrinkage is adaptive to the amount of information available in each\nfeature. The variability in the resulted sparse weight vector is further\ncontrolled by stability selection integrated with the informative truncation.\nTo facilitate better convergence, we adopt an annealing strategy on the\ntruncation rate, which leads to a balanced trade-off between exploration and\nexploitation in learning a sparse weight vector. Numerical experiments show\nthat our algorithm compares favorably with the original algorithm in terms of\nprediction accuracy, achieved sparsity and stability.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 21:34:34 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 22:41:12 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 00:50:38 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Ma", "Yuting", ""], ["Zheng", "Tian", ""]]}, {"id": "1604.06518", "submitter": "Vu Nguyen", "authors": "Trung Le and Tu Dinh Nguyen and Vu Nguyen and Dinh Phung", "title": "Approximation Vector Machines for Large-scale Online Learning", "comments": "54 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most challenging problems in kernel online learning is to bound\nthe model size and to promote the model sparsity. Sparse models not only\nimprove computation and memory usage, but also enhance the generalization\ncapacity, a principle that concurs with the law of parsimony. However,\ninappropriate sparsity modeling may also significantly degrade the performance.\nIn this paper, we propose Approximation Vector Machine (AVM), a model that can\nsimultaneously encourage the sparsity and safeguard its risk in compromising\nthe performance. When an incoming instance arrives, we approximate this\ninstance by one of its neighbors whose distance to it is less than a predefined\nthreshold. Our key intuition is that since the newly seen instance is expressed\nby its nearby neighbor the optimal performance can be analytically formulated\nand maintained. We develop theoretical foundations to support this intuition\nand further establish an analysis to characterize the gap between the\napproximation and optimal solutions. This gap crucially depends on the\nfrequency of approximation and the predefined threshold. We perform the\nconvergence analysis for a wide spectrum of loss functions including Hinge,\nsmooth Hinge, and Logistic for classification task, and $l_1$, $l_2$, and\n$\\epsilon$-insensitive for regression task. We conducted extensive experiments\nfor classification task in batch and online modes, and regression task in\nonline mode over several benchmark datasets. The results show that our proposed\nAVM achieved a comparable predictive performance with current state-of-the-art\nmethods while simultaneously achieving significant computational speed-up due\nto the ability of the proposed AVM in maintaining the model size.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 01:57:01 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 01:16:21 GMT"}, {"version": "v3", "created": "Wed, 5 Apr 2017 01:43:29 GMT"}, {"version": "v4", "created": "Sun, 28 May 2017 01:26:48 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Le", "Trung", ""], ["Nguyen", "Tu Dinh", ""], ["Nguyen", "Vu", ""], ["Phung", "Dinh", ""]]}, {"id": "1604.06529", "submitter": "Adhiguna Kuncoro", "authors": "Adhiguna Kuncoro, Yuichiro Sawai, Kevin Duh, Yuji Matsumoto", "title": "Dependency Parsing with LSTMs: An Empirical Evaluation", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a transition-based dependency parser using Recurrent Neural\nNetworks with Long Short-Term Memory (LSTM) units. This extends the feedforward\nneural network parser of Chen and Manning (2014) and enables modelling of\nentire sequences of shift/reduce transition decisions. On the Google Web\nTreebank, our LSTM parser is competitive with the best feedforward parser on\noverall accuracy and notably achieves more than 3% improvement for long-range\ndependencies, which has proved difficult for previous transition-based parsers\ndue to error propagation and limited context information. Our findings\nadditionally suggest that dropout regularisation on the embedding layer is\ncrucial to improve the LSTM's generalisation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 03:20:24 GMT"}, {"version": "v2", "created": "Thu, 30 Jun 2016 04:23:07 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Kuncoro", "Adhiguna", ""], ["Sawai", "Yuichiro", ""], ["Duh", "Kevin", ""], ["Matsumoto", "Yuji", ""]]}, {"id": "1604.06577", "submitter": "Fereshteh Asgari Fereshteh ASGARI", "authors": "Fereshteh Asgari and Alexis Sultan and Haoyi Xiong and Vincent\n  Gauthier and Mounim El-Yacoubi", "title": "CT-Mapper: Mapping Sparse Multimodal Cellular Trajectories using a\n  Multilayer Transportation Network", "comments": "Under revision in Computer Communication Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile phone data have recently become an attractive source of information\nabout mobility behavior. Since cell phone data can be captured in a passive way\nfor a large user population, they can be harnessed to collect well-sampled\nmobility information. In this paper, we propose CT-Mapper, an unsupervised\nalgorithm that enables the mapping of mobile phone traces over a multimodal\ntransport network. One of the main strengths of CT-Mapper is its capability to\nmap noisy sparse cellular multimodal trajectories over a multilayer\ntransportation network where the layers have different physical properties and\nnot only to map trajectories associated with a single layer. Such a network is\nmodeled by a large multilayer graph in which the nodes correspond to\nmetro/train stations or road intersections and edges correspond to connections\nbetween them. The mapping problem is modeled by an unsupervised HMM where the\nobservations correspond to sparse user mobile trajectories and the hidden\nstates to the multilayer graph nodes. The HMM is unsupervised as the transition\nand emission probabilities are inferred using respectively the physical\ntransportation properties and the information on the spatial coverage of\nantenna base stations. To evaluate CT-Mapper we collected cellular traces with\ntheir corresponding GPS trajectories for a group of volunteer users in Paris\nand vicinity (France). We show that CT-Mapper is able to accurately retrieve\nthe real cell phone user paths despite the sparsity of the observed trace\ntrajectories. Furthermore our transition probability model is up to 20% more\naccurate than other naive models.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 08:59:43 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Asgari", "Fereshteh", ""], ["Sultan", "Alexis", ""], ["Xiong", "Haoyi", ""], ["Gauthier", "Vincent", ""], ["El-Yacoubi", "Mounim", ""]]}, {"id": "1604.06602", "submitter": "Shounak Datta", "authors": "Shounak Datta, Supritam Bhattacharjee, Swagatam Das", "title": "Clustering with Missing Features: A Penalized Dissimilarity Measure\n  based approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world clustering problems are plagued by incomplete data\ncharacterized by missing or absent features for some or all of the data\ninstances. Traditional clustering methods cannot be directly applied to such\ndata without preprocessing by imputation or marginalization techniques. In this\narticle, we overcome this drawback by utilizing a penalized dissimilarity\nmeasure which we refer to as the Feature Weighted Penalty based Dissimilarity\n(FWPD). Using the FWPD measure, we modify the traditional k-means clustering\nalgorithm and the standard hierarchical agglomerative clustering algorithms so\nas to make them directly applicable to datasets with missing features. We\npresent time complexity analyses for these new techniques and also undertake a\ndetailed theoretical analysis showing that the new FWPD based k-means algorithm\nconverges to a local optimum within a finite number of iterations. We also\npresent a detailed method for simulating random as well as feature dependent\nmissingness. We report extensive experiments on various benchmark datasets for\ndifferent types of missingness showing that the proposed clustering techniques\nhave generally better results compared to some of the most well-known\nimputation methods which are commonly used to handle such incomplete data. We\nappend a possible extension of the proposed dissimilarity measure to the case\nof absent features (where the unobserved features are known to be undefined).\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 10:53:48 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 13:57:28 GMT"}, {"version": "v3", "created": "Fri, 11 Nov 2016 14:13:11 GMT"}, {"version": "v4", "created": "Wed, 28 Dec 2016 13:18:05 GMT"}, {"version": "v5", "created": "Thu, 12 Oct 2017 15:08:47 GMT"}, {"version": "v6", "created": "Thu, 19 Oct 2017 14:17:46 GMT"}, {"version": "v7", "created": "Sat, 7 Jul 2018 12:09:52 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Datta", "Shounak", ""], ["Bhattacharjee", "Supritam", ""], ["Das", "Swagatam", ""]]}, {"id": "1604.06626", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain", "title": "The Mean Partition Theorem of Consensus Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To devise efficient solutions for approximating a mean partition in consensus\nclustering, Dimitriadou et al. [3] presented a necessary condition of\noptimality for a consensus function based on least square distances. We show\nthat their result is pivotal for deriving interesting properties of consensus\nclustering beyond optimization. For this, we present the necessary condition of\noptimality in a slightly stronger form in terms of the Mean Partition Theorem\nand extend it to the Expected Partition Theorem. To underpin its versatility,\nwe show three examples that apply the Mean Partition Theorem: (i) equivalence\nof the mean partition and optimal multiple alignment, (ii) construction of\nprofiles and motifs, and (iii) relationship between consensus clustering and\ncluster stability.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 12:32:37 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 06:55:06 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Jain", "Brijnesh J.", ""]]}, {"id": "1604.06635", "submitter": "Xipeng Qiu", "authors": "Peng Qian, Xipeng Qiu, Xuanjing Huang", "title": "Bridging LSTM Architecture and the Neural Dynamics during Reading", "comments": "25th International Joint Conference on Artificial Intelligence\n  IJCAI-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the long short-term memory neural network (LSTM) has attracted wide\ninterest due to its success in many tasks. LSTM architecture consists of a\nmemory cell and three gates, which looks similar to the neuronal networks in\nthe brain. However, there still lacks the evidence of the cognitive\nplausibility of LSTM architecture as well as its working mechanism. In this\npaper, we study the cognitive plausibility of LSTM by aligning its internal\narchitecture with the brain activity observed via fMRI when the subjects read a\nstory. Experiment results show that the artificial memory vector in LSTM can\naccurately predict the observed sequential brain activities, indicating the\ncorrelation between LSTM architecture and the cognitive process of story\nreading.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 12:51:11 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Qian", "Peng", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1604.06730", "submitter": "Chee Chun Gan", "authors": "Chee Chun Gan and Gerard Learmonth", "title": "Developing an ICU scoring system with interaction terms using a genetic\n  algorithm", "comments": "21 pages, 6 tables, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ICU mortality scoring systems attempt to predict patient mortality using\npredictive models with various clinical predictors. Examples of such systems\nare APACHE, SAPS and MPM. However, most such scoring systems do not actively\nlook for and include interaction terms, despite physicians intuitively taking\nsuch interactions into account when making a diagnosis. One barrier to\nincluding such terms in predictive models is the difficulty of using most\nvariable selection methods in high-dimensional datasets. A genetic algorithm\nframework for variable selection with logistic regression models is used to\nsearch for two-way interaction terms in a clinical dataset of adult ICU\npatients, with separate models being built for each category of diagnosis upon\nadmittance to the ICU. The models had good discrimination across all\ncategories, with a weighted average AUC of 0.84 (>0.90 for several categories)\nand the genetic algorithm was able to find several significant interaction\nterms, which may be able to provide greater insight into mortality prediction\nfor health practitioners. The GA selected models had improved performance\nagainst stepwise selection and random forest models, and provides greater\nflexibility in terms of variable selection by being able to optimize over any\nmodeler-defined model performance metric instead of a specific variable\nimportance metric.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 16:20:29 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Gan", "Chee Chun", ""], ["Learmonth", "Gerard", ""]]}, {"id": "1604.06737", "submitter": "Cheng Guo", "authors": "Cheng Guo and Felix Berkhahn", "title": "Entity Embeddings of Categorical Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We map categorical variables in a function approximation problem into\nEuclidean spaces, which are the entity embeddings of the categorical variables.\nThe mapping is learned by a neural network during the standard supervised\ntraining process. Entity embedding not only reduces memory usage and speeds up\nneural networks compared with one-hot encoding, but more importantly by mapping\nsimilar values close to each other in the embedding space it reveals the\nintrinsic properties of the categorical variables. We applied it successfully\nin a recent Kaggle competition and were able to reach the third position with\nrelative simple features. We further demonstrate in this paper that entity\nembedding helps the neural network to generalize better when the data is sparse\nand statistics is unknown. Thus it is especially useful for datasets with lots\nof high cardinality features, where other methods tend to overfit. We also\ndemonstrate that the embeddings obtained from the trained neural network boost\nthe performance of all tested machine learning methods considerably when used\nas the input features instead. As entity embedding defines a distance measure\nfor categorical variables it can be used for visualizing categorical data and\nfor data clustering.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 16:34:30 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Guo", "Cheng", ""], ["Berkhahn", "Felix", ""]]}, {"id": "1604.06743", "submitter": "Li Zhou", "authors": "Li Zhou and Emma Brunskill", "title": "Latent Contextual Bandits and their Application to Personalized\n  Recommendations for New Users", "comments": "25th International Joint Conference on Artificial Intelligence (IJCAI\n  2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized recommendations for new users, also known as the cold-start\nproblem, can be formulated as a contextual bandit problem. Existing contextual\nbandit algorithms generally rely on features alone to capture user variability.\nSuch methods are inefficient in learning new users' interests. In this paper we\npropose Latent Contextual Bandits. We consider both the benefit of leveraging a\nset of learned latent user classes for new users, and how we can learn such\nlatent classes from prior users. We show that our approach achieves a better\nregret bound than existing algorithms. We also demonstrate the benefit of our\napproach using a large real world dataset and a preliminary user study.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 16:47:04 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Zhou", "Li", ""], ["Brunskill", "Emma", ""]]}, {"id": "1604.06778", "submitter": "Yan Duan", "authors": "Yan Duan, Xi Chen, Rein Houthooft, John Schulman, Pieter Abbeel", "title": "Benchmarking Deep Reinforcement Learning for Continuous Control", "comments": "14 pages, ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, researchers have made significant progress combining the advances\nin deep learning for learning feature representations with reinforcement\nlearning. Some notable examples include training agents to play Atari games\nbased on raw pixel data and to acquire advanced manipulation skills using raw\nsensory inputs. However, it has been difficult to quantify progress in the\ndomain of continuous control due to the lack of a commonly adopted benchmark.\nIn this work, we present a benchmark suite of continuous control tasks,\nincluding classic tasks like cart-pole swing-up, tasks with very high state and\naction dimensionality such as 3D humanoid locomotion, tasks with partial\nobservations, and tasks with hierarchical structure. We report novel findings\nbased on the systematic evaluation of a range of implemented reinforcement\nlearning algorithms. Both the benchmark and reference implementations are\nreleased at https://github.com/rllab/rllab in order to facilitate experimental\nreproducibility and to encourage adoption by other researchers.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 18:57:24 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 06:16:06 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 19:25:59 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Duan", "Yan", ""], ["Chen", "Xi", ""], ["Houthooft", "Rein", ""], ["Schulman", "John", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1604.06849", "submitter": "Shiwali Mohan", "authors": "Shiwali Mohan, James Kirk, John Laird", "title": "A Computational Model for Situated Task Learning with Interactive\n  Instruction", "comments": "International Conference on Cognitive Modeling, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning novel tasks is a complex cognitive activity requiring the learner to\nacquire diverse declarative and procedural knowledge. Prior ACT-R models of\nacquiring task knowledge from instruction focused on learning procedural\nknowledge from declarative instructions encoded in semantic memory. In this\npaper, we identify the requirements for designing compu- tational models that\nlearn task knowledge from situated task- oriented interactions with an expert\nand then describe and evaluate a model of learning from situated interactive\ninstruc- tion that is implemented in the Soar cognitive architecture.\n", "versions": [{"version": "v1", "created": "Sat, 23 Apr 2016 02:23:14 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Mohan", "Shiwali", ""], ["Kirk", "James", ""], ["Laird", "John", ""]]}, {"id": "1604.06915", "submitter": "Shai Shalev-Shwartz", "authors": "Shai Shalev-Shwartz and Amnon Shashua", "title": "On the Sample Complexity of End-to-end Training vs. Semantic Abstraction\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare the end-to-end training approach to a modular approach in which a\nsystem is decomposed into semantically meaningful components. We focus on the\nsample complexity aspect, in the regime where an extremely high accuracy is\nnecessary, as is the case in autonomous driving applications. We demonstrate\ncases in which the number of training examples required by the end-to-end\napproach is exponentially larger than the number of examples required by the\nsemantic abstraction approach.\n", "versions": [{"version": "v1", "created": "Sat, 23 Apr 2016 15:13:43 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Shashua", "Amnon", ""]]}, {"id": "1604.06968", "submitter": "Kevin A. Lai", "authors": "Kevin A. Lai, Anup B. Rao, Santosh Vempala", "title": "Agnostic Estimation of Mean and Covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the mean and covariance of a\ndistribution from iid samples in $\\mathbb{R}^n$, in the presence of an $\\eta$\nfraction of malicious noise; this is in contrast to much recent work where the\nnoise itself is assumed to be from a distribution of known type. The agnostic\nproblem includes many interesting special cases, e.g., learning the parameters\nof a single Gaussian (or finding the best-fit Gaussian) when $\\eta$ fraction of\ndata is adversarially corrupted, agnostically learning a mixture of Gaussians,\nagnostic ICA, etc. We present polynomial-time algorithms to estimate the mean\nand covariance with error guarantees in terms of information-theoretic lower\nbounds. As a corollary, we also obtain an agnostic algorithm for Singular Value\nDecomposition.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 00:23:51 GMT"}, {"version": "v2", "created": "Sun, 14 Aug 2016 19:50:58 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Lai", "Kevin A.", ""], ["Rao", "Anup B.", ""], ["Vempala", "Santosh", ""]]}, {"id": "1604.06985", "submitter": "Oswaldo Ludwig", "authors": "Oswaldo Ludwig", "title": "Deep Learning with Eigenvalue Decay Regularizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends our previous work on regularization of neural networks\nusing Eigenvalue Decay by employing a soft approximation of the dominant\neigenvalue in order to enable the calculation of its derivatives in relation to\nthe synaptic weights, and therefore the application of back-propagation, which\nis a primary demand for deep learning. Moreover, we extend our previous\ntheoretical analysis to deep neural networks and multiclass classification\nproblems. Our method is implemented as an additional regularizer in Keras, a\nmodular neural networks library written in Python, and evaluated in the\nbenchmark data sets Reuters Newswire Topics Classification, IMDB database for\nbinary sentiment classification, MNIST database of handwritten digits and\nCIFAR-10 data set for image classification.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 05:17:04 GMT"}, {"version": "v2", "created": "Sat, 30 Apr 2016 13:03:48 GMT"}, {"version": "v3", "created": "Sun, 8 May 2016 10:07:34 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Ludwig", "Oswaldo", ""]]}, {"id": "1604.07070", "submitter": "Shuai Zheng", "authors": "Shuai Zheng and James T. Kwok", "title": "Stochastic Variance-Reduced ADMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alternating direction method of multipliers (ADMM) is a powerful\noptimization solver in machine learning. Recently, stochastic ADMM has been\nintegrated with variance reduction methods for stochastic gradient, leading to\nSAG-ADMM and SDCA-ADMM that have fast convergence rates and low iteration\ncomplexities. However, their space requirements can still be high. In this\npaper, we propose an integration of ADMM with the method of stochastic variance\nreduced gradient (SVRG). Unlike another recent integration attempt called\nSCAS-ADMM, the proposed algorithm retains the fast convergence benefits of\nSAG-ADMM and SDCA-ADMM, but is more advantageous in that its storage\nrequirement is very low, even independent of the sample size $n$. We also\nextend the proposed method for nonconvex problems, and obtain a convergence\nrate of $O(1/T)$. Experimental results demonstrate that it is as fast as\nSAG-ADMM and SDCA-ADMM, much faster than SCAS-ADMM, and can be used on much\nbigger data sets.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 18:50:58 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 16:59:55 GMT"}, {"version": "v3", "created": "Sun, 16 Oct 2016 18:56:57 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Zheng", "Shuai", ""], ["Kwok", "James T.", ""]]}, {"id": "1604.07078", "submitter": "Timothy O'Shea", "authors": "Timothy J. O'Shea, Johnathan Corgan, T. Charles Clancy", "title": "Unsupervised Representation Learning of Structured Radio Communication\n  Signals", "comments": "4 pages, 9 figures, currently under conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore unsupervised representation learning of radio communication\nsignals in raw sampled time series representation. We demonstrate that we can\nlearn modulation basis functions using convolutional autoencoders and visually\nrecognize their relationship to the analytic bases used in digital\ncommunications. We also propose and evaluate quantitative met- rics for quality\nof encoding using domain relevant performance metrics.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 20:32:18 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["O'Shea", "Timothy J.", ""], ["Corgan", "Johnathan", ""], ["Clancy", "T. Charles", ""]]}, {"id": "1604.07093", "submitter": "Yanwei  Fu", "authors": "Yanwei Fu, Leonid Sigal", "title": "Semi-supervised Vocabulary-informed Learning", "comments": "10 pages, Accepted by CVPR 2016 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress in object categorization, in recent years, a\nnumber of important challenges remain, mainly, ability to learn from limited\nlabeled data and ability to recognize object classes within large, potentially\nopen, set of labels. Zero-shot learning is one way of addressing these\nchallenges, but it has only been shown to work with limited sized class\nvocabularies and typically requires separation between supervised and\nunsupervised classes, allowing former to inform the latter but not vice versa.\nWe propose the notion of semi-supervised vocabulary-informed learning to\nalleviate the above mentioned challenges and address problems of supervised,\nzero-shot and open set recognition using a unified framework. Specifically, we\npropose a maximum margin framework for semantic manifold-based recognition that\nincorporates distance constraints from (both supervised and unsupervised)\nvocabulary atoms, ensuring that labeled samples are projected closest to their\ncorrect prototypes, in the embedding space, than to others. We show that\nresulting model shows improvements in supervised, zero-shot, and large open set\nrecognition, with up to 310K class vocabulary on AwA and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 23:36:36 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Fu", "Yanwei", ""], ["Sigal", "Leonid", ""]]}, {"id": "1604.07101", "submitter": "Huasen Wu", "authors": "Huasen Wu and Xin Liu", "title": "Double Thompson Sampling for Dueling Bandits", "comments": "27 pages, 5 figures, 9 tables; accepted by 30th Conference on Neural\n  Information Processing Systems (NIPS), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Double Thompson Sampling (D-TS) algorithm for\ndueling bandit problems. As indicated by its name, D-TS selects both the first\nand the second candidates according to Thompson Sampling. Specifically, D-TS\nmaintains a posterior distribution for the preference matrix, and chooses the\npair of arms for comparison by sampling twice from the posterior distribution.\nThis simple algorithm applies to general Copeland dueling bandits, including\nCondorcet dueling bandits as its special case. For general Copeland dueling\nbandits, we show that D-TS achieves $O(K^2 \\log T)$ regret. For Condorcet\ndueling bandits, we further simplify the D-TS algorithm and show that the\nsimplified D-TS algorithm achieves $O(K \\log T + K^2 \\log \\log T)$ regret.\nSimulation results based on both synthetic and real-world data demonstrate the\nefficiency of the proposed D-TS algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 00:38:16 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 17:36:57 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Wu", "Huasen", ""], ["Liu", "Xin", ""]]}, {"id": "1604.07143", "submitter": "Erwan Scornet", "authors": "G\\'erard Biau (LPMA, LSTA), Erwan Scornet (LSTA), Johannes Welbl (UCL)", "title": "Neural Random Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an ensemble of randomized regression trees, it is possible to\nrestructure them as a collection of multilayered neural networks with\nparticular connection weights. Following this principle, we reformulate the\nrandom forest method of Breiman (2001) into a neural network setting, and in\nturn propose two new hybrid procedures that we call neural random forests. Both\npredictors exploit prior knowledge of regression trees for their architecture,\nhave less parameters to tune than standard networks, and less restrictions on\nthe geometry of the decision boundaries than trees. Consistency results are\nproved, and substantial numerical evidence is provided on both synthetic and\nreal data sets to assess the excellent performance of our methods in a large\nvariety of prediction problems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 06:43:47 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 07:42:50 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Biau", "G\u00e9rard", "", "LPMA, LSTA"], ["Scornet", "Erwan", "", "LSTA"], ["Welbl", "Johannes", "", "UCL"]]}, {"id": "1604.07176", "submitter": "Zhen Li", "authors": "Zhen Li and Yizhou Yu", "title": "Protein Secondary Structure Prediction Using Cascaded Convolutional and\n  Recurrent Neural Networks", "comments": "8 pages, 3 figures, Accepted by International Joint Conferences on\n  Artificial Intelligence (IJCAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.AI cs.LG cs.NE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein secondary structure prediction is an important problem in\nbioinformatics. Inspired by the recent successes of deep neural networks, in\nthis paper, we propose an end-to-end deep network that predicts protein\nsecondary structures from integrated local and global contextual features. Our\ndeep architecture leverages convolutional neural networks with different kernel\nsizes to extract multiscale local contextual features. In addition, considering\nlong-range dependencies existing in amino acid sequences, we set up a\nbidirectional neural network consisting of gated recurrent unit to capture\nglobal contextual features. Furthermore, multi-task learning is utilized to\npredict secondary structure labels and amino-acid solvent accessibility\nsimultaneously. Our proposed deep network demonstrates its effectiveness by\nachieving state-of-the-art performance, i.e., 69.7% Q8 accuracy on the public\nbenchmark CB513, 76.9% Q8 accuracy on CASP10 and 73.1% Q8 accuracy on CASP11.\nOur model and results are publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 09:17:18 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Li", "Zhen", ""], ["Yu", "Yizhou", ""]]}, {"id": "1604.07178", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad, Daoqiang Zhang", "title": "Weighted Spectral Cluster Ensemble", "comments": "IEEE International Conference on Data Mining (ICDM), 2015", "journal-ref": null, "doi": "10.1109/ICDM.2015.145", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering explores meaningful patterns in the non-labeled data sets. Cluster\nEnsemble Selection (CES) is a new approach, which can combine individual\nclustering results for increasing the performance of the final results.\nAlthough CES can achieve better final results in comparison with individual\nclustering algorithms and cluster ensemble methods, its performance can be\ndramatically affected by its consensus diversity metric and thresholding\nprocedure. There are two problems in CES: 1) most of the diversity metrics is\nbased on heuristic Shannon's entropy and 2) estimating threshold values are\nreally hard in practice. The main goal of this paper is proposing a robust\napproach for solving the above mentioned problems. Accordingly, this paper\ndevelops a novel framework for clustering problems, which is called Weighted\nSpectral Cluster Ensemble (WSCE), by exploiting some concepts from community\ndetection arena and graph based clustering. Under this framework, a new version\nof spectral clustering, which is called Two Kernels Spectral Clustering, is\nused for generating graphs based individual clustering results. Further, by\nusing modularity, which is a famous metric in the community detection, on the\ntransformed graph representation of individual clustering results, our approach\nprovides an effective diversity estimation for individual clustering results.\nMoreover, this paper introduces a new approach for combining the evaluated\nindividual clustering results without the procedure of thresholding.\nExperimental study on varied data sets demonstrates that the prosed approach\nachieves superior performance to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 09:29:21 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "1604.07180", "submitter": "Steffen Staab", "authors": "Steffen Staab and Sophie Stalla-Bourdillon and Laura Carmichael", "title": "Observing and Recommending from a Social Web with Biases", "comments": "Technical Report, University of Southampton, March 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research question this report addresses is: how, and to what extent,\nthose directly involved with the design, development and employment of a\nspecific black box algorithm can be certain that it is not unlawfully\ndiscriminating (directly and/or indirectly) against particular persons with\nprotected characteristics (e.g. gender, race and ethnicity)?\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 09:39:57 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Staab", "Steffen", ""], ["Stalla-Bourdillon", "Sophie", ""], ["Carmichael", "Laura", ""]]}, {"id": "1604.07209", "submitter": "Tobias Schnabel", "authors": "Tobias Schnabel, Adith Swaminathan, Peter Frazier, Thorsten Joachims", "title": "Unbiased Comparative Evaluation of Ranking Functions", "comments": "Under review; 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eliciting relevance judgments for ranking evaluation is labor-intensive and\ncostly, motivating careful selection of which documents to judge. Unlike\ntraditional approaches that make this selection deterministically,\nprobabilistic sampling has shown intriguing promise since it enables the design\nof estimators that are provably unbiased even when reusing data with missing\njudgments. In this paper, we first unify and extend these sampling approaches\nby viewing the evaluation problem as a Monte Carlo estimation task that applies\nto a large number of common IR metrics. Drawing on the theoretical clarity that\nthis view offers, we tackle three practical evaluation scenarios: comparing two\nsystems, comparing $k$ systems against a baseline, and ranking $k$ systems. For\neach scenario, we derive an estimator and a variance-optimizing sampling\ndistribution while retaining the strengths of sampling-based evaluation,\nincluding unbiasedness, reusability despite missing data, and ease of use in\npractice. In addition to the theoretical contribution, we empirically evaluate\nour methods against previously used sampling heuristics and find that they\ngenerally cut the number of required relevance judgments at least in half.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 11:28:21 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Schnabel", "Tobias", ""], ["Swaminathan", "Adith", ""], ["Frazier", "Peter", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1604.07211", "submitter": "Edip Demirbilek", "authors": "Edip Demirbilek and Jean-Charles Gr\\'egoire", "title": "Towards Reduced Reference Parametric Models for Estimating Audiovisual\n  Quality in Multimedia Services", "comments": "Accepted to ICC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed reduced reference parametric models for estimating\nperceived quality in audiovisual multimedia services. We have created 144\nunique configurations for audiovisual content including various application and\nnetwork parameters such as bitrates and distortions in terms of bandwidth,\npacket loss rate and jitter. To generate the data needed for model training and\nvalidation we have tasked 24 subjects, in a controlled environment, to rate the\noverall audiovisual quality on the absolute category rating (ACR) 5-level\nquality scale. We have developed models using Random Forest and Neural Network\nbased machine learning methods in order to estimate Mean Opinion Scores (MOS)\nvalues. We have used information retrieved from the packet headers and side\ninformation provided as network parameters for model training. Random Forest\nbased models have performed better in terms of Root Mean Square Error (RMSE)\nand Pearson correlation coefficient. The side information proved to be very\neffective in developing the model. We have found that, while the model\nperformance might be improved by replacing the side information with more\naccurate bit stream level measurements, they are performing well in estimating\nperceived quality in audiovisual multimedia services.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 11:43:09 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Demirbilek", "Edip", ""], ["Gr\u00e9goire", "Jean-Charles", ""]]}, {"id": "1604.07243", "submitter": "Mattia Desana", "authors": "Mattia Desana and Christoph Schn\\\"orr", "title": "Learning Arbitrary Sum-Product Network Leaves with\n  Expectation-Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sum-Product Networks with complex probability distribution at the leaves have\nbeen shown to be powerful tractable-inference probabilistic models. However,\nwhile learning the internal parameters has been amply studied, learning complex\nleaf distribution is an open problem with only few results available in special\ncases. In this paper we derive an efficient method to learn a very large class\nof leaf distributions with Expectation-Maximization. The EM updates have the\nform of simple weighted maximum likelihood problems, allowing to use any\ndistribution that can be learned with maximum likelihood, even approximately.\nThe algorithm has cost linear in the model size and converges even if only\npartial optimizations are performed. We demonstrate this approach with\nexperiments on twenty real-life datasets for density estimation, using tree\ngraphical models as leaves. Our model outperforms state-of-the-art methods for\nparameter learning despite using SPNs with much fewer parameters.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 13:22:55 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 16:42:59 GMT"}, {"version": "v3", "created": "Wed, 14 Jun 2017 14:08:22 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Desana", "Mattia", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1604.07255", "submitter": "Tom Zahavy", "authors": "Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J. Mankowitz, Shie\n  Mannor", "title": "A Deep Hierarchical Approach to Lifelong Learning in Minecraft", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a lifelong learning system that has the ability to reuse and\ntransfer knowledge from one task to another while efficiently retaining the\npreviously learned knowledge-base. Knowledge is transferred by learning\nreusable skills to solve tasks in Minecraft, a popular video game which is an\nunsolved and high-dimensional lifelong learning problem. These reusable skills,\nwhich we refer to as Deep Skill Networks, are then incorporated into our novel\nHierarchical Deep Reinforcement Learning Network (H-DRLN) architecture using\ntwo techniques: (1) a deep skill array and (2) skill distillation, our novel\nvariation of policy distillation (Rusu et. al. 2015) for learning skills. Skill\ndistillation enables the HDRLN to efficiently retain knowledge and therefore\nscale in lifelong learning, by accumulating knowledge and encapsulating\nmultiple reusable skills into a single distilled network. The H-DRLN exhibits\nsuperior performance and lower learning sample complexity compared to the\nregular Deep Q Network (Mnih et. al. 2015) in sub-domains of Minecraft.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 13:45:50 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 13:18:20 GMT"}, {"version": "v3", "created": "Wed, 30 Nov 2016 17:35:27 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Tessler", "Chen", ""], ["Givony", "Shahar", ""], ["Zahavy", "Tom", ""], ["Mankowitz", "Daniel J.", ""], ["Mannor", "Shie", ""]]}, {"id": "1604.07269", "submitter": "Ilya Loshchilov", "authors": "Ilya Loshchilov and Frank Hutter", "title": "CMA-ES for Hyperparameter Optimization of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperparameters of deep neural networks are often optimized by grid search,\nrandom search or Bayesian optimization. As an alternative, we propose to use\nthe Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known\nfor its state-of-the-art performance in derivative-free optimization. CMA-ES\nhas some useful invariance properties and is friendly to parallel evaluations\nof solutions. We provide a toy example comparing CMA-ES and state-of-the-art\nBayesian optimization algorithms for tuning the hyperparameters of a\nconvolutional neural network for the MNIST dataset on 30 GPUs in parallel.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 14:17:08 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Loshchilov", "Ilya", ""], ["Hutter", "Frank", ""]]}, {"id": "1604.07316", "submitter": "Urs Muller", "authors": "Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard\n  Firner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs\n  Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, Karol Zieba", "title": "End to End Learning for Self-Driving Cars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We trained a convolutional neural network (CNN) to map raw pixels from a\nsingle front-facing camera directly to steering commands. This end-to-end\napproach proved surprisingly powerful. With minimum training data from humans\nthe system learns to drive in traffic on local roads with or without lane\nmarkings and on highways. It also operates in areas with unclear visual\nguidance such as in parking lots and on unpaved roads.\n  The system automatically learns internal representations of the necessary\nprocessing steps such as detecting useful road features with only the human\nsteering angle as the training signal. We never explicitly trained it to\ndetect, for example, the outline of roads.\n  Compared to explicit decomposition of the problem, such as lane marking\ndetection, path planning, and control, our end-to-end system optimizes all\nprocessing steps simultaneously. We argue that this will eventually lead to\nbetter performance and smaller systems. Better performance will result because\nthe internal components self-optimize to maximize overall system performance,\ninstead of optimizing human-selected intermediate criteria, e.g., lane\ndetection. Such criteria understandably are selected for ease of human\ninterpretation which doesn't automatically guarantee maximum system\nperformance. Smaller networks are possible because the system learns to solve\nthe problem with the minimal number of processing steps.\n  We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX\nself-driving car computer also running Torch 7 for determining where to drive.\nThe system operates at 30 frames per second (FPS).\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 16:03:56 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Bojarski", "Mariusz", ""], ["Del Testa", "Davide", ""], ["Dworakowski", "Daniel", ""], ["Firner", "Bernhard", ""], ["Flepp", "Beat", ""], ["Goyal", "Prasoon", ""], ["Jackel", "Lawrence D.", ""], ["Monfort", "Mathew", ""], ["Muller", "Urs", ""], ["Zhang", "Jiakai", ""], ["Zhang", "Xin", ""], ["Zhao", "Jake", ""], ["Zieba", "Karol", ""]]}, {"id": "1604.07356", "submitter": "Francois Fagan", "authors": "Krzysztof Choromanski, Francois Fagan", "title": "Fast nonlinear embeddings via structured matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new paradigm for speeding up randomized computations of several\nfrequently used functions in machine learning. In particular, our paradigm can\nbe applied for improving computations of kernels based on random embeddings.\nAbove that, the presented framework covers multivariate randomized functions.\nAs a byproduct, we propose an algorithmic approach that also leads to a\nsignificant reduction of space complexity. Our method is based on careful\nrecycling of Gaussian vectors into structured matrices that share properties of\nfully random matrices. The quality of the proposed structured approach follows\nfrom combinatorial properties of the graphs encoding correlations between rows\nof these structured matrices. Our framework covers as special cases already\nknown structured approaches such as the Fast Johnson-Lindenstrauss Transform,\nbut is much more general since it can be applied also to highly nonlinear\nembeddings. We provide strong concentration results showing the quality of the\npresented paradigm.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 18:33:59 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Choromanski", "Krzysztof", ""], ["Fagan", "Francois", ""]]}, {"id": "1604.07379", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell,\n  Alexei A. Efros", "title": "Context Encoders: Feature Learning by Inpainting", "comments": "New results on ImageNet Generation", "journal-ref": "CVPR 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised visual feature learning algorithm driven by\ncontext-based pixel prediction. By analogy with auto-encoders, we propose\nContext Encoders -- a convolutional neural network trained to generate the\ncontents of an arbitrary image region conditioned on its surroundings. In order\nto succeed at this task, context encoders need to both understand the content\nof the entire image, as well as produce a plausible hypothesis for the missing\npart(s). When training context encoders, we have experimented with both a\nstandard pixel-wise reconstruction loss, as well as a reconstruction plus an\nadversarial loss. The latter produces much sharper results because it can\nbetter handle multiple modes in the output. We found that a context encoder\nlearns a representation that captures not just appearance but also the\nsemantics of visual structures. We quantitatively demonstrate the effectiveness\nof our learned features for CNN pre-training on classification, detection, and\nsegmentation tasks. Furthermore, context encoders can be used for semantic\ninpainting tasks, either stand-alone or as initialization for non-parametric\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 19:42:46 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 20:56:42 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Pathak", "Deepak", ""], ["Krahenbuhl", "Philipp", ""], ["Donahue", "Jeff", ""], ["Darrell", "Trevor", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1604.07484", "submitter": "Maziar Raissi", "authors": "Maziar Raissi and George Karniadakis", "title": "Deep Multi-fidelity Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel multi-fidelity framework that goes far beyond the\nclassical AR(1) Co-kriging scheme of Kennedy and O'Hagan (2000). Our method can\nhandle general discontinuous cross-correlations among systems with different\nlevels of fidelity. A combination of multi-fidelity Gaussian Processes (AR(1)\nCo-kriging) and deep neural networks enables us to construct a method that is\nimmune to discontinuities. We demonstrate the effectiveness of the new\ntechnology using standard benchmark problems designed to resemble the outputs\nof complicated high- and low-fidelity codes.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 00:44:42 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Raissi", "Maziar", ""], ["Karniadakis", "George", ""]]}, {"id": "1604.07554", "submitter": "Muhammad Yousefnezhad", "authors": "Maziar Kazemi, Muhammad Yousefnezhad, Saber Nourian", "title": "A New Approach in Persian Handwritten Letters Recognition Using Error\n  Correcting Output Coding", "comments": "Journal of Advances in Computer Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification Ensemble, which uses the weighed polling of outputs, is the\nart of combining a set of basic classifiers for generating high-performance,\nrobust and more stable results. This study aims to improve the results of\nidentifying the Persian handwritten letters using Error Correcting Output\nCoding (ECOC) ensemble method. Furthermore, the feature selection is used to\nreduce the costs of errors in our proposed method. ECOC is a method for\ndecomposing a multi-way classification problem into many binary classification\ntasks; and then combining the results of the subtasks into a hypothesized\nsolution to the original problem. Firstly, the image features are extracted by\nPrincipal Components Analysis (PCA). After that, ECOC is used for\nidentification the Persian handwritten letters which it uses Support Vector\nMachine (SVM) as the base classifier. The empirical results of applying this\nensemble method using 10 real-world data sets of Persian handwritten letters\nindicate that this method has better results in identifying the Persian\nhandwritten letters than other ensemble methods and also single\nclassifications. Moreover, by testing a number of different features, this\npaper found that we can reduce the additional cost in feature selection stage\nby using this method.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 07:43:59 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Kazemi", "Maziar", ""], ["Yousefnezhad", "Muhammad", ""], ["Nourian", "Saber", ""]]}, {"id": "1604.07638", "submitter": "Yixin Bao", "authors": "Yixin Bao, Xiaoke Wang, Zhi Wang, Chuan Wu, Francis C.M. Lau", "title": "Online Influence Maximization in Non-Stationary Social Networks", "comments": "10 pages. To appear in IEEE/ACM IWQoS 2016. Full version", "journal-ref": null, "doi": "10.1109/IWQoS.2016.7590438", "report-no": null, "categories": "cs.SI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks have been popular platforms for information propagation. An\nimportant use case is viral marketing: given a promotion budget, an advertiser\ncan choose some influential users as the seed set and provide them free or\ndiscounted sample products; in this way, the advertiser hopes to increase the\npopularity of the product in the users' friend circles by the world-of-mouth\neffect, and thus maximizes the number of users that information of the\nproduction can reach. There has been a body of literature studying the\ninfluence maximization problem. Nevertheless, the existing studies mostly\ninvestigate the problem on a one-off basis, assuming fixed known influence\nprobabilities among users, or the knowledge of the exact social network\ntopology. In practice, the social network topology and the influence\nprobabilities are typically unknown to the advertiser, which can be varying\nover time, i.e., in cases of newly established, strengthened or weakened social\nties. In this paper, we focus on a dynamic non-stationary social network and\ndesign a randomized algorithm, RSB, based on multi-armed bandit optimization,\nto maximize influence propagation over time. The algorithm produces a sequence\nof online decisions and calibrates its explore-exploit strategy utilizing\noutcomes of previous decisions. It is rigorously proven to achieve an\nupper-bounded regret in reward and applicable to large-scale social networks.\nPractical effectiveness of the algorithm is evaluated using both synthetic and\nreal-world datasets, which demonstrates that our algorithm outperforms previous\nstationary methods under non-stationary conditions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 12:02:55 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Bao", "Yixin", ""], ["Wang", "Xiaoke", ""], ["Wang", "Zhi", ""], ["Wu", "Chuan", ""], ["Lau", "Francis C. M.", ""]]}, {"id": "1604.07706", "submitter": "Shuai Li", "authors": "Nathan Korda and Balazs Szorenyi and Shuai Li", "title": "Distributed Clustering of Linear Bandits in Peer to Peer Networks", "comments": "The 33rd ICML, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide two distributed confidence ball algorithms for solving linear\nbandit problems in peer to peer networks with limited communication\ncapabilities. For the first, we assume that all the peers are solving the same\nlinear bandit problem, and prove that our algorithm achieves the optimal\nasymptotic regret rate of any centralised algorithm that can instantly\ncommunicate information between the peers. For the second, we assume that there\nare clusters of peers solving the same bandit problem within each cluster, and\nwe prove that our algorithm discovers these clusters, while achieving the\noptimal asymptotic regret rate within each one. Through experiments on several\nreal-world datasets, we demonstrate the performance of proposed algorithms\ncompared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 14:59:43 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 06:12:46 GMT"}, {"version": "v3", "created": "Tue, 7 Jun 2016 08:06:23 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Korda", "Nathan", ""], ["Szorenyi", "Balazs", ""], ["Li", "Shuai", ""]]}, {"id": "1604.07711", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain", "title": "Condorcet's Jury Theorem for Consensus Clustering and its Implications\n  for Diversity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Condorcet's Jury Theorem has been invoked for ensemble classifiers to\nindicate that the combination of many classifiers can have better predictive\nperformance than a single classifier. Such a theoretical underpinning is\nunknown for consensus clustering. This article extends Condorcet's Jury Theorem\nto the mean partition approach under the additional assumptions that a unique\nground-truth partition exists and sample partitions are drawn from a\nsufficiently small ball containing the ground-truth. As an implication of\npractical relevance, we question the claim that the quality of consensus\nclustering depends on the diversity of the sample partitions. Instead, we\nconjecture that limiting the diversity of the mean partitions is necessary for\ncontrolling the quality.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 15:14:49 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 10:06:28 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Jain", "Brijnesh J.", ""]]}, {"id": "1604.07759", "submitter": "Maxime Gasse", "authors": "Maxime Gasse and Alex Aussem", "title": "F-measure Maximization in Multi-Label Classification with Conditionally\n  Independent Label Subsets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a method to improve the exact F-measure maximization algorithm\ncalled GFM, proposed in (Dembczynski et al. 2011) for multi-label\nclassification, assuming the label set can be can partitioned into\nconditionally independent subsets given the input features. If the labels were\nall independent, the estimation of only $m$ parameters ($m$ denoting the number\nof labels) would suffice to derive Bayes-optimal predictions in $O(m^2)$\noperations. In the general case, $m^2+1$ parameters are required by GFM, to\nsolve the problem in $O(m^3)$ operations. In this work, we show that the number\nof parameters can be reduced further to $m^2/n$, in the best case, assuming the\nlabel set can be partitioned into $n$ conditionally independent subsets. As\nthis label partition needs to be estimated from the data beforehand, we use\nfirst the procedure proposed in (Gasse et al. 2015) that finds such partition\nand then infer the required parameters locally in each label subset. The latter\nare aggregated and serve as input to GFM to form the Bayes-optimal prediction.\nWe show on a synthetic experiment that the reduction in the number of\nparameters brings about significant benefits in terms of performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 17:18:24 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 12:26:27 GMT"}, {"version": "v3", "created": "Fri, 1 Jul 2016 13:17:01 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Gasse", "Maxime", ""], ["Aussem", "Alex", ""]]}, {"id": "1604.07796", "submitter": "Henry Lo", "authors": "Henry Z. Lo and Kevin Amaral and Wei Ding", "title": "Scale Normalization", "comments": "Preliminary version submitted to ICLR workshop 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the difficulties of training deep neural networks is caused by\nimproper scaling between layers. Scaling issues introduce exploding / gradient\nproblems, and have typically been addressed by careful scale-preserving\ninitialization. We investigate the value of preserving scale, or isometry,\nbeyond the initial weights. We propose two methods of maintaing isometry, one\nexact and one stochastic. Preliminary experiments show that for both\ndeterminant and scale-normalization effectively speeds up learning. Results\nsuggest that isometry is important in the beginning of learning, and\nmaintaining it leads to faster learning.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 19:04:59 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Lo", "Henry Z.", ""], ["Amaral", "Kevin", ""], ["Ding", "Wei", ""]]}, {"id": "1604.07866", "submitter": "Cristian Canton Ferrer", "authors": "Laura Leal-Taix\\'e, Cristian Canton Ferrer, Konrad Schindler", "title": "Learning by tracking: Siamese CNN for robust target association", "comments": null, "journal-ref": "Computer Vision and Pattern Recognition Conference Workshops\n  (CVPRW). DeepVision: Deep Learning for Computer Vision. 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach to the task of data association within\nthe context of pedestrian tracking, by introducing a two-stage learning scheme\nto match pairs of detections. First, a Siamese convolutional neural network\n(CNN) is trained to learn descriptors encoding local spatio-temporal structures\nbetween the two input image patches, aggregating pixel values and optical flow\ninformation. Second, a set of contextual features derived from the position and\nsize of the compared input patches are combined with the CNN output by means of\na gradient boosting classifier to generate the final matching probability. This\nlearning approach is validated by using a linear programming based multi-person\ntracker showing that even a simple and efficient tracker may outperform much\nmore complex models when fed with our learned matching probabilities. Results\non publicly available sequences show that our method meets state-of-the-art\nstandards in multiple people tracking.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 21:42:51 GMT"}, {"version": "v2", "created": "Fri, 29 Apr 2016 16:20:16 GMT"}, {"version": "v3", "created": "Thu, 4 Aug 2016 15:01:36 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Leal-Taix\u00e9", "Laura", ""], ["Ferrer", "Cristian Canton", ""], ["Schindler", "Konrad", ""]]}, {"id": "1604.07878", "submitter": "Muhammad Yousefnezhad", "authors": "Ali Reihanian, Behrouz Minaei-Bidgoli, Muhammad Yousefnezhad", "title": "Evaluating the effect of topic consideration in identifying communities\n  of rating-based social networks", "comments": "International Conference on Information and Knowledge Technology\n  (IKT) 2015", "journal-ref": null, "doi": "10.1109/IKT.2015.7288793", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding meaningful communities in social network has attracted the attentions\nof many researchers. The community structure of complex networks reveals both\ntheir organization and hidden relations among their constituents. Most of the\nresearches in the field of community detection mainly focus on the topological\nstructure of the network without performing any content analysis. Nowadays,\nreal world social networks are containing a vast range of information including\nshared objects, comments, following information, etc. In recent years, a number\nof researches have proposed approaches which consider both the contents that\nare interchanged in the networks and the topological structures of the networks\nin order to find more meaningful communities. In this research, the effect of\ntopic analysis in finding more meaningful communities in social networking\nsites in which the users express their feelings toward different objects (like\nmovies) by the means of rating is demonstrated by performing extensive\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 22:38:47 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Reihanian", "Ali", ""], ["Minaei-Bidgoli", "Behrouz", ""], ["Yousefnezhad", "Muhammad", ""]]}, {"id": "1604.07904", "submitter": "Ruck Thawonmas", "authors": "Tung Nguyen, Kazuki Mori, and Ruck Thawonmas", "title": "Image Colorization Using a Deep Convolutional Neural Network", "comments": null, "journal-ref": "Proc. of ASIAGRAPH 2016, Toyama, Japan, pp. 49-50, Mar. 5-6, 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach that uses deep learning techniques\nfor colorizing grayscale images. By utilizing a pre-trained convolutional\nneural network, which is originally designed for image classification, we are\nable to separate content and style of different images and recombine them into\na single image. We then propose a method that can add colors to a grayscale\nimage by combining its content with style of a color image having semantic\nsimilarity with the grayscale one. As an application, to our knowledge the\nfirst of its kind, we use the proposed method to colorize images of ukiyo-e a\ngenre of Japanese painting?and obtain interesting results, showing the\npotential of this method in the growing field of computer assisted art.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 02:16:43 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Nguyen", "Tung", ""], ["Mori", "Kazuki", ""], ["Thawonmas", "Ruck", ""]]}, {"id": "1604.07928", "submitter": "Shandian Zhe", "authors": "Shandian Zhe, Kai Zhang, Pengyuan Wang, Kuang-chih Lee, Zenglin Xu,\n  Yuan Qi, Zoubin Ghahramani", "title": "Distributed Flexible Nonlinear Tensor Factorization", "comments": "Gaussian process, tensor factorization, multidimensional arrays,\n  large scale, spark, map-reduce", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor factorization is a powerful tool to analyse multi-way data. Compared\nwith traditional multi-linear methods, nonlinear tensor factorization models\nare capable of capturing more complex relationships in the data. However, they\nare computationally expensive and may suffer severe learning bias in case of\nextreme data sparsity. To overcome these limitations, in this paper we propose\na distributed, flexible nonlinear tensor factorization model. Our model can\neffectively avoid the expensive computations and structural restrictions of the\nKronecker-product in existing TGP formulations, allowing an arbitrary subset of\ntensorial entries to be selected to contribute to the training. At the same\ntime, we derive a tractable and tight variational evidence lower bound (ELBO)\nthat enables highly decoupled, parallel computations and high-quality\ninference. Based on the new bound, we develop a distributed inference algorithm\nin the MapReduce framework, which is key-value-free and can fully exploit the\nmemory cache mechanism in fast MapReduce systems such as SPARK. Experimental\nresults fully demonstrate the advantages of our method over several\nstate-of-the-art approaches, in terms of both predictive performance and\ncomputational efficiency. Moreover, our approach shows a promising potential in\nthe application of Click-Through-Rate (CTR) prediction for online advertising.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 04:18:32 GMT"}, {"version": "v2", "created": "Sun, 22 May 2016 00:00:23 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Zhe", "Shandian", ""], ["Zhang", "Kai", ""], ["Wang", "Pengyuan", ""], ["Lee", "Kuang-chih", ""], ["Xu", "Zenglin", ""], ["Qi", "Yuan", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1604.08079", "submitter": "Paula Branco", "authors": "Paula Branco, Rita P. Ribeiro, Luis Torgo", "title": "UBL: an R package for Utility-based Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes the R package UBL that allows the use of several\nmethods for handling utility-based learning problems. Classification and\nregression problems that assume non-uniform costs and/or benefits pose serious\nchallenges to predictive analytic tasks. In the context of meteorology,\nfinance, medicine, ecology, among many other, specific domain information\nconcerning the preference bias of the users must be taken into account to\nenhance the models predictive performance. To deal with this problem, a large\nnumber of techniques was proposed by the research community for both\nclassification and regression tasks. The main goal of UBL package is to\nfacilitate the utility-based predictive analytic task by providing a set of\nmethods to deal with this type of problems in the R environment. It is a\nversatile tool that provides mechanisms to handle both regression and\nclassification (binary and multiclass) tasks. Moreover, UBL package allows the\nuser to specify his domain preferences, but it also provides some automatic\nmethods that try to infer those preference bias from the domain, considering\nsome common known settings.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 14:13:11 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 23:08:46 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Branco", "Paula", ""], ["Ribeiro", "Rita P.", ""], ["Torgo", "Luis", ""]]}, {"id": "1604.08098", "submitter": "Lei Han", "authors": "Lei Han, Kean Ming Tan, Ting Yang and Tong Zhang", "title": "Local Uncertainty Sampling for Large-Scale Multi-Class Logistic\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge for building statistical models in the big data era is that\nthe available data volume far exceeds the computational capability. A common\napproach for solving this problem is to employ a subsampled dataset that can be\nhandled by available computational resources. In this paper, we propose a\ngeneral subsampling scheme for large-scale multi-class logistic regression and\nexamine the variance of the resulting estimator. We show that asymptotically,\nthe proposed method always achieves a smaller variance than that of the uniform\nrandom sampling. Moreover, when the classes are conditionally imbalanced,\nsignificant improvement over uniform sampling can be achieved. Empirical\nperformance of the proposed method is compared to other methods on both\nsimulated and real-world datasets, and these results match and confirm our\ntheoretical analysis.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 15:00:12 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 09:05:18 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2018 05:08:08 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Han", "Lei", ""], ["Tan", "Kean Ming", ""], ["Yang", "Ting", ""], ["Zhang", "Tong", ""]]}, {"id": "1604.08153", "submitter": "Kai Arulkumaran", "authors": "Kai Arulkumaran, Nat Dilokthanakul, Murray Shanahan, Anil Anthony\n  Bharath", "title": "Classifying Options for Deep Reinforcement Learning", "comments": "IJCAI 2016 Workshop on Deep Reinforcement Learning: Frontiers and\n  Challenges", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we combine one method for hierarchical reinforcement learning -\nthe options framework - with deep Q-networks (DQNs) through the use of\ndifferent \"option heads\" on the policy network, and a supervisory network for\nchoosing between the different options. We utilise our setup to investigate the\neffects of architectural constraints in subtasks with positive and negative\ntransfer, across a range of network capacities. We empirically show that our\naugmented DQN has lower sample complexity when simultaneously learning subtasks\nwith negative transfer, without degrading performance when learning subtasks\nwith positive transfer.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 17:48:39 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 16:05:31 GMT"}, {"version": "v3", "created": "Mon, 19 Jun 2017 15:34:58 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Arulkumaran", "Kai", ""], ["Dilokthanakul", "Nat", ""], ["Shanahan", "Murray", ""], ["Bharath", "Anil Anthony", ""]]}, {"id": "1604.08220", "submitter": "Ragav Venkatesan", "authors": "Ragav Venkatesan, Baoxin Li", "title": "Diving deeper into mentee networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern computer vision is all about the possession of powerful image\nrepresentations. Deeper and deeper convolutional neural networks have been\nbuilt using larger and larger datasets and are made publicly available. A large\nswath of computer vision scientists use these pre-trained networks with varying\ndegrees of successes in various tasks. Even though there is tremendous success\nin copying these networks, the representational space is not learnt from the\ntarget dataset in a traditional manner. One of the reasons for opting to use a\npre-trained network over a network learnt from scratch is that small datasets\nprovide less supervision and require meticulous regularization, smaller and\ncareful tweaking of learning rates to even achieve stable learning without\nweight explosion. It is often the case that large deep networks are not\nportable, which necessitates the ability to learn mid-sized networks from\nscratch.\n  In this article, we dive deeper into training these mid-sized networks on\nsmall datasets from scratch by drawing additional supervision from a large\npre-trained network. Such learning also provides better generalization\naccuracies than networks trained with common regularization techniques such as\nl2, l1 and dropouts. We show that features learnt thus, are more general than\nthose learnt independently. We studied various characteristics of such networks\nand found some interesting behaviors.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 20:05:45 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Venkatesan", "Ragav", ""], ["Li", "Baoxin", ""]]}, {"id": "1604.08275", "submitter": "Nicolas Papernot", "authors": "Nicolas Papernot and Patrick McDaniel and Ananthram Swami and Richard\n  Harang", "title": "Crafting Adversarial Input Sequences for Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are frequently used to solve complex security\nproblems, as well as to make decisions in sensitive situations like guiding\nautonomous vehicles or predicting financial market behaviors. Previous efforts\nhave shown that numerous machine learning models were vulnerable to adversarial\nmanipulations of their inputs taking the form of adversarial samples. Such\ninputs are crafted by adding carefully selected perturbations to legitimate\ninputs so as to force the machine learning model to misbehave, for instance by\noutputting a wrong class if the machine learning task of interest is\nclassification. In fact, to the best of our knowledge, all previous work on\nadversarial samples crafting for neural network considered models used to solve\nclassification tasks, most frequently in computer vision applications. In this\npaper, we contribute to the field of adversarial machine learning by\ninvestigating adversarial input sequences for recurrent neural networks\nprocessing sequential data. We show that the classes of algorithms introduced\npreviously to craft adversarial samples misclassified by feed-forward neural\nnetworks can be adapted to recurrent neural networks. In a experiment, we show\nthat adversaries can craft adversarial sequences misleading both categorical\nand sequential recurrent neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 00:35:32 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Papernot", "Nicolas", ""], ["McDaniel", "Patrick", ""], ["Swami", "Ananthram", ""], ["Harang", "Richard", ""]]}, {"id": "1604.08291", "submitter": "Dacheng Tao", "authors": "Chang Xu, Dacheng Tao, Chao Xu", "title": "Streaming View Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An underlying assumption in conventional multi-view learning algorithms is\nthat all views can be simultaneously accessed. However, due to various factors\nwhen collecting and pre-processing data from different views, the streaming\nview setting, in which views arrive in a streaming manner, is becoming more\ncommon. By assuming that the subspaces of a multi-view model trained over past\nviews are stable, here we fine tune their combination weights such that the\nwell-trained multi-view model is compatible with new views. This largely\novercomes the burden of learning new view functions and updating past view\nfunctions. We theoretically examine convergence issues and the influence of\nstreaming views in the proposed algorithm. Experimental results on real-world\ndatasets suggest that studying the streaming views problem in multi-view\nlearning is significant and that the proposed algorithm can effectively handle\nstreaming views in different applications.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 02:37:03 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Xu", "Chang", ""], ["Tao", "Dacheng", ""], ["Xu", "Chao", ""]]}, {"id": "1604.08352", "submitter": "Th\\'eodore Bluche", "authors": "Th\\'eodore Bluche", "title": "Joint Line Segmentation and Transcription for End-to-End Handwritten\n  Paragraph Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline handwriting recognition systems require cropped text line images for\nboth training and recognition. On the one hand, the annotation of position and\ntranscript at line level is costly to obtain. On the other hand, automatic line\nsegmentation algorithms are prone to errors, compromising the subsequent\nrecognition. In this paper, we propose a modification of the popular and\nefficient multi-dimensional long short-term memory recurrent neural networks\n(MDLSTM-RNNs) to enable end-to-end processing of handwritten paragraphs. More\nparticularly, we replace the collapse layer transforming the two-dimensional\nrepresentation into a sequence of predictions by a recurrent version which can\nrecognize one line at a time. In the proposed model, a neural network performs\na kind of implicit line segmentation by computing attention weights on the\nimage representation. The experiments on paragraphs of Rimes and IAM database\nyield results that are competitive with those of networks trained at line\nlevel, and constitute a significant step towards end-to-end transcription of\nfull documents.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 09:08:30 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Bluche", "Th\u00e9odore", ""]]}, {"id": "1604.08382", "submitter": "Frederik Ruelens", "authors": "Bert J. Claessens and Peter Vrancx and Frederik Ruelens", "title": "Convolutional Neural Networks For Automatic State-Time Feature\n  Extraction in Reinforcement Learning Applied to Residential Load Control", "comments": "Submitted to Transactions on Smart Grid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct load control of a heterogeneous cluster of residential demand\nflexibility sources is a high-dimensional control problem with partial\nobservability. This work proposes a novel approach that uses a convolutional\nneural network to extract hidden state-time features to mitigate the curse of\npartial observability. More specific, a convolutional neural network is used as\na function approximator to estimate the state-action value function or\nQ-function in the supervised learning step of fitted Q-iteration. The approach\nis evaluated in a qualitative simulation, comprising a cluster of\nthermostatically controlled loads that only share their air temperature, whilst\ntheir envelope temperature remains hidden. The simulation results show that the\npresented approach is able to capture the underlying hidden features and\nsuccessfully reduce the electricity cost the cluster.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 11:53:47 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 15:52:42 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Claessens", "Bert J.", ""], ["Vrancx", "Peter", ""], ["Ruelens", "Frederik", ""]]}, {"id": "1604.08500", "submitter": "Zahra Roshan Zamir", "authors": "Z. Roshan Zamir", "title": "Detection of epileptic seizure in EEG signals using linear least squares\n  preprocessing", "comments": "Biological signal classification, Signal approximation, Feature\n  extraction, Data analysis, Linear least squares problems, EEG Seizure\n  detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An epileptic seizure is a transient event of abnormal excessive neuronal\ndischarge in the brain. This unwanted event can be obstructed by detection of\nelectrical changes in the brain that happen before the seizure takes place. The\nautomatic detection of seizures is necessary since the visual screening of EEG\nrecordings is a time consuming task and requires experts to improve the\ndiagnosis. Four linear least squares-based preprocessing models are proposed to\nextract key features of an EEG signal in order to detect seizures. The first\ntwo models are newly developed. The original signal (EEG) is approximated by a\nsinusoidal curve. Its amplitude is formed by a polynomial function and compared\nwith the pre developed spline function.Different statistical measures namely\nclassification accuracy, true positive and negative rates, false positive and\nnegative rates and precision are utilized to assess the performance of the\nproposed models. These metrics are derived from confusion matrices obtained\nfrom classifiers. Different classifiers are used over the original dataset and\nthe set of extracted features. The proposed models significantly reduce the\ndimension of the classification problem and the computational time while the\nclassification accuracy is improved in most cases. The first and third models\nare promising feature extraction methods. Logistic, LazyIB1, LazyIB5 and J48\nare the best classifiers. Their true positive and negative rates are $1$ while\nfalse positive and negative rates are zero and the corresponding precision\nvalues are $1$. Numerical results suggest that these models are robust and\nefficient for detecting epileptic seizure.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 01:01:26 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Zamir", "Z. Roshan", ""]]}, {"id": "1604.08608", "submitter": "Eric Makita", "authors": "Eric Makita, Artem Lenskiy", "title": "A movie genre prediction based on Multivariate Bernoulli model and genre\n  correlations", "comments": "5 pages, 8 figues, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Movie ratings play an important role both in determining the likelihood of a\npotential viewer to watch the movie and in reflecting the current viewer\nsatisfaction with the movie. They are available in several sources like the\ntelevision guide, best-selling reference books, newspaper columns, and\ntelevision programs. Furthermore, movie ratings are crucial for recommendation\nengines that track the behavior of all users and utilize the information to\nsuggest items they might like. Movie ratings in most cases, thus, provide\ninformation that might be more important than movie feature-based data. It is\nintuitively appealing that information about the viewing preferences in movie\ngenres is sufficient for predicting a genre of an unlabeled movie. In order to\npredict movie genres, we treat ratings as a feature vector, apply the Bernoulli\nevent model to estimate the likelihood of a movies given genre, and evaluate\nthe posterior probability of the genre of a given movie using the Bayes rule.\nThe goal of the proposed technique is to efficiently use the movie ratings for\nthe task of predicting movie genres. In our approach we attempted to answer the\nquestion: \"Given the set of users who watched a movie, is it possible to\npredict the genre of a movie based on its ratings?\" Our simulation results with\nMovieLens 100k data demonstrated the efficiency and accuracy of our proposed\ntechnique, achieving 59% prediction rate for exact prediction and 69% when\nincluding correlated genres.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 08:49:51 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Makita", "Eric", ""], ["Lenskiy", "Artem", ""]]}, {"id": "1604.08642", "submitter": "Yongyi Mao Dr", "authors": "Jianfeng Wen, Jianxin Li, Yongyi Mao, Shini Chen, Richong Zhang", "title": "On the representation and embedding of knowledge bases beyond binary\n  relations", "comments": "8 pages, to appear in IJCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The models developed to date for knowledge base embedding are all based on\nthe assumption that the relations contained in knowledge bases are binary. For\nthe training and testing of these embedding models, multi-fold (or n-ary)\nrelational data are converted to triples (e.g., in FB15K dataset) and\ninterpreted as instances of binary relations. This paper presents a canonical\nrepresentation of knowledge bases containing multi-fold relations. We show that\nthe existing embedding models on the popular FB15K datasets correspond to a\nsub-optimal modelling framework, resulting in a loss of structural information.\nWe advocate a novel modelling framework, which models multi-fold relations\ndirectly using this canonical representation. Using this framework, the\nexisting TransH model is generalized to a new model, m-TransH. We demonstrate\nexperimentally that m-TransH outperforms TransH by a large margin, thereby\nestablishing a new state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 22:42:38 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Wen", "Jianfeng", ""], ["Li", "Jianxin", ""], ["Mao", "Yongyi", ""], ["Chen", "Shini", ""], ["Zhang", "Richong", ""]]}, {"id": "1604.08685", "submitter": "Jiajun Wu", "authors": "Jiajun Wu, Tianfan Xue, Joseph J. Lim, Yuandong Tian, Joshua B.\n  Tenenbaum, Antonio Torralba, William T. Freeman", "title": "Single Image 3D Interpreter Network", "comments": "ECCV 2016 (oral). The first two authors contributed equally to this\n  work", "journal-ref": null, "doi": "10.1007/978-3-319-46466-4_22", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding 3D object structure from a single image is an important but\ndifficult task in computer vision, mostly due to the lack of 3D object\nannotations in real images. Previous work tackles this problem by either\nsolving an optimization task given 2D keypoint positions, or training on\nsynthetic data with ground truth 3D information. In this work, we propose 3D\nINterpreter Network (3D-INN), an end-to-end framework which sequentially\nestimates 2D keypoint heatmaps and 3D object structure, trained on both real\n2D-annotated images and synthetic 3D data. This is made possible mainly by two\ntechnical innovations. First, we propose a Projection Layer, which projects\nestimated 3D structure to 2D space, so that 3D-INN can be trained to predict 3D\nstructural parameters supervised by 2D annotations on real images. Second,\nheatmaps of keypoints serve as an intermediate representation connecting real\nand synthetic data, enabling 3D-INN to benefit from the variation and abundance\nof synthetic 3D objects, without suffering from the difference between the\nstatistics of real and synthesized images due to imperfect rendering. The\nnetwork achieves state-of-the-art performance on both 2D keypoint estimation\nand 3D structure recovery. We also show that the recovered 3D information can\nbe used in other vision applications, such as 3D rendering and image retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 04:52:46 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 19:35:54 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Wu", "Jiajun", ""], ["Xue", "Tianfan", ""], ["Lim", "Joseph J.", ""], ["Tian", "Yuandong", ""], ["Tenenbaum", "Joshua B.", ""], ["Torralba", "Antonio", ""], ["Freeman", "William T.", ""]]}, {"id": "1604.08716", "submitter": "Huy Phan", "authors": "Huy Phan, Marco Maass, Lars Hertel, Radoslaw Mazur, Ian McLoughlin,\n  Alfred Mertins", "title": "Learning Compact Structural Representations for Audio Events Using\n  Regressor Banks", "comments": "To appear in Proceedings of IEEE ICASSP 2016", "journal-ref": null, "doi": "10.1109/ICASSP.2016.7471667", "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new learned descriptor for audio signals which is efficient\nfor event representation. The entries of the descriptor are produced by\nevaluating a set of regressors on the input signal. The regressors are\nclass-specific and trained using the random regression forests framework. Given\nan input signal, each regressor estimates the onset and offset positions of the\ntarget event. The estimation confidence scores output by a regressor are then\nused to quantify how the target event aligns with the temporal structure of the\ncorresponding category. Our proposed descriptor has two advantages. First, it\nis compact, i.e. the dimensionality of the descriptor is equal to the number of\nevent classes. Second, we show that even simple linear classification models,\ntrained on our descriptor, yield better accuracies on audio event\nclassification task than not only the nonlinear baselines but also the\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 07:46:59 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Phan", "Huy", ""], ["Maass", "Marco", ""], ["Hertel", "Lars", ""], ["Mazur", "Radoslaw", ""], ["McLoughlin", "Ian", ""], ["Mertins", "Alfred", ""]]}, {"id": "1604.08723", "submitter": "Bob Sturm", "authors": "Bob L. Sturm, Jo\\~ao Felipe Santos, Oded Ben-Tal and Iryna Korshunova", "title": "Music transcription modelling and composition using deep learning", "comments": "16 pages, 4 figures, contribution to 1st Conference on Computer\n  Simulation of Musical Creativity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply deep learning methods, specifically long short-term memory (LSTM)\nnetworks, to music transcription modelling and composition. We build and train\nLSTM networks using approximately 23,000 music transcriptions expressed with a\nhigh-level vocabulary (ABC notation), and use them to generate new\ntranscriptions. Our practical aim is to create music transcription models\nuseful in particular contexts of music composition. We present results from\nthree perspectives: 1) at the population level, comparing descriptive\nstatistics of the set of training transcriptions and generated transcriptions;\n2) at the individual level, examining how a generated transcription reflects\nthe conventions of a music practice in the training transcriptions (Celtic\nfolk); 3) at the application level, using the system for idea generation in\nmusic composition. We make our datasets, software and sound examples open and\navailable: \\url{https://github.com/IraKorshunova/folk-rnn}.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 08:03:00 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Sturm", "Bob L.", ""], ["Santos", "Jo\u00e3o Felipe", ""], ["Ben-Tal", "Oded", ""], ["Korshunova", "Iryna", ""]]}, {"id": "1604.08740", "submitter": "Wouter Koolen", "authors": "Tim van Erven and Wouter M. Koolen", "title": "MetaGrad: Multiple Learning Rates in Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online convex optimization it is well known that certain subclasses of\nobjective functions are much easier than arbitrary convex functions. We are\ninterested in designing adaptive methods that can automatically get fast rates\nin as many such subclasses as possible, without any manual tuning. Previous\nadaptive methods are able to interpolate between strongly convex and general\nconvex functions. We present a new method, MetaGrad, that adapts to a much\nbroader class of functions, including exp-concave and strongly convex\nfunctions, but also various types of stochastic and non-stochastic functions\nwithout any curvature. For instance, MetaGrad can achieve logarithmic regret on\nthe unregularized hinge loss, even though it has no curvature, if the data come\nfrom a favourable probability distribution. MetaGrad's main feature is that it\nsimultaneously considers multiple learning rates. Unlike previous methods with\nprovable regret guarantees, however, its learning rates are not monotonically\ndecreasing over time and are not tuned based on a theoretically derived bound\non the regret. Instead, they are weighted directly proportional to their\nempirical performance on the data using a tilted exponential weights master\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 09:05:46 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 22:25:38 GMT"}, {"version": "v3", "created": "Tue, 1 Nov 2016 17:57:06 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["van Erven", "Tim", ""], ["Koolen", "Wouter M.", ""]]}, {"id": "1604.08772", "submitter": "Frederic Besse", "authors": "Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka\n  and Daan Wierstra", "title": "Towards Conceptual Compression", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple recurrent variational auto-encoder architecture that\nsignificantly improves image modeling. The system represents the\nstate-of-the-art in latent variable models for both the ImageNet and Omniglot\ndatasets. We show that it naturally separates global conceptual information\nfrom lower level details, thus addressing one of the fundamentally desired\nproperties of unsupervised learning. Furthermore, the possibility of\nrestricting ourselves to storing only global information about an image allows\nus to achieve high quality 'conceptual compression'.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 11:02:52 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Gregor", "Karol", ""], ["Besse", "Frederic", ""], ["Rezende", "Danilo Jimenez", ""], ["Danihelka", "Ivo", ""], ["Wierstra", "Daan", ""]]}, {"id": "1604.08852", "submitter": "Jeroen Zegers", "authors": "Jeroen Zegers, Hugo Van hamme", "title": "Joint Sound Source Separation and Speaker Recognition", "comments": "Submitted to INTERSPEECH2016. 4 pages, 1 extra page for references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative Matrix Factorization (NMF) has already been applied to learn\nspeaker characterizations from single or non-simultaneous speech for speaker\nrecognition applications. It is also known for its good performance in (blind)\nsource separation for simultaneous speech. This paper explains how NMF can be\nused to jointly solve the two problems in a multichannel speaker recognizer for\nsimultaneous speech. It is shown how state-of-the-art multichannel NMF for\nblind source separation can be easily extended to incorporate speaker\nrecognition. Experiments on the CHiME corpus show that this method outperforms\nthe sequential approach of first applying source separation, followed by\nspeaker recognition that uses state-of-the-art i-vector techniques.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 14:32:03 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Zegers", "Jeroen", ""], ["Van hamme", "Hugo", ""]]}, {"id": "1604.08859", "submitter": "Alexandre de Br\\'ebisson", "authors": "Alexandre de Br\\'ebisson, Pascal Vincent", "title": "The Z-loss: a shift and scale invariant classification loss belonging to\n  the Spherical Family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being the standard loss function to train multi-class neural\nnetworks, the log-softmax has two potential limitations. First, it involves\ncomputations that scale linearly with the number of output classes, which can\nrestrict the size of problems we are able to tackle with current hardware.\nSecond, it remains unclear how close it matches the task loss such as the top-k\nerror rate or other non-differentiable evaluation metrics which we aim to\noptimize ultimately. In this paper, we introduce an alternative classification\nloss function, the Z-loss, which is designed to address these two issues.\nUnlike the log-softmax, it has the desirable property of belonging to the\nspherical loss family (Vincent et al., 2015), a class of loss functions for\nwhich training can be performed very efficiently with a complexity independent\nof the number of output classes. We show experimentally that it significantly\noutperforms the other spherical loss functions previously investigated.\nFurthermore, we show on a word language modeling task that it also outperforms\nthe log-softmax with respect to certain ranking scores, such as top-k scores,\nsuggesting that the Z-loss has the flexibility to better match the task loss.\nThese qualities thus makes the Z-loss an appealing candidate to train very\nefficiently large output networks such as word-language models or other extreme\nclassification problems. On the One Billion Word (Chelba et al., 2014) dataset,\nwe are able to train a model with the Z-loss 40 times faster than the\nlog-softmax and more than 4 times faster than the hierarchical softmax.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 14:53:00 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 15:17:34 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["de Br\u00e9bisson", "Alexandre", ""], ["Vincent", "Pascal", ""]]}, {"id": "1604.08880", "submitter": "Shane Halloran", "authors": "Nils Y. Hammerla, Shane Halloran and Thomas Ploetz", "title": "Deep, Convolutional, and Recurrent Models for Human Activity Recognition\n  using Wearables", "comments": "Extended version has been accepted for publication at International\n  Joint Conference on Artificial Intelligence (IJCAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity recognition (HAR) in ubiquitous computing is beginning to\nadopt deep learning to substitute for well-established analysis techniques that\nrely on hand-crafted feature extraction and classification techniques. From\nthese isolated applications of custom deep architectures it is, however,\ndifficult to gain an overview of their suitability for problems ranging from\nthe recognition of manipulative gestures to the segmentation and identification\nof physical activities like running or ascending stairs. In this paper we\nrigorously explore deep, convolutional, and recurrent approaches across three\nrepresentative datasets that contain movement data captured with wearable\nsensors. We describe how to train recurrent approaches in this setting,\nintroduce a novel regularisation approach, and illustrate how they outperform\nthe state-of-the-art on a large benchmark dataset. Across thousands of\nrecognition experiments with randomly sampled model configurations we\ninvestigate the suitability of each model for different tasks in HAR, explore\nthe impact of hyperparameters using the fANOVA framework, and provide\nguidelines for the practitioner who wants to apply deep learning in their\nproblem setting.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 15:38:44 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Hammerla", "Nils Y.", ""], ["Halloran", "Shane", ""], ["Ploetz", "Thomas", ""]]}, {"id": "1604.08934", "submitter": "Sebastijan Dumancic", "authors": "Sebastijan Dumancic and Hendrik Blockeel", "title": "An expressive dissimilarity measure for relational clustering using\n  neighbourhood trees", "comments": "9 pages, 3 figures, 4 tables, submitted to ECMLPKDD 2017", "journal-ref": null, "doi": "10.1007/s10994-017-5644-6", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is an underspecified task: there are no universal criteria for\nwhat makes a good clustering. This is especially true for relational data,\nwhere similarity can be based on the features of individuals, the relationships\nbetween them, or a mix of both. Existing methods for relational clustering have\nstrong and often implicit biases in this respect. In this paper, we introduce a\nnovel similarity measure for relational data. It is the first measure to\nincorporate a wide variety of types of similarity, including similarity of\nattributes, similarity of relational context, and proximity in a hypergraph. We\nexperimentally evaluate how using this similarity affects the quality of\nclustering on very different types of datasets. The experiments demonstrate\nthat (a) using this similarity in standard clustering methods consistently\ngives good results, whereas other measures work well only on datasets that\nmatch their bias; and (b) on most datasets, the novel similarity outperforms\neven the best among the existing ones.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 18:48:53 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 08:45:19 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Dumancic", "Sebastijan", ""], ["Blockeel", "Hendrik", ""]]}]