[{"id": "1603.00050", "submitter": "Glenn Healey", "authors": "Glenn Healey", "title": "Learning, Visualizing, and Exploiting a Model for the Intrinsic Value of\n  a Batted Ball", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for learning the intrinsic value of a batted ball in\nbaseball. This work addresses the fundamental problem of separating the value\nof a batted ball at contact from factors such as the defense, weather, and\nballpark that can affect its observed outcome. The algorithm uses a Bayesian\nmodel to construct a continuous mapping from a vector of batted ball parameters\nto an intrinsic measure defined as the expected value of a linear weights\nrepresentation for run value. A kernel method is used to build nonparametric\nestimates for the component probability density functions in Bayes theorem from\na set of over one hundred thousand batted ball measurements recorded by the\nHITf/x system during the 2014 major league baseball (MLB) season.\nCross-validation is used to determine the optimal vector of smoothing\nparameters for the density estimates. Properties of the mapping are visualized\nby considering reduced-dimension subsets of the batted ball parameter space. We\nuse the mapping to derive statistics for intrinsic quality of contact for\nbatters and pitchers which have the potential to improve the accuracy of player\nmodels and forecasting systems. We also show that the new approach leads to a\nsimple automated measure of contact-adjusted defense and provides insight into\nthe impact of environmental variables on batted balls.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 06:27:40 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Healey", "Glenn", ""]]}, {"id": "1603.00106", "submitter": "Saurav Ghosh", "authors": "Saurav Ghosh, Prithwish Chakraborty, Emily Cohn, John S. Brownstein,\n  and Naren Ramakrishnan", "title": "Characterizing Diseases from Unstructured Text: A Vocabulary Driven\n  Word2vec Approach", "comments": "this paper has been submitted to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional disease surveillance can be augmented with a wide variety of\nreal-time sources such as, news and social media. However, these sources are in\ngeneral unstructured and, construction of surveillance tools such as\ntaxonomical correlations and trace mapping involves considerable human\nsupervision. In this paper, we motivate a disease vocabulary driven word2vec\nmodel (Dis2Vec) to model diseases and constituent attributes as word embeddings\nfrom the HealthMap news corpus. We use these word embeddings to automatically\ncreate disease taxonomies and evaluate our model against corresponding human\nannotated taxonomies. We compare our model accuracies against several\nstate-of-the art word2vec methods. Our results demonstrate that Dis2Vec\noutperforms traditional distributed vector representations in its ability to\nfaithfully capture taxonomical attributes across different class of diseases\nsuch as endemic, emerging and rare.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 00:45:18 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 20:45:50 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Ghosh", "Saurav", ""], ["Chakraborty", "Prithwish", ""], ["Cohn", "Emily", ""], ["Brownstein", "John S.", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1603.00145", "submitter": "Shifeng Liu", "authors": "Shifeng Liu, Zheng Hu, Sujit Dey and Xin Ke", "title": "On Tie Strength Augmented Social Correlation for Inferring Preference of\n  Mobile Telco Users", "comments": "This paper has been modified and the writing may make reader confused", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For mobile telecom operators, it is critical to build preference profiles of\ntheir customers and connected users, which can help operators make better\nmarketing strategies, and provide more personalized services. With the\ndeployment of deep packet inspection (DPI) in telecom networks, it is possible\nfor the telco operators to obtain user online preference. However, DPI has its\nlimitations and user preference derived only from DPI faces sparsity and cold\nstart problems. To better infer the user preference, social correlation in\ntelco users network derived from Call Detailed Records (CDRs) with regard to\nonline preference is investigated. Though widely verified in several online\nsocial networks, social correlation between online preference of users in\nmobile telco networks, where the CDRs derived relationship are of less social\nproperties and user mobile internet surfing activities are not visible to\nneighbourhood, has not been explored at a large scale. Based on a real world\ntelecom dataset including CDRs and preference of more than $550K$ users for\nseveral months, we verified that correlation does exist between online\npreference in such \\textit{ambiguous} social network. Furthermore, we found\nthat the stronger ties that users build, the more similarity between their\npreference may have. After defining the preference inferring task as a Top-$K$\nrecommendation problem, we incorporated Matrix Factorization Collaborative\nFiltering model with social correlation and tie strength based on call patterns\nto generate Top-$K$ preferred categories for users. The proposed Tie Strength\nAugmented Social Recommendation (TSASoRec) model takes data sparsity and cold\nstart user problems into account, considering both the recorded and missing\nrecorded category entries. The experiment on real dataset shows the proposed\nmodel can better infer user preference, especially for cold start users.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 05:20:47 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 22:06:05 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Liu", "Shifeng", ""], ["Hu", "Zheng", ""], ["Dey", "Sujit", ""], ["Ke", "Xin", ""]]}, {"id": "1603.00162", "submitter": "Nadav Cohen", "authors": "Nadav Cohen, Amnon Shashua", "title": "Convolutional Rectifier Networks as Generalized Tensor Decompositions", "comments": null, "journal-ref": "Proceedings of The 33rd International Conference on Machine\n  Learning, pp. 955-963, 2016", "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional rectifier networks, i.e. convolutional neural networks with\nrectified linear activation and max or average pooling, are the cornerstone of\nmodern deep learning. However, despite their wide use and success, our\ntheoretical understanding of the expressive properties that drive these\nnetworks is partial at best. On the other hand, we have a much firmer grasp of\nthese issues in the world of arithmetic circuits. Specifically, it is known\nthat convolutional arithmetic circuits possess the property of \"complete depth\nefficiency\", meaning that besides a negligible set, all functions that can be\nimplemented by a deep network of polynomial size, require exponential size in\norder to be implemented (or even approximated) by a shallow network. In this\npaper we describe a construction based on generalized tensor decompositions,\nthat transforms convolutional arithmetic circuits into convolutional rectifier\nnetworks. We then use mathematical tools available from the world of arithmetic\ncircuits to prove new results. First, we show that convolutional rectifier\nnetworks are universal with max pooling but not with average pooling. Second,\nand more importantly, we show that depth efficiency is weaker with\nconvolutional rectifier networks than it is with convolutional arithmetic\ncircuits. This leads us to believe that developing effective methods for\ntraining convolutional arithmetic circuits, thereby fulfilling their expressive\npotential, may give rise to a deep learning architecture that is provably\nsuperior to convolutional rectifier networks but has so far been overlooked by\npractitioners.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 06:44:34 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 12:52:16 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Cohen", "Nadav", ""], ["Shashua", "Amnon", ""]]}, {"id": "1603.00223", "submitter": "Liang Lu", "authors": "Liang Lu, Lingpeng Kong, Chris Dyer, Noah A. Smith and Steve Renals", "title": "Segmental Recurrent Neural Networks for End-to-end Speech Recognition", "comments": "5 pages, 2 figures, accepted by Interspeech 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the segmental recurrent neural network for end-to-end acoustic\nmodelling. This model connects the segmental conditional random field (CRF)\nwith a recurrent neural network (RNN) used for feature extraction. Compared to\nmost previous CRF-based acoustic models, it does not rely on an external system\nto provide features or segmentation boundaries. Instead, this model\nmarginalises out all the possible segmentations, and features are extracted\nfrom the RNN trained together with the segmental CRF. In essence, this model is\nself-contained and can be trained end-to-end. In this paper, we discuss\npractical training and decoding issues as well as the method to speed up the\ntraining in the context of speech recognition. We performed experiments on the\nTIMIT dataset. We achieved 17.3 phone error rate (PER) from the first-pass\ndecoding --- the best reported result using CRFs, despite the fact that we only\nused a zeroth-order CRF and without using any language model.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 10:43:43 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 10:29:23 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Lu", "Liang", ""], ["Kong", "Lingpeng", ""], ["Dyer", "Chris", ""], ["Smith", "Noah A.", ""], ["Renals", "Steve", ""]]}, {"id": "1603.00391", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "Caglar Gulcehre, Marcin Moczulski, Misha Denil and Yoshua Bengio", "title": "Noisy Activation Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Common nonlinear activation functions used in neural networks can cause\ntraining difficulties due to the saturation behavior of the activation\nfunction, which may hide dependencies that are not visible to vanilla-SGD\n(using first order gradients only). Gating mechanisms that use softly\nsaturating activation functions to emulate the discrete switching of digital\nlogic circuits are good examples of this. We propose to exploit the injection\nof appropriate noise so that the gradients may flow easily, even if the\nnoiseless application of the activation function would yield zero gradient.\nLarge noise will dominate the noise-free gradient and allow stochastic gradient\ndescent toexplore more. By adding noise only to the problematic parts of the\nactivation function, we allow the optimization procedure to explore the\nboundary between the degenerate (saturating) and the well-behaved parts of the\nactivation function. We also establish connections to simulated annealing, when\nthe amount of noise is annealed down, making it easier to optimize hard\nobjective functions. We find experimentally that replacing such saturating\nactivation functions by noisy variants helps training in many contexts,\nyielding state-of-the-art or competitive results on different datasets and\ntask, especially when training seems to be the most difficult, e.g., when\ncurriculum learning is necessary to obtain good results.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 18:30:15 GMT"}, {"version": "v2", "created": "Sun, 6 Mar 2016 20:51:57 GMT"}, {"version": "v3", "created": "Sun, 3 Apr 2016 21:41:47 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Moczulski", "Marcin", ""], ["Denil", "Misha", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1603.00427", "submitter": "Felipe Pinheiro", "authors": "Felipe C. Pinheiro and C\\'assio G. Lopes", "title": "A Nonlinear Adaptive Filter Based on the Model of Simple Multilinear\n  Functionals", "comments": "5 pages, one of references, plus extra page attached", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear adaptive filtering allows for modeling of some additional aspects\nof a general system and usually relies on highly complex algorithms, such as\nthose based on the Volterra series. Through the use of the Kronecker product\nand some basic facts of tensor algebra, we propose a simple model of\nnonlinearity, one that can be interpreted as a product of the outputs of K FIR\nlinear filters, and compute its cost function together with its gradient, which\nallows for some analysis of the optimization problem. We use these results it\nin a stochastic gradient framework, from which we derive an LMS-like algorithm\nand investigate the problems of multi-modality in the mean-square error surface\nand the choice of adequate initial conditions. Its computational complexity is\ncalculated. The new algorithm is tested in a system identification setup and is\ncompared with other polynomial algorithms from the literature, presenting\nfavorable convergence and/or computational complexity.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 19:53:02 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Pinheiro", "Felipe C.", ""], ["Lopes", "C\u00e1ssio G.", ""]]}, {"id": "1603.00441", "submitter": "Ruizhi Liao", "authors": "Ruizhi Liao, Cristian Roman, Peter Ball, Shumao Ou, Liping Chen", "title": "Crowdsourcing On-street Parking Space Detection", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the number of vehicles continues to grow, parking spaces are at a premium\nin city streets. Additionally, due to the lack of knowledge about street\nparking spaces, heuristic circling the blocks not only costs drivers' time and\nfuel, but also increases city congestion. In the wake of recent trend to build\nconvenient, green and energy-efficient smart cities, we rethink common\ntechniques adopted by high-profile smart parking systems, and present a\nuser-engaged (crowdsourcing) and sonar-based prototype to identify urban\non-street parking spaces. The prototype includes an ultrasonic sensor, a GPS\nreceiver and associated Arduino micro-controllers. It is mounted on the\npassenger side of a car to measure the distance from the vehicle to the nearest\nroadside obstacle. Multiple road tests are conducted around Wheatley, Oxford to\ngather results and emulate the crowdsourcing approach. By extracting parked\nvehicles' features from the collected trace, a supervised learning algorithm is\ndeveloped to estimate roadside parking occupancy and spot illegal parking\nvehicles. A quantity estimation model is derived to calculate the required\nnumber of sensing units to cover urban streets. The estimation is\nquantitatively compared to a fixed sensing solution. The results show that the\ncrowdsourcing way would need substantially fewer sensors compared to the fixed\nsensing system.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 20:22:08 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Liao", "Ruizhi", ""], ["Roman", "Cristian", ""], ["Ball", "Peter", ""], ["Ou", "Shumao", ""], ["Chen", "Liping", ""]]}, {"id": "1603.00448", "submitter": "Chelsea Finn", "authors": "Chelsea Finn, Sergey Levine, Pieter Abbeel", "title": "Guided Cost Learning: Deep Inverse Optimal Control via Policy\n  Optimization", "comments": "International Conference on Machine Learning (ICML), 2016, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning can acquire complex behaviors from high-level\nspecifications. However, defining a cost function that can be optimized\neffectively and encodes the correct task is challenging in practice. We explore\nhow inverse optimal control (IOC) can be used to learn behaviors from\ndemonstrations, with applications to torque control of high-dimensional robotic\nsystems. Our method addresses two key challenges in inverse optimal control:\nfirst, the need for informative features and effective regularization to impose\nstructure on the cost, and second, the difficulty of learning the cost function\nunder unknown dynamics for high-dimensional continuous systems. To address the\nformer challenge, we present an algorithm capable of learning arbitrary\nnonlinear cost functions, such as neural networks, without meticulous feature\nengineering. To address the latter challenge, we formulate an efficient\nsample-based approximation for MaxEnt IOC. We evaluate our method on a series\nof simulated tasks and real-world robotic manipulation problems, demonstrating\nsubstantial improvement over prior methods both in terms of task complexity and\nsample efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 20:35:56 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2016 07:30:36 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 16:53:46 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Finn", "Chelsea", ""], ["Levine", "Sergey", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1603.00522", "submitter": "Swati Gupta", "authors": "Swati Gupta, Michel Goemans, Patrick Jaillet", "title": "Solving Combinatorial Games using Products, Projections and\n  Lexicographically Optimal Bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to find Nash-equilibria for two-player zero-sum games where each\nplayer plays combinatorial objects like spanning trees, matchings etc, we\nconsider two online learning algorithms: the online mirror descent (OMD)\nalgorithm and the multiplicative weights update (MWU) algorithm. The OMD\nalgorithm requires the computation of a certain Bregman projection, that has\nclosed form solutions for simple convex sets like the Euclidean ball or the\nsimplex. However, for general polyhedra one often needs to exploit the general\nmachinery of convex optimization. We give a novel primal-style algorithm for\ncomputing Bregman projections on the base polytopes of polymatroids. Next, in\nthe case of the MWU algorithm, although it scales logarithmically in the number\nof pure strategies or experts $N$ in terms of regret, the algorithm takes time\npolynomial in $N$; this especially becomes a problem when learning\ncombinatorial objects. We give a general recipe to simulate the multiplicative\nweights update algorithm in time polynomial in their natural dimension. This is\nuseful whenever there exists a polynomial time generalized counting oracle\n(even if approximate) over these objects. Finally, using the combinatorial\nstructure of symmetric Nash-equilibria (SNE) when both players play bases of\nmatroids, we show that these can be found with a single projection or convex\nminimization (without using online learning).\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 23:02:38 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Gupta", "Swati", ""], ["Goemans", "Michel", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1603.00531", "submitter": "Kui Yu", "authors": "Kui Yu, Wei Ding, Xindong Wu", "title": "LOFS: Library of Online Streaming Feature Selection", "comments": null, "journal-ref": "Knowledge-based Systems, 113(2016):1-3", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an emerging research direction, online streaming feature selection deals\nwith sequentially added dimensions in a feature space while the number of data\ninstances is fixed. Online streaming feature selection provides a new,\ncomplementary algorithmic methodology to enrich online feature selection,\nespecially targets to high dimensionality in big data analytics. This paper\nintroduces the first comprehensive open-source library for use in MATLAB that\nimplements the state-of-the-art algorithms of online streaming feature\nselection. The library is designed to facilitate the development of new\nalgorithms in this exciting research direction and make comparisons between the\nnew methods and existing ones available.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 00:21:00 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Yu", "Kui", ""], ["Ding", "Wei", ""], ["Wu", "Xindong", ""]]}, {"id": "1603.00564", "submitter": "Ahmed El Alaoui", "authors": "Ahmed El Alaoui, Xiang Cheng, Aaditya Ramdas, Martin J. Wainwright,\n  Michael I. Jordan", "title": "Asymptotic behavior of $\\ell_p$-based Laplacian regularization in\n  semi-supervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a weighted graph with $N$ vertices, consider a real-valued regression\nproblem in a semi-supervised setting, where one observes $n$ labeled vertices,\nand the task is to label the remaining ones. We present a theoretical study of\n$\\ell_p$-based Laplacian regularization under a $d$-dimensional geometric\nrandom graph model. We provide a variational characterization of the\nperformance of this regularized learner as $N$ grows to infinity while $n$\nstays constant, the associated optimality conditions lead to a partial\ndifferential equation that must be satisfied by the associated function\nestimate $\\hat{f}$. From this formulation we derive several predictions on the\nlimiting behavior the $d$-dimensional function $\\hat{f}$, including (a) a phase\ntransition in its smoothness at the threshold $p = d + 1$, and (b) a tradeoff\nbetween smoothness and sensitivity to the underlying unlabeled data\ndistribution $P$. Thus, over the range $p \\leq d$, the function estimate\n$\\hat{f}$ is degenerate and \"spiky,\" whereas for $p\\geq d+1$, the function\nestimate $\\hat{f}$ is smooth. We show that the effect of the underlying density\nvanishes monotonically with $p$, such that in the limit $p = \\infty$,\ncorresponding to the so-called Absolutely Minimal Lipschitz Extension, the\nestimate $\\hat{f}$ is independent of the distribution $P$. Under the assumption\nof semi-supervised smoothness, ignoring $P$ can lead to poor statistical\nperformance, in particular, we construct a specific example for $d=1$ to\ndemonstrate that $p=2$ has lower risk than $p=\\infty$ due to the former penalty\nadapting to $P$ and the latter ignoring it. We also provide simulations that\nverify the accuracy of our predictions for finite sample sizes. Together, these\nproperties show that $p = d+1$ is an optimal choice, yielding a function\nestimate $\\hat{f}$ that is both smooth and non-degenerate, while remaining\nmaximally sensitive to $P$.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 03:31:28 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Alaoui", "Ahmed El", ""], ["Cheng", "Xiang", ""], ["Ramdas", "Aaditya", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1603.00570", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "Without-Replacement Sampling for Stochastic Gradient Methods:\n  Convergence Results and Application to Distributed Optimization", "comments": "Fixed a few minor typos, and slightly tightened Corollary 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient methods for machine learning and optimization problems\nare usually analyzed assuming data points are sampled \\emph{with} replacement.\nIn practice, however, sampling \\emph{without} replacement is very common,\neasier to implement in many cases, and often performs better. In this paper, we\nprovide competitive convergence guarantees for without-replacement sampling,\nunder various scenarios, for three types of algorithms: Any algorithm with\nonline regret guarantees, stochastic gradient descent, and SVRG. A useful\napplication of our SVRG analysis is a nearly-optimal algorithm for regularized\nleast squares in a distributed setting, in terms of both communication\ncomplexity and runtime complexity, when the data is randomly partitioned and\nthe condition number can be as large as the data size per machine (up to\nlogarithmic factors). Our proof techniques combine ideas from stochastic\noptimization, adversarial online learning, and transductive learning theory,\nand can potentially be applied to other stochastic optimization and learning\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 04:02:57 GMT"}, {"version": "v2", "created": "Fri, 29 Apr 2016 00:29:34 GMT"}, {"version": "v3", "created": "Mon, 17 Oct 2016 03:58:41 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1603.00576", "submitter": "Shahin Shahrampour", "authors": "Shahin Shahrampour, Alexander Rakhlin, Ali Jadbabaie", "title": "Distributed Estimation of Dynamic Parameters : Regret Analysis", "comments": "6 pages, To appear in American Control Conference 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the estimation of a time- varying parameter in a\nnetwork. A group of agents sequentially receive noisy signals about the\nparameter (or moving target), which does not follow any particular dynamics.\nThe parameter is not observable to an individual agent, but it is globally\nidentifiable for the whole network. Viewing the problem with an online\noptimization lens, we aim to provide the finite-time or non-asymptotic analysis\nof the problem. To this end, we use a notion of dynamic regret which suits the\nonline, non-stationary nature of the problem. In our setting, dynamic regret\ncan be recognized as a finite-time counterpart of stability in the mean- square\nsense. We develop a distributed, online algorithm for tracking the moving\ntarget. Defining the path-length as the consecutive differences between target\nlocations, we express an upper bound on regret in terms of the path-length of\nthe target and network errors. We further show the consistency of the result\nwith static setting and noiseless observations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 04:50:34 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Shahrampour", "Shahin", ""], ["Rakhlin", "Alexander", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1603.00622", "submitter": "Gregory Kahn", "authors": "Gregory Kahn, Tianhao Zhang, Sergey Levine, Pieter Abbeel", "title": "PLATO: Policy Learning using Adaptive Trajectory Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy search can in principle acquire complex strategies for control of\nrobots and other autonomous systems. When the policy is trained to process raw\nsensory inputs, such as images and depth maps, it can also acquire a strategy\nthat combines perception and control. However, effectively processing such\ncomplex inputs requires an expressive policy class, such as a large neural\nnetwork. These high-dimensional policies are difficult to train, especially\nwhen learning to control safety-critical systems. We propose PLATO, an\nalgorithm that trains complex control policies with supervised learning, using\nmodel-predictive control (MPC) to generate the supervision, hence never in need\nof running a partially trained and potentially unsafe policy. PLATO uses an\nadaptive training method to modify the behavior of MPC to gradually match the\nlearned policy in order to generate training samples at states that are likely\nto be visited by the learned policy. PLATO also maintains the MPC cost as an\nobjective to avoid highly undesirable actions that would result from strictly\nfollowing the learned policy before it has been fully trained. We prove that\nthis type of adaptive MPC expert produces supervision that leads to good\nlong-horizon performance of the resulting policy. We also empirically\ndemonstrate that MPC can still avoid dangerous on-policy actions in unexpected\nsituations during training. Our empirical results on a set of challenging\nsimulated aerial vehicle tasks demonstrate that, compared to prior methods,\nPLATO learns faster, experiences substantially fewer catastrophic failures\n(crashes) during training, and often converges to a better policy.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 08:50:57 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 23:58:42 GMT"}, {"version": "v3", "created": "Mon, 26 Sep 2016 18:12:26 GMT"}, {"version": "v4", "created": "Mon, 27 Feb 2017 02:20:54 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Kahn", "Gregory", ""], ["Zhang", "Tianhao", ""], ["Levine", "Sergey", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1603.00709", "submitter": "Mouna Ben Ishak", "authors": "Mouna Ben Ishak (LARODEC), Rajani Chulyadyo (LINA), Philippe Leray\n  (LINA)", "title": "Probabilistic Relational Model Benchmark Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The validation of any database mining methodology goes through an evaluation\nprocess where benchmarks availability is essential. In this paper, we aim to\nrandomly generate relational database benchmarks that allow to check\nprobabilistic dependencies among the attributes. We are particularly interested\nin Probabilistic Relational Models (PRMs), which extend Bayesian Networks (BNs)\nto a relational data mining context and enable effective and robust reasoning\nover relational data. Even though a panoply of works have focused, separately ,\non the generation of random Bayesian networks and relational databases, no work\nhas been identified for PRMs on that track. This paper provides an algorithmic\napproach for generating random PRMs from scratch to fill this gap. The proposed\nmethod allows to generate PRMs as well as synthetic relational data from a\nrandomly generated relational schema and a random set of probabilistic\ndependencies. This can be of interest not only for machine learning researchers\nto evaluate their proposals in a common framework, but also for databases\ndesigners to evaluate the effectiveness of the components of a database\nmanagement system.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 13:46:31 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Ishak", "Mouna Ben", "", "LARODEC"], ["Chulyadyo", "Rajani", "", "LINA"], ["Leray", "Philippe", "", "LINA"]]}, {"id": "1603.00748", "submitter": "Shixiang Gu", "authors": "Shixiang Gu and Timothy Lillicrap and Ilya Sutskever and Sergey Levine", "title": "Continuous Deep Q-Learning with Model-based Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-free reinforcement learning has been successfully applied to a range of\nchallenging problems, and has recently been extended to handle large neural\nnetwork policies and value functions. However, the sample complexity of\nmodel-free algorithms, particularly when using high-dimensional function\napproximators, tends to limit their applicability to physical systems. In this\npaper, we explore algorithms and representations to reduce the sample\ncomplexity of deep reinforcement learning for continuous control tasks. We\npropose two complementary techniques for improving the efficiency of such\nalgorithms. First, we derive a continuous variant of the Q-learning algorithm,\nwhich we call normalized adantage functions (NAF), as an alternative to the\nmore commonly used policy gradient and actor-critic methods. NAF representation\nallows us to apply Q-learning with experience replay to continuous tasks, and\nsubstantially improves performance on a set of simulated robotic control tasks.\nTo further improve the efficiency of our approach, we explore the use of\nlearned models for accelerating model-free reinforcement learning. We show that\niteratively refitted local linear models are especially effective for this, and\ndemonstrate substantially faster learning on domains where such models are\napplicable.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 15:28:25 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Gu", "Shixiang", ""], ["Lillicrap", "Timothy", ""], ["Sutskever", "Ilya", ""], ["Levine", "Sergey", ""]]}, {"id": "1603.00751", "submitter": "Nikola Milo\\v{s}evi\\'c MSc", "authors": "Nikola Milosevic", "title": "Equity forecast: Predicting long term stock price movement using machine\n  learning", "comments": "9 pages, 3 tables, computational finance, algorithmic finance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.GN", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Long term investment is one of the major investment strategies. However,\ncalculating intrinsic value of some company and evaluating shares for long term\ninvestment is not easy, since analyst have to care about a large number of\nfinancial indicators and evaluate them in a right manner. So far, little help\nin predicting the direction of the company value over the longer period of time\nhas been provided from the machines. In this paper we present a machine\nlearning aided approach to evaluate the equity's future price over the long\ntime. Our method is able to correctly predict whether some company's value will\nbe 10% higher or not over the period of one year in 76.5% of cases.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 15:33:30 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 12:55:27 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Milosevic", "Nikola", ""]]}, {"id": "1603.00788", "submitter": "Alp Kucukelbir", "authors": "Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, David M.\n  Blei", "title": "Automatic Differentiation Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic modeling is iterative. A scientist posits a simple model, fits\nit to her data, refines it according to her analysis, and repeats. However,\nfitting complex models to large data is a bottleneck in this process. Deriving\nalgorithms for new models can be both mathematically and computationally\nchallenging, which makes it difficult to efficiently cycle through the steps.\nTo this end, we develop automatic differentiation variational inference (ADVI).\nUsing our method, the scientist only provides a probabilistic model and a\ndataset, nothing else. ADVI automatically derives an efficient variational\ninference algorithm, freeing the scientist to refine and explore many models.\nADVI supports a broad class of models-no conjugacy assumptions are required. We\nstudy ADVI across ten different models and apply it to a dataset with millions\nof observations. ADVI is integrated into Stan, a probabilistic programming\nsystem; it is available for immediate use.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 16:43:15 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Kucukelbir", "Alp", ""], ["Tran", "Dustin", ""], ["Ranganath", "Rajesh", ""], ["Gelman", "Andrew", ""], ["Blei", "David M.", ""]]}, {"id": "1603.00810", "submitter": "Marta R. Costa-Juss\\`a", "authors": "Marta R. Costa-Juss\\`a and Jos\\'e A. R. Fonollosa", "title": "Character-based Neural Machine Translation", "comments": "Accepted for publication at ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (MT) has reached state-of-the-art results.\nHowever, one of the main challenges that neural MT still faces is dealing with\nvery large vocabularies and morphologically rich languages. In this paper, we\npropose a neural MT system using character-based embeddings in combination with\nconvolutional and highway layers to replace the standard lookup-based word\nrepresentations. The resulting unlimited-vocabulary and affix-aware source word\nembeddings are tested in a state-of-the-art neural MT based on an\nattention-based bidirectional recurrent neural network. The proposed MT scheme\nprovides improved results even when the source language is not morphologically\nrich. Improvements up to 3 BLEU points are obtained in the German-English WMT\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 18:01:57 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 14:02:48 GMT"}, {"version": "v3", "created": "Thu, 30 Jun 2016 10:28:36 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Costa-Juss\u00e0", "Marta R.", ""], ["Fonollosa", "Jos\u00e9 A. R.", ""]]}, {"id": "1603.00845", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Junting Pan, Kevin McGuinness, Elisa Sayrol, Noel O'Connor and Xavier\n  Giro-i-Nieto", "title": "Shallow and Deep Convolutional Networks for Saliency Prediction", "comments": "Preprint of the paper accepted at 2016 IEEE Conference on Computer\n  Vision and Pattern Recognition (CVPR). Source code and models available at\n  https://github.com/imatge-upc/saliency-2016-cvpr. Junting Pan and Kevin\n  McGuinness contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of salient areas in images has been traditionally addressed\nwith hand-crafted features based on neuroscience principles. This paper,\nhowever, addresses the problem with a completely data-driven approach by\ntraining a convolutional neural network (convnet). The learning process is\nformulated as a minimization of a loss function that measures the Euclidean\ndistance of the predicted saliency map with the provided ground truth. The\nrecent publication of large datasets of saliency prediction has provided enough\ndata to train end-to-end architectures that are both fast and accurate. Two\ndesigns are proposed: a shallow convnet trained from scratch, and a another\ndeeper solution whose first three layers are adapted from another network\ntrained for classification. To the authors knowledge, these are the first\nend-to-end CNNs trained and tested for the purpose of saliency prediction.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 19:54:02 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Pan", "Junting", ""], ["McGuinness", "Kevin", ""], ["Sayrol", "Elisa", ""], ["O'Connor", "Noel", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1603.00856", "submitter": "Steven Kearnes", "authors": "Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, Patrick\n  Riley", "title": "Molecular Graph Convolutions: Moving Beyond Fingerprints", "comments": "See \"Version information\" section", "journal-ref": "J Comput Aided Mol Des (2016)", "doi": "10.1007/s10822-016-9938-8", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular \"fingerprints\" encoding structural information are the workhorse of\ncheminformatics and machine learning in drug discovery applications. However,\nfingerprint representations necessarily emphasize particular aspects of the\nmolecular structure while ignoring others, rather than allowing the model to\nmake data-driven decisions. We describe molecular \"graph convolutions\", a\nmachine learning architecture for learning from undirected graphs, specifically\nsmall molecules. Graph convolutions use a simple encoding of the molecular\ngraph---atoms, bonds, distances, etc.---which allows the model to take greater\nadvantage of information in the graph structure. Although graph convolutions do\nnot outperform all fingerprint-based methods, they (along with other\ngraph-based methods) represent a new paradigm in ligand-based virtual screening\nwith exciting opportunities for future improvement.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 20:34:08 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 22:26:57 GMT"}, {"version": "v3", "created": "Thu, 18 Aug 2016 17:17:05 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Kearnes", "Steven", ""], ["McCloskey", "Kevin", ""], ["Berndl", "Marc", ""], ["Pande", "Vijay", ""], ["Riley", "Patrick", ""]]}, {"id": "1603.00892", "submitter": "Nikola Mrk\\v{s}i\\'c", "authors": "Nikola Mrk\\v{s}i\\'c and Diarmuid \\'O S\\'eaghdha and Blaise Thomson and\n  Milica Ga\\v{s}i\\'c and Lina Rojas-Barahona and Pei-Hao Su and David Vandyke\n  and Tsung-Hsien Wen and Steve Young", "title": "Counter-fitting Word Vectors to Linguistic Constraints", "comments": "Paper accepted for the 15th Annual Conference of the North American\n  Chapter of the Association for Computational Linguistics (NAACL 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel counter-fitting method which injects\nantonymy and synonymy constraints into vector space representations in order to\nimprove the vectors' capability for judging semantic similarity. Applying this\nmethod to publicly available pre-trained word vectors leads to a new state of\nthe art performance on the SimLex-999 dataset. We also show how the method can\nbe used to tailor the word vector space for the downstream task of dialogue\nstate tracking, resulting in robust improvements across different dialogue\ndomains.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 21:19:36 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Mrk\u0161i\u0107", "Nikola", ""], ["S\u00e9aghdha", "Diarmuid \u00d3", ""], ["Thomson", "Blaise", ""], ["Ga\u0161i\u0107", "Milica", ""], ["Rojas-Barahona", "Lina", ""], ["Su", "Pei-Hao", ""], ["Vandyke", "David", ""], ["Wen", "Tsung-Hsien", ""], ["Young", "Steve", ""]]}, {"id": "1603.00930", "submitter": "Adam Summerville", "authors": "Adam Summerville and Michael Mateas", "title": "Super Mario as a String: Platformer Level Generation Via LSTMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The procedural generation of video game levels has existed for at least 30\nyears, but only recently have machine learning approaches been used to generate\nlevels without specifying the rules for generation. A number of these have\nlooked at platformer levels as a sequence of characters and performed\ngeneration using Markov chains. In this paper we examine the use of Long\nShort-Term Memory recurrent neural networks (LSTMs) for the purpose of\ngenerating levels trained from a corpus of Super Mario Brothers levels. We\nanalyze a number of different data representations and how the generated levels\nfit into the space of human authored Super Mario Brothers levels.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 23:44:03 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 21:26:58 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Summerville", "Adam", ""], ["Mateas", "Michael", ""]]}, {"id": "1603.00954", "submitter": "Hanie Sedghi", "authors": "Hanie Sedghi and Anima Anandkumar", "title": "Training Input-Output Recurrent Neural Networks through Spectral Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of training input-output recurrent neural networks\n(RNN) for sequence labeling tasks. We propose a novel spectral approach for\nlearning the network parameters. It is based on decomposition of the\ncross-moment tensor between the output and a non-linear transformation of the\ninput, based on score functions. We guarantee consistent learning with\npolynomial sample and computational complexity under transparent conditions\nsuch as non-degeneracy of model parameters, polynomial activations for the\nneurons, and a Markovian evolution of the input sequence. We also extend our\nresults to Bidirectional RNN which uses both previous and future information to\noutput the label at each time point, and is employed in many NLP tasks such as\nPOS tagging.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 03:14:47 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 03:46:12 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 07:03:46 GMT"}, {"version": "v4", "created": "Fri, 22 Jul 2016 19:52:21 GMT"}, {"version": "v5", "created": "Mon, 31 Oct 2016 18:30:51 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Sedghi", "Hanie", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1603.00982", "submitter": "Yu-An Chung", "authors": "Yu-An Chung, Chao-Chung Wu, Chia-Hao Shen, Hung-Yi Lee, Lin-Shan Lee", "title": "Audio Word2Vec: Unsupervised Learning of Audio Segment Representations\n  using Sequence-to-sequence Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vector representations of fixed dimensionality for words (in text)\noffered by Word2Vec have been shown to be very useful in many application\nscenarios, in particular due to the semantic information they carry. This paper\nproposes a parallel version, the Audio Word2Vec. It offers the vector\nrepresentations of fixed dimensionality for variable-length audio segments.\nThese vector representations are shown to describe the sequential phonetic\nstructures of the audio segments to a good degree, with very attractive real\nworld applications such as query-by-example Spoken Term Detection (STD). In\nthis STD application, the proposed approach significantly outperformed the\nconventional Dynamic Time Warping (DTW) based approaches at significantly lower\ncomputation requirements. We propose unsupervised learning of Audio Word2Vec\nfrom audio data without human annotation using Sequence-to-sequence Audoencoder\n(SA). SA consists of two RNNs equipped with Long Short-Term Memory (LSTM)\nunits: the first RNN (encoder) maps the input audio sequence into a vector\nrepresentation of fixed dimensionality, and the second RNN (decoder) maps the\nrepresentation back to the input audio sequence. The two RNNs are jointly\ntrained by minimizing the reconstruction error. Denoising Sequence-to-sequence\nAutoencoder (DSA) is furthered proposed offering more robust learning.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 05:44:51 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 14:16:28 GMT"}, {"version": "v3", "created": "Thu, 17 Mar 2016 06:11:47 GMT"}, {"version": "v4", "created": "Sat, 11 Jun 2016 03:40:23 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Chung", "Yu-An", ""], ["Wu", "Chao-Chung", ""], ["Shen", "Chia-Hao", ""], ["Lee", "Hung-Yi", ""], ["Lee", "Lin-Shan", ""]]}, {"id": "1603.00988", "submitter": "Qianli Liao", "authors": "Hrushikesh Mhaskar, Qianli Liao, Tomaso Poggio", "title": "Learning Functions: When Is Deep Better Than Shallow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the universal approximation property holds both for hierarchical and\nshallow networks, we prove that deep (hierarchical) networks can approximate\nthe class of compositional functions with the same accuracy as shallow networks\nbut with exponentially lower number of training parameters as well as\nVC-dimension. This theorem settles an old conjecture by Bengio on the role of\ndepth in networks. We then define a general class of scalable, shift-invariant\nalgorithms to show a simple and natural set of requirements that justify deep\nconvolutional networks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 06:26:31 GMT"}, {"version": "v2", "created": "Sat, 5 Mar 2016 22:41:36 GMT"}, {"version": "v3", "created": "Thu, 24 Mar 2016 04:22:29 GMT"}, {"version": "v4", "created": "Sun, 29 May 2016 16:43:23 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Mhaskar", "Hrushikesh", ""], ["Liao", "Qianli", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1603.01025", "submitter": "Edward Lee", "authors": "Daisuke Miyashita and Edward H. Lee and Boris Murmann", "title": "Convolutional Neural Networks using Logarithmic Data Representation", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in convolutional neural networks have considered model\ncomplexity and hardware efficiency to enable deployment onto embedded systems\nand mobile devices. For example, it is now well-known that the arithmetic\noperations of deep networks can be encoded down to 8-bit fixed-point without\nsignificant deterioration in performance. However, further reduction in\nprecision down to as low as 3-bit fixed-point results in significant losses in\nperformance. In this paper we propose a new data representation that enables\nstate-of-the-art networks to be encoded to 3 bits with negligible loss in\nclassification performance. To perform this, we take advantage of the fact that\nthe weights and activations in a trained network naturally have non-uniform\ndistributions. Using non-uniform, base-2 logarithmic representation to encode\nweights, communicate activations, and perform dot-products enables networks to\n1) achieve higher classification accuracies than fixed-point at the same\nresolution and 2) eliminate bulky digital multipliers. Finally, we propose an\nend-to-end training procedure that uses log representation at 5-bits, which\nachieves higher final test accuracy than linear at 5-bits.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 08:51:52 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 03:32:30 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Miyashita", "Daisuke", ""], ["Lee", "Edward H.", ""], ["Murmann", "Boris", ""]]}, {"id": "1603.01067", "submitter": "Itir Onal Ertugrul", "authors": "Itir Onal, Mete Ozay, Eda Mizrak, Ilke Oztekin, Fatos T. Yarman Vural", "title": "Modeling the Sequence of Brain Volumes by Local Mesh Models for Brain\n  Decoding", "comments": "13 pages, 10 figures, submitted to JSTSP Special Issue on Advanced\n  Signal Processing in Brain Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We represent the sequence of fMRI (Functional Magnetic Resonance Imaging)\nbrain volumes recorded during a cognitive stimulus by a graph which consists of\na set of local meshes. The corresponding cognitive process, encoded in the\nbrain, is then represented by these meshes each of which is estimated assuming\na linear relationship among the voxel time series in a predefined locality.\nFirst, we define the concept of locality in two neighborhood systems, namely,\nthe spatial and functional neighborhoods. Then, we construct spatially and\nfunctionally local meshes around each voxel, called seed voxel, by connecting\nit either to its spatial or functional p-nearest neighbors. The mesh formed\naround a voxel is a directed sub-graph with a star topology, where the\ndirection of the edges is taken towards the seed voxel at the center of the\nmesh. We represent the time series recorded at each seed voxel in terms of\nlinear combination of the time series of its p-nearest neighbors in the mesh.\nThe relationships between a seed voxel and its neighbors are represented by the\nedge weights of each mesh, and are estimated by solving a linear regression\nequation. The estimated mesh edge weights lead to a better representation of\ninformation in the brain for encoding and decoding of the cognitive tasks. We\ntest our model on a visual object recognition and emotional memory retrieval\nexperiments using Support Vector Machines that are trained using the mesh edge\nweights as features. In the experimental analysis, we observe that the edge\nweights of the spatial and functional meshes perform better than the\nstate-of-the-art brain decoding models.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 12:06:00 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Onal", "Itir", ""], ["Ozay", "Mete", ""], ["Mizrak", "Eda", ""], ["Oztekin", "Ilke", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "1603.01121", "submitter": "Johannes Heinrich", "authors": "Johannes Heinrich, David Silver", "title": "Deep Reinforcement Learning from Self-Play in Imperfect-Information\n  Games", "comments": "updated version, incorporating conference feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications can be described as large-scale games of\nimperfect information. To deal with these challenging domains, prior work has\nfocused on computing Nash equilibria in a handcrafted abstraction of the\ndomain. In this paper we introduce the first scalable end-to-end approach to\nlearning approximate Nash equilibria without prior domain knowledge. Our method\ncombines fictitious self-play with deep reinforcement learning. When applied to\nLeduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium,\nwhereas common reinforcement learning methods diverged. In Limit Texas Holdem,\na poker game of real-world scale, NFSP learnt a strategy that approached the\nperformance of state-of-the-art, superhuman algorithms based on significant\ndomain expertise.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 15:01:54 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 15:28:30 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Heinrich", "Johannes", ""], ["Silver", "David", ""]]}, {"id": "1603.01354", "submitter": "Xuezhe Ma", "authors": "Xuezhe Ma, Eduard Hovy", "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF", "comments": "10 pages, 3 figures. To appear on ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art sequence labeling systems traditionally require large\namounts of task-specific knowledge in the form of hand-crafted features and\ndata pre-processing. In this paper, we introduce a novel neutral network\narchitecture that benefits from both word- and character-level representations\nautomatically, by using combination of bidirectional LSTM, CNN and CRF. Our\nsystem is truly end-to-end, requiring no feature engineering or data\npre-processing, thus making it applicable to a wide range of sequence labeling\ntasks. We evaluate our system on two data sets for two sequence labeling tasks\n--- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003\ncorpus for named entity recognition (NER). We obtain state-of-the-art\nperformance on both the two data --- 97.55\\% accuracy for POS tagging and\n91.21\\% F1 for NER.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 05:55:02 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 06:19:37 GMT"}, {"version": "v3", "created": "Tue, 8 Mar 2016 05:16:17 GMT"}, {"version": "v4", "created": "Mon, 14 Mar 2016 21:46:13 GMT"}, {"version": "v5", "created": "Sun, 29 May 2016 00:42:15 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Ma", "Xuezhe", ""], ["Hovy", "Eduard", ""]]}, {"id": "1603.01359", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung and Svetha Venkatesh", "title": "Learning deep representation of multityped objects and tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep multitask architecture to integrate multityped\nrepresentations of multimodal objects. This multitype exposition is less\nabstract than the multimodal characterization, but more machine-friendly, and\nthus is more precise to model. For example, an image can be described by\nmultiple visual views, which can be in the forms of bag-of-words (counts) or\ncolor/texture histograms (real-valued). At the same time, the image may have\nseveral social tags, which are best described using a sparse binary vector. Our\ndeep model takes as input multiple type-specific features, narrows the\ncross-modality semantic gaps, learns cross-type correlation, and produces a\nhigh-level homogeneous representation. At the same time, the model supports\nheterogeneously typed tasks. We demonstrate the capacity of the model on two\napplications: social image retrieval and multiple concept prediction. The deep\narchitecture produces more compact representation, naturally integrates\nmultiviews and multimodalities, exploits better side information, and most\nimportantly, performs competitively against baselines.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 06:34:24 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1603.01374", "submitter": "Sarathkrishna Swaminathan", "authors": "John Moeller, Sarathkrishna Swaminathan, Suresh Venkatasubramanian", "title": "A Unified View of Localized Kernel Learning", "comments": "14 pages, 2 figures, 4 tables. Reformatted version of the accepted\n  SDM 2016 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Kernel Learning, or MKL, extends (kernelized) SVM by attempting to\nlearn not only a classifier/regressor but also the best kernel for the training\ntask, usually from a combination of existing kernel functions. Most MKL methods\nseek the combined kernel that performs best over every training example,\nsacrificing performance in some areas to seek a global optimum. Localized\nkernel learning (LKL) overcomes this limitation by allowing the training\nalgorithm to match a component kernel to the examples that can exploit it best.\nSeveral approaches to the localized kernel learning problem have been explored\nin the last several years. We unify many of these approaches under one simple\nsystem and design a new algorithm with improved performance. We also develop\nenhanced versions of existing algorithms, with an eye on scalability and\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 08:29:03 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Moeller", "John", ""], ["Swaminathan", "Sarathkrishna", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1603.01431", "submitter": "Devansh Arpit", "authors": "Devansh Arpit, Yingbo Zhou, Bhargava U. Kota, Venu Govindaraju", "title": "Normalization Propagation: A Parametric Technique for Removing Internal\n  Covariate Shift in Deep Networks", "comments": "11 pages, ICML 2016, appendix added to the last version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the authors of Batch Normalization (BN) identify and address an\nimportant problem involved in training deep networks-- Internal Covariate\nShift-- the current solution has certain drawbacks. Specifically, BN depends on\nbatch statistics for layerwise input normalization during training which makes\nthe estimates of mean and standard deviation of input (distribution) to hidden\nlayers inaccurate for validation due to shifting parameter values (especially\nduring initial training epochs). Also, BN cannot be used with batch-size 1\nduring training. We address these drawbacks by proposing a non-adaptive\nnormalization technique for removing internal covariate shift, that we call\nNormalization Propagation. Our approach does not depend on batch statistics,\nbut rather uses a data-independent parametric estimate of mean and\nstandard-deviation in every layer thus being computationally faster compared\nwith BN. We exploit the observation that the pre-activation before Rectified\nLinear Units follow Gaussian distribution in deep networks, and that once the\nfirst and second order statistics of any given dataset are normalized, we can\nforward propagate this normalization without the need for recalculating the\napproximate statistics for hidden layers.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 12:01:58 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2016 16:41:25 GMT"}, {"version": "v3", "created": "Mon, 23 May 2016 23:01:55 GMT"}, {"version": "v4", "created": "Mon, 30 May 2016 02:08:06 GMT"}, {"version": "v5", "created": "Sun, 3 Jul 2016 20:17:44 GMT"}, {"version": "v6", "created": "Tue, 12 Jul 2016 13:57:19 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Arpit", "Devansh", ""], ["Zhou", "Yingbo", ""], ["Kota", "Bhargava U.", ""], ["Govindaraju", "Venu", ""]]}, {"id": "1603.01450", "submitter": "arXiv Admin", "authors": "Hossein Vahabi, Paul Lagr\\'ee, Claire Vernade, Olivier Capp\\'e", "title": "Sequential ranking under random semi-bandit feedback", "comments": "This submission has been withdrawn by arXiv administrators due to\n  irreconcilable authorship dispute", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many web applications, a recommendation is not a single item suggested to\na user but a list of possibly interesting contents that may be ranked in some\ncontexts. The combinatorial bandit problem has been studied quite extensively\nthese last two years and many theoretical results now exist : lower bounds on\nthe regret or asymptotically optimal algorithms. However, because of the\nvariety of situations that can be considered, results are designed to solve the\nproblem for a specific reward structure such as the Cascade Model. The present\nwork focuses on the problem of ranking items when the user is allowed to click\non several items while scanning the list from top to bottom.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 13:25:56 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 15:36:22 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Vahabi", "Hossein", ""], ["Lagr\u00e9e", "Paul", ""], ["Vernade", "Claire", ""], ["Capp\u00e9", "Olivier", ""]]}, {"id": "1603.01597", "submitter": "Jeroen De Gussem", "authors": "Mike Kestemont, Jeroen De Gussem", "title": "Integrated Sequence Tagging for Medieval Latin Using Deep Representation\n  Learning", "comments": null, "journal-ref": "Journal of Data Mining & Digital Humanities, Special Issue on\n  Computer-Aided Processing of Intertextuality in Ancient Languages, Towards a\n  Digital Ecosystem: NLP. Corpus infrastructure. Methods for Retrieving Texts\n  and Computing Text Similarities (August 6, 2017) jdmdh:3835", "doi": "10.46298/jdmdh.1398", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider two sequence tagging tasks for medieval Latin:\npart-of-speech tagging and lemmatization. These are both basic, yet\nfoundational preprocessing steps in applications such as text re-use detection.\nNevertheless, they are generally complicated by the considerable orthographic\nvariation which is typical of medieval Latin. In Digital Classics, these tasks\nare traditionally solved in a (i) cascaded and (ii) lexicon-dependent fashion.\nFor example, a lexicon is used to generate all the potential lemma-tag pairs\nfor a token, and next, a context-aware PoS-tagger is used to select the most\nappropriate tag-lemma pair. Apart from the problems with out-of-lexicon items,\nerror percolation is a major downside of such approaches. In this paper we\nexplore the possibility to elegantly solve these tasks using a single,\nintegrated approach. For this, we make use of a layered neural network\narchitecture from the field of deep representation learning.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 20:13:56 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 08:18:10 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Kestemont", "Mike", ""], ["De Gussem", "Jeroen", ""]]}, {"id": "1603.01670", "submitter": "Tao Wei", "authors": "Tao Wei, Changhu Wang, Yong Rui, Chang Wen Chen", "title": "Network Morphism", "comments": "Under review for ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a systematic study on how to morph a well-trained\nneural network to a new one so that its network function can be completely\npreserved. We define this as \\emph{network morphism} in this research. After\nmorphing a parent network, the child network is expected to inherit the\nknowledge from its parent network and also has the potential to continue\ngrowing into a more powerful one with much shortened training time. The first\nrequirement for this network morphism is its ability to handle diverse morphing\ntypes of networks, including changes of depth, width, kernel size, and even\nsubnet. To meet this requirement, we first introduce the network morphism\nequations, and then develop novel morphing algorithms for all these morphing\ntypes for both classic and convolutional neural networks. The second\nrequirement for this network morphism is its ability to deal with non-linearity\nin a network. We propose a family of parametric-activation functions to\nfacilitate the morphing of any continuous non-linear activation neurons.\nExperimental results on benchmark datasets and typical neural networks\ndemonstrate the effectiveness of the proposed network morphism scheme.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 02:06:43 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 16:36:00 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Wei", "Tao", ""], ["Wang", "Changhu", ""], ["Rui", "Yong", ""], ["Chen", "Chang Wen", ""]]}, {"id": "1603.01716", "submitter": "Balint Antal", "authors": "B\\'alint Antal", "title": "Classifier ensemble creation via false labelling", "comments": null, "journal-ref": "Knowledge-based Systems 89: pp. 278-287. (2015)", "doi": "10.1016/j.knosys.2015.07.009", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel approach to classifier ensemble creation is presented.\nWhile other ensemble creation techniques are based on careful selection of\nexisting classifiers or preprocessing of the data, the presented approach\nautomatically creates an optimal labelling for a number of classifiers, which\nare then assigned to the original data instances and fed to classifiers. The\napproach has been evaluated on high-dimensional biomedical datasets. The\nresults show that the approach outperformed individual approaches in all cases.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 12:01:00 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Antal", "B\u00e1lint", ""]]}, {"id": "1603.01801", "submitter": "Gaurav Pandey", "authors": "Gaurav Pandey and Ambedkar Dukkipati", "title": "Variational methods for Conditional Multimodal Deep Learning", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of conditional modality learning,\nwhereby one is interested in generating one modality given the other. While it\nis straightforward to learn a joint distribution over multiple modalities using\na deep multimodal architecture, we observe that such models aren't very\neffective at conditional generation. Hence, we address the problem by learning\nconditional distributions between the modalities. We use variational methods\nfor maximizing the corresponding conditional log-likelihood. The resultant deep\nmodel, which we refer to as conditional multimodal autoencoder (CMMA), forces\nthe latent representation obtained from a single modality alone to be `close'\nto the joint representation obtained from multiple modalities. We use the\nproposed model to generate faces from attributes. We show that the faces\ngenerated from attributes using the proposed model, are qualitatively and\nquantitatively more representative of the attributes from which they were\ngenerated, than those obtained by other deep generative models. We also propose\na secondary task, whereby the existing faces are modified by modifying the\ncorresponding attributes. We observe that the modifications in face introduced\nby the proposed model are representative of the corresponding modifications in\nattributes.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 07:33:03 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 10:57:41 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Pandey", "Gaurav", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1603.01840", "submitter": "Gal Dalal", "authors": "Gal Dalal, Elad Gilboa, Shie Mannor", "title": "Hierarchical Decision Making In Electricity Grid Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power grid is a complex and vital system that necessitates careful\nreliability management. Managing the grid is a difficult problem with multiple\ntime scales of decision making and stochastic behavior due to renewable energy\ngenerations, variable demand and unplanned outages. Solving this problem in the\nface of uncertainty requires a new methodology with tractable algorithms. In\nthis work, we introduce a new model for hierarchical decision making in complex\nsystems. We apply reinforcement learning (RL) methods to learn a proxy, i.e., a\nlevel of abstraction, for real-time power grid reliability. We devise an\nalgorithm that alternates between slow time-scale policy improvement, and fast\ntime-scale value function approximation. We compare our results to prevailing\nheuristics, and show the strength of our method.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 16:30:34 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Dalal", "Gal", ""], ["Gilboa", "Elad", ""], ["Mannor", "Shie", ""]]}, {"id": "1603.01855", "submitter": "Sougata Chaudhuri", "authors": "Sougata Chaudhuri and Ambuj Tewari", "title": "Online Learning to Rank with Feedback at the Top", "comments": "Appearing in AISTATS 2016", "journal-ref": "AISTATS 16, volume 51 of JMLR Workshop and Conference Proceedings,\n  pg.-277-285, 2016", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an online learning to rank setting in which, at each round, an\noblivious adversary generates a list of $m$ documents, pertaining to a query,\nand the learner produces scores to rank the documents. The adversary then\ngenerates a relevance vector and the learner updates its ranker according to\nthe feedback received. We consider the setting where the feedback is restricted\nto be the relevance levels of only the top $k$ documents in the ranked list for\n$k \\ll m$. However, the performance of learner is judged based on the\nunrevealed full relevance vectors, using an appropriate learning to rank loss\nfunction. We develop efficient algorithms for well known losses in the\npointwise, pairwise and listwise families. We also prove that no online\nalgorithm can have sublinear regret, with top-1 feedback, for any loss that is\ncalibrated with respect to NDCG. We apply our algorithms on benchmark datasets\ndemonstrating efficient online learning of a ranking function from highly\nrestricted feedback.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 18:43:54 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Chaudhuri", "Sougata", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1603.01860", "submitter": "Sougata Chaudhuri", "authors": "Ambuj Tewari and Sougata Chaudhuri", "title": "Generalization error bounds for learning to rank: Does the length of\n  document lists matter?", "comments": "Appeared in ICML 2015. arXiv admin note: substantial text overlap\n  with arXiv:1405.0586", "journal-ref": "ICML 2015, volume 37 of JMLR Workshop and Conference Proceedings,\n  pg.- 315-323, 2015", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the generalization ability of algorithms for learning to rank at\na query level, a problem also called subset ranking. Existing generalization\nerror bounds necessarily degrade as the size of the document list associated\nwith a query increases. We show that such a degradation is not intrinsic to the\nproblem. For several loss functions, including the cross-entropy loss used in\nthe well known ListNet method, there is \\emph{no} degradation in generalization\nability as document lists become longer. We also provide novel generalization\nerror bounds under $\\ell_1$ regularization and faster convergence rates if the\nloss function is smooth.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 19:01:53 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Tewari", "Ambuj", ""], ["Chaudhuri", "Sougata", ""]]}, {"id": "1603.01870", "submitter": "Sougata Chaudhuri", "authors": "Sougata Chaudhuri and Georgios Theocharous and Mohammad Ghavamzadeh", "title": "Personalized Advertisement Recommendation: A Ranking Approach to Address\n  the Ubiquitous Click Sparsity Problem", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of personalized advertisement recommendation (PAR),\nwhich consist of a user visiting a system (website) and the system displaying\none of $K$ ads to the user. The system uses an internal ad recommendation\npolicy to map the user's profile (context) to one of the ads. The user either\nclicks or ignores the ad and correspondingly, the system updates its\nrecommendation policy. PAR problem is usually tackled by scalable\n\\emph{contextual bandit} algorithms, where the policies are generally based on\nclassifiers. A practical problem in PAR is extreme click sparsity, due to very\nfew users actually clicking on ads. We systematically study the drawback of\nusing contextual bandit algorithms based on classifier-based policies, in face\nof extreme click sparsity. We then suggest an alternate policy, based on\nrankers, learnt by optimizing the Area Under the Curve (AUC) ranking loss,\nwhich can significantly alleviate the problem of click sparsity. We conduct\nextensive experiments on public datasets, as well as three industry proprietary\ndatasets, to illustrate the improvement in click-through-rate (CTR) obtained by\nusing the ranker-based policy over classifier-based policies.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 20:26:41 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Chaudhuri", "Sougata", ""], ["Theocharous", "Georgios", ""], ["Ghavamzadeh", "Mohammad", ""]]}, {"id": "1603.01901", "submitter": "Behrouz Behmardi", "authors": "Behrouz Behmardi, Forrest Briggs, Xiaoli Z. Fern, and Raviv Raich", "title": "Confidence-Constrained Maximum Entropy Framework for Learning from\n  Multi-Instance Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-instance data, in which each object (bag) contains a collection of\ninstances, are widespread in machine learning, computer vision, bioinformatics,\nsignal processing, and social sciences. We present a maximum entropy (ME)\nframework for learning from multi-instance data. In this approach each bag is\nrepresented as a distribution using the principle of ME. We introduce the\nconcept of confidence-constrained ME (CME) to simultaneously learn the\nstructure of distribution space and infer each distribution. The shared\nstructure underlying each density is used to learn from instances inside each\nbag. The proposed CME is free of tuning parameters. We devise a fast\noptimization algorithm capable of handling large scale multi-instance data. In\nthe experimental section, we evaluate the performance of the proposed approach\nin terms of exact rank recovery in the space of distributions and compare it\nwith the regularized ME approach. Moreover, we compare the performance of CME\nwith Multi-Instance Learning (MIL) state-of-the-art algorithms and show a\ncomparable performance in terms of accuracy with reduced computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 00:30:10 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Behmardi", "Behrouz", ""], ["Briggs", "Forrest", ""], ["Fern", "Xiaoli Z.", ""], ["Raich", "Raviv", ""]]}, {"id": "1603.01913", "submitter": "Yangfeng Ji", "authors": "Yangfeng Ji and Gholamreza Haffari and Jacob Eisenstein", "title": "A Latent Variable Recurrent Neural Network for Discourse Relation\n  Language Models", "comments": "NAACL 2016 camera ready, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel latent variable recurrent neural network\narchitecture for jointly modeling sequences of words and (possibly latent)\ndiscourse relations between adjacent sentences. A recurrent neural network\ngenerates individual words, thus reaping the benefits of\ndiscriminatively-trained vector representations. The discourse relations are\nrepresented with a latent variable, which can be predicted or marginalized,\ndepending on the task. The resulting model can therefore employ a training\nobjective that includes not only discourse relation classification, but also\nword prediction. As a result, it outperforms state-of-the-art alternatives for\ntwo tasks: implicit discourse relation classification in the Penn Discourse\nTreebank, and dialog act classification in the Switchboard corpus. Furthermore,\nby marginalizing over latent discourse relations at test time, we obtain a\ndiscourse informed language model, which improves over a strong LSTM baseline.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 01:54:56 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 16:58:10 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Ji", "Yangfeng", ""], ["Haffari", "Gholamreza", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1603.02010", "submitter": "Borja Balle", "authors": "Borja Balle, Maziar Gomrokchi, Doina Precup", "title": "Differentially Private Policy Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first differentially private algorithms for reinforcement\nlearning, which apply to the task of evaluating a fixed policy. We establish\ntwo approaches for achieving differential privacy, provide a theoretical\nanalysis of the privacy and utility of the two algorithms, and show promising\nresults on simple empirical examples.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 11:23:57 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Balle", "Borja", ""], ["Gomrokchi", "Maziar", ""], ["Precup", "Doina", ""]]}, {"id": "1603.02038", "submitter": "Ruben Martinez-Cantin", "authors": "Jos\\'e Nogueira, Ruben Martinez-Cantin, Alexandre Bernardino and\n  Lorenzo Jamone", "title": "Unscented Bayesian Optimization for Safe Robot Grasping", "comments": "conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the robot grasp optimization problem of unknown objects\nconsidering uncertainty in the input space. Grasping unknown objects can be\nachieved by using a trial and error exploration strategy. Bayesian optimization\nis a sample efficient optimization algorithm that is especially suitable for\nthis setups as it actively reduces the number of trials for learning about the\nfunction to optimize. In fact, this active object exploration is the same\nstrategy that infants do to learn optimal grasps. One problem that arises while\nlearning grasping policies is that some configurations of grasp parameters may\nbe very sensitive to error in the relative pose between the object and robot\nend-effector. We call these configurations unsafe because small errors during\ngrasp execution may turn good grasps into bad grasps. Therefore, to reduce the\nrisk of grasp failure, grasps should be planned in safe areas. We propose a new\nalgorithm, Unscented Bayesian optimization that is able to perform sample\nefficient optimization while taking into consideration input noise to find safe\noptima. The contribution of Unscented Bayesian optimization is twofold as if\nprovides a new decision process that drives exploration to safe regions and a\nnew selection procedure that chooses the optimal in terms of its safety without\nextra analysis or computational cost. Both contributions are rooted on the\nstrong theory behind the unscented transformation, a popular nonlinear\napproximation method. We show its advantages with respect to the classical\nBayesian optimization both in synthetic problems and in realistic robot grasp\nsimulations. The results highlights that our method achieves optimal and robust\ngrasping policies after few trials while the selected grasps remain in safe\nregions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 12:51:43 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Nogueira", "Jos\u00e9", ""], ["Martinez-Cantin", "Ruben", ""], ["Bernardino", "Alexandre", ""], ["Jamone", "Lorenzo", ""]]}, {"id": "1603.02041", "submitter": "Diana Borsa", "authors": "Diana Borsa and Thore Graepel and John Shawe-Taylor", "title": "Learning Shared Representations in Multi-task Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a paradigm in multi-task reinforcement learning (MT-RL) in\nwhich an agent is placed in an environment and needs to learn to perform a\nseries of tasks, within this space. Since the environment does not change,\nthere is potentially a lot of common ground amongst tasks and learning to solve\nthem individually seems extremely wasteful. In this paper, we explicitly model\nand learn this shared structure as it arises in the state-action value space.\nWe will show how one can jointly learn optimal value-functions by modifying the\npopular Value-Iteration and Policy-Iteration procedures to accommodate this\nshared representation assumption and leverage the power of multi-task\nsupervised learning. Finally, we demonstrate that the proposed model and\ntraining procedures, are able to infer good value functions, even under low\nsamples regimes. In addition to data efficiency, we will show in our analysis,\nthat learning abstractions of the state space jointly across tasks leads to\nmore robust, transferable representations with the potential for better\ngeneralization. this shared representation assumption and leverage the power of\nmulti-task supervised learning. Finally, we demonstrate that the proposed model\nand training procedures, are able to infer good value functions, even under low\nsamples regimes. In addition to data efficiency, we will show in our analysis,\nthat learning abstractions of the state space jointly across tasks leads to\nmore robust, transferable representations with the potential for better\ngeneralization.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 13:03:30 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Borsa", "Diana", ""], ["Graepel", "Thore", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1603.02074", "submitter": "Mohammed Rayyan Sheriff", "authors": "Mohammed Rayyan Sheriff and Debasish Chatterjee", "title": "Optimal dictionary for least squares representation", "comments": "24 pages", "journal-ref": "Journal of Machine Learning Research, Vol. 18, Paper No. 107, pp.\n  1-28, 2017", "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionaries are collections of vectors used for representations of random\nvectors in Euclidean spaces. Recent research on optimal dictionaries is focused\non constructing dictionaries that offer sparse representations, i.e.,\n$\\ell_0$-optimal representations. Here we consider the problem of finding\noptimal dictionaries with which representations of samples of a random vector\nare optimal in an $\\ell_2$-sense: optimality of representation is defined as\nattaining the minimal average $\\ell_2$-norm of the coefficients used to\nrepresent the random vector. With the help of recent results on rank-$1$\ndecompositions of symmetric positive semidefinite matrices, we provide an\nexplicit description of $\\ell_2$-optimal dictionaries as well as their\nalgorithmic constructions in polynomial time.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 14:13:24 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 14:44:44 GMT"}, {"version": "v3", "created": "Mon, 1 May 2017 15:49:49 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Sheriff", "Mohammed Rayyan", ""], ["Chatterjee", "Debasish", ""]]}, {"id": "1603.02185", "submitter": "Jialei Wang", "authors": "Jialei Wang, Mladen Kolar, Nathan Srebro", "title": "Distributed Multi-Task Learning with Shared Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of distributed multi-task learning with shared\nrepresentation, where each machine aims to learn a separate, but related, task\nin an unknown shared low-dimensional subspaces, i.e. when the predictor matrix\nhas low rank. We consider a setting where each task is handled by a different\nmachine, with samples for the task available locally on the machine, and study\ncommunication-efficient methods for exploiting the shared structure.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 18:11:54 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Wang", "Jialei", ""], ["Kolar", "Mladen", ""], ["Srebro", "Nathan", ""]]}, {"id": "1603.02194", "submitter": "Oren Barkan", "authors": "Oren Barkan, Jonathan Weill and Amir Averbuch", "title": "Gaussian Process Regression for Out-of-Sample Extension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold learning methods are useful for high dimensional data analysis. Many\nof the existing methods produce a low dimensional representation that attempts\nto describe the intrinsic geometric structure of the original data. Typically,\nthis process is computationally expensive and the produced embedding is limited\nto the training data. In many real life scenarios, the ability to produce\nembedding of unseen samples is essential. In this paper we propose a Bayesian\nnon-parametric approach for out-of-sample extension. The method is based on\nGaussian Process Regression and independent of the manifold learning algorithm.\nAdditionally, the method naturally provides a measure for the degree of\nabnormality for a newly arrived data point that did not participate in the\ntraining process. We derive the mathematical connection between the proposed\nmethod and the Nystrom extension and show that the latter is a special case of\nthe former. We present extensive experimental results that demonstrate the\nperformance of the proposed method and compare it to other existing\nout-of-sample extension methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 18:35:51 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2016 16:56:21 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Barkan", "Oren", ""], ["Weill", "Jonathan", ""], ["Averbuch", "Amir", ""]]}, {"id": "1603.02199", "submitter": "Sergey Levine", "authors": "Sergey Levine, Peter Pastor, Alex Krizhevsky, Deirdre Quillen", "title": "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning\n  and Large-Scale Data Collection", "comments": "This is an extended version of \"Learning Hand-Eye Coordination for\n  Robotic Grasping with Large-Scale Data Collection,\" ISER 2016. Draft modified\n  to correct typo in Algorithm 1 and add a link to the publicly available\n  dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a learning-based approach to hand-eye coordination for robotic\ngrasping from monocular images. To learn hand-eye coordination for grasping, we\ntrained a large convolutional neural network to predict the probability that\ntask-space motion of the gripper will result in successful grasps, using only\nmonocular camera images and independently of camera calibration or the current\nrobot pose. This requires the network to observe the spatial relationship\nbetween the gripper and objects in the scene, thus learning hand-eye\ncoordination. We then use this network to servo the gripper in real time to\nachieve successful grasps. To train our network, we collected over 800,000\ngrasp attempts over the course of two months, using between 6 and 14 robotic\nmanipulators at any given time, with differences in camera placement and\nhardware. Our experimental evaluation demonstrates that our method achieves\neffective real-time control, can successfully grasp novel objects, and corrects\nmistakes by continuous servoing.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 18:53:00 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 23:01:46 GMT"}, {"version": "v3", "created": "Sat, 2 Apr 2016 23:50:24 GMT"}, {"version": "v4", "created": "Sun, 28 Aug 2016 23:32:37 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Levine", "Sergey", ""], ["Pastor", "Peter", ""], ["Krizhevsky", "Alex", ""], ["Quillen", "Deirdre", ""]]}, {"id": "1603.02250", "submitter": "Satyen Kale", "authors": "Dean Foster, Satyen Kale and Howard Karloff", "title": "Online Sparse Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online sparse linear regression problem, which is the problem\nof sequentially making predictions observing only a limited number of features\nin each round, to minimize regret with respect to the best sparse linear\nregressor, where prediction accuracy is measured by square loss. We give an\ninefficient algorithm that obtains regret bounded by $\\tilde{O}(\\sqrt{T})$\nafter $T$ prediction rounds. We complement this result by showing that no\nalgorithm running in polynomial time per iteration can achieve regret bounded\nby $O(T^{1-\\delta})$ for any constant $\\delta > 0$ unless $\\text{NP} \\subseteq\n\\text{BPP}$. This computational hardness result resolves an open problem\npresented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al. (2013).\nThis hardness result holds even if the algorithm is allowed to access more\nfeatures than the best sparse linear regressor up to a logarithmic factor in\nthe dimension.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 20:49:52 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Foster", "Dean", ""], ["Kale", "Satyen", ""], ["Karloff", "Howard", ""]]}, {"id": "1603.02412", "submitter": "Tomoya Murata", "authors": "Tomoya Murata and Taiji Suzuki", "title": "Stochastic dual averaging methods using variance reduction techniques\n  for regularized empirical risk minimization problems", "comments": "30 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a composite convex minimization problem associated with\nregularized empirical risk minimization, which often arises in machine\nlearning. We propose two new stochastic gradient methods that are based on\nstochastic dual averaging method with variance reduction. Our methods generate\na sparser solution than the existing methods because we do not need to take the\naverage of the history of the solutions. This is favorable in terms of both\ninterpretability and generalization. Moreover, our methods have theoretical\nsupport for both a strongly and a non-strongly convex regularizer and achieve\nthe best known convergence rates among existing nonaccelerated stochastic\ngradient methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 08:26:28 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Murata", "Tomoya", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1603.02494", "submitter": "Tapesh Santra", "authors": "Tapesh Santra", "title": "A Bayesian non-parametric method for clustering high-dimensional binary\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real life problems, objects are described by large number of binary\nfeatures. For instance, documents are characterized by presence or absence of\ncertain keywords; cancer patients are characterized by presence or absence of\ncertain mutations etc. In such cases, grouping together similar\nobjects/profiles based on such high dimensional binary features is desirable,\nbut challenging. Here, I present a Bayesian non parametric algorithm for\nclustering high dimensional binary data. It uses a Dirichlet Process (DP)\nmixture model and simulated annealing to not only cluster binary data, but also\nfind optimal number of clusters in the data. The performance of the algorithm\nwas evaluated and compared with other algorithms using simulated datasets. It\noutperformed all other clustering methods that were tested in the simulation\nstudies. It was also used to cluster real datasets arising from document\nanalysis, handwritten image analysis and cancer research. It successfully\ndivided a set of documents based on their topics, hand written images based on\ndifferent styles of writing digits and identified tissue and mutation\nspecificity of chemotherapy treatments.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 12:02:59 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Santra", "Tapesh", ""]]}, {"id": "1603.02501", "submitter": "Harish Ramaswamy", "authors": "Harish G. Ramaswamy and Clayton Scott and Ambuj Tewari", "title": "Mixture Proportion Estimation via Kernel Embedding of Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture proportion estimation (MPE) is the problem of estimating the weight\nof a component distribution in a mixture, given samples from the mixture and\ncomponent. This problem constitutes a key part in many \"weakly supervised\nlearning\" problems like learning with positive and unlabelled samples, learning\nwith label noise, anomaly detection and crowdsourcing. While there have been\nseveral methods proposed to solve this problem, to the best of our knowledge no\nefficient algorithm with a proven convergence rate towards the true proportion\nexists for this problem. We fill this gap by constructing a provably correct\nalgorithm for MPE, and derive convergence rates under certain assumptions on\nthe distribution. Our method is based on embedding distributions onto an RKHS,\nand implementing it only requires solving a simple convex quadratic programming\nproblem a few times. We run our algorithm on several standard classification\ndatasets, and demonstrate that it performs comparably to or better than other\nalgorithms on most datasets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 12:43:29 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 16:41:44 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Ramaswamy", "Harish G.", ""], ["Scott", "Clayton", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1603.02514", "submitter": "Weidi Xu", "authors": "Weidi Xu, Haoze Sun, Chao Deng, Ying Tan", "title": "Variational Autoencoders for Semi-supervised Text Classification", "comments": "8 pages, 4 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Although semi-supervised variational autoencoder (SemiVAE) works in image\nclassification task, it fails in text classification task if using vanilla LSTM\nas its decoder. From a perspective of reinforcement learning, it is verified\nthat the decoder's capability to distinguish between different categorical\nlabels is essential. Therefore, Semi-supervised Sequential Variational\nAutoencoder (SSVAE) is proposed, which increases the capability by feeding\nlabel into its decoder RNN at each time-step. Two specific decoder structures\nare investigated and both of them are verified to be effective. Besides, in\norder to reduce the computational complexity in training, a novel optimization\nmethod is proposed, which estimates the gradient of the unlabeled objective\nfunction by sampling, along with two variance reduction techniques.\nExperimental results on Large Movie Review Dataset (IMDB) and AG's News corpus\nshow that the proposed approach significantly improves the classification\naccuracy compared with pure-supervised classifiers, and achieves competitive\nperformance against previous advanced methods. State-of-the-art results can be\nobtained by integrating other pretraining-based methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 13:24:45 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 14:33:50 GMT"}, {"version": "v3", "created": "Thu, 24 Nov 2016 08:18:31 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Xu", "Weidi", ""], ["Sun", "Haoze", ""], ["Deng", "Chao", ""], ["Tan", "Ying", ""]]}, {"id": "1603.02532", "submitter": "Antti Honkela", "authors": "Otte Hein\\\"avaara, Janne Lepp\\\"a-aho, Jukka Corander and Antti Honkela", "title": "On the inconsistency of $\\ell_1$-penalised sparse precision matrix\n  estimation", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Various $\\ell_1$-penalised estimation methods such as graphical lasso and\nCLIME are widely used for sparse precision matrix estimation. Many of these\nmethods have been shown to be consistent under various quantitative assumptions\nabout the underlying true covariance matrix. Intuitively, these conditions are\nrelated to situations where the penalty term will dominate the optimisation. In\nthis paper, we explore the consistency of $\\ell_1$-based methods for a class of\nsparse latent variable -like models, which are strongly motivated by several\ntypes of applications. We show that all $\\ell_1$-based methods fail\ndramatically for models with nearly linear dependencies between the variables.\nWe also study the consistency on models derived from real gene expression data\nand note that the assumptions needed for consistency never hold even for modest\nsized gene networks and $\\ell_1$-based methods also become unreliable in\npractice for larger networks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 14:24:11 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Hein\u00e4vaara", "Otte", ""], ["Lepp\u00e4-aho", "Janne", ""], ["Corander", "Jukka", ""], ["Honkela", "Antti", ""]]}, {"id": "1603.02578", "submitter": "Mathieu Guillame-Bert", "authors": "Mathieu Guillame-Bert and Artur Dubrawski", "title": "Batched Lazy Decision Trees", "comments": "7 pages, 2 figures, 3 tables, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a batched lazy algorithm for supervised classification using\ndecision trees. It avoids unnecessary visits to irrelevant nodes when it is\nused to make predictions with either eagerly or lazily trained decision trees.\nA set of experiments demonstrate that the proposed algorithm can outperform\nboth the conventional and lazy decision tree algorithms in terms of computation\ntime as well as memory consumption, without compromising accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 16:36:31 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Guillame-Bert", "Mathieu", ""], ["Dubrawski", "Artur", ""]]}, {"id": "1603.02597", "submitter": "Tim Smith", "authors": "Tim Smith", "title": "Prediction of Infinite Words with Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classic problem of sequence prediction, a predictor receives a\nsequence of values from an emitter and tries to guess the next value before it\nappears. The predictor masters the emitter if there is a point after which all\nof the predictor's guesses are correct. In this paper we consider the case in\nwhich the predictor is an automaton and the emitted values are drawn from a\nfinite set; i.e., the emitted sequence is an infinite word. We examine the\npredictive capabilities of finite automata, pushdown automata, stack automata\n(a generalization of pushdown automata), and multihead finite automata. We\nrelate our predicting automata to purely periodic words, ultimately periodic\nwords, and multilinear words, describing novel prediction algorithms for\nmastering these sequences.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 17:12:09 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Smith", "Tim", ""]]}, {"id": "1603.02626", "submitter": "Nicolas Gillis", "authors": "Olivier Sobrie and Nicolas Gillis and Vincent Mousseau and Marc Pirlot", "title": "UTA-poly and UTA-splines: additive value functions with polynomial\n  marginals", "comments": "30 pages, 16 figures, 4 tables. No major changes since the first\n  version (few typos, adding references, discussions)", "journal-ref": "European Journal of Operational Research 264, pp. 405-418, 2018", "doi": "10.1016/j.ejor.2017.03.021", "report-no": null, "categories": "math.OC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive utility function models are widely used in multiple criteria\ndecision analysis. In such models, a numerical value is associated to each\nalternative involved in the decision problem. It is computed by aggregating the\nscores of the alternative on the different criteria of the decision problem.\nThe score of an alternative is determined by a marginal value function that\nevolves monotonically as a function of the performance of the alternative on\nthis criterion. Determining the shape of the marginals is not easy for a\ndecision maker. It is easier for him/her to make statements such as\n\"alternative $a$ is preferred to $b$\". In order to help the decision maker, UTA\ndisaggregation procedures use linear programming to approximate the marginals\nby piecewise linear functions based only on such statements. In this paper, we\npropose to infer polynomials and splines instead of piecewise linear functions\nfor the marginals. In this aim, we use semidefinite programming instead of\nlinear programming. We illustrate this new elicitation method and present some\nexperimental results.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 17:49:03 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 13:23:07 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Sobrie", "Olivier", ""], ["Gillis", "Nicolas", ""], ["Mousseau", "Vincent", ""], ["Pirlot", "Marc", ""]]}, {"id": "1603.02636", "submitter": "Lucas Beyer", "authors": "Lucas Beyer and Alexander Hermans and Bastian Leibe", "title": "DROW: Real-Time Deep Learning based Wheelchair Detection in 2D Range\n  Data", "comments": "Lucas Beyer and Alexander Hermans contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the DROW detector, a deep learning based detector for 2D range\ndata. Laser scanners are lighting invariant, provide accurate range data, and\ntypically cover a large field of view, making them interesting sensors for\nrobotics applications. So far, research on detection in laser range data has\nbeen dominated by hand-crafted features and boosted classifiers, potentially\nlosing performance due to suboptimal design choices. We propose a Convolutional\nNeural Network (CNN) based detector for this task. We show how to effectively\napply CNNs for detection in 2D range data, and propose a depth preprocessing\nstep and voting scheme that significantly improve CNN performance. We\ndemonstrate our approach on wheelchairs and walkers, obtaining state of the art\ndetection results. Apart from the training data, none of our design choices\nlimits the detector to these two classes, though. We provide a ROS node for our\ndetector and release our dataset containing 464k laser scans, out of which 24k\nwere annotated.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 19:39:19 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 18:06:28 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Beyer", "Lucas", ""], ["Hermans", "Alexander", ""], ["Leibe", "Bastian", ""]]}, {"id": "1603.02638", "submitter": "Le Riche Rodolphe", "authors": "Hossein Mohammadi, Rodolphe Le Riche, Eric Touboul", "title": "Small ensembles of kriging models for optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Efficient Global Optimization (EGO) algorithm uses a conditional\nGaus-sian Process (GP) to approximate an objective function known at a finite\nnumber of observation points and sequentially adds new points which maximize\nthe Expected Improvement criterion according to the GP. The important factor\nthat controls the efficiency of EGO is the GP covariance function (or kernel)\nwhich should be chosen according to the objective function. Traditionally, a\npa-rameterized family of covariance functions is considered whose parameters\nare learned through statistical procedures such as maximum likelihood or\ncross-validation. However, it may be questioned whether statistical procedures\nfor learning covariance functions are the most efficient for optimization as\nthey target a global agreement between the GP and the observations which is not\nthe ultimate goal of optimization. Furthermore, statistical learning procedures\nare computationally expensive. The main alternative to the statistical learning\nof the GP is self-adaptation, where the algorithm tunes the kernel parameters\nbased on their contribution to objective function improvement. After\nquestioning the possibility of self-adaptation for kriging based optimizers,\nthis paper proposes a novel approach for tuning the length-scale of the GP in\nEGO: At each iteration, a small ensemble of kriging models structured by their\nlength-scales is created. All of the models contribute to an iterate in an\nEGO-like fashion. Then, the set of models is densified around the model whose\nlength-scale yielded the best iterate and further points are produced.\nNumerical experiments are provided which motivate the use of many\nlength-scales. The tested implementation does not perform better than the\nclassical EGO algorithm in a sequential context but show the potential of the\napproach for parallel implementations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 19:42:14 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Mohammadi", "Hossein", ""], ["Riche", "Rodolphe Le", ""], ["Touboul", "Eric", ""]]}, {"id": "1603.02644", "submitter": "Christophe Dupuy", "authors": "Christophe Dupuy (SIERRA), Francis Bach (LIENS, SIERRA)", "title": "Online but Accurate Inference for Latent Variable Models with Local\n  Gibbs Sampling", "comments": null, "journal-ref": "Journal of Machine Learning Research, Journal of Machine Learning\n  Research, 2017, 18, pp.1 - 45", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study parameter inference in large-scale latent variable models. We first\npropose an unified treatment of online inference for latent variable models\nfrom a non-canonical exponential family, and draw explicit links between\nseveral previously proposed frequentist or Bayesian methods. We then propose a\nnovel inference method for the frequentist estimation of parameters, that\nadapts MCMC methods to online inference of latent variable models with the\nproper use of local Gibbs sampling. Then, for latent Dirich-let allocation,we\nprovide an extensive set of experiments and comparisons with existing work,\nwhere our new approach outperforms all previously proposed methods. In\nparticular, using Gibbs sampling for latent variable inference is superior to\nvariational inference in terms of test log-likelihoods. Moreover, Bayesian\ninference through variational methods perform poorly, sometimes leading to\nworse fits with latent variables of higher dimensionality.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 19:57:47 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 20:00:25 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2016 12:54:53 GMT"}, {"version": "v4", "created": "Tue, 30 Jan 2018 07:19:15 GMT"}, {"version": "v5", "created": "Wed, 31 Jan 2018 08:38:26 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Dupuy", "Christophe", "", "SIERRA"], ["Bach", "Francis", "", "LIENS, SIERRA"]]}, {"id": "1603.02695", "submitter": "Charlie Marshak", "authors": "Mihai Cucuringu, Charlie Marshak, Dillon Montag, and Puck Rombach", "title": "Rank Aggregation for Course Sequence Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this work, we adapt the rank aggregation framework for the discovery of\noptimal course sequences at the university level. Each student provides a\npartial ranking of the courses taken throughout his or her undergraduate\ncareer. We compute pairwise rank comparisons between courses based on the order\nstudents typically take them, aggregate the results over the entire student\npopulation, and then obtain a proxy for the rank offset between pairs of\ncourses. We extract a global ranking of the courses via several state-of-the\nart algorithms for ranking with pairwise noisy information, including\nSerialRank, Rank Centrality, and the recent SyncRank based on the group\nsynchronization problem. We test this application of rank aggregation on 15\nyears of student data from the Department of Mathematics at the University of\nCalifornia, Los Angeles (UCLA). Furthermore, we experiment with the above\napproach on different subsets of the student population conditioned on final\nGPA, and highlight several differences in the obtained rankings that uncover\nhidden pre-requisites in the Mathematics curriculum.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 17:29:03 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Cucuringu", "Mihai", ""], ["Marshak", "Charlie", ""], ["Montag", "Dillon", ""], ["Rombach", "Puck", ""]]}, {"id": "1603.02752", "submitter": "Max Simchowitz", "authors": "Max Simchowitz, Kevin Jamieson, Benjamin Recht", "title": "Best-of-K Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the Best-of-K Bandit game: At each time the player chooses\na subset S among all N-choose-K possible options and observes reward max(X(i) :\ni in S) where X is a random vector drawn from a joint distribution. The\nobjective is to identify the subset that achieves the highest expected reward\nwith high probability using as few queries as possible. We present\ndistribution-dependent lower bounds based on a particular construction which\nforce a learner to consider all N-choose-K subsets, and match naive extensions\nof known upper bounds in the bandit setting obtained by treating each subset as\na separate arm. Nevertheless, we present evidence that exhaustive search may be\navoided for certain, favorable distributions because the influence of\nhigh-order order correlations may be dominated by lower order statistics.\nFinally, we present an algorithm and analysis for independent arms, which\nmitigates the surprising non-trivial information occlusion that occurs due to\nonly observing the max in the subset. This may inform strategies for more\ngeneral dependent measures, and we complement these result with independent-arm\nlower bounds.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 00:55:58 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 20:06:50 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Simchowitz", "Max", ""], ["Jamieson", "Kevin", ""], ["Recht", "Benjamin", ""]]}, {"id": "1603.02754", "submitter": "Tianqi Chen", "authors": "Tianqi Chen and Carlos Guestrin", "title": "XGBoost: A Scalable Tree Boosting System", "comments": "KDD'16 changed all figures to type1", "journal-ref": null, "doi": "10.1145/2939672.2939785", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree boosting is a highly effective and widely used machine learning method.\nIn this paper, we describe a scalable end-to-end tree boosting system called\nXGBoost, which is used widely by data scientists to achieve state-of-the-art\nresults on many machine learning challenges. We propose a novel sparsity-aware\nalgorithm for sparse data and weighted quantile sketch for approximate tree\nlearning. More importantly, we provide insights on cache access patterns, data\ncompression and sharding to build a scalable tree boosting system. By combining\nthese insights, XGBoost scales beyond billions of examples using far fewer\nresources than existing systems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 01:11:51 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 22:11:40 GMT"}, {"version": "v3", "created": "Fri, 10 Jun 2016 23:23:51 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Chen", "Tianqi", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1603.02763", "submitter": "Marina Meila", "authors": "James McQueen and Marina Meila and Jacob VanderPlas and Zhongyue Zhang", "title": "megaman: Manifold Learning with Millions of points", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold Learning is a class of algorithms seeking a low-dimensional\nnon-linear representation of high-dimensional data. Thus manifold learning\nalgorithms are, at least in theory, most applicable to high-dimensional data\nand sample sizes to enable accurate estimation of the manifold. Despite this,\nmost existing manifold learning implementations are not particularly scalable.\nHere we present a Python package that implements a variety of manifold learning\nalgorithms in a modular and scalable fashion, using fast approximate neighbors\nsearches and fast sparse eigendecompositions. The package incorporates\ntheoretical advances in manifold learning, such as the unbiased Laplacian\nestimator and the estimation of the embedding distortion by the Riemannian\nmetric method. In benchmarks, even on a single-core desktop computer, our code\nembeds millions of data points in minutes, and takes just 200 minutes to embed\nthe main sample of galaxy spectra from the Sloan Digital Sky Survey ---\nconsisting of 0.6 million samples in 3750-dimensions --- a task which has not\npreviously been possible.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 02:05:11 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["McQueen", "James", ""], ["Meila", "Marina", ""], ["VanderPlas", "Jacob", ""], ["Zhang", "Zhongyue", ""]]}, {"id": "1603.02806", "submitter": "Gustau Camps-Valls", "authors": "Emma Izquierdo-Verdiguier, Valero Laparra, Robert Jenssen, Luis\n  G\\'omez-Chova, Gustau Camps-Valls", "title": "Optimized Kernel Entropy Components", "comments": "IEEE Transactions on Neural Networks and Learning Systems, 2016", "journal-ref": null, "doi": "10.1109/TNNLS.2016.2530403.", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses two main issues of the standard Kernel Entropy Component\nAnalysis (KECA) algorithm: the optimization of the kernel decomposition and the\noptimization of the Gaussian kernel parameter. KECA roughly reduces to a\nsorting of the importance of kernel eigenvectors by entropy instead of by\nvariance as in Kernel Principal Components Analysis. In this work, we propose\nan extension of the KECA method, named Optimized KECA (OKECA), that directly\nextracts the optimal features retaining most of the data entropy by means of\ncompacting the information in very few features (often in just one or two). The\nproposed method produces features which have higher expressive power. In\nparticular, it is based on the Independent Component Analysis (ICA) framework,\nand introduces an extra rotation to the eigen-decomposition, which is optimized\nvia gradient ascent search. This maximum entropy preservation suggests that\nOKECA features are more efficient than KECA features for density estimation. In\naddition, a critical issue in both methods is the selection of the kernel\nparameter since it critically affects the resulting performance. Here we\nanalyze the most common kernel length-scale selection criteria. Results of both\nmethods are illustrated in different synthetic and real problems. Results show\nthat 1) OKECA returns projections with more expressive power than KECA, 2) the\nmost successful rule for estimating the kernel parameter is based on maximum\nlikelihood, and 3) OKECA is more robust to the selection of the length-scale\nparameter in kernel density estimation.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 08:09:59 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Izquierdo-Verdiguier", "Emma", ""], ["Laparra", "Valero", ""], ["Jenssen", "Robert", ""], ["G\u00f3mez-Chova", "Luis", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "1603.02836", "submitter": "Anirban Santara", "authors": "Anirban Santara, Debapriya Maji, DP Tejas, Pabitra Mitra and Arobinda\n  Gupta", "title": "Faster learning of deep stacked autoencoders on multi-core systems using\n  synchronized layer-wise pre-training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are capable of modelling highly non-linear functions by\ncapturing different levels of abstraction of data hierarchically. While\ntraining deep networks, first the system is initialized near a good optimum by\ngreedy layer-wise unsupervised pre-training. However, with burgeoning data and\nincreasing dimensions of the architecture, the time complexity of this approach\nbecomes enormous. Also, greedy pre-training of the layers often turns\ndetrimental by over-training a layer causing it to lose harmony with the rest\nof the network. In this paper a synchronized parallel algorithm for\npre-training deep networks on multi-core machines has been proposed. Different\nlayers are trained by parallel threads running on different cores with regular\nsynchronization. Thus the pre-training process becomes faster and chances of\nover-training are reduced. This is experimentally validated using a stacked\nautoencoder for dimensionality reduction of MNIST handwritten digit database.\nThe proposed algorithm achieved 26\\% speed-up compared to greedy layer-wise\npre-training for achieving the same reconstruction accuracy substantiating its\npotential as an alternative.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 10:31:00 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Santara", "Anirban", ""], ["Maji", "Debapriya", ""], ["Tejas", "DP", ""], ["Mitra", "Pabitra", ""], ["Gupta", "Arobinda", ""]]}, {"id": "1603.02839", "submitter": "Hadi Daneshmand", "authors": "Hadi Daneshmand, Aurelien Lucchi, Thomas Hofmann", "title": "Starting Small -- Learning with Adaptive Sample Sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many machine learning problems, data is abundant and it may be\nprohibitive to make multiple passes through the full training set. In this\ncontext, we investigate strategies for dynamically increasing the effective\nsample size, when using iterative methods such as stochastic gradient descent.\nOur interest is motivated by the rise of variance-reduced methods, which\nachieve linear convergence rates that scale favorably for smaller sample sizes.\nExploiting this feature, we show -- theoretically and empirically -- how to\nobtain significant speed-ups with a novel algorithm that reaches statistical\naccuracy on an $n$-sample in $2n$, instead of $n \\log n$ steps.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 10:52:53 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 12:33:13 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Daneshmand", "Hadi", ""], ["Lucchi", "Aurelien", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1603.03116", "submitter": "Antonio Valerio Miceli Barone", "authors": "Antonio Valerio Miceli Barone", "title": "Low-rank passthrough neural networks", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various common deep learning architectures, such as LSTMs, GRUs, Resnets and\nHighway Networks, employ state passthrough connections that support training\nwith high feed-forward depth or recurrence over many time steps. These\n\"Passthrough Networks\" architectures also enable the decoupling of the network\nstate size from the number of parameters of the network, a possibility has been\nstudied by \\newcite{Sak2014} with their low-rank parametrization of the LSTM.\nIn this work we extend this line of research, proposing effective, low-rank and\nlow-rank plus diagonal matrix parametrizations for Passthrough Networks which\nexploit this decoupling property, reducing the data complexity and memory\nrequirements of the network while preserving its memory capacity. This is\nparticularly beneficial in low-resource settings as it supports expressive\nmodels with a compact parametrization less susceptible to overfitting. We\npresent competitive experimental results on several tasks, including language\nmodeling and a near state of the art result on sequential randomly-permuted\nMNIST classification, a hard task on natural data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 01:04:07 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 19:38:30 GMT"}, {"version": "v3", "created": "Mon, 9 Jul 2018 16:19:29 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Barone", "Antonio Valerio Miceli", ""]]}, {"id": "1603.03130", "submitter": "Gang Niu", "authors": "Gang Niu, Marthinus Christoffel du Plessis, Tomoya Sakai, Yao Ma, and\n  Masashi Sugiyama", "title": "Theoretical Comparisons of Positive-Unlabeled Learning against\n  Positive-Negative Learning", "comments": "NIPS 2016 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In PU learning, a binary classifier is trained from positive (P) and\nunlabeled (U) data without negative (N) data. Although N data is missing, it\nsometimes outperforms PN learning (i.e., ordinary supervised learning).\nHitherto, neither theoretical nor experimental analysis has been given to\nexplain this phenomenon. In this paper, we theoretically compare PU (and NU)\nlearning against PN learning based on the upper bounds on estimation errors. We\nfind simple conditions when PU and NU learning are likely to outperform PN\nlearning, and we prove that, in terms of the upper bounds, either PU or NU\nlearning (depending on the class-prior probability and the sizes of P and N\ndata) given infinite U data will improve on PN learning. Our theoretical\nfindings well agree with the experimental results on artificial and benchmark\ndata even when the experimental setup does not match the theoretical\nassumptions exactly.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 02:53:52 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 04:35:47 GMT"}, {"version": "v3", "created": "Fri, 28 Oct 2016 13:37:46 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Niu", "Gang", ""], ["Plessis", "Marthinus Christoffel du", ""], ["Sakai", "Tomoya", ""], ["Ma", "Yao", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1603.03158", "submitter": "Nathaniel Grammel", "authors": "Nathaniel Grammel, Lisa Hellerstein, Devorah Kletenik, Patrick Lin", "title": "Scenario Submodular Cover", "comments": "32 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in Machine Learning can be modeled as submodular optimization\nproblems. Recent work has focused on stochastic or adaptive versions of these\nproblems. We consider the Scenario Submodular Cover problem, which is a\ncounterpart to the Stochastic Submodular Cover problem studied by Golovin and\nKrause. In Scenario Submodular Cover, the goal is to produce a cover with\nminimum expected cost, where the expectation is with respect to an empirical\njoint distribution, given as input by a weighted sample of realizations. In\ncontrast, in Stochastic Submodular Cover, the variables of the input\ndistribution are assumed to be independent, and the distribution of each\nvariable is given as input. Building on algorithms developed by Cicalese et al.\nand Golovin and Krause for related problems, we give two approximation\nalgorithms for Scenario Submodular Cover over discrete distributions. The first\nachieves an approximation factor of O(log Qm), where m is the size of the\nsample and Q is the goal utility. The second, simpler algorithm achieves an\napproximation bound of O(log QW), where Q is the goal utility and W is the sum\nof the integer weights. (Both bounds assume an integer-valued utility\nfunction.) Our results yield approximation bounds for other problems involving\nnon-independent distributions that are explicitly specified by their support.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 06:43:52 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Grammel", "Nathaniel", ""], ["Hellerstein", "Lisa", ""], ["Kletenik", "Devorah", ""], ["Lin", "Patrick", ""]]}, {"id": "1603.03185", "submitter": "Ouais Alsharif", "authors": "Ian McGraw, Rohit Prabhavalkar, Raziel Alvarez, Montse Gonzalez\n  Arenas, Kanishka Rao, David Rybach, Ouais Alsharif, Hasim Sak, Alexander\n  Gruenstein, Francoise Beaufays, Carolina Parada", "title": "Personalized Speech recognition on mobile devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a large vocabulary speech recognition system that is accurate,\nhas low latency, and yet has a small enough memory and computational footprint\nto run faster than real-time on a Nexus 5 Android smartphone. We employ a\nquantized Long Short-Term Memory (LSTM) acoustic model trained with\nconnectionist temporal classification (CTC) to directly predict phoneme\ntargets, and further reduce its memory footprint using an SVD-based compression\nscheme. Additionally, we minimize our memory footprint by using a single\nlanguage model for both dictation and voice command domains, constructed using\nBayesian interpolation. Finally, in order to properly handle device-specific\ninformation, such as proper names and other context-dependent information, we\ninject vocabulary items into the decoder graph and bias the language model\non-the-fly. Our system achieves 13.5% word error rate on an open-ended\ndictation task, running with a median speed that is seven times faster than\nreal-time.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 08:51:51 GMT"}, {"version": "v2", "created": "Fri, 11 Mar 2016 22:25:39 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["McGraw", "Ian", ""], ["Prabhavalkar", "Rohit", ""], ["Alvarez", "Raziel", ""], ["Arenas", "Montse Gonzalez", ""], ["Rao", "Kanishka", ""], ["Rybach", "David", ""], ["Alsharif", "Ouais", ""], ["Sak", "Hasim", ""], ["Gruenstein", "Alexander", ""], ["Beaufays", "Francoise", ""], ["Parada", "Carolina", ""]]}, {"id": "1603.03236", "submitter": "Sebastian Weichwald", "authors": "James Townsend, Niklas Koep, Sebastian Weichwald", "title": "Pymanopt: A Python Toolbox for Optimization on Manifolds using Automatic\n  Differentiation", "comments": null, "journal-ref": "Journal of Machine Learning Research, 17(137):1-5, 2016 (\n  https://jmlr.org/papers/v17/16-177.html )", "doi": null, "report-no": null, "categories": "cs.MS cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization on manifolds is a class of methods for optimization of an\nobjective function, subject to constraints which are smooth, in the sense that\nthe set of points which satisfy the constraints admits the structure of a\ndifferentiable manifold. While many optimization problems are of the described\nform, technicalities of differential geometry and the laborious calculation of\nderivatives pose a significant barrier for experimenting with these methods.\n  We introduce Pymanopt (available at https://pymanopt.github.io), a toolbox\nfor optimization on manifolds, implemented in Python, that---similarly to the\nManopt Matlab toolbox---implements several manifold geometries and optimization\nalgorithms. Moreover, we lower the barriers to users further by using automated\ndifferentiation for calculating derivative information, saving users time and\nsaving them from potential calculation and implementation errors.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 12:23:12 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 12:46:31 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2016 10:04:13 GMT"}, {"version": "v4", "created": "Thu, 8 Sep 2016 09:23:08 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Townsend", "James", ""], ["Koep", "Niklas", ""], ["Weichwald", "Sebastian", ""]]}, {"id": "1603.03281", "submitter": "UshaRani Yelipe", "authors": "Yelipe UshaRani, P. Sammulal", "title": "An Innovative Imputation and Classification Approach for Accurate\n  Disease Prediction", "comments": "Special Issue of Journal IJCSIS indexed in Web of Science and Thomson\n  Reuters ISI. https://sites.google.com/site/ijcsis/vol-14-s1-feb-2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imputation of missing attribute values in medical datasets for extracting\nhidden knowledge from medical datasets is an interesting research topic of\ninterest which is very challenging. One cannot eliminate missing values in\nmedical records. The reason may be because some tests may not been conducted as\nthey are cost effective, values missed when conducting clinical trials, values\nmay not have been recorded to name some of the reasons. Data mining researchers\nhave been proposing various approaches to find and impute missing values to\nincrease classification accuracies so that disease may be predicted accurately.\nIn this paper, we propose a novel imputation approach for imputation of missing\nvalues and performing classification after fixing missing values. The approach\nis based on clustering concept and aims at dimensionality reduction of the\nrecords. The case study discussed shows that missing values can be fixed and\nimputed efficiently by achieving dimensionality reduction. The importance of\nproposed approach for classification is visible in the case study which assigns\nsingle class label in contrary to multi-label assignment if dimensionality\nreduction is not performed.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 14:31:33 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["UshaRani", "Yelipe", ""], ["Sammulal", "P.", ""]]}, {"id": "1603.03336", "submitter": "Francois Belletti", "authors": "Francois W. Belletti, Evan R. Sparks, Michael J. Franklin, Alexandre\n  M. Bayen, Joseph E. Gonzalez", "title": "Scalable Linear Causal Inference for Irregularly Sampled Time Series\n  with Long Range Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear causal analysis is central to a wide range of important application\nspanning finance, the physical sciences, and engineering. Much of the existing\nliterature in linear causal analysis operates in the time domain.\nUnfortunately, the direct application of time domain linear causal analysis to\nmany real-world time series presents three critical challenges: irregular\ntemporal sampling, long range dependencies, and scale. Moreover, real-world\ndata is often collected at irregular time intervals across vast arrays of\ndecentralized sensors and with long range dependencies which make naive time\ndomain correlation estimators spurious. In this paper we present a frequency\ndomain based estimation framework which naturally handles irregularly sampled\ndata and long range dependencies while enabled memory and communication\nefficient distributed processing of time series data. By operating in the\nfrequency domain we eliminate the need to interpolate and help mitigate the\neffects of long range dependencies. We implement and evaluate our new work-flow\nin the distributed setting using Apache Spark and demonstrate on both Monte\nCarlo simulations and high-frequency financial trading that we can accurately\nrecover causal structure at scale.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 17:12:03 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Belletti", "Francois W.", ""], ["Sparks", "Evan R.", ""], ["Franklin", "Michael J.", ""], ["Bayen", "Alexandre M.", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "1603.03515", "submitter": "Lin Chen", "authors": "Lin Chen, Hamed Hassani, Amin Karbasi", "title": "Near-Optimal Active Learning of Halfspaces via Query Synthesis in the\n  Noisy Setting", "comments": "Accepted by AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of actively learning a linear\nclassifier through query synthesis where the learner can construct artificial\nqueries in order to estimate the true decision boundaries. This problem has\nrecently gained a lot of interest in automated science and adversarial reverse\nengineering for which only heuristic algorithms are known. In such\napplications, queries can be constructed de novo to elicit information (e.g.,\nautomated science) or to evade detection with minimal cost (e.g., adversarial\nreverse engineering). We develop a general framework, called dimension coupling\n(DC), that 1) reduces a d-dimensional learning problem to d-1 low dimensional\nsub-problems, 2) solves each sub-problem efficiently, 3) appropriately\naggregates the results and outputs a linear classifier, and 4) provides a\ntheoretical guarantee for all possible schemes of aggregation. The proposed\nmethod is proved resilient to noise. We show that the DC framework avoids the\ncurse of dimensionality: its computational complexity scales linearly with the\ndimension. Moreover, we show that the query complexity of DC is near optimal\n(within a constant factor of the optimum algorithm). To further support our\ntheoretical analysis, we compare the performance of DC with the existing work.\nWe observe that DC consistently outperforms the prior arts in terms of query\ncomplexity while often running orders of magnitude faster.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 04:18:48 GMT"}, {"version": "v2", "created": "Sat, 12 Nov 2016 17:39:47 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Chen", "Lin", ""], ["Hassani", "Hamed", ""], ["Karbasi", "Amin", ""]]}, {"id": "1603.03541", "submitter": "Chenxia Wu", "authors": "Chenxia Wu, Jiemi Zhang, Ozan Sener, Bart Selman, Silvio Savarese,\n  Ashutosh Saxena", "title": "Watch-n-Patch: Unsupervised Learning of Actions and Relations", "comments": "arXiv admin note: text overlap with arXiv:1512.04208", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a large variation in the activities that humans perform in their\neveryday lives. We consider modeling these composite human activities which\ncomprises multiple basic level actions in a completely unsupervised setting.\nOur model learns high-level co-occurrence and temporal relations between the\nactions. We consider the video as a sequence of short-term action clips, which\ncontains human-words and object-words. An activity is about a set of\naction-topics and object-topics indicating which actions are present and which\nobjects are interacting with. We then propose a new probabilistic model\nrelating the words and the topics. It allows us to model long-range action\nrelations that commonly exist in the composite activities, which is challenging\nin previous works. We apply our model to the unsupervised action segmentation\nand clustering, and to a novel application that detects forgotten actions,\nwhich we call action patching. For evaluation, we contribute a new challenging\nRGB-D activity video dataset recorded by the new Kinect v2, which contains\nseveral human daily activities as compositions of multiple actions interacting\nwith different objects. Moreover, we develop a robotic system that watches\npeople and reminds people by applying our action patching algorithm. Our\nrobotic setup can be easily deployed on any assistive robot.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 07:13:59 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Wu", "Chenxia", ""], ["Zhang", "Jiemi", ""], ["Sener", "Ozan", ""], ["Selman", "Bart", ""], ["Savarese", "Silvio", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1603.03627", "submitter": "Roberto Luis Shinmoto Torres", "authors": "Roberto L. Shinmoto Torres and Damith C. Ranasinghe and Qinfeng Shi\n  and Anton van den Hengel", "title": "Learning from Imbalanced Multiclass Sequential Data Streams Using\n  Dynamically Weighted Conditional Random Fields", "comments": "28 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study introduces a method for improving the classification\nperformance of imbalanced multiclass data streams from wireless body worn\nsensors. Data imbalance is an inherent problem in activity recognition caused\nby the irregular time distribution of activities, which are sequential and\ndependent on previous movements. We use conditional random fields (CRF), a\ngraphical model for structured classification, to take advantage of\ndependencies between activities in a sequence. However, CRFs do not consider\nthe negative effects of class imbalance during training. We propose a\nclass-wise dynamically weighted CRF (dWCRF) where weights are automatically\ndetermined during training by maximizing the expected overall F-score. Our\nresults based on three case studies from a healthcare application using a\nbatteryless body worn sensor, demonstrate that our method, in general, improves\noverall and minority class F-score when compared to other CRF based classifiers\nand achieves similar or better overall and class-wise performance when compared\nto SVM based classifiers under conditions of limited training data. We also\nconfirm the performance of our approach using an additional battery powered\nbody worn sensor dataset, achieving similar results in cases of high class\nimbalance.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 13:51:37 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Torres", "Roberto L. Shinmoto", ""], ["Ranasinghe", "Damith C.", ""], ["Shi", "Qinfeng", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1603.03657", "submitter": "Koen Groenland", "authors": "Koen Groenland, Sander Bohte", "title": "Efficient forward propagation of time-sequences in convolutional neural\n  networks using Deep Shifting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a Convolutional Neural Network is used for on-the-fly evaluation of\ncontinuously updating time-sequences, many redundant convolution operations are\nperformed. We propose the method of Deep Shifting, which remembers previously\ncalculated results of convolution operations in order to minimize the number of\ncalculations. The reduction in complexity is at least a constant and in the\nbest case quadratic. We demonstrate that this method does indeed save\nsignificant computation time in a practical implementation, especially when the\nnetworks receives a large number of time-frames.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 15:16:09 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Groenland", "Koen", ""], ["Bohte", "Sander", ""]]}, {"id": "1603.03678", "submitter": "Kristjan Greenewald", "authors": "Kristjan Greenewald, Stephen Kelley, Alfred Hero", "title": "Nonstationary Distance Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in distance metric learning has focused on learning\ntransformations of data that best align with provided sets of pairwise\nsimilarity and dissimilarity constraints. The learned transformations lead to\nimproved retrieval, classification, and clustering algorithms due to the better\nadapted distance or similarity measures. Here, we introduce the problem of\nlearning these transformations when the underlying constraint generation\nprocess is nonstationary. This nonstationarity can be due to changes in either\nthe ground-truth clustering used to generate constraints or changes to the\nfeature subspaces in which the class structure is apparent. We propose and\nevaluate COMID-SADL, an adaptive, online approach for learning and tracking\noptimal metrics as they change over time that is highly robust to a variety of\nnonstationary behaviors in the changing metric. We demonstrate COMID-SADL on\nboth real and synthetic data sets and show significant performance improvements\nrelative to previously proposed batch and online distance metric learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 16:16:45 GMT"}, {"version": "v2", "created": "Sun, 22 May 2016 09:27:10 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Greenewald", "Kristjan", ""], ["Kelley", "Stephen", ""], ["Hero", "Alfred", ""]]}, {"id": "1603.03685", "submitter": "Lorenzo Livi", "authors": "Lorenzo Livi, Filippo Maria Bianchi, Cesare Alippi", "title": "Determination of the edge of criticality in echo state networks through\n  Fisher information maximization", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2016.2644268", "report-no": null, "categories": "physics.data-an cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a widely accepted fact that the computational capability of recurrent\nneural networks is maximized on the so-called \"edge of criticality\". Once the\nnetwork operates in this configuration, it performs efficiently on a specific\napplication both in terms of (i) low prediction error and (ii) high short-term\nmemory capacity. Since the behavior of recurrent networks is strongly\ninfluenced by the particular input signal driving the dynamics, a universal,\napplication-independent method for determining the edge of criticality is still\nmissing. In this paper, we aim at addressing this issue by proposing a\ntheoretically motivated, unsupervised method based on Fisher information for\ndetermining the edge of criticality in recurrent neural networks. It is proven\nthat Fisher information is maximized for (finite-size) systems operating in\nsuch critical regions. However, Fisher information is notoriously difficult to\ncompute and either requires the probability density function or the conditional\ndependence of the system states with respect to the model parameters. The paper\ntakes advantage of a recently-developed non-parametric estimator of the Fisher\ninformation matrix and provides a method to determine the critical region of\necho state networks, a particular class of recurrent networks. The considered\ncontrol parameters, which indirectly affect the echo state network performance,\nare explored to identify those configurations lying on the edge of criticality\nand, as such, maximizing Fisher information and computational performance.\nExperimental results on benchmarks and real-world data demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 16:32:23 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 20:21:29 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Livi", "Lorenzo", ""], ["Bianchi", "Filippo Maria", ""], ["Alippi", "Cesare", ""]]}, {"id": "1603.03703", "submitter": "Jaesik Choi", "authors": "Kallol Roy and Anh Tong and Jaesik Choi", "title": "Searching for Topological Symmetry in Data Haystack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding interesting symmetrical topological structures in high-dimensional\nsystems is an important problem in statistical machine learning. Limited amount\nof available high-dimensional data and its sensitivity to noise pose\ncomputational challenges to find symmetry. Our paper presents a new method to\nfind local symmetries in a low-dimensional 2-D grid structure which is embedded\nin high-dimensional structure. To compute the symmetry in a grid structure, we\nintroduce three legal grid moves (i) Commutation (ii) Cyclic Permutation (iii)\nStabilization on sets of local grid squares, grid blocks. The three grid moves\nare legal transformations as they preserve the statistical distribution of\nhamming distances in each grid block. We propose and coin the term of grid\nsymmetry of data on the 2-D data grid as the invariance of statistical\ndistributions of hamming distance are preserved after a sequence of grid moves.\nWe have computed and analyzed the grid symmetry of data on multivariate\nGaussian distributions and Gamma distributions with noise.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 17:47:00 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Roy", "Kallol", ""], ["Tong", "Anh", ""], ["Choi", "Jaesik", ""]]}, {"id": "1603.03713", "submitter": "Flavian Vasile", "authors": "Flavian Vasile, Damien Lefortier, Olivier Chapelle", "title": "Cost-sensitive Learning for Utility Optimization in Online Advertising\n  Auctions", "comments": "First version of the paper was presented at NIPS 2015 Workshop on\n  E-Commerce: https://sites.google.com/site/nips15ecommerce/papers Third\n  version of the paper will be presented at AdKDD 2017 Workshop:\n  adkdd17.wixsite.com/adkddtargetad2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most challenging problems in computational advertising is the\nprediction of click-through and conversion rates for bidding in online\nadvertising auctions. An unaddressed problem in previous approaches is the\nexistence of highly non-uniform misprediction costs. While for model evaluation\nthese costs have been taken into account through recently proposed\nbusiness-aware offline metrics -- such as the Utility metric which measures the\nimpact on advertiser profit -- this is not the case when training the models\nthemselves. In this paper, to bridge the gap, we formally analyze the\nrelationship between optimizing the Utility metric and the log loss, which is\nconsidered as one of the state-of-the-art approaches in conversion modeling.\nOur analysis motivates the idea of weighting the log loss with the business\nvalue of the predicted outcome. We present and analyze a new cost weighting\nscheme and show that significant gains in offline and online performance can be\nachieved.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 18:21:55 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 14:37:00 GMT"}, {"version": "v3", "created": "Wed, 12 Jul 2017 09:30:53 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Vasile", "Flavian", ""], ["Lefortier", "Damien", ""], ["Chapelle", "Olivier", ""]]}, {"id": "1603.03714", "submitter": "Amit Daniely", "authors": "Galit Bary-Weisberg and Amit Daniely and Shai Shalev-Shwartz", "title": "Distribution Free Learning with Local Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model of learning with \\emph{local membership queries} interpolates\nbetween the PAC model and the membership queries model by allowing the learner\nto query the label of any example that is similar to an example in the training\nset. This model, recently proposed and studied by Awasthi, Feldman and Kanade,\naims to facilitate practical use of membership queries.\n  We continue this line of work, proving both positive and negative results in\nthe {\\em distribution free} setting. We restrict to the boolean cube $\\{-1,\n1\\}^n$, and say that a query is $q$-local if it is of a hamming distance $\\le\nq$ from some training example. On the positive side, we show that $1$-local\nqueries already give an additional strength, and allow to learn a certain type\nof DNF formulas. On the negative side, we show that even\n$\\left(n^{0.99}\\right)$-local queries cannot help to learn various classes\nincluding Automata, DNFs and more. Likewise, $q$-local queries for any constant\n$q$ cannot help to learn Juntas, Decision Trees, Sparse Polynomials and more.\nMoreover, for these classes, an algorithm that uses\n$\\left(\\log^{0.99}(n)\\right)$-local queries would lead to a breakthrough in the\nbest known running times.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 18:23:44 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Bary-Weisberg", "Galit", ""], ["Daniely", "Amit", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1603.03724", "submitter": "Niharika Gauraha Niharika Gauraha", "authors": "Niharika Gauraha, Swapan K. Parui", "title": "Efficient Clustering of Correlated Variables and Variable Selection in\n  High-Dimensional Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Adaptive Cluster Lasso(ACL) method for variable\nselection in high dimensional sparse regression models with strongly correlated\nvariables. To handle correlated variables, the concept of clustering or\ngrouping variables and then pursuing model fitting is widely accepted. When the\ndimension is very high, finding an appropriate group structure is as difficult\nas the original problem. The ACL is a three-stage procedure where, at the first\nstage, we use the Lasso(or its adaptive or thresholded version) to do initial\nselection, then we also include those variables which are not selected by the\nLasso but are strongly correlated with the variables selected by the Lasso. At\nthe second stage we cluster the variables based on the reduced set of\npredictors and in the third stage we perform sparse estimation such as Lasso on\ncluster representatives or the group Lasso based on the structures generated by\nclustering procedure. We show that our procedure is consistent and efficient in\nfinding true underlying population group structure(under assumption of\nirrepresentable and beta-min conditions). We also study the group selection\nconsistency of our method and we support the theory using simulated and\npseudo-real dataset examples.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 19:06:33 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Gauraha", "Niharika", ""], ["Parui", "Swapan K.", ""]]}, {"id": "1603.03768", "submitter": "Ulugbek Kamilov", "authors": "Ulugbek S. Kamilov, Dehong Liu, Hassan Mansour, and Petros T.\n  Boufounos", "title": "A Recursive Born Approach to Nonlinear Inverse Scattering", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2016.2579647", "report-no": null, "categories": "cs.LG physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Iterative Born Approximation (IBA) is a well-known method for describing\nwaves scattered by semi-transparent objects. In this paper, we present a novel\nnonlinear inverse scattering method that combines IBA with an edge-preserving\ntotal variation (TV) regularizer. The proposed method is obtained by relating\niterations of IBA to layers of a feedforward neural network and developing a\ncorresponding error backpropagation algorithm for efficiently estimating the\npermittivity of the object. Simulations illustrate that, by accounting for\nmultiple scattering, the method successfully recovers the permittivity\ndistribution where the traditional linear inverse scattering fails.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 13:01:11 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Kamilov", "Ulugbek S.", ""], ["Liu", "Dehong", ""], ["Mansour", "Hassan", ""], ["Boufounos", "Petros T.", ""]]}, {"id": "1603.03788", "submitter": "Andrey Kormilitzin", "authors": "Ilya Chevyrev, Andrey Kormilitzin", "title": "A Primer on the Signature Method in Machine Learning", "comments": "45 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In these notes, we wish to provide an introduction to the signature method,\nfocusing on its basic theoretical properties and recent numerical applications.\n  The notes are split into two parts. The first part focuses on the definition\nand fundamental properties of the signature of a path, or the path signature.\nWe have aimed for a minimalistic approach, assuming only familiarity with\nclassical real analysis and integration theory, and supplementing theory with\nstraightforward examples. We have chosen to focus in detail on the principle\nproperties of the signature which we believe are fundamental to understanding\nits role in applications. We also present an informal discussion on some of its\ndeeper properties and briefly mention the role of the signature in rough paths\ntheory, which we hope could serve as a light introduction to rough paths for\nthe interested reader.\n  The second part of these notes discusses practical applications of the path\nsignature to the area of machine learning. The signature approach represents a\nnon-parametric way for extraction of characteristic features from data. The\ndata are converted into a multi-dimensional path by means of various embedding\nalgorithms and then processed for computation of individual terms of the\nsignature which summarise certain information contained in the data. The\nsignature thus transforms raw data into a set of features which are used in\nmachine learning tasks. We will review current progress in applications of\nsignatures to machine learning problems.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 21:24:42 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Chevyrev", "Ilya", ""], ["Kormilitzin", "Andrey", ""]]}, {"id": "1603.03827", "submitter": "Franck Dernoncourt", "authors": "Ji Young Lee, Franck Dernoncourt", "title": "Sequential Short-Text Classification with Recurrent and Convolutional\n  Neural Networks", "comments": "Accepted as a conference paper at NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent approaches based on artificial neural networks (ANNs) have shown\npromising results for short-text classification. However, many short texts\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\nand most existing ANN-based systems do not leverage the preceding short texts\nwhen classifying a subsequent one. In this work, we present a model based on\nrecurrent neural networks and convolutional neural networks that incorporates\nthe preceding short texts. Our model achieves state-of-the-art results on three\ndifferent datasets for dialog act prediction.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 00:02:51 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Lee", "Ji Young", ""], ["Dernoncourt", "Franck", ""]]}, {"id": "1603.03833", "submitter": "Rouhollah Rahmatizadeh", "authors": "Rouhollah Rahmatizadeh, Pooya Abolghasemi, Aman Behal, Ladislau\n  B\\\"ol\\\"oni", "title": "From virtual demonstration to real-world manipulation using LSTM and MDN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots assisting the disabled or elderly must perform complex manipulation\ntasks and must adapt to the home environment and preferences of their user.\nLearning from demonstration is a promising choice, that would allow the\nnon-technical user to teach the robot different tasks. However, collecting\ndemonstrations in the home environment of a disabled user is time consuming,\ndisruptive to the comfort of the user, and presents safety challenges. It would\nbe desirable to perform the demonstrations in a virtual environment. In this\npaper we describe a solution to the challenging problem of behavior transfer\nfrom virtual demonstration to a physical robot. The virtual demonstrations are\nused to train a deep neural network based controller, which is using a Long\nShort Term Memory (LSTM) recurrent neural network to generate trajectories. The\ntraining process uses a Mixture Density Network (MDN) to calculate an error\nsignal suitable for the multimodal nature of demonstrations. The controller\nlearned in the virtual environment is transferred to a physical robot (a\nRethink Robotics Baxter). An off-the-shelf vision component is used to\nsubstitute for geometric knowledge available in the simulation and an inverse\nkinematics module is used to allow the Baxter to enact the trajectory. Our\nexperimental studies validate the three contributions of the paper: (1) the\ncontroller learned from virtual demonstrations can be used to successfully\nperform the manipulation tasks on a physical robot, (2) the LSTM+MDN\narchitectural choice outperforms other choices, such as the use of feedforward\nnetworks and mean-squared error based training signals and (3) allowing\nimperfect demonstrations in the training set also allows the controller to\nlearn how to correct its manipulation mistakes.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 00:47:38 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 23:56:19 GMT"}, {"version": "v3", "created": "Wed, 15 Nov 2017 20:31:07 GMT"}, {"version": "v4", "created": "Wed, 22 Nov 2017 02:44:36 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Rahmatizadeh", "Rouhollah", ""], ["Abolghasemi", "Pooya", ""], ["Behal", "Aman", ""], ["B\u00f6l\u00f6ni", "Ladislau", ""]]}, {"id": "1603.03977", "submitter": "Shuang Song", "authors": "Shuang Song, Yizhen Wang, Kamalika Chaudhuri", "title": "Pufferfish Privacy Mechanisms for Correlated Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern databases include personal and sensitive correlated data, such as\nprivate information on users connected together in a social network, and\nmeasurements of physical activity of single subjects across time. However,\ndifferential privacy, the current gold standard in data privacy, does not\nadequately address privacy issues in this kind of data.\n  This work looks at a recent generalization of differential privacy, called\nPufferfish, that can be used to address privacy in correlated data. The main\nchallenge in applying Pufferfish is a lack of suitable mechanisms. We provide\nthe first mechanism -- the Wasserstein Mechanism -- which applies to any\ngeneral Pufferfish framework. Since this mechanism may be computationally\ninefficient, we provide an additional mechanism that applies to some practical\ncases such as physical activity measurements across time, and is\ncomputationally efficient. Our experimental evaluations indicate that this\nmechanism provides privacy and utility for synthetic as well as real data in\ntwo separate domains.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 00:47:15 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 06:01:57 GMT"}, {"version": "v3", "created": "Sun, 12 Mar 2017 22:47:02 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Song", "Shuang", ""], ["Wang", "Yizhen", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1603.03980", "submitter": "Ravi Ganti", "authors": "Nikhil Rao, Ravi Ganti, Laura Balzano, Rebecca Willett, Robert Nowak", "title": "On Learning High Dimensional Structured Single Index Models", "comments": "7 pages, 3 tables, 1 Figure, substantial text overlap with\n  arXiv:1506.08910; Accepted for publication at AAAI 2017; added new\n  experimental results comparing our method to a single layer neural network", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single Index Models (SIMs) are simple yet flexible semi-parametric models for\nmachine learning, where the response variable is modeled as a monotonic\nfunction of a linear combination of features. Estimation in this context\nrequires learning both the feature weights and the nonlinear function that\nrelates features to observations. While methods have been described to learn\nSIMs in the low dimensional regime, a method that can efficiently learn SIMs in\nhigh dimensions, and under general structural assumptions, has not been\nforthcoming. In this paper, we propose computationally efficient algorithms for\nSIM inference in high dimensions with structural constraints. Our general\napproach specializes to sparsity, group sparsity, and low-rank assumptions\namong others. Experiments show that the proposed method enjoys superior\npredictive performance when compared to generalized linear models, and achieves\nresults comparable to or better than single layer feedforward neural networks\nwith significantly less computational cost.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 01:53:40 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 22:55:39 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Rao", "Nikhil", ""], ["Ganti", "Ravi", ""], ["Balzano", "Laura", ""], ["Willett", "Rebecca", ""], ["Nowak", "Robert", ""]]}, {"id": "1603.04000", "submitter": "Shumeet Baluja", "authors": "Shumeet Baluja", "title": "Learning Typographic Style", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typography is a ubiquitous art form that affects our understanding,\nperception, and trust in what we read. Thousands of different font-faces have\nbeen created with enormous variations in the characters. In this paper, we\nlearn the style of a font by analyzing a small subset of only four letters.\nFrom these four letters, we learn two tasks. The first is a discrimination\ntask: given the four letters and a new candidate letter, does the new letter\nbelong to the same font? Second, given the four basis letters, can we generate\nall of the other letters with the same characteristics as those in the basis\nset? We use deep neural networks to address both tasks, quantitatively and\nqualitatively measure the results in a variety of novel manners, and present a\nthorough investigation of the weaknesses and strengths of the approach.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 05:44:57 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Baluja", "Shumeet", ""]]}, {"id": "1603.04002", "submitter": "Richard Nock", "authors": "Giorgio Patrini, Richard Nock, Stephen Hardy, Tiberio Caetano", "title": "Fast Learning from Distributed Datasets without Entity Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following data fusion scenario: two datasets/peers contain the\nsame real-world entities described using partially shared features, e.g.\nbanking and insurance company records of the same customer base. Our goal is to\nlearn a classifier in the cross product space of the two domains, in the hard\ncase in which no shared ID is available -- e.g. due to anonymization.\nTraditionally, the problem is approached by first addressing entity matching\nand subsequently learning the classifier in a standard manner. We present an\nend-to-end solution which bypasses matching entities, based on the recently\nintroduced concept of Rademacher observations (rados). Informally, we replace\nthe minimisation of a loss over examples, which requires to solve entity\nresolution, by the equivalent minimisation of a (different) loss over rados.\nAmong others, key properties we show are (i) a potentially huge subset of these\nrados does not require to perform entity matching, and (ii) the algorithm that\nprovably minimizes the rado loss over these rados has time and space\ncomplexities smaller than the algorithm minimizing the equivalent example loss.\nLast, we relax a key assumption of the model, that the data is vertically\npartitioned among peers --- in this case, we would not even know the existence\nof a solution to entity resolution. In this more general setting, experiments\nvalidate the possibility of significantly beating even the optimal peer in\nhindsight.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 06:03:39 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Patrini", "Giorgio", ""], ["Nock", "Richard", ""], ["Hardy", "Stephen", ""], ["Caetano", "Tiberio", ""]]}, {"id": "1603.04118", "submitter": "Ravi Ganti", "authors": "Aniruddha Bhargava, Ravi Ganti, Robert Nowak", "title": "Active Algorithms For Preference Learning Problems with Multiple\n  Populations", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we model the problem of learning preferences of a population as\nan active learning problem. We propose an algorithm can adaptively choose pairs\nof items to show to users coming from a heterogeneous population, and use the\nobtained reward to decide which pair of items to show next. We provide\ncomputationally efficient algorithms with provable sample complexity guarantees\nfor this problem in both the noiseless and noisy cases. In the process of\nestablishing sample complexity guarantees for our algorithms, we establish new\nresults using a Nystr{\\\"o}m-like method which can be of independent interest.\nWe supplement our theoretical results with experimental comparisons.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 03:08:24 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 16:48:58 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Bhargava", "Aniruddha", ""], ["Ganti", "Ravi", ""], ["Nowak", "Robert", ""]]}, {"id": "1603.04119", "submitter": "Alekh Agarwal", "authors": "David Abel, Alekh Agarwal, Fernando Diaz, Akshay Krishnamurthy, Robert\n  E. Schapire", "title": "Exploratory Gradient Boosting for Reinforcement Learning in Complex\n  Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional observations and complex real-world dynamics present major\nchallenges in reinforcement learning for both function approximation and\nexploration. We address both of these challenges with two complementary\ntechniques: First, we develop a gradient-boosting style, non-parametric\nfunction approximator for learning on $Q$-function residuals. And second, we\npropose an exploration strategy inspired by the principles of state abstraction\nand information acquisition under uncertainty. We demonstrate the empirical\neffectiveness of these techniques, first, as a preliminary check, on two\nstandard tasks (Blackjack and $n$-Chain), and then on two much larger and more\nrealistic tasks with high-dimensional observation spaces. Specifically, we\nintroduce two benchmarks built within the game Minecraft where the observations\nare pixel arrays of the agent's visual field. A combination of our two\nalgorithmic techniques performs competitively on the standard\nreinforcement-learning tasks while consistently and substantially outperforming\nbaselines on the two tasks with high-dimensional observation spaces. The new\nfunction approximator, exploration strategy, and evaluation benchmarks are each\nof independent interest in the pursuit of reinforcement-learning methods that\nscale to real-world domains.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 03:16:25 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Abel", "David", ""], ["Agarwal", "Alekh", ""], ["Diaz", "Fernando", ""], ["Krishnamurthy", "Akshay", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1603.04136", "submitter": "Kun Yuan", "authors": "Kun Yuan, Bicheng Ying, and Ali H. Sayed", "title": "On the Influence of Momentum Acceleration on Online Learning", "comments": "66 pages, 9 figures, to appear in Journal of Machine Learning\n  Research, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article examines in some detail the convergence rate and\nmean-square-error performance of momentum stochastic gradient methods in the\nconstant step-size and slow adaptation regime. The results establish that\nmomentum methods are equivalent to the standard stochastic gradient method with\na re-scaled (larger) step-size value. The size of the re-scaling is determined\nby the value of the momentum parameter. The equivalence result is established\nfor all time instants and not only in steady-state. The analysis is carried out\nfor general strongly convex and smooth risk functions, and is not limited to\nquadratic risks. One notable conclusion is that the well-known bene ts of\nmomentum constructions for deterministic optimization problems do not\nnecessarily carry over to the adaptive online setting when small constant\nstep-sizes are used to enable continuous adaptation and learn- ing in the\npresence of persistent gradient noise. From simulations, the equivalence\nbetween momentum and standard stochastic gradient methods is also observed for\nnon-differentiable and non-convex problems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 05:05:54 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 06:27:47 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2016 23:18:27 GMT"}, {"version": "v4", "created": "Wed, 12 Oct 2016 05:19:07 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Yuan", "Kun", ""], ["Ying", "Bicheng", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1603.04153", "submitter": "Changho Suh", "authors": "Minje Jang, Sunghyun Kim, Changho Suh, Sewoong Oh", "title": "Top-$K$ Ranking from Pairwise Comparisons: When Spectral Ranking is\n  Optimal", "comments": "23 pages, 3 figures, submitted to the Journals of Machine Learning\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT cs.SI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the top-$K$ rank aggregation problem. Suppose a collection of\nitems is compared in pairs repeatedly, and we aim to recover a consistent\nordering that focuses on the top-$K$ ranked items based on partially revealed\npreference information. We investigate the Bradley-Terry-Luce model in which\none ranks items according to their perceived utilities modeled as noisy\nobservations of their underlying true utilities. Our main contributions are\ntwo-fold. First, in a general comparison model where item pairs to compare are\ngiven a priori, we attain an upper and lower bound on the sample size for\nreliable recovery of the top-$K$ ranked items. Second, more importantly,\nextending the result to a random comparison model where item pairs to compare\nare chosen independently with some probability, we show that in slightly\nrestricted regimes, the gap between the derived bounds reduces to a constant\nfactor, hence reveals that a spectral method can achieve the minimax optimality\non the (order-wise) sample size required for top-$K$ ranking. That is to say,\nwe demonstrate a spectral method alone to be sufficient to achieve the\noptimality and advantageous in terms of computational complexity, as it does\nnot require an additional stage of maximum likelihood estimation that a\nstate-of-the-art scheme employs to achieve the optimality. We corroborate our\nmain results by numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 07:01:28 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Jang", "Minje", ""], ["Kim", "Sunghyun", ""], ["Suh", "Changho", ""], ["Oh", "Sewoong", ""]]}, {"id": "1603.04186", "submitter": "Amir Rosenfeld", "authors": "Amir Rosenfeld, Shimon Ullman", "title": "Visual Concept Recognition and Localization via Iterative Introspection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have been shown to develop internal\nrepresentations, which correspond closely to semantically meaningful objects\nand parts, although trained solely on class labels. Class Activation Mapping\n(CAM) is a recent method that makes it possible to easily highlight the image\nregions contributing to a network's classification decision. We build upon\nthese two developments to enable a network to re-examine informative image\nregions, which we term introspection. We propose a weakly-supervised iterative\nscheme, which shifts its center of attention to increasingly discriminative\nregions as it progresses, by alternating stages of classification and\nintrospection. We evaluate our method and show its effectiveness over a range\nof several datasets, where we obtain competitive or state-of-the-art results:\non Stanford-40 Actions, we set a new state-of the art of 81.74%. On\nFGVC-Aircraft and the Stanford Dogs dataset, we show consistent improvements\nover baselines, some of which include significantly more supervision.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 10:18:03 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 13:27:37 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Rosenfeld", "Amir", ""], ["Ullman", "Shimon", ""]]}, {"id": "1603.04190", "submitter": "Wojciech Kot{\\l}owski", "authors": "Wojciech Kot{\\l}owski, Wouter M. Koolen, Alan Malek", "title": "Online Isotonic Regression", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online version of the isotonic regression problem. Given a\nset of linearly ordered points (e.g., on the real line), the learner must\npredict labels sequentially at adversarially chosen positions and is evaluated\nby her total squared loss compared against the best isotonic (non-decreasing)\nfunction in hindsight. We survey several standard online learning algorithms\nand show that none of them achieve the optimal regret exponent; in fact, most\nof them (including Online Gradient Descent, Follow the Leader and Exponential\nWeights) incur linear regret. We then prove that the Exponential Weights\nalgorithm played over a covering net of isotonic functions has a regret bounded\nby $O\\big(T^{1/3} \\log^{2/3}(T)\\big)$ and present a matching $\\Omega(T^{1/3})$\nlower bound on regret. We provide a computationally efficient version of this\nalgorithm. We also analyze the noise-free case, in which the revealed labels\nare isotonic, and show that the bound can be improved to $O(\\log T)$ or even to\n$O(1)$ (when the labels are revealed in isotonic order). Finally, we extend the\nanalysis beyond squared loss and give bounds for entropic loss and absolute\nloss.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 10:26:23 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 15:52:29 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Kot\u0142owski", "Wojciech", ""], ["Koolen", "Wouter M.", ""], ["Malek", "Alan", ""]]}, {"id": "1603.04245", "submitter": "Andre Wibisono", "authors": "Andre Wibisono, Ashia C. Wilson, Michael I. Jordan", "title": "A Variational Perspective on Accelerated Methods in Optimization", "comments": "38 pages. Subsumes an earlier working draft arXiv:1509.03616", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated gradient methods play a central role in optimization, achieving\noptimal rates in many settings. While many generalizations and extensions of\nNesterov's original acceleration method have been proposed, it is not yet clear\nwhat is the natural scope of the acceleration concept. In this paper, we study\naccelerated methods from a continuous-time perspective. We show that there is a\nLagrangian functional that we call the \\emph{Bregman Lagrangian} which\ngenerates a large class of accelerated methods in continuous time, including\n(but not limited to) accelerated gradient descent, its non-Euclidean extension,\nand accelerated higher-order gradient methods. We show that the continuous-time\nlimit of all of these methods correspond to traveling the same curve in\nspacetime at different speeds. From this perspective, Nesterov's technique and\nmany of its generalizations can be viewed as a systematic way to go from the\ncontinuous-time curves generated by the Bregman Lagrangian to a family of\ndiscrete-time accelerated algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 13:00:18 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Wibisono", "Andre", ""], ["Wilson", "Ashia C.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1603.04259", "submitter": "Oren Barkan", "authors": "Oren Barkan and Noam Koenigstein", "title": "Item2Vec: Neural Item Embedding for Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Collaborative Filtering (CF) algorithms are item-based in the sense that\nthey analyze item-item relations in order to produce item similarities.\nRecently, several works in the field of Natural Language Processing (NLP)\nsuggested to learn a latent representation of words using neural embedding\nalgorithms. Among them, the Skip-gram with Negative Sampling (SGNS), also known\nas word2vec, was shown to provide state-of-the-art results on various\nlinguistics tasks. In this paper, we show that item-based CF can be cast in the\nsame framework of neural word embedding. Inspired by SGNS, we describe a method\nwe name item2vec for item-based CF that produces embedding for items in a\nlatent space. The method is capable of inferring item-item relations even when\nuser information is not available. We present experimental results that\ndemonstrate the effectiveness of the item2vec method and show it is competitive\nwith SVD.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 13:37:03 GMT"}, {"version": "v2", "created": "Sat, 19 Mar 2016 13:45:53 GMT"}, {"version": "v3", "created": "Mon, 20 Feb 2017 20:37:53 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Barkan", "Oren", ""], ["Koenigstein", "Noam", ""]]}, {"id": "1603.04283", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk and Dusko Pavlovic", "title": "Universal probability-free prediction", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct universal prediction systems in the spirit of Popper's\nfalsifiability and Kolmogorov complexity and randomness. These prediction\nsystems do not depend on any statistical assumptions (but under the IID\nassumption they dominate, to within the usual accuracy, conformal prediction).\nOur constructions give rise to a theory of algorithmic complexity and\nrandomness of time containing analogues of several notions and results of the\nclassical theory of Kolmogorov complexity and randomness.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 14:43:48 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 12:02:43 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Vovk", "Vladimir", ""], ["Pavlovic", "Dusko", ""]]}, {"id": "1603.04319", "submitter": "Jalal Etesami", "authors": "Jalal Etesami, Negar Kiyavash, Kun Zhang, Kushagra Singhal", "title": "Learning Network of Multivariate Hawkes Processes: A Time Series\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the influence structure of multiple time series data is of great\ninterest to many disciplines. This paper studies the problem of recovering the\ncausal structure in network of multivariate linear Hawkes processes. In such\nprocesses, the occurrence of an event in one process affects the probability of\noccurrence of new events in some other processes. Thus, a natural notion of\ncausality exists between such processes captured by the support of the\nexcitation matrix. We show that the resulting causal influence network is\nequivalent to the Directed Information graph (DIG) of the processes, which\nencodes the causal factorization of the joint distribution of the processes.\nFurthermore, we present an algorithm for learning the support of excitation\nmatrix (or equivalently the DIG). The performance of the algorithm is evaluated\non synthesized multivariate Hawkes networks as well as a stock market and\nMemeTracker real-world dataset.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 16:08:26 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Etesami", "Jalal", ""], ["Kiyavash", "Negar", ""], ["Zhang", "Kun", ""], ["Singhal", "Kushagra", ""]]}, {"id": "1603.04350", "submitter": "Yuanzhi Li", "authors": "Elad Hazan, Yuanzhi Li", "title": "An optimal algorithm for bandit convex optimization", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online convex optimization against an arbitrary\nadversary with bandit feedback, known as bandit convex optimization. We give\nthe first $\\tilde{O}(\\sqrt{T})$-regret algorithm for this setting based on a\nnovel application of the ellipsoid method to online learning. This bound is\nknown to be tight up to logarithmic factors. Our analysis introduces new tools\nin discrete convex geometry.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 17:15:15 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 17:46:58 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Hazan", "Elad", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1603.04416", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk, Ilia Nouretdinov, Valentina Fedorova, Ivan Petej, and\n  Alex Gammerman", "title": "Criteria of efficiency for conformal prediction", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study optimal conformity measures for various criteria of efficiency of\nclassification in an idealised setting. This leads to an important class of\ncriteria of efficiency that we call probabilistic; it turns out that the most\nstandard criteria of efficiency used in literature on conformal prediction are\nnot probabilistic unless the problem of classification is binary. We consider\nboth unconditional and label-conditional conformal prediction.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 19:49:07 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 12:57:51 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Vovk", "Vladimir", ""], ["Nouretdinov", "Ilia", ""], ["Fedorova", "Valentina", ""], ["Petej", "Ivan", ""], ["Gammerman", "Alex", ""]]}, {"id": "1603.04467", "submitter": "Ian Goodfellow", "authors": "Mart\\'in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng\n  Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,\n  Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,\n  Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh\n  Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,\n  Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar,\n  Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol\n  Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang\n  Zheng", "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed\n  Systems", "comments": "Version 2 updates only the metadata, to correct the formatting of\n  Mart\\'in Abadi's name", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TensorFlow is an interface for expressing machine learning algorithms, and an\nimplementation for executing such algorithms. A computation expressed using\nTensorFlow can be executed with little or no change on a wide variety of\nheterogeneous systems, ranging from mobile devices such as phones and tablets\nup to large-scale distributed systems of hundreds of machines and thousands of\ncomputational devices such as GPU cards. The system is flexible and can be used\nto express a wide variety of algorithms, including training and inference\nalgorithms for deep neural network models, and it has been used for conducting\nresearch and for deploying machine learning systems into production across more\nthan a dozen areas of computer science and other fields, including speech\nrecognition, computer vision, robotics, information retrieval, natural language\nprocessing, geographic information extraction, and computational drug\ndiscovery. This paper describes the TensorFlow interface and an implementation\nof that interface that we have built at Google. The TensorFlow API and a\nreference implementation were released as an open-source package under the\nApache 2.0 license in November, 2015 and are available at www.tensorflow.org.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 20:50:20 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 16:57:12 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Abadi", "Mart\u00edn", ""], ["Agarwal", "Ashish", ""], ["Barham", "Paul", ""], ["Brevdo", "Eugene", ""], ["Chen", "Zhifeng", ""], ["Citro", "Craig", ""], ["Corrado", "Greg S.", ""], ["Davis", "Andy", ""], ["Dean", "Jeffrey", ""], ["Devin", "Matthieu", ""], ["Ghemawat", "Sanjay", ""], ["Goodfellow", "Ian", ""], ["Harp", "Andrew", ""], ["Irving", "Geoffrey", ""], ["Isard", "Michael", ""], ["Jia", "Yangqing", ""], ["Jozefowicz", "Rafal", ""], ["Kaiser", "Lukasz", ""], ["Kudlur", "Manjunath", ""], ["Levenberg", "Josh", ""], ["Mane", "Dan", ""], ["Monga", "Rajat", ""], ["Moore", "Sherry", ""], ["Murray", "Derek", ""], ["Olah", "Chris", ""], ["Schuster", "Mike", ""], ["Shlens", "Jonathon", ""], ["Steiner", "Benoit", ""], ["Sutskever", "Ilya", ""], ["Talwar", "Kunal", ""], ["Tucker", "Paul", ""], ["Vanhoucke", "Vincent", ""], ["Vasudevan", "Vijay", ""], ["Viegas", "Fernanda", ""], ["Vinyals", "Oriol", ""], ["Warden", "Pete", ""], ["Wattenberg", "Martin", ""], ["Wicke", "Martin", ""], ["Yu", "Yuan", ""], ["Zheng", "Xiaoqiang", ""]]}, {"id": "1603.04506", "submitter": "Alexander Gammerman", "authors": "Paolo Toccacheli, Ilia Nouretdinov and Alexander Gammerman", "title": "Conformal Predictors for Compound Activity Prediction", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents an application of Conformal Predictors to a\nchemoinformatics problem of identifying activities of chemical compounds. The\npaper addresses some specific challenges of this domain: a large number of\ncompounds (training examples), high-dimensionality of feature space, sparseness\nand a strong class imbalance. A variant of conformal predictors called\nInductive Mondrian Conformal Predictor is applied to deal with these\nchallenges. Results are presented for several non-conformity measures (NCM)\nextracted from underlying algorithms and different kernels. A number of\nperformance measures are used in order to demonstrate the flexibility of\nInductive Mondrian Conformal Predictors in dealing with such a complex set of\ndata.\n  Keywords: Conformal Prediction, Confidence Estimation, Chemoinformatics,\nNon-Conformity Measure.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 23:37:37 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Toccacheli", "Paolo", ""], ["Nouretdinov", "Ilia", ""], ["Gammerman", "Alexander", ""]]}, {"id": "1603.04530", "submitter": "Jimei Yang", "authors": "Jimei Yang, Brian Price, Scott Cohen, Honglak Lee, Ming-Hsuan Yang", "title": "Object Contour Detection with a Fully Convolutional Encoder-Decoder\n  Network", "comments": "Accepted by CVPR2016 as spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a deep learning algorithm for contour detection with a fully\nconvolutional encoder-decoder network. Different from previous low-level edge\ndetection, our algorithm focuses on detecting higher-level object contours. Our\nnetwork is trained end-to-end on PASCAL VOC with refined ground truth from\ninaccurate polygon annotations, yielding much higher precision in object\ncontour detection than previous methods. We find that the learned model\ngeneralizes well to unseen object classes from the same super-categories on MS\nCOCO and can match state-of-the-art edge detection on BSDS500 with fine-tuning.\nBy combining with the multiscale combinatorial grouping algorithm, our method\ncan generate high-quality segmented object proposals, which significantly\nadvance the state-of-the-art on PASCAL VOC (improving average recall from 0.62\nto 0.67) with a relatively small amount of candidates ($\\sim$1660 per image).\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 02:11:49 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Yang", "Jimei", ""], ["Price", "Brian", ""], ["Cohen", "Scott", ""], ["Lee", "Honglak", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1603.04535", "submitter": "Ke Yan", "authors": "Ke Yan, Lu Kou, and David Zhang", "title": "Learning Domain-Invariant Subspace using Domain Features and\n  Independence Maximization", "comments": "Accepted", "journal-ref": null, "doi": "10.1109/TCYB.2016.2633306", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation algorithms are useful when the distributions of the\ntraining and the test data are different. In this paper, we focus on the\nproblem of instrumental variation and time-varying drift in the field of\nsensors and measurement, which can be viewed as discrete and continuous\ndistributional change in the feature space. We propose maximum independence\ndomain adaptation (MIDA) and semi-supervised MIDA (SMIDA) to address this\nproblem. Domain features are first defined to describe the background\ninformation of a sample, such as the device label and acquisition time. Then,\nMIDA learns a subspace which has maximum independence with the domain features,\nso as to reduce the inter-domain discrepancy in distributions. A feature\naugmentation strategy is also designed to project samples according to their\nbackgrounds so as to improve the adaptation. The proposed algorithms are\nflexible and fast. Their effectiveness is verified by experiments on synthetic\ndatasets and four real-world ones on sensors, measurement, and computer vision.\nThey can greatly enhance the practicability of sensor systems, as well as\nextend the application scope of existing domain adaptation algorithms by\nuniformly handling different kinds of distributional change.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 02:56:22 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 01:39:22 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Yan", "Ke", ""], ["Kou", "Lu", ""], ["Zhang", "David", ""]]}, {"id": "1603.04549", "submitter": "Vijay Kamble", "authors": "Ramesh Johari, Vijay Kamble and Yash Kanoria", "title": "Matching while Learning", "comments": "This paper has been accepted for publication in Operations Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem faced by a service platform that needs to match\nlimited supply with demand but also to learn the attributes of new users in\norder to match them better in the future. We introduce a benchmark model with\nheterogeneous \"workers\" (demand) and a limited supply of \"jobs\" that arrive\nover time. Job types are known to the platform, but worker types are unknown\nand must be learned by observing match outcomes. Workers depart after\nperforming a certain number of jobs. The expected payoff from a match depends\non the pair of types and the goal is to maximize the steady-state rate of\naccumulation of payoff. Though we use terminology inspired by labor markets,\nour framework applies more broadly to platforms where a limited supply of\nheterogeneous products is matched to users over time.\n  Our main contribution is a complete characterization of the structure of the\noptimal policy in the limit that each worker performs many jobs. The platform\nfaces a trade-off for each worker between myopically maximizing payoffs\n(exploitation) and learning the type of the worker (exploration). This creates\na multitude of multi-armed bandit problems, one for each worker, coupled\ntogether by the constraint on availability of jobs of different types (capacity\nconstraints). We find that the platform should estimate a shadow price for each\njob type, and use the payoffs adjusted by these prices, first, to determine its\nlearning goals and then, for each worker, (i) to balance learning with payoffs\nduring the \"exploration phase,\" and (ii) to myopically match after it has\nachieved its learning goals during the \"exploitation phase.\"\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 04:29:31 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 00:11:06 GMT"}, {"version": "v3", "created": "Mon, 1 Oct 2018 00:39:01 GMT"}, {"version": "v4", "created": "Wed, 28 Nov 2018 21:36:16 GMT"}, {"version": "v5", "created": "Sat, 7 Dec 2019 18:16:30 GMT"}, {"version": "v6", "created": "Thu, 23 Apr 2020 19:49:49 GMT"}, {"version": "v7", "created": "Wed, 5 Aug 2020 22:17:03 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Johari", "Ramesh", ""], ["Kamble", "Vijay", ""], ["Kanoria", "Yash", ""]]}, {"id": "1603.04553", "submitter": "Xuezhe Ma", "authors": "Xuezhe Ma and Zhengzhong Liu and Eduard Hovy", "title": "Unsupervised Ranking Model for Entity Coreference Resolution", "comments": "Accepted by NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coreference resolution is one of the first stages in deep language\nunderstanding and its importance has been well recognized in the natural\nlanguage processing community. In this paper, we propose a generative,\nunsupervised ranking model for entity coreference resolution by introducing\nresolution mode variables. Our unsupervised system achieves 58.44% F1 score of\nthe CoNLL metric on the English data from the CoNLL-2012 shared task (Pradhan\net al., 2012), outperforming the Stanford deterministic system (Lee et al.,\n2013) by 3.01%.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 04:39:15 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Ma", "Xuezhe", ""], ["Liu", "Zhengzhong", ""], ["Hovy", "Eduard", ""]]}, {"id": "1603.04733", "submitter": "Christos Louizos", "authors": "Christos Louizos and Max Welling", "title": "Structured and Efficient Variational Deep Learning with Matrix Gaussian\n  Posteriors", "comments": "Updated results with the original folds in the regression\n  experiments. Appearing in the International Conference on Machine Learning\n  (ICML) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a variational Bayesian neural network where the parameters are\ngoverned via a probability distribution on random matrices. Specifically, we\nemploy a matrix variate Gaussian \\cite{gupta1999matrix} parameter posterior\ndistribution where we explicitly model the covariance among the input and\noutput dimensions of each layer. Furthermore, with approximate covariance\nmatrices we can achieve a more efficient way to represent those correlations\nthat is also cheaper than fully factorized parameter posteriors. We further\nshow that with the \"local reprarametrization trick\"\n\\cite{kingma2015variational} on this posterior distribution we arrive at a\nGaussian Process \\cite{rasmussen2006gaussian} interpretation of the hidden\nunits in each layer and we, similarly with \\cite{gal2015dropout}, provide\nconnections with deep Gaussian processes. We continue in taking advantage of\nthis duality and incorporate \"pseudo-data\" \\cite{snelson2005sparse} in our\nmodel, which in turn allows for more efficient sampling while maintaining the\nproperties of the original model. The validity of the proposed approach is\nverified through extensive experiments.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 16:01:14 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 10:21:07 GMT"}, {"version": "v3", "created": "Mon, 4 Apr 2016 11:29:16 GMT"}, {"version": "v4", "created": "Sun, 29 May 2016 07:18:12 GMT"}, {"version": "v5", "created": "Thu, 23 Jun 2016 19:03:47 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Louizos", "Christos", ""], ["Welling", "Max", ""]]}, {"id": "1603.04779", "submitter": "Yanghao Li", "authors": "Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, Xiaodi Hou", "title": "Revisiting Batch Normalization For Practical Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) have shown unprecedented success in various\ncomputer vision applications such as image classification and object detection.\nHowever, it is still a common annoyance during the training phase, that one has\nto prepare at least thousands of labeled images to fine-tune a network to a\nspecific domain. Recent study (Tommasi et al. 2015) shows that a DNN has strong\ndependency towards the training dataset, and the learned features cannot be\neasily transferred to a different but relevant task without fine-tuning. In\nthis paper, we propose a simple yet powerful remedy, called Adaptive Batch\nNormalization (AdaBN) to increase the generalization ability of a DNN. By\nmodulating the statistics in all Batch Normalization layers across the network,\nour approach achieves deep adaptation effect for domain adaptation tasks. In\ncontrary to other deep learning domain adaptation methods, our method does not\nrequire additional components, and is parameter-free. It archives\nstate-of-the-art performance despite its surprising simplicity. Furthermore, we\ndemonstrate that our method is complementary with other existing methods.\nCombining AdaBN with existing domain adaptation treatments may further improve\nmodel performance.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 17:44:32 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 03:57:19 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2016 08:41:43 GMT"}, {"version": "v4", "created": "Tue, 8 Nov 2016 06:11:30 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Li", "Yanghao", ""], ["Wang", "Naiyan", ""], ["Shi", "Jianping", ""], ["Liu", "Jiaying", ""], ["Hou", "Xiaodi", ""]]}, {"id": "1603.04833", "submitter": "Anirban Santara", "authors": "Debapriya Maji, Anirban Santara, Pabitra Mitra and Debdoot Sheet", "title": "Ensemble of Deep Convolutional Neural Networks for Learning to Detect\n  Retinal Vessels in Fundus Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision impairment due to pathological damage of the retina can largely be\nprevented through periodic screening using fundus color imaging. However the\nchallenge with large scale screening is the inability to exhaustively detect\nfine blood vessels crucial to disease diagnosis. In this work we present a\ncomputational imaging framework using deep and ensemble learning for reliable\ndetection of blood vessels in fundus color images. An ensemble of deep\nconvolutional neural networks is trained to segment vessel and non-vessel areas\nof a color fundus image. During inference, the responses of the individual\nConvNets of the ensemble are averaged to form the final segmentation. In\nexperimental evaluation with the DRIVE database, we achieve the objective of\nvessel detection with maximum average accuracy of 94.7\\% and area under ROC\ncurve of 0.9283.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 19:40:34 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Maji", "Debapriya", ""], ["Santara", "Anirban", ""], ["Mitra", "Pabitra", ""], ["Sheet", "Debdoot", ""]]}, {"id": "1603.04882", "submitter": "Qiang Wu", "authors": "Qiang Wu", "title": "Bias Correction for Regularized Regression and its Application in\n  Learning with Streaming Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to reduce the bias of ridge regression and\nregularization kernel network. When applied to a single data set the new\nalgorithms have comparable learning performance with the original ones. When\napplied to incremental learning with block wise streaming data the new\nalgorithms are more efficient due to bias reduction. Both theoretical\ncharacterizations and simulation studies are used to verify the effectiveness\nof these new algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 20:46:46 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Wu", "Qiang", ""]]}, {"id": "1603.04904", "submitter": "Roderich Gross", "authors": "Wei Li, Melvin Gauci and Roderich Gross", "title": "Turing learning: a metric-free approach to inferring behavior and its\n  application to swarms", "comments": "camera-ready version", "journal-ref": "Swarm Intelligence, 10(3):211-243, 2016", "doi": "10.1007/s11721-016-0126-1", "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Turing Learning, a novel system identification method for\ninferring the behavior of natural or artificial systems. Turing Learning\nsimultaneously optimizes two populations of computer programs, one representing\nmodels of the behavior of the system under investigation, and the other\nrepresenting classifiers. By observing the behavior of the system as well as\nthe behaviors produced by the models, two sets of data samples are obtained.\nThe classifiers are rewarded for discriminating between these two sets, that\nis, for correctly categorizing data samples as either genuine or counterfeit.\nConversely, the models are rewarded for 'tricking' the classifiers into\ncategorizing their data samples as genuine. Unlike other methods for system\nidentification, Turing Learning does not require predefined metrics to quantify\nthe difference between the system and its models. We present two case studies\nwith swarms of simulated robots and prove that the underlying behaviors cannot\nbe inferred by a metric-based system identification method. By contrast, Turing\nLearning infers the behaviors with high accuracy. It also produces a useful\nby-product - the classifiers - that can be used to detect abnormal behavior in\nthe swarm. Moreover, we show that Turing Learning also successfully infers the\nbehavior of physical robot swarms. The results show that collective behaviors\ncan be directly inferred from motion trajectories of individuals in the swarm,\nwhich may have significant implications for the study of animal collectives.\nFurthermore, Turing Learning could prove useful whenever a behavior is not\neasily characterizable using metrics, making it suitable for a wide range of\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 22:20:52 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 07:37:00 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Li", "Wei", ""], ["Gauci", "Melvin", ""], ["Gross", "Roderich", ""]]}, {"id": "1603.04918", "submitter": "Shahzad Bhatti", "authors": "Shahzad Bhatti, Carolyn Beck, Angelia Nedic", "title": "Data Clustering and Graph Partitioning via Simulated Mixing", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering approaches have led to well-accepted algorithms for\nfinding accurate clusters in a given dataset. However, their application to\nlarge-scale datasets has been hindered by computational complexity of\neigenvalue decompositions. Several algorithms have been proposed in the recent\npast to accelerate spectral clustering, however they compromise on the accuracy\nof the spectral clustering to achieve faster speed. In this paper, we propose a\nnovel spectral clustering algorithm based on a mixing process on a graph.\nUnlike the existing spectral clustering algorithms, our algorithm does not\nrequire computing eigenvectors. Specifically, it finds the equivalent of a\nlinear combination of eigenvectors of the normalized similarity matrix weighted\nwith corresponding eigenvalues. This linear combination is then used to\npartition the dataset into meaningful clusters. Simulations on real datasets\nshow that partitioning datasets based on such linear combinations of\neigenvectors achieves better accuracy than standard spectral clustering methods\nas the number of clusters increase. Our algorithm can easily be implemented in\na distributed setting.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 23:06:19 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Bhatti", "Shahzad", ""], ["Beck", "Carolyn", ""], ["Nedic", "Angelia", ""]]}, {"id": "1603.04930", "submitter": "Michael Iliadis", "authors": "Michael Iliadis, Leonidas Spinoulas, Aggelos K. Katsaggelos", "title": "Deep Fully-Connected Networks for Video Compressive Sensing", "comments": "14 pages, to appear in Elsevier Digital Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a deep learning framework for video compressive\nsensing. The proposed formulation enables recovery of video frames in a few\nseconds at significantly improved reconstruction quality compared to previous\napproaches. Our investigation starts by learning a linear mapping between video\nsequences and corresponding measured frames which turns out to provide\npromising results. We then extend the linear formulation to deep\nfully-connected networks and explore the performance gains using deeper\narchitectures. Our analysis is always driven by the applicability of the\nproposed framework on existing compressive video architectures. Extensive\nsimulations on several video sequences document the superiority of our approach\nboth quantitatively and qualitatively. Finally, our analysis offers insights\ninto understanding how dataset sizes and number of layers affect reconstruction\nperformance while raising a few points for future investigation.\n  Code is available at Github: https://github.com/miliadis/DeepVideoCS\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 01:15:35 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 23:26:43 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Iliadis", "Michael", ""], ["Spinoulas", "Leonidas", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "1603.04947", "submitter": "Zhen Hu", "authors": "Zhen Hu, Zhuyin Xue", "title": "On the Complexity of One-class SVM for Multiple Instance Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In traditional multiple instance learning (MIL), both positive and negative\nbags are required to learn a prediction function. However, a high human cost is\nneeded to know the label of each bag---positive or negative. Only positive bags\ncontain our focus (positive instances) while negative bags consist of noise or\nbackground (negative instances). So we do not expect to spend too much to label\nthe negative bags. Contrary to our expectation, nearly all existing MIL methods\nrequire enough negative bags besides positive ones. In this paper we propose an\nalgorithm called \"Positive Multiple Instance\" (PMI), which learns a classifier\ngiven only a set of positive bags. So the annotation of negative bags becomes\nunnecessary in our method. PMI is constructed based on the assumption that the\nunknown positive instances in positive bags be similar each other and\nconstitute one compact cluster in feature space and the negative instances\nlocate outside this cluster. The experimental results demonstrate that PMI\nachieves the performances close to or a little worse than those of the\ntraditional MIL algorithms on benchmark and real data sets. However, the number\nof training bags in PMI is reduced significantly compared with traditional MIL\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 03:30:59 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Hu", "Zhen", ""], ["Xue", "Zhuyin", ""]]}, {"id": "1603.04954", "submitter": "Aryan Mokhtari", "authors": "Aryan Mokhtari and Shahin Shahrampour and Ali Jadbabaie and Alejandro\n  Ribeiro", "title": "Online Optimization in Dynamic Environments: Improved Regret Rates for\n  Strongly Convex Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address tracking of a time-varying parameter with unknown\ndynamics. We formalize the problem as an instance of online optimization in a\ndynamic setting. Using online gradient descent, we propose a method that\nsequentially predicts the value of the parameter and in turn suffers a loss.\nThe objective is to minimize the accumulation of losses over the time horizon,\na notion that is termed dynamic regret. While existing methods focus on convex\nloss functions, we consider strongly convex functions so as to provide better\nguarantees of performance. We derive a regret bound that captures the\npath-length of the time-varying parameter, defined in terms of the distance\nbetween its consecutive values. In other words, the bound represents the\nnatural connection of tracking quality to the rate of change of the parameter.\nWe provide numerical experiments to complement our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 04:12:32 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Mokhtari", "Aryan", ""], ["Shahrampour", "Shahin", ""], ["Jadbabaie", "Ali", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1603.04981", "submitter": "Vijay Kamble", "authors": "Vijay Kamble, Patrick Loiseau, Jean Walrand", "title": "An Approximate Dynamic Programming Approach to Adversarial Online\n  Learning", "comments": "There was an error in the statement of Proposition 4.2 in the\n  previous version that is fixed in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an approximate dynamic programming (ADP) approach to compute\napproximations of the optimal strategies and of the minimal losses that can be\nguaranteed in discounted repeated games with vector-valued losses. Such games\nprominently arise in the analysis of regret in repeated decision-making in\nadversarial environments, also known as adversarial online learning. At the\ncore of our approach is a characterization of the lower Pareto frontier of the\nset of expected losses that a player can guarantee in these games as the unique\nfixed point of a set-valued dynamic programming operator. When applied to the\nproblem of regret minimization with discounted losses, our approach yields\nalgorithms that achieve markedly improved performance bounds compared to\noff-the-shelf online learning algorithms like Hedge. These results thus suggest\nthe significant potential of ADP-based approaches in adversarial online\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 07:04:24 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 03:08:00 GMT"}, {"version": "v3", "created": "Sun, 31 Dec 2017 23:42:51 GMT"}, {"version": "v4", "created": "Sun, 7 Jan 2018 23:51:48 GMT"}, {"version": "v5", "created": "Sun, 30 Sep 2018 23:51:16 GMT"}, {"version": "v6", "created": "Mon, 26 Oct 2020 16:55:34 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kamble", "Vijay", ""], ["Loiseau", "Patrick", ""], ["Walrand", "Jean", ""]]}, {"id": "1603.04989", "submitter": "Bamdev Mishra", "authors": "Bamdev Mishra and Rodolphe Sepulchre", "title": "Scaled stochastic gradient descent for low-rank matrix completion", "comments": "Accepted to IEEE CDC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper looks at a scaled variant of the stochastic gradient descent\nalgorithm for the matrix completion problem. Specifically, we propose a novel\nmatrix-scaling of the partial derivatives that acts as an efficient\npreconditioning for the standard stochastic gradient descent algorithm. This\nproposed matrix-scaling provides a trade-off between local and global second\norder information. It also resolves the issue of scale invariance that exists\nin matrix factorization models. The overall computational complexity is linear\nwith the number of known entries, thereby extending to a large-scale setup.\nNumerical comparisons show that the proposed algorithm competes favorably with\nstate-of-the-art algorithms on various different benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 08:48:57 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 14:51:58 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Mishra", "Bamdev", ""], ["Sepulchre", "Rodolphe", ""]]}, {"id": "1603.05027", "submitter": "Kaiming He", "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun", "title": "Identity Mappings in Deep Residual Networks", "comments": "ECCV 2016 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep residual networks have emerged as a family of extremely deep\narchitectures showing compelling accuracy and nice convergence behaviors. In\nthis paper, we analyze the propagation formulations behind the residual\nbuilding blocks, which suggest that the forward and backward signals can be\ndirectly propagated from one block to any other block, when using identity\nmappings as the skip connections and after-addition activation. A series of\nablation experiments support the importance of these identity mappings. This\nmotivates us to propose a new residual unit, which makes training easier and\nimproves generalization. We report improved results using a 1001-layer ResNet\non CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.\nCode is available at: https://github.com/KaimingHe/resnet-1k-layers\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 10:53:56 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 09:40:08 GMT"}, {"version": "v3", "created": "Mon, 25 Jul 2016 15:18:32 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["He", "Kaiming", ""], ["Zhang", "Xiangyu", ""], ["Ren", "Shaoqing", ""], ["Sun", "Jian", ""]]}, {"id": "1603.05106", "submitter": "Shakir Mohamed", "authors": "Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor,\n  Daan Wierstra", "title": "One-Shot Generalization in Deep Generative Models", "comments": "8pgs, 1pg references, 1pg appendix, In Proceedings of the 33rd\n  International Conference on Machine Learning, JMLR: W&CP volume 48, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans have an impressive ability to reason about new concepts and\nexperiences from just a single example. In particular, humans have an ability\nfor one-shot generalization: an ability to encounter a new concept, understand\nits structure, and then be able to generate compelling alternative variations\nof the concept. We develop machine learning systems with this important\ncapacity by developing new deep generative models, models that combine the\nrepresentational power of deep learning with the inferential power of Bayesian\nreasoning. We develop a class of sequential generative models that are built on\nthe principles of feedback and attention. These two characteristics lead to\ngenerative models that are among the state-of-the art in density estimation and\nimage generation. We demonstrate the one-shot generalization ability of our\nmodels using three tasks: unconditional sampling, generating new exemplars of a\ngiven concept, and generating new exemplars of a family of concepts. In all\ncases our models are able to generate compelling and diverse samples---having\nseen new examples just once---providing an important class of general-purpose\nmodels for one-shot machine learning.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 14:10:00 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 12:57:19 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Rezende", "Danilo Jimenez", ""], ["Mohamed", "Shakir", ""], ["Danihelka", "Ivo", ""], ["Gregor", "Karol", ""], ["Wierstra", "Daan", ""]]}, {"id": "1603.05145", "submitter": "Qiyang Zhao", "authors": "Qiyang Zhao, Lewis D Griffin", "title": "Suppressing the Unusual: towards Robust CNNs using Symmetric Activation\n  Functions", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many deep Convolutional Neural Networks (CNN) make incorrect predictions on\nadversarial samples obtained by imperceptible perturbations of clean samples.\nWe hypothesize that this is caused by a failure to suppress unusual signals\nwithin network layers. As remedy we propose the use of Symmetric Activation\nFunctions (SAF) in non-linear signal transducer units. These units suppress\nsignals of exceptional magnitude. We prove that SAF networks can perform\nclassification tasks to arbitrary precision in a simplified situation. In\npractice, rather than use SAFs alone, we add them into CNNs to improve their\nrobustness. The modified CNNs can be easily trained using popular strategies\nwith the moderate training load. Our experiments on MNIST and CIFAR-10 show\nthat the modified CNNs perform similarly to plain ones on clean samples, and\nare remarkably more robust against adversarial and nonsense samples.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 15:35:07 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Zhao", "Qiyang", ""], ["Griffin", "Lewis D", ""]]}, {"id": "1603.05152", "submitter": "Kleanthis Malialis", "authors": "Kleanthis Malialis and Jun Wang and Gary Brooks and George Frangou", "title": "Feature Selection as a Multiagent Coordination Problem", "comments": "AAMAS-16 Workshop on Adaptive and Learning Agents (ALA-16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Datasets with hundreds to tens of thousands features is the new norm. Feature\nselection constitutes a central problem in machine learning, where the aim is\nto derive a representative set of features from which to construct a\nclassification (or prediction) model for a specific task. Our experimental\nstudy involves microarray gene expression datasets, these are high-dimensional\nand noisy datasets that contain genetic data typically used for distinguishing\nbetween benign or malicious tissues or classifying different types of cancer.\nIn this paper, we formulate feature selection as a multiagent coordination\nproblem and propose a novel feature selection method using multiagent\nreinforcement learning. The central idea of the proposed approach is to\n\"assign\" a reinforcement learning agent to each feature where each agent learns\nto control a single feature, we refer to this approach as MARL. Applying this\nto microarray datasets creates an enormous multiagent coordination problem\nbetween thousands of learning agents. To address the scalability challenge we\napply a form of reward shaping called CLEAN rewards. We compare in total nine\nfeature selection methods, including state-of-the-art methods, and show that\nthe proposed method using CLEAN rewards can significantly scale-up, thus\noutperforming the rest of learning-based methods. We further show that a hybrid\nvariant of MARL achieves the best overall performance.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 15:49:37 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Malialis", "Kleanthis", ""], ["Wang", "Jun", ""], ["Brooks", "Gary", ""], ["Frangou", "George", ""]]}, {"id": "1603.05191", "submitter": "Martin Tak\\'a\\v{c}", "authors": "Chenxin Ma and Martin Tak\\'a\\v{c}", "title": "Distributed Inexact Damped Newton Method: Data Partitioning and\n  Load-Balancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study inexact dumped Newton method implemented in a\ndistributed environment. We start with an original DiSCO algorithm\n[Communication-Efficient Distributed Optimization of Self-Concordant Empirical\nLoss, Yuchen Zhang and Lin Xiao, 2015]. We will show that this algorithm may\nnot scale well and propose an algorithmic modifications which will lead to less\ncommunications, better load-balancing and more efficient computation. We\nperform numerical experiments with an regularized empirical loss minimization\ninstance described by a 273GB dataset.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 17:50:33 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Ma", "Chenxin", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1603.05201", "submitter": "Wenling Shang", "authors": "Wenling Shang, Kihyuk Sohn, Diogo Almeida, Honglak Lee", "title": "Understanding and Improving Convolutional Neural Networks via\n  Concatenated Rectified Linear Units", "comments": "ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, convolutional neural networks (CNNs) have been used as a powerful\ntool to solve many problems of machine learning and computer vision. In this\npaper, we aim to provide insight on the property of convolutional neural\nnetworks, as well as a generic method to improve the performance of many CNN\narchitectures. Specifically, we first examine existing CNN models and observe\nan intriguing property that the filters in the lower layers form pairs (i.e.,\nfilters with opposite phase). Inspired by our observation, we propose a novel,\nsimple yet effective activation scheme called concatenated ReLU (CRelu) and\ntheoretically analyze its reconstruction property in CNNs. We integrate CRelu\ninto several state-of-the-art CNN architectures and demonstrate improvement in\ntheir recognition performance on CIFAR-10/100 and ImageNet datasets with fewer\ntrainable parameters. Our results suggest that better understanding of the\nproperties of CNNs can lead to significant performance improvement with a\nsimple modification.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 18:17:36 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 05:18:36 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Shang", "Wenling", ""], ["Sohn", "Kihyuk", ""], ["Almeida", "Diogo", ""], ["Lee", "Honglak", ""]]}, {"id": "1603.05324", "submitter": "Shiwen Zhao", "authors": "Shiwen Zhao and Barbara E. Engelhardt and Sayan Mukherjee and David B.\n  Dunson", "title": "Fast moment estimation for generalized latent Dirichlet models", "comments": "corrected a typo in figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a generalized method of moments (GMM) approach for fast parameter\nestimation in a new class of Dirichlet latent variable models with mixed data\ntypes. Parameter estimation via GMM has been demonstrated to have computational\nand statistical advantages over alternative methods, such as expectation\nmaximization, variational inference, and Markov chain Monte Carlo. The key\ncomputational advan- tage of our method (MELD) is that parameter estimation\ndoes not require instantiation of the latent variables. Moreover, a\nrepresentational advantage of the GMM approach is that the behavior of the\nmodel is agnostic to distributional assumptions of the observations. We derive\npopulation moment conditions after marginalizing out the sample-specific\nDirichlet latent variables. The moment conditions only depend on component mean\nparameters. We illustrate the utility of our approach on simulated data,\ncomparing results from MELD to alternative methods, and we show the promise of\nour approach through the application of MELD to several data sets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 00:36:39 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 18:12:35 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Zhao", "Shiwen", ""], ["Engelhardt", "Barbara E.", ""], ["Mukherjee", "Sayan", ""], ["Dunson", "David B.", ""]]}, {"id": "1603.05359", "submitter": "Shi Zong", "authors": "Shi Zong, Hao Ni, Kenny Sung, Nan Rosemary Ke, Zheng Wen, and\n  Branislav Kveton", "title": "Cascading Bandits for Large-Scale Recommendation Problems", "comments": "Accepted to UAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recommender systems recommend a list of items. The user examines the\nlist, from the first item to the last, and often chooses the first attractive\nitem and does not examine the rest. This type of user behavior can be modeled\nby the cascade model. In this work, we study cascading bandits, an online\nlearning variant of the cascade model where the goal is to recommend $K$ most\nattractive items from a large set of $L$ candidate items. We propose two\nalgorithms for solving this problem, which are based on the idea of linear\ngeneralization. The key idea in our solutions is that we learn a predictor of\nthe attraction probabilities of items from their features, as opposing to\nlearning the attraction probability of each item independently as in the\nexisting work. This results in practical learning algorithms whose regret does\nnot depend on the number of items $L$. We bound the regret of one algorithm and\ncomprehensively evaluate the other on a range of recommendation problems. The\nalgorithm performs well and outperforms all baselines.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 05:37:12 GMT"}, {"version": "v2", "created": "Thu, 30 Jun 2016 17:07:26 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Zong", "Shi", ""], ["Ni", "Hao", ""], ["Sung", "Kenny", ""], ["Ke", "Nan Rosemary", ""], ["Wen", "Zheng", ""], ["Kveton", "Branislav", ""]]}, {"id": "1603.05412", "submitter": "Mattia  Zorzi", "authors": "Diego Romeres and Mattia Zorzi and Raffaello Camoriano and Alessandro\n  Chiuso", "title": "Online semi-parametric learning for inverse dynamics modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a semi-parametric algorithm for online learning of a\nrobot inverse dynamics model. It combines the strength of the parametric and\nnon-parametric modeling. The former exploits the rigid body dynamics equa-\ntion, while the latter exploits a suitable kernel function. We provide an\nextensive comparison with other methods from the literature using real data\nfrom the iCub humanoid robot. In doing so we also compare two different\ntechniques, namely cross validation and marginal likelihood optimization, for\nestimating the hyperparameters of the kernel function.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 10:14:27 GMT"}, {"version": "v2", "created": "Sun, 9 Oct 2016 08:24:39 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Romeres", "Diego", ""], ["Zorzi", "Mattia", ""], ["Camoriano", "Raffaello", ""], ["Chiuso", "Alessandro", ""]]}, {"id": "1603.05544", "submitter": "Linnan Wang", "authors": "Linnan Wang, Yi Yang, Martin Renqiang Min, Srimat Chakradhar", "title": "Accelerating Deep Neural Network Training with Inconsistent Stochastic\n  Gradient Descent", "comments": "The patent of ISGD belongs to NEC Labs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SGD is the widely adopted method to train CNN. Conceptually it approximates\nthe population with a randomly sampled batch; then it evenly trains batches by\nconducting a gradient update on every batch in an epoch. In this paper, we\ndemonstrate Sampling Bias, Intrinsic Image Difference and Fixed Cycle Pseudo\nRandom Sampling differentiate batches in training, which then affect learning\nspeeds on them. Because of this, the unbiased treatment of batches involved in\nSGD creates improper load balancing. To address this issue, we present\nInconsistent Stochastic Gradient Descent (ISGD) to dynamically vary training\neffort according to learning statuses on batches. Specifically ISGD leverages\ntechniques in Statistical Process Control to identify a undertrained batch.\nOnce a batch is undertrained, ISGD solves a new subproblem, a chasing logic\nplus a conservative constraint, to accelerate the training on the batch while\navoid drastic parameter changes. Extensive experiments on a variety of datasets\ndemonstrate ISGD converges faster than SGD. In training AlexNet, ISGD is\n21.05\\% faster than SGD to reach 56\\% top1 accuracy under the exactly same\nexperiment setup. We also extend ISGD to work on multiGPU or heterogeneous\ndistributed system based on data parallelism, enabling the batch size to be the\nkey to scalability. Then we present the study of ISGD batch size to the\nlearning rate, parallelism, synchronization cost, system saturation and\nscalability. We conclude the optimal ISGD batch size is machine dependent.\nVarious experiments on a multiGPU system validate our claim. In particular,\nISGD trains AlexNet to 56.3% top1 and 80.1% top5 accuracy in 11.5 hours with 4\nNVIDIA TITAN X at the batch size of 1536.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 15:49:48 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 05:35:22 GMT"}, {"version": "v3", "created": "Tue, 28 Mar 2017 13:56:03 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Wang", "Linnan", ""], ["Yang", "Yi", ""], ["Min", "Martin Renqiang", ""], ["Chakradhar", "Srimat", ""]]}, {"id": "1603.05587", "submitter": "Mohammad Ghasemi Hamed", "authors": "Mohammad Ghasemi Hamed and Masoud Ebadi Kivaj", "title": "Reliable Prediction Intervals for Local Linear Regression", "comments": "40 pages,11 figures, 10 tables and 1 algorithm. arXiv admin note:\n  text overlap with arXiv:1402.5874", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces two methods for estimating reliable prediction\nintervals for local linear least-squares regressions, named Bounded Oscillation\nPrediction Intervals (BOPI). It also proposes a new measure for comparing\ninterval prediction models named Equivalent Gaussian Standard Deviation (EGSD).\nThe experimental results compare BOPI to other methods using coverage\nprobability, Mean Interval Size and the introduced EGSD measure. The results\nwere generally in favor of the BOPI on considered benchmark regression\ndatasets. It also, reports simulation studies validating the BOPI method's\nreliability.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 17:39:12 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 17:54:37 GMT"}, {"version": "v3", "created": "Wed, 30 Mar 2016 21:52:48 GMT"}, {"version": "v4", "created": "Fri, 1 Apr 2016 10:23:38 GMT"}, {"version": "v5", "created": "Tue, 12 Jul 2016 17:39:50 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Hamed", "Mohammad Ghasemi", ""], ["Kivaj", "Masoud Ebadi", ""]]}, {"id": "1603.05614", "submitter": "Qilian Yu", "authors": "Qilian Yu, Easton Li Xu, Shuguang Cui", "title": "Streaming Algorithms for News and Scientific Literature Recommendation:\n  Submodular Maximization with a d-Knapsack Constraint", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular maximization problems belong to the family of combinatorial\noptimization problems and enjoy wide applications. In this paper, we focus on\nthe problem of maximizing a monotone submodular function subject to a\n$d$-knapsack constraint, for which we propose a streaming algorithm that\nachieves a $\\left(\\frac{1}{1+2d}-\\epsilon\\right)$-approximation of the optimal\nvalue, while it only needs one single pass through the dataset without storing\nall the data in the memory. In our experiments, we extensively evaluate the\neffectiveness of our proposed algorithm via two applications: news\nrecommendation and scientific literature recommendation. It is observed that\nthe proposed streaming algorithm achieves both execution speedup and memory\nsaving by several orders of magnitude, compared with existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 19:01:12 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 16:15:56 GMT"}, {"version": "v3", "created": "Tue, 5 Jul 2016 00:43:45 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Yu", "Qilian", ""], ["Xu", "Easton Li", ""], ["Cui", "Shuguang", ""]]}, {"id": "1603.05629", "submitter": "Hanjun Dai", "authors": "Hanjun Dai, Bo Dai, Le Song", "title": "Discriminative Embeddings of Latent Variable Models for Structured Data", "comments": "ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel classifiers and regressors designed for structured data, such as\nsequences, trees and graphs, have significantly advanced a number of\ninterdisciplinary areas such as computational biology and drug design.\nTypically, kernels are designed beforehand for a data type which either exploit\nstatistics of the structures or make use of probabilistic generative models,\nand then a discriminative classifier is learned based on the kernels via convex\noptimization. However, such an elegant two-stage approach also limited kernel\nmethods from scaling up to millions of data points, and exploiting\ndiscriminative information to learn feature representations.\n  We propose, structure2vec, an effective and scalable approach for structured\ndata representation based on the idea of embedding latent variable models into\nfeature spaces, and learning such feature spaces using discriminative\ninformation. Interestingly, structure2vec extracts features by performing a\nsequence of function mappings in a way similar to graphical model inference\nprocedures, such as mean field and belief propagation. In applications\ninvolving millions of data points, we showed that structure2vec runs 2 times\nfaster, produces models which are $10,000$ times smaller, while at the same\ntime achieving the state-of-the-art predictive performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 19:29:46 GMT"}, {"version": "v2", "created": "Sat, 19 Mar 2016 19:26:55 GMT"}, {"version": "v3", "created": "Sun, 29 May 2016 19:12:05 GMT"}, {"version": "v4", "created": "Mon, 26 Sep 2016 23:52:45 GMT"}, {"version": "v5", "created": "Sat, 11 Jan 2020 03:00:02 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Dai", "Hanjun", ""], ["Dai", "Bo", ""], ["Song", "Le", ""]]}, {"id": "1603.05642", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Elad Hazan", "title": "Optimal Black-Box Reductions Between Optimization Objectives", "comments": "new applications of our optimal reductions are obtained in this\n  version 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diverse world of machine learning applications has given rise to a\nplethora of algorithms and optimization methods, finely tuned to the specific\nregression or classification task at hand. We reduce the complexity of\nalgorithm design for machine learning by reductions: we develop reductions that\ntake a method developed for one setting and apply it to the entire spectrum of\nsmoothness and strong-convexity in applications.\n  Furthermore, unlike existing results, our new reductions are OPTIMAL and more\nPRACTICAL. We show how these new reductions give rise to new and faster running\ntimes on training linear classifiers for various families of loss functions,\nand conclude with experiments showing their successes also in practice.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 19:51:59 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 05:11:42 GMT"}, {"version": "v3", "created": "Fri, 20 May 2016 17:03:15 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Hazan", "Elad", ""]]}, {"id": "1603.05643", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Elad Hazan", "title": "Variance Reduction for Faster Non-Convex Optimization", "comments": "polished writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problem in non-convex optimization of efficiently\nreaching a stationary point. In contrast to the convex case, in the long\nhistory of this basic problem, the only known theoretical results on\nfirst-order non-convex optimization remain to be full gradient descent that\nconverges in $O(1/\\varepsilon)$ iterations for smooth objectives, and\nstochastic gradient descent that converges in $O(1/\\varepsilon^2)$ iterations\nfor objectives that are sum of smooth functions.\n  We provide the first improvement in this line of research. Our result is\nbased on the variance reduction trick recently introduced to convex\noptimization, as well as a brand new analysis of variance reduction that is\nsuitable for non-convex optimization. For objectives that are sum of smooth\nfunctions, our first-order minibatch stochastic method converges with an\n$O(1/\\varepsilon)$ rate, and is faster than full gradient descent by\n$\\Omega(n^{1/3})$.\n  We demonstrate the effectiveness of our methods on empirical risk\nminimizations with non-convex loss functions and training neural nets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 19:55:12 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 02:34:00 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Hazan", "Elad", ""]]}, {"id": "1603.05673", "submitter": "Hamidreza Chinaei", "authors": "Samantha Wong and Hamidreza Chinaei and Frank Rudzicz", "title": "Predicting health inspection results from online restaurant reviews", "comments": "7 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Informatics around public health are increasingly shifting from the\nprofessional to the public spheres. In this work, we apply linguistic analytics\nto restaurant reviews, from Yelp, in order to automatically predict official\nhealth inspection reports. We consider two types of feature sets, i.e., keyword\ndetection and topic model features, and use these in several classification\nmethods. Our empirical analysis shows that these extracted features can predict\npublic health inspection reports with over 90% accuracy using simple support\nvector machines.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 20:20:32 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Wong", "Samantha", ""], ["Chinaei", "Hamidreza", ""], ["Rudzicz", "Frank", ""]]}, {"id": "1603.05691", "submitter": "Gregor Urban", "authors": "Gregor Urban, Krzysztof J. Geras, Samira Ebrahimi Kahou, Ozlem Aslan,\n  Shengjie Wang, Rich Caruana, Abdelrahman Mohamed, Matthai Philipose and Matt\n  Richardson", "title": "Do Deep Convolutional Nets Really Need to be Deep and Convolutional?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yes, they do. This paper provides the first empirical demonstration that deep\nconvolutional models really need to be both deep and convolutional, even when\ntrained with methods such as distillation that allow small or shallow models of\nhigh accuracy to be trained. Although previous research showed that shallow\nfeed-forward nets sometimes can learn the complex functions previously learned\nby deep nets while using the same number of parameters as the deep models they\nmimic, in this paper we demonstrate that the same methods cannot be used to\ntrain accurate models on CIFAR-10 unless the student models contain multiple\nlayers of convolution. Although the student models do not have to be as deep as\nthe teacher model they mimic, the students need multiple convolutional layers\nto learn functions of comparable accuracy as the deep convolutional teacher.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 21:10:38 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 02:40:37 GMT"}, {"version": "v3", "created": "Fri, 4 Nov 2016 08:24:34 GMT"}, {"version": "v4", "created": "Sat, 4 Mar 2017 00:24:45 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Urban", "Gregor", ""], ["Geras", "Krzysztof J.", ""], ["Kahou", "Samira Ebrahimi", ""], ["Aslan", "Ozlem", ""], ["Wang", "Shengjie", ""], ["Caruana", "Rich", ""], ["Mohamed", "Abdelrahman", ""], ["Philipose", "Matthai", ""], ["Richardson", "Matt", ""]]}, {"id": "1603.05800", "submitter": "Zhiyun Lu", "authors": "Zhiyun Lu, Dong Guo, Alireza Bagheri Garakani, Kuan Liu, Avner May,\n  Aurelien Bellet, Linxi Fan, Michael Collins, Brian Kingsbury, Michael\n  Picheny, Fei Sha", "title": "A Comparison between Deep Neural Nets and Kernel Acoustic Models for\n  Speech Recognition", "comments": "arXiv admin note: text overlap with arXiv:1411.4000", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study large-scale kernel methods for acoustic modeling and compare to DNNs\non performance metrics related to both acoustic modeling and recognition.\nMeasuring perplexity and frame-level classification accuracy, kernel-based\nacoustic models are as effective as their DNN counterparts. However, on\ntoken-error-rates DNN models can be significantly better. We have discovered\nthat this might be attributed to DNN's unique strength in reducing both the\nperplexity and the entropy of the predicted posterior probabilities. Motivated\nby our findings, we propose a new technique, entropy regularized perplexity,\nfor model selection. This technique can noticeably improve the recognition\nperformance of both types of models, and reduces the gap between them. While\neffective on Broadcast News, this technique could be also applicable to other\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 09:16:01 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Lu", "Zhiyun", ""], ["Guo", "Dong", ""], ["Garakani", "Alireza Bagheri", ""], ["Liu", "Kuan", ""], ["May", "Avner", ""], ["Bellet", "Aurelien", ""], ["Fan", "Linxi", ""], ["Collins", "Michael", ""], ["Kingsbury", "Brian", ""], ["Picheny", "Michael", ""], ["Sha", "Fei", ""]]}, {"id": "1603.05824", "submitter": "Lars Hertel", "authors": "Lars Hertel, Huy Phan, Alfred Mertins", "title": "Comparing Time and Frequency Domain for Audio Event Recognition Using\n  Deep Learning", "comments": "5 pages, accepted version for publication in Proceedings of the IEEE\n  International Joint Conference on Neural Networks (IJCNN), July 2016,\n  Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing acoustic events is an intricate problem for a machine and an\nemerging field of research. Deep neural networks achieve convincing results and\nare currently the state-of-the-art approach for many tasks. One advantage is\ntheir implicit feature learning, opposite to an explicit feature extraction of\nthe input signal. In this work, we analyzed whether more discriminative\nfeatures can be learned from either the time-domain or the frequency-domain\nrepresentation of the audio signal. For this purpose, we trained multiple deep\nnetworks with different architectures on the Freiburg-106 and ESC-10 datasets.\nOur results show that feature learning from the frequency domain is superior to\nthe time domain. Moreover, additionally using convolution and pooling layers,\nto explore local structures of the audio signal, significantly improves the\nrecognition performance and achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 10:38:23 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Hertel", "Lars", ""], ["Phan", "Huy", ""], ["Mertins", "Alfred", ""]]}, {"id": "1603.05850", "submitter": "Joey Tianyi Zhou Dr", "authors": "Joey Tianyi Zhou, Ivor W. Tsang, Shen-Shyang Ho and Klaus-Robert\n  Muller", "title": "N-ary Error Correcting Coding Scheme", "comments": "Under submission to IEEE Transaction on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coding matrix design plays a fundamental role in the prediction\nperformance of the error correcting output codes (ECOC)-based multi-class task.\n{In many-class classification problems, e.g., fine-grained categorization, it\nis difficult to distinguish subtle between-class differences under existing\ncoding schemes due to a limited choices of coding values.} In this paper, we\ninvestigate whether one can relax existing binary and ternary code design to\n$N$-ary code design to achieve better classification performance. {In\nparticular, we present a novel $N$-ary coding scheme that decomposes the\noriginal multi-class problem into simpler multi-class subproblems, which is\nsimilar to applying a divide-and-conquer method.} The two main advantages of\nsuch a coding scheme are as follows: (i) the ability to construct more\ndiscriminative codes and (ii) the flexibility for the user to select the best\n$N$ for ECOC-based classification. We show empirically that the optimal $N$\n(based on classification performance) lies in $[3, 10]$ with some trade-off in\ncomputational cost. Moreover, we provide theoretical insights on the dependency\nof the generalization error bound of an $N$-ary ECOC on the average base\nclassifier generalization error and the minimum distance between any two codes\nconstructed. Extensive experimental results on benchmark multi-class datasets\nshow that the proposed coding scheme achieves superior prediction performance\nover the state-of-the-art coding methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 11:51:09 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Zhou", "Joey Tianyi", ""], ["Tsang", "Ivor W.", ""], ["Ho", "Shen-Shyang", ""], ["Muller", "Klaus-Robert", ""]]}, {"id": "1603.05933", "submitter": "Andreas Hock", "authors": "Andreas Hock, Angela P. Schoellig", "title": "Distributed Iterative Learning Control for a Team of Quadrotors", "comments": "To be presented at CDC 2016! Video can be found at\n  https://www.youtube.com/watch?v=Qw598DRw6-Q", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to enable a team of quadrotors to learn how to\naccurately track a desired trajectory while holding a given formation. We solve\nthis problem in a distributed manner, where each vehicle has only access to the\ninformation of its neighbors. The desired trajectory is only available to one\n(or few) vehicles. We present a distributed iterative learning control (ILC)\napproach where each vehicle learns from the experience of its own and its\nneighbors' previous task repetitions, and adapts its feedforward input to\nimprove performance. Existing algorithms are extended in theory to make them\nmore applicable to real-world experiments. In particular, we prove stability\nfor any causal learning function with gains chosen according to a simple scalar\ncondition. Previous proofs were restricted to a specific learning function that\nonly depends on the tracking error derivative (D-type ILC). Our extension\nprovides more degrees of freedom in the ILC design and, as a result, better\nperformance can be achieved. We also show that stability is not affected by a\nlinear dynamic coupling between neighbors. This allows us to use an additional\nconsensus feedback controller to compensate for non-repetitive disturbances.\nExperiments with two quadrotors attest the effectiveness of the proposed\ndistributed multi-agent ILC approach. This is the first work to show\ndistributed ILC in experiment.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 17:35:09 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 12:15:38 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Hock", "Andreas", ""], ["Schoellig", "Angela P.", ""]]}, {"id": "1603.05953", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu", "title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods", "comments": "Version 6. Appeared in Symposium on Theory of Computing (STOC 2017)\n  and Journal of Machine Learning Research (JMLR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nesterov's momentum trick is famously known for accelerating gradient\ndescent, and has been proven useful in building fast iterative algorithms.\nHowever, in the stochastic setting, counterexamples exist and prevent\nNesterov's momentum from providing similar acceleration, even if the underlying\nproblem is convex and finite-sum.\n  We introduce $\\mathtt{Katyusha}$, a direct, primal-only stochastic gradient\nmethod to fix this issue. In convex finite-sum stochastic optimization,\n$\\mathtt{Katyusha}$ has an optimal accelerated convergence rate, and enjoys an\noptimal parallel linear speedup in the mini-batch setting.\n  The main ingredient is $\\textit{Katyusha momentum}$, a novel \"negative\nmomentum\" on top of Nesterov's momentum. It can be incorporated into a\nvariance-reduction based algorithm and speed it up, both in terms of\n$\\textit{sequential and parallel}$ performance. Since variance reduction has\nbeen successfully applied to a growing list of practical problems, our paper\nsuggests that in each of such cases, one could potentially try to give Katyusha\na hug.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 18:46:05 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 18:13:51 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 02:12:23 GMT"}, {"version": "v4", "created": "Sun, 21 Aug 2016 21:49:29 GMT"}, {"version": "v5", "created": "Tue, 2 May 2017 06:05:45 GMT"}, {"version": "v6", "created": "Mon, 24 Sep 2018 04:49:31 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""]]}, {"id": "1603.05962", "submitter": "Stanislas Lauly", "authors": "Stanislas Lauly, Yin Zheng, Alexandre Allauzen, Hugo Larochelle", "title": "Document Neural Autoregressive Distribution Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach based on feed-forward neural networks for learning the\ndistribution of textual documents. This approach is inspired by the Neural\nAutoregressive Distribution Estimator(NADE) model, which has been shown to be a\ngood estimator of the distribution of discrete-valued igh-dimensional vectors.\nIn this paper, we present how NADE can successfully be adapted to the case of\ntextual data, retaining from NADE the property that sampling or computing the\nprobability of observations can be done exactly and efficiently. The approach\ncan also be used to learn deep representations of documents that are\ncompetitive to those learned by the alternative topic modeling approaches.\nFinally, we describe how the approach can be combined with a regular neural\nnetwork N-gram model and substantially improve its performance, by making its\nlearned representation sensitive to the larger, document-specific context.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 19:24:44 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Lauly", "Stanislas", ""], ["Zheng", "Yin", ""], ["Allauzen", "Alexandre", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1603.06035", "submitter": "Shihua Zhang", "authors": "Wenwen Min, Juan Liu, Shihua Zhang", "title": "L0-norm Sparse Graph-regularized SVD for Biclustering", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning the \"blocking\" structure is a central challenge for high dimensional\ndata (e.g., gene expression data). Recently, a sparse singular value\ndecomposition (SVD) has been used as a biclustering tool to achieve this goal.\nHowever, this model ignores the structural information between variables (e.g.,\ngene interaction graph). Although typical graph-regularized norm can\nincorporate such prior graph information to get accurate discovery and better\ninterpretability, it fails to consider the opposite effect of variables with\ndifferent signs. Motivated by the development of sparse coding and\ngraph-regularized norm, we propose a novel sparse graph-regularized SVD as a\npowerful biclustering tool for analyzing high-dimensional data. The key of this\nmethod is to impose two penalties including a novel graph-regularized norm\n($|\\pmb{u}|\\pmb{L}|\\pmb{u}|$) and $L_0$-norm ($\\|\\pmb{u}\\|_0$) on singular\nvectors to induce structural sparsity and enhance interpretability. We design\nan efficient Alternating Iterative Sparse Projection (AISP) algorithm to solve\nit. Finally, we apply our method and related ones to simulated and real data to\nshow its efficiency in capturing natural blocking structures.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 02:53:48 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Min", "Wenwen", ""], ["Liu", "Juan", ""], ["Zhang", "Shihua", ""]]}, {"id": "1603.06038", "submitter": "Evgeny Frolov", "authors": "Evgeny Frolov and Ivan Oseledets", "title": "Tensor Methods and Recommender Systems", "comments": "Submitted to WIREs Data Mining and Knowledge Discovery. 41 page, 3\n  figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A substantial progress in development of new and efficient tensor\nfactorization techniques has led to an extensive research of their\napplicability in recommender systems field. Tensor-based recommender models\npush the boundaries of traditional collaborative filtering techniques by taking\ninto account a multifaceted nature of real environments, which allows to\nproduce more accurate, situational (e.g. context-aware, criteria-driven)\nrecommendations. Despite the promising results, tensor-based methods are poorly\ncovered in existing recommender systems surveys. This survey aims to complement\nprevious works and provide a comprehensive overview on the subject. To the best\nof our knowledge, this is the first attempt to consolidate studies from various\napplication domains in an easily readable, digestible format, which helps to\nget a notion of the current state of the field. We also provide a high level\ndiscussion of the future perspectives and directions for further improvement of\ntensor-based recommendation systems.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 03:38:47 GMT"}, {"version": "v2", "created": "Sun, 18 Feb 2018 14:44:44 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Frolov", "Evgeny", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1603.06042", "submitter": "Daniel Andor", "authors": "Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro\n  Presta, Kuzman Ganchev, Slav Petrov and Michael Collins", "title": "Globally Normalized Transition-Based Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a globally normalized transition-based neural network model that\nachieves state-of-the-art part-of-speech tagging, dependency parsing and\nsentence compression results. Our model is a simple feed-forward neural network\nthat operates on a task-specific transition system, yet achieves comparable or\nbetter accuracies than recurrent models. We discuss the importance of global as\nopposed to local normalization: a key insight is that the label bias problem\nimplies that globally normalized models can be strictly more expressive than\nlocally normalized models.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 03:56:03 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 13:43:30 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Andor", "Daniel", ""], ["Alberti", "Chris", ""], ["Weiss", "David", ""], ["Severyn", "Aliaksei", ""], ["Presta", "Alessandro", ""], ["Ganchev", "Kuzman", ""], ["Petrov", "Slav", ""], ["Collins", "Michael", ""]]}, {"id": "1603.06052", "submitter": "Chengtao Li", "authors": "Chengtao Li, Stefanie Jegelka and Suvrit Sra", "title": "Fast DPP Sampling for Nystr\\\"om with Application to Kernel Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nystr\\\"om method has long been popular for scaling up kernel methods. Its\ntheoretical guarantees and empirical performance rely critically on the quality\nof the landmarks selected. We study landmark selection for Nystr\\\"om using\nDeterminantal Point Processes (DPPs), discrete probability models that allow\ntractable generation of diverse samples. We prove that landmarks selected via\nDPPs guarantee bounds on approximation errors; subsequently, we analyze\nimplications for kernel ridge regression. Contrary to prior reservations due to\ncubic complexity of DPPsampling, we show that (under certain conditions) Markov\nchain DPP sampling requires only linear time in the size of the data. We\npresent several empirical results that support our theoretical analysis, and\ndemonstrate the superior performance of DPP-based landmark selection compared\nwith existing approaches.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 06:14:28 GMT"}, {"version": "v2", "created": "Sat, 28 May 2016 04:04:55 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Li", "Chengtao", ""], ["Jegelka", "Stefanie", ""], ["Sra", "Suvrit", ""]]}, {"id": "1603.06060", "submitter": "Abhijit Guha Roy", "authors": "Abhijit Guha Roy and Debdoot Sheet", "title": "DASA: Domain Adaptation in Stacked Autoencoders using Systematic Dropout", "comments": "Accepted at Asian Conference on Pattern Recognition 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation deals with adapting behaviour of machine learning based\nsystems trained using samples in source domain to their deployment in target\ndomain where the statistics of samples in both domains are dissimilar. The task\nof directly training or adapting a learner in the target domain is challenged\nby lack of abundant labeled samples. In this paper we propose a technique for\ndomain adaptation in stacked autoencoder (SAE) based deep neural networks (DNN)\nperformed in two stages: (i) unsupervised weight adaptation using systematic\ndropouts in mini-batch training, (ii) supervised fine-tuning with limited\nnumber of labeled samples in target domain. We experimentally evaluate\nperformance in the problem of retinal vessel segmentation where the SAE-DNN is\ntrained using large number of labeled samples in the source domain (DRIVE\ndataset) and adapted using less number of labeled samples in target domain\n(STARE dataset). The performance of SAE-DNN measured using $logloss$ in source\ndomain is $0.19$, without and with adaptation are $0.40$ and $0.18$, and $0.39$\nwhen trained exclusively with limited samples in target domain. The area under\nROC curve is observed respectively as $0.90$, $0.86$, $0.92$ and $0.87$. The\nhigh efficiency of vessel segmentation with DASA strongly substantiates our\nclaim.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 07:27:56 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Roy", "Abhijit Guha", ""], ["Sheet", "Debdoot", ""]]}, {"id": "1603.06078", "submitter": "Oliver Nalbach", "authors": "Oliver Nalbach, Elena Arabadzhiyska, Dushyant Mehta, Hans-Peter\n  Seidel, Tobias Ritschel", "title": "Deep Shading: Convolutional Neural Networks for Screen-Space Shading", "comments": null, "journal-ref": null, "doi": "10.1111/cgf.13225", "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, convolutional neural networks (CNNs) have recently\nachieved new levels of performance for several inverse problems where RGB pixel\nappearance is mapped to attributes such as positions, normals or reflectance.\nIn computer graphics, screen-space shading has recently increased the visual\nquality in interactive image synthesis, where per-pixel attributes such as\npositions, normals or reflectance of a virtual 3D scene are converted into RGB\npixel appearance, enabling effects like ambient occlusion, indirect light,\nscattering, depth-of-field, motion blur, or anti-aliasing. In this paper we\nconsider the diagonal problem: synthesizing appearance from given per-pixel\nattributes using a CNN. The resulting Deep Shading simulates various\nscreen-space effects at competitive quality and speed while not being\nprogrammed by human experts but learned from example images.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 10:29:57 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 11:11:55 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Nalbach", "Oliver", ""], ["Arabadzhiyska", "Elena", ""], ["Mehta", "Dushyant", ""], ["Seidel", "Hans-Peter", ""], ["Ritschel", "Tobias", ""]]}, {"id": "1603.06111", "submitter": "Lili Mou", "authors": "Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, Zhi Jin", "title": "How Transferable are Neural Networks in NLP Applications?", "comments": "Accepted by EMNLP-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is aimed to make use of valuable knowledge in a source\ndomain to help model performance in a target domain. It is particularly\nimportant to neural networks, which are very likely to be overfitting. In some\nfields like image processing, many studies have shown the effectiveness of\nneural network-based transfer learning. For neural NLP, however, existing\nstudies have only casually applied transfer learning, and conclusions are\ninconsistent. In this paper, we conduct systematic case studies and provide an\nilluminating picture on the transferability of neural networks in NLP.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 16:38:31 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 07:45:31 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Mou", "Lili", ""], ["Meng", "Zhao", ""], ["Yan", "Rui", ""], ["Li", "Ge", ""], ["Xu", "Yan", ""], ["Zhang", "Lu", ""], ["Jin", "Zhi", ""]]}, {"id": "1603.06127", "submitter": "Petr Baudi\\v{s}", "authors": "Petr Baudi\\v{s}, Jan Pichl, Tom\\'a\\v{s} Vysko\\v{c}il, Jan \\v{S}ediv\\'y", "title": "Sentence Pair Scoring: Towards Unified Framework for Text Comprehension", "comments": "submitted as paper to CoNLL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We review the task of Sentence Pair Scoring, popular in the literature in\nvarious forms - viewed as Answer Sentence Selection, Semantic Text Scoring,\nNext Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a\ncomponent of Memory Networks.\n  We argue that all such tasks are similar from the model perspective and\npropose new baselines by comparing the performance of common IR metrics and\npopular convolutional, recurrent and attention-based neural models across many\nSentence Pair Scoring tasks and datasets. We discuss the problem of evaluating\nrandomized models, propose a statistically grounded methodology, and attempt to\nimprove comparisons by releasing new datasets that are much harder than some of\nthe currently used well explored benchmarks. We introduce a unified open source\nsoftware framework with easily pluggable models and tasks, which enables us to\nexperiment with multi-task reusability of trained sentence model. We set a new\nstate-of-art in performance on the Ubuntu Dialogue dataset.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 18:35:26 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 03:10:26 GMT"}, {"version": "v3", "created": "Fri, 6 May 2016 22:17:36 GMT"}, {"version": "v4", "created": "Tue, 17 May 2016 14:08:38 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Baudi\u0161", "Petr", ""], ["Pichl", "Jan", ""], ["Vysko\u010dil", "Tom\u00e1\u0161", ""], ["\u0160ediv\u00fd", "Jan", ""]]}, {"id": "1603.06129", "submitter": "Rishabh Singh", "authors": "Sahil Bhatia and Rishabh Singh", "title": "Automated Correction for Syntax Errors in Programming Assignments using\n  Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for automatically generating repair feedback for syntax\nerrors for introductory programming problems. Syntax errors constitute one of\nthe largest classes of errors (34%) in our dataset of student submissions\nobtained from a MOOC course on edX. The previous techniques for generating\nautomated feed- back on programming assignments have focused on functional\ncorrectness and style considerations of student programs. These techniques\nanalyze the program AST of the program and then perform some dynamic and\nsymbolic analyses to compute repair feedback. Unfortunately, it is not possible\nto generate ASTs for student pro- grams with syntax errors and therefore the\nprevious feedback techniques are not applicable in repairing syntax errors.\n  We present a technique for providing feedback on syntax errors that uses\nRecurrent neural networks (RNNs) to model syntactically valid token sequences.\nOur approach is inspired from the recent work on learning language models from\nBig Code (large code corpus). For a given programming assignment, we first\nlearn an RNN to model all valid token sequences using the set of syntactically\ncorrect student submissions. Then, for a student submission with syntax errors,\nwe query the learnt RNN model with the prefix to- ken sequence to predict token\nsequences that can fix the error by either replacing or inserting the predicted\ntoken sequence at the error location. We evaluate our technique on over 14, 000\nstudent submissions with syntax errors. Our technique can completely re- pair\n31.69% (4501/14203) of submissions with syntax errors and in addition partially\ncorrect 6.39% (908/14203) of the submissions.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 18:43:28 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Bhatia", "Sahil", ""], ["Singh", "Rishabh", ""]]}, {"id": "1603.06147", "submitter": "Junyoung Chung", "authors": "Junyoung Chung, Kyunghyun Cho and Yoshua Bengio", "title": "A Character-Level Decoder without Explicit Segmentation for Neural\n  Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing machine translation systems, whether phrase-based or neural,\nhave relied almost exclusively on word-level modelling with explicit\nsegmentation. In this paper, we ask a fundamental question: can neural machine\ntranslation generate a character sequence without any explicit segmentation? To\nanswer this question, we evaluate an attention-based encoder-decoder with a\nsubword-level encoder and a character-level decoder on four language\npairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15.\nOur experiments show that the models with a character-level decoder outperform\nthe ones with a subword-level decoder on all of the four language pairs.\nFurthermore, the ensembles of neural models with a character-level decoder\noutperform the state-of-the-art non-neural machine translation systems on\nEn-Cs, En-De and En-Fi and perform comparably on En-Ru.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 21:35:04 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 20:57:23 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 04:06:01 GMT"}, {"version": "v4", "created": "Tue, 21 Jun 2016 01:12:22 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Chung", "Junyoung", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1603.06159", "submitter": "Sashank J. Reddi", "authors": "Sashank J. Reddi, Suvrit Sra, Barnabas Poczos, Alex Smola", "title": "Fast Incremental Method for Nonconvex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a fast incremental aggregated gradient method for optimizing\nnonconvex problems of the form $\\min_x \\sum_i f_i(x)$. Specifically, we analyze\nthe SAGA algorithm within an Incremental First-order Oracle framework, and show\nthat it converges to a stationary point provably faster than both gradient\ndescent and stochastic gradient descent. We also discuss a Polyak's special\nclass of nonconvex problems for which SAGA converges at a linear rate to the\nglobal optimum. Finally, we analyze the practically valuable regularized and\nminibatch variants of SAGA. To our knowledge, this paper presents the first\nanalysis of fast convergence for an incremental aggregated gradient method for\nnonconvex problems.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 23:28:44 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Reddi", "Sashank J.", ""], ["Sra", "Suvrit", ""], ["Poczos", "Barnabas", ""], ["Smola", "Alex", ""]]}, {"id": "1603.06160", "submitter": "Sashank J. Reddi", "authors": "Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, Alex Smola", "title": "Stochastic Variance Reduction for Nonconvex Optimization", "comments": "Minor feedback changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonconvex finite-sum problems and analyze stochastic variance\nreduced gradient (SVRG) methods for them. SVRG and related methods have\nrecently surged into prominence for convex optimization given their edge over\nstochastic gradient descent (SGD); but their theoretical analysis almost\nexclusively assumes convexity. In contrast, we prove non-asymptotic rates of\nconvergence (to stationary points) of SVRG for nonconvex optimization, and show\nthat it is provably faster than SGD and gradient descent. We also analyze a\nsubclass of nonconvex problems on which SVRG attains linear convergence to the\nglobal optimum. We extend our analysis to mini-batch variants of SVRG, showing\n(theoretical) linear speedup due to mini-batching in parallel settings.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 23:37:38 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 23:08:20 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Reddi", "Sashank J.", ""], ["Hefny", "Ahmed", ""], ["Sra", "Suvrit", ""], ["Poczos", "Barnabas", ""], ["Smola", "Alex", ""]]}, {"id": "1603.06170", "submitter": "Zhijian Ou", "authors": "Haotian Xu, Zhijian Ou", "title": "Joint Stochastic Approximation learning of Helmholtz Machines", "comments": "Fixing typos. Published at ICLR-2016 Workshop Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though with progress, model learning and performing posterior inference still\nremains a common challenge for using deep generative models, especially for\nhandling discrete hidden variables. This paper is mainly concerned with\nalgorithms for learning Helmholz machines, which is characterized by pairing\nthe generative model with an auxiliary inference model. A common drawback of\nprevious learning algorithms is that they indirectly optimize some bounds of\nthe targeted marginal log-likelihood. In contrast, we successfully develop a\nnew class of algorithms, based on stochastic approximation (SA) theory of the\nRobbins-Monro type, to directly optimize the marginal log-likelihood and\nsimultaneously minimize the inclusive KL-divergence. The resulting learning\nalgorithm is thus called joint SA (JSA). Moreover, we construct an effective\nMCMC operator for JSA. Our results on the MNIST datasets demonstrate that the\nJSA's performance is consistently superior to that of competing algorithms like\nRWS, for learning a range of difficult models.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 00:55:06 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 13:16:25 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Xu", "Haotian", ""], ["Ou", "Zhijian", ""]]}, {"id": "1603.06212", "submitter": "Randal Olson", "authors": "Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, Jason H. Moore", "title": "Evaluation of a Tree-based Pipeline Optimization Tool for Automating\n  Data Science", "comments": "8 pages, 5 figures, preprint to appear in GECCO 2016, edits not yet\n  made from reviewer comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the field of data science continues to grow, there will be an\never-increasing demand for tools that make machine learning accessible to\nnon-experts. In this paper, we introduce the concept of tree-based pipeline\noptimization for automating one of the most tedious parts of machine\nlearning---pipeline design. We implement an open source Tree-based Pipeline\nOptimization Tool (TPOT) in Python and demonstrate its effectiveness on a\nseries of simulated and real-world benchmark data sets. In particular, we show\nthat TPOT can design machine learning pipelines that provide a significant\nimprovement over a basic machine learning analysis while requiring little to no\ninput nor prior knowledge from the user. We also address the tendency for TPOT\nto design overly complex pipelines by integrating Pareto optimization, which\nproduces compact pipelines without sacrificing classification accuracy. As\nsuch, this work represents an important step toward fully automating machine\nlearning pipeline design.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 13:32:27 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Olson", "Randal S.", ""], ["Bartley", "Nathan", ""], ["Urbanowicz", "Ryan J.", ""], ["Moore", "Jason H.", ""]]}, {"id": "1603.06220", "submitter": "Pejman Khadivi", "authors": "Pejman Khadivi, Ravi Tandon, Naren Ramakrishnan", "title": "Flow of Information in Feed-Forward Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feed-forward deep neural networks have been used extensively in various\nmachine learning applications. Developing a precise understanding of the\nunderling behavior of neural networks is crucial for their efficient\ndeployment. In this paper, we use an information theoretic approach to study\nthe flow of information in a neural network and to determine how entropy of\ninformation changes between consecutive layers. Moreover, using the Information\nBottleneck principle, we develop a constrained optimization problem that can be\nused in the training process of a deep neural network. Furthermore, we\ndetermine a lower bound for the level of data representation that can be\nachieved in a deep neural network with an acceptable level of distortion.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 14:39:27 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Khadivi", "Pejman", ""], ["Tandon", "Ravi", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1603.06265", "submitter": "Paul Christiano", "authors": "Paul Christiano", "title": "Collaborative prediction with expert advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many practical learning systems aggregate data across many users, while\nlearning theory traditionally considers a single learner who trusts all of\ntheir observations. A case in point is the foundational learning problem of\nprediction with expert advice. To date, there has been no theoretical study of\nthe general collaborative version of prediction with expert advice, in which\nmany users face a similar problem and would like to share their experiences in\norder to learn faster. A key issue in this collaborative framework is\nrobustness: generally algorithms that aggregate data are vulnerable to\nmanipulation by even a small number of dishonest users.\n  We exhibit the first robust collaborative algorithm for prediction with\nexpert advice. When all users are honest and have similar tastes our algorithm\nmatches the performance of pooling data and using a traditional algorithm. But\nour algorithm also guarantees that adding users never significantly degrades\nperformance, even if the additional users behave adversarially. We achieve\nstrong guarantees even when the overwhelming majority of users behave\nadversarially. As a special case, our algorithm is extremely robust to\nvariation amongst the users.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 20:34:32 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 20:41:12 GMT"}, {"version": "v3", "created": "Fri, 8 Apr 2016 00:36:06 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Christiano", "Paul", ""]]}, {"id": "1603.06270", "submitter": "Zhilin Yang", "authors": "Zhilin Yang and Ruslan Salakhutdinov and William Cohen", "title": "Multi-Task Cross-Lingual Sequence Tagging from Scratch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep hierarchical recurrent neural network for sequence tagging.\nGiven a sequence of words, our model employs deep gated recurrent units on both\ncharacter and word levels to encode morphology and context information, and\napplies a conditional random field layer to predict the tags. Our model is task\nindependent, language independent, and feature engineering free. We further\nextend our model to multi-task and cross-lingual joint training by sharing the\narchitecture and parameters. Our model achieves state-of-the-art results in\nmultiple languages on several benchmark tasks including POS tagging, chunking,\nand NER. We also demonstrate that multi-task and cross-lingual joint training\ncan improve the performance in various cases.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 21:15:56 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 15:07:39 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Yang", "Zhilin", ""], ["Salakhutdinov", "Ruslan", ""], ["Cohen", "William", ""]]}, {"id": "1603.06288", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Gautam Dasarathy, Junier B. Oliva, Jeff\n  Schneider, Barnabas Poczos", "title": "Multi-fidelity Gaussian Process Bandit Optimisation", "comments": "Preliminary version appeared at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scientific and engineering applications, we are tasked with the\nmaximisation of an expensive to evaluate black box function $f$. Traditional\nsettings for this problem assume just the availability of this single function.\nHowever, in many cases, cheap approximations to $f$ may be obtainable. For\nexample, the expensive real world behaviour of a robot can be approximated by a\ncheap computer simulation. We can use these approximations to eliminate low\nfunction value regions cheaply and use the expensive evaluations of $f$ in a\nsmall but promising region and speedily identify the optimum. We formalise this\ntask as a \\emph{multi-fidelity} bandit problem where the target function and\nits approximations are sampled from a Gaussian process. We develop MF-GP-UCB, a\nnovel method based on upper confidence bound techniques. In our theoretical\nanalysis we demonstrate that it exhibits precisely the above behaviour, and\nachieves better regret than strategies which ignore multi-fidelity information.\nEmpirically, MF-GP-UCB outperforms such naive strategies and other\nmulti-fidelity methods on several synthetic and real experiments.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 22:58:43 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 00:29:30 GMT"}, {"version": "v3", "created": "Sat, 4 Aug 2018 01:25:57 GMT"}, {"version": "v4", "created": "Fri, 15 Mar 2019 18:05:28 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Dasarathy", "Gautam", ""], ["Oliva", "Junier B.", ""], ["Schneider", "Jeff", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1603.06318", "submitter": "Zhiting Hu", "authors": "Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, Eric Xing", "title": "Harnessing Deep Neural Networks with Logic Rules", "comments": "Fix typos in appendix. ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining deep neural networks with structured logic rules is desirable to\nharness flexibility and reduce uninterpretability of the neural models. We\npropose a general framework capable of enhancing various types of neural\nnetworks (e.g., CNNs and RNNs) with declarative first-order logic rules.\nSpecifically, we develop an iterative distillation method that transfers the\nstructured information of logic rules into the weights of neural networks. We\ndeploy the framework on a CNN for sentiment analysis, and an RNN for named\nentity recognition. With a few highly intuitive rules, we obtain substantial\nimprovements and achieve state-of-the-art or comparable results to previous\nbest-performing systems.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 03:33:20 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 05:28:21 GMT"}, {"version": "v3", "created": "Tue, 19 Jul 2016 23:30:48 GMT"}, {"version": "v4", "created": "Tue, 15 Nov 2016 21:41:21 GMT"}, {"version": "v5", "created": "Tue, 26 Mar 2019 05:16:10 GMT"}, {"version": "v6", "created": "Sat, 8 Aug 2020 07:38:00 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Hu", "Zhiting", ""], ["Ma", "Xuezhe", ""], ["Liu", "Zhengzhong", ""], ["Hovy", "Eduard", ""], ["Xing", "Eric", ""]]}, {"id": "1603.06348", "submitter": "Abhishek Gupta", "authors": "Abhishek Gupta, Clemens Eppner, Sergey Levine, Pieter Abbeel", "title": "Learning Dexterous Manipulation for a Soft Robotic Hand from Human\n  Demonstration", "comments": "Accepted at International Conference on Intelligent Robots and\n  Systems(IROS) 2016. Pdf file updated for stylistic consistency", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dexterous multi-fingered hands can accomplish fine manipulation behaviors\nthat are infeasible with simple robotic grippers. However, sophisticated\nmulti-fingered hands are often expensive and fragile. Low-cost soft hands offer\nan appealing alternative to more conventional devices, but present considerable\nchallenges in sensing and actuation, making them difficult to apply to more\ncomplex manipulation tasks. In this paper, we describe an approach to learning\nfrom demonstration that can be used to train soft robotic hands to perform\ndexterous manipulation tasks. Our method uses object-centric demonstrations,\nwhere a human demonstrates the desired motion of manipulated objects with their\nown hands, and the robot autonomously learns to imitate these demonstrations\nusing reinforcement learning. We propose a novel algorithm that allows us to\nblend and select a subset of the most feasible demonstrations to learn to\nimitate on the hardware, which we use with an extension of the guided policy\nsearch framework to use multiple demonstrations to learn generalizable neural\nnetwork policies. We demonstrate our approach on the RBO Hand 2, with learned\nmotor skills for turning a valve, manipulating an abacus, and grasping.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 08:00:44 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 22:52:49 GMT"}, {"version": "v3", "created": "Mon, 20 Mar 2017 17:11:44 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Gupta", "Abhishek", ""], ["Eppner", "Clemens", ""], ["Levine", "Sergey", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1603.06352", "submitter": "Tomer Koren", "authors": "Elad Hazan, Tomer Koren, Roi Livni, Yishay Mansour", "title": "Online Learning with Low Rank Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of prediction with expert advice when the losses of\nthe experts have low-dimensional structure: they are restricted to an unknown\n$d$-dimensional subspace. We devise algorithms with regret bounds that are\nindependent of the number of experts and depend only on the rank $d$. For the\nstochastic model we show a tight bound of $\\Theta(\\sqrt{dT})$, and extend it to\na setting of an approximate $d$ subspace. For the adversarial model we show an\nupper bound of $O(d\\sqrt{T})$ and a lower bound of $\\Omega(\\sqrt{dT})$.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 08:29:05 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 06:47:33 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Hazan", "Elad", ""], ["Koren", "Tomer", ""], ["Livni", "Roi", ""], ["Mansour", "Yishay", ""]]}, {"id": "1603.06393", "submitter": "Jiatao Gu", "authors": "Jiatao Gu, Zhengdong Lu, Hang Li and Victor O.K. Li", "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning", "comments": "10 pages, 5 figures, accepted by ACL2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address an important problem in sequence-to-sequence (Seq2Seq) learning\nreferred to as copying, in which certain segments in the input sequence are\nselectively replicated in the output sequence. A similar phenomenon is\nobservable in human language communication. For example, humans tend to repeat\nentity names or even long phrases in conversation. The challenge with regard to\ncopying in Seq2Seq is that new machinery is needed to decide when to perform\nthe operation. In this paper, we incorporate copying into neural network-based\nSeq2Seq learning and propose a new model called CopyNet with encoder-decoder\nstructure. CopyNet can nicely integrate the regular way of word generation in\nthe decoder with the new copying mechanism which can choose sub-sequences in\nthe input sequence and put them at proper places in the output sequence. Our\nempirical study on both synthetic data sets and real world data sets\ndemonstrates the efficacy of CopyNet. For example, CopyNet can outperform\nregular RNN-based model with remarkable margins on text summarization tasks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 11:35:08 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 03:33:58 GMT"}, {"version": "v3", "created": "Wed, 8 Jun 2016 13:53:21 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Gu", "Jiatao", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1603.06430", "submitter": "Seonwoo Min", "authors": "Seonwoo Min, Byunghan Lee, Sungroh Yoon", "title": "Deep Learning in Bioinformatics", "comments": "Accepted for Briefings in Bioinformatics (18-Jun-2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of big data, transformation of biomedical big data into valuable\nknowledge has been one of the most important challenges in bioinformatics. Deep\nlearning has advanced rapidly since the early 2000s and now demonstrates\nstate-of-the-art performance in various fields. Accordingly, application of\ndeep learning in bioinformatics to gain insight from data has been emphasized\nin both academia and industry. Here, we review deep learning in bioinformatics,\npresenting examples of current research. To provide a useful and comprehensive\nperspective, we categorize research both by the bioinformatics domain (i.e.,\nomics, biomedical imaging, biomedical signal processing) and deep learning\narchitecture (i.e., deep neural networks, convolutional neural networks,\nrecurrent neural networks, emergent architectures) and present brief\ndescriptions of each study. Additionally, we discuss theoretical and practical\nissues of deep learning in bioinformatics and suggest future research\ndirections. We believe that this review will provide valuable insights and\nserve as a starting point for researchers to apply deep learning approaches in\ntheir bioinformatics studies.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 13:55:02 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 03:01:37 GMT"}, {"version": "v3", "created": "Sat, 26 Mar 2016 05:14:12 GMT"}, {"version": "v4", "created": "Mon, 4 Apr 2016 09:48:55 GMT"}, {"version": "v5", "created": "Sun, 19 Jun 2016 09:16:30 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Min", "Seonwoo", ""], ["Lee", "Byunghan", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1603.06478", "submitter": "Kathrin Bujna", "authors": "Johannes Bl\\\"omer, Sascha Brauer, Kathrin Bujna", "title": "Hard-Clustering with Gaussian Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training the parameters of statistical models to describe a given data set is\na central task in the field of data mining and machine learning. A very popular\nand powerful way of parameter estimation is the method of maximum likelihood\nestimation (MLE). Among the most widely used families of statistical models are\nmixture models, especially, mixtures of Gaussian distributions. A popular\nhard-clustering variant of the MLE problem is the so-called complete-data\nmaximum likelihood estimation (CMLE) method. The standard approach to solve the\nCMLE problem is the Classification-Expectation-Maximization (CEM) algorithm.\nUnfortunately, it is only guaranteed that the algorithm converges to some\n(possibly arbitrarily poor) stationary point of the objective function.\n  In this paper, we present two algorithms for a restricted version of the CMLE\nproblem. That is, our algorithms approximate reasonable solutions to the CMLE\nproblem which satisfy certain natural properties. Moreover, they compute\nsolutions whose cost (i.e. complete-data log-likelihood values) are at most a\nfactor $(1+\\epsilon)$ worse than the cost of the solutions that we search for.\nNote the CMLE problem in its most general, i.e. unrestricted, form is not well\ndefined and allows for trivial optimal solutions that can be thought of as\ndegenerated solutions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 16:02:27 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Bl\u00f6mer", "Johannes", ""], ["Brauer", "Sascha", ""], ["Bujna", "Kathrin", ""]]}, {"id": "1603.06531", "submitter": "Otkrist Gupta", "authors": "Otkrist Gupta, Dan Raviv, Ramesh Raskar", "title": "Deep video gesture recognition using illumination invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present architectures based on deep neural nets for gesture\nrecognition in videos, which are invariant to local scaling. We amalgamate\nautoencoder and predictor architectures using an adaptive weighting scheme\ncoping with a reduced size labeled dataset, while enriching our models from\nenormous unlabeled sets. We further improve robustness to lighting conditions\nby introducing a new adaptive filer based on temporal local scale\nnormalization. We provide superior results over known methods, including recent\nreported approaches based on neural nets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 18:33:29 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Gupta", "Otkrist", ""], ["Raviv", "Dan", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1603.06541", "submitter": "Ping Li", "authors": "Ping Li", "title": "A Comparison Study of Nonlinear Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we compare 5 different nonlinear kernels: min-max, RBF, fRBF\n(folded RBF), acos, and acos-$\\chi^2$, on a wide range of publicly available\ndatasets. The proposed fRBF kernel performs very similarly to the RBF kernel.\nBoth RBF and fRBF kernels require an important tuning parameter ($\\gamma$).\nInterestingly, for a significant portion of the datasets, the min-max kernel\noutperforms the best-tuned RBF/fRBF kernels. The acos kernel and acos-$\\chi^2$\nkernel also perform well in general and in some datasets achieve the best\naccuracies.\n  One crucial issue with the use of nonlinear kernels is the excessive\ncomputational and memory cost. These days, one increasingly popular strategy is\nto linearize the kernels through various randomization algorithms. In our\nstudy, the randomization method for the min-max kernel demonstrates excellent\nperformance compared to the randomization methods for other types of nonlinear\nkernels, measured in terms of the number of nonzero terms in the transformed\ndataset.\n  Our study provides evidence for supporting the use of the min-max kernel and\nthe corresponding randomized linearization method (i.e., the so-called \"0-bit\nCWS\"). Furthermore, the results motivate at least two directions for future\nresearch: (i) To develop new (and linearizable) nonlinear kernels for better\naccuracies; and (ii) To develop better linearization algorithms for improving\nthe current linearization methods for the RBF kernel, the acos kernel, and the\nacos-$\\chi^2$ kernel. One attempt is to combine the min-max kernel with the\nacos kernel or the acos-$\\chi^2$ kernel. The advantages of these two new and\ntuning-free nonlinear kernels are demonstrated vias our extensive experiments.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 19:11:50 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1603.06554", "submitter": "Mohamed Amer", "authors": "Timothy J. Shields, Mohamed R. Amer, Max Ehrlich, Amir Tamrakar", "title": "Action-Affect Classification and Morphing using Multi-Task\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Most recent work focused on affect from facial expressions, and not as much\non body. This work focuses on body affect analysis. Affect does not occur in\nisolation. Humans usually couple affect with an action in natural interactions;\nfor example, a person could be talking and smiling. Recognizing body affect in\nsequences requires efficient algorithms to capture both the micro movements\nthat differentiate between happy and sad and the macro variations between\ndifferent actions. We depart from traditional approaches for time-series data\nanalytics by proposing a multi-task learning model that learns a shared\nrepresentation that is well-suited for action-affect classification as well as\ngeneration. For this paper we choose Conditional Restricted Boltzmann Machines\nto be our building block. We propose a new model that enhances the CRBM model\nwith a factored multi-task component to become Multi-Task Conditional\nRestricted Boltzmann Machines (MTCRBMs). We evaluate our approach on two\npublicly available datasets, the Body Affect dataset and the Tower Game\ndataset, and show superior classification performance improvement over the\nstate-of-the-art, as well as the generative abilities of our model.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 19:38:07 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Shields", "Timothy J.", ""], ["Amer", "Mohamed R.", ""], ["Ehrlich", "Max", ""], ["Tamrakar", "Amir", ""]]}, {"id": "1603.06560", "submitter": "Lisha Li", "authors": "Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, Ameet\n  Talwalkar", "title": "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization", "comments": "Changes: - Updated to JMLR version", "journal-ref": "Journal of Machine Learning Research 18 (2018) 1-52", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance of machine learning algorithms depends critically on identifying\na good set of hyperparameters. While recent approaches use Bayesian\noptimization to adaptively select configurations, we focus on speeding up\nrandom search through adaptive resource allocation and early-stopping. We\nformulate hyperparameter optimization as a pure-exploration non-stochastic\ninfinite-armed bandit problem where a predefined resource like iterations, data\nsamples, or features is allocated to randomly sampled configurations. We\nintroduce a novel algorithm, Hyperband, for this framework and analyze its\ntheoretical properties, providing several desirable guarantees. Furthermore, we\ncompare Hyperband with popular Bayesian optimization methods on a suite of\nhyperparameter optimization problems. We observe that Hyperband can provide\nover an order-of-magnitude speedup over our competitor set on a variety of\ndeep-learning and kernel-based learning problems.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 19:51:04 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 19:04:57 GMT"}, {"version": "v3", "created": "Wed, 23 Nov 2016 19:39:54 GMT"}, {"version": "v4", "created": "Mon, 18 Jun 2018 23:01:43 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Li", "Lisha", ""], ["Jamieson", "Kevin", ""], ["DeSalvo", "Giulia", ""], ["Rostamizadeh", "Afshin", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "1603.06571", "submitter": "Oren Barkan", "authors": "Oren Barkan", "title": "Bayesian Neural Word Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several works in the domain of natural language processing\npresented successful methods for word embedding. Among them, the Skip-Gram with\nnegative sampling, known also as word2vec, advanced the state-of-the-art of\nvarious linguistics tasks. In this paper, we propose a scalable Bayesian neural\nword embedding algorithm. The algorithm relies on a Variational Bayes solution\nfor the Skip-Gram objective and a detailed step by step description is\nprovided. We present experimental results that demonstrate the performance of\nthe proposed algorithm for word analogy and similarity tasks on six different\ndatasets and show it is competitive with the original Skip-Gram method.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 16:32:06 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2016 16:49:11 GMT"}, {"version": "v3", "created": "Mon, 20 Feb 2017 20:45:33 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Barkan", "Oren", ""]]}, {"id": "1603.06624", "submitter": "R Devon Hjelm", "authors": "R. Devon Hjelm and Sergey M. Plis and Vince C. Calhoun", "title": "Variational Autoencoders for Feature Detection of Magnetic Resonance\n  Imaging Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA), as an approach to the blind\nsource-separation (BSS) problem, has become the de-facto standard in many\nmedical imaging settings. Despite successes and a large ongoing research\neffort, the limitation of ICA to square linear transformations have not been\novercome, so that general INFOMAX is still far from being realized. As an\nalternative, we present feature analysis in medical imaging as a problem solved\nby Helmholtz machines, which include dimensionality reduction and\nreconstruction of the raw data under the same objective, and which recently\nhave overcome major difficulties in inference and learning with deep and\nnonlinear configurations. We demonstrate one approach to training Helmholtz\nmachines, variational auto-encoders (VAE), as a viable approach toward feature\nextraction with magnetic resonance imaging (MRI) data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 21:31:36 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Hjelm", "R. Devon", ""], ["Plis", "Sergey M.", ""], ["Calhoun", "Vince C.", ""]]}, {"id": "1603.06653", "submitter": "Eder Santana", "authors": "Eder Santana, Matthew Emigh and Jose C Principe", "title": "Information Theoretic-Learning Auto-Encoder", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Information Theoretic-Learning (ITL) divergence measures for\nvariational regularization of neural networks. We also explore ITL-regularized\nautoencoders as an alternative to variational autoencoding bayes, adversarial\nautoencoders and generative adversarial networks for randomly generating sample\ndata without explicitly defining a partition function. This paper also\nformalizes, generative moment matching networks under the ITL framework.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 01:05:47 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Santana", "Eder", ""], ["Emigh", "Matthew", ""], ["Principe", "Jose C", ""]]}, {"id": "1603.06679", "submitter": "Wenya Wang", "authors": "Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier and Xiaokui Xiao", "title": "Recursive Neural Conditional Random Fields for Aspect-based Sentiment\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In aspect-based sentiment analysis, extracting aspect terms along with the\nopinions being expressed from user-generated content is one of the most\nimportant subtasks. Previous studies have shown that exploiting connections\nbetween aspect and opinion terms is promising for this task. In this paper, we\npropose a novel joint model that integrates recursive neural networks and\nconditional random fields into a unified framework for explicit aspect and\nopinion terms co-extraction. The proposed model learns high-level\ndiscriminative features and double propagate information between aspect and\nopinion terms, simultaneously. Moreover, it is flexible to incorporate\nhand-crafted features into the proposed model to further boost its information\nextraction performance. Experimental results on the SemEval Challenge 2014\ndataset show the superiority of our proposed model over several baseline\nmethods as well as the winning systems of the challenge.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 05:59:00 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 06:24:06 GMT"}, {"version": "v3", "created": "Mon, 19 Sep 2016 14:00:43 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Wang", "Wenya", ""], ["Pan", "Sinno Jialin", ""], ["Dahlmeier", "Daniel", ""], ["Xiao", "Xiaokui", ""]]}, {"id": "1603.06708", "submitter": "Changsheng Li", "authors": "Changsheng Li and Fan Wei and Junchi Yan and Weishan Dong and Qingshan\n  Liu and Xiaoyu Zhang and Hongyuan Zha", "title": "A Self-Paced Regularization Framework for Multi-Label Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel multi-label learning framework, called\nMulti-Label Self-Paced Learning (MLSPL), in an attempt to incorporate the\nself-paced learning strategy into multi-label learning regime. In light of the\nbenefits of adopting the easy-to-hard strategy proposed by self-paced learning,\nthe devised MLSPL aims to learn multiple labels jointly by gradually including\nlabel learning tasks and instances into model training from the easy to the\nhard. We first introduce a self-paced function as a regularizer in the\nmulti-label learning formulation, so as to simultaneously rank priorities of\nthe label learning tasks and the instances in each learning iteration.\nConsidering that different multi-label learning scenarios often need different\nself-paced schemes during optimization, we thus propose a general way to find\nthe desired self-paced functions. Experimental results on three benchmark\ndatasets suggest the state-of-the-art performance of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 09:03:40 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 14:54:28 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Li", "Changsheng", ""], ["Wei", "Fan", ""], ["Yan", "Junchi", ""], ["Dong", "Weishan", ""], ["Liu", "Qingshan", ""], ["Zhang", "Xiaoyu", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1603.06743", "submitter": "Makoto Yamada", "authors": "Makoto Yamada, Koh Takeuchi, Tomoharu Iwata, John Shawe-Taylor, Samuel\n  Kaski", "title": "Localized Lasso for High-Dimensional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the localized Lasso, which is suited for learning models that\nare both interpretable and have a high predictive power in problems with high\ndimensionality $d$ and small sample size $n$. More specifically, we consider a\nfunction defined by local sparse models, one at each data point. We introduce\nsample-wise network regularization to borrow strength across the models, and\nsample-wise exclusive group sparsity (a.k.a., $\\ell_{1,2}$ norm) to introduce\ndiversity into the choice of feature sets in the local models. The local models\nare interpretable in terms of similarity of their sparsity patterns. The cost\nfunction is convex, and thus has a globally optimal solution. Moreover, we\npropose a simple yet efficient iterative least-squares based optimization\nprocedure for the localized Lasso, which does not need a tuning parameter, and\nis guaranteed to converge to a globally optimal solution. The solution is\nempirically shown to outperform alternatives for both simulated and genomic\npersonalized medicine data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 11:41:28 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 13:43:21 GMT"}, {"version": "v3", "created": "Fri, 14 Oct 2016 02:15:06 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Yamada", "Makoto", ""], ["Takeuchi", "Koh", ""], ["Iwata", "Tomoharu", ""], ["Shawe-Taylor", "John", ""], ["Kaski", "Samuel", ""]]}, {"id": "1603.06782", "submitter": "Aryan Mokhtari", "authors": "Aryan Mokhtari and Alec Koppel and Alejandro Ribeiro", "title": "Doubly Random Parallel Stochastic Methods for Large Scale Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning problems over training sets in which both, the number of\ntraining examples and the dimension of the feature vectors, are large. To solve\nthese problems we propose the random parallel stochastic algorithm (RAPSA). We\ncall the algorithm random parallel because it utilizes multiple processors to\noperate in a randomly chosen subset of blocks of the feature vector. We call\nthe algorithm parallel stochastic because processors choose elements of the\ntraining set randomly and independently. Algorithms that are parallel in either\nof these dimensions exist, but RAPSA is the first attempt at a methodology that\nis parallel in both, the selection of blocks and the selection of elements of\nthe training set. In RAPSA, processors utilize the randomly chosen functions to\ncompute the stochastic gradient component associated with a randomly chosen\nblock. The technical contribution of this paper is to show that this minimally\ncoordinated algorithm converges to the optimal classifier when the training\nobjective is convex. In particular, we show that: (i) When using decreasing\nstepsizes, RAPSA converges almost surely over the random choice of blocks and\nfunctions. (ii) When using constant stepsizes, convergence is to a neighborhood\nof optimality with a rate that is linear in expectation. RAPSA is numerically\nevaluated on the MNIST digit recognition problem.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 13:29:46 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Mokhtari", "Aryan", ""], ["Koppel", "Alec", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1603.06805", "submitter": "Dieter Hendricks", "authors": "Dieter Hendricks", "title": "Using real-time cluster configurations of streaming asynchronous\n  features as online state descriptors in financial markets", "comments": "19 pages, 6 figures, 3 tables, under review at Pattern Recognition\n  Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.LG q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scheme for online, unsupervised state discovery and detection\nfrom streaming, multi-featured, asynchronous data in high-frequency financial\nmarkets. Online feature correlations are computed using an unbiased, lossless\nFourier estimator. A high-speed maximum likelihood clustering algorithm is then\nused to find the feature cluster configuration which best explains the\nstructure in the correlation matrix. We conjecture that this feature\nconfiguration is a candidate descriptor for the temporal state of the system.\nUsing a simple cluster configuration similarity metric, we are able to\nenumerate the state space based on prevailing feature configurations. The\nproposed state representation removes the need for human-driven data\npre-processing for state attribute specification, allowing a learning agent to\nfind structure in streaming data, discern changes in the system, enumerate its\nperceived state space and learn suitable action-selection policies.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 14:23:42 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 14:40:42 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Hendricks", "Dieter", ""]]}, {"id": "1603.06807", "submitter": "Iulian Vlad Serban", "authors": "Iulian Vlad Serban, Alberto Garc\\'ia-Dur\\'an, Caglar Gulcehre, Sungjin\n  Ahn, Sarath Chandar, Aaron Courville, Yoshua Bengio", "title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M\n  Factoid Question-Answer Corpus", "comments": "13 pages, 1 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, large-scale supervised learning corpora have enabled\nmachine learning researchers to make substantial advances. However, to this\ndate, there are no large-scale question-answer corpora available. In this paper\nwe present the 30M Factoid Question-Answer Corpus, an enormous question answer\npair corpus produced by applying a novel neural network architecture on the\nknowledge base Freebase to transduce facts into natural language questions. The\nproduced question answer pairs are evaluated both by human evaluators and using\nautomatic evaluation metrics, including well-established machine translation\nand sentence similarity metrics. Across all evaluation criteria the\nquestion-generation model outperforms the competing template-based baseline.\nFurthermore, when presented to human evaluators, the generated questions appear\ncomparable in quality to real human-generated questions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 14:25:16 GMT"}, {"version": "v2", "created": "Sun, 29 May 2016 20:00:20 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Serban", "Iulian Vlad", ""], ["Garc\u00eda-Dur\u00e1n", "Alberto", ""], ["Gulcehre", "Caglar", ""], ["Ahn", "Sungjin", ""], ["Chandar", "Sarath", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1603.06829", "submitter": "Otkrist Gupta", "authors": "Otkrist Gupta, Dan Raviv and Ramesh Raskar", "title": "Multi-velocity neural networks for gesture recognition in videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new action recognition deep neural network which adaptively\nlearns the best action velocities in addition to the classification. While deep\nneural networks have reached maturity for image understanding tasks, we are\nstill exploring network topologies and features to handle the richer\nenvironment of video clips. Here, we tackle the problem of multiple velocities\nin action recognition, and provide state-of-the-art results for gesture\nrecognition, on known and new collected datasets. We further provide the\ntraining steps for our semi-supervised network, suited to learn from huge\nunlabeled datasets with only a fraction of labeled examples.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 15:26:26 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Gupta", "Otkrist", ""], ["Raviv", "Dan", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1603.06859", "submitter": "Fabricio de Franca Olivetti", "authors": "Andr\\'e L. V. Coelho and Fabr\\'icio O. de Fran\\c{c}a", "title": "Enhanced perceptrons using contrastive biclusters", "comments": "article under review by Neural Computing and Applications, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptrons are neuronal devices capable of fully discriminating linearly\nseparable classes. Although straightforward to implement and train, their\napplicability is usually hindered by non-trivial requirements imposed by\nreal-world classification problems. Therefore, several approaches, such as\nkernel perceptrons, have been conceived to counteract such difficulties. In\nthis paper, we investigate an enhanced perceptron model based on the notion of\ncontrastive biclusters. From this perspective, a good discriminative bicluster\ncomprises a subset of data instances belonging to one class that show high\ncoherence across a subset of features and high differentiation from nearest\ninstances of the other class under the same features (referred to as its\ncontrastive bicluster). Upon each local subspace associated with a pair of\ncontrastive biclusters a perceptron is trained and the model with highest area\nunder the receiver operating characteristic curve (AUC) value is selected as\nthe final classifier. Experiments conducted on a range of data sets, including\nthose related to a difficult biosignal classification problem, show that the\nproposed variant can be indeed very useful, prevailing in most of the cases\nupon standard and kernel perceptrons in terms of accuracy and AUC measures.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 16:32:26 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Coelho", "Andr\u00e9 L. V.", ""], ["de Fran\u00e7a", "Fabr\u00edcio O.", ""]]}, {"id": "1603.06861", "submitter": "Anastasios Kyrillidis", "authors": "Vatsal Shah, Megasthenis Asteris, Anastasios Kyrillidis, Sujay\n  Sanghavi", "title": "Trading-off variance and complexity in stochastic gradient descent", "comments": "14 pages, 13 figures, first edition on 9th of October 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent is the method of choice for large-scale machine\nlearning problems, by virtue of its light complexity per iteration. However, it\nlags behind its non-stochastic counterparts with respect to the convergence\nrate, due to high variance introduced by the stochastic updates. The popular\nStochastic Variance-Reduced Gradient (SVRG) method mitigates this shortcoming,\nintroducing a new update rule which requires infrequent passes over the entire\ninput dataset to compute the full-gradient.\n  In this work, we propose CheapSVRG, a stochastic variance-reduction\noptimization scheme. Our algorithm is similar to SVRG but instead of the full\ngradient, it uses a surrogate which can be efficiently computed on a small\nsubset of the input data. It achieves a linear convergence rate ---up to some\nerror level, depending on the nature of the optimization problem---and features\na trade-off between the computational complexity and the convergence rate.\nEmpirical evaluation shows that CheapSVRG performs at least competitively\ncompared to the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 16:34:26 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Shah", "Vatsal", ""], ["Asteris", "Megasthenis", ""], ["Kyrillidis", "Anastasios", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1603.06881", "submitter": "Nihar Shah", "authors": "Nihar B. Shah, Sivaraman Balakrishnan, Martin J. Wainwright", "title": "Feeling the Bern: Adaptive Estimators for Bernoulli Probabilities of\n  Pairwise Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study methods for aggregating pairwise comparison data in order to\nestimate outcome probabilities for future comparisons among a collection of n\nitems. Working within a flexible framework that imposes only a form of strong\nstochastic transitivity (SST), we introduce an adaptivity index defined by the\nindifference sets of the pairwise comparison probabilities. In addition to\nmeasuring the usual worst-case risk of an estimator, this adaptivity index also\ncaptures the extent to which the estimator adapts to instance-specific\ndifficulty relative to an oracle estimator. We prove three main results that\ninvolve this adaptivity index and different algorithms. First, we propose a\nthree-step estimator termed Count-Randomize-Least squares (CRL), and show that\nit has adaptivity index upper bounded as $\\sqrt{n}$ up to logarithmic factors.\nWe then show that that conditional on the hardness of planted clique, no\ncomputationally efficient estimator can achieve an adaptivity index smaller\nthan $\\sqrt{n}$. Second, we show that a regularized least squares estimator can\nachieve a poly-logarithmic adaptivity index, thereby demonstrating a\n$\\sqrt{n}$-gap between optimal and computationally achievable adaptivity.\nFinally, we prove that the standard least squares estimator, which is known to\nbe optimally adaptive in several closely related problems, fails to adapt in\nthe context of estimating pairwise probabilities.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 17:28:08 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Shah", "Nihar B.", ""], ["Balakrishnan", "Sivaraman", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1603.07044", "submitter": "Wei-Ning Hsu", "authors": "Wei-Ning Hsu, Yu Zhang and James Glass", "title": "Recurrent Neural Network Encoder with Attention for Community Question\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply a general recurrent neural network (RNN) encoder framework to\ncommunity question answering (cQA) tasks. Our approach does not rely on any\nlinguistic processing, and can be applied to different languages or domains.\nFurther improvements are observed when we extend the RNN encoders with a neural\nattention mechanism that encourages reasoning over entire sequences. To deal\nwith practical issues such as data sparsity and imbalanced labels, we apply\nvarious techniques such as transfer learning and multitask learning. Our\nexperiments on the SemEval-2016 cQA task show 10% improvement on a MAP score\ncompared to an information retrieval-based approach, and achieve comparable\nperformance to a strong handcrafted feature-based method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 01:52:54 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Hsu", "Wei-Ning", ""], ["Zhang", "Yu", ""], ["Glass", "James", ""]]}, {"id": "1603.07094", "submitter": "Kai Morino", "authors": "Motohide Higaki, Kai Morino, Hiroshi Murata, Ryo Asaoka, and Kenji\n  Yamanishi", "title": "Predicting Glaucoma Visual Field Loss by Hierarchically Aggregating\n  Clustering-based Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study addresses the issue of predicting the glaucomatous visual field\nloss from patient disease datasets. Our goal is to accurately predict the\nprogress of the disease in individual patients. As very few measurements are\navailable for each patient, it is difficult to produce good predictors for\nindividuals. A recently proposed clustering-based method enhances the power of\nprediction using patient data with similar spatiotemporal patterns. Each\npatient is categorized into a cluster of patients, and a predictive model is\nconstructed using all of the data in the class. Predictions are highly\ndependent on the quality of clustering, but it is difficult to identify the\nbest clustering method. Thus, we propose a method for aggregating cluster-based\npredictors to obtain better prediction accuracy than from a single\ncluster-based prediction. Further, the method shows very high performances by\nhierarchically aggregating experts generated from several cluster-based\nmethods. We use real datasets to demonstrate that our method performs\nsignificantly better than conventional clustering-based and patient-wise\nregression methods, because the hierarchical aggregating strategy has a\nmechanism whereby good predictors in a small community can thrive.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 09:06:19 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Higaki", "Motohide", ""], ["Morino", "Kai", ""], ["Murata", "Hiroshi", ""], ["Asaoka", "Ryo", ""], ["Yamanishi", "Kenji", ""]]}, {"id": "1603.07195", "submitter": "Mark Eisen", "authors": "Mark Eisen, Aryan Mokhtari, Alejandro Ribeiro", "title": "A Decentralized Quasi-Newton Method for Dual Formulations of Consensus\n  Optimization", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers consensus optimization problems where each node of a\nnetwork has access to a different summand of an aggregate cost function. Nodes\ntry to minimize the aggregate cost function, while they exchange information\nonly with their neighbors. We modify the dual decomposition method to\nincorporate a curvature correction inspired by the\nBroyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton method. The resulting dual\nD-BFGS method is a fully decentralized algorithm in which nodes approximate\ncurvature information of themselves and their neighbors through the\nsatisfaction of a secant condition. Dual D-BFGS is of interest in consensus\noptimization problems that are not well conditioned, making first order\ndecentralized methods ineffective, and in which second order information is not\nreadily available, making decentralized second order methods infeasible.\nAsynchronous implementation is discussed and convergence of D-BFGS is\nestablished formally for both synchronous and asynchronous implementations.\nPerformance advantages relative to alternative decentralized algorithms are\nshown numerically.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 14:24:39 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Eisen", "Mark", ""], ["Mokhtari", "Aryan", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1603.07235", "submitter": "Oncel Tuzel", "authors": "Oncel Tuzel, Yuichi Taguchi, and John R. Hershey", "title": "Global-Local Face Upsampling Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face hallucination, which is the task of generating a high-resolution face\nimage from a low-resolution input image, is a well-studied problem that is\nuseful in widespread application areas. Face hallucination is particularly\nchallenging when the input face resolution is very low (e.g., 10 x 12 pixels)\nand/or the image is captured in an uncontrolled setting with large pose and\nillumination variations. In this paper, we revisit the algorithm introduced in\n[1] and present a deep interpretation of this framework that achieves\nstate-of-the-art under such challenging scenarios. In our deep network\narchitecture the global and local constraints that define a face can be\nefficiently modeled and learned end-to-end using training data. Conceptually\nour network design can be partitioned into two sub-networks: the first one\nimplements the holistic face reconstruction according to global constraints,\nand the second one enhances face-specific details and enforces local patch\nstatistics. We optimize the deep network using a new loss function for\nsuper-resolution that combines reconstruction error with a learned face quality\nmeasure in adversarial setting, producing improved visual results. We conduct\nextensive experiments in both controlled and uncontrolled setups and show that\nour algorithm improves the state of the art both numerically and visually.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 15:29:09 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 15:31:01 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Tuzel", "Oncel", ""], ["Taguchi", "Yuichi", ""], ["Hershey", "John R.", ""]]}, {"id": "1603.07249", "submitter": "Juan C. Cuevas-Tello", "authors": "Juan C. Cuevas-Tello and Manuel Valenzuela-Rendon and Juan A.\n  Nolazco-Flores", "title": "A Tutorial on Deep Neural Networks for Intelligent Systems", "comments": "30 pages, 19 figures, unpublished technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing Intelligent Systems involves artificial intelligence approaches\nincluding artificial neural networks. Here, we present a tutorial of Deep\nNeural Networks (DNNs), and some insights about the origin of the term \"deep\";\nreferences to deep learning are also given. Restricted Boltzmann Machines,\nwhich are the core of DNNs, are discussed in detail. An example of a simple\ntwo-layer network, performing unsupervised learning for unlabeled data, is\nshown. Deep Belief Networks (DBNs), which are used to build networks with more\nthan two layers, are also described. Moreover, examples for supervised learning\nwith DNNs performing simple prediction and classification tasks, are presented\nand explained. This tutorial includes two intelligent pattern recognition\napplications: hand- written digits (benchmark known as MNIST) and speech\nrecognition.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 15:55:20 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Cuevas-Tello", "Juan C.", ""], ["Valenzuela-Rendon", "Manuel", ""], ["Nolazco-Flores", "Juan A.", ""]]}, {"id": "1603.07285", "submitter": "Francesco Visin", "authors": "Vincent Dumoulin, Francesco Visin", "title": "A guide to convolution arithmetic for deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a guide to help deep learning practitioners understand and\nmanipulate convolutional neural network architectures. The guide clarifies the\nrelationship between various properties (input shape, kernel shape, zero\npadding, strides and output shape) of convolutional, pooling and transposed\nconvolutional layers, as well as the relationship between convolutional and\ntransposed convolutional layers. Relationships are derived for various cases,\nand are illustrated in order to make them intuitive.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 17:52:21 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 18:54:25 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Dumoulin", "Vincent", ""], ["Visin", "Francesco", ""]]}, {"id": "1603.07292", "submitter": "Shayak Sen", "authors": "Aleksandar Chakarov, Aditya Nori, Sriram Rajamani, Shayak Sen, and\n  Deepak Vijaykeerthy", "title": "Debugging Machine Learning Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike traditional programs (such as operating systems or word processors)\nwhich have large amounts of code, machine learning tasks use programs with\nrelatively small amounts of code (written in machine learning libraries), but\nvoluminous amounts of data. Just like developers of traditional programs debug\nerrors in their code, developers of machine learning tasks debug and fix errors\nin their data. However, algorithms and tools for debugging and fixing errors in\ndata are less common, when compared to their counterparts for detecting and\nfixing errors in code. In this paper, we consider classification tasks where\nerrors in training data lead to misclassifications in test points, and propose\nan automated method to find the root causes of such misclassifications. Our\nroot cause analysis is based on Pearl's theory of causation, and uses Pearl's\nPS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi,\nencodes the computation of PS as a probabilistic program, and uses recent work\non probabilistic programs and transformations on probabilistic programs (along\nwith gray-box models of machine learning algorithms) to efficiently compute PS.\nPsi is able to identify root causes of data errors in interesting data sets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 18:30:37 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Chakarov", "Aleksandar", ""], ["Nori", "Aditya", ""], ["Rajamani", "Sriram", ""], ["Sen", "Shayak", ""], ["Vijaykeerthy", "Deepak", ""]]}, {"id": "1603.07294", "submitter": "James Foulds", "authors": "James Foulds, Joseph Geumlek, Max Welling, Kamalika Chaudhuri", "title": "On the Theory and Practice of Privacy-Preserving Bayesian Data Analysis", "comments": "Updated to match the accepted UAI version. Generalized the ARE result\n  and included a more detailed proof. Improved some figures, etc", "journal-ref": "Proceedings of the 32nd Conference on Uncertainty in Artificial\n  Intelligence (UAI), 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference has great promise for the privacy-preserving analysis of\nsensitive data, as posterior sampling automatically preserves differential\nprivacy, an algorithmic notion of data privacy, under certain conditions\n(Dimitrakakis et al., 2014; Wang et al., 2015). While this one posterior sample\n(OPS) approach elegantly provides privacy \"for free,\" it is data inefficient in\nthe sense of asymptotic relative efficiency (ARE). We show that a simple\nalternative based on the Laplace mechanism, the workhorse of differential\nprivacy, is as asymptotically efficient as non-private posterior inference,\nunder general assumptions. This technique also has practical advantages\nincluding efficient use of the privacy budget for MCMC. We demonstrate the\npracticality of our approach on a time-series analysis of sensitive military\nrecords from the Afghanistan and Iraq wars disclosed by the Wikileaks\norganization.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 18:31:05 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 00:00:10 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Foulds", "James", ""], ["Geumlek", "Joseph", ""], ["Welling", "Max", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1603.07323", "submitter": "Zhibing Zhao", "authors": "Zhibing Zhao, Peter Piech, Lirong Xia", "title": "Learning Mixtures of Plackett-Luce Models", "comments": "26 pages, 2 figures; remove (incorrectly) generated date; add summary\n  to section 6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the identifiability and efficient learning problems\nof finite mixtures of Plackett-Luce models for rank data. We prove that for any\n$k\\geq 2$, the mixture of $k$ Plackett-Luce models for no more than $2k-1$\nalternatives is non-identifiable and this bound is tight for $k=2$. For generic\nidentifiability, we prove that the mixture of $k$ Plackett-Luce models over $m$\nalternatives is generically identifiable if $k\\leq\\lfloor\\frac {m-2}\n2\\rfloor!$. We also propose an efficient generalized method of moments (GMM)\nalgorithm to learn the mixture of two Plackett-Luce models and show that the\nalgorithm is consistent. Our experiments show that our GMM algorithm is\nsignificantly faster than the EMM algorithm by Gormley and Murphy (2008), while\nachieving competitive statistical efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 19:58:37 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 15:34:42 GMT"}, {"version": "v3", "created": "Tue, 7 Jun 2016 15:46:54 GMT"}, {"version": "v4", "created": "Sat, 7 Mar 2020 00:15:33 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Zhao", "Zhibing", ""], ["Piech", "Peter", ""], ["Xia", "Lirong", ""]]}, {"id": "1603.07341", "submitter": "Tayfun Gokmen", "authors": "Tayfun Gokmen, Yurii Vlasov", "title": "Acceleration of Deep Neural Network Training with Resistive Cross-Point\n  Devices", "comments": "19 pages, 5 figures, 2 tables", "journal-ref": "Front. Neurosci 10, 333 (2016)", "doi": "10.3389/fnins.2016.00333", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural networks (DNN) have demonstrated significant\nbusiness impact in large scale analysis and classification tasks such as speech\nrecognition, visual object detection, pattern extraction, etc. Training of\nlarge DNNs, however, is universally considered as time consuming and\ncomputationally intensive task that demands datacenter-scale computational\nresources recruited for many days. Here we propose a concept of resistive\nprocessing unit (RPU) devices that can potentially accelerate DNN training by\norders of magnitude while using much less power. The proposed RPU device can\nstore and update the weight values locally thus minimizing data movement during\ntraining and allowing to fully exploit the locality and the parallelism of the\ntraining algorithm. We identify the RPU device and system specifications for\nimplementation of an accelerator chip for DNN training in a realistic\nCMOS-compatible technology. For large DNNs with about 1 billion weights this\nmassively parallel RPU architecture can achieve acceleration factors of 30,000X\ncompared to state-of-the-art microprocessors while providing power efficiency\nof 84,000 GigaOps/s/W. Problems that currently require days of training on a\ndatacenter-size cluster with thousands of machines can be addressed within\nhours on a single RPU accelerator. A system consisted of a cluster of RPU\naccelerators will be able to tackle Big Data problems with trillions of\nparameters that is impossible to address today like, for example, natural\nspeech recognition and translation between all world languages, real-time\nanalytics on large streams of business and scientific data, integration and\nanalysis of multimodal sensory data flows from massive number of IoT (Internet\nof Things) sensors.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 20:13:11 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Gokmen", "Tayfun", ""], ["Vlasov", "Yurii", ""]]}, {"id": "1603.07400", "submitter": "Raqibul Hasan", "authors": "Raqibul Hasan, and Tarek Taha", "title": "A Reconfigurable Low Power High Throughput Architecture for Deep Network\n  Training", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General purpose computing systems are used for a large variety of\napplications. Extensive supports for flexibility in these systems limit their\nenergy efficiencies. Neural networks, including deep networks, are widely used\nfor signal processing and pattern recognition applications. In this paper we\npropose a multicore architecture for deep neural network based processing.\nMemristor crossbars are utilized to provide low power high throughput execution\nof neural networks. The system has both training and recognition (evaluation of\nnew input) capabilities. The proposed system could be used for classification,\ndimensionality reduction, feature extraction, and anomaly detection\napplications. The system level area and power benefits of the specialized\narchitecture is compared with the NVIDIA Telsa K20 GPGPU. Our experimental\nevaluations show that the proposed architecture can provide up to five orders\nof magnitude more energy efficiency over GPGPUs for deep neural network\nprocessing.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 00:52:22 GMT"}, {"version": "v2", "created": "Wed, 15 Jun 2016 01:26:31 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Hasan", "Raqibul", ""], ["Taha", "Tarek", ""]]}, {"id": "1603.07421", "submitter": "Ye Yuan", "authors": "Ye Yuan, Mu Li, Jun Liu, Claire J. Tomlin", "title": "On the Powerball Method for Optimization", "comments": null, "journal-ref": null, "doi": "10.1109/LCSYS.2019.2913770", "report-no": null, "categories": "cs.SY cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method to accelerate the convergence of optimization\nalgorithms. This method simply adds a power coefficient $\\gamma\\in[0,1)$ to the\ngradient during optimization. We call this the Powerball method and analyze the\nconvergence rate for the Powerball method for strongly convex functions. While\ntheoretically the Powerball method is guaranteed to have a linear convergence\nrate in the same order of the gradient method, we show that empirically it\nsignificantly outperforms the gradient descent and Newton's method, especially\nduring the initial iterations. We demonstrate that the Powerball method\nprovides a $10$-fold speedup of the convergence of both gradient descent and\nL-BFGS on multiple real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 03:13:40 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 16:15:35 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2016 12:57:04 GMT"}, {"version": "v4", "created": "Fri, 1 Sep 2017 08:23:07 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Yuan", "Ye", ""], ["Li", "Mu", ""], ["Liu", "Jun", ""], ["Tomlin", "Claire J.", ""]]}, {"id": "1603.07454", "submitter": "Chao Ma", "authors": "Chao Ma, Tianchenghou, Bin Lan, Jinhui Xu, Zhenhua Zhang", "title": "Deep Extreme Feature Extraction: New MVA Method for Searching Particles\n  in High Energy Physics", "comments": "20 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Deep Extreme Feature Extraction (DEFE), a new\nensemble MVA method for searching $\\tau^{+}\\tau^{-}$ channel of Higgs bosons in\nhigh energy physics. DEFE can be viewed as a deep ensemble learning scheme that\ntrains a strongly diverse set of neural feature learners without explicitly\nencouraging diversity and penalizing correlations. This is achieved by adopting\nan implicit neural controller (not involved in feedforward compuation) that\ndirectly controls and distributes gradient flows from higher level deep\nprediction network. Such model-independent controller results in that every\nsingle local feature learned are used in the feature-to-output mapping stage,\navoiding the blind averaging of features. DEFE makes the ensembles 'deep' in\nthe sense that it allows deep post-process of these features that tries to\nlearn to select and abstract the ensemble of neural feature learners. With the\napplication of this model, a selection regions full of signal process can be\nobtained through the training of a miniature collision events set. In\ncomparison of the Classic Deep Neural Network, DEFE shows a state-of-the-art\nperformance: the error rate has decreased by about 37\\%, the accuracy has\nbroken through 90\\% for the first time, along with the discovery significance\nhas reached a standard deviation of 6.0 $\\sigma$. Experimental data shows that,\nDEFE is able to train an ensemble of discriminative feature learners that\nboosts the overperformance of final prediction.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 07:12:20 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Ma", "Chao", ""], ["Tianchenghou", "", ""], ["Lan", "Bin", ""], ["Xu", "Jinhui", ""], ["Zhang", "Zhenhua", ""]]}, {"id": "1603.07584", "submitter": "Rodrigo Cerqueira Gonzalez Pena", "authors": "Rodrigo Pena, Xavier Bresson, Pierre Vandergheynst", "title": "Source Localization on Graphs via l1 Recovery and Spectral Graph Theory", "comments": "5 pages, 5 figures, published in \"Image, Video, and Multidimensional\n  Signal Processing Workshop (IVMSP), 2016 IEEE 12th\"", "journal-ref": null, "doi": "10.1109/IVMSPW.2016.7528230", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We cast the problem of source localization on graphs as the simultaneous\nproblem of sparse recovery and diffusion kernel learning. An l1 regularization\nterm enforces the sparsity constraint while we recover the sources of diffusion\nfrom a single snapshot of the diffusion process. The diffusion kernel is\nestimated by assuming the process to be as generic as the standard heat\ndiffusion. We show with synthetic data that we can concomitantly learn the\ndiffusion kernel and the sources, given an estimated initialization. We\nvalidate our model with cholera mortality and atmospheric tracer diffusion\ndata, showing also that the accuracy of the solution depends on the\nconstruction of the graph from the data points.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 14:15:17 GMT"}, {"version": "v2", "created": "Sat, 19 Nov 2016 08:56:13 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Pena", "Rodrigo", ""], ["Bresson", "Xavier", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1603.07646", "submitter": "Saurabh Kataria", "authors": "Saurabh Kataria", "title": "Recursive Neural Language Architecture for Tag Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning distributed representations for tags from\ntheir associated content for the task of tag recommendation. Considering\ntagging information is usually very sparse, effective learning from content and\ntag association is very crucial and challenging task. Recently, various neural\nrepresentation learning models such as WSABIE and its variants show promising\nperformance, mainly due to compact feature representations learned in a\nsemantic space. However, their capacity is limited by a linear compositional\napproach for representing tags as sum of equal parts and hurt their\nperformance. In this work, we propose a neural feedback relevance model for\nlearning tag representations with weighted feature representations. Our\nexperiments on two widely used datasets show significant improvement for\nquality of recommendations over various baselines.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 16:39:37 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Kataria", "Saurabh", ""]]}, {"id": "1603.07704", "submitter": "Quan Liu", "authors": "Quan Liu, Hui Jiang, Andrew Evdokimov, Zhen-Hua Ling, Xiaodan Zhu, Si\n  Wei, Yu Hu", "title": "Probabilistic Reasoning via Deep Learning: Neural Association Models", "comments": "Probabilistic reasoning, Winograd Schema Challenge, Deep learning,\n  Neural Networks, Distributed Representation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new deep learning approach, called neural\nassociation model (NAM), for probabilistic reasoning in artificial\nintelligence. We propose to use neural networks to model association between\nany two events in a domain. Neural networks take one event as input and compute\na conditional probability of the other event to model how likely these two\nevents are to be associated. The actual meaning of the conditional\nprobabilities varies between applications and depends on how the models are\ntrained. In this work, as two case studies, we have investigated two NAM\nstructures, namely deep neural networks (DNN) and relation-modulated neural\nnets (RMNN), on several probabilistic reasoning tasks in AI, including\nrecognizing textual entailment, triple classification in multi-relational\nknowledge bases and commonsense reasoning. Experimental results on several\npopular datasets derived from WordNet, FreeBase and ConceptNet have all\ndemonstrated that both DNNs and RMNNs perform equally well and they can\nsignificantly outperform the conventional methods available for these reasoning\ntasks. Moreover, compared with DNNs, RMNNs are superior in knowledge transfer,\nwhere a pre-trained model can be quickly extended to an unseen relation after\nobserving only a few training samples. To further prove the effectiveness of\nthe proposed models, in this work, we have applied NAMs to solving challenging\nWinograd Schema (WS) problems. Experiments conducted on a set of WS problems\nprove that the proposed models have the potential for commonsense reasoning.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 18:54:18 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 14:31:17 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Liu", "Quan", ""], ["Jiang", "Hui", ""], ["Evdokimov", "Andrew", ""], ["Ling", "Zhen-Hua", ""], ["Zhu", "Xiaodan", ""], ["Wei", "Si", ""], ["Hu", "Yu", ""]]}, {"id": "1603.07772", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Cuiling Lan, Junliang Xing, Wenjun Zeng, Yanghao Li, Li\n  Shen, Xiaohui Xie", "title": "Co-occurrence Feature Learning for Skeleton based Action Recognition\n  using Regularized Deep LSTM Networks", "comments": "AAAI 2016 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeleton based action recognition distinguishes human actions using the\ntrajectories of skeleton joints, which provide a very good representation for\ndescribing actions. Considering that recurrent neural networks (RNNs) with Long\nShort-Term Memory (LSTM) can learn feature representations and model long-term\ntemporal dependencies automatically, we propose an end-to-end fully connected\ndeep LSTM network for skeleton based action recognition. Inspired by the\nobservation that the co-occurrences of the joints intrinsically characterize\nhuman actions, we take the skeleton as the input at each time slot and\nintroduce a novel regularization scheme to learn the co-occurrence features of\nskeleton joints. To train the deep LSTM network effectively, we propose a new\ndropout algorithm which simultaneously operates on the gates, cells, and output\nresponses of the LSTM neurons. Experimental results on three human action\nrecognition datasets consistently demonstrate the effectiveness of the proposed\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 22:43:55 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Zhu", "Wentao", ""], ["Lan", "Cuiling", ""], ["Xing", "Junliang", ""], ["Zeng", "Wenjun", ""], ["Li", "Yanghao", ""], ["Shen", "Li", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1603.07810", "submitter": "Andreas Veit", "authors": "Andreas Veit, Serge Belongie, Theofanis Karaletsos", "title": "Conditional Similarity Networks", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What makes images similar? To measure the similarity between images, they are\ntypically embedded in a feature-vector space, in which their distance preserve\nthe relative dissimilarity. However, when learning such similarity embeddings\nthe simplifying assumption is commonly made that images are only compared to\none unique measure of similarity. A main reason for this is that contradicting\nnotions of similarities cannot be captured in a single space. To address this\nshortcoming, we propose Conditional Similarity Networks (CSNs) that learn\nembeddings differentiated into semantically distinct subspaces that capture the\ndifferent notions of similarities. CSNs jointly learn a disentangled embedding\nwhere features for different similarities are encoded in separate dimensions as\nwell as masks that select and reweight relevant dimensions to induce a subspace\nthat encodes a specific similarity notion. We show that our approach learns\ninterpretable image representations with visually relevant semantic subspaces.\nFurther, when evaluating on triplet questions from multiple similarity notions\nour model even outperforms the accuracy obtained by training individual\nspecialized networks for each notion separately.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 02:52:02 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 12:41:01 GMT"}, {"version": "v3", "created": "Mon, 10 Apr 2017 15:18:21 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Veit", "Andreas", ""], ["Belongie", "Serge", ""], ["Karaletsos", "Theofanis", ""]]}, {"id": "1603.07828", "submitter": "Bo-Wei Chen", "authors": "Bo-Wei Chen", "title": "Privacy-Preserved Big Data Analysis Based on Asymmetric Imputation\n  Kernels and Multiside Similarities", "comments": "Incomplete data analysis, partial similarity, multiside similarity,\n  privacy preservation, kernel ridge regression (KRR), missing values, data\n  imputation, kernel method, cloud computing, data analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents an efficient approach for incomplete data classification,\nwhere the entries of samples are missing or masked due to privacy preservation.\nTo deal with these incomplete data, a new kernel function with asymmetric\nintrinsic mappings is proposed in this study. Such a new kernel uses three-side\nsimilarities for kernel matrix formation. The similarity between a testing\ninstance and a training sample relies not only on their distance but also on\nthe relation between the testing sample and the centroid of the class, where\nthe training sample belongs. This reduces biased estimation compared with\ntypical methods when only one training sample is used for kernel matrix\nformation. Furthermore, centroid generation does not involve any clustering\nalgorithms. The proposed kernel is capable of performing data imputation by\nusing class-dependent averages. This enhances Fisher Discriminant Ratios and\ndata discriminability. Experiments on two open databases were carried out for\nevaluating the proposed method. The result indicated that the accuracy of the\nproposed method was higher than that of the baseline. These findings thereby\ndemonstrated the effectiveness of the proposed idea.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 06:04:30 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 14:20:34 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Chen", "Bo-Wei", ""]]}, {"id": "1603.07834", "submitter": "Adedotun Akintayo", "authors": "Adedotun Akintayo, Nigel Lee, Vikas Chawla, Mark Mullaney, Christopher\n  Marett, Asheesh Singh, Arti Singh, Greg Tylka, Baskar Ganapathysubramaniam,\n  Soumik Sarkar", "title": "An end-to-end convolutional selective autoencoder approach to Soybean\n  Cyst Nematode eggs detection", "comments": "A 10 pages, 8 figures International Conference on Machine\n  Leaning(ICML) Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel selective autoencoder approach within the\nframework of deep convolutional networks. The crux of the idea is to train a\ndeep convolutional autoencoder to suppress undesired parts of an image frame\nwhile allowing the desired parts resulting in efficient object detection. The\nefficacy of the framework is demonstrated on a critical plant science problem.\nIn the United States, approximately $1 billion is lost per annum due to a\nnematode infection on soybean plants. Currently, plant-pathologists rely on\nlabor-intensive and time-consuming identification of Soybean Cyst Nematode\n(SCN) eggs in soil samples via manual microscopy. The proposed framework\nattempts to significantly expedite the process by using a series of manually\nlabeled microscopic images for training followed by automated high-throughput\negg detection. The problem is particularly difficult due to the presence of a\nlarge population of non-egg particles (disturbances) in the image frames that\nare very similar to SCN eggs in shape, pose and illumination. Therefore, the\nselective autoencoder is trained to learn unique features related to the\ninvariant shapes and sizes of the SCN eggs without handcrafting. After that, a\ncomposite non-maximum suppression and differencing is applied at the\npost-processing stage.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 07:12:32 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Akintayo", "Adedotun", ""], ["Lee", "Nigel", ""], ["Chawla", "Vikas", ""], ["Mullaney", "Mark", ""], ["Marett", "Christopher", ""], ["Singh", "Asheesh", ""], ["Singh", "Arti", ""], ["Tylka", "Greg", ""], ["Ganapathysubramaniam", "Baskar", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1603.07839", "submitter": "Adedotun Akintayo", "authors": "Adedotun Akintayo, Kin Gwn Lore, Soumalya Sarkar, Soumik Sarkar", "title": "Early Detection of Combustion Instabilities using Deep Convolutional\n  Selective Autoencoders on Hi-speed Flame Video", "comments": "A 10 pages, 10 figures submission for Applied Data Science Track of\n  KDD16", "journal-ref": null, "doi": "10.1145/1235", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an end-to-end convolutional selective autoencoder\napproach for early detection of combustion instabilities using rapidly arriving\nflame image frames. The instabilities arising in combustion processes cause\nsignificant deterioration and safety issues in various human-engineered systems\nsuch as land and air based gas turbine engines. These properties are described\nas self-sustaining, large amplitude pressure oscillations and show varying\nspatial scales periodic coherent vortex structure shedding. However, such\ninstability is extremely difficult to detect before a combustion process\nbecomes completely unstable due to its sudden (bifurcation-type) nature. In\nthis context, an autoencoder is trained to selectively mask stable flame and\nallow unstable flame image frames. In that process, the model learns to\nidentify and extract rich descriptive and explanatory flame shape features.\nWith such a training scheme, the selective autoencoder is shown to be able to\ndetect subtle instability features as a combustion process makes transition\nfrom stable to unstable region. As a consequence, the deep learning tool-chain\ncan perform as an early detection framework for combustion instabilities that\nwill have a transformative impact on the safety and performance of modern\nengines.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 08:02:41 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Akintayo", "Adedotun", ""], ["Lore", "Kin Gwn", ""], ["Sarkar", "Soumalya", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1603.07846", "submitter": "Wei Wang", "authors": "Wei Wang, Gang Chen, Haibo Chen, Tien Tuan Anh Dinh, Jinyang Gao, Beng\n  Chin Ooi, Kian-Lee Tan and Sheng Wang", "title": "Deep Learning At Scale and At Ease", "comments": "submitted to TOMM (under review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning techniques have enjoyed success in various multimedia\napplications, such as image classification and multi-modal data analysis. Large\ndeep learning models are developed for learning rich representations of complex\ndata. There are two challenges to overcome before deep learning can be widely\nadopted in multimedia and other applications. One is usability, namely the\nimplementation of different models and training algorithms must be done by\nnon-experts without much effort especially when the model is large and complex.\nThe other is scalability, that is the deep learning system must be able to\nprovision for a huge demand of computing resources for training large models\nwith massive datasets. To address these two challenges, in this paper, we\ndesign a distributed deep learning platform called SINGA which has an intuitive\nprogramming model based on the common layer abstraction of deep learning\nmodels. Good scalability is achieved through flexible distributed training\narchitecture and specific optimization techniques. SINGA runs on GPUs as well\nas on CPUs, and we show that it outperforms many other state-of-the-art deep\nlearning systems. Our experience with developing and training deep learning\nmodels for real-life multimedia applications in SINGA shows that the platform\nis both usable and scalable.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 08:46:02 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Wang", "Wei", ""], ["Chen", "Gang", ""], ["Chen", "Haibo", ""], ["Dinh", "Tien Tuan Anh", ""], ["Gao", "Jinyang", ""], ["Ooi", "Beng Chin", ""], ["Tan", "Kian-Lee", ""], ["Wang", "Sheng", ""]]}, {"id": "1603.07849", "submitter": "Eric Makita", "authors": "Eric Makita, Artem Lenskiy", "title": "A multinomial probabilistic model for movie genre predictions", "comments": "5 pages, 4 figures, 8th International Conference on Machine Learning\n  and Computing, Hong Kong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a movie genre-prediction based on multinomial probability\nmodel. To the best of our knowledge, this problem has not been addressed yet in\nthe field of recommender system. The prediction of a movie genre has many\npractical applications including complementing the items categories given by\nexperts and providing a surprise effect in the recommendations given to a user.\nWe employ mulitnomial event model to estimate a likelihood of a movie given\ngenre and the Bayes rule to evaluate the posterior probability of a genre given\na movie. Experiments with the MovieLens dataset validate our approach. We\nachieved 70% prediction rate using only 15% of the whole set for training.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 08:49:39 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Makita", "Eric", ""], ["Lenskiy", "Artem", ""]]}, {"id": "1603.07866", "submitter": "Romain Couillet", "authors": "Romain Couillet, Gilles Wainrib, Harry Sevi, Hafiz Tiomoko Ali", "title": "The Asymptotic Performance of Linear Echo State Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a study of the mean-square error (MSE) performance of linear\necho-state neural networks is performed, both for training and testing tasks.\nConsidering the realistic setting of noise present at the network nodes, we\nderive deterministic equivalents for the aforementioned MSE in the limit where\nthe number of input data $T$ and network size $n$ both grow large. Specializing\nthen the network connectivity matrix to specific random settings, we further\nobtain simple formulas that provide new insights on the performance of such\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 10:27:00 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Couillet", "Romain", ""], ["Wainrib", "Gilles", ""], ["Sevi", "Harry", ""], ["Ali", "Hafiz Tiomoko", ""]]}, {"id": "1603.07879", "submitter": "Raja Kishor D Mr.", "authors": "D. Raja Kishor, N. B. Venkateswarlu", "title": "Hybridization of Expectation-Maximization and K-Means Algorithms for\n  Better Clustering Performance", "comments": "17 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The present work proposes hybridization of Expectation-Maximization (EM) and\nK-Means techniques as an attempt to speed-up the clustering process. Though\nboth K-Means and EM techniques look into different areas, K-means can be viewed\nas an approximate way to obtain maximum likelihood estimates for the means.\nAlong with the proposed algorithm for hybridization, the present work also\nexperiments with the Standard EM algorithm. Six different datasets are used for\nthe experiments of which three are synthetic datasets. Clustering fitness and\nSum of Squared Errors (SSE) are computed for measuring the clustering\nperformance. In all the experiments it is observed that the proposed algorithm\nfor hybridization of EM and K-Means techniques is consistently taking less\nexecution time with acceptable Clustering Fitness value and less SSE than the\nstandard EM algorithm. It is also observed that the proposed algorithm is\nproducing better clustering results than the Cluster package of Purdue\nUniversity.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 11:09:22 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Kishor", "D. Raja", ""], ["Venkateswarlu", "N. B.", ""]]}, {"id": "1603.07886", "submitter": "Shanlin Zhong", "authors": "Peijie Yin, Hong Qiao, Wei Wu, Lu Qi, YinLin Li, Shanlin Zhong, Bo\n  Zhang", "title": "A Novel Biologically Mechanism-Based Visual Cognition Model--Automatic\n  Extraction of Semantics, Formation of Integrated Concepts and Re-selection\n  Features for Ambiguity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integration between biology and information science benefits both fields.\nMany related models have been proposed, such as computational visual cognition\nmodels, computational motor control models, integrations of both and so on. In\ngeneral, the robustness and precision of recognition is one of the key problems\nfor object recognition models.\n  In this paper, inspired by features of human recognition process and their\nbiological mechanisms, a new integrated and dynamic framework is proposed to\nmimic the semantic extraction, concept formation and feature re-selection in\nhuman visual processing. The main contributions of the proposed model are as\nfollows:\n  (1) Semantic feature extraction: Local semantic features are learnt from\nepisodic features that are extracted from raw images through a deep neural\nnetwork;\n  (2) Integrated concept formation: Concepts are formed with local semantic\ninformation and structural information learnt through network.\n  (3) Feature re-selection: When ambiguity is detected during recognition\nprocess, distinctive features according to the difference between ambiguous\ncandidates are re-selected for recognition.\n  Experimental results on hand-written digits and facial shape dataset show\nthat, compared with other methods, the new proposed model exhibits higher\nrobustness and precision for visual recognition, especially in the condition\nwhen input samples are smantic ambiguous. Meanwhile, the introduced biological\nmechanisms further strengthen the interaction between neuroscience and\ninformation science.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 11:47:16 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Yin", "Peijie", ""], ["Qiao", "Hong", ""], ["Wu", "Wei", ""], ["Qi", "Lu", ""], ["Li", "YinLin", ""], ["Zhong", "Shanlin", ""], ["Zhang", "Bo", ""]]}, {"id": "1603.07893", "submitter": "Hengjian Jia", "authors": "Hengjian Jia", "title": "Investigation Into The Effectiveness Of Long Short Term Memory Networks\n  For Stock Price Prediction", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The effectiveness of long short term memory networks trained by\nbackpropagation through time for stock price prediction is explored in this\npaper. A range of different architecture LSTM networks are constructed trained\nand tested.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 12:28:02 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 11:28:45 GMT"}, {"version": "v3", "created": "Sun, 28 Aug 2016 09:56:23 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Jia", "Hengjian", ""]]}, {"id": "1603.07980", "submitter": "Joseph Dulny III", "authors": "Joseph Dulny III and Michael Kim", "title": "Developing Quantum Annealer Driven Data Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning applications are limited by computational power. In this\npaper, we gain novel insights into the application of quantum annealing (QA) to\nmachine learning (ML) through experiments in natural language processing (NLP),\nseizure prediction, and linear separability testing. These experiments are\nperformed on QA simulators and early-stage commercial QA hardware and compared\nto an unprecedented number of traditional ML techniques. We extend QBoost, an\nearly implementation of a binary classifier that utilizes a quantum annealer,\nvia resampling and ensembling of predicted probabilities to produce a more\nrobust class estimator. To determine the strengths and weaknesses of this\napproach, resampled QBoost (RQBoost) is tested across several datasets and\ncompared to QBoost and traditional ML. We show and explain how QBoost in\ncombination with a commercial QA device are unable to perfectly separate binary\nclass data which is linearly separable via logistic regression with shrinkage.\nWe further explore the performance of RQBoost in the space of NLP and seizure\nprediction and find QA-enabled ML using QBoost and RQBoost is outperformed by\ntraditional techniques. Additionally, we provide a detailed discussion of\nalgorithmic constraints and trade-offs imposed by the use of this QA hardware.\nThrough these experiments, we provide unique insights into the state of quantum\nML via boosting and the use of quantum annealing hardware that are valuable to\ninstitutions interested in applying QA to problems in ML and beyond.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 18:36:33 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Dulny", "Joseph", "III"], ["Kim", "Michael", ""]]}, {"id": "1603.08023", "submitter": "Ryan Lowe T.", "authors": "Chia-Wei Liu, Ryan Lowe, Iulian V. Serban, Michael Noseworthy, Laurent\n  Charlin, Joelle Pineau", "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of\n  Unsupervised Evaluation Metrics for Dialogue Response Generation", "comments": "First 4 authors had equal contribution. 13 pages, 5 tables, 6\n  figures. EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate evaluation metrics for dialogue response generation systems\nwhere supervised labels, such as task completion, are not available. Recent\nworks in response generation have adopted metrics from machine translation to\ncompare a model's generated response to a single target response. We show that\nthese metrics correlate very weakly with human judgements in the non-technical\nTwitter domain, and not at all in the technical Ubuntu domain. We provide\nquantitative and qualitative results highlighting specific weaknesses in\nexisting metrics, and provide recommendations for future development of better\nautomatic evaluation metrics for dialogue systems.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 20:32:21 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 18:28:32 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Liu", "Chia-Wei", ""], ["Lowe", "Ryan", ""], ["Serban", "Iulian V.", ""], ["Noseworthy", "Michael", ""], ["Charlin", "Laurent", ""], ["Pineau", "Joelle", ""]]}, {"id": "1603.08028", "submitter": "Daniel Cullina", "authors": "Daniel Cullina, Kushagra Singhal, Negar Kiyavash, Prateek Mittal", "title": "On the Simultaneous Preservation of Privacy and Community Structure in\n  Anonymized Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of performing community detection on a network, while\nmaintaining privacy, assuming that the adversary has access to an auxiliary\ncorrelated network. We ask the question \"Does there exist a regime where the\nnetwork cannot be deanonymized perfectly, yet the community structure could be\nlearned?.\" To answer this question, we derive information theoretic converses\nfor the perfect deanonymization problem using the Stochastic Block Model and\nedge sub-sampling. We also provide an almost tight achievability result for\nperfect deanonymization.\n  We also evaluate the performance of percolation based deanonymization\nalgorithm on Stochastic Block Model data-sets that satisfy the conditions of\nour converse. Although our converse applies to exact deanonymization, the\nalgorithm fails drastically when the conditions of the converse are met.\nAdditionally, we study the effect of edge sub-sampling on the community\nstructure of a real world dataset. Results show that the dataset falls under\nthe purview of the idea of this paper. There results suggest that it may be\npossible to prove stronger partial deanonymizability converses, which would\nenable better privacy guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 20:45:32 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Cullina", "Daniel", ""], ["Singhal", "Kushagra", ""], ["Kiyavash", "Negar", ""], ["Mittal", "Prateek", ""]]}, {"id": "1603.08029", "submitter": "Diogo Almeida", "authors": "Sasha Targ, Diogo Almeida, Kevin Lyman", "title": "Resnet in Resnet: Generalizing Residual Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual networks (ResNets) have recently achieved state-of-the-art on\nchallenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep\ndual-stream architecture that generalizes ResNets and standard CNNs and is\neasily implemented with no computational overhead. RiR consistently improves\nperformance over ResNets, outperforms architectures with similar amounts of\naugmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 20:55:40 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Targ", "Sasha", ""], ["Almeida", "Diogo", ""], ["Lyman", "Kevin", ""]]}, {"id": "1603.08035", "submitter": "Horia Mania", "authors": "Horia Mania, Aaditya Ramdas, Martin J. Wainwright, Michael I. Jordan,\n  Benjamin Recht", "title": "On kernel methods for covariates that are rankings", "comments": "35 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutation-valued features arise in a variety of applications, either in a\ndirect way when preferences are elicited over a collection of items, or an\nindirect way in which numerical ratings are converted to a ranking. To date,\nthere has been relatively limited study of regression, classification, and\ntesting problems based on permutation-valued features, as opposed to\npermutation-valued responses. This paper studies the use of reproducing kernel\nHilbert space methods for learning from permutation-valued features. These\nmethods embed the rankings into an implicitly defined function space, and allow\nfor efficient estimation of regression and test functions in this richer space.\nOur first contribution is to characterize both the feature spaces and spectral\nproperties associated with two kernels for rankings, the Kendall and Mallows\nkernels. Using tools from representation theory, we explain the limited\nexpressive power of the Kendall kernel by characterizing its degenerate\nspectrum, and in sharp contrast, we prove that Mallows' kernel is universal and\ncharacteristic. We also introduce families of polynomial kernels that\ninterpolate between the Kendall (degree one) and Mallows' (infinite degree)\nkernels. We show the practical effectiveness of our methods via applications to\nEurobarometer survey data as well as a Movielens ratings dataset.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 21:09:54 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 21:58:31 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Mania", "Horia", ""], ["Ramdas", "Aaditya", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""], ["Recht", "Benjamin", ""]]}, {"id": "1603.08037", "submitter": "Kevin Jamieson", "authors": "Kevin Jamieson and Daniel Haas and Ben Recht", "title": "On the Detection of Mixture Distributions with applications to the Most\n  Biased Coin Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the trade-off between two different kinds of pure\nexploration: breadth versus depth. The most biased coin problem asks how many\ntotal coin flips are required to identify a \"heavy\" coin from an infinite bag\ncontaining both \"heavy\" coins with mean $\\theta_1 \\in (0,1)$, and \"light\" coins\nwith mean $\\theta_0 \\in (0,\\theta_1)$, where heavy coins are drawn from the bag\nwith probability $\\alpha \\in (0,1/2)$. The key difficulty of this problem lies\nin distinguishing whether the two kinds of coins have very similar means, or\nwhether heavy coins are just extremely rare. This problem has applications in\ncrowdsourcing, anomaly detection, and radio spectrum search. Chandrasekaran et.\nal. (2014) recently introduced a solution to this problem but it required\nperfect knowledge of $\\theta_0,\\theta_1,\\alpha$. In contrast, we derive\nalgorithms that are adaptive to partial or absent knowledge of the problem\nparameters. Moreover, our techniques generalize beyond coins to more general\ninstances of infinitely many armed bandit problems. We also prove lower bounds\nthat show our algorithm's upper bounds are tight up to $\\log$ factors, and on\nthe way characterize the sample complexity of differentiating between a single\nparametric distribution and a mixture of two such distributions. As a result,\nthese bounds have surprising implications both for solutions to the most biased\ncoin problem and for anomaly detection when only partial information about the\nparameters is known.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 21:22:59 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Jamieson", "Kevin", ""], ["Haas", "Daniel", ""], ["Recht", "Ben", ""]]}, {"id": "1603.08042", "submitter": "Ouais Alsharif", "authors": "Rohit Prabhavalkar, Ouais Alsharif, Antoine Bruguier, Ian McGraw", "title": "On the Compression of Recurrent Neural Networks with an Application to\n  LVCSR acoustic modeling for Embedded Speech Recognition", "comments": "Accepted in ICASSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of compressing recurrent neural networks (RNNs). In\nparticular, we focus on the compression of RNN acoustic models, which are\nmotivated by the goal of building compact and accurate speech recognition\nsystems which can be run efficiently on mobile devices. In this work, we\npresent a technique for general recurrent model compression that jointly\ncompresses both recurrent and non-recurrent inter-layer weight matrices. We\nfind that the proposed technique allows us to reduce the size of our Long\nShort-Term Memory (LSTM) acoustic model to a third of its original size with\nnegligible loss in accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 21:43:28 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 15:19:30 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Prabhavalkar", "Rohit", ""], ["Alsharif", "Ouais", ""], ["Bruguier", "Antoine", ""], ["McGraw", "Ian", ""]]}, {"id": "1603.08148", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou and Yoshua\n  Bengio", "title": "Pointing the Unknown Words", "comments": "ACL 2016 Oral Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of rare and unknown words is an important issue that can\npotentially influence the performance of many NLP systems, including both the\ntraditional count-based and the deep learning models. We propose a novel way to\ndeal with the rare and unseen words for the neural network models using\nattention. Our model uses two softmax layers in order to predict the next word\nin conditional language models: one predicts the location of a word in the\nsource sentence, and the other predicts a word in the shortlist vocabulary. At\neach time-step, the decision of which softmax layer to use choose adaptively\nmade by an MLP which is conditioned on the context.~We motivate our work from a\npsychological evidence that humans naturally have a tendency to point towards\nobjects in the context or the environment when the name of an object is not\nknown.~We observe improvements on two tasks, neural machine translation on the\nEuroparl English to French parallel corpora and text summarization on the\nGigaword dataset using our proposed model.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 22:31:57 GMT"}, {"version": "v2", "created": "Sun, 3 Apr 2016 21:12:57 GMT"}, {"version": "v3", "created": "Sun, 21 Aug 2016 20:03:39 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Ahn", "Sungjin", ""], ["Nallapati", "Ramesh", ""], ["Zhou", "Bowen", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1603.08155", "submitter": "Justin Johnson", "authors": "Justin Johnson, Alexandre Alahi, Li Fei-Fei", "title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider image transformation problems, where an input image is\ntransformed into an output image. Recent methods for such problems typically\ntrain feed-forward convolutional neural networks using a \\emph{per-pixel} loss\nbetween the output and ground-truth images. Parallel work has shown that\nhigh-quality images can be generated by defining and optimizing\n\\emph{perceptual} loss functions based on high-level features extracted from\npretrained networks. We combine the benefits of both approaches, and propose\nthe use of perceptual loss functions for training feed-forward networks for\nimage transformation tasks. We show results on image style transfer, where a\nfeed-forward network is trained to solve the optimization problem proposed by\nGatys et al in real-time. Compared to the optimization-based method, our\nnetwork gives similar qualitative results but is three orders of magnitude\nfaster. We also experiment with single-image super-resolution, where replacing\na per-pixel loss with a perceptual loss gives visually pleasing results.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 01:04:27 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Johnson", "Justin", ""], ["Alahi", "Alexandre", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1603.08212", "submitter": "Ethan Fetaya", "authors": "Ita Lifshitz, Ethan Fetaya and Shimon Ullman", "title": "Human Pose Estimation using Deep Consensus Voting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of human pose estimation from a single\nstill image. We propose a novel approach where each location in the image votes\nfor the position of each keypoint using a convolutional neural net. The voting\nscheme allows us to utilize information from the whole image, rather than rely\non a sparse set of keypoint locations. Using dense, multi-target votes, not\nonly produces good keypoint predictions, but also enables us to compute\nimage-dependent joint keypoint probabilities by looking at consensus voting.\nThis differs from most previous methods where joint probabilities are learned\nfrom relative keypoint locations and are independent of the image. We finally\ncombine the keypoints votes and joint probabilities in order to identify the\noptimal pose configuration. We show our competitive performance on the MPII\nHuman Pose and Leeds Sports Pose datasets.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 12:45:33 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Lifshitz", "Ita", ""], ["Fetaya", "Ethan", ""], ["Ullman", "Shimon", ""]]}, {"id": "1603.08233", "submitter": "Randal Olson", "authors": "Randal S. Olson, Jason H. Moore, Christoph Adami", "title": "Evolution of active categorical image classification via saccadic eye\n  movement", "comments": "10 pages, 5 figures, to appear in PPSN 2016 conference proceedings", "journal-ref": "Lecture Notes in Computer Science 9921 (2016) 581-590", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern recognition and classification is a central concern for modern\ninformation processing systems. In particular, one key challenge to image and\nvideo classification has been that the computational cost of image processing\nscales linearly with the number of pixels in the image or video. Here we\npresent an intelligent machine (the \"active categorical classifier,\" or ACC)\nthat is inspired by the saccadic movements of the eye, and is capable of\nclassifying images by selectively scanning only a portion of the image. We\nharness evolutionary computation to optimize the ACC on the MNIST hand-written\ndigit classification task, and provide a proof-of-concept that the ACC works on\nnoisy multi-class data. We further analyze the ACC and demonstrate its ability\nto classify images after viewing only a fraction of the pixels, and provide\ninsight on future research paths to further improve upon the ACC presented\nhere.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 16:36:43 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 21:00:53 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Olson", "Randal S.", ""], ["Moore", "Jason H.", ""], ["Adami", "Christoph", ""]]}, {"id": "1603.08253", "submitter": "Devon Merrill", "authors": "Devon Merrill", "title": "Negative Learning Rates and P-Learning", "comments": "Embarrassingly poor manuscript with many errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method of training a differentiable function approximator for a\nregression task using negative examples. We effect this training using negative\nlearning rates. We also show how this method can be used to perform direct\npolicy learning in a reinforcement learning setting.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 20:02:13 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 11:10:33 GMT"}, {"version": "v3", "created": "Sun, 17 Jun 2018 16:42:04 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Merrill", "Devon", ""]]}, {"id": "1603.08262", "submitter": "Kamil Rocki", "authors": "Kamil Rocki", "title": "Towards Machine Intelligence", "comments": "10 pages, submitted to AGI-16. arXiv admin note: substantial text\n  overlap with arXiv:1512.01926", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a theory of a single general-purpose learning algorithm which\ncould explain the principles of its operation. This theory assumes that the\nbrain has some initial rough architecture, a small library of simple innate\ncircuits which are prewired at birth and proposes that all significant mental\nalgorithms can be learned. Given current understanding and observations, this\npaper reviews and lists the ingredients of such an algorithm from both\narchitectural and functional perspectives.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 22:01:59 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Rocki", "Kamil", ""]]}, {"id": "1603.08293", "submitter": "Feiping Nie", "authors": "Feiping Nie and Heng Huang", "title": "Non-Greedy L21-Norm Maximization for Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is one of the most important unsupervised\nmethods to handle high-dimensional data. However, due to the high computational\ncomplexity of its eigen decomposition solution, it hard to apply PCA to the\nlarge-scale data with high dimensionality. Meanwhile, the squared L2-norm based\nobjective makes it sensitive to data outliers. In recent research, the L1-norm\nmaximization based PCA method was proposed for efficient computation and being\nrobust to outliers. However, this work used a greedy strategy to solve the\neigen vectors. Moreover, the L1-norm maximization based objective may not be\nthe correct robust PCA formulation, because it loses the theoretical connection\nto the minimization of data reconstruction error, which is one of the most\nimportant intuitions and goals of PCA. In this paper, we propose to maximize\nthe L21-norm based robust PCA objective, which is theoretically connected to\nthe minimization of reconstruction error. More importantly, we propose the\nefficient non-greedy optimization algorithms to solve our objective and the\nmore general L21-norm maximization problem with theoretically guaranteed\nconvergence. Experimental results on real world data sets show the\neffectiveness of the proposed method for principal component analysis.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 03:37:26 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Nie", "Feiping", ""], ["Huang", "Heng", ""]]}, {"id": "1603.08296", "submitter": "Evgeny Nikulchev", "authors": "L. Demidova, E. Nikulchev, Yu. Sokolova", "title": "The SVM Classifier Based on the Modified Particle Swarm Optimization", "comments": "9 pages", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications 7 (2016) 16-24", "doi": "10.14569/IJACSA.2016.070203", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of development of the SVM classifier based on the modified\nparticle swarm optimization has been considered. This algorithm carries out the\nsimultaneous search of the kernel function type, values of the kernel function\nparameters and value of the regularization parameter for the SVM classifier.\nSuch SVM classifier provides the high quality of data classification. The idea\nof particles' {\\guillemotleft}regeneration{\\guillemotright} is put on the basis\nof the modified particle swarm optimization algorithm. At the realization of\nthis idea, some particles change their kernel function type to the one which\ncorresponds to the particle with the best value of the classification accuracy.\nThe offered particle swarm optimization algorithm allows reducing the time\nexpenditures for development of the SVM classifier. The results of experimental\nstudies confirm the efficiency of this algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 20:12:44 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Demidova", "L.", ""], ["Nikulchev", "E.", ""], ["Sokolova", "Yu.", ""]]}, {"id": "1603.08318", "submitter": "Xiaojie Guo", "authors": "Xiaojie Guo", "title": "Exclusivity Regularized Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been recognized that the diversity of base learners is of utmost\nimportance to a good ensemble. This paper defines a novel measurement of\ndiversity, termed as exclusivity. With the designed exclusivity, we further\npropose an ensemble model, namely Exclusivity Regularized Machine (ERM), to\njointly suppress the training error of ensemble and enhance the diversity\nbetween bases. Moreover, an Augmented Lagrange Multiplier based algorithm is\ncustomized to effectively and efficiently seek the optimal solution of ERM.\nTheoretical analysis on convergence and global optimality of the proposed\nalgorithm, as well as experiments are provided to reveal the efficacy of our\nmethod and show its superiority over state-of-the-art alternatives in terms of\naccuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 05:58:15 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 07:02:05 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Guo", "Xiaojie", ""]]}, {"id": "1603.08321", "submitter": "Linlin Chao", "authors": "Linlin Chao, Jianhua Tao, Minghao Yang, Ya Li and Zhengqi Wen", "title": "Audio Visual Emotion Recognition with Temporal Alignment and Perception\n  Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on two key problems for audio-visual emotion recognition\nin the video. One is the audio and visual streams temporal alignment for\nfeature level fusion. The other one is locating and re-weighting the perception\nattentions in the whole audio-visual stream for better recognition. The Long\nShort Term Memory Recurrent Neural Network (LSTM-RNN) is employed as the main\nclassification architecture. Firstly, soft attention mechanism aligns the audio\nand visual streams. Secondly, seven emotion embedding vectors, which are\ncorresponding to each classification emotion type, are added to locate the\nperception attentions. The locating and re-weighting process is also based on\nthe soft attention mechanism. The experiment results on EmotiW2015 dataset and\nthe qualitative analysis show the efficiency of the proposed two techniques.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 06:06:10 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Chao", "Linlin", ""], ["Tao", "Jianhua", ""], ["Yang", "Minghao", ""], ["Li", "Ya", ""], ["Wen", "Zhengqi", ""]]}, {"id": "1603.08342", "submitter": "{\\L}ukasz Olech Piotr", "authors": "{\\L}ukasz P. Olech and Mariusz Paradowski", "title": "Hierarchical Gaussian Mixture Model with Objects Attached to Terminal\n  and Non-terminal Dendrogram Nodes", "comments": "This article was presented on CORES2015 conference\n  http://cores.pwr.wroc.pl/ . The final publication is available at Springer\n  via http://dx.doi.org/10.1007/978-3-319-26227-7_18", "journal-ref": "Proceedings of the CORES 2015 conf., pp. 191-201. Springer\n  International Publishing, Cham (2016)", "doi": "10.1007/978-3-319-26227-7_18", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hierarchical clustering algorithm based on Gaussian mixture model is\npresented. The key difference to regular hierarchical mixture models is the\nability to store objects in both terminal and nonterminal nodes. Upper levels\nof the hierarchy contain sparsely distributed objects, while lower levels\ncontain densely represented ones. As it was shown by experiments, this ability\nhelps in noise detection (modelling). Furthermore, compared to regular\nhierarchical mixture model, the presented method generates more compact\ndendrograms with higher quality measured by adopted F-measure.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 08:54:03 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Olech", "\u0141ukasz P.", ""], ["Paradowski", "Mariusz", ""]]}, {"id": "1603.08358", "submitter": "Siddhartha Chandra", "authors": "Siddhartha Chandra and Iasonas Kokkinos", "title": "Fast, Exact and Multi-Scale Inference for Semantic Image Segmentation\n  with Deep Gaussian CRFs", "comments": "Our code is available at https://github.com/siddharthachandra/gcrf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a structured prediction technique that combines the\nvirtues of Gaussian Conditional Random Fields (G-CRF) with Deep Learning: (a)\nour structured prediction task has a unique global optimum that is obtained\nexactly from the solution of a linear system (b) the gradients of our model\nparameters are analytically computed using closed form expressions, in contrast\nto the memory-demanding contemporary deep structured prediction approaches that\nrely on back-propagation-through-time, (c) our pairwise terms do not have to be\nsimple hand-crafted expressions, as in the line of works building on the\nDenseCRF, but can rather be `discovered' from data through deep architectures,\nand (d) out system can trained in an end-to-end manner. Building on standard\ntools from numerical analysis we develop very efficient algorithms for\ninference and learning, as well as a customized technique adapted to the\nsemantic segmentation task. This efficiency allows us to explore more\nsophisticated architectures for structured prediction in deep learning: we\nintroduce multi-resolution architectures to couple information across scales in\na joint optimization framework, yielding systematic improvements. We\ndemonstrate the utility of our approach on the challenging VOC PASCAL 2012\nimage segmentation benchmark, showing substantial improvements over strong\nbaselines. We make all of our code and experiments available at\n{https://github.com/siddharthachandra/gcrf}\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 10:55:20 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 17:50:46 GMT"}, {"version": "v3", "created": "Sat, 26 Nov 2016 10:43:11 GMT"}, {"version": "v4", "created": "Tue, 29 Nov 2016 14:52:20 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Chandra", "Siddhartha", ""], ["Kokkinos", "Iasonas", ""]]}, {"id": "1603.08367", "submitter": "Markus Thom", "authors": "Markus Thom and G\\\"unther Palm", "title": "Sparse Activity and Sparse Connectivity in Supervised Learning", "comments": "See http://jmlr.org/papers/v14/thom13a.html for the authoritative\n  version", "journal-ref": "Journal of Machine Learning Research, vol. 14, pp. 1091-1143, 2013", "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparseness is a useful regularizer for learning in a wide range of\napplications, in particular in neural networks. This paper proposes a model\ntargeted at classification tasks, where sparse activity and sparse connectivity\nare used to enhance classification capabilities. The tool for achieving this is\na sparseness-enforcing projection operator which finds the closest vector with\na pre-defined sparseness for any given vector. In the theoretical part of this\npaper, a comprehensive theory for such a projection is developed. In\nconclusion, it is shown that the projection is differentiable almost everywhere\nand can thus be implemented as a smooth neuronal transfer function. The entire\nmodel can hence be tuned end-to-end using gradient-based methods. Experiments\non the MNIST database of handwritten digits show that classification\nperformance can be boosted by sparse activity or sparse connectivity. With a\ncombination of both, performance can be significantly better compared to\nclassical non-sparse approaches.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 12:06:49 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Thom", "Markus", ""], ["Palm", "G\u00fcnther", ""]]}, {"id": "1603.08474", "submitter": "Oswaldo Ludwig", "authors": "Oswaldo Ludwig, Xiao Liu, Parisa Kordjamshidi, Marie-Francine Moens", "title": "Deep Embedding for Spatial Role Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the visually informed embedding of word (VIEW), a\ncontinuous vector representation for a word extracted from a deep neural model\ntrained using the Microsoft COCO data set to forecast the spatial arrangements\nbetween visual objects, given a textual description. The model is composed of a\ndeep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory\n(LSTM) network, the latter being preceded by an embedding layer. The VIEW is\napplied to transferring multimodal background knowledge to Spatial Role\nLabeling (SpRL) algorithms, which recognize spatial relations between objects\nmentioned in the text. This work also contributes with a new method to select\ncomplementary features and a fine-tuning method for MLP that improves the $F1$\nmeasure in classifying the words into spatial roles. The VIEW is evaluated with\nthe Task 3 of SemEval-2013 benchmark data set, SpaceEval.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 18:38:46 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Ludwig", "Oswaldo", ""], ["Liu", "Xiao", ""], ["Kordjamshidi", "Parisa", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "1603.08482", "submitter": "Sida Wang", "authors": "Sida I. Wang and Arun Tejasvi Chaganty and Percy Liang", "title": "Estimating Mixture Models via Mixtures of Polynomials", "comments": "NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture modeling is a general technique for making any simple model more\nexpressive through weighted combination. This generality and simplicity in part\nexplains the success of the Expectation Maximization (EM) algorithm, in which\nupdates are easy to derive for a wide class of mixture models. However, the\nlikelihood of a mixture model is non-convex, so EM has no known global\nconvergence guarantees. Recently, method of moments approaches offer global\nguarantees for some mixture models, but they do not extend easily to the range\nof mixture models that exist. In this work, we present Polymom, an unifying\nframework based on method of moments in which estimation procedures are easily\nderivable, just as in EM. Polymom is applicable when the moments of a single\nmixture component are polynomials of the parameters. Our key observation is\nthat the moments of the mixture model are a mixture of these polynomials, which\nallows us to cast estimation as a Generalized Moment Problem. We solve its\nrelaxations using semidefinite optimization, and then extract parameters using\nideas from computer algebra. This framework allows us to draw insights and\napply tools from convex optimization, computer algebra and the theory of\nmoments to study problems in statistical estimation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 18:55:02 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Wang", "Sida I.", ""], ["Chaganty", "Arun Tejasvi", ""], ["Liang", "Percy", ""]]}, {"id": "1603.08561", "submitter": "Ishan Misra", "authors": "Ishan Misra and C. Lawrence Zitnick and Martial Hebert", "title": "Shuffle and Learn: Unsupervised Learning using Temporal Order\n  Verification", "comments": "Accepted at ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an approach for learning a visual representation\nfrom the raw spatiotemporal signals in videos. Our representation is learned\nwithout supervision from semantic labels. We formulate our method as an\nunsupervised sequential verification task, i.e., we determine whether a\nsequence of frames from a video is in the correct temporal order. With this\nsimple task and no semantic labels, we learn a powerful visual representation\nusing a Convolutional Neural Network (CNN). The representation contains\ncomplementary information to that learned from supervised image datasets like\nImageNet. Qualitative results show that our method captures information that is\ntemporally varying, such as human pose. When used as pre-training for action\nrecognition, our method gives significant gains over learning without external\ndata on benchmark datasets like UCF101 and HMDB51. To demonstrate its\nsensitivity to human pose, we show results for pose estimation on the FLIC and\nMPII datasets that are competitive, or better than approaches using\nsignificantly more supervision. Our method can be combined with supervised\nrepresentations to provide an additional boost in accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 21:00:43 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 17:26:01 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Misra", "Ishan", ""], ["Zitnick", "C. Lawrence", ""], ["Hebert", "Martial", ""]]}, {"id": "1603.08575", "submitter": "Th\\'eophane  Weber", "authors": "S. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David\n  Szepesvari, Koray Kavukcuoglu and Geoffrey E. Hinton", "title": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for efficient inference in structured image models\nthat explicitly reason about objects. We achieve this by performing\nprobabilistic inference using a recurrent neural network that attends to scene\nelements and processes them one at a time. Crucially, the model itself learns\nto choose the appropriate number of inference steps. We use this scheme to\nlearn to perform inference in partially specified 2D models (variable-sized\nvariational auto-encoders) and fully specified 3D models (probabilistic\nrenderers). We show that such models learn to identify multiple objects -\ncounting, locating and classifying the elements of a scene - without any\nsupervision, e.g., decomposing 3D images with various numbers of objects in a\nsingle forward pass of a neural network. We further show that the networks\nproduce accurate inferences when compared to supervised counterparts, and that\ntheir structure leads to improved generalization.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 21:59:08 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2016 06:27:26 GMT"}, {"version": "v3", "created": "Fri, 12 Aug 2016 16:05:08 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Eslami", "S. M. Ali", ""], ["Heess", "Nicolas", ""], ["Weber", "Theophane", ""], ["Tassa", "Yuval", ""], ["Szepesvari", "David", ""], ["Kavukcuoglu", "Koray", ""], ["Hinton", "Geoffrey E.", ""]]}, {"id": "1603.08604", "submitter": "Matthew Dixon", "authors": "Matthew Dixon, Diego Klabjan, Jin Hoon Bang", "title": "Classification-based Financial Markets Prediction using Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are powerful types of artificial neural networks\n(ANNs) that use several hidden layers. They have recently gained considerable\nattention in the speech transcription and image recognition community\n(Krizhevsky et al., 2012) for their superior predictive properties including\nrobustness to overfitting. However their application to algorithmic trading has\nnot been previously researched, partly because of their computational\ncomplexity. This paper describes the application of DNNs to predicting\nfinancial market movement directions. In particular we describe the\nconfiguration and training approach and then demonstrate their application to\nbacktesting a simple trading strategy over 43 different Commodity and FX future\nmid-prices at 5-minute intervals. All results in this paper are generated using\na C++ implementation on the Intel Xeon Phi co-processor which is 11.4x faster\nthan the serial version and a Python strategy backtesting environment both of\nwhich are available as open source code written by the authors.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 01:26:04 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 19:49:53 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Dixon", "Matthew", ""], ["Klabjan", "Diego", ""], ["Bang", "Jin Hoon", ""]]}, {"id": "1603.08616", "submitter": "Lin Chen", "authors": "Lin Chen, Forrest W Crawford, Amin Karbasi", "title": "Submodular Variational Inference for Network Reconstruction", "comments": "Accepted for UAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world and online social networks, individuals receive and transmit\ninformation in real time. Cascading information transmissions (e.g. phone\ncalls, text messages, social media posts) may be understood as a realization of\na diffusion process operating on the network, and its branching path can be\nrepresented by a directed tree. The process only traverses and thus reveals a\nlimited portion of the edges. The network reconstruction/inference problem is\nto infer the unrevealed connections. Most existing approaches derive a\nlikelihood and attempt to find the network topology maximizing the likelihood,\na problem that is highly intractable. In this paper, we focus on the network\nreconstruction problem for a broad class of real-world diffusion processes,\nexemplified by a network diffusion scheme called respondent-driven sampling\n(RDS). We prove that under realistic and general models of network diffusion,\nthe posterior distribution of an observed RDS realization is a Bayesian\nlog-submodular model.We then propose VINE (Variational Inference for Network\nrEconstruction), a novel, accurate, and computationally efficient variational\ninference algorithm, for the network reconstruction problem under this model.\nCrucially, we do not assume any particular probabilistic model for the\nunderlying network. VINE recovers any connected graph with high accuracy as\nshown by our experimental results on real-life networks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 02:13:17 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 06:58:29 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Chen", "Lin", ""], ["Crawford", "Forrest W", ""], ["Karbasi", "Amin", ""]]}, {"id": "1603.08661", "submitter": "Tor Lattimore", "authors": "Tor Lattimore", "title": "Regret Analysis of the Anytime Optimally Confident UCB Algorithm", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I introduce and analyse an anytime version of the Optimally Confident UCB\n(OCUCB) algorithm designed for minimising the cumulative regret in finite-armed\nstochastic bandits with subgaussian noise. The new algorithm is simple,\nintuitive (in hindsight) and comes with the strongest finite-time regret\nguarantees for a horizon-free algorithm so far. I also show a finite-time lower\nbound that nearly matches the upper bound.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 07:12:14 GMT"}, {"version": "v2", "created": "Fri, 6 May 2016 19:06:26 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Lattimore", "Tor", ""]]}, {"id": "1603.08704", "submitter": "Seyed Mostafa Kia", "authors": "Seyed Mostafa Kia", "title": "Interpretability of Multivariate Brain Maps in Brain Decoding:\n  Definition and Quantification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain decoding is a popular multivariate approach for hypothesis testing in\nneuroimaging. It is well known that the brain maps derived from weights of\nlinear classifiers are hard to interpret because of high correlations between\npredictors, low signal to noise ratios, and the high dimensionality of\nneuroimaging data. Therefore, improving the interpretability of brain decoding\napproaches is of primary interest in many neuroimaging studies. Despite\nextensive studies of this type, at present, there is no formal definition for\ninterpretability of multivariate brain maps. As a consequence, there is no\nquantitative measure for evaluating the interpretability of different brain\ndecoding methods. In this paper, first, we present a theoretical definition of\ninterpretability in brain decoding; we show that the interpretability of\nmultivariate brain maps can be decomposed into their reproducibility and\nrepresentativeness. Second, as an application of the proposed theoretical\ndefinition, we formalize a heuristic method for approximating the\ninterpretability of multivariate brain maps in a binary magnetoencephalography\n(MEG) decoding scenario. Third, we propose to combine the approximated\ninterpretability and the performance of the brain decoding model into a new\nmulti-objective criterion for model selection. Our results for the MEG data\nshow that optimizing the hyper-parameters of the regularized linear classifier\nbased on the proposed criterion results in more informative multivariate brain\nmaps. More importantly, the presented definition provides the theoretical\nbackground for quantitative evaluation of interpretability, and hence,\nfacilitates the development of more effective brain decoding algorithms in the\nfuture.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 10:04:07 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Kia", "Seyed Mostafa", ""]]}, {"id": "1603.08767", "submitter": "Daniel Pop", "authors": "Daniel Pop", "title": "Machine Learning and Cloud Computing: Survey of Distributed and SaaS\n  Solutions", "comments": "This manuscript was originally published as IEAT Technical Report at\n  https://www.ieat.ro/technical-reports in 2012", "journal-ref": null, "doi": null, "report-no": "IEAT-TR-2012-1", "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Applying popular machine learning algorithms to large amounts of data raised\nnew challenges for the ML practitioners. Traditional ML libraries does not\nsupport well processing of huge datasets, so that new approaches were needed.\nParallelization using modern parallel computing frameworks, such as MapReduce,\nCUDA, or Dryad gained in popularity and acceptance, resulting in new ML\nlibraries developed on top of these frameworks. We will briefly introduce the\nmost prominent industrial and academic outcomes, such as Apache Mahout,\nGraphLab or Jubatus.\n  We will investigate how cloud computing paradigm impacted the field of ML.\nFirst direction is of popular statistics tools and libraries (R system, Python)\ndeployed in the cloud. A second line of products is augmenting existing tools\nwith plugins that allow users to create a Hadoop cluster in the cloud and run\njobs on it. Next on the list are libraries of distributed implementations for\nML algorithms, and on-premise deployments of complex systems for data analytics\nand data mining. Last approach on the radar of this survey is ML as\nSoftware-as-a-Service, several BigData start-ups (and large companies as well)\nalready opening their solutions to the market.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 13:29:35 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Pop", "Daniel", ""]]}, {"id": "1603.08815", "submitter": "Dustin Tran", "authors": "Dustin Tran, Minjae Kim, Finale Doshi-Velez", "title": "Spectral M-estimation with Applications to Hidden Markov Models", "comments": "Appears in Artificial Intelligence and Statistics, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Method of moment estimators exhibit appealing statistical properties, such as\nasymptotic unbiasedness, for nonconvex problems. However, they typically\nrequire a large number of samples and are extremely sensitive to model\nmisspecification. In this paper, we apply the framework of M-estimation to\ndevelop both a generalized method of moments procedure and a principled method\nfor regularization. Our proposed M-estimator obtains optimal sample efficiency\nrates (in the class of moment-based estimators) and the same well-known rates\non prediction accuracy as other spectral estimators. It also makes it\nstraightforward to incorporate regularization into the sample moment\nconditions. We demonstrate empirically the gains in sample efficiency from our\napproach on hidden Markov models.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 15:34:29 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Tran", "Dustin", ""], ["Kim", "Minjae", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1603.08831", "submitter": "Fabio Massimo Zennaro", "authors": "Fabio Massimo Zennaro, Ke Chen", "title": "Towards Understanding Sparse Filtering: A Theoretical Perspective", "comments": "54 pages, 9 figures", "journal-ref": null, "doi": "10.1016/j.neunet.2017.11.010", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a theoretical analysis to understand sparse\nfiltering, a recent and effective algorithm for unsupervised learning. The aim\nof this research is not to show whether or how well sparse filtering works, but\nto understand why and when sparse filtering does work. We provide a thorough\ntheoretical analysis of sparse filtering and its properties, and further offer\nan experimental validation of the main outcomes of our theoretical analysis. We\nshow that sparse filtering works by explicitly maximizing the entropy of the\nlearned representation through the maximization of the proxy of sparsity, and\nby implicitly preserving mutual information between original and learned\nrepresentations through the constraint of preserving a structure of the data,\nspecifically the structure defined by relations of neighborhoodness under the\ncosine distance. Furthermore, we empirically validate our theoretical results\nwith artificial and real data sets, and we apply our theoretical understanding\nto explain the success of sparse filtering on real-world problems. Our work\nprovides a strong theoretical basis for understanding sparse filtering: it\nhighlights assumptions and conditions for success behind this feature\ndistribution learning algorithm, and provides insights for developing new\nfeature distribution learning algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 16:23:32 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 21:34:44 GMT"}, {"version": "v3", "created": "Thu, 10 Aug 2017 22:57:16 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zennaro", "Fabio Massimo", ""], ["Chen", "Ke", ""]]}, {"id": "1603.08861", "submitter": "Zhilin Yang", "authors": "Zhilin Yang and William W. Cohen and Ruslan Salakhutdinov", "title": "Revisiting Semi-Supervised Learning with Graph Embeddings", "comments": "ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a semi-supervised learning framework based on graph embeddings.\nGiven a graph between instances, we train an embedding for each instance to\njointly predict the class label and the neighborhood context in the graph. We\ndevelop both transductive and inductive variants of our method. In the\ntransductive variant of our method, the class labels are determined by both the\nlearned embeddings and input feature vectors, while in the inductive variant,\nthe embeddings are defined as a parametric function of the feature vectors, so\npredictions can be made on instances not seen during training. On a large and\ndiverse set of benchmark tasks, including text classification, distantly\nsupervised entity extraction, and entity classification, we show improved\nperformance over many of the existing models.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 17:46:16 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 23:57:09 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Yang", "Zhilin", ""], ["Cohen", "William W.", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1603.08981", "submitter": "Shuang Li", "authors": "Shuang Li, Yao Xie, Mehrdad Farajtabar, Apurv Verma, and Le Song", "title": "Detecting weak changes in dynamic events over networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large volume of networked streaming event data are becoming increasingly\navailable in a wide variety of applications, such as social network analysis,\nInternet traffic monitoring and healthcare analytics. Streaming event data are\ndiscrete observation occurred in continuous time, and the precise time interval\nbetween two events carries a great deal of information about the dynamics of\nthe underlying systems. How to promptly detect changes in these dynamic systems\nusing these streaming event data? In this paper, we propose a novel\nchange-point detection framework for multi-dimensional event data over\nnetworks. We cast the problem into sequential hypothesis test, and derive the\nlikelihood ratios for point processes, which are computed efficiently via an\nEM-like algorithm that is parameter-free and can be computed in a distributed\nfashion. We derive a highly accurate theoretical characterization of the\nfalse-alarm-rate, and show that it can achieve weak signal detection by\naggregating local statistics over time and networks. Finally, we demonstrate\nthe good performance of our algorithm on numerical examples and real-world\ndatasets from twitter and Memetracker.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 21:54:56 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 20:09:56 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Li", "Shuang", ""], ["Xie", "Yao", ""], ["Farajtabar", "Mehrdad", ""], ["Verma", "Apurv", ""], ["Song", "Le", ""]]}, {"id": "1603.08988", "submitter": "Yusuf Bugra Erol", "authors": "Yusuf Bugra Erol, Yi Wu, Lei Li, Stuart Russell", "title": "Towards Practical Bayesian Parameter and State Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint state and parameter estimation is a core problem for dynamic Bayesian\nnetworks. Although modern probabilistic inference toolkits make it relatively\neasy to specify large and practically relevant probabilistic models, the silver\nbullet---an efficient and general online inference algorithm for such\nproblems---remains elusive, forcing users to write special-purpose code for\neach application. We propose a novel blackbox algorithm -- a hybrid of particle\nfiltering for state variables and assumed density filtering for parameter\nvariables. It has following advantages: (a) it is efficient due to its online\nnature, and (b) it is applicable to both discrete and continuous parameter\nspaces . On a variety of toy and real models, our system is able to generate\nmore accurate results within a fixed computation budget. This preliminary\nevidence indicates that the proposed approach is likely to be of practical use.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 22:41:17 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Erol", "Yusuf Bugra", ""], ["Wu", "Yi", ""], ["Li", "Lei", ""], ["Russell", "Stuart", ""]]}, {"id": "1603.09000", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Andrea Montanari", "title": "Online Rules for Control of False Discovery Rate and False Discovery\n  Exceedance", "comments": "44 pages, 9 figures, to appear in Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple hypothesis testing is a core problem in statistical inference and\narises in almost every scientific field. Given a set of null hypotheses\n$\\mathcal{H}(n) = (H_1,\\dotsc, H_n)$, Benjamini and Hochberg introduced the\nfalse discovery rate (FDR), which is the expected proportion of false positives\namong rejected null hypotheses, and proposed a testing procedure that controls\nFDR below a pre-assigned significance level. Nowadays FDR is the criterion of\nchoice for large scale multiple hypothesis testing. In this paper we consider\nthe problem of controlling FDR in an \"online manner\". Concretely, we consider\nan ordered --possibly infinite-- sequence of null hypotheses $\\mathcal{H} =\n(H_1,H_2,H_3,\\dots )$ where, at each step $i$, the statistician must decide\nwhether to reject hypothesis $H_i$ having access only to the previous\ndecisions. This model was introduced by Foster and Stine. We study a class of\n\"generalized alpha-investing\" procedures and prove that any rule in this class\ncontrols online FDR, provided $p$-values corresponding to true nulls are\nindependent from the other $p$-values. (Earlier work only established mFDR\ncontrol.) Next, we obtain conditions under which generalized alpha-investing\ncontrols FDR in the presence of general $p$-values dependencies. Finally, we\ndevelop a modified set of procedures that also allow to control the false\ndiscovery exceedance (the tail of the proportion of false discoveries).\nNumerical simulations and analytical results indicate that online procedures do\nnot incur a large loss in statistical power with respect to offline approaches,\nsuch as Benjamini-Hochberg.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 23:41:51 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 23:49:37 GMT"}, {"version": "v3", "created": "Thu, 6 Jul 2017 23:37:43 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Javanmard", "Adel", ""], ["Montanari", "Andrea", ""]]}, {"id": "1603.09025", "submitter": "Tim Cooijmans", "authors": "Tim Cooijmans, Nicolas Ballas, C\\'esar Laurent, \\c{C}a\\u{g}lar\n  G\\\"ul\\c{c}ehre and Aaron Courville", "title": "Recurrent Batch Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a reparameterization of LSTM that brings the benefits of batch\nnormalization to recurrent neural networks. Whereas previous works only apply\nbatch normalization to the input-to-hidden transformation of RNNs, we\ndemonstrate that it is both possible and beneficial to batch-normalize the\nhidden-to-hidden transition, thereby reducing internal covariate shift between\ntime steps. We evaluate our proposal on various sequential problems such as\nsequence classification, language modeling and question answering. Our\nempirical results show that our batch-normalized LSTM consistently leads to\nfaster convergence and improved generalization.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 02:57:20 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 20:45:03 GMT"}, {"version": "v3", "created": "Mon, 4 Apr 2016 17:16:08 GMT"}, {"version": "v4", "created": "Tue, 12 Apr 2016 19:23:44 GMT"}, {"version": "v5", "created": "Tue, 28 Feb 2017 00:59:42 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Cooijmans", "Tim", ""], ["Ballas", "Nicolas", ""], ["Laurent", "C\u00e9sar", ""], ["G\u00fcl\u00e7ehre", "\u00c7a\u011flar", ""], ["Courville", "Aaron", ""]]}, {"id": "1603.09035", "submitter": "Ignacio Cano", "authors": "Ignacio Cano, Markus Weimer, Dhruv Mahajan, Carlo Curino and Giovanni\n  Matteo Fumarola", "title": "Towards Geo-Distributed Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latency to end-users and regulatory requirements push large companies to\nbuild data centers all around the world. The resulting data is \"born\"\ngeographically distributed. On the other hand, many machine learning\napplications require a global view of such data in order to achieve the best\nresults. These types of applications form a new class of learning problems,\nwhich we call Geo-Distributed Machine Learning (GDML). Such applications need\nto cope with: 1) scarce and expensive cross-data center bandwidth, and 2)\ngrowing privacy concerns that are pushing for stricter data sovereignty\nregulations. Current solutions to learning from geo-distributed data sources\nrevolve around the idea of first centralizing the data in one data center, and\nthen training locally. As machine learning algorithms are\ncommunication-intensive, the cost of centralizing the data is thought to be\noffset by the lower cost of intra-data center communication during training. In\nthis work, we show that the current centralized practice can be far from\noptimal, and propose a system for doing geo-distributed training. Furthermore,\nwe argue that the geo-distributed approach is structurally more amenable to\ndealing with regulatory constraints, as raw data never leaves the source data\ncenter. Our empirical evaluation on three real datasets confirms the general\nvalidity of our approach, and shows that GDML is not only possible but also\nadvisable in many scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 04:05:29 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Cano", "Ignacio", ""], ["Weimer", "Markus", ""], ["Mahajan", "Dhruv", ""], ["Curino", "Carlo", ""], ["Fumarola", "Giovanni Matteo", ""]]}, {"id": "1603.09048", "submitter": "Kuan-Hao Huang", "authors": "Kuan-Hao Huang and Hsuan-Tien Lin", "title": "Cost-Sensitive Label Embedding for Multi-Label Classification", "comments": null, "journal-ref": null, "doi": "10.1007/s10994-017-5659-z", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label embedding (LE) is an important family of multi-label classification\nalgorithms that digest the label information jointly for better performance.\nDifferent real-world applications evaluate performance by different cost\nfunctions of interest. Current LE algorithms often aim to optimize one specific\ncost function, but they can suffer from bad performance with respect to other\ncost functions. In this paper, we resolve the performance issue by proposing a\nnovel cost-sensitive LE algorithm that takes the cost function of interest into\naccount. The proposed algorithm, cost-sensitive label embedding with\nmultidimensional scaling (CLEMS), approximates the cost information with the\ndistances of the embedded vectors by using the classic multidimensional scaling\napproach for manifold learning. CLEMS is able to deal with both symmetric and\nasymmetric cost functions, and effectively makes cost-sensitive decisions by\nnearest-neighbor decoding within the embedded vectors. We derive theoretical\nresults that justify how CLEMS achieves the desired cost-sensitivity.\nFurthermore, extensive experimental results demonstrate that CLEMS is\nsignificantly better than a wide spectrum of existing LE algorithms and\nstate-of-the-art cost-sensitive algorithms across different cost functions.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 06:19:02 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 06:08:14 GMT"}, {"version": "v3", "created": "Wed, 1 Jun 2016 08:05:10 GMT"}, {"version": "v4", "created": "Tue, 11 Jul 2017 04:45:25 GMT"}, {"version": "v5", "created": "Sat, 5 Aug 2017 04:05:40 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Huang", "Kuan-Hao", ""], ["Lin", "Hsuan-Tien", ""]]}, {"id": "1603.09050", "submitter": "Nguyen Viet Cuong", "authors": "Nguyen Viet Cuong, Nan Ye, Wee Sun Lee", "title": "Robustness of Bayesian Pool-based Active Learning Against Prior\n  Misspecification", "comments": "This paper is published at AAAI Conference on Artificial Intelligence\n  (AAAI 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the robustness of active learning (AL) algorithms against prior\nmisspecification: whether an algorithm achieves similar performance using a\nperturbed prior as compared to using the true prior. In both the average and\nworst cases of the maximum coverage setting, we prove that all\n$\\alpha$-approximate algorithms are robust (i.e., near $\\alpha$-approximate) if\nthe utility is Lipschitz continuous in the prior. We further show that\nrobustness may not be achieved if the utility is non-Lipschitz. This suggests\nwe should use a Lipschitz utility for AL if robustness is required. For the\nminimum cost setting, we can also obtain a robustness result for approximate AL\nalgorithms. Our results imply that many commonly used AL algorithms are robust\nagainst perturbed priors. We then propose the use of a mixture prior to\nalleviate the problem of prior misspecification. We analyze the robustness of\nthe uniform mixture prior and show experimentally that it performs reasonably\nwell in practice.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 06:21:42 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Cuong", "Nguyen Viet", ""], ["Ye", "Nan", ""], ["Lee", "Wee Sun", ""]]}, {"id": "1603.09064", "submitter": "Edith Cohen", "authors": "Edith Cohen", "title": "Semi-Supervised Learning on Graphs through Reach and Distance Diffusion", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning (SSL) is an indispensable tool when there are few\nlabeled entities and many unlabeled entities for which we want to predict\nlabels. With graph-based methods, entities correspond to nodes in a graph and\nedges represent strong relations. At the heart of SSL algorithms is the\nspecification of a dense {\\em kernel} of pairwise affinity values from the\ngraph structure. A learning algorithm is then trained on the kernel together\nwith labeled entities. The most popular kernels are {\\em spectral} and include\nthe highly scalable \"symmetric\" Laplacian methods, that compute a soft labels\nusing Jacobi iterations, and \"asymmetric\" methods including Personalized Page\nRank (PPR) which use short random walks and apply with directed relations, such\nas like, follow, or hyperlinks.\n  We introduce {\\em Reach diffusion} and {\\em Distance diffusion} kernels that\nbuild on powerful social and economic models of centrality and influence in\nnetworks and capture the directed pairwise relations that underline social\ninfluence. Inspired by the success of social influence as an alternative to\nspectral centrality such as Page Rank, we explore SSL with our kernels and\ndevelop highly scalable algorithms for parameter setting, label learning, and\nsampling. We perform preliminary experiments that demonstrate the properties\nand potential of our kernels.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 07:51:58 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 18:21:01 GMT"}, {"version": "v3", "created": "Sat, 13 Aug 2016 05:57:56 GMT"}, {"version": "v4", "created": "Mon, 26 Sep 2016 07:08:42 GMT"}, {"version": "v5", "created": "Fri, 20 Jan 2017 18:34:52 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Cohen", "Edith", ""]]}, {"id": "1603.09123", "submitter": "Byunghan Lee", "authors": "Byunghan Lee, Junghwan Baek, Seunghyun Park, and Sungroh Yoon", "title": "deepTarget: End-to-end Learning Framework for microRNA Target Prediction\n  using Deep Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MicroRNAs (miRNAs) are short sequences of ribonucleic acids that control the\nexpression of target messenger RNAs (mRNAs) by binding them. Robust prediction\nof miRNA-mRNA pairs is of utmost importance in deciphering gene regulations but\nhas been challenging because of high false positive rates, despite a deluge of\ncomputational tools that normally require laborious manual feature extraction.\nThis paper presents an end-to-end machine learning framework for miRNA target\nprediction. Leveraged by deep recurrent neural networks-based auto-encoding and\nsequence-sequence interaction learning, our approach not only delivers an\nunprecedented level of accuracy but also eliminates the need for manual feature\nextraction. The performance gap between the proposed method and existing\nalternatives is substantial (over 25% increase in F-measure), and deepTarget\ndelivers a quantum leap in the long-standing challenge of robust miRNA target\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 10:59:36 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 07:43:11 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Lee", "Byunghan", ""], ["Baek", "Junghwan", ""], ["Park", "Seunghyun", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1603.09128", "submitter": "Simon \\v{S}uster", "authors": "Simon \\v{S}uster and Ivan Titov and Gertjan van Noord", "title": "Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders", "comments": "11 pages, to appear at NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to learning multi-sense word embeddings relying both\non monolingual and bilingual information. Our model consists of an encoder,\nwhich uses monolingual and bilingual context (i.e. a parallel sentence) to\nchoose a sense for a given word, and a decoder which predicts context words\nbased on the chosen sense. The two components are estimated jointly. We observe\nthat the word representations induced from bilingual data outperform the\nmonolingual counterparts across a range of evaluation tasks, even though\ncrosslingual information is not available at test time.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 11:09:01 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["\u0160uster", "Simon", ""], ["Titov", "Ivan", ""], ["van Noord", "Gertjan", ""]]}, {"id": "1603.09170", "submitter": "Bin Wang", "authors": "Bin Wang, Zhijian Ou, Yong He, Akinori Kawamura", "title": "Model Interpolation with Trans-dimensional Random Field Language Models\n  for Speech Recognition", "comments": "three pages, 2 experiment result tables, reporting the WERs on an\n  Englisth dateset and a Chinese dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant language models (LMs) such as n-gram and neural network (NN)\nmodels represent sentence probabilities in terms of conditionals. In contrast,\na new trans-dimensional random field (TRF) LM has been recently introduced to\nshow superior performances, where the whole sentence is modeled as a random\nfield. In this paper, we examine how the TRF models can be interpolated with\nthe NN models, and obtain 12.1\\% and 17.9\\% relative error rate reductions over\n6-gram LMs for English and Chinese speech recognition respectively through\nlog-linear combination.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 13:09:20 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 13:19:06 GMT"}, {"version": "v3", "created": "Tue, 26 Jul 2016 02:36:33 GMT"}, {"version": "v4", "created": "Wed, 17 Aug 2016 01:54:32 GMT"}, {"version": "v5", "created": "Sat, 20 Aug 2016 03:50:16 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Wang", "Bin", ""], ["Ou", "Zhijian", ""], ["He", "Yong", ""], ["Kawamura", "Akinori", ""]]}, {"id": "1603.09233", "submitter": "Rahul Meshram", "authors": "Rahul Meshram, Aditya Gopalan and D. Manjunath", "title": "Optimal Recommendation to Users that React: Online Learning for a Class\n  of POMDPs", "comments": "8 pages, submitted to conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and study a model for an Automated Online Recommendation System\n(AORS) in which a user's preferences can be time-dependent and can also depend\non the history of past recommendations and play-outs. The three key features of\nthe model that makes it more realistic compared to existing models for\nrecommendation systems are (1) user preference is inherently latent, (2)\ncurrent recommendations can affect future preferences, and (3) it allows for\nthe development of learning algorithms with provable performance guarantees.\nThe problem is cast as an average-cost restless multi-armed bandit for a given\nuser, with an independent partially observable Markov decision process (POMDP)\nfor each item of content. We analyze the POMDP for a single arm, describe its\nstructural properties, and characterize its optimal policy. We then develop a\nThompson sampling-based online reinforcement learning algorithm to learn the\nparameters of the model and optimize utility from the binary responses of the\nusers to continuous recommendations. We then analyze the performance of the\nlearning algorithm and characterize the regret. Illustrative numerical results\nand directions for extension to the restless hidden Markov multi-armed bandit\nproblem are also presented.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 14:58:32 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Meshram", "Rahul", ""], ["Gopalan", "Aditya", ""], ["Manjunath", "D.", ""]]}, {"id": "1603.09260", "submitter": "Vladimir Jojic", "authors": "Tianxiang Gao and Vladimir Jojic", "title": "Degrees of Freedom in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore degrees of freedom in deep sigmoidal neural\nnetworks. We show that the degrees of freedom in these models is related to the\nexpected optimism, which is the expected difference between test error and\ntraining error. We provide an efficient Monte-Carlo method to estimate the\ndegrees of freedom for multi-class classification methods. We show degrees of\nfreedom are lower than the parameter count in a simple XOR network. We extend\nthese results to neural nets trained on synthetic and real data, and\ninvestigate impact of network's architecture and different regularization\nchoices. The degrees of freedom in deep networks are dramatically smaller than\nthe number of parameters, in some real datasets several orders of magnitude.\nFurther, we observe that for fixed number of parameters, deeper networks have\nless degrees of freedom exhibiting a regularization-by-depth.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 16:16:57 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 14:45:35 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Gao", "Tianxiang", ""], ["Jojic", "Vladimir", ""]]}, {"id": "1603.09381", "submitter": "Peng Li", "authors": "Peng Li and Heng Huang", "title": "Clinical Information Extraction via Convolutional Neural Network", "comments": "arXiv admin note: text overlap with arXiv:1408.5882 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report an implementation of a clinical information extraction tool that\nleverages deep neural network to annotate event spans and their attributes from\nraw clinical notes and pathology reports. Our approach uses context words and\ntheir part-of-speech tags and shape information as features. Then we hire\ntemporal (1D) convolutional neural network to learn hidden feature\nrepresentations. Finally, we use Multilayer Perceptron (MLP) to predict event\nspans. The empirical evaluation demonstrates that our approach significantly\noutperforms baselines.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 20:57:07 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Li", "Peng", ""], ["Huang", "Heng", ""]]}, {"id": "1603.09382", "submitter": "Yu Sun", "authors": "Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Weinberger", "title": "Deep Networks with Stochastic Depth", "comments": "first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very deep convolutional networks with hundreds of layers have led to\nsignificant reductions in error on competitive benchmarks. Although the\nunmatched expressiveness of the many layers can be highly desirable at test\ntime, training very deep networks comes with its own set of challenges. The\ngradients can vanish, the forward flow often diminishes, and the training time\ncan be painfully slow. To address these problems, we propose stochastic depth,\na training procedure that enables the seemingly contradictory setup to train\nshort networks and use deep networks at test time. We start with very deep\nnetworks but during training, for each mini-batch, randomly drop a subset of\nlayers and bypass them with the identity function. This simple approach\ncomplements the recent success of residual networks. It reduces training time\nsubstantially and improves the test error significantly on almost all data sets\nthat we used for evaluation. With stochastic depth we can increase the depth of\nresidual networks even beyond 1200 layers and still yield meaningful\nimprovements in test error (4.91% on CIFAR-10).\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 20:58:07 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 18:42:37 GMT"}, {"version": "v3", "created": "Thu, 28 Jul 2016 23:24:16 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Huang", "Gao", ""], ["Sun", "Yu", ""], ["Liu", "Zhuang", ""], ["Sedra", "Daniel", ""], ["Weinberger", "Kilian", ""]]}, {"id": "1603.09420", "submitter": "Jianxin Wu", "authors": "Guo-Bing Zhou and Jianxin Wu and Chen-Lin Zhang and Zhi-Hua Zhou", "title": "Minimal Gated Unit for Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently recurrent neural networks (RNN) has been very successful in handling\nsequence data. However, understanding RNN and finding the best practices for\nRNN is a difficult task, partly because there are many competing and complex\nhidden units (such as LSTM and GRU). We propose a gated unit for RNN, named as\nMinimal Gated Unit (MGU), since it only contains one gate, which is a minimal\ndesign among all gated hidden units. The design of MGU benefits from evaluation\nresults on LSTM and GRU in the literature. Experiments on various sequence data\nshow that MGU has comparable accuracy with GRU, but has a simpler structure,\nfewer parameters, and faster training. Hence, MGU is suitable in RNN's\napplications. Its simple architecture also means that it is easier to evaluate\nand tune, and in principle it is easier to study MGU's properties theoretically\nand empirically.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 00:01:10 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Zhou", "Guo-Bing", ""], ["Wu", "Jianxin", ""], ["Zhang", "Chen-Lin", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1603.09441", "submitter": "Ian Dewancker", "authors": "Ian Dewancker, Michael McCourt, Scott Clark, Patrick Hayes, Alexandra\n  Johnson and George Ke", "title": "A Stratified Analysis of Bayesian Optimization Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical analysis serves as an important complement to theoretical analysis\nfor studying practical Bayesian optimization. Often empirical insights expose\nstrengths and weaknesses inaccessible to theoretical analysis. We define two\nmetrics for comparing the performance of Bayesian optimization methods and\npropose a ranking mechanism for summarizing performance within various genres\nor strata of test functions. These test functions serve to mimic the complexity\nof hyperparameter optimization problems, the most prominent application of\nBayesian optimization, but with a closed form which allows for rapid evaluation\nand more predictable behavior. This offers a flexible and efficient way to\ninvestigate functions with specific properties of interest, such as oscillatory\nbehavior or an optimum on the domain boundary.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 02:23:24 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Dewancker", "Ian", ""], ["McCourt", "Michael", ""], ["Clark", "Scott", ""], ["Hayes", "Patrick", ""], ["Johnson", "Alexandra", ""], ["Ke", "George", ""]]}, {"id": "1603.09469", "submitter": "Hyunsuk Ko", "authors": "Hyunsuk Ko, Rui Song, C.-C. Jay Kuo", "title": "A ParaBoost Stereoscopic Image Quality Assessment (PBSIQA) System", "comments": null, "journal-ref": "Journal of Visual Communication and Image Representation, 2017", "doi": "10.1016/j.jvcir.2017.02.014", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of stereoscopic image quality assessment, which finds\napplications in 3D visual content delivery such as 3DTV, is investigated in\nthis work. Specifically, we propose a new ParaBoost (parallel-boosting)\nstereoscopic image quality assessment (PBSIQA) system. The system consists of\ntwo stages. In the first stage, various distortions are classified into a few\ntypes, and individual quality scorers targeting at a specific distortion type\nare developed. These scorers offer complementary performance in face of a\ndatabase consisting of heterogeneous distortion types. In the second stage,\nscores from multiple quality scorers are fused to achieve the best overall\nperformance, where the fuser is designed based on the parallel boosting idea\nborrowed from machine learning. Extensive experimental results are conducted to\ncompare the performance of the proposed PBSIQA system with those of existing\nstereo image quality assessment (SIQA) metrics. The developed quality metric\ncan serve as an objective function to optimize the performance of a 3D content\ndelivery system.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 06:55:25 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Ko", "Hyunsuk", ""], ["Song", "Rui", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1603.09473", "submitter": "Ruining He", "authors": "Ruining He and Charles Packer and Julian McAuley", "title": "Learning Compatibility Across Categories for Heterogeneous Item\n  Recommendation", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying relationships between items is a key task of an online\nrecommender system, in order to help users discover items that are functionally\ncomplementary or visually compatible. In domains like clothing recommendation,\nthis task is particularly challenging since a successful system should be\ncapable of handling a large corpus of items, a huge amount of relationships\namong them, as well as the high-dimensional and semantically complicated\nfeatures involved. Furthermore, the human notion of \"compatibility\" to capture\ngoes beyond mere similarity: For two items to be compatible---whether jeans and\na t-shirt, or a laptop and a charger---they should be similar in some ways, but\nsystematically different in others.\n  In this paper we propose a novel method, Monomer, to learn complicated and\nheterogeneous relationships between items in product recommendation settings.\nRecently, scalable methods have been developed that address this task by\nlearning similarity metrics on top of the content of the products involved.\nHere our method relaxes the metricity assumption inherent in previous work and\nmodels multiple localized notions of 'relatedness,' so as to uncover ways in\nwhich related items should be systematically similar, and systematically\ndifferent. Quantitatively, we show that our system achieves state-of-the-art\nperformance on large-scale compatibility prediction tasks, especially in cases\nwhere there is substantial heterogeneity between related items. Qualitatively,\nwe demonstrate that richer notions of compatibility can be learned that go\nbeyond similarity, and that our model can make effective recommendations of\nheterogeneous content.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 07:22:30 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 07:25:36 GMT"}, {"version": "v3", "created": "Thu, 29 Sep 2016 00:43:21 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["He", "Ruining", ""], ["Packer", "Charles", ""], ["McAuley", "Julian", ""]]}, {"id": "1603.09509", "submitter": "Zhenyao Zhu", "authors": "Zhenyao Zhu, Jesse H. Engel, Awni Hannun", "title": "Learning Multiscale Features Directly From Waveforms", "comments": "\"fix typo in the title\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has dramatically improved the performance of speech recognition\nsystems through learning hierarchies of features optimized for the task at\nhand. However, true end-to-end learning, where features are learned directly\nfrom waveforms, has only recently reached the performance of hand-tailored\nrepresentations based on the Fourier transform. In this paper, we detail an\napproach to use convolutional filters to push past the inherent tradeoff of\ntemporal and frequency resolution that exists for spectral representations. At\nincreased computational cost, we show that increasing temporal resolution via\nreduced stride and increasing frequency resolution via additional filters\ndelivers significant performance improvements. Further, we find more efficient\nrepresentations by simultaneously learning at multiple scales, leading to an\noverall decrease in word error rate on a difficult internal speech test set by\n20.7% relative to networks with the same number of parameters trained on\nspectrograms.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 09:54:44 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 14:17:09 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Zhu", "Zhenyao", ""], ["Engel", "Jesse H.", ""], ["Hannun", "Awni", ""]]}, {"id": "1603.09620", "submitter": "Hans Verstraete", "authors": "Laurens Bliek, Hans R. G. W. Verstraete, Michel Verhaegen, and Sander\n  Wahls", "title": "Online Optimization with Costly and Noisy Measurements using Random\n  Fourier Expansions", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes DONE, an online optimization algorithm that iteratively\nminimizes an unknown function based on costly and noisy measurements. The\nalgorithm maintains a surrogate of the unknown function in the form of a random\nFourier expansion (RFE). The surrogate is updated whenever a new measurement is\navailable, and then used to determine the next measurement point. The algorithm\nis comparable to Bayesian optimization algorithms, but its computational\ncomplexity per iteration does not depend on the number of measurements. We\nderive several theoretical results that provide insight on how the\nhyper-parameters of the algorithm should be chosen. The algorithm is compared\nto a Bayesian optimization algorithm for a benchmark problem and three\napplications, namely, optical coherence tomography, optical beam-forming\nnetwork tuning, and robot arm control. It is found that the DONE algorithm is\nsignificantly faster than Bayesian optimization in the discussed problems,\nwhile achieving a similar or better performance.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 15:00:06 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 14:07:04 GMT"}, {"version": "v3", "created": "Thu, 29 Sep 2016 11:13:00 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Bliek", "Laurens", ""], ["Verstraete", "Hans R. G. W.", ""], ["Verhaegen", "Michel", ""], ["Wahls", "Sander", ""]]}, {"id": "1603.09630", "submitter": "Pawel Swietojanski", "authors": "Pawel Swietojanski and Steve Renals", "title": "Differentiable Pooling for Unsupervised Acoustic Model Adaptation", "comments": "11 pages, 7 Tables, 7 Figures in IEEE/ACM Transactions on Audio,\n  Speech, and Language Processing, vol. 24, num. 11, 2016", "journal-ref": null, "doi": "10.1109/TASLP.2016.2584700", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep neural network (DNN) acoustic model that includes\nparametrised and differentiable pooling operators. Unsupervised acoustic model\nadaptation is cast as the problem of updating the decision boundaries\nimplemented by each pooling operator. In particular, we experiment with two\ntypes of pooling parametrisations: learned $L_p$-norm pooling and weighted\nGaussian pooling, in which the weights of both operators are treated as\nspeaker-dependent. We perform investigations using three different large\nvocabulary speech recognition corpora: AMI meetings, TED talks and Switchboard\nconversational telephone speech. We demonstrate that differentiable pooling\noperators provide a robust and relatively low-dimensional way to adapt acoustic\nmodels, with relative word error rates reductions ranging from 5--20% with\nrespect to unadapted systems, which themselves are better than the baseline\nfully-connected DNN-based acoustic models. We also investigate how the proposed\ntechniques work under various adaptation conditions including the quality of\nadaptation data and complementarity to other feature- and model-space\nadaptation methods, as well as providing an analysis of the characteristics of\neach of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 15:10:40 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 18:12:49 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Swietojanski", "Pawel", ""], ["Renals", "Steve", ""]]}, {"id": "1603.09631", "submitter": "Miroslav Vodol\\'an", "authors": "Miroslav Vodol\\'an, Filip Jur\\v{c}\\'i\\v{c}ek", "title": "Data Collection for Interactive Learning through the Dialog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a dataset collected from natural dialogs which enables to\ntest the ability of dialog systems to learn new facts from user utterances\nthroughout the dialog. This interactive learning will help with one of the most\nprevailing problems of open domain dialog system, which is the sparsity of\nfacts a dialog system can reason about. The proposed dataset, consisting of\n1900 collected dialogs, allows simulation of an interactive gaining of\ndenotations and questions explanations from users which can be used for the\ninteractive learning.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 15:13:51 GMT"}, {"version": "v2", "created": "Sun, 15 May 2016 13:03:26 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Vodol\u00e1n", "Miroslav", ""], ["Jur\u010d\u00ed\u010dek", "Filip", ""]]}, {"id": "1603.09638", "submitter": "Berkay Celik", "authors": "Z. Berkay Celik, Patrick McDaniel, Rauf Izmailov, Nicolas Papernot,\n  Ryan Sheatsley, Raquel Alvarez, Ananthram Swami", "title": "Detection under Privileged Information", "comments": "A short version of this paper is accepted to ASIACCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For well over a quarter century, detection systems have been driven by models\nlearned from input features collected from real or simulated environments. An\nartifact (e.g., network event, potential malware sample, suspicious email) is\ndeemed malicious or non-malicious based on its similarity to the learned model\nat runtime. However, the training of the models has been historically limited\nto only those features available at runtime. In this paper, we consider an\nalternate learning approach that trains models using \"privileged\"\ninformation--features available at training time but not at runtime--to improve\nthe accuracy and resilience of detection systems. In particular, we adapt and\nextend recent advances in knowledge transfer, model influence, and distillation\nto enable the use of forensic or other data unavailable at runtime in a range\nof security domains. An empirical evaluation shows that privileged information\nincreases precision and recall over a system with no privileged information: we\nobserve up to 7.7% relative decrease in detection error for fast-flux bot\ndetection, 8.6% for malware traffic detection, 7.3% for malware classification,\nand 16.9% for face recognition. We explore the limitations and applications of\ndifferent privileged information techniques in detection systems. Such\ntechniques provide a new means for detection systems to learn from data that\nwould otherwise not be available at runtime.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 15:28:45 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 13:59:01 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2016 01:17:06 GMT"}, {"version": "v4", "created": "Sat, 31 Mar 2018 02:12:21 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Celik", "Z. Berkay", ""], ["McDaniel", "Patrick", ""], ["Izmailov", "Rauf", ""], ["Papernot", "Nicolas", ""], ["Sheatsley", "Ryan", ""], ["Alvarez", "Raquel", ""], ["Swami", "Ananthram", ""]]}, {"id": "1603.09643", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Lantian Li and Dong Wang", "title": "Multi-task Recurrent Model for Speech and Speaker Recognition", "comments": "APSIPA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although highly correlated, speech and speaker recognition have been regarded\nas two independent tasks and studied by two communities. This is certainly not\nthe way that people behave: we decipher both speech content and speaker traits\nat the same time. This paper presents a unified model to perform speech and\nspeaker recognition simultaneously and altogether. The model is based on a\nunified neural network where the output of one task is fed to the input of the\nother, leading to a multi-task recurrent network. Experiments show that the\njoint model outperforms the task-specific models on both the two tasks.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 15:37:29 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 05:54:30 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2016 06:25:01 GMT"}, {"version": "v4", "created": "Tue, 27 Sep 2016 12:27:17 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Li", "Lantian", ""], ["Wang", "Dong", ""]]}, {"id": "1603.09738", "submitter": "Atef Shaar", "authors": "Atef Shaar, Talel Abdessalem, Olivier Segard", "title": "Pessimistic Uplift Modeling", "comments": "This paper has been withdrawn by the author(s) for improvement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uplift modeling is a machine learning technique that aims to model treatment\neffects heterogeneity. It has been used in business and health sectors to\npredict the effect of a specific action on a given individual. Despite its\nadvantages, uplift models show high sensitivity to noise and disturbance, which\nleads to unreliable results. In this paper we show different approaches to\naddress the problem of uplift modeling, we demonstrate how disturbance in data\ncan affect uplift measurement. We propose a new approach, we call it\nPessimistic Uplift Modeling, that minimizes disturbance effects. We compared\nour approach with the existing uplift methods, on simulated and real data-sets.\nThe experiments show that our approach outperforms the existing approaches,\nespecially in the case of high noise data environment.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 19:48:13 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 13:53:01 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Shaar", "Atef", ""], ["Abdessalem", "Talel", ""], ["Segard", "Olivier", ""]]}, {"id": "1603.09739", "submitter": "Prithwish Chakraborty", "authors": "Prithwish Chakraborty and Sathappan Muthiah and Ravi Tandon and Naren\n  Ramakrishnan", "title": "Hierarchical Quickest Change Detection via Surrogates", "comments": "Submitted to a journal. See demo at\n  https://prithwi.github.io/hqcd_supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection (CD) in time series data is a critical problem as it reveal\nchanges in the underlying generative processes driving the time series. Despite\nhaving received significant attention, one important unexplored aspect is how\nto efficiently utilize additional correlated information to improve the\ndetection and the understanding of changepoints. We propose hierarchical\nquickest change detection (HQCD), a framework that formalizes the process of\nincorporating additional correlated sources for early changepoint detection.\nThe core ideas behind HQCD are rooted in the theory of quickest detection and\nHQCD can be regarded as its novel generalization to a hierarchical setting. The\nsources are classified into targets and surrogates, and HQCD leverages this\nstructure to systematically assimilate observed data to update changepoint\nstatistics across layers. The decision on actual changepoints are provided by\nminimizing the delay while still maintaining reliability bounds. In addition,\nHQCD also uncovers interesting relations between changes at targets from\nchanges across surrogates. We validate HQCD for reliability and performance\nagainst several state-of-the-art methods for both synthetic dataset (known\nchangepoints) and several real-life examples (unknown changepoints). Our\nexperiments indicate that we gain significant robustness without loss of\ndetection delay through HQCD. Our real-life experiments also showcase the\nusefulness of the hierarchical setting by connecting the surrogate sources\n(such as Twitter chatter) to target sources (such as Employment related\nprotests that ultimately lead to major uprisings).\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 19:50:45 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Chakraborty", "Prithwish", ""], ["Muthiah", "Sathappan", ""], ["Tandon", "Ravi", ""], ["Ramakrishnan", "Naren", ""]]}]