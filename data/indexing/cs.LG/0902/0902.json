[{"id": "0902.0392", "submitter": "Christos Dimitrakakis", "authors": "Christos Dimitrakakis", "title": "Tree Exploration for Bayesian RL Exploration", "comments": "13 pages, 1 figure. Slightly extended and corrected version (notation\n  errors and lower bound calculation) of homonymous paper presented at the\n  conference of Computational Intelligence for Modelling, Control and\n  Automation 2008 (CIMCA'08)", "journal-ref": null, "doi": null, "report-no": "IAS-08-04", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in reinforcement learning has produced algorithms for optimal\ndecision making under uncertainty that fall within two main types. The first\nemploys a Bayesian framework, where optimality improves with increased\ncomputational time. This is because the resulting planning task takes the form\nof a dynamic programming problem on a belief tree with an infinite number of\nstates. The second type employs relatively simple algorithm which are shown to\nsuffer small regret within a distribution-free framework. This paper presents a\nlower bound and a high probability upper bound on the optimal value function\nfor the nodes in the Bayesian belief tree, which are analogous to similar\nbounds in POMDPs. The bounds are then used to create more efficient strategies\nfor exploring the tree. The resulting algorithms are compared with the\ndistribution-free algorithm UCB1, as well as a simpler baseline algorithm on\nmulti-armed bandit problems.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2009 22:37:23 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2011 08:13:36 GMT"}], "update_date": "2011-09-22", "authors_parsed": [["Dimitrakakis", "Christos", ""]]}, {"id": "0902.1227", "submitter": "Raajay Viswanathan", "authors": "Avinash Achar, Srivatsan Laxman, Raajay Viswanathan and P. S. Sastry", "title": "Discovering general partial orders in event streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent episode discovery is a popular framework for pattern discovery in\nevent streams. An episode is a partially ordered set of nodes with each node\nassociated with an event type. Efficient (and separate) algorithms exist for\nepisode discovery when the associated partial order is total (serial episode)\nand trivial (parallel episode). In this paper, we propose efficient algorithms\nfor discovering frequent episodes with general partial orders. These algorithms\ncan be easily specialized to discover serial or parallel episodes. Also, the\nalgorithms are flexible enough to be specialized for mining in the space of\ncertain interesting subclasses of partial orders. We point out that there is an\ninherent combinatorial explosion in frequent partial order mining and most\nimportantly, frequency alone is not a sufficient measure of interestingness. We\npropose a new interestingness measure for general partial order episodes and a\ndiscovery method based on this measure, for filtering out uninteresting partial\norders. Simulations demonstrate the effectiveness of our algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2009 07:50:02 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2009 06:18:30 GMT"}], "update_date": "2009-12-11", "authors_parsed": [["Achar", "Avinash", ""], ["Laxman", "Srivatsan", ""], ["Viswanathan", "Raajay", ""], ["Sastry", "P. S.", ""]]}, {"id": "0902.1258", "submitter": "Baptiste Jeudy", "authors": "Baptiste Jeudy (LAHC), Fran\\c{c}ois Rioult (GREYC)", "title": "Extraction de concepts sous contraintes dans des donn\\'ees d'expression\n  de g\\`enes", "comments": null, "journal-ref": "Conf\\'erence sur l'apprentissage automatique, Nice : France (2005)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a technique to extract constrained formal concepts.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2009 18:01:09 GMT"}], "update_date": "2009-02-10", "authors_parsed": [["Jeudy", "Baptiste", "", "LAHC"], ["Rioult", "Fran\u00e7ois", "", "GREYC"]]}, {"id": "0902.1259", "submitter": "Baptiste Jeudy", "authors": "Baptiste Jeudy (LAHC, EURISE), Fran\\c{c}ois Rioult (GREYC)", "title": "Database Transposition for Constrained (Closed) Pattern Mining", "comments": null, "journal-ref": "Knowledge Discovery in Inductive Databases, Third International\n  Workshop, KDID 2004, Pisa, Italy, Septembre 2004, Revised Selected and\n  Invited Papers, Bart Goethals, Arno Siebes (Ed.) (2004) 89-107", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, different works proposed a new way to mine patterns in databases\nwith pathological size. For example, experiments in genome biology usually\nprovide databases with thousands of attributes (genes) but only tens of objects\n(experiments). In this case, mining the \"transposed\" database runs through a\nsmaller search space, and the Galois connection allows to infer the closed\npatterns of the original database. We focus here on constrained pattern mining\nfor those unusual databases and give a theoretical framework for database and\nconstraint transposition. We discuss the properties of constraint transposition\nand look into classical constraints. We then address the problem of generating\nthe closed patterns of the original database satisfying the constraint,\nstarting from those mined in the \"transposed\" database. Finally, we show how to\ngenerate all the patterns satisfying the constraint from the closed ones.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2009 18:01:56 GMT"}], "update_date": "2009-02-10", "authors_parsed": [["Jeudy", "Baptiste", "", "LAHC, EURISE"], ["Rioult", "Fran\u00e7ois", "", "GREYC"]]}, {"id": "0902.1284", "submitter": "Daniel Hsu", "authors": "Daniel Hsu, Sham M. Kakade, John Langford, Tong Zhang", "title": "Multi-Label Prediction via Compressed Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multi-label prediction problems with large output spaces under\nthe assumption of output sparsity -- that the target (label) vectors have small\nsupport. We develop a general theory for a variant of the popular error\ncorrecting output code scheme, using ideas from compressed sensing for\nexploiting this sparsity. The method can be regarded as a simple reduction from\nmulti-label regression problems to binary regression problems. We show that the\nnumber of subproblems need only be logarithmic in the total number of possible\nlabels, making this approach radically more efficient than others. We also\nstate and prove robustness guarantees for this method in the form of regret\ntransform bounds (in general), and also provide a more detailed analysis for\nthe linear prediction setting.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2009 02:30:06 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2009 16:23:28 GMT"}], "update_date": "2009-06-02", "authors_parsed": [["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""], ["Langford", "John", ""], ["Zhang", "Tong", ""]]}, {"id": "0902.2751", "submitter": "Arman Didandeh", "authors": "Nima Mirbakhsh, Arman Didandeh", "title": "Object Classification by means of Multi-Feature Concept Learning in a\n  Multi Expert-Agent System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of some objects in classes of concepts is an essential and\neven breathtaking task in many applications. A solution is discussed here based\non Multi-Agent systems. A kernel of some expert agents in several classes is to\nconsult a central agent decide among the classification problem of a certain\nobject. This kernel is moderated with the center agent, trying to manage the\nquerying agents for any decision problem by means of a data-header like feature\nset. Agents have cooperation among concepts related to the classes of this\nclassification decision-making; and may affect on each others' results on a\ncertain query object in a multi-agent learning approach. This leads to an\nonline feature learning via the consulting trend. The performance is discussed\nto be much better in comparison to some other prior trends while system's\nmessage passing overload is decreased to less agents and the expertism helps\nthe performance and operability of system win the comparison.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2009 18:39:53 GMT"}, {"version": "v2", "created": "Wed, 18 Feb 2009 06:29:16 GMT"}, {"version": "v3", "created": "Sat, 21 Feb 2009 16:20:01 GMT"}, {"version": "v4", "created": "Sun, 1 Mar 2009 10:35:34 GMT"}], "update_date": "2009-03-01", "authors_parsed": [["Mirbakhsh", "Nima", ""], ["Didandeh", "Arman", ""]]}, {"id": "0902.3176", "submitter": "John Langford", "authors": "Alina Beygelzimer, John Langford, and Pradeep Ravikumar", "title": "Error-Correcting Tournaments", "comments": "Minor wording improvements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a family of pairwise tournaments reducing $k$-class classification\nto binary classification. These reductions are provably robust against a\nconstant fraction of binary errors. The results improve on the PECOC\nconstruction \\cite{SECOC} with an exponential improvement in computation, from\n$O(k)$ to $O(\\log_2 k)$, and the removal of a square root in the regret\ndependence, matching the best possible computation and regret up to a constant.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2009 16:01:24 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2009 01:14:27 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2009 22:06:32 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2010 15:03:58 GMT"}], "update_date": "2010-02-03", "authors_parsed": [["Beygelzimer", "Alina", ""], ["Langford", "John", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "0902.3223", "submitter": "Jose Brito", "authors": "Jose Brito, Mauricio Lila, Flavio Montenegro, Nelson Maculan", "title": "An Exact Algorithm for the Stratification Problem with Proportional\n  Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report a new optimal resolution for the statistical stratification problem\nunder proportional sampling allocation among strata. Consider a finite\npopulation of N units, a random sample of n units selected from this population\nand a number L of strata. Thus, we have to define which units belong to each\nstratum so as to minimize the variance of a total estimator for one desired\nvariable of interest in each stratum,and consequently reduce the overall\nvariance for such quantity. In order to solve this problem, an exact algorithm\nbased on the concept of minimal path in a graph is proposed and assessed.\nComputational results using real data from IBGE (Brazilian Central Statistical\nOffice) are provided.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2009 19:12:59 GMT"}], "update_date": "2009-02-24", "authors_parsed": [["Brito", "Jose", ""], ["Lila", "Mauricio", ""], ["Montenegro", "Flavio", ""], ["Maculan", "Nelson", ""]]}, {"id": "0902.3373", "submitter": "Elisa Fromont", "authors": "Marie-Odile Cordier (INRIA - Irisa), Elisa Fromont (LAHC), Ren\\'e\n  Quiniou (INRIA - Irisa)", "title": "Learning rules from multisource data for cardiac monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper formalises the concept of learning symbolic rules from multisource\ndata in a cardiac monitoring context. Our sources, electrocardiograms and\narterial blood pressure measures, describe cardiac behaviours from different\nviewpoints. To learn interpretable rules, we use an Inductive Logic Programming\n(ILP) method. We develop an original strategy to cope with the dimensionality\nissues caused by using this ILP technique on a rich multisource language. The\nresults show that our method greatly improves the feasibility and the\nefficiency of the process while staying accurate. They also confirm the\nbenefits of using multiple sources to improve the diagnosis of cardiac\narrhythmias.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2009 13:47:53 GMT"}], "update_date": "2009-02-20", "authors_parsed": [["Cordier", "Marie-Odile", "", "INRIA - Irisa"], ["Fromont", "Elisa", "", "LAHC"], ["Quiniou", "Ren\u00e9", "", "INRIA - Irisa"]]}, {"id": "0902.3430", "submitter": "Afshin Rostamizadeh", "authors": "Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh", "title": "Domain Adaptation: Learning Bounds and Algorithms", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the general problem of domain adaptation which arises in\na variety of applications where the distribution of the labeled sample\navailable somewhat differs from that of the test data. Building on previous\nwork by Ben-David et al. (2007), we introduce a novel distance between\ndistributions, discrepancy distance, that is tailored to adaptation problems\nwith arbitrary loss functions. We give Rademacher complexity bounds for\nestimating the discrepancy distance from finite samples for different loss\nfunctions. Using this distance, we derive novel generalization bounds for\ndomain adaptation for a wide family of loss functions. We also present a series\nof novel adaptation bounds for large classes of regularization-based\nalgorithms, including support vector machines and kernel ridge regression based\non the empirical discrepancy. This motivates our analysis of the problem of\nminimizing the empirical discrepancy for various loss functions for which we\nalso give novel algorithms. We report the results of preliminary experiments\nthat demonstrate the benefits of our discrepancy minimization algorithms for\ndomain adaptation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2009 18:42:16 GMT"}, {"version": "v2", "created": "Mon, 23 Feb 2009 16:56:37 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Mansour", "Yishay", ""], ["Mohri", "Mehryar", ""], ["Rostamizadeh", "Afshin", ""]]}, {"id": "0902.3526", "submitter": "Gilles Stoltz", "authors": "Gabor Lugosi, Omiros Papaspiliopoulos, Gilles Stoltz (DMA, GREGH)", "title": "Online Multi-task Learning with Hard Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss multi-task online learning when a decision maker has to deal\nsimultaneously with M tasks. The tasks are related, which is modeled by\nimposing that the M-tuple of actions taken by the decision maker needs to\nsatisfy certain constraints. We give natural examples of such restrictions and\nthen discuss a general class of tractable constraints, for which we introduce\ncomputationally efficient ways of selecting actions, essentially by reducing to\nan on-line shortest path problem. We briefly discuss \"tracking\" and \"bandit\"\nversions of the problem and extend the model in various ways, including\nnon-additive global losses and uncountably infinite sets of tasks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2009 07:39:13 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2009 14:50:53 GMT"}], "update_date": "2009-03-27", "authors_parsed": [["Lugosi", "Gabor", "", "DMA, GREGH"], ["Papaspiliopoulos", "Omiros", "", "DMA, GREGH"], ["Stoltz", "Gilles", "", "DMA, GREGH"]]}, {"id": "0902.3846", "submitter": "Amit Singer", "authors": "Amit Singer, Mihai Cucuringu", "title": "Uniqueness of Low-Rank Matrix Completion by Rigidity Theory", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of completing a low-rank matrix from a subset of its entries is\noften encountered in the analysis of incomplete data sets exhibiting an\nunderlying factor model with applications in collaborative filtering, computer\nvision and control. Most recent work had been focused on constructing efficient\nalgorithms for exact or approximate recovery of the missing matrix entries and\nproving lower bounds for the number of known entries that guarantee a\nsuccessful recovery with high probability. A related problem from both the\nmathematical and algorithmic point of view is the distance geometry problem of\nrealizing points in a Euclidean space from a given subset of their pairwise\ndistances. Rigidity theory answers basic questions regarding the uniqueness of\nthe realization satisfying a given partial set of distances. We observe that\nbasic ideas and tools of rigidity theory can be adapted to determine uniqueness\nof low-rank matrix completion, where inner products play the role that\ndistances play in rigidity theory. This observation leads to an efficient\nrandomized algorithm for testing both local and global unique completion.\nCrucial to our analysis is a new matrix, which we call the completion matrix,\nthat serves as the analogue of the rigidity matrix.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2009 04:05:48 GMT"}], "update_date": "2009-02-24", "authors_parsed": [["Singer", "Amit", ""], ["Cucuringu", "Mihai", ""]]}, {"id": "0902.4127", "submitter": "Vladimir Vovk", "authors": "Alexey Chernov and Vladimir Vovk", "title": "Prediction with expert evaluators' advice", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new protocol for prediction with expert advice in which each\nexpert evaluates the learner's and his own performance using a loss function\nthat may change over time and may be different from the loss functions used by\nthe other experts. The learner's goal is to perform better or not much worse\nthan each expert, as evaluated by that expert, for all experts simultaneously.\nIf the loss functions used by the experts are all proper scoring rules and all\nmixable, we show that the defensive forecasting algorithm enjoys the same\nperformance guarantee as that attainable by the Aggregating Algorithm in the\nstandard setting and known to be optimal. This result is also applied to the\ncase of \"specialist\" (or \"sleeping\") experts. In this case, the defensive\nforecasting algorithm reduces to a simple modification of the Aggregating\nAlgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2009 11:47:03 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2009 16:28:41 GMT"}], "update_date": "2009-03-23", "authors_parsed": [["Chernov", "Alexey", ""], ["Vovk", "Vladimir", ""]]}, {"id": "0902.4228", "submitter": "Vamsi Potluru", "authors": "Vamsi K. Potluru, Sergey M. Plis, Morten Morup, Vince D. Calhoun,\n  Terran Lane", "title": "Multiplicative updates For Non-Negative Kernel SVM", "comments": "4 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present multiplicative updates for solving hard and soft margin support\nvector machines (SVM) with non-negative kernels. They follow as a natural\nextension of the updates for non-negative matrix factorization. No additional\nparam- eter setting, such as choosing learning, rate is required. Ex- periments\ndemonstrate rapid convergence to good classifiers. We analyze the rates of\nasymptotic convergence of the up- dates and establish tight bounds. We test the\nperformance on several datasets using various non-negative kernels and report\nequivalent generalization errors to that of a standard SVM.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2009 20:38:32 GMT"}], "update_date": "2009-02-25", "authors_parsed": [["Potluru", "Vamsi K.", ""], ["Plis", "Sergey M.", ""], ["Morup", "Morten", ""], ["Calhoun", "Vince D.", ""], ["Lane", "Terran", ""]]}]