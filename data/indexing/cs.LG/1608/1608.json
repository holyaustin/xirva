[{"id": "1608.00027", "submitter": "Rhiannon Rose", "authors": "Rhiannon V. Rose, Daniel J. Lizotte", "title": "gLOP: the global and Local Penalty for Capturing Predictive\n  Heterogeneity", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When faced with a supervised learning problem, we hope to have rich enough\ndata to build a model that predicts future instances well. However, in\npractice, problems can exhibit predictive heterogeneity: most instances might\nbe relatively easy to predict, while others might be predictive outliers for\nwhich a model trained on the entire dataset does not perform well. Identifying\nthese can help focus future data collection. We present gLOP, the global and\nLocal Penalty, a framework for capturing predictive heterogeneity and\nidentifying predictive outliers. gLOP is based on penalized regression for\nmultitask learning, which improves learning by leveraging training signal\ninformation from related tasks. We give two optimization algorithms for gLOP,\none space-efficient, and another giving the full regularization path. We also\ncharacterize uniqueness in terms of the data and tuning parameters, and present\nempirical results on synthetic data and on two health research problems.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 20:57:06 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Rose", "Rhiannon V.", ""], ["Lizotte", "Daniel J.", ""]]}, {"id": "1608.00100", "submitter": "Nikos Katzouris", "authors": "Nikos Katzouris, Alexander Artikis, Georgios Paliouras", "title": "Online Learning of Event Definitions", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 1 PDF figure", "journal-ref": "Theory and Practice of Logic Programming 16(5-6), 817-833, 2016", "doi": "10.1017/S1471068416000260", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems for symbolic event recognition infer occurrences of events in time\nusing a set of event definitions in the form of first-order rules. The Event\nCalculus is a temporal logic that has been used as a basis in event recognition\napplications, providing among others, direct connections to machine learning,\nvia Inductive Logic Programming (ILP). We present an ILP system for online\nlearning of Event Calculus theories. To allow for a single-pass learning\nstrategy, we use the Hoeffding bound for evaluating clauses on a subset of the\ninput stream. We employ a decoupling scheme of the Event Calculus axioms during\nthe learning process, that allows to learn each clause in isolation. Moreover,\nwe use abductive-inductive logic programming techniques to handle unobserved\ntarget predicates. We evaluate our approach on an activity recognition\napplication and compare it to a number of batch learning techniques. We obtain\nresults of comparable predicative accuracy with significant speed-ups in\ntraining time. We also outperform hand-crafted rules and match the performance\nof a sound incremental learner that can only operate on noise-free datasets.\nThis paper is under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 10:44:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Katzouris", "Nikos", ""], ["Artikis", "Alexander", ""], ["Paliouras", "Georgios", ""]]}, {"id": "1608.00104", "submitter": "Chenguang Wang", "authors": "Chenguang Wang, Yangqiu Song, Dan Roth, Ming Zhang, Jiawei Han", "title": "World Knowledge as Indirect Supervision for Document Clustering", "comments": "33 pages, 53 figures, ACM TKDD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key obstacles in making learning protocols realistic in\napplications is the need to supervise them, a costly process that often\nrequires hiring domain experts. We consider the framework to use the world\nknowledge as indirect supervision. World knowledge is general-purpose\nknowledge, which is not designed for any specific domain. Then the key\nchallenges are how to adapt the world knowledge to domains and how to represent\nit for learning. In this paper, we provide an example of using world knowledge\nfor domain dependent document clustering. We provide three ways to specify the\nworld knowledge to domains by resolving the ambiguity of the entities and their\ntypes, and represent the data with world knowledge as a heterogeneous\ninformation network. Then we propose a clustering algorithm that can cluster\nmultiple types and incorporate the sub-type information as constraints. In the\nexperiments, we use two existing knowledge bases as our sources of world\nknowledge. One is Freebase, which is collaboratively collected knowledge about\nentities and their organizations. The other is YAGO2, a knowledge base\nautomatically extracted from Wikipedia and maps knowledge to the linguistic\nknowledge base, WordNet. Experimental results on two text benchmark datasets\n(20newsgroups and RCV1) show that incorporating world knowledge as indirect\nsupervision can significantly outperform the state-of-the-art clustering\nalgorithms as well as clustering algorithms enhanced with world knowledge\nfeatures.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 11:53:04 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Wang", "Chenguang", ""], ["Song", "Yangqiu", ""], ["Roth", "Dan", ""], ["Zhang", "Ming", ""], ["Han", "Jiawei", ""]]}, {"id": "1608.00159", "submitter": "Hamid Dadkhahi", "authors": "Hamid Dadkhahi and Benjamin M. Marlin", "title": "Learning Tree-Structured Detection Cascades for Heterogeneous Networks\n  of Embedded Devices", "comments": "arXiv admin note: substantial text overlap with arXiv:1607.03730", "journal-ref": null, "doi": "10.1145/3097983.3098169", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new approach to learning cascaded classifiers for\nuse in computing environments that involve networks of heterogeneous and\nresource-constrained, low-power embedded compute and sensing nodes. We present\na generalization of the classical linear detection cascade to the case of\ntree-structured cascades where different branches of the tree execute on\ndifferent physical compute nodes in the network. Different nodes have access to\ndifferent features, as well as access to potentially different computation and\nenergy resources. We concentrate on the problem of jointly learning the\nparameters for all of the classifiers in the cascade given a fixed cascade\narchitecture and a known set of costs required to carry out the computation at\neach node.To accomplish the objective of joint learning of all detectors, we\npropose a novel approach to combining classifier outputs during training that\nbetter matches the hard cascade setting in which the learned system will be\ndeployed. This work is motivated by research in the area of mobile health where\nenergy efficient real time detectors integrating information from multiple\nwireless on-body sensors and a smart phone are needed for real-time monitoring\nand delivering just- in-time adaptive interventions. We apply our framework to\ntwo activity recognition datasets as well as the problem of cigarette smoking\ndetection from a combination of wrist-worn actigraphy data and respiration\nchest band data.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 19:52:56 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 19:12:43 GMT"}, {"version": "v3", "created": "Sat, 18 Feb 2017 10:10:40 GMT"}, {"version": "v4", "created": "Sat, 24 Jun 2017 23:30:59 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Dadkhahi", "Hamid", ""], ["Marlin", "Benjamin M.", ""]]}, {"id": "1608.00182", "submitter": "Peng Tang", "authors": "Peng Tang, Xinggang Wang, Baoguang Shi, Xiang Bai, Wenyu Liu, Zhuowen\n  Tu", "title": "Deep FisherNet for Object Classification", "comments": "submitted to NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great success of convolutional neural networks (CNN) for the\nimage classification task on datasets like Cifar and ImageNet, CNN's\nrepresentation power is still somewhat limited in dealing with object images\nthat have large variation in size and clutter, where Fisher Vector (FV) has\nshown to be an effective encoding strategy. FV encodes an image by aggregating\nlocal descriptors with a universal generative Gaussian Mixture Model (GMM). FV\nhowever has limited learning capability and its parameters are mostly fixed\nafter constructing the codebook. To combine together the best of the two\nworlds, we propose in this paper a neural network structure with FV layer being\npart of an end-to-end trainable system that is differentiable; we name our\nnetwork FisherNet that is learnable using backpropagation. Our proposed\nFisherNet combines convolutional neural network training and Fisher Vector\nencoding in a single end-to-end structure. We observe a clear advantage of\nFisherNet over plain CNN and standard FV in terms of both classification\naccuracy and computational efficiency on the challenging PASCAL VOC object\nclassification task.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 03:56:30 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Tang", "Peng", ""], ["Wang", "Xinggang", ""], ["Shi", "Baoguang", ""], ["Bai", "Xiang", ""], ["Liu", "Wenyu", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1608.00218", "submitter": "Ilija Ilievski", "authors": "Ilija Ilievski and Jiashi Feng", "title": "Hyperparameter Transfer Learning through Surrogate Alignment for\n  Efficient Deep Neural Network Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several optimization methods have been successfully applied to the\nhyperparameter optimization of deep neural networks (DNNs). The methods work by\nmodeling the joint distribution of hyperparameter values and corresponding\nerror. Those methods become less practical when applied to modern DNNs whose\ntraining may take a few days and thus one cannot collect sufficient\nobservations to accurately model the distribution. To address this challenging\nissue, we propose a method that learns to transfer optimal hyperparameter\nvalues for a small source dataset to hyperparameter values with comparable\nperformance on a dataset of interest. As opposed to existing transfer learning\nmethods, our proposed method does not use hand-designed features. Instead, it\nuses surrogates to model the hyperparameter-error distributions of the two\ndatasets and trains a neural network to learn the transfer function. Extensive\nexperiments on three CV benchmark datasets clearly demonstrate the efficiency\nof our method.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 14:09:17 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Ilievski", "Ilija", ""], ["Feng", "Jiashi", ""]]}, {"id": "1608.00220", "submitter": "Pierre Thodoroff", "authors": "Pierre Thodoroff, Joelle Pineau, Andrew Lim", "title": "Learning Robust Features using Deep Learning for Automatic Seizure\n  Detection", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and evaluate the capacity of a deep neural network to learn robust\nfeatures from EEG to automatically detect seizures. This is a challenging\nproblem because seizure manifestations on EEG are extremely variable both\ninter- and intra-patient. By simultaneously capturing spectral, temporal and\nspatial information our recurrent convolutional neural network learns a general\nspatially invariant representation of a seizure. The proposed approach exceeds\nsignificantly previous results obtained on cross-patient classifiers both in\nterms of sensitivity and false positive rate. Furthermore, our model proves to\nbe robust to missing channel and variable electrode montage.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 14:28:15 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Thodoroff", "Pierre", ""], ["Pineau", "Joelle", ""], ["Lim", "Andrew", ""]]}, {"id": "1608.00242", "submitter": "Konstantinos Georgatzis", "authors": "Konstantinos Georgatzis, Christopher K. I. Williams, Christopher\n  Hawthorne", "title": "Input-Output Non-Linear Dynamical Systems applied to Physiological\n  Condition Monitoring", "comments": "15 pages, 4 figures, Presented at 2016 Machine Learning and\n  Healthcare Conference (MLHC 2016), Los Angeles, CA, camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a non-linear dynamical system for modelling the effect of drug\ninfusions on the vital signs of patients admitted in Intensive Care Units\n(ICUs). More specifically we are interested in modelling the effect of a widely\nused anaesthetic drug (Propofol) on a patient's monitored depth of anaesthesia\nand haemodynamics. We compare our approach with one from the\nPharmacokinetics/Pharmacodynamics (PK/PD) literature and show that we can\nprovide significant improvements in performance without requiring the\nincorporation of expert physiological knowledge in our system.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 16:58:03 GMT"}, {"version": "v2", "created": "Sat, 8 Oct 2016 15:56:32 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Georgatzis", "Konstantinos", ""], ["Williams", "Christopher K. I.", ""], ["Hawthorne", "Christopher", ""]]}, {"id": "1608.00250", "submitter": "Wouter Kouw", "authors": "Wouter M. Kouw and Marco Loog", "title": "On Regularization Parameter Estimation under Covariate Shift", "comments": "6 pages, 2 figures, 2 tables. Accepted to ICPR 2016", "journal-ref": "23rd International Conference on Pattern Recognition (ICPR),\n  Cancun, 2016, pp. 426-431", "doi": "10.1109/ICPR.2016.7899671", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper identifies a problem with the usual procedure for\nL2-regularization parameter estimation in a domain adaptation setting. In such\na setting, there are differences between the distributions generating the\ntraining data (source domain) and the test data (target domain). The usual\ncross-validation procedure requires validation data, which can not be obtained\nfrom the unlabeled target data. The problem is that if one decides to use\nsource validation data, the regularization parameter is underestimated. One\npossible solution is to scale the source validation data through importance\nweighting, but we show that this correction is not sufficient. We conclude the\npaper with an empirical analysis of the effect of several importance weight\nestimators on the estimation of the regularization parameter.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 19:02:39 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Kouw", "Wouter M.", ""], ["Loog", "Marco", ""]]}, {"id": "1608.00318", "submitter": "Sungjin Ahn", "authors": "Sungjin Ahn, Heeyoul Choi, Tanel P\\\"arnamaa, Yoshua Bengio", "title": "A Neural Knowledge Language Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current language models have a significant limitation in the ability to\nencode and decode factual knowledge. This is mainly because they acquire such\nknowledge from statistical co-occurrences although most of the knowledge words\nare rarely observed. In this paper, we propose a Neural Knowledge Language\nModel (NKLM) which combines symbolic knowledge provided by the knowledge graph\nwith the RNN language model. By predicting whether the word to generate has an\nunderlying fact or not, the model can generate such knowledge-related words by\ncopying from the description of the predicted fact. In experiments, we show\nthat the NKLM significantly improves the performance while generating a much\nsmaller number of unknown words.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 04:42:49 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 15:34:01 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Ahn", "Sungjin", ""], ["Choi", "Heeyoul", ""], ["P\u00e4rnamaa", "Tanel", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1608.00359", "submitter": "Nikolas Hemion", "authors": "Nikolas J. Hemion", "title": "Discovering Latent States for Model Learning: Applying Sensorimotor\n  Contingencies Theory and Predictive Processing to Model Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous robots need to be able to adapt to unforeseen situations and to\nacquire new skills through trial and error. Reinforcement learning in principle\noffers a suitable methodological framework for this kind of autonomous\nlearning. However current computational reinforcement learning agents mostly\nlearn each individual skill entirely from scratch. How can we enable artificial\nagents, such as robots, to acquire some form of generic knowledge, which they\ncould leverage for the learning of new skills? This paper argues that, like the\nbrain, the cognitive system of artificial agents has to develop a world model\nto support adaptive behavior and learning. Inspiration is taken from two recent\ndevelopments in the cognitive science literature: predictive processing\ntheories of cognition, and the sensorimotor contingencies theory of perception.\nBased on these, a hypothesis is formulated about what the content of\ninformation might be that is encoded in an internal world model, and how an\nagent could autonomously acquire it. A computational model is described to\nformalize this hypothesis, and is evaluated in a series of simulation\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 09:09:04 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Hemion", "Nikolas J.", ""]]}, {"id": "1608.00466", "submitter": "Madhusudan Lakshmana", "authors": "Madhusudan Lakshmana, Sundararajan Sellamanickam, Shirish Shevade,\n  Keerthi Selvaraj", "title": "Learning Semantically Coherent and Reusable Kernels in Convolution\n  Neural Nets for Sentence Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art CNN models give good performance on sentence\nclassification tasks. The purpose of this work is to empirically study\ndesirable properties such as semantic coherence, attention mechanism and\nreusability of CNNs in these tasks. Semantically coherent kernels are\npreferable as they are a lot more interpretable for explaining the decision of\nthe learned CNN model. We observe that the learned kernels do not have semantic\ncoherence. Motivated by this observation, we propose to learn kernels with\nsemantic coherence using clustering scheme combined with Word2Vec\nrepresentation and domain knowledge such as SentiWordNet. We suggest a\ntechnique to visualize attention mechanism of CNNs for decision explanation\npurpose. Reusable property enables kernels learned on one problem to be used in\nanother problem. This helps in efficient learning as only a few additional\ndomain specific filters may have to be learned. We demonstrate the efficacy of\nour core ideas of learning semantically coherent kernels and leveraging\nreusable kernels for efficient learning on several benchmark datasets.\nExperimental results show the usefulness of our approach by achieving\nperformance close to the state-of-the-art methods but with semantic and\nreusable properties.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 15:14:08 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 03:57:26 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Lakshmana", "Madhusudan", ""], ["Sellamanickam", "Sundararajan", ""], ["Shevade", "Shirish", ""], ["Selvaraj", "Keerthi", ""]]}, {"id": "1608.00530", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks, Kevin Gimpel", "title": "Early Methods for Detecting Adversarial Images", "comments": "ICLR 2017 Workshop Contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning classifiers are vulnerable to adversarial\nperturbations. An adversarial perturbation modifies an input to change a\nclassifier's prediction without causing the input to seem substantially\ndifferent to human perception. We deploy three methods to detect adversarial\nimages. Adversaries trying to bypass our detectors must make the adversarial\nimage less pathological or they will fail trying. Our best detection method\nreveals that adversarial images place abnormal emphasis on the lower-ranked\nprincipal components from PCA. Other detectors and a colorful saliency map are\nin an appendix.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 19:13:58 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 18:03:47 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Hendrycks", "Dan", ""], ["Gimpel", "Kevin", ""]]}, {"id": "1608.00550", "submitter": "Ping Li", "authors": "Ping Li and Cun-Hui Zhang", "title": "Theory of the GMM Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop some theoretical results for a robust similarity measure named\n\"generalized min-max\" (GMM). This similarity has direct applications in machine\nlearning as a positive definite kernel and can be efficiently computed via\nprobabilistic hashing. Owing to the discrete nature, the hashed values can also\nbe used for efficient near neighbor search. We prove the theoretical limit of\nGMM and the consistency result, assuming that the data follow an elliptical\ndistribution, which is a very general family of distributions and includes the\nmultivariate $t$-distribution as a special case. The consistency result holds\nas long as the data have bounded first moment (an assumption which essentially\nholds for datasets commonly encountered in practice). Furthermore, we establish\nthe asymptotic normality of GMM. Compared to the \"cosine\" similarity which is\nroutinely adopted in current practice in statistics and machine learning, the\nconsistency of GMM requires much weaker conditions. Interestingly, when the\ndata follow the $t$-distribution with $\\nu$ degrees of freedom, GMM typically\nprovides a better measure of similarity than \"cosine\" roughly when $\\nu<8$\n(which is already very close to normal). These theoretical results will help\nexplain the recent success of GMM in learning tasks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 19:45:57 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Li", "Ping", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1608.00611", "submitter": "Priyadarshini Panda", "authors": "Priyadarshini Panda, and Kaushik Roy", "title": "Attention Tree: Learning Hierarchies of Visual Features for Large-Scale\n  Image Recognition", "comments": "11 pages, 8 figures, Under review in IEEE Transactions on Neural\n  Networks and Learning systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges in machine learning is to design a computationally\nefficient multi-class classifier while maintaining the output accuracy and\nperformance. In this paper, we present a tree-based classifier: Attention Tree\n(ATree) for large-scale image classification that uses recursive Adaboost\ntraining to construct a visual attention hierarchy. The proposed attention\nmodel is inspired from the biological 'selective tuning mechanism for cortical\nvisual processing'. We exploit the inherent feature similarity across images in\ndatasets to identify the input variability and use recursive optimization\nprocedure, to determine data partitioning at each node, thereby, learning the\nattention hierarchy. A set of binary classifiers is organized on top of the\nlearnt hierarchy to minimize the overall test-time complexity. The attention\nmodel maximizes the margins for the binary classifiers for optimal decision\nboundary modelling, leading to better performance at minimal complexity. The\nproposed framework has been evaluated on both Caltech-256 and SUN datasets and\nachieves accuracy improvement over state-of-the-art tree-based methods at\nsignificantly lower computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 20:51:29 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Panda", "Priyadarshini", ""], ["Roy", "Kaushik", ""]]}, {"id": "1608.00619", "submitter": "Bo-Wei Chen", "authors": "Bo-Wei Chen", "title": "Recursion-Free Online Multiple Incremental/Decremental Analysis Based on\n  Ridge Support Vector Learning", "comments": "Ridge support vector machine (Ridge SVM), Ridge support vector\n  regression (Ridge SVR), multiple incremental learning, multiple decremental\n  learning, online learning, batch learning, cloud computing, big data\n  analysis, data analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a rapid multiple incremental and decremental mechanism\nbased on Weight-Error Curves (WECs) for support-vector analysis. Recursion-free\ncomputation is proposed for predicting the Lagrangian multipliers of new\nsamples. This study examines Ridge Support Vector Models, subsequently devising\na recursion-free function derived from WECs. With the proposed function, all\nthe new Lagrangian multipliers can be computed at once without using any\ngradual step sizes. Moreover, such a function relaxes a constraint, where the\nincrement of new multiple Lagrangian multipliers should be the same in the\nprevious work, thereby easily satisfying the requirement of KKT conditions. The\nproposed mechanism no longer requires typical bookkeeping strategies, which\ncompute the step size by checking all the training samples in each incremental\nround.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 21:13:12 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 19:55:58 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Chen", "Bo-Wei", ""]]}, {"id": "1608.00621", "submitter": "Bo-Wei Chen", "authors": "Bo-Wei Chen, Nik Nailah Binti Abdullah, and Sangoh Park", "title": "Efficient Multiple Incremental Computation for Kernel Ridge Regression\n  with Bayesian Uncertainty Modeling", "comments": "Multiple incremental analysis, multiple decremental analysis,\n  incremental learning, kernel ridge regression (KRR), recursive KRR,\n  uncertainty analysis, kernelized Bayesian regression, Gaussian process, batch\n  learning, online learning, edge computing, fog computing, regression,\n  classification", "journal-ref": null, "doi": "10.1016/j.future.2017.08.053", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents an efficient incremental/decremental approach for big\nstreams based on Kernel Ridge Regression (KRR), a frequently used data analysis\nin cloud centers. To avoid reanalyzing the whole dataset whenever sensors\nreceive new training data, typical incremental KRR used a single-instance\nmechanism for updating an existing system. However, this inevitably increased\nredundant computational time, not to mention applicability to big streams. To\nthis end, the proposed mechanism supports incremental/decremental processing\nfor both single and multiple samples (i.e., batch processing). A large scale of\ndata can be divided into batches, processed by a machine, without sacrificing\nthe accuracy. Moreover, incremental/decremental analyses in empirical and\nintrinsic space are also proposed in this study to handle different types of\ndata either with a large number of samples or high feature dimensions, whereas\ntypical methods focused only on one type. At the end of this study, we further\nthe proposed mechanism to statistical Kernelized Bayesian Regression, so that\nuncertainty modeling with incremental/decremental computation becomes\napplicable. Experimental results showed that computational time was\nsignificantly reduced, better than the original nonincremental design and the\ntypical single incremental method. Furthermore, the accuracy of the proposed\nmethod remained the same as the baselines. This implied that the system\nenhanced efficiency without sacrificing the accuracy. These findings proved\nthat the proposed method was appropriate for variable streaming data analysis,\nthereby demonstrating the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 21:21:07 GMT"}, {"version": "v2", "created": "Sun, 18 Sep 2016 04:15:19 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 03:14:27 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Chen", "Bo-Wei", ""], ["Abdullah", "Nik Nailah Binti", ""], ["Park", "Sangoh", ""]]}, {"id": "1608.00627", "submitter": "Shreyansh Daftry", "authors": "Shreyansh Daftry, J. Andrew Bagnell and Martial Hebert", "title": "Learning Transferable Policies for Monocular Reactive MAV Control", "comments": "International Symposium on Experimental Robotics (ISER 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to transfer knowledge gained in previous tasks into new contexts\nis one of the most important mechanisms of human learning. Despite this,\nadapting autonomous behavior to be reused in partially similar settings is\nstill an open problem in current robotics research. In this paper, we take a\nsmall step in this direction and propose a generic framework for learning\ntransferable motion policies. Our goal is to solve a learning problem in a\ntarget domain by utilizing the training data in a different but related source\ndomain. We present this in the context of an autonomous MAV flight using\nmonocular reactive control, and demonstrate the efficacy of our proposed\napproach through extensive real-world flight experiments in outdoor cluttered\nenvironments.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 21:53:04 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Daftry", "Shreyansh", ""], ["Bagnell", "J. Andrew", ""], ["Hebert", "Martial", ""]]}, {"id": "1608.00647", "submitter": "Jake Marcus", "authors": "Narges Razavian, Jake Marcus, David Sontag", "title": "Multi-task Prediction of Disease Onsets from Longitudinal Lab Tests", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disparate areas of machine learning have benefited from models that can take\nraw data with little preprocessing as input and learn rich representations of\nthat raw data in order to perform well on a given prediction task. We evaluate\nthis approach in healthcare by using longitudinal measurements of lab tests,\none of the more raw signals of a patient's health state widely available in\nclinical data, to predict disease onsets. In particular, we train a Long\nShort-Term Memory (LSTM) recurrent neural network and two novel convolutional\nneural networks for multi-task prediction of disease onset for 133 conditions\nbased on 18 common lab tests measured over time in a cohort of 298K patients\nderived from 8 years of administrative claims data. We compare the neural\nnetworks to a logistic regression with several hand-engineered, clinically\nrelevant features. We find that the representation-based learning approaches\nsignificantly outperform this baseline. We believe that our work suggests a new\navenue for patient risk stratification based solely on lab results.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 00:09:22 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 23:55:56 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 21:55:00 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Razavian", "Narges", ""], ["Marcus", "Jake", ""], ["Sontag", "David", ""]]}, {"id": "1608.00667", "submitter": "Hong-Min Chu", "authors": "Hong-Min Chu, Hsuan-Tien Lin", "title": "Can Active Learning Experience Be Transferred?", "comments": "10 pages, 8 figs, 4 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning is an important machine learning problem in reducing the\nhuman labeling effort. Current active learning strategies are designed from\nhuman knowledge, and are applied on each dataset in an immutable manner. In\nother words, experience about the usefulness of strategies cannot be updated\nand transferred to improve active learning on other datasets. This paper\ninitiates a pioneering study on whether active learning experience can be\ntransferred. We first propose a novel active learning model that linearly\naggregates existing strategies. The linear weights can then be used to\nrepresent the active learning experience. We equip the model with the popular\nlinear upper- confidence-bound (LinUCB) algorithm for contextual bandit to\nupdate the weights. Finally, we extend our model to transfer the experience\nacross datasets with the technique of biased regularization. Empirical studies\ndemonstrate that the learned experience not only is competitive with existing\nstrategies on most single datasets, but also can be transferred across datasets\nto improve the performance on future learning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 01:30:25 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Chu", "Hong-Min", ""], ["Lin", "Hsuan-Tien", ""]]}, {"id": "1608.00686", "submitter": "David Sontag", "authors": "Yoni Halpern and Steven Horng and David Sontag", "title": "Clinical Tagging with Joint Probabilistic Models", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for parameter estimation in bipartite probabilistic\ngraphical models for joint prediction of clinical conditions from the\nelectronic medical record. The method does not rely on the availability of\ngold-standard labels, but rather uses noisy labels, called anchors, for\nlearning. We provide a likelihood-based objective and a moments-based\ninitialization that are effective at learning the model parameters. The learned\nmodel is evaluated in a task of assigning a heldout clinical condition to\npatients based on retrospective analysis of the records, and outperforms\nbaselines which do not account for the noisiness in the labels or do not model\nthe conditions jointly.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 03:09:59 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 15:36:13 GMT"}, {"version": "v3", "created": "Thu, 22 Sep 2016 00:37:40 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Halpern", "Yoni", ""], ["Horng", "Steven", ""], ["Sontag", "David", ""]]}, {"id": "1608.00704", "submitter": "Shalmali Joshi", "authors": "Shalmali Joshi, Suriya Gunasekar, David Sontag, Joydeep Ghosh", "title": "Identifiable Phenotyping using Constrained Non-Negative Matrix\n  Factorization", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new algorithm for automated and simultaneous phenotyping\nof multiple co-occurring medical conditions, also referred as comorbidities,\nusing clinical notes from the electronic health records (EHRs). A basic latent\nfactor estimation technique of non-negative matrix factorization (NMF) is\naugmented with domain specific constraints to obtain sparse latent factors that\nare anchored to a fixed set of chronic conditions. The proposed anchoring\nmechanism ensures a one-to-one identifiable and interpretable mapping between\nthe latent factors and the target comorbidities. Qualitative assessment of the\nempirical results by clinical experts suggests that the proposed model learns\nclinically interpretable phenotypes while being predictive of 30 day mortality.\nThe proposed method can be readily adapted to any non-negative EHR data across\nvarious healthcare institutions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 06:03:53 GMT"}, {"version": "v2", "created": "Wed, 24 Aug 2016 18:02:34 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 13:01:04 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Joshi", "Shalmali", ""], ["Gunasekar", "Suriya", ""], ["Sontag", "David", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1608.00712", "submitter": "Timothy La Fond", "authors": "Timothy La Fond, Jennifer Neville, Brian Gallagher", "title": "Size-Consistent Statistics for Anomaly Detection in Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task in network analysis is the detection of anomalous events in\na network time series. These events could merely be times of interest in the\nnetwork timeline or they could be examples of malicious activity or network\nmalfunction. Hypothesis testing using network statistics to summarize the\nbehavior of the network provides a robust framework for the anomaly detection\ndecision process. Unfortunately, choosing network statistics that are dependent\non confounding factors like the total number of nodes or edges can lead to\nincorrect conclusions (e.g., false positives and false negatives). In this\ndissertation we describe the challenges that face anomaly detection in dynamic\nnetwork streams regarding confounding factors. We also provide two solutions to\navoiding error due to confounding factors: the first is a randomization testing\nmethod that controls for confounding factors, and the second is a set of\nsize-consistent network statistics which avoid confounding due to the most\ncommon factors, edge count and node count.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 06:55:44 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["La Fond", "Timothy", ""], ["Neville", "Jennifer", ""], ["Gallagher", "Brian", ""]]}, {"id": "1608.00737", "submitter": "Nikolas Hemion", "authors": "Nikolas J. Hemion", "title": "Context Discovery for Model Learning in Partially Observable\n  Environments", "comments": "6th Joint IEEE International Conference on Development and Learning\n  and on Epigenetic Robotics (IEEE ICDL-EPIROB 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to learn a model is essential for the success of autonomous\nagents. Unfortunately, learning a model is difficult in partially observable\nenvironments, where latent environmental factors influence what the agent\nobserves. In the absence of a supervisory training signal, autonomous agents\ntherefore require a mechanism to autonomously discover these environmental\nfactors, or sensorimotor contexts.\n  This paper presents a method to discover sensorimotor contexts in partially\nobservable environments, by constructing a hierarchical transition model. The\nmethod is evaluated in a simulation experiment, in which a robot learns that\ndifferent rooms are characterized by different objects that are found in them.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 08:57:14 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Hemion", "Nikolas J.", ""]]}, {"id": "1608.00778", "submitter": "Maja Rudolph", "authors": "Maja R. Rudolph, Francisco J. R. Ruiz, Stephan Mandt, David M. Blei", "title": "Exponential Family Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are a powerful approach for capturing semantic similarity\namong terms in a vocabulary. In this paper, we develop exponential family\nembeddings, a class of methods that extends the idea of word embeddings to\nother types of high-dimensional data. As examples, we studied neural data with\nreal-valued observations, count data from a market basket analysis, and ratings\ndata from a movie recommendation system. The main idea is to model each\nobservation conditioned on a set of other observations. This set is called the\ncontext, and the way the context is defined is a modeling choice that depends\non the problem. In language the context is the surrounding words; in\nneuroscience the context is close-by neurons; in market basket data the context\nis other items in the shopping cart. Each type of embedding model defines the\ncontext, the exponential family of conditional distributions, and how the\nlatent embedding vectors are shared across data. We infer the embeddings with a\nscalable algorithm based on stochastic gradient descent. On all three\napplications - neural activity of zebrafish, users' shopping behavior, and\nmovie ratings - we found exponential family embedding models to be more\neffective than other types of dimension reduction. They better reconstruct\nheld-out data and find interesting qualitative structure.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 11:44:19 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 15:12:54 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Rudolph", "Maja R.", ""], ["Ruiz", "Francisco J. R.", ""], ["Mandt", "Stephan", ""], ["Blei", "David M.", ""]]}, {"id": "1608.00781", "submitter": "Edward J. Yoon", "authors": "Edward J. Yoon", "title": "Horn: A System for Parallel Training and Regularizing of Large-Scale\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "EP-909420F9A6E94B3691E5EE413DAD353E", "categories": "cs.DC cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  I introduce a new distributed system for effective training and regularizing\nof Large-Scale Neural Networks on distributed computing architectures. The\nexperiments demonstrate the effectiveness of flexible model partitioning and\nparallelization strategies based on neuron-centric computation model, with an\nimplementation of the collective and parallel dropout neural networks training.\nExperiments are performed on MNIST handwritten digits classification including\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 11:57:09 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 22:01:12 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Yoon", "Edward J.", ""]]}, {"id": "1608.00835", "submitter": "Suleiman Yerima", "authors": "Suleiman Y. Yerima, Sakir Sezer, Igor Muttik", "title": "High Accuracy Android Malware Detection Using Ensemble Learning", "comments": null, "journal-ref": "IET Information Security 9 (6), April 2015, pp. 313-320", "doi": "10.1049/iet-ifs.2014.0099", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With over 50 billion downloads and more than 1.3 million apps in the Google\nofficial market, Android has continued to gain popularity amongst smartphone\nusers worldwide. At the same time there has been a rise in malware targeting\nthe platform, with more recent strains employing highly sophisticated detection\navoidance techniques. As traditional signature based methods become less potent\nin detecting unknown malware, alternatives are needed for timely zero-day\ndiscovery. Thus this paper proposes an approach that utilizes ensemble learning\nfor Android malware detection. It combines advantages of static analysis with\nthe efficiency and performance of ensemble machine learning to improve Android\nmalware detection accuracy. The machine learning models are built using a large\nrepository of malware samples and benign apps from a leading antivirus vendor.\nExperimental results and analysis presented shows that the proposed method\nwhich uses a large feature space to leverage the power of ensemble learning is\ncapable of 97.3 to 99 percent detection accuracy with very low false positive\nrates.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 14:24:47 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Yerima", "Suleiman Y.", ""], ["Sezer", "Sakir", ""], ["Muttik", "Igor", ""]]}, {"id": "1608.00842", "submitter": "Peter Sch\\\"uffler", "authors": "Peter J. Sch\\\"uffler, Judy Sarungbam, Hassan Muhammad, Ed Reznik,\n  Satish K. Tickoo, Thomas J. Fuchs", "title": "Mitochondria-based Renal Cell Carcinoma Subtyping: Learning from Deep\n  vs. Flat Feature Representations", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accurate subtyping of renal cell carcinoma (RCC) is of crucial importance for\nunderstanding disease progression and for making informed treatment decisions.\nNew discoveries of significant alterations to mitochondria between subtypes\nmake immunohistochemical (IHC) staining based image classification an\nimperative. Until now, accurate quantification and subtyping was made\nimpossible by huge IHC variations, the absence of cell membrane staining for\ncytoplasm segmentation as well as the complete lack of systems for robust and\nreproducible image based classification. In this paper we present a\ncomprehensive classification framework to overcome these challenges for tissue\nmicroarrays (TMA) of RCCs. We compare and evaluate models based on domain\nspecific hand-crafted \"flat\"-features versus \"deep\" feature representations\nfrom various layers of a pre-trained convolutional neural network (CNN). The\nbest model reaches a cross-validation accuracy of 89%, which demonstrates for\nthe first time, that robust mitochondria-based subtyping of renal cancer is\nfeasible\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 14:38:02 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Sch\u00fcffler", "Peter J.", ""], ["Sarungbam", "Judy", ""], ["Muhammad", "Hassan", ""], ["Reznik", "Ed", ""], ["Tickoo", "Satish K.", ""], ["Fuchs", "Thomas J.", ""]]}, {"id": "1608.00848", "submitter": "Suleiman Yerima", "authors": "Suleiman Y. Yerima, Sakir Sezer, Gavin McWilliams, Igor Muttik", "title": "A New Android Malware Detection Approach Using Bayesian Classification", "comments": "IEEE 27th International Conference on Advanced Information Networking\n  and Applications (AINA 2013),pp.121-128, 25-28 March 2013", "journal-ref": null, "doi": "10.1109/AINA.2013.88", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile malware has been growing in scale and complexity as smartphone usage\ncontinues to rise. Android has surpassed other mobile platforms as the most\npopular whilst also witnessing a dramatic increase in malware targeting the\nplatform. A worrying trend that is emerging is the increasing sophistication of\nAndroid malware to evade detection by traditional signature-based scanners. As\nsuch, Android app marketplaces remain at risk of hosting malicious apps that\ncould evade detection before being downloaded by unsuspecting users. Hence, in\nthis paper we present an effective approach to alleviate this problem based on\nBayesian classification models obtained from static code analysis. The models\nare built from a collection of code and app characteristics that provide\nindicators of potential malicious activities. The models are evaluated with\nreal malware samples in the wild and results of experiments are presented to\ndemonstrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 14:48:49 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Yerima", "Suleiman Y.", ""], ["Sezer", "Sakir", ""], ["McWilliams", "Gavin", ""], ["Muttik", "Igor", ""]]}, {"id": "1608.00853", "submitter": "Daniel Roy", "authors": "Gintare Karolina Dziugaite, Zoubin Ghahramani, Daniel M. Roy", "title": "A study of the effect of JPG compression on adversarial images", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network image classifiers are known to be vulnerable to adversarial\nimages, i.e., natural images which have been modified by an adversarial\nperturbation specifically designed to be imperceptible to humans yet fool the\nclassifier. Not only can adversarial images be generated easily, but these\nimages will often be adversarial for networks trained on disjoint subsets of\ndata or with different architectures. Adversarial images represent a potential\nsecurity risk as well as a serious machine learning challenge---it is clear\nthat vulnerable neural networks perceive images very differently from humans.\nNoting that virtually every image classification data set is composed of JPG\nimages, we evaluate the effect of JPG compression on the classification of\nadversarial images. For Fast-Gradient-Sign perturbations of small magnitude, we\nfound that JPG compression often reverses the drop in classification accuracy\nto a large extent, but not always. As the magnitude of the perturbations\nincreases, JPG recompression alone is insufficient to reverse the effect.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 14:57:18 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Dziugaite", "Gintare Karolina", ""], ["Ghahramani", "Zoubin", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1608.00860", "submitter": "Jie Chen", "authors": "Jie Chen, Haim Avron, Vikas Sindhwani", "title": "Hierarchically Compositional Kernels for Scalable Nonparametric Learning", "comments": "Journal of Machine Learning Research, vol 18, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel class of kernels to alleviate the high computational cost\nof large-scale nonparametric learning with kernel methods. The proposed kernel\nis defined based on a hierarchical partitioning of the underlying data domain,\nwhere the Nystr\\\"om method (a globally low-rank approximation) is married with\na locally lossless approximation in a hierarchical fashion. The kernel\nmaintains (strict) positive-definiteness. The corresponding kernel matrix\nadmits a recursively off-diagonal low-rank structure, which allows for fast\nlinear algebra computations. Suppressing the factor of data dimension, the\nmemory and arithmetic complexities for training a regression or a classifier\nare reduced from $O(n^2)$ and $O(n^3)$ to $O(nr)$ and $O(nr^2)$, respectively,\nwhere $n$ is the number of training examples and $r$ is the rank on each level\nof the hierarchy. Although other randomized approximate kernels entail a\nsimilar complexity, empirical results show that the proposed kernel achieves a\nmatching performance with a smaller $r$. We demonstrate comprehensive\nexperiments to show the effective use of the proposed kernel on data sizes up\nto the order of millions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 15:07:25 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 15:11:25 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Chen", "Jie", ""], ["Avron", "Haim", ""], ["Sindhwani", "Vikas", ""]]}, {"id": "1608.00866", "submitter": "Suleiman Yerima", "authors": "BooJoong Kang, Suleiman Y. Yerima, Kieran McLaughlin, Sakir Sezer", "title": "PageRank in Malware Categorization", "comments": "In RACS:Proceedings of the 2015 Conference on Research in Adaptive\n  and Convergent Systems. (pp. 291-295). Czech Republic: Association for\n  Computing Machinery (ACM)", "journal-ref": null, "doi": "10.1145/2811411.2811514", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a malware categorization method that models malware\nbehavior in terms of instructions using PageRank. PageRank computes ranks of\nweb pages based on structural information and can also compute ranks of\ninstructions that represent the structural information of the instructions in\nmalware analysis methods. Our malware categorization method uses the computed\nranks as features in machine learning algorithms. In the evaluation, we compare\nthe effectiveness of different PageRank algorithms and also investigate bagging\nand boosting algorithms to improve the categorization accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 15:26:41 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Kang", "BooJoong", ""], ["Yerima", "Suleiman Y.", ""], ["McLaughlin", "Kieran", ""], ["Sezer", "Sakir", ""]]}, {"id": "1608.00876", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi, Rong Zhou, Nesreen K. Ahmed", "title": "Relational Similarity Machines", "comments": "MLG16", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Relational Similarity Machines (RSM): a fast, accurate,\nand flexible relational learning framework for supervised and semi-supervised\nlearning tasks. Despite the importance of relational learning, most existing\nmethods are hard to adapt to different settings, due to issues with efficiency,\nscalability, accuracy, and flexibility for handling a wide variety of\nclassification problems, data, constraints, and tasks. For instance, many\nexisting methods perform poorly for multi-class classification problems, graphs\nthat are sparsely labeled or network data with low relational autocorrelation.\nIn contrast, the proposed relational learning framework is designed to be (i)\nfast for learning and inference at real-time interactive rates, and (ii)\nflexible for a variety of learning settings (multi-class problems), constraints\n(few labeled instances), and application domains. The experiments demonstrate\nthe effectiveness of RSM for a variety of tasks and data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 15:48:58 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Zhou", "Rong", ""], ["Ahmed", "Nesreen K.", ""]]}, {"id": "1608.00895", "submitter": "Patrick Doetsch", "authors": "Patrick Doetsch, Albert Zeyer, Paul Voigtlaender, Ilya Kulikov, Ralf\n  Schl\\\"uter, Hermann Ney", "title": "RETURNN: The RWTH Extensible Training framework for Universal Recurrent\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we release our extensible and easily configurable neural network\ntraining software. It provides a rich set of functional layers with a\nparticular focus on efficient training of recurrent neural network topologies\non multiple GPUs. The source of the software package is public and freely\navailable for academic research purposes and can be used as a framework or as a\nstandalone tool which supports a flexible configuration. The software allows to\ntrain state-of-the-art deep bidirectional long short-term memory (LSTM) models\non both one dimensional data like speech or two dimensional data like\nhandwritten text and was used to develop successful submission systems in\nseveral evaluation campaigns.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 16:43:27 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 14:25:28 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Doetsch", "Patrick", ""], ["Zeyer", "Albert", ""], ["Voigtlaender", "Paul", ""], ["Kulikov", "Ilya", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "1608.00920", "submitter": "Shun Kataoka", "authors": "Shun Kataoka, Takuto Kobayashi, Muneki Yasuda, and Kazuyuki Tanaka", "title": "Community Detection Algorithm Combining Stochastic Block Model and\n  Attribute Data Clustering", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": "10.7566/JPSJ.85.114802", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm to detect the community structure in a network\nthat utilizes both the network structure and vertex attribute data. Suppose we\nhave the network structure together with the vertex attribute data, that is,\nthe information assigned to each vertex associated with the community to which\nit belongs. The problem addressed this paper is the detection of the community\nstructure from the information of both the network structure and the vertex\nattribute data. Our approach is based on the Bayesian approach that models the\nposterior probability distribution of the community labels. The detection of\nthe community structure in our method is achieved by using belief propagation\nand an EM algorithm. We numerically verified the performance of our method\nusing computer-generated networks and real-world networks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 10:21:08 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Kataoka", "Shun", ""], ["Kobayashi", "Takuto", ""], ["Yasuda", "Muneki", ""], ["Tanaka", "Kazuyuki", ""]]}, {"id": "1608.01072", "submitter": "Fateme Fahiman Mrs", "authors": "Fateme Fahiman, Jame C.Bezdek, Sarah M.Erfani, Christopher Leckie,\n  Marimuthu Palaniswami", "title": "Fuzzy c-Shape: A new algorithm for clustering finite time series\n  waveforms", "comments": "12 pages, 3 figures, submitted at IEEE Transaction on Fuzzy Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existence of large volumes of time series data in many applications has\nmotivated data miners to investigate specialized methods for mining time series\ndata. Clustering is a popular data mining method due to its powerful\nexploratory nature and its usefulness as a preprocessing step for other data\nmining techniques. This article develops two novel clustering algorithms for\ntime series data that are extensions of a crisp c-shapes algorithm. The two new\nalgorithms are heuristic derivatives of fuzzy c-means (FCM). Fuzzy c-Shapes\nplus (FCS+) replaces the inner product norm in the FCM model with a shape-based\ndistance function. Fuzzy c-Shapes double plus (FCS++) uses the shape-based\ndistance, and also replaces the FCM cluster centers with shape-extracted\nprototypes. Numerical experiments on 48 real time series data sets show that\nthe two new algorithms outperform state-of-the-art shape-based clustering\nalgorithms in terms of accuracy and efficiency. Four external cluster validity\nindices (the Rand index, Adjusted Rand Index, Variation of Information, and\nNormalized Mutual Information) are used to match candidate partitions generated\nby each of the studied algorithms. All four indices agree that for these finite\nwaveform data sets, FCS++ gives a small improvement over FCS+, and in turn,\nFCS+ is better than the original crisp c-shapes method. Finally, we apply two\ntests of statistical significance to the three algorithms. The Wilcoxon and\nFriedman statistics both rank the three algorithms in exactly the same way as\nthe four cluster validity indices.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 04:42:02 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Fahiman", "Fateme", ""], ["Bezdek", "Jame C.", ""], ["Erfani", "Sarah M.", ""], ["Leckie", "Christopher", ""], ["Palaniswami", "Marimuthu", ""]]}, {"id": "1608.01127", "submitter": "Alban Laflaqui\\`ere Dr", "authors": "Alban Laflaqui\\`ere", "title": "Autonomous Grounding of Visual Field Experience through Sensorimotor\n  Prediction", "comments": "6 pages, 4 figures, ICDL-Epirob 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a developmental framework, autonomous robots need to explore the world and\nlearn how to interact with it. Without an a priori model of the system, this\nopens the challenging problem of having robots master their interface with the\nworld: how to perceive their environment using their sensors, and how to act in\nit using their motors. The sensorimotor approach of perception claims that a\nnaive agent can learn to master this interface by capturing regularities in the\nway its actions transform its sensory inputs. In this paper, we apply such an\napproach to the discovery and mastery of the visual field associated with a\nvisual sensor. A computational model is formalized and applied to a simulated\nsystem to illustrate the approach.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 09:25:35 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Laflaqui\u00e8re", "Alban", ""]]}, {"id": "1608.01198", "submitter": "Dong Huang", "authors": "Dong Huang, Chang-Dong Wang, Jian-Huang Lai, Yun Liang, Shan Bian, Yu\n  Chen", "title": "Ensemble-driven support vector clustering: From ensemble learning to\n  automatic parameter estimation", "comments": "To appear in ICPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector clustering (SVC) is a versatile clustering technique that is\nable to identify clusters of arbitrary shapes by exploiting the kernel trick.\nHowever, one hurdle that restricts the application of SVC lies in its\nsensitivity to the kernel parameter and the trade-off parameter. Although many\nextensions of SVC have been developed, to the best of our knowledge, there is\nstill no algorithm that is able to effectively estimate the two crucial\nparameters in SVC without supervision. In this paper, we propose a novel\nsupport vector clustering approach termed ensemble-driven support vector\nclustering (EDSVC), which for the first time tackles the automatic parameter\nestimation problem for SVC based on ensemble learning, and is capable of\nproducing robust clustering results in a purely unsupervised manner.\nExperimental results on multiple real-world datasets demonstrate the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 14:19:00 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 15:28:15 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Huang", "Dong", ""], ["Wang", "Chang-Dong", ""], ["Lai", "Jian-Huang", ""], ["Liang", "Yun", ""], ["Bian", "Shan", ""], ["Chen", "Yu", ""]]}, {"id": "1608.01230", "submitter": "Eder Santana", "authors": "Eder Santana, George Hotz", "title": "Learning a Driving Simulator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comma.ai's approach to Artificial Intelligence for self-driving cars is based\non an agent that learns to clone driver behaviors and plans maneuvers by\nsimulating future events in the road. This paper illustrates one of our\nresearch approaches for driving simulation. One where we learn to simulate.\nHere we investigate variational autoencoders with classical and learned cost\nfunctions using generative adversarial networks for embedding road frames.\nAfterwards, we learn a transition model in the embedded space using action\nconditioned Recurrent Neural Networks. We show that our approach can keep\npredicting realistic looking video for several frames despite the transition\nmodel being optimized without a cost function in the pixel space.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 15:49:12 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Santana", "Eder", ""], ["Hotz", "George", ""]]}, {"id": "1608.01238", "submitter": "Manuel Ciosici", "authors": "Manuel R. Ciosici", "title": "Improving Quality of Hierarchical Clustering for Large Data Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Brown clustering is a hard, hierarchical, bottom-up clustering of words in a\nvocabulary. Words are assigned to clusters based on their usage pattern in a\ngiven corpus. The resulting clusters and hierarchical structure can be used in\nconstructing class-based language models and for generating features to be used\nin NLP tasks. Because of its high computational cost, the most-used version of\nBrown clustering is a greedy algorithm that uses a window to restrict its\nsearch space. Like other clustering algorithms, Brown clustering finds a\nsub-optimal, but nonetheless effective, mapping of words to clusters. Because\nof its ability to produce high-quality, human-understandable cluster, Brown\nclustering has seen high uptake the NLP research community where it is used in\nthe preprocessing and feature generation steps.\n  Little research has been done towards improving the quality of Brown\nclusters, despite the greedy and heuristic nature of the algorithm. The\napproaches tried so far have focused on: studying the effect of the\ninitialisation in a similar algorithm; tuning the parameters used to define the\ndesired number of clusters and the behaviour of the algorithm; and including a\nseparate parameter to differentiate the window from the desired number of\nclusters. However, some of these approaches have not yielded significant\nimprovements in cluster quality.\n  In this thesis, a close analysis of the Brown algorithm is provided,\nrevealing important under-specifications and weaknesses in the original\nalgorithm. These have serious effects on cluster quality and reproducibility of\nresearch using Brown clustering. In the second part of the thesis, two\nmodifications are proposed. Finally, a thorough evaluation is performed,\nconsidering both the optimization criterion of Brown clustering and the\nperformance of the resulting class-based language models.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 16:12:23 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Ciosici", "Manuel R.", ""]]}, {"id": "1608.01264", "submitter": "Niao He", "authors": "Niao He, Zaid Harchaoui, Yichen Wang, Le Song", "title": "Fast and Simple Optimization for Poisson Likelihood Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poisson likelihood models have been prevalently used in imaging, social\nnetworks, and time series analysis. We propose fast, simple,\ntheoretically-grounded, and versatile, optimization algorithms for Poisson\nlikelihood modeling. The Poisson log-likelihood is concave but not\nLipschitz-continuous. Since almost all gradient-based optimization algorithms\nrely on Lipschitz-continuity, optimizing Poisson likelihood models with a\nguarantee of convergence can be challenging, especially for large-scale\nproblems.\n  We present a new perspective allowing to efficiently optimize a wide range of\npenalized Poisson likelihood objectives. We show that an appropriate saddle\npoint reformulation enjoys a favorable geometry and a smooth structure.\nTherefore, we can design a new gradient-based optimization algorithm with\n$O(1/t)$ convergence rate, in contrast to the usual $O(1/\\sqrt{t})$ rate of\nnon-smooth minimization alternatives. Furthermore, in order to tackle problems\nwith large samples, we also develop a randomized block-decomposition variant\nthat enjoys the same convergence rate yet more efficient iteration cost.\n  Experimental results on several point process applications including social\nnetwork estimation and temporal recommendation show that the proposed algorithm\nand its randomized block variant outperform existing methods both on synthetic\nand real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 17:33:16 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["He", "Niao", ""], ["Harchaoui", "Zaid", ""], ["Wang", "Yichen", ""], ["Song", "Le", ""]]}, {"id": "1608.01281", "submitter": "Navdeep Jaitly", "authors": "Yuping Luo, Chung-Cheng Chiu, Navdeep Jaitly, Ilya Sutskever", "title": "Learning Online Alignments with Continuous Rewards Policy Gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence models with soft attention had significant success in\nmachine translation, speech recognition, and question answering. Though capable\nand easy to use, they require that the entirety of the input sequence is\navailable at the beginning of inference, an assumption that is not valid for\ninstantaneous translation and speech recognition. To address this problem, we\npresent a new method for solving sequence-to-sequence problems using hard\nonline alignments instead of soft offline alignments. The online alignments\nmodel is able to start producing outputs without the need to first process the\nentire input sequence. A highly accurate online sequence-to-sequence model is\nuseful because it can be used to build an accurate voice-based instantaneous\ntranslator. Our model uses hard binary stochastic decisions to select the\ntimesteps at which outputs will be produced. The model is trained to produce\nthese stochastic decisions using a standard policy gradient method. In our\nexperiments, we show that this model achieves encouraging performance on TIMIT\nand Wall Street Journal (WSJ) speech recognition datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 18:35:12 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Luo", "Yuping", ""], ["Chiu", "Chung-Cheng", ""], ["Jaitly", "Navdeep", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1608.01410", "submitter": "Hyun-Chul Kim", "authors": "Hyun-Chul Kim", "title": "Bayesian Kernel and Mutual $k$-Nearest Neighbor Regression", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Bayesian extensions of two nonparametric regression methods which\nare kernel and mutual $k$-nearest neighbor regression methods. Derived based on\nGaussian process models for regression, the extensions provide distributions\nfor target value estimates and the framework to select the hyperparameters. It\nis shown that both the proposed methods asymptotically converge to kernel and\nmutual $k$-nearest neighbor regression methods, respectively. The simulation\nresults show that the proposed methods can select proper hyperparameters and\nare better than or comparable to the former methods for an artificial data set\nand a real world data set.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 01:33:34 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Kim", "Hyun-Chul", ""]]}, {"id": "1608.01747", "submitter": "Yukun Chen", "authors": "Yukun Chen, Jianbo Ye, and Jia Li", "title": "A Distance for HMMs based on Aggregated Wasserstein Metric and State\n  Registration", "comments": "submitted to ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework, named Aggregated Wasserstein, for computing a\ndissimilarity measure or distance between two Hidden Markov Models with state\nconditional distributions being Gaussian. For such HMMs, the marginal\ndistribution at any time spot follows a Gaussian mixture distribution, a fact\nexploited to softly match, aka register, the states in two HMMs. We refer to\nsuch HMMs as Gaussian mixture model-HMM (GMM-HMM). The registration of states\nis inspired by the intrinsic relationship of optimal transport and the\nWasserstein metric between distributions. Specifically, the components of the\nmarginal GMMs are matched by solving an optimal transport problem where the\ncost between components is the Wasserstein metric for Gaussian distributions.\nThe solution of the optimization problem is a fast approximation to the\nWasserstein metric between two GMMs. The new Aggregated Wasserstein distance is\na semi-metric and can be computed without generating Monte Carlo samples. It is\ninvariant to relabeling or permutation of the states. This distance quantifies\nthe dissimilarity of GMM-HMMs by measuring both the difference between the two\nmarginal GMMs and the difference between the two transition matrices. Our new\ndistance is tested on the tasks of retrieval and classification of time series.\nExperiments on both synthetic data and real data have demonstrated its\nadvantages in terms of accuracy as well as efficiency in comparison with\nexisting distances based on the Kullback-Leibler divergence.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 03:37:46 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Chen", "Yukun", ""], ["Ye", "Jianbo", ""], ["Li", "Jia", ""]]}, {"id": "1608.01874", "submitter": "Avisek Lahiri", "authors": "Avisek Lahiri, Biswajit Paria, Prabir Kumar Biswas", "title": "Forward Stagewise Additive Model for Collaborative Multiview Boosting", "comments": "The manuscript is currently under \"Minor Revision\" at IEEE\n  Transactions on Neural Networks and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiview assisted learning has gained significant attention in recent years\nin supervised learning genre. Availability of high performance computing\ndevices enables learning algorithms to search simultaneously over multiple\nviews or feature spaces to obtain an optimum classification performance. The\npaper is a pioneering attempt of formulating a mathematical foundation for\nrealizing a multiview aided collaborative boosting architecture for multiclass\nclassification. Most of the present algorithms apply multiview learning\nheuristically without exploring the fundamental mathematical changes imposed on\ntraditional boosting. Also, most of the algorithms are restricted to two class\nor view setting. Our proposed mathematical framework enables collaborative\nboosting across any finite dimensional view spaces for multiclass learning. The\nboosting framework is based on forward stagewise additive model which minimizes\na novel exponential loss function. We show that the exponential loss function\nessentially captures difficulty of a training sample space instead of the\ntraditional `1/0' loss. The new algorithm restricts a weak view from over\nlearning and thereby preventing overfitting. The model is inspired by our\nearlier attempt on collaborative boosting which was devoid of mathematical\njustification. The proposed algorithm is shown to converge much nearer to\nglobal minimum in the exponential loss space and thus supersedes our previous\nalgorithm. The paper also presents analytical and numerical analysis of\nconvergence and margin bounds for multiview boosting algorithms and we show\nthat our proposed ensemble learning manifests lower error bound and higher\nmargin compared to our previous model. Also, the proposed model is compared\nwith traditional boosting and recent multiview boosting algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 13:19:47 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Lahiri", "Avisek", ""], ["Paria", "Biswajit", ""], ["Biswas", "Prabir Kumar", ""]]}, {"id": "1608.01976", "submitter": "Rashish Tandon", "authors": "Rashish Tandon, Si Si, Pradeep Ravikumar, Inderjit Dhillon", "title": "Kernel Ridge Regression via Partitioning", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a divide and conquer approach to Kernel Ridge\nRegression (KRR). Given n samples, the division step involves separating the\npoints based on some underlying disjoint partition of the input space (possibly\nvia clustering), and then computing a KRR estimate for each partition. The\nconquering step is simple: for each partition, we only consider its own local\nestimate for prediction. We establish conditions under which we can give\ngeneralization bounds for this estimator, as well as achieve optimal minimax\nrates. We also show that the approximation error component of the\ngeneralization error is lesser than when a single KRR estimate is fit on the\ndata: thus providing both statistical and computational advantages over a\nsingle KRR estimate over the entire data (or an averaging over random\npartitions as in other recent work, [30]). Lastly, we provide experimental\nvalidation for our proposed estimator and our assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 19:02:19 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Tandon", "Rashish", ""], ["Si", "Si", ""], ["Ravikumar", "Pradeep", ""], ["Dhillon", "Inderjit", ""]]}, {"id": "1608.02010", "submitter": "Si Si", "authors": "Cho-Jui Hsieh and Si Si and Inderjit S. Dhillon", "title": "Communication-Efficient Parallel Block Minimization for Kernel Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel machines often yield superior predictive performance on various tasks;\nhowever, they suffer from severe computational challenges. In this paper, we\nshow how to overcome the important challenge of speeding up kernel machines. In\nparticular, we develop a parallel block minimization framework for solving\nkernel machines, including kernel SVM and kernel logistic regression. Our\nframework proceeds by dividing the problem into smaller subproblems by forming\na block-diagonal approximation of the Hessian matrix. The subproblems are then\nsolved approximately in parallel. After that, a communication efficient line\nsearch procedure is developed to ensure sufficient reduction of the objective\nfunction value at each iteration. We prove global linear convergence rate of\nthe proposed method with a wide class of subproblem solvers, and our analysis\ncovers strongly convex and some non-strongly convex functions. We apply our\nalgorithm to solve large-scale kernel SVM problems on distributed systems, and\nshow a significant improvement over existing parallel solvers. As an example,\non the covtype dataset with half-a-million samples, our algorithm can obtain an\napproximate solution with 96% accuracy in 20 seconds using 32 machines, while\nall the other parallel kernel SVM solvers require more than 2000 seconds to\nachieve a solution with 95% accuracy. Moreover, our algorithm can scale to very\nlarge data sets, such as the kdd algebra dataset with 8 million samples and 20\nmillion features.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 20:15:51 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Hsieh", "Cho-Jui", ""], ["Si", "Si", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1608.02071", "submitter": "Yun Liu", "authors": "Yun Liu, Kun-Ta Chuang, Fu-Wen Liang, Huey-Jen Su, Collin M. Stultz,\n  John V. Guttag", "title": "Transferring Knowledge from Text to Predict Disease Onset", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": "Proceedings of Machine Learning Research Volume 56", "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many domains such as medicine, training data is in short supply. In such\ncases, external knowledge is often helpful in building predictive models. We\npropose a novel method to incorporate publicly available domain expertise to\nbuild accurate models. Specifically, we use word2vec models trained on a\ndomain-specific corpus to estimate the relevance of each feature's text\ndescription to the prediction problem. We use these relevance estimates to\nrescale the features, causing more important features to experience weaker\nregularization.\n  We apply our method to predict the onset of five chronic diseases in the next\nfive years in two genders and two age groups. Our rescaling approach improves\nthe accuracy of the model, particularly when there are few positive examples.\nFurthermore, our method selects 60% fewer features, easing interpretation by\nphysicians. Our method is applicable to other domains where feature and outcome\ndescriptions are available.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 06:24:59 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Liu", "Yun", ""], ["Chuang", "Kun-Ta", ""], ["Liang", "Fu-Wen", ""], ["Su", "Huey-Jen", ""], ["Stultz", "Collin M.", ""], ["Guttag", "John V.", ""]]}, {"id": "1608.02076", "submitter": "Hao Cheng", "authors": "Hao Cheng and Hao Fang and Xiaodong He and Jianfeng Gao and Li Deng", "title": "Bi-directional Attention with Agreement for Dependency Parsing", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel bi-directional attention model for dependency parsing,\nwhich learns to agree on headword predictions from the forward and backward\nparsing directions. The parsing procedure for each direction is formulated as\nsequentially querying the memory component that stores continuous headword\nembeddings. The proposed parser makes use of {\\it soft} headword embeddings,\nallowing the model to implicitly capture high-order parsing history without\ndramatically increasing the computational complexity. We conduct experiments on\nEnglish, Chinese, and 12 other languages from the CoNLL 2006 shared task,\nshowing that the proposed model achieves state-of-the-art unlabeled attachment\nscores on 6 languages.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 07:16:31 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 08:52:31 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Cheng", "Hao", ""], ["Fang", "Hao", ""], ["He", "Xiaodong", ""], ["Gao", "Jianfeng", ""], ["Deng", "Li", ""]]}, {"id": "1608.02126", "submitter": "Adam Lesnikowski", "authors": "Adam Lesnikowski", "title": "How Much Did it Rain? Predicting Real Rainfall Totals Based on Radar\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We applied a variety of parametric and non-parametric machine learning models\nto predict the probability distribution of rainfall based on 1M training\nexamples over a single year across several U.S. states. Our top performing\nmodel based on a squared loss objective was a cross-validated parametric\nk-nearest-neighbor predictor that took about six days to compute, and was\ncompetitive in a world-wide competition.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 16:30:47 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Lesnikowski", "Adam", ""]]}, {"id": "1608.02128", "submitter": "Michael McCoyd", "authors": "Michael McCoyd and David Wagner", "title": "Spoofing 2D Face Detection: Machines See People Who Aren't There", "comments": "9 pages, 19 figures, submitted to AISec", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is increasingly used to make sense of the physical world yet\nmay suffer from adversarial manipulation. We examine the Viola-Jones 2D face\ndetection algorithm to study whether images can be created that humans do not\nnotice as faces yet the algorithm detects as faces. We show that it is possible\nto construct images that Viola-Jones recognizes as containing faces yet no\nhuman would consider a face. Moreover, we show that it is possible to construct\nimages that fool facial detection even when they are printed and then\nphotographed.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 16:50:26 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["McCoyd", "Michael", ""], ["Wagner", "David", ""]]}, {"id": "1608.02146", "submitter": "John Lipor", "authors": "John Lipor and Laura Balzano", "title": "Leveraging Union of Subspace Structure to Improve Constrained Clustering", "comments": "11 pages, 8 figures", "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning, in PMLR 70:2130-2139 (2017)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many clustering problems in computer vision and other contexts are also\nclassification problems, where each cluster shares a meaningful label. Subspace\nclustering algorithms in particular are often applied to problems that fit this\ndescription, for example with face images or handwritten digits. While it is\nstraightforward to request human input on these datasets, our goal is to reduce\nthis input as much as possible. We present a pairwise-constrained clustering\nalgorithm that actively selects queries based on the union-of-subspaces model.\nThe central step of the algorithm is in querying points of minimum margin\nbetween estimated subspaces; analogous to classifier margin, these lie near the\ndecision boundary. We prove that points lying near the intersection of\nsubspaces are points with low margin. Our procedure can be used after any\nsubspace clustering algorithm that outputs an affinity matrix. We demonstrate\non several datasets that our algorithm drives the clustering error down\nconsiderably faster than the state-of-the-art active query algorithms on\ndatasets with subspace structure and is competitive on other datasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 19:29:58 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 21:17:33 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Lipor", "John", ""], ["Balzano", "Laura", ""]]}, {"id": "1608.02198", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman", "title": "A General Characterization of the Statistical Query Complexity", "comments": "Minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical query (SQ) algorithms are algorithms that have access to an {\\em\nSQ oracle} for the input distribution $D$ instead of i.i.d.~ samples from $D$.\nGiven a query function $\\phi:X \\rightarrow [-1,1]$, the oracle returns an\nestimate of ${\\bf E}_{ x\\sim D}[\\phi(x)]$ within some tolerance $\\tau_\\phi$\nthat roughly corresponds to the number of samples.\n  In this work we demonstrate that the complexity of solving general problems\nover distributions using SQ algorithms can be captured by a relatively simple\nnotion of statistical dimension that we introduce. SQ algorithms capture a\nbroad spectrum of algorithmic approaches used in theory and practice, most\nnotably, convex optimization techniques. Hence our statistical dimension allows\nto investigate the power of a variety of algorithmic approaches by analyzing a\nsingle linear-algebraic parameter. Such characterizations were investigated\nover the past 20 years in learning theory but prior characterizations are\nrestricted to the much simpler setting of classification problems relative to a\nfixed distribution on the domain (Blum et al., 1994; Bshouty and Feldman, 2002;\nYang, 2001; Simon, 2007; Feldman, 2012; Szorenyi, 2009). Our characterization\nis also the first to precisely characterize the necessary tolerance of queries.\nWe give applications of our techniques to two open problems in learning theory\nand to algorithms that are subject to memory and communication constraints.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 09:35:44 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 19:55:31 GMT"}, {"version": "v3", "created": "Mon, 17 Apr 2017 06:12:23 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Feldman", "Vitaly", ""]]}, {"id": "1608.02239", "submitter": "Edward Johns", "authors": "Edward Johns, Stefan Leutenegger and Andrew J. Davison", "title": "Deep Learning a Grasp Function for Grasping under Gripper Pose\n  Uncertainty", "comments": "IROS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method for parallel-jaw grasping of isolated\nobjects from depth images, under large gripper pose uncertainty. Whilst most\napproaches aim to predict the single best grasp pose from an image, our method\nfirst predicts a score for every possible grasp pose, which we denote the grasp\nfunction. With this, it is possible to achieve grasping robust to the gripper's\npose uncertainty, by smoothing the grasp function with the pose uncertainty\nfunction. Therefore, if the single best pose is adjacent to a region of poor\ngrasp quality, that pose will no longer be chosen, and instead a pose will be\nchosen which is surrounded by a region of high grasp quality. To learn this\nfunction, we train a Convolutional Neural Network which takes as input a single\ndepth image of an object, and outputs a score for each grasp pose across the\nimage. Training data for this is generated by use of physics simulation and\ndepth image simulation with 3D object meshes, to enable acquisition of\nsufficient data without requiring exhaustive real-world experiments. We\nevaluate with both synthetic and real experiments, and show that the learned\ngrasp score is more robust to gripper pose uncertainty than when this\nuncertainty is not accounted for.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 16:30:42 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Johns", "Edward", ""], ["Leutenegger", "Stefan", ""], ["Davison", "Andrew J.", ""]]}, {"id": "1608.02257", "submitter": "Chang Liu", "authors": "Chang Liu, Bo Li, Yevgeniy Vorobeychik, Alina Oprea", "title": "Robust High-Dimensional Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of supervised learning techniques has made them ubiquitous\nin research and practice. In high-dimensional settings, supervised learning\ncommonly relies on dimensionality reduction to improve performance and identify\nthe most important factors in predicting outcomes. However, the economic\nimportance of learning has made it a natural target for adversarial\nmanipulation of training data, which we term poisoning attacks. Prior\napproaches to dealing with robust supervised learning rely on strong\nassumptions about the nature of the feature matrix, such as feature\nindependence and sub-Gaussian noise with low variance. We propose an integrated\nmethod for robust regression that relaxes these assumptions, assuming only that\nthe feature matrix can be well approximated by a low-rank matrix. Our\ntechniques integrate improved robust low-rank matrix approximation and robust\nprinciple component regression, and yield strong performance guarantees.\nMoreover, we experimentally show that our methods significantly outperform\nstate of the art both in running time and prediction error.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 19:03:52 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 20:20:17 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Liu", "Chang", ""], ["Li", "Bo", ""], ["Vorobeychik", "Yevgeniy", ""], ["Oprea", "Alina", ""]]}, {"id": "1608.02292", "submitter": "Thushan Ganegedara", "authors": "Thushan Ganegedara, Lionel Ott and Fabio Ramos", "title": "Online Adaptation of Deep Architectures with Reinforcement Learning", "comments": "ECAI 2016, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning has become crucial to many problems in machine learning. As\nmore data is collected sequentially, quickly adapting to changes in the data\ndistribution can offer several competitive advantages such as avoiding loss of\nprior knowledge and more efficient learning. However, adaptation to changes in\nthe data distribution (also known as covariate shift) needs to be performed\nwithout compromising past knowledge already built in into the model to cope\nwith voluminous and dynamic data. In this paper, we propose an online stacked\nDenoising Autoencoder whose structure is adapted through reinforcement\nlearning. Our algorithm forces the network to exploit and explore favourable\narchitectures employing an estimated utility function that maximises the\naccuracy of an unseen validation sequence. Different actions, such as Pool,\nIncrement and Merge are available to modify the structure of the network. As we\nobserve through a series of experiments, our approach is more responsive,\nrobust, and principled than its counterparts for non-stationary as well as\nstationary data distributions. Experimental results indicate that our algorithm\nperforms better at preserving gained prior knowledge and responding to changes\nin the data distribution.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 01:10:51 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Ganegedara", "Thushan", ""], ["Ott", "Lionel", ""], ["Ramos", "Fabio", ""]]}, {"id": "1608.02301", "submitter": "Marzyeh Ghassemi", "authors": "Marzyeh Ghassemi, Zeeshan Syed, Daryush D. Mehta, Jarrad H. Van Stan,\n  Robert E. Hillman, and John V. Guttag", "title": "Uncovering Voice Misuse Using Symbolic Mismatch", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice disorders affect an estimated 14 million working-aged Americans, and\nmany more worldwide. We present the first large scale study of vocal misuse\nbased on long-term ambulatory data collected by an accelerometer placed on the\nneck. We investigate an unsupervised data mining approach to uncovering latent\ninformation about voice misuse.\n  We segment signals from over 253 days of data from 22 subjects into over a\nhundred million single glottal pulses (closures of the vocal folds), cluster\nsegments into symbols, and use symbolic mismatch to uncover differences between\npatients and matched controls, and between patients pre- and post-treatment.\nOur results show significant behavioral differences between patients and\ncontrols, as well as between some pre- and post-treatment patients. Our\nproposed approach provides an objective basis for helping diagnose behavioral\nvoice disorders, and is a first step towards a more data-driven understanding\nof the impact of voice therapy.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 02:39:42 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Ghassemi", "Marzyeh", ""], ["Syed", "Zeeshan", ""], ["Mehta", "Daryush D.", ""], ["Van Stan", "Jarrad H.", ""], ["Hillman", "Robert E.", ""], ["Guttag", "John V.", ""]]}, {"id": "1608.02341", "submitter": "Nicola Di Mauro", "authors": "Antonio Vergari and Nicola Di Mauro and Floriana Esposito", "title": "Towards Representation Learning with Tractable Probabilistic Models", "comments": "10 pages, submitted to ECML-PKDD 2016 Doctoral Consortium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic models learned as density estimators can be exploited in\nrepresentation learning beside being toolboxes used to answer inference queries\nonly. However, how to extract useful representations highly depends on the\nparticular model involved. We argue that tractable inference, i.e. inference\nthat can be computed in polynomial time, can enable general schemes to extract\nfeatures from black box models. We plan to investigate how Tractable\nProbabilistic Models (TPMs) can be exploited to generate embeddings by random\nquery evaluations. We devise two experimental designs to assess and compare\ndifferent TPMs as feature extractors in an unsupervised representation learning\nframework. We show some experimental results on standard image datasets by\napplying such a method to Sum-Product Networks and Mixture of Trees as\ntractable models generating embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 07:44:24 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Vergari", "Antonio", ""], ["Di Mauro", "Nicola", ""], ["Esposito", "Floriana", ""]]}, {"id": "1608.02484", "submitter": "Ofir Pele", "authors": "Ofir Pele and Yakir Ben-Aliz", "title": "Interpolated Discretized Embedding of Single Vectors and Vector Pairs\n  for Classification, Metric Learning and Distance Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new embedding method for a single vector and for a pair of\nvectors. This embedding method enables: a) efficient classification and\nregression of functions of single vectors; b) efficient approximation of\ndistance functions; and c) non-Euclidean, semimetric learning. To the best of\nour knowledge, this is the first work that enables learning any general,\nnon-Euclidean, semimetrics. That is, our method is a universal semimetric\nlearning and approximation method that can approximate any distance function\nwith as high accuracy as needed with or without semimetric constraints. The\nproject homepage including code is at: http://www.ariel.ac.il/sites/ofirpele/ID\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 15:23:26 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Pele", "Ofir", ""], ["Ben-Aliz", "Yakir", ""]]}, {"id": "1608.02546", "submitter": "Jeffrey Pawlick", "authors": "Jeffrey Pawlick and Quanyan Zhu", "title": "A Stackelberg Game Perspective on the Conflict Between Machine Learning\n  and Data Obfuscation", "comments": "6 pages, Presented at the 2016 IEEE International Workshop on\n  Information Forensics and Security (IEEE WIFS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data is the new oil; this refrain is repeated extensively in the age of\ninternet tracking, machine learning, and data analytics. As data collection\nbecomes more personal and pervasive, however, public pressure is mounting for\nprivacy protection. In this atmosphere, developers have created applications to\nadd noise to user attributes visible to tracking algorithms. This creates a\nstrategic interaction between trackers and users when incentives to maintain\nprivacy and improve accuracy are misaligned. In this paper, we conceptualize\nthis conflict through an N+1-player, augmented Stackelberg game. First a\nmachine learner declares a privacy protection level, and then users respond by\nchoosing their own perturbation amounts. We use the general frameworks of\ndifferential privacy and empirical risk minimization to quantify the utility\ncomponents due to privacy and accuracy, respectively. In equilibrium, each user\nperturbs her data independently, which leads to a high net loss in accuracy. To\nremedy this scenario, we show that the learner improves his utility by\nproactively perturbing the data himself. While other work in this area has\nstudied privacy markets and mechanism design for truthful reporting of user\ninformation, we take a different viewpoint by considering both user and learner\nperturbation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 18:30:22 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 17:41:49 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Pawlick", "Jeffrey", ""], ["Zhu", "Quanyan", ""]]}, {"id": "1608.02689", "submitter": "Nanyun Peng", "authors": "Nanyun Peng and Mark Dredze", "title": "Multi-task Domain Adaptation for Sequence Tagging", "comments": "The version for ACL 2017 Repl4NLP workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many domain adaptation approaches rely on learning cross domain shared\nrepresentations to transfer the knowledge learned in one domain to other\ndomains. Traditional domain adaptation only considers adapting for one task. In\nthis paper, we explore multi-task representation learning under the domain\nadaptation scenario. We propose a neural network framework that supports domain\nadaptation for multiple tasks simultaneously, and learns shared representations\nthat better generalize for domain adaptation. We apply the proposed framework\nto domain adaptation for sequence tagging problems considering two tasks:\nChinese word segmentation and named entity recognition. Experiments show that\nmulti-task domain adaptation works better than disjoint domain adaptation for\neach task, and achieves the state-of-the-art results for both tasks in the\nsocial media domain.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 04:38:38 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 14:31:13 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Peng", "Nanyun", ""], ["Dredze", "Mark", ""]]}, {"id": "1608.02693", "submitter": "Mehul Bhatt", "authors": "Jakob Suchan and Mehul Bhatt and Carl Schultz", "title": "Deeply Semantic Inductive Spatio-Temporal Learning", "comments": "Accepted for publication at ILP 2016: 26th International Conference\n  on Inductive Logic Programming 4th - 6th September 2016, London. Keywords:\n  Spatio-Temporal Learning; Dynamic Visuo-Spatial Imagery; Declarative Spatial\n  Reasoning; Inductive Logic Programming; AI and Art", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an inductive spatio-temporal learning framework rooted in\ninductive logic programming. With an emphasis on visuo-spatial language, logic,\nand cognition, the framework supports learning with relational spatio-temporal\nfeatures identifiable in a range of domains involving the processing and\ninterpretation of dynamic visuo-spatial imagery. We present a prototypical\nsystem, and an example application in the domain of computing for visual arts\nand computational cognitive science.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 05:48:51 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""], ["Schultz", "Carl", ""]]}, {"id": "1608.02717", "submitter": "Mateusz Malinowski", "authors": "Ashkan Mokarian and Mateusz Malinowski and Mario Fritz", "title": "Mean Box Pooling: A Rich Image Representation and Output Embedding for\n  the Visual Madlibs Task", "comments": "Accepted to BMVC'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Mean Box Pooling, a novel visual representation that pools over\nCNN representations of a large number, highly overlapping object proposals. We\nshow that such representation together with nCCA, a successful multimodal\nembedding technique, achieves state-of-the-art performance on the Visual\nMadlibs task. Moreover, inspired by the nCCA's objective function, we extend\nclassical CNN+LSTM approach to train the network by directly maximizing the\nsimilarity between the internal representation of the deep learning\narchitecture and candidate answers. Again, such approach achieves a significant\nimprovement over the prior work that also uses CNN+LSTM approach on Visual\nMadlibs.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 08:24:02 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Mokarian", "Ashkan", ""], ["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1608.02728", "submitter": "Martin Simonovsky", "authors": "Martin Simonovsky and Nikos Komodakis", "title": "OnionNet: Sharing Features in Cascaded Deep Classifiers", "comments": "Accepted to BMVC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of our work is speeding up evaluation of deep neural networks in\nretrieval scenarios, where conventional architectures may spend too much time\non negative examples. We propose to replace a monolithic network with our novel\ncascade of feature-sharing deep classifiers, called OnionNet, where subsequent\nstages may add both new layers as well as new feature channels to the previous\nones. Importantly, intermediate feature maps are shared among classifiers,\npreventing them from the necessity of being recomputed. To accomplish this, the\nmodel is trained end-to-end in a principled way under a joint loss. We validate\nour approach in theory and on a synthetic benchmark. As a result demonstrated\nin three applications (patch matching, object detection, and image retrieval),\nour cascade can operate significantly faster than both monolithic networks and\ntraditional cascades without sharing at the cost of marginal decrease in\nprecision.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 08:59:47 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Simonovsky", "Martin", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1608.02731", "submitter": "Ian Osband", "authors": "Ian Osband, Benjamin Van Roy", "title": "Posterior Sampling for Reinforcement Learning Without Episodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a brief technical note to clarify some of the issues with applying\nthe application of the algorithm posterior sampling for reinforcement learning\n(PSRL) in environments without fixed episodes. In particular, this paper aims\nto:\n  - Review some of results which have been proven for finite horizon MDPs\n(Osband et al 2013, 2014a, 2014b, 2016) and also for MDPs with finite ergodic\nstructure (Gopalan et al 2014).\n  - Review similar results for optimistic algorithms in infinite horizon\nproblems (Jaksch et al 2010, Bartlett and Tewari 2009, Abbasi-Yadkori and\nSzepesvari 2011), with particular attention to the dynamic episode growth.\n  - Highlight the delicate technical issue which has led to a fault in the\nproof of the lazy-PSRL algorithm (Abbasi-Yadkori and Szepesvari 2015). We\npresent an explicit counterexample to this style of argument. Therefore, we\nsuggest that the Theorem 2 in (Abbasi-Yadkori and Szepesvari 2015) be instead\nconsidered a conjecture, as it has no rigorous proof.\n  - Present pragmatic approaches to apply PSRL in infinite horizon problems. We\nconjecture that, under some additional assumptions, it will be possible to\nobtain bounds $O( \\sqrt{T} )$ even without episodic reset.\n  We hope that this note serves to clarify existing results in the field of\nreinforcement learning and provides interesting motivation for future work.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 09:01:13 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1608.02732", "submitter": "Ian Osband", "authors": "Ian Osband, Benjamin Van Roy", "title": "On Lower Bounds for Regret in Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a brief technical note to clarify the state of lower bounds on regret\nfor reinforcement learning. In particular, this paper:\n  - Reproduces a lower bound on regret for reinforcement learning, similar to\nthe result of Theorem 5 in the journal UCRL2 paper (Jaksch et al 2010).\n  - Clarifies that the proposed proof of Theorem 6 in the REGAL paper (Bartlett\nand Tewari 2009) does not hold using the standard techniques without further\nwork. We suggest that this result should instead be considered a conjecture as\nit has no rigorous proof.\n  - Suggests that the conjectured lower bound given by (Bartlett and Tewari\n2009) is incorrect and, in fact, it is possible to improve the scaling of the\nupper bound to match the weaker lower bounds presented in this paper.\n  We hope that this note serves to clarify existing results in the field of\nreinforcement learning and provides interesting motivation for future work.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 09:02:01 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1608.02861", "submitter": "Oleksii Pokotylo", "authors": "Oleksii Pokotylo and Karl Mosler", "title": "Classification with the pot-pot plot", "comments": "The Online Appendix is available at\n  https://dl.dropboxusercontent.com/u/61551177/Classification_with_the_pot-pot_plot_Online_Appendix.pdf", "journal-ref": "Statistical Papers (2016)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a procedure for supervised classification that is based on\npotential functions. The potential of a class is defined as a kernel density\nestimate multiplied by the class's prior probability. The method transforms the\ndata to a potential-potential (pot-pot) plot, where each data point is mapped\nto a vector of potentials. Separation of the classes, as well as classification\nof new data points, is performed on this plot. For this, either the\n$\\alpha$-procedure ($\\alpha$-P) or $k$-nearest neighbors ($k$-NN) are employed.\nFor data that are generated from continuous distributions, these classifiers\nprove to be strongly Bayes-consistent. The potentials depend on the kernel and\nits bandwidth used in the density estimate. We investigate several variants of\nbandwidth selection, including joint and separate pre-scaling and a bandwidth\nregression approach. The new method is applied to benchmark data from the\nliterature, including simulated data sets as well as 50 sets of real data. It\ncompares favorably to known classification methods such as LDA, QDA, max kernel\ndensity estimates, $k$-NN, and $DD$-plot classification using depth functions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 16:46:53 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Pokotylo", "Oleksii", ""], ["Mosler", "Karl", ""]]}, {"id": "1608.02888", "submitter": "Ayad Ghany Ismaeel", "authors": "Ayad Ghany Ismaeel, Dina Yousif Mikhail", "title": "Effective Data Mining Technique for Classification Cancers via Mutations\n  in Gene using Neural Network", "comments": "8 pages, 8 figures, 1 Table", "journal-ref": "(IJACSA) International Journal of Advanced Computer Science and\n  Applications, Vol. 7, No. 7, 2016. Pages 69-76", "doi": "10.14569/IJACSA.2016.070710", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The prediction plays the important role in detecting efficient protection and\ntherapy of cancer. The prediction of mutations in gene needs a diagnostic and\nclassification, which is based on the whole database (big dataset), to reach\nsufficient accuracy results. Since the tumor suppressor P53 is approximately\nabout fifty percentage of all human tumors because mutations that occur in the\nTP53 gene into the cells. So, this paper is applied on tumor p53, where the\nproblem is there are several primitive databases (excel database) contain\ndatasets of TP53 gene with its tumor protein p53, these databases are rich\ndatasets that cover all mutations and cause diseases (cancers). But these Data\nBases cannot reach to predict and diagnosis cancers, i.e. the big datasets have\nnot efficient Data Mining method, which can predict, diagnosis the mutation,\nand classify the cancer of patient. The goal of this paper to reach a Data\nMining technique, that employs neural network, which bases on the big datasets.\nAlso, offers friendly predictions, flexible, and effective classified cancers,\nin order to overcome the previous techniques drawbacks. This proposed technique\nis done by using two approaches, first, bioinformatics techniques by using\nBLAST, CLUSTALW, etc, in order to know if there are malignant mutations or not.\nThe second, data mining by using neural network; it is selected (12) out of\n(53) TP53 gene database fields. To clarify, one of these 12 fields (gene\nlocation field) did not exists in TP53 gene database; therefore, it is added to\nthe database of TP53 gene in training and testing back propagation algorithm,\nin order to classify specifically the types of cancers. Feed Forward Back\nPropagation supports this Data Mining method with data training rate (1) and\nMean Square Error (MSE) (0.00000000000001). This effective technique allows in\na quick, accurate and easy way to classify the type of cancer.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 12:48:40 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Ismaeel", "Ayad Ghany", ""], ["Mikhail", "Dina Yousif", ""]]}, {"id": "1608.02893", "submitter": "David Cox", "authors": "David Cox", "title": "Syntactically Informed Text Compression with Recurrent Neural Networks", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-contained system for constructing natural language models\nfor use in text compression. Our system improves upon previous neural network\nbased models by utilizing recent advances in syntactic parsing -- Google's\nSyntaxNet -- to augment character-level recurrent neural networks. RNNs have\nproven exceptional in modeling sequence data such as text, as their\narchitecture allows for modeling of long-term contextual information.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 01:30:45 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 20:55:41 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Cox", "David", ""]]}, {"id": "1608.02971", "submitter": "Karan Budhraja", "authors": "Karan K. Budhraja and Tim Oates", "title": "Neuroevolution-Based Inverse Reinforcement Learning", "comments": "12 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Learning from Demonstration is targeted at learning to perform\ntasks based on observed examples. One approach to Learning from Demonstration\nis Inverse Reinforcement Learning, in which actions are observed to infer\nrewards. This work combines a feature based state evaluation approach to\nInverse Reinforcement Learning with neuroevolution, a paradigm for modifying\nneural networks based on their performance on a given task. Neural networks are\nused to learn from a demonstrated expert policy and are evolved to generate a\npolicy similar to the demonstration. The algorithm is discussed and evaluated\nagainst competitive feature-based Inverse Reinforcement Learning approaches. At\nthe cost of execution time, neural networks allow for non-linear combinations\nof features in state evaluations. These valuations may correspond to state\nvalue or state reward. This results in better correspondence to observed\nexamples as opposed to using linear combinations. This work also extends\nexisting work on Bayesian Non-Parametric Feature Construction for Inverse\nReinforcement Learning by using non-linear combinations of intermediate data to\nimprove performance. The algorithm is observed to be specifically suitable for\na linearly solvable non-deterministic Markov Decision Processes in which\nmultiple rewards are sparsely scattered in state space. A conclusive\nperformance hierarchy between evaluated algorithms is presented.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 20:04:40 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Budhraja", "Karan K.", ""], ["Oates", "Tim", ""]]}, {"id": "1608.02996", "submitter": "Antonio Valerio Miceli Barone", "authors": "Antonio Valerio Miceli Barone", "title": "Towards cross-lingual distributed representations without parallel text\n  trained with adversarial autoencoders", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current approaches to learning vector representations of text that are\ncompatible between different languages usually require some amount of parallel\ntext, aligned at word, sentence or at least document level. We hypothesize\nhowever, that different natural languages share enough semantic structure that\nit should be possible, in principle, to learn compatible vector representations\njust by analyzing the monolingual distribution of words.\n  In order to evaluate this hypothesis, we propose a scheme to map word vectors\ntrained on a source language to vectors semantically compatible with word\nvectors trained on a target language using an adversarial autoencoder.\n  We present preliminary qualitative results and discuss possible future\ndevelopments of this technique, such as applications to cross-lingual sentence\nrepresentations.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 22:24:16 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Barone", "Antonio Valerio Miceli", ""]]}, {"id": "1608.03016", "submitter": "Yuncheng Li", "authors": "Yuncheng Li, LiangLiang Cao, Jiang Zhu, Jiebo Luo", "title": "Mining Fashion Outfit Composition Using An End-to-End Deep Learning\n  Approach on Set Data", "comments": "IEEE TMM", "journal-ref": null, "doi": "10.1109/TMM.2017.2690144", "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composing fashion outfits involves deep understanding of fashion standards\nwhile incorporating creativity for choosing multiple fashion items (e.g.,\nJewelry, Bag, Pants, Dress). In fashion websites, popular or high-quality\nfashion outfits are usually designed by fashion experts and followed by large\naudiences. In this paper, we propose a machine learning system to compose\nfashion outfits automatically. The core of the proposed automatic composition\nsystem is to score fashion outfit candidates based on the appearances and\nmeta-data. We propose to leverage outfit popularity on fashion oriented\nwebsites to supervise the scoring component. The scoring component is a\nmulti-modal multi-instance deep learning system that evaluates instance\naesthetics and set compatibility simultaneously. In order to train and evaluate\nthe proposed composition system, we have collected a large scale fashion outfit\ndataset with 195K outfits and 368K fashion items from Polyvore. Although the\nfashion outfit scoring and composition is rather challenging, we have achieved\nan AUC of 85% for the scoring component, and an accuracy of 77% for a\nconstrained composition task.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 01:11:32 GMT"}, {"version": "v2", "created": "Sat, 15 Apr 2017 05:26:23 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Li", "Yuncheng", ""], ["Cao", "LiangLiang", ""], ["Zhu", "Jiang", ""], ["Luo", "Jiebo", ""]]}, {"id": "1608.03023", "submitter": "Branislav Kveton", "authors": "Sumeet Katariya, Branislav Kveton, Csaba Szepesvari, Claire Vernade,\n  and Zheng Wen", "title": "Stochastic Rank-1 Bandits", "comments": "Proceedings of the 20th International Conference on Artificial\n  Intelligence and Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose stochastic rank-$1$ bandits, a class of online learning problems\nwhere at each step a learning agent chooses a pair of row and column arms, and\nreceives the product of their values as a reward. The main challenge of the\nproblem is that the individual values of the row and column are unobserved. We\nassume that these values are stochastic and drawn independently. We propose a\ncomputationally-efficient algorithm for solving our problem, which we call\nRank1Elim. We derive a $O((K + L) (1 / \\Delta) \\log n)$ upper bound on its\n$n$-step regret, where $K$ is the number of rows, $L$ is the number of columns,\nand $\\Delta$ is the minimum of the row and column gaps; under the assumption\nthat the mean row and column rewards are bounded away from zero. To the best of\nour knowledge, we present the first bandit algorithm that finds the maximum\nentry of a rank-$1$ matrix whose regret is linear in $K + L$, $1 / \\Delta$, and\n$\\log n$. We also derive a nearly matching lower bound. Finally, we evaluate\nRank1Elim empirically on multiple problems. We observe that it leverages the\nstructure of our problems and can learn near-optimal solutions even if our\nmodeling assumptions are mildly violated.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 01:51:36 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 06:56:24 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 07:58:32 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Katariya", "Sumeet", ""], ["Kveton", "Branislav", ""], ["Szepesvari", "Csaba", ""], ["Vernade", "Claire", ""], ["Wen", "Zheng", ""]]}, {"id": "1608.03100", "submitter": "Aditi Raghunathan", "authors": "Aditi Raghunathan, Roy Frostig, John Duchi, Percy Liang", "title": "Estimation from Indirect Supervision with Linear Moments", "comments": "12 pages, 7 figures, extended and updated version of our paper\n  appearing in ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In structured prediction problems where we have indirect supervision of the\noutput, maximum marginal likelihood faces two computational obstacles:\nnon-convexity of the objective and intractability of even a single gradient\ncomputation. In this paper, we bypass both obstacles for a class of what we\ncall linear indirectly-supervised problems. Our approach is simple: we solve a\nlinear system to estimate sufficient statistics of the model, which we then use\nto estimate parameters via convex optimization. We analyze the statistical\nproperties of our approach and show empirically that it is effective in two\nsettings: learning with local privacy constraints and learning from low-cost\ncount-based annotations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 09:19:07 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Raghunathan", "Aditi", ""], ["Frostig", "Roy", ""], ["Duchi", "John", ""], ["Liang", "Percy", ""]]}, {"id": "1608.03248", "submitter": "Luiz F. O. Chamon", "authors": "Luiz F. O. Chamon and Cassio G. Lopes", "title": "Combination of LMS Adaptive Filters with Coefficients Feedback", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel combinations of adaptive filters have been effectively used to\nimprove the performance of adaptive algorithms and address well-known\ntrade-offs, such as convergence rate vs. steady-state error. Nevertheless,\ntypical combinations suffer from a convergence stagnation issue due to the fact\nthat the component filters run independently. Solutions to this issue usually\ninvolve conditional transfers of coefficients between filters, which although\neffective, are hard to generalize to combinations with more filters or when\nthere is no clearly faster adaptive filter. In this work, a more natural\nsolution is proposed by cyclically feeding back the combined coefficient vector\nto all component filters. Besides coping with convergence stagnation, this new\ntopology improves tracking and supervisor stability, and bridges an important\nconceptual gap between combinations of adaptive filters and variable step size\nschemes. We analyze the steady-state, tracking, and transient performance of\nthis topology for LMS component filters and supervisors with generic activation\nfunctions. Numerical examples are used to illustrate how coefficients feedback\ncan improve the performance of parallel combinations at a small computational\noverhead.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 18:15:58 GMT"}, {"version": "v2", "created": "Sun, 19 Nov 2017 22:48:13 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Chamon", "Luiz F. O.", ""], ["Lopes", "Cassio G.", ""]]}, {"id": "1608.03287", "submitter": "Tomaso Poggio", "authors": "Hrushikesh Mhaskar and Tomaso Poggio", "title": "Deep vs. shallow networks : An approximation theory perspective", "comments": "14 pages, 4 figures, to be published in a Journal", "journal-ref": null, "doi": null, "report-no": "CBMM Memo 54", "categories": "cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper briefy reviews several recent results on hierarchical architectures\nfor learning from examples, that may formally explain the conditions under\nwhich Deep Convolutional Neural Networks perform much better in function\napproximation problems than shallow, one-hidden layer architectures. The paper\nannounces new results for a non-smooth activation function - the ReLU function\n- used in present-day neural networks, as well as for the Gaussian networks. We\npropose a new definition of relative dimension to encapsulate different notions\nof sparsity of a function class that can possibly be exploited by deep networks\nbut not by shallow ones to drastically reduce the complexity required for\napproximation and learning.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 20:02:40 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Mhaskar", "Hrushikesh", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1608.03333", "submitter": "Kuan Liu", "authors": "Kuan Liu, Xing Shi, Anoop Kumar, Linhong Zhu, Prem Natarajan", "title": "Temporal Learning and Sequence Modeling for a Job Recommender System", "comments": "a shorter version in proceedings of RecSys Challenge 2016", "journal-ref": null, "doi": "10.1145/2987538.2987540", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our solution to the job recommendation task for RecSys Challenge\n2016. The main contribution of our work is to combine temporal learning with\nsequence modeling to capture complex user-item activity patterns to improve job\nrecommendations. First, we propose a time-based ranking model applied to\nhistorical observations and a hybrid matrix factorization over time re-weighted\ninteractions. Second, we exploit sequence properties in user-items activities\nand develop a RNN-based recommendation model. Our solution achieved 5$^{th}$\nplace in the challenge among more than 100 participants. Notably, the strong\nperformance of our RNN approach shows a promising new direction in employing\nsequence modeling for recommendation systems.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 00:48:00 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Liu", "Kuan", ""], ["Shi", "Xing", ""], ["Kumar", "Anoop", ""], ["Zhu", "Linhong", ""], ["Natarajan", "Prem", ""]]}, {"id": "1608.03339", "submitter": "Shaobo Lin", "authors": "Shao-Bo Lin, Xin Guo, Ding-Xuan Zhou", "title": "Distributed learning with regularized least squares", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed learning with the least squares regularization scheme in\na reproducing kernel Hilbert space (RKHS). By a divide-and-conquer approach,\nthe algorithm partitions a data set into disjoint data subsets, applies the\nleast squares regularization scheme to each data subset to produce an output\nfunction, and then takes an average of the individual output functions as a\nfinal global estimator or predictor. We show with error bounds in expectation\nin both the $L^2$-metric and RKHS-metric that the global output function of\nthis distributed learning is a good approximation to the algorithm processing\nthe whole data in one single machine. Our error bounds are sharp and stated in\na general setting without any eigenfunction assumption. The analysis is\nachieved by a novel second order decomposition of operator differences in our\nintegral operator approach. Even for the classical least squares regularization\nscheme in the RKHS associated with a general kernel, we give the best learning\nrate in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 01:20:23 GMT"}, {"version": "v2", "created": "Sat, 11 Mar 2017 07:45:26 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Lin", "Shao-Bo", ""], ["Guo", "Xin", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1608.03344", "submitter": "Chenwei Zhang", "authors": "Chenwei Zhang, Sihong Xie, Yaliang Li, Jing Gao, Wei Fan, Philip S. Yu", "title": "Multi-source Hierarchical Prediction Consolidation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In big data applications such as healthcare data mining, due to privacy\nconcerns, it is necessary to collect predictions from multiple information\nsources for the same instance, with raw features being discarded or withheld\nwhen aggregating multiple predictions. Besides, crowd-sourced labels need to be\naggregated to estimate the ground truth of the data. Because of the imperfect\npredictive models or human crowdsourcing workers, noisy and conflicting\ninformation is ubiquitous and inevitable. Although state-of-the-art aggregation\nmethods have been proposed to handle label spaces with flat structures, as the\nlabel space is becoming more and more complicated, aggregation under a label\nhierarchical structure becomes necessary but has been largely ignored. These\nlabel hierarchies can be quite informative as they are usually created by\ndomain experts to make sense of highly complex label correlations for many\nreal-world cases like protein functionality interactions or disease\nrelationships.\n  We propose a novel multi-source hierarchical prediction consolidation method\nto effectively exploits the complicated hierarchical label structures to\nresolve the noisy and conflicting information that inherently originates from\nmultiple imperfect sources. We formulate the problem as an optimization problem\nwith a closed-form solution. The proposed method captures the smoothness\noverall information sources as well as penalizing any consolidation result that\nviolates the constraints derived from the label hierarchy. The hierarchical\ninstance similarity, as well as the consolidation result, are inferred in a\ntotally unsupervised, iterative fashion. Experimental results on both synthetic\nand real-world datasets show the effectiveness of the proposed method over\nexisting alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 01:55:04 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Zhang", "Chenwei", ""], ["Xie", "Sihong", ""], ["Li", "Yaliang", ""], ["Gao", "Jing", ""], ["Fan", "Wei", ""], ["Yu", "Philip S.", ""]]}, {"id": "1608.03530", "submitter": "Jason T. L. Wang", "authors": "Nihir Patel and Jason T. L. Wang", "title": "Semi-Supervised Prediction of Gene Regulatory Networks Using Machine\n  Learning Algorithms", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Use of computational methods to predict gene regulatory networks (GRNs) from\ngene expression data is a challenging task. Many studies have been conducted\nusing unsupervised methods to fulfill the task; however, such methods usually\nyield low prediction accuracies due to the lack of training data. In this\narticle, we propose semi-supervised methods for GRN prediction by utilizing two\nmachine learning algorithms, namely support vector machines (SVM) and random\nforests (RF). The semi-supervised methods make use of unlabeled data for\ntraining. We investigate inductive and transductive learning approaches, both\nof which adopt an iterative procedure to obtain reliable negative training data\nfrom the unlabeled data. We then apply our semi-supervised methods to gene\nexpression data of Escherichia coli and Saccharomyces cerevisiae, and evaluate\nthe performance of our methods using the expression data. Our analysis\nindicated that the transductive learning approach outperformed the inductive\nlearning approach for both organisms. However, there was no conclusive\ndifference identified in the performance of SVM and RF. Experimental results\nalso showed that the proposed semi-supervised methods performed better than\nexisting supervised methods for both organisms.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 16:52:03 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Patel", "Nihir", ""], ["Wang", "Jason T. L.", ""]]}, {"id": "1608.03533", "submitter": "Chitta Ranjan", "authors": "Chitta Ranjan, Samaneh Ebrahimi and Kamran Paynabar", "title": "Sequence Graph Transform (SGT): A Feature Embedding Function for\n  Sequence Data Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence feature embedding is a challenging task due to un-structuredness of\nsequences -- arbitrary strings of arbitrary length. Existing methods are\nefficient in extracting short-term dependencies but typically suffer from\ncomputation issues for the long-term. Sequence Graph Transform (SGT), a feature\nembedding function, that can extract varying amount of short- to long-term\ndependencies without increasing the computation is proposed. SGT's properties\nare analytically proved for interpretation under normal and uniform\ndistribution assumptions. SGT features yield significantly superior results in\nsequence clustering and classification with higher accuracy and lower\ncomputation as compared to the existing methods, including the state-of-the-art\nsequence/string Kernels and LSTM.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 16:59:19 GMT"}, {"version": "v10", "created": "Thu, 27 Feb 2020 19:47:52 GMT"}, {"version": "v11", "created": "Wed, 4 Mar 2020 14:54:16 GMT"}, {"version": "v12", "created": "Fri, 8 May 2020 20:03:02 GMT"}, {"version": "v13", "created": "Thu, 29 Oct 2020 11:49:41 GMT"}, {"version": "v14", "created": "Tue, 18 May 2021 00:03:21 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2016 14:01:03 GMT"}, {"version": "v3", "created": "Tue, 23 Aug 2016 20:03:41 GMT"}, {"version": "v4", "created": "Wed, 28 Sep 2016 00:20:49 GMT"}, {"version": "v5", "created": "Wed, 19 Oct 2016 05:04:59 GMT"}, {"version": "v6", "created": "Sun, 27 Nov 2016 01:43:12 GMT"}, {"version": "v7", "created": "Wed, 30 Nov 2016 06:35:26 GMT"}, {"version": "v8", "created": "Tue, 31 Jan 2017 03:50:58 GMT"}, {"version": "v9", "created": "Sun, 30 Apr 2017 07:21:43 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ranjan", "Chitta", ""], ["Ebrahimi", "Samaneh", ""], ["Paynabar", "Kamran", ""]]}, {"id": "1608.03544", "submitter": "Claudio Gentile", "authors": "Claudio Gentile, Shuai Li, Purushottam Kar, Alexandros Karatzoglou,\n  Evans Etrue, Giovanni Zappella", "title": "On Context-Dependent Clustering of Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a novel cluster-of-bandit algorithm CAB for collaborative\nrecommendation tasks that implements the underlying feedback sharing mechanism\nby estimating the neighborhood of users in a context-dependent manner. CAB\nmakes sharp departures from the state of the art by incorporating collaborative\neffects into inference as well as learning processes in a manner that\nseamlessly interleaving explore-exploit tradeoffs and collaborative steps. We\nprove regret bounds under various assumptions on the data, which exhibit a\ncrisp dependence on the expected number of clusters over the users, a natural\nmeasure of the statistical difficulty of the learning task. Experiments on\nproduction and real-world datasets show that CAB offers significantly increased\nprediction performance against a representative pool of state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 14:13:28 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 17:16:22 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Gentile", "Claudio", ""], ["Li", "Shuai", ""], ["Kar", "Purushottam", ""], ["Karatzoglou", "Alexandros", ""], ["Etrue", "Evans", ""], ["Zappella", "Giovanni", ""]]}, {"id": "1608.03585", "submitter": "Matthias Poloczek", "authors": "Matthias Poloczek, Jialei Wang, and Peter I. Frazier", "title": "Warm Starting Bayesian Optimization", "comments": "To Appear in the Proc. of the 2016 Winter Simulation Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for warm-starting Bayesian optimization, that reduces\nthe solution time required to solve an optimization problem that is one in a\nsequence of related problems. This is useful when optimizing the output of a\nstochastic simulator that fails to provide derivative information, for which\nBayesian optimization methods are well-suited. Solving sequences of related\noptimization problems arises when making several business decisions using one\noptimization model and input data collected over different time periods or\nmarkets. While many gradient-based methods can be warm started by initiating\noptimization at the solution to the previous problem, this warm start approach\ndoes not apply to Bayesian optimization methods, which carry a full metamodel\nof the objective function from iteration to iteration. Our approach builds a\njoint statistical model of the entire collection of related objective\nfunctions, and uses a value of information calculation to recommend points to\nevaluate.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 19:56:27 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Poloczek", "Matthias", ""], ["Wang", "Jialei", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1608.03639", "submitter": "Truyen Tran", "authors": "Trang Pham, Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Faster Training of Very Deep Networks Via p-Norm Gates", "comments": "To appear in ICPR'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major contributing factor to the recent advances in deep neural networks is\nstructural units that let sensory information and gradients to propagate\neasily. Gating is one such structure that acts as a flow control. Gates are\nemployed in many recent state-of-the-art recurrent models such as LSTM and GRU,\nand feedforward models such as Residual Nets and Highway Networks. This enables\nlearning in very deep networks with hundred layers and helps achieve\nrecord-breaking results in vision (e.g., ImageNet with Residual Nets) and NLP\n(e.g., machine translation with GRU). However, there is limited work in\nanalysing the role of gating in the learning process. In this paper, we propose\na flexible $p$-norm gating scheme, which allows user-controllable flow and as a\nconsequence, improve the learning speed. This scheme subsumes other existing\ngating schemes, including those in GRU, Highway Networks and Residual Nets as\nspecial cases. Experiments on large sequence and vector datasets demonstrate\nthat the proposed gating scheme helps improve the learning speed significantly\nwithout extra overhead.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 23:48:44 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Pham", "Trang", ""], ["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1608.03643", "submitter": "Santosh Vempala", "authors": "Ravi Kannan and Santosh Vempala", "title": "Chi-squared Amplification: Identifying Hidden Hubs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following general hidden hubs model: an $n \\times n$ random\nmatrix $A$ with a subset $S$ of $k$ special rows (hubs): entries in rows\noutside $S$ are generated from the probability distribution $p_0 \\sim\nN(0,\\sigma_0^2)$; for each row in $S$, some $k$ of its entries are generated\nfrom $p_1 \\sim N(0,\\sigma_1^2)$, $\\sigma_1>\\sigma_0$, and the rest of the\nentries from $p_0$. The problem is to identify the high-degree hubs\nefficiently. This model includes and significantly generalizes the planted\nGaussian Submatrix Model, where the special entries are all in a $k \\times k$\nsubmatrix. There are two well-known barriers: if $k\\geq c\\sqrt{n\\ln n}$, just\nthe row sums are sufficient to find $S$ in the general model. For the submatrix\nproblem, this can be improved by a $\\sqrt{\\ln n}$ factor to $k \\ge c\\sqrt{n}$\nby spectral methods or combinatorial methods. In the variant with $p_0=\\pm 1$\n(with probability $1/2$ each) and $p_1\\equiv 1$, neither barrier has been\nbroken.\n  We give a polynomial-time algorithm to identify all the hidden hubs with high\nprobability for $k \\ge n^{0.5-\\delta}$ for some $\\delta >0$, when\n$\\sigma_1^2>2\\sigma_0^2$. The algorithm extends to the setting where planted\nentries might have different variances each at least as large as $\\sigma_1^2$.\nWe also show a nearly matching lower bound: for $\\sigma_1^2 \\le 2\\sigma_0^2$,\nthere is no polynomial-time Statistical Query algorithm for distinguishing\nbetween a matrix whose entries are all from $N(0,\\sigma_0^2)$ and a matrix with\n$k=n^{0.5-\\delta}$ hidden hubs for any $\\delta >0$. The lower bound as well as\nthe algorithm are related to whether the chi-squared distance of the two\ndistributions diverges. At the critical value $\\sigma_1^2=2\\sigma_0^2$, we show\nthat the general hidden hubs problem can be solved for $k\\geq c\\sqrt n(\\ln\nn)^{1/4}$, improving on the naive row sum-based method.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 00:36:42 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 16:31:27 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Kannan", "Ravi", ""], ["Vempala", "Santosh", ""]]}, {"id": "1608.03644", "submitter": "Yanjun  Qi Dr.", "authors": "Jack Lanchantin, Ritambhara Singh, Beilun Wang, and Yanjun Qi", "title": "Deep Motif Dashboard: Visualizing and Understanding Genomic Sequences\n  Using Deep Neural Networks", "comments": "11 pages, 2 figures. Updated for PSB submission, Pacific Symposium on\n  Biocomputing (PSB) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) models have recently obtained state-of-the-art\nprediction accuracy for the transcription factor binding (TFBS) site\nclassification task. However, it remains unclear how these approaches identify\nmeaningful DNA sequence signals and give insights as to why TFs bind to certain\nlocations. In this paper, we propose a toolkit called the Deep Motif Dashboard\n(DeMo Dashboard) which provides a suite of visualization strategies to extract\nmotifs, or sequence patterns from deep neural network models for TFBS\nclassification. We demonstrate how to visualize and understand three important\nDNN models: convolutional, recurrent, and convolutional-recurrent networks. Our\nfirst visualization method is finding a test sequence's saliency map which uses\nfirst-order derivatives to describe the importance of each nucleotide in making\nthe final prediction. Second, considering recurrent models make predictions in\na temporal manner (from one end of a TFBS sequence to the other), we introduce\ntemporal output scores, indicating the prediction score of a model over time\nfor a sequential input. Lastly, a class-specific visualization strategy finds\nthe optimal input sequence for a given TFBS positive class via stochastic\ngradient optimization. Our experimental results indicate that a\nconvolutional-recurrent architecture performs the best among the three\narchitectures. The visualization techniques indicate that CNN-RNN makes\npredictions by modeling both motifs as well as dependencies among them.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 00:43:59 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 14:00:44 GMT"}, {"version": "v3", "created": "Tue, 4 Oct 2016 16:37:45 GMT"}, {"version": "v4", "created": "Tue, 18 Oct 2016 20:20:22 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Lanchantin", "Jack", ""], ["Singh", "Ritambhara", ""], ["Wang", "Beilun", ""], ["Qi", "Yanjun", ""]]}, {"id": "1608.03647", "submitter": "Tom Ameloot", "authors": "Tom J. Ameloot and Jan Van den Bussche", "title": "Learning with Value-Ramp", "comments": "Version 2: fixed notation in definition of transition + clarified a\n  sentence in the Introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a learning principle based on the intuition of forming ramps. The\nagent tries to follow an increasing sequence of values until the agent meets a\npeak of reward. The resulting Value-Ramp algorithm is natural, easy to\nconfigure, and has a robust implementation with natural numbers.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 07:24:57 GMT"}, {"version": "v2", "created": "Sun, 23 Apr 2017 16:46:17 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Ameloot", "Tom J.", ""], ["Bussche", "Jan Van den", ""]]}, {"id": "1608.03665", "submitter": "Wei Wen", "authors": "Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li", "title": "Learning Structured Sparsity in Deep Neural Networks", "comments": "Accepted by NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High demand for computation resources severely hinders deployment of\nlarge-scale Deep Neural Networks (DNN) in resource constrained devices. In this\nwork, we propose a Structured Sparsity Learning (SSL) method to regularize the\nstructures (i.e., filters, channels, filter shapes, and layer depth) of DNNs.\nSSL can: (1) learn a compact structure from a bigger DNN to reduce computation\ncost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently\naccelerate the DNNs evaluation. Experimental results show that SSL achieves on\naverage 5.1x and 3.1x speedups of convolutional layer computation of AlexNet\nagainst CPU and GPU, respectively, with off-the-shelf libraries. These speedups\nare about twice speedups of non-structured sparsity; (3) regularize the DNN\nstructure to improve classification accuracy. The results show that for\nCIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual\nNetwork (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%,\nwhich is still slightly higher than that of original ResNet with 32 layers. For\nAlexNet, structure regularization by SSL also reduces the error by around ~1%.\nOpen source code is in https://github.com/wenwei202/caffe/tree/scnn\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 03:20:43 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 20:46:15 GMT"}, {"version": "v3", "created": "Fri, 19 Aug 2016 05:58:48 GMT"}, {"version": "v4", "created": "Tue, 18 Oct 2016 04:03:41 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Wen", "Wei", ""], ["Wu", "Chunpeng", ""], ["Wang", "Yandan", ""], ["Chen", "Yiran", ""], ["Li", "Hai", ""]]}, {"id": "1608.03694", "submitter": "Sungjoon Choi", "authors": "Sungjoon Choi, Kyungjae Lee, Andy Park, Songhwai Oh", "title": "Density Matching Reward Learning", "comments": "Submitted to Workshop on Algorithmic Foundations of Robotics (WAFR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the problem of inferring the underlying reward\nfunction of an expert given demonstrations, which is often referred to as\ninverse reinforcement learning (IRL). In particular, we propose a model-free\ndensity-based IRL algorithm, named density matching reward learning (DMRL),\nwhich does not require model dynamics. The performance of DMRL is analyzed\ntheoretically and the sample complexity is derived. Furthermore, the proposed\nDMRL is extended to handle nonlinear IRL problems by assuming that the reward\nfunction is in the reproducing kernel Hilbert space (RKHS) and kernel DMRL\n(KDMRL) is proposed. The parameters for KDMRL can be computed analytically,\nwhich greatly reduces the computation time. The performance of KDMRL is\nextensively evaluated in two sets of experiments: grid world and track driving\nexperiments. In grid world experiments, the proposed KDMRL method is compared\nwith both model-based and model-free IRL methods and shows superior performance\non a nonlinear reward setting and competitive performance on a linear reward\nsetting in terms of expected value differences. Then we move on to more\nrealistic experiments of learning different driving styles for autonomous\nnavigation in complex and dynamic tracks using KDMRL and receding horizon\ncontrol.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 07:26:59 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Choi", "Sungjoon", ""], ["Lee", "Kyungjae", ""], ["Park", "Andy", ""], ["Oh", "Songhwai", ""]]}, {"id": "1608.03714", "submitter": "Haiping Huang", "authors": "Haiping Huang and Taro Toyoizumi", "title": "Unsupervised feature learning from finite data by message passing:\n  discontinuous versus continuous phase transition", "comments": "8 pages, 7 figures (5 pages, 4 figures in the main text and 3 pages\n  of appendix)", "journal-ref": "Phys. Rev. E 94, 062310 (2016)", "doi": "10.1103/PhysRevE.94.062310", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised neural network learning extracts hidden features from unlabeled\ntraining data. This is used as a pretraining step for further supervised\nlearning in deep networks. Hence, understanding unsupervised learning is of\nfundamental importance. Here, we study the unsupervised learning from a finite\nnumber of data, based on the restricted Boltzmann machine learning. Our study\ninspires an efficient message passing algorithm to infer the hidden feature,\nand estimate the entropy of candidate features consistent with the data. Our\nanalysis reveals that the learning requires only a few data if the feature is\nsalient and extensively many if the feature is weak. Moreover, the entropy of\ncandidate features monotonically decreases with data size and becomes negative\n(i.e., entropy crisis) before the message passing becomes unstable, suggesting\na discontinuous phase transition. In terms of convergence time of the message\npassing algorithm, the unsupervised learning exhibits an easy-hard-easy\nphenomenon as the training data size increases. All these properties are\nreproduced in an approximate Hopfield model, with an exception that the entropy\ncrisis is absent, and only continuous phase transition is observed. This key\ndifference is also confirmed in a handwritten digits dataset. This study\ndeepens our understanding of unsupervised learning from a finite number of\ndata, and may provide insights into its role in training deep networks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 08:35:22 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 01:49:13 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Huang", "Haiping", ""], ["Toyoizumi", "Taro", ""]]}, {"id": "1608.03793", "submitter": "Rajiv Shah", "authors": "Rajiv Shah and Rob Romijnders", "title": "Applying Deep Learning to Basketball Trajectories", "comments": "KDD 2016, Large Scale Sports Analytic Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the emerging trends for sports analytics is the growing use of player\nand ball tracking data. A parallel development is deep learning predictive\napproaches that use vast quantities of data with less reliance on feature\nengineering. This paper applies recurrent neural networks in the form of\nsequence modeling to predict whether a three-point shot is successful. The\nmodels are capable of learning the trajectory of a basketball without any\nknowledge of physics. For comparison, a baseline static machine learning model\nwith a full set of features, such as angle and velocity, in addition to the\npositional data is also tested. Using a dataset of over 20,000 three pointers\nfrom NBA SportVu data, the models based simply on sequential positional data\noutperform a static feature rich machine learning model in predicting whether a\nthree-point shot is successful. This suggests deep learning models may offer an\nimprovement to traditional feature based machine learning methods for tracking\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 13:50:24 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 18:36:44 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Shah", "Rajiv", ""], ["Romijnders", "Rob", ""]]}, {"id": "1608.03811", "submitter": "Joani Mitro", "authors": "Joani Mitro", "title": "Content-based image retrieval tutorial", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper functions as a tutorial for individuals interested to enter the\nfield of information retrieval but wouldn't know where to begin from. It\ndescribes two fundamental yet efficient image retrieval techniques, the first\nbeing k - nearest neighbors (knn) and the second support vector machines(svm).\nThe goal is to provide the reader with both the theoretical and practical\naspects in order to acquire a better understanding. Along with this tutorial we\nhave also developed the equivalent software1 using the MATLAB environment in\norder to illustrate the techniques, so that the reader can have a hands-on\nexperience.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 14:40:46 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Mitro", "Joani", ""]]}, {"id": "1608.03866", "submitter": "Shripad Gade", "authors": "Shripad Gade and Nitin H. Vaidya", "title": "Distributed Optimization for Client-Server Architecture with Negative\n  Gradient Weights", "comments": "[Submitted 12 Aug., 2016. Revised 18 Dec.,2016.] Added Section 3.1,\n  added additional discussion to Section 5, added references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Availability of both massive datasets and computing resources have made\nmachine learning and predictive analytics extremely pervasive. In this work we\npresent a synchronous algorithm and architecture for distributed optimization\nmotivated by privacy requirements posed by applications in machine learning. We\npresent an algorithm for the recently proposed multi-parameter-server\narchitecture. We consider a group of parameter servers that learn a model based\non randomized gradients received from clients. Clients are computational\nentities with private datasets (inducing a private objective function), that\nevaluate and upload randomized gradients to the parameter servers. The\nparameter servers perform model updates based on received gradients and share\nthe model parameters with other servers. We prove that the proposed algorithm\ncan optimize the overall objective function for a very general architecture\ninvolving $C$ clients connected to $S$ parameter servers in an arbitrary time\nvarying topology and the parameter servers forming a connected network.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 18:34:06 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 15:19:25 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Gade", "Shripad", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1608.03902", "submitter": "Muhammad Imran", "authors": "Dat Tien Nguyen, Kamela Ali Al Mannai, Shafiq Joty, Hassan Sajjad,\n  Muhammad Imran, Prasenjit Mitra", "title": "Rapid Classification of Crisis-Related Data on Social Networks using\n  Convolutional Neural Networks", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of social media, in particular microblogging platforms such as\nTwitter, as a conduit for actionable and tactical information during disasters\nis increasingly acknowledged. However, time-critical analysis of big crisis\ndata on social media streams brings challenges to machine learning techniques,\nespecially the ones that use supervised learning. The Scarcity of labeled data,\nparticularly in the early hours of a crisis, delays the machine learning\nprocess. The current state-of-the-art classification methods require a\nsignificant amount of labeled data specific to a particular event for training\nplus a lot of feature engineering to achieve best results. In this work, we\nintroduce neural network based classification methods for binary and\nmulti-class tweet classification task. We show that neural network based models\ndo not require any feature engineering and perform better than state-of-the-art\nmethods. In the early hours of a disaster when no labeled data is available,\nour proposed method makes the best use of the out-of-event data and achieves\ngood results.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 20:19:16 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Nguyen", "Dat Tien", ""], ["Mannai", "Kamela Ali Al", ""], ["Joty", "Shafiq", ""], ["Sajjad", "Hassan", ""], ["Imran", "Muhammad", ""], ["Mitra", "Prasenjit", ""]]}, {"id": "1608.03933", "submitter": "Lijun Zhang", "authors": "Lijun Zhang, Tianbao Yang, Jinfeng Yi, Rong Jin, Zhi-Hua Zhou", "title": "Improved Dynamic Regret for Non-degenerate Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a growing research interest in the analysis of\ndynamic regret, which measures the performance of an online learner against a\nsequence of local minimizers. By exploiting the strong convexity, previous\nstudies have shown that the dynamic regret can be upper bounded by the\npath-length of the comparator sequence. In this paper, we illustrate that the\ndynamic regret can be further improved by allowing the learner to query the\ngradient of the function multiple times, and meanwhile the strong convexity can\nbe weakened to other non-degenerate conditions. Specifically, we introduce the\nsquared path-length, which could be much smaller than the path-length, as a new\nregularity of the comparator sequence. When multiple gradients are accessible\nto the learner, we first demonstrate that the dynamic regret of strongly convex\nfunctions can be upper bounded by the minimum of the path-length and the\nsquared path-length. We then extend our theoretical guarantee to functions that\nare semi-strongly convex or self-concordant. To the best of our knowledge, this\nis the first time that semi-strong convexity and self-concordance are utilized\nto tighten the dynamic regret.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 03:24:11 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 03:15:12 GMT"}, {"version": "v3", "created": "Thu, 2 Nov 2017 07:15:50 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Zhang", "Lijun", ""], ["Yang", "Tianbao", ""], ["Yi", "Jinfeng", ""], ["Jin", "Rong", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1608.03974", "submitter": "Giovanni Montana", "authors": "Rudra P K Poudel and Pablo Lamata and Giovanni Montana", "title": "Recurrent Fully Convolutional Neural Networks for Multi-slice MRI\n  Cardiac Segmentation", "comments": "MICCAI Workshop RAMBO 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cardiac magnetic resonance imaging, fully-automatic segmentation of the\nheart enables precise structural and functional measurements to be taken, e.g.\nfrom short-axis MR images of the left-ventricle. In this work we propose a\nrecurrent fully-convolutional network (RFCN) that learns image representations\nfrom the full stack of 2D slices and has the ability to leverage inter-slice\nspatial dependences through internal memory units. RFCN combines anatomical\ndetection and segmentation into a single architecture that is trained\nend-to-end thus significantly reducing computational time, simplifying the\nsegmentation pipeline, and potentially enabling real-time applications. We\nreport on an investigation of RFCN using two datasets, including the publicly\navailable MICCAI 2009 Challenge dataset. Comparisons have been carried out\nbetween fully convolutional networks and deep restricted Boltzmann machines,\nincluding a recurrent version that leverages inter-slice spatial correlation.\nOur studies suggest that RFCN produces state-of-the-art results and can\nsubstantially improve the delineation of contours near the apex of the heart.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 11:19:22 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Poudel", "Rudra P K", ""], ["Lamata", "Pablo", ""], ["Montana", "Giovanni", ""]]}, {"id": "1608.03983", "submitter": "Ilya Loshchilov", "authors": "Ilya Loshchilov and Frank Hutter", "title": "SGDR: Stochastic Gradient Descent with Warm Restarts", "comments": "ICLR 2017 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restart techniques are common in gradient-free optimization to deal with\nmultimodal functions. Partial warm restarts are also gaining popularity in\ngradient-based optimization to improve the rate of convergence in accelerated\ngradient schemes to deal with ill-conditioned functions. In this paper, we\npropose a simple warm restart technique for stochastic gradient descent to\nimprove its anytime performance when training deep neural networks. We\nempirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where\nwe demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively.\nWe also demonstrate its advantages on a dataset of EEG recordings and on a\ndownsampled version of the ImageNet dataset. Our source code is available at\nhttps://github.com/loshchil/SGDR\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 13:46:05 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 13:05:07 GMT"}, {"version": "v3", "created": "Thu, 23 Feb 2017 14:33:00 GMT"}, {"version": "v4", "created": "Mon, 6 Mar 2017 13:06:59 GMT"}, {"version": "v5", "created": "Wed, 3 May 2017 16:28:09 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Loshchilov", "Ilya", ""], ["Hutter", "Frank", ""]]}, {"id": "1608.04037", "submitter": "Davi Frossard", "authors": "Davi E. N. Frossard, Igor O. Nunes, Renato A. Krohling", "title": "An approach to dealing with missing values in heterogeneous data using\n  k-nearest neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques such as clusterization, neural networks and decision making\nusually rely on algorithms that are not well suited to deal with missing\nvalues. However, real world data frequently contains such cases. The simplest\nsolution is to either substitute them by a best guess value or completely\ndisregard the missing values. Unfortunately, both approaches can lead to biased\nresults. In this paper, we propose a technique for dealing with missing values\nin heterogeneous data using imputation based on the k-nearest neighbors\nalgorithm. It can handle real (which we refer to as crisp henceforward),\ninterval and fuzzy data. The effectiveness of the algorithm is tested on\nseveral datasets and the numerical results are promising.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 23:45:21 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Frossard", "Davi E. N.", ""], ["Nunes", "Igor O.", ""], ["Krohling", "Renato A.", ""]]}, {"id": "1608.04062", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Shiyu Chang, Qing Ling, Shuai Huang, Xia Hu, Honghui\n  Shi, Thomas S. Huang", "title": "Stacked Approximated Regression Machine: A Simple Deep Learning Approach", "comments": "This manuscript has been withdrawn by the authors. Please see the\n  updated text for details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the agreement of my coauthors, I Zhangyang Wang would like to withdraw\nthe manuscript \"Stacked Approximated Regression Machine: A Simple Deep Learning\nApproach\". Some experimental procedures were not included in the manuscript,\nwhich makes a part of important claims not meaningful. In the relevant\nresearch, I was solely responsible for carrying out the experiments; the other\ncoauthors joined in the discussions leading to the main algorithm.\n  Please see the updated text for more details.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 05:35:11 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 17:46:13 GMT"}], "update_date": "2016-10-02", "authors_parsed": [["Wang", "Zhangyang", ""], ["Chang", "Shiyu", ""], ["Ling", "Qing", ""], ["Huang", "Shuai", ""], ["Hu", "Xia", ""], ["Shi", "Honghui", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1608.04063", "submitter": "Hyun-Chul Kim", "authors": "Hyun-Chul Kim", "title": "Bayesian Model Selection Methods for Mutual and Symmetric $k$-Nearest\n  Neighbor Classification", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-nearest neighbor classification method ($k$-NNC) is one of the\nsimplest nonparametric classification methods. The mutual $k$-NN classification\nmethod (M$k$NNC) is a variant of $k$-NNC based on mutual neighborship. We\npropose another variant of $k$-NNC, the symmetric $k$-NN classification method\n(S$k$NNC) based on both mutual neighborship and one-sided neighborship. The\nperformance of M$k$NNC and S$k$NNC depends on the parameter $k$ as the one of\n$k$-NNC does. We propose the ways how M$k$NN and S$k$NN classification can be\nperformed based on Bayesian mutual and symmetric $k$-NN regression methods with\nthe selection schemes for the parameter $k$. Bayesian mutual and symmetric\n$k$-NN regression methods are based on Gaussian process models, and it turns\nout that they can do M$k$NN and S$k$NN classification with new encodings of\ntarget values (class labels). The simulation results show that the proposed\nmethods are better than or comparable to $k$-NNC, M$k$NNC and S$k$NNC with the\nparameter $k$ selected by the leave-one-out cross validation method not only\nfor an artificial data set but also for real world data sets.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 06:01:21 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Kim", "Hyun-Chul", ""]]}, {"id": "1608.04077", "submitter": "Sungho Shin", "authors": "Sungho Shin, Kyuyeon Hwang, and Wonyong Sung", "title": "Generative Knowledge Transfer for Neural Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a generative knowledge transfer technique that\ntrains an RNN based language model (student network) using text and output\nprobabilities generated from a previously trained RNN (teacher network). The\ntext generation can be conducted by either the teacher or the student network.\nWe can also improve the performance by taking the ensemble of soft labels\nobtained from multiple teacher networks. This method can be used for privacy\nconscious language model adaptation because no user data is directly used for\ntraining. Especially, when the soft labels of multiple devices are aggregated\nvia a trusted third party, we can expect very strong privacy protection.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 09:19:26 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 20:20:08 GMT"}, {"version": "v3", "created": "Tue, 28 Feb 2017 08:25:33 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Shin", "Sungho", ""], ["Hwang", "Kyuyeon", ""], ["Sung", "Wonyong", ""]]}, {"id": "1608.04080", "submitter": "Sungho Shin", "authors": "Sungho Shin and Wonyong Sung", "title": "Dynamic Hand Gesture Recognition for Wearable Devices with Low\n  Complexity Recurrent Neural Networks", "comments": "This paper was accepted in ISCAS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gesture recognition is a very essential technology for many wearable devices.\nWhile previous algorithms are mostly based on statistical methods including the\nhidden Markov model, we develop two dynamic hand gesture recognition techniques\nusing low complexity recurrent neural network (RNN) algorithms. One is based on\nvideo signal and employs a combined structure of a convolutional neural network\n(CNN) and an RNN. The other uses accelerometer data and only requires an RNN.\nFixed-point optimization that quantizes most of the weights into two bits is\nconducted to optimize the amount of memory size for weight storage and reduce\nthe power consumption in hardware and software based implementations.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 09:32:17 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Shin", "Sungho", ""], ["Sung", "Wonyong", ""]]}, {"id": "1608.04171", "submitter": "Yuanlong Li", "authors": "Yuanlong Li, Han Hu, Yonggang Wen, Jun Zhang", "title": "Power Data Classification: A Hybrid of a Novel Local Time Warping and\n  LSTM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, for the purpose of data centre energy consumption monitoring\nand analysis, we propose to detect the running programs in a server by\nclassifying the observed power consumption series. Time series classification\nproblem has been extensively studied with various distance measurements\ndeveloped; also recently the deep learning based sequence models have been\nproved to be promising. In this paper, we propose a novel distance measurement\nand build a time series classification algorithm hybridizing nearest neighbour\nand long short term memory (LSTM) neural network. More specifically, first we\npropose a new distance measurement termed as Local Time Warping (LTW), which\nutilizes a user-specified set for local warping, and is designed to be\nnon-commutative and non-dynamic programming. Second we hybridize the 1NN-LTW\nand LSTM together. In particular, we combine the prediction probability vector\nof 1NN-LTW and LSTM to determine the label of the test cases. Finally, using\nthe power consumption data from a real data center, we show that the proposed\nLTW can improve the classification accuracy of DTW from about 84% to 90%. Our\nexperimental results prove that the proposed LTW is competitive on our data set\ncompared with existed DTW variants and its non-commutative feature is indeed\nbeneficial. We also test a linear version of LTW and it can significantly\noutperform existed linear runtime lower bound methods like LB_Keogh.\nFurthermore, with the hybrid algorithm, for the power series classification\ntask we achieve an accuracy up to about 93%. Our research can inspire more\nstudies on time series distance measurement and the hybrid of the deep learning\nmodels with other traditional models.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 02:49:17 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 12:51:46 GMT"}, {"version": "v3", "created": "Sat, 8 Oct 2016 04:33:02 GMT"}, {"version": "v4", "created": "Wed, 7 Jun 2017 04:34:06 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Li", "Yuanlong", ""], ["Hu", "Han", ""], ["Wen", "Yonggang", ""], ["Zhang", "Jun", ""]]}, {"id": "1608.04219", "submitter": "Matthew England Dr", "authors": "Zongyan Huang, Matthew England, James H. Davenport and Lawrence C.\n  Paulson", "title": "Using Machine Learning to Decide When to Precondition Cylindrical\n  Algebraic Decomposition With Groebner Bases", "comments": null, "journal-ref": "Proceedings of the 18th International Symposium on Symbolic and\n  Numeric Algorithms for Scientific Computing (SYNASC '16), pp. 45--52. IEEE,\n  2016", "doi": "10.1109/SYNASC.2016.020", "report-no": null, "categories": "cs.SC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cylindrical Algebraic Decomposition (CAD) is a key tool in computational\nalgebraic geometry, particularly for quantifier elimination over real-closed\nfields. However, it can be expensive, with worst case complexity doubly\nexponential in the size of the input. Hence it is important to formulate the\nproblem in the best manner for the CAD algorithm. One possibility is to\nprecondition the input polynomials using Groebner Basis (GB) theory. Previous\nexperiments have shown that while this can often be very beneficial to the CAD\nalgorithm, for some problems it can significantly worsen the CAD performance.\n  In the present paper we investigate whether machine learning, specifically a\nsupport vector machine (SVM), may be used to identify those CAD problems which\nbenefit from GB preconditioning. We run experiments with over 1000 problems\n(many times larger than previous studies) and find that the machine learned\nchoice does better than the human-made heuristic.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 09:44:29 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Huang", "Zongyan", ""], ["England", "Matthew", ""], ["Davenport", "James H.", ""], ["Paulson", "Lawrence C.", ""]]}, {"id": "1608.04236", "submitter": "Andrew Brock", "authors": "Andrew Brock, Theodore Lim, J.M. Ritchie, Nick Weston", "title": "Generative and Discriminative Voxel Modeling with Convolutional Neural\n  Networks", "comments": "9 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When working with three-dimensional data, choice of representation is key. We\nexplore voxel-based models, and present evidence for the viability of\nvoxellated representations in applications including shape modeling and object\nclassification. Our key contributions are methods for training voxel-based\nvariational autoencoders, a user interface for exploring the latent space\nlearned by the autoencoder, and a deep convolutional neural network\narchitecture for object classification. We address challenges unique to\nvoxel-based representations, and empirically evaluate our models on the\nModelNet benchmark, where we demonstrate a 51.5% relative improvement in the\nstate of the art for object classification.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 11:14:35 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 08:06:24 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Brock", "Andrew", ""], ["Lim", "Theodore", ""], ["Ritchie", "J. M.", ""], ["Weston", "Nick", ""]]}, {"id": "1608.04245", "submitter": "Mike Gartrell", "authors": "Mike Gartrell, Ulrich Paquet, Noam Koenigstein", "title": "The Bayesian Low-Rank Determinantal Point Process Mixture Model", "comments": "9 pages, 6 figures. This article draws heavily from arXiv:1602.05436", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are an elegant model for encoding\nprobabilities over subsets, such as shopping baskets, of a ground set, such as\nan item catalog. They are useful for a number of machine learning tasks,\nincluding product recommendation. DPPs are parametrized by a positive\nsemi-definite kernel matrix. Recent work has shown that using a low-rank\nfactorization of this kernel provides remarkable scalability improvements that\nopen the door to training on large-scale datasets and computing online\nrecommendations, both of which are infeasible with standard DPP models that use\na full-rank kernel. In this paper we present a low-rank DPP mixture model that\nallows us to represent the latent structure present in observed subsets as a\nmixture of a number of component low-rank DPPs, where each component DPP is\nresponsible for representing a portion of the observed data. The mixture model\nallows us to effectively address the capacity constraints of the low-rank DPP\nmodel. We present an efficient and scalable Markov Chain Monte Carlo (MCMC)\nlearning algorithm for our model that uses Gibbs sampling and stochastic\ngradient Hamiltonian Monte Carlo (SGHMC). Using an evaluation on several\nreal-world product recommendation datasets, we show that our low-rank DPP\nmixture model provides substantially better predictive performance than is\npossible with a single low-rank or full-rank DPP, and significantly better\nperformance than several other competing recommendation methods in many cases.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 11:42:51 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 10:41:32 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Gartrell", "Mike", ""], ["Paquet", "Ulrich", ""], ["Koenigstein", "Noam", ""]]}, {"id": "1608.04320", "submitter": "Han Guo", "authors": "Namrata Vaswani, Han Guo", "title": "Correlated-PCA: Principal Components' Analysis when Data and Noise are\n  Correlated", "comments": "NIPS 2016 (to appear). Longer version submitted to IEEE Trans. Sig.\n  Proc. is at http://www.ece.iastate.edu/~namrata/TSP_corPCA.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a matrix of observed data, Principal Components Analysis (PCA) computes\na small number of orthogonal directions that contain most of its variability.\nProvably accurate solutions for PCA have been in use for a long time. However,\nto the best of our knowledge, all existing theoretical guarantees for it assume\nthat the data and the corrupting noise are mutually independent, or at least\nuncorrelated. This is valid in practice often, but not always. In this paper,\nwe study the PCA problem in the setting where the data and noise can be\ncorrelated. Such noise is often also referred to as \"data-dependent noise\". We\nobtain a correctness result for the standard eigenvalue decomposition (EVD)\nbased solution to PCA under simple assumptions on the data-noise correlation.\nWe also develop and analyze a generalization of EVD, cluster-EVD, that improves\nupon EVD in certain regimes.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 16:32:57 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 17:55:02 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Vaswani", "Namrata", ""], ["Guo", "Han", ""]]}, {"id": "1608.04331", "submitter": "Jared Culbertson", "authors": "Jared Culbertson, Dan P. Guralnik, Jakob Hansen, Peter F. Stiller", "title": "Consistency constraints for overlapping data clustering", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine overlapping clustering schemes with functorial constraints, in the\nspirit of Carlsson--Memoli. This avoids issues arising from the chaining\nrequired by partition-based methods. Our principal result shows that any\nclustering functor is naturally constrained to refine single-linkage clusters\nand be refined by maximal-linkage clusters. We work in the context of metric\nspaces with non-expansive maps, which is appropriate for modeling data\nprocessing which does not increase information content.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 17:12:09 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Culbertson", "Jared", ""], ["Guralnik", "Dan P.", ""], ["Hansen", "Jakob", ""], ["Stiller", "Peter F.", ""]]}, {"id": "1608.04348", "submitter": "Jeff Calder", "authors": "Bilal Abbasi, Jeff Calder, Adam M. Oberman", "title": "Anomaly detection and classification for streaming data using PDEs", "comments": null, "journal-ref": "SIAM Journal on Applied Math, 78(2), 921--941, 2018", "doi": "10.1137/17M1121184", "report-no": null, "categories": "cs.LG cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nondominated sorting, also called Pareto Depth Analysis (PDA), is widely used\nin multi-objective optimization and has recently found important applications\nin multi-criteria anomaly detection. Recently, a partial differential equation\n(PDE) continuum limit was discovered for nondominated sorting leading to a very\nfast approximate sorting algorithm called PDE-based ranking. We propose in this\npaper a fast real-time streaming version of the PDA algorithm for anomaly\ndetection that exploits the computational advantages of PDE continuum limits.\nFurthermore, we derive new PDE continuum limits for sorting points within their\nnondominated layers and show how the new PDEs can be used to classify anomalies\nbased on which criterion was more significantly violated. We also prove\nstatistical convergence rates for PDE-based ranking, and present the results of\nnumerical experiments with both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 18:03:51 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 19:50:07 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Abbasi", "Bilal", ""], ["Calder", "Jeff", ""], ["Oberman", "Adam M.", ""]]}, {"id": "1608.04363", "submitter": "Justin Salamon", "authors": "Justin Salamon and Juan Pablo Bello", "title": "Deep Convolutional Neural Networks and Data Augmentation for\n  Environmental Sound Classification", "comments": "Accepted November 2016, IEEE Signal Processing Letters. Copyright\n  IEEE. Personal use of this material is permitted. Permission from IEEE must\n  be obtained for all other uses, in any current or future media, including\n  reprinting/republishing this material, creating new collective works, for\n  resale or redistribution, or reuse of any copyrighted component of this work\n  in other works", "journal-ref": null, "doi": "10.1109/LSP.2017.2657381", "report-no": null, "categories": "cs.SD cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of deep convolutional neural networks (CNN) to learn\ndiscriminative spectro-temporal patterns makes them well suited to\nenvironmental sound classification. However, the relative scarcity of labeled\ndata has impeded the exploitation of this family of high-capacity models. This\nstudy has two primary contributions: first, we propose a deep convolutional\nneural network architecture for environmental sound classification. Second, we\npropose the use of audio data augmentation for overcoming the problem of data\nscarcity and explore the influence of different augmentations on the\nperformance of the proposed CNN architecture. Combined with data augmentation,\nthe proposed model produces state-of-the-art results for environmental sound\nclassification. We show that the improved performance stems from the\ncombination of a deep, high-capacity model and an augmented training set: this\ncombination outperforms both the proposed CNN without augmentation and a\n\"shallow\" dictionary learning model with augmentation. Finally, we examine the\ninfluence of each augmentation on the model's classification accuracy for each\nclass, and observe that the accuracy for each class is influenced differently\nby each augmentation, suggesting that the performance of the model could be\nimproved further by applying class-conditional data augmentation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 18:57:10 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 17:48:04 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Salamon", "Justin", ""], ["Bello", "Juan Pablo", ""]]}, {"id": "1608.04414", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman", "title": "Generalization of ERM in Stochastic Convex Optimization: The Dimension\n  Strikes Back", "comments": "Added illustrations of functions used in some of the constructions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In stochastic convex optimization the goal is to minimize a convex function\n$F(x) \\doteq {\\mathbf E}_{{\\mathbf f}\\sim D}[{\\mathbf f}(x)]$ over a convex set\n$\\cal K \\subset {\\mathbb R}^d$ where $D$ is some unknown distribution and each\n$f(\\cdot)$ in the support of $D$ is convex over $\\cal K$. The optimization is\ncommonly based on i.i.d.~samples $f^1,f^2,\\ldots,f^n$ from $D$. A standard\napproach to such problems is empirical risk minimization (ERM) that optimizes\n$F_S(x) \\doteq \\frac{1}{n}\\sum_{i\\leq n} f^i(x)$. Here we consider the question\nof how many samples are necessary for ERM to succeed and the closely related\nquestion of uniform convergence of $F_S$ to $F$ over $\\cal K$. We demonstrate\nthat in the standard $\\ell_p/\\ell_q$ setting of Lipschitz-bounded functions\nover a $\\cal K$ of bounded radius, ERM requires sample size that scales\nlinearly with the dimension $d$. This nearly matches standard upper bounds and\nimproves on $\\Omega(\\log d)$ dependence proved for $\\ell_2/\\ell_2$ setting by\nShalev-Shwartz et al. (2009). In stark contrast, these problems can be solved\nusing dimension-independent number of samples for $\\ell_2/\\ell_2$ setting and\n$\\log d$ dependence for $\\ell_1/\\ell_\\infty$ setting using other approaches. We\nfurther show that our lower bound applies even if the functions in the support\nof $D$ are smooth and efficiently computable and even if an $\\ell_1$\nregularization term is added. Finally, we demonstrate that for a more general\nclass of bounded-range (but not Lipschitz-bounded) stochastic convex programs\nan infinite gap appears already in dimension 2.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 21:19:51 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 00:46:58 GMT"}, {"version": "v3", "created": "Mon, 26 Dec 2016 06:37:48 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Feldman", "Vitaly", ""]]}, {"id": "1608.04426", "submitter": "Baiyang Wang", "authors": "Baiyang Wang, Diego Klabjan", "title": "Regularization for Unsupervised Deep Neural Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised neural networks, such as restricted Boltzmann machines (RBMs)\nand deep belief networks (DBNs), are powerful tools for feature selection and\npattern recognition tasks. We demonstrate that overfitting occurs in such\nmodels just as in deep feedforward neural networks, and discuss possible\nregularization methods to reduce overfitting. We also propose a \"partial\"\napproach to improve the efficiency of Dropout/DropConnect in this scenario, and\ndiscuss the theoretical justification of these methods from model convergence\nand likelihood bounds. Finally, we compare the performance of these methods\nbased on their likelihood and classification error rates for various pattern\nrecognition data sets.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 22:28:05 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 23:41:53 GMT"}, {"version": "v3", "created": "Mon, 5 Sep 2016 16:53:55 GMT"}, {"version": "v4", "created": "Fri, 17 Feb 2017 02:49:12 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Wang", "Baiyang", ""], ["Klabjan", "Diego", ""]]}, {"id": "1608.04428", "submitter": "Alexander Gaunt", "authors": "Alexander L. Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman,\n  Pushmeet Kohli, Jonathan Taylor, Daniel Tarlow", "title": "TerpreT: A Probabilistic Programming Language for Program Induction", "comments": "50 pages, 20 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study machine learning formulations of inductive program synthesis; given\ninput-output examples, we try to synthesize source code that maps inputs to\ncorresponding outputs. Our aims are to develop new machine learning approaches\nbased on neural networks and graphical models, and to understand the\ncapabilities of machine learning techniques relative to traditional\nalternatives, such as those based on constraint solving from the programming\nlanguages community.\n  Our key contribution is the proposal of TerpreT, a domain-specific language\nfor expressing program synthesis problems. TerpreT is similar to a\nprobabilistic programming language: a model is composed of a specification of a\nprogram representation (declarations of random variables) and an interpreter\ndescribing how programs map inputs to outputs (a model connecting unknowns to\nobservations). The inference task is to observe a set of input-output examples\nand infer the underlying program. TerpreT has two main benefits. First, it\nenables rapid exploration of a range of domains, program representations, and\ninterpreter models. Second, it separates the model specification from the\ninference algorithm, allowing like-to-like comparisons between different\napproaches to inference. From a single TerpreT specification we automatically\nperform inference using four different back-ends. These are based on gradient\ndescent, linear program (LP) relaxations for graphical models, discrete\nsatisfiability solving, and the Sketch program synthesis system.\n  We illustrate the value of TerpreT by developing several interpreter models\nand performing an empirical comparison between alternative inference\nalgorithms. Our key empirical finding is that constraint solvers dominate the\ngradient descent and LP-based formulations. We conclude with suggestions for\nthe machine learning community to make progress on program synthesis.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 22:34:50 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Gaunt", "Alexander L.", ""], ["Brockschmidt", "Marc", ""], ["Singh", "Rishabh", ""], ["Kushman", "Nate", ""], ["Kohli", "Pushmeet", ""], ["Taylor", "Jonathan", ""], ["Tarlow", "Daniel", ""]]}, {"id": "1608.04468", "submitter": "Tobias Schnabel", "authors": "Thorsten Joachims, Adith Swaminathan, Tobias Schnabel", "title": "Unbiased Learning-to-Rank with Biased Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit feedback (e.g., clicks, dwell times, etc.) is an abundant source of\ndata in human-interactive systems. While implicit feedback has many advantages\n(e.g., it is inexpensive to collect, user centric, and timely), its inherent\nbiases are a key obstacle to its effective use. For example, position bias in\nsearch rankings strongly influences how many clicks a result receives, so that\ndirectly using click data as a training signal in Learning-to-Rank (LTR)\nmethods yields sub-optimal results. To overcome this bias problem, we present a\ncounterfactual inference framework that provides the theoretical basis for\nunbiased LTR via Empirical Risk Minimization despite biased data. Using this\nframework, we derive a Propensity-Weighted Ranking SVM for discriminative\nlearning from implicit feedback, where click models take the role of the\npropensity estimator. In contrast to most conventional approaches to de-bias\nthe data using click models, this allows training of ranking functions even in\nsettings where queries do not repeat. Beyond the theoretical support, we show\nempirically that the proposed learning method is highly effective in dealing\nwith biases, that it is robust to noise and propensity model misspecification,\nand that it scales efficiently. We also demonstrate the real-world\napplicability of our approach on an operational search engine, where it\nsubstantially improves retrieval performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 02:56:24 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Joachims", "Thorsten", ""], ["Swaminathan", "Adith", ""], ["Schnabel", "Tobias", ""]]}, {"id": "1608.04471", "submitter": "Dilin Wang", "authors": "Qiang Liu and Dilin Wang", "title": "Stein Variational Gradient Descent: A General Purpose Bayesian Inference\n  Algorithm", "comments": "To appear in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general purpose variational inference algorithm that forms a\nnatural counterpart of gradient descent for optimization. Our method\niteratively transports a set of particles to match the target distribution, by\napplying a form of functional gradient descent that minimizes the KL\ndivergence. Empirical studies are performed on various real world models and\ndatasets, on which our method is competitive with existing state-of-the-art\nmethods. The derivation of our method is based on a new theoretical result that\nconnects the derivative of KL divergence under smooth transforms with Stein's\nidentity and a recently proposed kernelized Stein discrepancy, which is of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 03:24:20 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 05:13:47 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 17:31:39 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Qiang", ""], ["Wang", "Dilin", ""]]}, {"id": "1608.04478", "submitter": "Zheng Tracy Ke", "authors": "Zheng Tracy Ke", "title": "A Geometrical Approach to Topic Model Estimation", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the probabilistic topic models, the quantity of interest---a low-rank\nmatrix consisting of topic vectors---is hidden in the text corpus matrix,\nmasked by noise, and the Singular Value Decomposition (SVD) is a potentially\nuseful tool for learning such a low-rank matrix. However, the connection\nbetween this low-rank matrix and the singular vectors of the text corpus matrix\nare usually complicated and hard to spell out, so how to use SVD for learning\ntopic models faces challenges. In this paper, we overcome the challenge by\nrevealing a surprising insight: there is a low-dimensional simplex structure\nwhich can be viewed as a bridge between the low-rank matrix of interest and the\nSVD of the text corpus matrix, and allows us to conveniently reconstruct the\nformer using the latter. Such an insight motivates a new SVD approach to\nlearning topic models, which we analyze with delicate random matrix theory and\nderive the rate of convergence. We support our methods and theory numerically,\nusing both simulated data and real data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 04:31:52 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Ke", "Zheng Tracy", ""]]}, {"id": "1608.04493", "submitter": "Anbang Yao", "authors": "Yiwen Guo, Anbang Yao, Yurong Chen", "title": "Dynamic Network Surgery for Efficient DNNs", "comments": "Accepted by NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become a ubiquitous technology to improve machine\nintelligence. However, most of the existing deep models are structurally very\ncomplex, making them difficult to be deployed on the mobile platforms with\nlimited computational power. In this paper, we propose a novel network\ncompression method called dynamic network surgery, which can remarkably reduce\nthe network complexity by making on-the-fly connection pruning. Unlike the\nprevious methods which accomplish this task in a greedy way, we properly\nincorporate connection splicing into the whole process to avoid incorrect\npruning and make it as a continual network maintenance. The effectiveness of\nour method is proved with experiments. Without any accuracy loss, our method\ncan efficiently compress the number of parameters in LeNet-5 and AlexNet by a\nfactor of $\\bm{108}\\times$ and $\\bm{17.7}\\times$ respectively, proving that it\noutperforms the recent pruning method by considerable margins. Code and some\nmodels are available at https://github.com/yiwenguo/Dynamic-Network-Surgery.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 06:23:05 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 00:17:25 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Guo", "Yiwen", ""], ["Yao", "Anbang", ""], ["Chen", "Yurong", ""]]}, {"id": "1608.04550", "submitter": "Joachim Van Der Herten", "authors": "Joachim van der Herten and Ivo Couckuyt and Dirk Deschrijver and Tom\n  Dhaene", "title": "Fast Calculation of the Knowledge Gradient for Optimization of\n  Deterministic Engineering Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel efficient method for computing the Knowledge-Gradient policy for\nContinuous Parameters (KGCP) for deterministic optimization is derived. The\ndifferences with Expected Improvement (EI), a popular choice for Bayesian\noptimization of deterministic engineering simulations, are explored. Both\npolicies and the Upper Confidence Bound (UCB) policy are compared on a number\nof benchmark functions including a problem from structural dynamics. It is\nempirically shown that KGCP has similar performance as the EI policy for many\nproblems, but has better convergence properties for complex (multi-modal)\noptimization problems as it emphasizes more on exploration when the model is\nconfident about the shape of optimal regions. In addition, the relationship\nbetween Maximum Likelihood Estimation (MLE) and slice sampling for estimation\nof the hyperparameters of the underlying models, and the complexity of the\nproblem at hand, is studied.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 11:26:25 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["van der Herten", "Joachim", ""], ["Couckuyt", "Ivo", ""], ["Deschrijver", "Dirk", ""], ["Dhaene", "Tom", ""]]}, {"id": "1608.04581", "submitter": "Jim Jing-Yan Wang", "authors": "Ru-Ze Liang, Wei Xie, Weizhi Li, Hongqi Wang, Jim Jing-Yan Wang, Lisa\n  Taylor", "title": "A novel transfer learning method based on common space mapping and\n  weighted domain matching", "comments": "arXiv admin note: text overlap with arXiv:1605.06673", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel learning framework for the problem of\ndomain transfer learning. We map the data of two domains to one single common\nspace, and learn a classifier in this common space. Then we adapt the common\nclassifier to the two domains by adding two adaptive functions to it\nrespectively. In the common space, the target domain data points are weighted\nand matched to the target domain in term of distributions. The weighting terms\nof source domain data points and the target domain classification responses are\nalso regularized by the local reconstruction coefficients. The novel transfer\nlearning framework is evaluated over some benchmark cross-domain data sets, and\nit outperforms the existing state-of-the-art transfer learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 13:17:51 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Liang", "Ru-Ze", ""], ["Xie", "Wei", ""], ["Li", "Weizhi", ""], ["Wang", "Hongqi", ""], ["Wang", "Jim Jing-Yan", ""], ["Taylor", "Lisa", ""]]}, {"id": "1608.04585", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev and Vladislav Ishimtsev", "title": "Conformalized density- and distance-based anomaly detection in\n  time-series data", "comments": "9 pages, 3 figures, conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomalies (unusual patterns) in time-series data give essential, and often\nactionable information in critical situations. Examples can be found in such\nfields as healthcare, intrusion detection, finance, security and flight safety.\nIn this paper we propose new conformalized density- and distance-based anomaly\ndetection algorithms for a one-dimensional time-series data. The algorithms use\na combination of a feature extraction method, an approach to assess a score\nwhether a new observation differs significantly from a previously observed\ndata, and a probabilistic interpretation of this score based on the conformal\nparadigm.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 13:32:05 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Ishimtsev", "Vladislav", ""]]}, {"id": "1608.04622", "submitter": "Filippo Maria Bianchi", "authors": "Sigurd L{\\o}kse, Filippo Maria Bianchi and Robert Jenssen", "title": "Training Echo State Networks with Regularization through Dimensionality\n  Reduction", "comments": null, "journal-ref": null, "doi": "10.1007/s12559-017-9450-z", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new framework to train an Echo State Network to\npredict real valued time-series. The method consists in projecting the output\nof the internal layer of the network on a space with lower dimensionality,\nbefore training the output layer to learn the target task. Notably, we enforce\na regularization constraint that leads to better generalization capabilities.\nWe evaluate the performances of our approach on several benchmark tests, using\ndifferent techniques to train the readout of the network, achieving superior\npredictive performance when using the proposed framework. Finally, we provide\nan insight on the effectiveness of the implemented mechanics through a\nvisualization of the trajectory in the phase space and relying on the\nmethodologies of nonlinear time-series analysis. By applying our method on well\nknown chaotic systems, we provide evidence that the lower dimensional embedding\nretains the dynamical properties of the underlying system better than the\nfull-dimensional internal states of the network.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 14:41:12 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["L\u00f8kse", "Sigurd", ""], ["Bianchi", "Filippo Maria", ""], ["Jenssen", "Robert", ""]]}, {"id": "1608.04636", "submitter": "Julie Nutini", "authors": "Hamed Karimi, Julie Nutini and Mark Schmidt", "title": "Linear Convergence of Gradient and Proximal-Gradient Methods Under the\n  Polyak-\\L{}ojasiewicz Condition", "comments": "[v4]: Fixes a constant factor in the PL-->QG proof, which also\n  simplifies the PL-->EB proof and implies that PL implies EB and QB with the\n  same constant", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1963, Polyak proposed a simple condition that is sufficient to show a\nglobal linear convergence rate for gradient descent. This condition is a\nspecial case of the \\L{}ojasiewicz inequality proposed in the same year, and it\ndoes not require strong convexity (or even convexity). In this work, we show\nthat this much-older Polyak-\\L{}ojasiewicz (PL) inequality is actually weaker\nthan the main conditions that have been explored to show linear convergence\nrates without strong convexity over the last 25 years. We also use the PL\ninequality to give new analyses of randomized and greedy coordinate descent\nmethods, sign-based gradient descent methods, and stochastic gradient methods\nin the classic setting (with decreasing or constant step-sizes) as well as the\nvariance-reduced setting. We further propose a generalization that applies to\nproximal-gradient methods for non-smooth optimization, leading to simple proofs\nof linear convergence of these methods. Along the way, we give simple\nconvergence results for a wide variety of problems in machine learning: least\nsquares, logistic regression, boosting, resilient backpropagation,\nL1-regularization, support vector machines, stochastic dual coordinate ascent,\nand stochastic variance-reduced gradient methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 15:28:24 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 04:08:30 GMT"}, {"version": "v3", "created": "Mon, 11 Jun 2018 22:47:30 GMT"}, {"version": "v4", "created": "Sat, 12 Sep 2020 23:03:52 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Karimi", "Hamed", ""], ["Nutini", "Julie", ""], ["Schmidt", "Mark", ""]]}, {"id": "1608.04647", "submitter": "Mihai Capot\\u{a}", "authors": "Michael J. Anderson, Mihai Capot\\u{a}, Javier S. Turek, Xia Zhu,\n  Theodore L. Willke, Yida Wang, Po-Hsuan Chen, Jeremy R. Manning, Peter J.\n  Ramadge, Kenneth A. Norman", "title": "Enabling Factor Analysis on Thousand-Subject Neuroimaging Datasets", "comments": null, "journal-ref": null, "doi": "10.1109/BigData.2016.7840719", "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scale of functional magnetic resonance image data is rapidly increasing\nas large multi-subject datasets are becoming widely available and\nhigh-resolution scanners are adopted. The inherent low-dimensionality of the\ninformation in this data has led neuroscientists to consider factor analysis\nmethods to extract and analyze the underlying brain activity. In this work, we\nconsider two recent multi-subject factor analysis methods: the Shared Response\nModel and Hierarchical Topographic Factor Analysis. We perform analytical,\nalgorithmic, and code optimization to enable multi-node parallel\nimplementations to scale. Single-node improvements result in 99x and 1812x\nspeedups on these two methods, and enables the processing of larger datasets.\nOur distributed implementations show strong scaling of 3.3x and 5.5x\nrespectively with 20 nodes on real datasets. We also demonstrate weak scaling\non a synthetic dataset with 1024 subjects, on up to 1024 nodes and 32,768\ncores.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 16:05:14 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 02:30:07 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Anderson", "Michael J.", ""], ["Capot\u0103", "Mihai", ""], ["Turek", "Javier S.", ""], ["Zhu", "Xia", ""], ["Willke", "Theodore L.", ""], ["Wang", "Yida", ""], ["Chen", "Po-Hsuan", ""], ["Manning", "Jeremy R.", ""], ["Ramadge", "Peter J.", ""], ["Norman", "Kenneth A.", ""]]}, {"id": "1608.04674", "submitter": "Eric Chi", "authors": "Bethany Lusch, Eric C. Chi, J. Nathan Kutz", "title": "Shape Constrained Tensor Decompositions using Sparse Representations in\n  Over-Complete Libraries", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider $N$-way data arrays and low-rank tensor factorizations where the\ntime mode is coded as a sparse linear combination of temporal elements from an\nover-complete library. Our method, Shape Constrained Tensor Decomposition\n(SCTD) is based upon the CANDECOMP/PARAFAC (CP) decomposition which produces\n$r$-rank approximations of data tensors via outer products of vectors in each\ndimension of the data. By constraining the vector in the temporal dimension to\nknown analytic forms which are selected from a large set of candidate\nfunctions, more readily interpretable decompositions are achieved and analytic\ntime dependencies discovered. The SCTD method circumvents traditional {\\em\nflattening} techniques where an $N$-way array is reshaped into a matrix in\norder to perform a singular value decomposition. A clear advantage of the SCTD\nalgorithm is its ability to extract transient and intermittent phenomena which\nis often difficult for SVD-based methods. We motivate the SCTD method using\nseveral intuitively appealing results before applying it on a number of\nhigh-dimensional, real-world data sets in order to illustrate the efficiency of\nthe algorithm in extracting interpretable spatio-temporal modes. With the rise\nof data-driven discovery methods, the decomposition proposed provides a viable\ntechnique for analyzing multitudes of data in a more comprehensible fashion.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 17:00:48 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Lusch", "Bethany", ""], ["Chi", "Eric C.", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "1608.04689", "submitter": "Hongyu Guo", "authors": "Martin Renqiang Min, Hongyu Guo, Dongjin Song", "title": "A Shallow High-Order Parametric Approach to Data Visualization and\n  Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explicit high-order feature interactions efficiently capture essential\nstructural knowledge about the data of interest and have been used for\nconstructing generative models. We present a supervised discriminative\nHigh-Order Parametric Embedding (HOPE) approach to data visualization and\ncompression. Compared to deep embedding models with complicated deep\narchitectures, HOPE generates more effective high-order feature mapping through\nan embarrassingly simple shallow model. Furthermore, two approaches to\ngenerating a small number of exemplars conveying high-order interactions to\nrepresent large-scale data sets are proposed. These exemplars in combination\nwith the feature mapping learned by HOPE effectively capture essential data\nvariations. Moreover, through HOPE, these exemplars are employed to increase\nthe computational efficiency of kNN classification for fast information\nretrieval by thousands of times. For classification in two-dimensional\nembedding space on MNIST and USPS datasets, our shallow method HOPE with simple\nSigmoid transformations significantly outperforms state-of-the-art supervised\ndeep embedding models based on deep neural networks, and even achieved\nhistorically low test error rate of 0.65% in two-dimensional space on MNIST,\nwhich demonstrates the representational efficiency and power of supervised\nshallow models with high-order feature interactions.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 17:54:40 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Min", "Martin Renqiang", ""], ["Guo", "Hongyu", ""], ["Song", "Dongjin", ""]]}, {"id": "1608.04700", "submitter": "Antoine Zambelli", "authors": "Antoine Zambelli", "title": "A Data-Driven Approach to Estimating the Number of Clusters in\n  Hierarchical Clustering", "comments": "6 pages, 7 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two new methods for estimating the number of clusters in a\nhierarchical clustering framework in the hopes of creating a fully automated\nprocess with no human intervention. The methods are completely data-driven and\nrequire no input from the researcher, and as such are fully automated. They are\nquite easy to implement and not computationally intensive in the least. We\nanalyze performance on several simulated data sets and the Biobase Gene\nExpression Set, comparing our methods to the established Gap statistic and\nElbow methods and outperforming both in multi-cluster scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 18:35:09 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Zambelli", "Antoine", ""]]}, {"id": "1608.04759", "submitter": "Emmanouil Zampetakis", "authors": "Themistoklis Gouleakis, Christos Tzamos and Manolis Zampetakis", "title": "Faster Sublinear Algorithms using Conditional Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conditional sampling oracle for a probability distribution D returns\nsamples from the conditional distribution of D restricted to a specified subset\nof the domain. A recent line of work (Chakraborty et al. 2013 and Cannone et\nal. 2014) has shown that having access to such a conditional sampling oracle\nrequires only polylogarithmic or even constant number of samples to solve\ndistribution testing problems like identity and uniformity. This significantly\nimproves over the standard sampling model where polynomially many samples are\nnecessary.\n  Inspired by these results, we introduce a computational model based on\nconditional sampling to develop sublinear algorithms with exponentially faster\nruntimes compared to standard sublinear algorithms. We focus on geometric\noptimization problems over points in high dimensional Euclidean space. Access\nto these points is provided via a conditional sampling oracle that takes as\ninput a succinct representation of a subset of the domain and outputs a\nuniformly random point in that subset. We study two well studied problems:\nk-means clustering and estimating the weight of the minimum spanning tree. In\ncontrast to prior algorithms for the classic model, our algorithms have time,\nspace and sample complexity that is polynomial in the dimension and\npolylogarithmic in the number of points.\n  Finally, we comment on the applicability of the model and compare with\nexisting ones like streaming, parallel and distributed computational models.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 20:03:58 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Gouleakis", "Themistoklis", ""], ["Tzamos", "Christos", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "1608.04773", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu and Yuanzhi Li", "title": "Faster Principal Component Regression and Stable Matrix Chebyshev\n  Approximation", "comments": "title changed and minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve principal component regression (PCR), up to a multiplicative\naccuracy $1+\\gamma$, by reducing the problem to $\\tilde{O}(\\gamma^{-1})$\nblack-box calls of ridge regression. Therefore, our algorithm does not require\nany explicit construction of the top principal components, and is suitable for\nlarge-scale PCR instances. In contrast, previous result requires\n$\\tilde{O}(\\gamma^{-2})$ such black-box calls.\n  We obtain this result by developing a general stable recurrence formula for\nmatrix Chebyshev polynomials, and a degree-optimal polynomial approximation to\nthe matrix sign function. Our techniques may be of independent interests,\nespecially when designing iterative methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 20:48:02 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 19:35:38 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1608.04783", "submitter": "Aileme Omogbai Aileme Omogbai", "authors": "Aileme Omogbai", "title": "Application of multiview techniques to NHANES dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disease prediction or classification using health datasets involve using\nwell-known predictors associated with the disease as features for the models.\nThis study considers multiple data components of an individual's health, using\nthe relationship between variables to generate features that may improve the\nperformance of disease classification models. In order to capture information\nfrom different aspects of the data, this project uses a multiview learning\napproach, using Canonical Correlation Analysis (CCA), a technique that finds\nprojections with maximum correlations between two data views. Data categories\ncollected from the NHANES survey (1999-2014) are used as views to learn the\nmultiview representations. The usefulness of the representations is\ndemonstrated by applying them as features in a Diabetes classification task.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 21:20:30 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Omogbai", "Aileme", ""]]}, {"id": "1608.04789", "submitter": "Joshua Peterson", "authors": "Steven Tang, Joshua C. Peterson, Zachary A. Pardos", "title": "Modelling Student Behavior using Granular Large Scale Action Data from a\n  MOOC", "comments": "15 pages, 7 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital learning environments generate a precise record of the actions\nlearners take as they interact with learning materials and complete exercises\ntowards comprehension. With this high quantity of sequential data comes the\npotential to apply time series models to learn about underlying behavioral\npatterns and trends that characterize successful learning based on the granular\nrecord of student actions. There exist several methods for looking at\nlongitudinal, sequential data like those recorded from learning environments.\nIn the field of language modelling, traditional n-gram techniques and modern\nrecurrent neural network (RNN) approaches have been applied to algorithmically\nfind structure in language and predict the next word given the previous words\nin the sentence or paragraph as input. In this paper, we draw an analogy to\nthis work by treating student sequences of resource views and interactions in a\nMOOC as the inputs and predicting students' next interaction as outputs. In\nthis study, we train only on students who received a certificate of completion.\nIn doing so, the model could potentially be used for recommendation of\nsequences eventually leading to success, as opposed to perpetuating\nunproductive behavior. Given that the MOOC used in our study had over 3,500\nunique resources, predicting the exact resource that a student will interact\nwith next might appear to be a difficult classification problem. We find that\nsimply following the syllabus (built-in structure of the course) gives on\naverage 23% accuracy in making this prediction, followed by the n-gram method\nwith 70.4%, and RNN based methods with 72.2%. This research lays the ground\nwork for recommendation in a MOOC and other digital learning environments where\nhigh volumes of sequential data exist.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 21:46:48 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Tang", "Steven", ""], ["Peterson", "Joshua C.", ""], ["Pardos", "Zachary A.", ""]]}, {"id": "1608.04802", "submitter": "Elad Eban", "authors": "Elad ET. Eban, Mariano Schain, Alan Mackey, Ariel Gordon, Rif A.\n  Saurous, Gal Elidan", "title": "Scalable Learning of Non-Decomposable Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern retrieval systems are often driven by an underlying machine learning\nmodel. The goal of such systems is to identify and possibly rank the few most\nrelevant items for a given query or context. Thus, such systems are typically\nevaluated using a ranking-based performance metric such as the area under the\nprecision-recall curve, the $F_\\beta$ score, precision at fixed recall, etc.\nObviously, it is desirable to train such systems to optimize the metric of\ninterest.\n  In practice, due to the scalability limitations of existing approaches for\noptimizing such objectives, large-scale retrieval systems are instead trained\nto maximize classification accuracy, in the hope that performance as measured\nvia the true objective will also be favorable. In this work we present a\nunified framework that, using straightforward building block bounds, allows for\nhighly scalable optimization of a wide range of ranking-based objectives. We\ndemonstrate the advantage of our approach on several real-life retrieval\nproblems that are significantly larger than those considered in the literature,\nwhile achieving substantial improvement in performance over the\naccuracy-objective baseline.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 23:11:14 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 07:54:51 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Eban", "Elad ET.", ""], ["Schain", "Mariano", ""], ["Mackey", "Alan", ""], ["Gordon", "Ariel", ""], ["Saurous", "Rif A.", ""], ["Elidan", "Gal", ""]]}, {"id": "1608.04830", "submitter": "Truyen Tran", "authors": "Kien Do, Truyen Tran, Dinh Phung and Svetha Venkatesh", "title": "Outlier Detection on Mixed-Type Data: An Energy-based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection amounts to finding data points that differ significantly\nfrom the norm. Classic outlier detection methods are largely designed for\nsingle data type such as continuous or discrete. However, real world data is\nincreasingly heterogeneous, where a data point can have both discrete and\ncontinuous attributes. Handling mixed-type data in a disciplined way remains a\ngreat challenge. In this paper, we propose a new unsupervised outlier detection\nmethod for mixed-type data based on Mixed-variate Restricted Boltzmann Machine\n(Mv.RBM). The Mv.RBM is a principled probabilistic method that models data\ndensity. We propose to use \\emph{free-energy} derived from Mv.RBM as outlier\nscore to detect outliers as those data points lying in low density regions. The\nmethod is fast to learn and compute, is scalable to massive datasets. At the\nsame time, the outlier score is identical to data negative log-density up-to an\nadditive constant. We evaluate the proposed method on synthetic and real-world\ndatasets and demonstrate that (a) a proper handling mixed-types is necessary in\noutlier detection, and (b) free-energy of Mv.RBM is a powerful and efficient\noutlier scoring method, which is highly competitive against state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 01:41:40 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Do", "Kien", ""], ["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1608.04839", "submitter": "Ghassen Jerfel", "authors": "Ghassen Jerfel, Mehmet E. Basbug, Barbara E. Engelhardt", "title": "Dynamic Collaborative Filtering with Compound Poisson Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based collaborative filtering analyzes user-item interactions to infer\nlatent factors that represent user preferences and item characteristics in\norder to predict future interactions. Most collaborative filtering algorithms\nassume that these latent factors are static, although it has been shown that\nuser preferences and item perceptions drift over time. In this paper, we\npropose a conjugate and numerically stable dynamic matrix factorization (DCPF)\nbased on compound Poisson matrix factorization that models the smoothly\ndrifting latent factors using Gamma-Markov chains. We propose a numerically\nstable Gamma chain construction, and then present a stochastic variational\ninference approach to estimate the parameters of our model. We apply our model\nto time-stamped ratings data sets: Netflix, Yelp, and Last.fm, where DCPF\nachieves a higher predictive accuracy than state-of-the-art static and dynamic\nfactorization models.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 02:38:44 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 19:19:24 GMT"}, {"version": "v3", "created": "Tue, 1 Nov 2016 07:23:16 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Jerfel", "Ghassen", ""], ["Basbug", "Mehmet E.", ""], ["Engelhardt", "Barbara E.", ""]]}, {"id": "1608.04846", "submitter": "Po-Hsuan Chen", "authors": "Po-Hsuan Chen, Xia Zhu, Hejia Zhang, Javier S. Turek, Janice Chen,\n  Theodore L. Willke, Uri Hasson, Peter J. Ramadge", "title": "A Convolutional Autoencoder for Multi-Subject fMRI Data Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the most effective way to aggregate multi-subject fMRI data is a\nlong-standing and challenging problem. It is of increasing interest in\ncontemporary fMRI studies of human cognition due to the scarcity of data per\nsubject and the variability of brain anatomy and functional response across\nsubjects. Recent work on latent factor models shows promising results in this\ntask but this approach does not preserve spatial locality in the brain. We\nexamine two ways to combine the ideas of a factor model and a searchlight based\nanalysis to aggregate multi-subject fMRI data while preserving spatial\nlocality. We first do this directly by combining a recent factor method known\nas a shared response model with searchlight analysis. Then we design a\nmulti-view convolutional autoencoder for the same task. Both approaches\npreserve spatial locality and have competitive or better performance compared\nwith standard searchlight analysis and the shared response model applied across\nthe whole brain. We also report a system design to handle the computational\nchallenge of training the convolutional autoencoder.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 03:49:56 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Chen", "Po-Hsuan", ""], ["Zhu", "Xia", ""], ["Zhang", "Hejia", ""], ["Turek", "Javier S.", ""], ["Chen", "Janice", ""], ["Willke", "Theodore L.", ""], ["Hasson", "Uri", ""], ["Ramadge", "Peter J.", ""]]}, {"id": "1608.04872", "submitter": "Bernhard C. Geiger", "authors": "Bernhard C. Geiger, Rana Ali Amjad", "title": "Hard Clusters Maximize Mutual Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate mutual information as a cost function for\nclustering, and show in which cases hard, i.e., deterministic, clusters are\noptimal. Using convexity properties of mutual information, we show that certain\nformulations of the information bottleneck problem are solved by hard clusters.\nSimilarly, hard clusters are optimal for the information-theoretic\nco-clustering problem that deals with simultaneous clustering of two dependent\ndata sets. If both data sets have to be clustered using the same cluster\nassignment, hard clusters are not optimal in general. We point at interesting\nand practically relevant special cases of this so-called pairwise clustering\nproblem, for which we can either prove or have evidence that hard clusters are\noptimal. Our results thus show that one can relax the otherwise combinatorial\nhard clustering problem to a real-valued optimization problem with the same\nglobal optimum.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 06:38:35 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Geiger", "Bernhard C.", ""], ["Amjad", "Rana Ali", ""]]}, {"id": "1608.04929", "submitter": "Theja Tulabandhula", "authors": "K J Prabuchandran, Tejas Bodas and Theja Tulabandhula", "title": "Reinforcement Learning algorithms for regret minimization in structured\n  Markov Decision Processes", "comments": "An extended abstract appears in AAMAS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent goal in the Reinforcement Learning (RL) framework is to choose a\nsequence of actions or a policy to maximize the reward collected or minimize\nthe regret incurred in a finite time horizon. For several RL problems in\noperation research and optimal control, the optimal policy of the underlying\nMarkov Decision Process (MDP) is characterized by a known structure. The\ncurrent state of the art algorithms do not utilize this known structure of the\noptimal policy while minimizing regret. In this work, we develop new RL\nalgorithms that exploit the structure of the optimal policy to minimize regret.\nNumerical experiments on MDPs with structured optimal policies show that our\nalgorithms have better performance, are easy to implement, have a smaller\nrun-time and require less number of random number generations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 11:35:32 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Prabuchandran", "K J", ""], ["Bodas", "Tejas", ""], ["Tulabandhula", "Theja", ""]]}, {"id": "1608.04980", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "Caglar Gulcehre, Marcin Moczulski, Francesco Visin, Yoshua Bengio", "title": "Mollifying Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimization of deep neural networks can be more challenging than\ntraditional convex optimization problems due to the highly non-convex nature of\nthe loss function, e.g. it can involve pathological landscapes such as\nsaddle-surfaces that can be difficult to escape for algorithms based on simple\ngradient descent. In this paper, we attack the problem of optimization of\nhighly non-convex neural networks by starting with a smoothed -- or\n\\textit{mollified} -- objective function that gradually has a more non-convex\nenergy landscape during the training. Our proposition is inspired by the recent\nstudies in continuation methods: similar to curriculum methods, we begin\nlearning an easier (possibly convex) objective function and let it evolve\nduring the training, until it eventually goes back to being the original,\ndifficult to optimize, objective function. The complexity of the mollified\nnetworks is controlled by a single hyperparameter which is annealed during the\ntraining. We show improvements on various difficult optimization tasks and\nestablish a relationship with recent works on continuation methods for neural\nnetworks and mollifiers.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 14:37:34 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Moczulski", "Marcin", ""], ["Visin", "Francesco", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1608.05001", "submitter": "Fei Hu", "authors": "Fei Hu, Changjiu Pu, Haowei Gao, Mengzi Tang and Li Li", "title": "An image compression and encryption scheme based on deep learning", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stacked Auto-Encoder (SAE) is a kind of deep learning algorithm for\nunsupervised learning. Which has multi layers that project the vector\nrepresentation of input data into a lower vector space. These projection\nvectors are dense representations of the input data. As a result, SAE can be\nused for image compression. Using chaotic logistic map, the compression ones\ncan further be encrypted. In this study, an application of image compression\nand encryption is suggested using SAE and chaotic logistic map. Experiments\nshow that this application is feasible and effective. It can be used for image\ntransmission and image protection on internet simultaneously.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 14:51:25 GMT"}, {"version": "v2", "created": "Sun, 9 Oct 2016 02:27:20 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Hu", "Fei", ""], ["Pu", "Changjiu", ""], ["Gao", "Haowei", ""], ["Tang", "Mengzi", ""], ["Li", "Li", ""]]}, {"id": "1608.05081", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton, Xiujun Li, Jianfeng Gao, Lihong Li, Faisal Ahmed,\n  Li Deng", "title": "BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for\n  Task-Oriented Dialogue Systems", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm that significantly improves the efficiency of\nexploration for deep Q-learning agents in dialogue systems. Our agents explore\nvia Thompson sampling, drawing Monte Carlo samples from a Bayes-by-Backprop\nneural network. Our algorithm learns much faster than common exploration\nstrategies such as $\\epsilon$-greedy, Boltzmann, bootstrapping, and\nintrinsic-reward-based ones. Additionally, we show that spiking the replay\nbuffer with experiences from just a few successful episodes can make Q-learning\nfeasible when it might otherwise fail.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 20:00:04 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 18:20:55 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 18:01:04 GMT"}, {"version": "v4", "created": "Thu, 23 Nov 2017 10:24:17 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Lipton", "Zachary C.", ""], ["Li", "Xiujun", ""], ["Gao", "Jianfeng", ""], ["Li", "Lihong", ""], ["Ahmed", "Faisal", ""], ["Deng", "Li", ""]]}, {"id": "1608.05127", "submitter": "Vikas Chawla", "authors": "Vikas Chawla, Hsiang Sing Naik, Adedotun Akintayo, Dermot Hayes,\n  Patrick Schnable, Baskar Ganapathysubramanian, Soumik Sarkar", "title": "A Bayesian Network approach to County-Level Corn Yield Prediction using\n  historical data and expert knowledge", "comments": "8 pages, In Proceedings of the 22nd ACM SIGKDD Workshop on Data\n  Science for Food, Energy and Water , 2016 (San Francisco, CA, USA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crop yield forecasting is the methodology of predicting crop yields prior to\nharvest. The availability of accurate yield prediction frameworks have enormous\nimplications from multiple standpoints, including impact on the crop commodity\nfutures markets, formulation of agricultural policy, as well as crop insurance\nrating. The focus of this work is to construct a corn yield predictor at the\ncounty scale. Corn yield (forecasting) depends on a complex, interconnected set\nof variables that include economic, agricultural, management and meteorological\nfactors. Conventional forecasting is either knowledge-based computer programs\n(that simulate plant-weather-soil-management interactions) coupled with\ntargeted surveys or statistical model based. The former is limited by the need\nfor painstaking calibration, while the latter is limited to univariate analysis\nor similar simplifying assumptions that fail to capture the complex\ninterdependencies affecting yield. In this paper, we propose a data-driven\napproach that is \"gray box\" i.e. that seamlessly utilizes expert knowledge in\nconstructing a statistical network model for corn yield forecasting. Our\nmultivariate gray box model is developed on Bayesian network analysis to build\na Directed Acyclic Graph (DAG) between predictors and yield. Starting from a\ncomplete graph connecting various carefully chosen variables and yield, expert\nknowledge is used to prune or strengthen edges connecting variables.\nSubsequently the structure (connectivity and edge weights) of the DAG that\nmaximizes the likelihood of observing the training data is identified via\noptimization. We curated an extensive set of historical data (1948-2012) for\neach of the 99 counties in Iowa as data to train the model.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 23:30:04 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Chawla", "Vikas", ""], ["Naik", "Hsiang Sing", ""], ["Akintayo", "Adedotun", ""], ["Hayes", "Dermot", ""], ["Schnable", "Patrick", ""], ["Ganapathysubramanian", "Baskar", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1608.05152", "submitter": "Brendan Juba", "authors": "Brendan Juba", "title": "Conditional Sparse Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning and statistics typically focus on building models that\ncapture the vast majority of the data, possibly ignoring a small subset of data\nas \"noise\" or \"outliers.\" By contrast, here we consider the problem of jointly\nidentifying a significant (but perhaps small) segment of a population in which\nthere is a highly sparse linear regression fit, together with the coefficients\nfor the linear fit. We contend that such tasks are of interest both because the\nmodels themselves may be able to achieve better predictions in such special\ncases, but also because they may aid our understanding of the data. We give\nalgorithms for such problems under the sup norm, when this unknown segment of\nthe population is described by a k-DNF condition and the regression fit is\ns-sparse for constant k and s. For the variants of this problem when the\nregression fit is not so sparse or using expected error, we also give a\npreliminary algorithm and highlight the question as a challenge for future\nwork.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 01:30:49 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Juba", "Brendan", ""]]}, {"id": "1608.05182", "submitter": "Yanbo Xu", "authors": "Yanbo Xu, Yanxun Xu and Suchi Saria", "title": "A Bayesian Nonparametric Approach for Estimating Individualized\n  Treatment-Response Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the continuous response over time to\ninterventions using observational time series---a retrospective dataset where\nthe policy by which the data are generated is unknown to the learner. We are\nmotivated by applications where response varies by individuals and therefore,\nestimating responses at the individual-level is valuable for personalizing\ndecision-making. We refer to this as the problem of estimating individualized\ntreatment response (ITR) curves. In statistics, G-computation formula (Robins,\n1986) has been commonly used for estimating treatment responses from\nobservational data containing sequential treatment assignments. However, past\nstudies have focused predominantly on obtaining point-in-time estimates at the\npopulation level. We leverage the G-computation formula and develop a novel\nBayesian nonparametric (BNP) method that can flexibly model functional data and\nprovide posterior inference over the treatment response curves at both the\nindividual and population level. On a challenging dataset containing time\nseries from patients admitted to a hospital, we estimate responses to\ntreatments used in managing kidney function and show that the resulting fits\nare more accurate than alternative approaches. Accurate methods for obtaining\nITRs from observational data can dramatically accelerate the pace at which\npersonalized treatment plans become possible.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 05:31:53 GMT"}, {"version": "v2", "created": "Sat, 10 Dec 2016 16:44:14 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Xu", "Yanbo", ""], ["Xu", "Yanxun", ""], ["Saria", "Suchi", ""]]}, {"id": "1608.05225", "submitter": "Joachim van der Herten", "authors": "Joachim van der Herten and Ivo Couckuyt and Dirk Deschrijver and Tom\n  Dhaene", "title": "Active Learning for Approximation of Expensive Functions with Normal\n  Distributed Output Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When approximating a black-box function, sampling with active learning\nfocussing on regions with non-linear responses tends to improve accuracy. We\npresent the FLOLA-Voronoi method introduced previously for deterministic\nresponses, and theoretically derive the impact of output uncertainty. The\nalgorithm automatically puts more emphasis on exploration to provide more\ninformation to the models.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 10:15:54 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["van der Herten", "Joachim", ""], ["Couckuyt", "Ivo", ""], ["Deschrijver", "Dirk", ""], ["Dhaene", "Tom", ""]]}, {"id": "1608.05258", "submitter": "Tatiana Shpakova", "authors": "Tatiana Shpakova and Francis Bach", "title": "Parameter Learning for Log-supermodular Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider log-supermodular models on binary variables, which are\nprobabilistic models with negative log-densities which are submodular. These\nmodels provide probabilistic interpretations of common combinatorial\noptimization tasks such as image segmentation. In this paper, we focus\nprimarily on parameter estimation in the models from known upper-bounds on the\nintractable log-partition function. We show that the bound based on separable\noptimization on the base polytope of the submodular function is always inferior\nto a bound based on \"perturb-and-MAP\" ideas. Then, to learn parameters, given\nthat our approximation of the log-partition function is an expectation (over\nour own randomization), we use a stochastic subgradient technique to maximize a\nlower-bound on the log-likelihood. This can also be extended to conditional\nmaximum likelihood. We illustrate our new results in a set of experiments in\nbinary image denoising, where we highlight the flexibility of a probabilistic\nmodel to learn with missing data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 13:55:41 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Shpakova", "Tatiana", ""], ["Bach", "Francis", ""]]}, {"id": "1608.05275", "submitter": "Elad Mezuman", "authors": "Elad Mezuman and Yair Weiss", "title": "A Tight Convex Upper Bound on the Likelihood of a Finite Mixture", "comments": "icpr 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The likelihood function of a finite mixture model is a non-convex function\nwith multiple local maxima and commonly used iterative algorithms such as EM\nwill converge to different solutions depending on initial conditions. In this\npaper we ask: is it possible to assess how far we are from the global maximum\nof the likelihood? Since the likelihood of a finite mixture model can grow\nunboundedly by centering a Gaussian on a single datapoint and shrinking the\ncovariance, we constrain the problem by assuming that the parameters of the\nindividual models are members of a large discrete set (e.g. estimating a\nmixture of two Gaussians where the means and variances of both Gaussians are\nmembers of a set of a million possible means and variances). For this setting\nwe show that a simple upper bound on the likelihood can be computed using\nconvex optimization and we analyze conditions under which the bound is\nguaranteed to be tight. This bound can then be used to assess the quality of\nsolutions found by EM (where the final result is projected on the discrete set)\nor any other mixture estimation algorithm. For any dataset our method allows us\nto find a finite mixture model together with a dataset-specific bound on how\nfar the likelihood of this mixture is from the global optimum of the likelihood\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 14:27:45 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Mezuman", "Elad", ""], ["Weiss", "Yair", ""]]}, {"id": "1608.05277", "submitter": "Lambert Schomaker", "authors": "Lambert Schomaker", "title": "Caveats on Bayesian and hidden-Markov models (v2.8)", "comments": "Difference of v2.8 with v2.7: a) Final empirical (simulation) table\n  for word-trie experiment with epsilon-in-the-probabilities; b) Some small\n  text changes; c) used ispell", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a number of fundamental and practical problems in the\napplication of hidden-Markov models and Bayes when applied to cursive-script\nrecognition. Several problems, however, will have an effect in other\napplication areas. The most fundamental problem is the propagation of error in\nthe product of probabilities. This is a common and pervasive problem which\ndeserves more attention. On the basis of Monte Carlo modeling, tables for the\nexpected relative error are given. It seems that it is distributed according to\na continuous Poisson distribution over log probabilities. A second essential\nproblem is related to the appropriateness of the Markov assumption. Basic tests\nwill reveal whether a problem requires modeling of the stochastics of\nseriality, at all. Examples are given of lexical encodings which cover 95-99%\nclassification accuracy of a lexicon, with removed sequence information, for\nseveral European languages. Finally, a summary of results on a non- Bayes,\nnon-Markov method in handwriting recognition are presented, with very\nacceptable results and minimal modeling or training requirements using\nnearest-mean classification.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 14:32:04 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 17:21:36 GMT"}, {"version": "v3", "created": "Tue, 3 Jan 2017 12:31:30 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Schomaker", "Lambert", ""]]}, {"id": "1608.05343", "submitter": "Max Jaderberg", "authors": "Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol\n  Vinyals, Alex Graves, David Silver, Koray Kavukcuoglu", "title": "Decoupled Neural Interfaces using Synthetic Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training directed neural networks typically requires forward-propagating data\nthrough a computation graph, followed by backpropagating error signal, to\nproduce weight updates. All layers, or more generally, modules, of the network\nare therefore locked, in the sense that they must wait for the remainder of the\nnetwork to execute forwards and propagate error backwards before they can be\nupdated. In this work we break this constraint by decoupling modules by\nintroducing a model of the future computation of the network graph. These\nmodels predict what the result of the modelled subgraph will produce using only\nlocal information. In particular we focus on modelling error gradients: by\nusing the modelled synthetic gradient in place of true backpropagated error\ngradients we decouple subgraphs, and can update them independently and\nasynchronously i.e. we realise decoupled neural interfaces. We show results for\nfeed-forward models, where every layer is trained asynchronously, recurrent\nneural networks (RNNs) where predicting one's future gradient extends the time\nover which the RNN can effectively model, and also a hierarchical RNN system\nwith ticking at different timescales. Finally, we demonstrate that in addition\nto predicting gradients, the same framework can be used to predict inputs,\nresulting in models which are decoupled in both the forward and backwards pass\n-- amounting to independent networks which co-learn such that they can be\ncomposed into a single functioning corporation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 17:29:09 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 10:52:04 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Jaderberg", "Max", ""], ["Czarnecki", "Wojciech Marian", ""], ["Osindero", "Simon", ""], ["Vinyals", "Oriol", ""], ["Graves", "Alex", ""], ["Silver", "David", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1608.05347", "submitter": "Feras Saad", "authors": "Feras Saad, Vikash Mansinghka", "title": "Probabilistic Data Analysis with Probabilistic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic techniques are central to data analysis, but different\napproaches can be difficult to apply, combine, and compare. This paper\nintroduces composable generative population models (CGPMs), a computational\nabstraction that extends directed graphical models and can be used to describe\nand compose a broad class of probabilistic data analysis techniques. Examples\ninclude hierarchical Bayesian models, multivariate kernel methods,\ndiscriminative machine learning, clustering algorithms, dimensionality\nreduction, and arbitrary probabilistic programs. We also demonstrate the\nintegration of CGPMs into BayesDB, a probabilistic programming platform that\ncan express data analysis tasks using a modeling language and a structured\nquery language. The practical value is illustrated in two ways. First, CGPMs\nare used in an analysis that identifies satellite data records which probably\nviolate Kepler's Third Law, by composing causal probabilistic programs with\nnon-parametric Bayes in under 50 lines of probabilistic code. Second, for\nseveral representative data analysis tasks, we report on lines of code and\naccuracy measurements of various CGPMs, plus comparisons with standard baseline\nsolutions from Python and MATLAB libraries.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 17:47:53 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Saad", "Feras", ""], ["Mansinghka", "Vikash", ""]]}, {"id": "1608.05401", "submitter": "Shripad Gade", "authors": "Shripad Gade and Nitin H. Vaidya", "title": "Distributed Optimization of Convex Sum of Non-Convex Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a distributed solution to optimizing a convex function composed of\nseveral non-convex functions. Each non-convex function is privately stored with\nan agent while the agents communicate with neighbors to form a network. We show\nthat coupled consensus and projected gradient descent algorithm proposed in [1]\ncan optimize convex sum of non-convex functions under an additional assumption\non gradient Lipschitzness. We further discuss the applications of this analysis\nin improving privacy in distributed optimization.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 19:57:41 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Gade", "Shripad", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1608.05560", "submitter": "Yang Wang", "authors": "Yang Wang, Wenjie Zhang, Lin Wu, Xuemin Lin, Meng Fang, Shirui Pan", "title": "Iterative Views Agreement: An Iterative Low-Rank based Structured\n  Optimization Method to Multi-View Spectral Clustering", "comments": "Accepted to appear in IJCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view spectral clustering, which aims at yielding an agreement or\nconsensus data objects grouping across multi-views with their graph laplacian\nmatrices, is a fundamental clustering problem. Among the existing methods,\nLow-Rank Representation (LRR) based method is quite superior in terms of its\neffectiveness, intuitiveness and robustness to noise corruptions. However, it\naggressively tries to learn a common low-dimensional subspace for multi-view\ndata, while inattentively ignoring the local manifold structure in each view,\nwhich is critically important to the spectral clustering; worse still, the\nlow-rank minimization is enforced to achieve the data correlation consensus\namong all views, failing to flexibly preserve the local manifold structure for\neach view. In this paper, 1) we propose a multi-graph laplacian regularized LRR\nwith each graph laplacian corresponding to one view to characterize its local\nmanifold structure. 2) Instead of directly enforcing the low-rank minimization\namong all views for correlation consensus, we separately impose low-rank\nconstraint on each view, coupled with a mutual structural consensus constraint,\nwhere it is able to not only well preserve the local manifold structure but\nalso serve as a constraint for that from other views, which iteratively makes\nthe views more agreeable. Extensive experiments on real-world multi-view data\nsets demonstrate its superiority.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 10:25:46 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Wang", "Yang", ""], ["Zhang", "Wenjie", ""], ["Wu", "Lin", ""], ["Lin", "Xuemin", ""], ["Fang", "Meng", ""], ["Pan", "Shirui", ""]]}, {"id": "1608.05581", "submitter": "Jean Golay", "authors": "Jean Golay and Mikhail Kanevski", "title": "Unsupervised Feature Selection Based on the Morisita Estimator of\n  Intrinsic Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with a new filter algorithm for selecting the smallest\nsubset of features carrying all the information content of a data set (i.e. for\nremoving redundant features). It is an advanced version of the fractal\ndimension reduction technique, and it relies on the recently introduced\nMorisita estimator of Intrinsic Dimension (ID). Here, the ID is used to\nquantify dependencies between subsets of features, which allows the effective\nprocessing of highly non-linear data. The proposed algorithm is successfully\ntested on simulated and real world case studies. Different levels of sample\nsize and noise are examined along with the variability of the results. In\naddition, a comprehensive procedure based on random forests shows that the data\ndimensionality is significantly reduced by the algorithm without loss of\nrelevant information. And finally, comparisons with benchmark feature selection\ntechniques demonstrate the promising performance of this new filter.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 12:28:21 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 16:02:12 GMT"}, {"version": "v3", "created": "Wed, 14 Dec 2016 15:56:35 GMT"}, {"version": "v4", "created": "Sun, 5 Mar 2017 16:54:00 GMT"}, {"version": "v5", "created": "Fri, 2 Jun 2017 19:07:02 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Golay", "Jean", ""], ["Kanevski", "Mikhail", ""]]}, {"id": "1608.05610", "submitter": "Yevgeny Seldin", "authors": "Niklas Thiemann and Christian Igel and Olivier Wintenberger and\n  Yevgeny Seldin", "title": "A Strongly Quasiconvex PAC-Bayesian Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new PAC-Bayesian bound and a way of constructing a hypothesis\nspace, so that the bound is convex in the posterior distribution and also\nconvex in a trade-off parameter between empirical performance of the posterior\ndistribution and its complexity. The complexity is measured by the\nKullback-Leibler divergence to a prior. We derive an alternating procedure for\nminimizing the bound. We show that the bound can be rewritten as a\none-dimensional function of the trade-off parameter and provide sufficient\nconditions under which the function has a single global minimum. When the\nconditions are satisfied the alternating minimization is guaranteed to converge\nto the global minimum of the bound. We provide experimental results\ndemonstrating that rigorous minimization of the bound is competitive with\ncross-validation in tuning the trade-off between complexity and empirical\nperformance. In all our experiments the trade-off turned to be quasiconvex even\nwhen the sufficient conditions were violated.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 14:21:18 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 09:45:07 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Thiemann", "Niklas", ""], ["Igel", "Christian", ""], ["Wintenberger", "Olivier", ""], ["Seldin", "Yevgeny", ""]]}, {"id": "1608.05639", "submitter": "Minh Ha Quang", "authors": "Ha Quang Minh", "title": "Operator-Valued Bochner Theorem, Fourier Feature Maps for\n  Operator-Valued Kernels, and Vector-Valued Learning", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework for computing random operator-valued feature\nmaps for operator-valued positive definite kernels. This is a generalization of\nthe random Fourier features for scalar-valued kernels to the operator-valued\ncase. Our general setting is that of operator-valued kernels corresponding to\nRKHS of functions with values in a Hilbert space. We show that in general, for\na given kernel, there are potentially infinitely many random feature maps,\nwhich can be bounded or unbounded. Most importantly, given a kernel, we present\na general, closed form formula for computing a corresponding probability\nmeasure, which is required for the construction of the Fourier features, and\nwhich, unlike the scalar case, is not uniquely and automatically determined by\nthe kernel. We also show that, under appropriate conditions, random bounded\nfeature maps can always be computed. Furthermore, we show the uniform\nconvergence, under the Hilbert-Schmidt norm, of the resulting approximate\nkernel to the exact kernel on any compact subset of Euclidean space. Our\nconvergence requires differentiable kernels, an improvement over the\ntwice-differentiability requirement in previous work in the scalar setting. We\nthen show how operator-valued feature maps and their approximations can be\nemployed in a general vector-valued learning framework. The mathematical\nformulation is illustrated by numerical examples on matrix-valued kernels.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 15:34:43 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Minh", "Ha Quang", ""]]}, {"id": "1608.05745", "submitter": "Edward Choi", "authors": "Edward Choi, Mohammad Taha Bahadori, Joshua A. Kulas, Andy Schuetz,\n  Walter F. Stewart, Jimeng Sun", "title": "RETAIN: An Interpretable Predictive Model for Healthcare using Reverse\n  Time Attention Mechanism", "comments": "Accepted at Neural Information Processing Systems (NIPS) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accuracy and interpretability are two dominant features of successful\npredictive models. Typically, a choice must be made in favor of complex black\nbox models such as recurrent neural networks (RNN) for accuracy versus less\naccurate but more interpretable traditional models such as logistic regression.\nThis tradeoff poses challenges in medicine where both accuracy and\ninterpretability are important. We addressed this challenge by developing the\nREverse Time AttentIoN model (RETAIN) for application to Electronic Health\nRecords (EHR) data. RETAIN achieves high accuracy while remaining clinically\ninterpretable and is based on a two-level neural attention model that detects\ninfluential past visits and significant clinical variables within those visits\n(e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR\ndata in a reverse time order so that recent clinical visits are likely to\nreceive higher attention. RETAIN was tested on a large health system EHR\ndataset with 14 million visits completed by 263K patients over an 8 year period\nand demonstrated predictive accuracy and computational scalability comparable\nto state-of-the-art methods such as RNN, and ease of interpretability\ncomparable to traditional models.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 21:54:46 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 06:03:43 GMT"}, {"version": "v3", "created": "Wed, 14 Sep 2016 19:45:03 GMT"}, {"version": "v4", "created": "Sun, 26 Feb 2017 15:13:31 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Choi", "Edward", ""], ["Bahadori", "Mohammad Taha", ""], ["Kulas", "Joshua A.", ""], ["Schuetz", "Andy", ""], ["Stewart", "Walter F.", ""], ["Sun", "Jimeng", ""]]}, {"id": "1608.05749", "submitter": "Xinyang Yi", "authors": "Xinyang Yi, Constantine Caramanis, Sujay Sanghavi", "title": "Solving a Mixture of Many Random Linear Equations by Tensor\n  Decomposition and Alternating Minimization", "comments": "39 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of solving mixed random linear equations with $k$\ncomponents. This is the noiseless setting of mixed linear regression. The goal\nis to estimate multiple linear models from mixed samples in the case where the\nlabels (which sample corresponds to which model) are not observed. We give a\ntractable algorithm for the mixed linear equation problem, and show that under\nsome technical conditions, our algorithm is guaranteed to solve the problem\nexactly with sample complexity linear in the dimension, and polynomial in $k$,\nthe number of components. Previous approaches have required either exponential\ndependence on $k$, or super-linear dependence on the dimension. The proposed\nalgorithm is a combination of tensor decomposition and alternating\nminimization. Our analysis involves proving that the initialization provided by\nthe tensor method allows alternating minimization, which is equivalent to EM in\nour setting, to converge to the global optimum at a linear rate.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 22:10:46 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Yi", "Xinyang", ""], ["Caramanis", "Constantine", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1608.05754", "submitter": "Shashanka Ubaru", "authors": "Shashanka Ubaru, Yousef Saad, Abd-Krim Seghouane", "title": "Fast estimation of approximate matrix ranks using spectral densities", "comments": null, "journal-ref": "Neural Computation, Vol. 29, No. 5, pp. 1317-1351 (May 2017)", "doi": "10.1162/NECO_a_00951", "report-no": null, "categories": "cs.NA cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many machine learning and data related applications, it is required to\nhave the knowledge of approximate ranks of large data matrices at hand. In this\npaper, we present two computationally inexpensive techniques to estimate the\napproximate ranks of such large matrices. These techniques exploit approximate\nspectral densities, popular in physics, which are probability density\ndistributions that measure the likelihood of finding eigenvalues of the matrix\nat a given point on the real line. Integrating the spectral density over an\ninterval gives the eigenvalue count of the matrix in that interval. Therefore\nthe rank can be approximated by integrating the spectral density over a\ncarefully selected interval. Two different approaches are discussed to estimate\nthe approximate rank, one based on Chebyshev polynomials and the other based on\nthe Lanczos algorithm. In order to obtain the appropriate interval, it is\nnecessary to locate a gap between the eigenvalues that correspond to noise and\nthe relevant eigenvalues that contribute to the matrix rank. A method for\nlocating this gap and selecting the interval of integration is proposed based\non the plot of the spectral density. Numerical experiments illustrate the\nperformance of these techniques on matrices from typical applications.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 23:07:08 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Ubaru", "Shashanka", ""], ["Saad", "Yousef", ""], ["Seghouane", "Abd-Krim", ""]]}, {"id": "1608.05812", "submitter": "Suleiman Yerima", "authors": "Suleiman Y. Yerima, Sakir Sezer, Gavin McWilliams", "title": "Analysis of Bayesian Classification based Approaches for Android Malware\n  Detection", "comments": "arXiv admin note: text overlap with arXiv:1608.00848", "journal-ref": "IET Information Security, Volume 8, Issue 1, January 2014, pp.\n  25-36, Print ISSN 1751-8709, Online ISSN 1751-8717", "doi": "10.1049/iet-ifs.2013.0095", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile malware has been growing in scale and complexity spurred by the\nunabated uptake of smartphones worldwide. Android is fast becoming the most\npopular mobile platform resulting in sharp increase in malware targeting the\nplatform. Additionally, Android malware is evolving rapidly to evade detection\nby traditional signature-based scanning. Despite current detection measures in\nplace, timely discovery of new malware is still a critical issue. This calls\nfor novel approaches to mitigate the growing threat of zero-day Android\nmalware. Hence, in this paper we develop and analyze proactive Machine Learning\napproaches based on Bayesian classification aimed at uncovering unknown Android\nmalware via static analysis. The study, which is based on a large malware\nsample set of majority of the existing families, demonstrates detection\ncapabilities with high accuracy. Empirical results and comparative analysis are\npresented offering useful insight towards development of effective\nstatic-analytic Bayesian classification based solutions for detecting unknown\nAndroid malware.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 12:10:49 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Yerima", "Suleiman Y.", ""], ["Sezer", "Sakir", ""], ["McWilliams", "Gavin", ""]]}, {"id": "1608.05889", "submitter": "Jing Wang", "authors": "Jing Wang and Meng Wang and Peipei Li and Luoqi Liu and Zhongqiu Zhao\n  and Xuegang Hu and Xindong Wu", "title": "Online Feature Selection with Group Structure Analysis", "comments": "IEEE Transactions on Knowledge and Data Engineering,2015", "journal-ref": null, "doi": "10.1109/TKDE.2015.2441716", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online selection of dynamic features has attracted intensive interest in\nrecent years. However, existing online feature selection methods evaluate\nfeatures individually and ignore the underlying structure of feature stream.\nFor instance, in image analysis, features are generated in groups which\nrepresent color, texture and other visual information. Simply breaking the\ngroup structure in feature selection may degrade performance. Motivated by this\nfact, we formulate the problem as an online group feature selection. The\nproblem assumes that features are generated individually but there are group\nstructure in the feature stream. To the best of our knowledge, this is the\nfirst time that the correlation among feature stream has been considered in the\nonline feature selection process. To solve this problem, we develop a novel\nonline group feature selection method named OGFS. Our proposed approach\nconsists of two stages: online intra-group selection and online inter-group\nselection. In the intra-group selection, we design a criterion based on\nspectral analysis to select discriminative features in each group. In the\ninter-group selection, we utilize a linear regression model to select an\noptimal subset. This two-stage procedure continues until there are no more\nfeatures arriving or some predefined stopping conditions are met. %Our method\nhas been applied Finally, we apply our method to multiple tasks including image\nclassification %, face verification and face verification. Extensive empirical\nstudies performed on real-world and benchmark data sets demonstrate that our\nmethod outperforms other state-of-the-art online feature selection %method\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 02:39:48 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Wang", "Jing", ""], ["Wang", "Meng", ""], ["Li", "Peipei", ""], ["Liu", "Luoqi", ""], ["Zhao", "Zhongqiu", ""], ["Hu", "Xuegang", ""], ["Wu", "Xindong", ""]]}, {"id": "1608.05921", "submitter": "Dongwoo Kim", "authors": "Dongwoo Kim, Lexing Xie, Cheng Soon Ong", "title": "Probabilistic Knowledge Graph Construction: Compositional and\n  Incremental Approaches", "comments": "The 25th ACM International Conference on Information and Knowledge\n  Management (CIKM 2016)", "journal-ref": null, "doi": "10.1145/2983323.2983677", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph construction consists of two tasks: extracting information\nfrom external resources (knowledge population) and inferring missing\ninformation through a statistical analysis on the extracted information\n(knowledge completion). In many cases, insufficient external resources in the\nknowledge population hinder the subsequent statistical inference. The gap\nbetween these two processes can be reduced by an incremental population\napproach. We propose a new probabilistic knowledge graph factorisation method\nthat benefits from the path structure of existing knowledge (e.g. syllogism)\nand enables a common modelling approach to be used for both incremental\npopulation and knowledge completion tasks. More specifically, the probabilistic\nformulation allows us to develop an incremental population algorithm that\ntrades off exploitation-exploration. Experiments on three benchmark datasets\nshow that the balanced exploitation-exploration helps the incremental\npopulation, and the additional path structure helps to predict missing\ninformation in knowledge completion.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 11:49:53 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 04:52:33 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Kim", "Dongwoo", ""], ["Xie", "Lexing", ""], ["Ong", "Cheng Soon", ""]]}, {"id": "1608.05949", "submitter": "Dhananjay Kimothi", "authors": "Dhananjay Kimothi, Akshay Soni, Pravesh Biyani, James M. Hogan", "title": "Distributed Representations for Biological Sequence Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological sequence comparison is a key step in inferring the relatedness of\nvarious organisms and the functional similarity of their components. Thanks to\nthe Next Generation Sequencing efforts, an abundance of sequence data is now\navailable to be processed for a range of bioinformatics applications. Embedding\na biological sequence over a nucleotide or amino acid alphabet in a lower\ndimensional vector space makes the data more amenable for use by current\nmachine learning tools, provided the quality of embedding is high and it\ncaptures the most meaningful information of the original sequences. Motivated\nby recent advances in the text document embedding literature, we present a new\nmethod, called seq2vec, to represent a complete biological sequence in an\nEuclidean space. The new representation has the potential to capture the\ncontextual information of the original sequence necessary for sequence\ncomparison tasks. We test our embeddings with protein sequence classification\nand retrieval tasks and demonstrate encouraging outcomes.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 14:58:01 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 07:54:51 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Kimothi", "Dhananjay", ""], ["Soni", "Akshay", ""], ["Biyani", "Pravesh", ""], ["Hogan", "James M.", ""]]}, {"id": "1608.05983", "submitter": "Ian Gemp", "authors": "Ian Gemp, Ishan Durugkar, Mario Parente, M. Darby Dyar, Sridhar\n  Mahadevan", "title": "Inverting Variational Autoencoders for Improved Generative Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in semi-supervised learning with deep generative models have\nshown promise in generalizing from small labeled datasets\n($\\mathbf{x},\\mathbf{y}$) to large unlabeled ones ($\\mathbf{x}$). In the case\nwhere the codomain has known structure, a large unfeatured dataset\n($\\mathbf{y}$) is potentially available. We develop a parameter-efficient, deep\nsemi-supervised generative model for the purpose of exploiting this untapped\ndata source. Empirical results show improved performance in disentangling\nlatent variable semantics as well as improved discriminative prediction on\nMartian spectroscopic and handwritten digit domains.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 19:02:27 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 14:20:27 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Gemp", "Ian", ""], ["Durugkar", "Ishan", ""], ["Parente", "Mario", ""], ["Dyar", "M. Darby", ""], ["Mahadevan", "Sridhar", ""]]}, {"id": "1608.05995", "submitter": "Ming Lin", "authors": "Ming Lin and Jieping Ye", "title": "A Non-convex One-Pass Framework for Generalized Factorization Machine\n  and Rank-One Matrix Sensing", "comments": "accepted by NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an efficient alternating framework for learning a generalized\nversion of Factorization Machine (gFM) on steaming data with provable\nguarantees. When the instances are sampled from $d$ dimensional random Gaussian\nvectors and the target second order coefficient matrix in gFM is of rank $k$,\nour algorithm converges linearly, achieves $O(\\epsilon)$ recovery error after\nretrieving $O(k^{3}d\\log(1/\\epsilon))$ training instances, consumes $O(kd)$\nmemory in one-pass of dataset and only requires matrix-vector product\noperations in each iteration. The key ingredient of our framework is a\nconstruction of an estimation sequence endowed with a so-called Conditionally\nIndependent RIP condition (CI-RIP). As special cases of gFM, our framework can\nbe applied to symmetric or asymmetric rank-one matrix sensing problems, such as\ninductive matrix completion and phase retrieval.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 20:28:29 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 17:54:50 GMT"}, {"version": "v3", "created": "Mon, 12 Sep 2016 21:43:05 GMT"}, {"version": "v4", "created": "Wed, 14 Sep 2016 02:24:22 GMT"}, {"version": "v5", "created": "Tue, 25 Oct 2016 21:23:23 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Lin", "Ming", ""], ["Ye", "Jieping", ""]]}, {"id": "1608.06007", "submitter": "Theodoros Tsiligkaridis", "authors": "Athanasios Tsiligkaridis and Theodoros Tsiligkaridis", "title": "Distributed Probabilistic Bisection Search using Social Learning", "comments": "5 Pages, Accepted to ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel distributed probabilistic bisection algorithm using social\nlearning with application to target localization. Each agent in the network\nfirst constructs a query about the target based on its local information and\nobtains a noisy response. Agents then perform a Bayesian update of their\nbeliefs followed by an averaging of the log beliefs over local neighborhoods.\nThis two stage algorithm consisting of repeated querying and averaging runs\nuntil convergence. We derive bounds on the rate of convergence of the beliefs\nat the correct target location. Numerical simulations show that our method\noutperforms current state of the art methods.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 22:14:48 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 02:17:22 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Tsiligkaridis", "Athanasios", ""], ["Tsiligkaridis", "Theodoros", ""]]}, {"id": "1608.06010", "submitter": "Yun Wang", "authors": "Yun Wang, Xu Chen and Peter J. Ramadge", "title": "Feedback-Controlled Sequential Lasso Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way to solve lasso problems when the dictionary does not fit into\navailable memory is to first screen the dictionary to remove unneeded features.\nPrior research has shown that sequential screening methods offer the greatest\npromise in this endeavor. Most existing work on sequential screening targets\nthe context of tuning parameter selection, where one screens and solves a\nsequence of $N$ lasso problems with a fixed grid of geometrically spaced\nregularization parameters. In contrast, we focus on the scenario where a target\nregularization parameter has already been chosen via cross-validated model\nselection, and we then need to solve many lasso instances using this fixed\nvalue. In this context, we propose and explore a feedback controlled sequential\nscreening scheme. Feedback is used at each iteration to select the next problem\nto be solved. This allows the sequence of problems to be adapted to the\ninstance presented and the number of intermediate problems to be automatically\nselected. We demonstrate our feedback scheme using several datasets including a\ndictionary of approximate size 100,000 by 300,000.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 23:40:56 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 22:52:30 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Wang", "Yun", ""], ["Chen", "Xu", ""], ["Ramadge", "Peter J.", ""]]}, {"id": "1608.06014", "submitter": "Yun Wang", "authors": "Yun Wang and Peter J. Ramadge", "title": "The Symmetry of a Simple Optimization Problem in Lasso Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently dictionary screening has been proposed as an effective way to\nimprove the computational efficiency of solving the lasso problem, which is one\nof the most commonly used method for learning sparse representations. To\naddress today's ever increasing large dataset, effective screening relies on a\ntight region bound on the solution to the dual lasso. Typical region bounds are\nin the form of an intersection of a sphere and multiple half spaces. One way to\ntighten the region bound is using more half spaces, which however, adds to the\noverhead of solving the high dimensional optimization problem in lasso\nscreening. This paper reveals the interesting property that the optimization\nproblem only depends on the projection of features onto the subspace spanned by\nthe normals of the half spaces. This property converts an optimization problem\nin high dimension to much lower dimension, and thus sheds light on reducing the\ncomputation overhead of lasso screening based on tighter region bounds.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 23:48:43 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 22:05:24 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Wang", "Yun", ""], ["Ramadge", "Peter J.", ""]]}, {"id": "1608.06027", "submitter": "Kamil Rocki", "authors": "Kamil M Rocki", "title": "Surprisal-Driven Feedback in Recurrent Networks", "comments": "ICLR 2017 submission, fixed some equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural nets are widely used for predicting temporal data. Their\ninherent deep feedforward structure allows learning complex sequential\npatterns. It is believed that top-down feedback might be an important missing\ningredient which in theory could help disambiguate similar patterns depending\non broader context. In this paper we introduce surprisal-driven recurrent\nnetworks, which take into account past error information when making new\npredictions. This is achieved by continuously monitoring the discrepancy\nbetween most recent predictions and the actual observations. Furthermore, we\nshow that it outperforms other stochastic and fully deterministic approaches on\nenwik8 character level prediction task achieving 1.37 BPC on the test portion\nof the text.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 01:42:45 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 17:26:02 GMT"}, {"version": "v3", "created": "Mon, 5 Sep 2016 04:42:02 GMT"}, {"version": "v4", "created": "Wed, 19 Oct 2016 04:32:46 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Rocki", "Kamil M", ""]]}, {"id": "1608.06031", "submitter": "Mingda Qiao", "authors": "Lijie Chen, Jian Li, Mingda Qiao", "title": "Towards Instance Optimal Bounds for Best Arm Identification", "comments": "Accepted to COLT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical best arm identification (Best-$1$-Arm) problem, we are given\n$n$ stochastic bandit arms, each associated with a reward distribution with an\nunknown mean. We would like to identify the arm with the largest mean with\nprobability at least $1-\\delta$, using as few samples as possible.\nUnderstanding the sample complexity of Best-$1$-Arm has attracted significant\nattention since the last decade. However, the exact sample complexity of the\nproblem is still unknown.\n  Recently, Chen and Li made the gap-entropy conjecture concerning the instance\nsample complexity of Best-$1$-Arm. Given an instance $I$, let $\\mu_{[i]}$ be\nthe $i$th largest mean and $\\Delta_{[i]}=\\mu_{[1]}-\\mu_{[i]}$ be the\ncorresponding gap. $H(I)=\\sum_{i=2}^n\\Delta_{[i]}^{-2}$ is the complexity of\nthe instance. The gap-entropy conjecture states that\n$\\Omega\\left(H(I)\\cdot\\left(\\ln\\delta^{-1}+\\mathsf{Ent}(I)\\right)\\right)$ is an\ninstance lower bound, where $\\mathsf{Ent}(I)$ is an entropy-like term\ndetermined by the gaps, and there is a $\\delta$-correct algorithm for\nBest-$1$-Arm with sample complexity\n$O\\left(H(I)\\cdot\\left(\\ln\\delta^{-1}+\\mathsf{Ent}(I)\\right)+\\Delta_{[2]}^{-2}\\ln\\ln\\Delta_{[2]}^{-1}\\right)$.\nIf the conjecture is true, we would have a complete understanding of the\ninstance-wise sample complexity of Best-$1$-Arm.\n  We make significant progress towards the resolution of the gap-entropy\nconjecture. For the upper bound, we provide a highly nontrivial algorithm which\nrequires \\[O\\left(H(I)\\cdot\\left(\\ln\\delta^{-1}\n+\\mathsf{Ent}(I)\\right)+\\Delta_{[2]}^{-2}\\ln\\ln\\Delta_{[2]}^{-1}\\mathrm{polylog}(n,\\delta^{-1})\\right)\\]\nsamples in expectation. For the lower bound, we show that for any Gaussian\nBest-$1$-Arm instance with gaps of the form $2^{-k}$, any $\\delta$-correct\nmonotone algorithm requires $\\Omega\\left(H(I)\\cdot\\left(\\ln\\delta^{-1} +\n\\mathsf{Ent}(I)\\right)\\right)$ samples in expectation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 02:05:10 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 02:19:52 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Chen", "Lijie", ""], ["Li", "Jian", ""], ["Qiao", "Mingda", ""]]}, {"id": "1608.06048", "submitter": "Ajinkya More", "authors": "Ajinkya More", "title": "Survey of resampling techniques for improving classification performance\n  in unbalanced datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of classification problems need to deal with data imbalance between\nclasses. Often it is desired to have a high recall on the minority class while\nmaintaining a high precision on the majority class. In this paper, we review a\nnumber of resampling techniques proposed in literature to handle unbalanced\ndatasets and study their effect on classification performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 04:27:28 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["More", "Ajinkya", ""]]}, {"id": "1608.06049", "submitter": "Felix Juefei-Xu", "authors": "Felix Juefei-Xu, Vishnu Naresh Boddeti, Marios Savvides", "title": "Local Binary Convolutional Neural Networks", "comments": "To appear in CVPR 2017 as Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose local binary convolution (LBC), an efficient alternative to\nconvolutional layers in standard convolutional neural networks (CNN). The\ndesign principles of LBC are motivated by local binary patterns (LBP). The LBC\nlayer comprises of a set of fixed sparse pre-defined binary convolutional\nfilters that are not updated during the training process, a non-linear\nactivation function and a set of learnable linear weights. The linear weights\ncombine the activated filter responses to approximate the corresponding\nactivated filter responses of a standard convolutional layer. The LBC layer\naffords significant parameter savings, 9x to 169x in the number of learnable\nparameters compared to a standard convolutional layer. Furthermore, the sparse\nand binary nature of the weights also results in up to 9x to 169x savings in\nmodel size compared to a standard convolutional layer. We demonstrate both\ntheoretically and experimentally that our local binary convolution layer is a\ngood approximation of a standard convolutional layer. Empirically, CNNs with\nLBC layers, called local binary convolutional neural networks (LBCNN), achieves\nperformance parity with regular CNNs on a range of visual datasets (MNIST,\nSVHN, CIFAR-10, and ImageNet) while enjoying significant computational savings.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 04:32:21 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 17:02:44 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Juefei-Xu", "Felix", ""], ["Boddeti", "Vishnu Naresh", ""], ["Savvides", "Marios", ""]]}, {"id": "1608.06072", "submitter": "Ibrahim Alabdulmohsin", "authors": "Ibrahim Alabdulmohsin", "title": "Uniform Generalization, Concentration, and Adaptive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One fundamental goal in any learning algorithm is to mitigate its risk for\noverfitting. Mathematically, this requires that the learning algorithm enjoys a\nsmall generalization risk, which is defined either in expectation or in\nprobability. Both types of generalization are commonly used in the literature.\nFor instance, generalization in expectation has been used to analyze\nalgorithms, such as ridge regression and SGD, whereas generalization in\nprobability is used in the VC theory, among others. Recently, a third notion of\ngeneralization has been studied, called uniform generalization, which requires\nthat the generalization risk vanishes uniformly in expectation across all\nbounded parametric losses. It has been shown that uniform generalization is, in\nfact, equivalent to an information-theoretic stability constraint, and that it\nrecovers classical results in learning theory. It is achievable under various\nsettings, such as sample compression schemes, finite hypothesis spaces, finite\ndomains, and differential privacy. However, the relationship between uniform\ngeneralization and concentration remained unknown. In this paper, we answer\nthis question by proving that, while a generalization in expectation does not\nimply a generalization in probability, a uniform generalization in expectation\ndoes imply concentration. We establish a chain rule for the uniform\ngeneralization risk of the composition of hypotheses and use it to derive a\nlarge deviation bound. Finally, we prove that the bound is tight.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 07:47:56 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 16:36:11 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Alabdulmohsin", "Ibrahim", ""]]}, {"id": "1608.06154", "submitter": "Pankaj Malhotra Mr.", "authors": "Pankaj Malhotra, Vishnu TV, Anusha Ramakrishnan, Gaurangi Anand,\n  Lovekesh Vig, Puneet Agarwal, Gautam Shroff", "title": "Multi-Sensor Prognostics using an Unsupervised Health Index based on\n  LSTM Encoder-Decoder", "comments": "Presented at 1st ACM SIGKDD Workshop on Machine Learning for\n  Prognostics and Health Management, San Francisco, CA, USA, 2016. 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many approaches for estimation of Remaining Useful Life (RUL) of a machine,\nusing its operational sensor data, make assumptions about how a system degrades\nor a fault evolves, e.g., exponential degradation. However, in many domains\ndegradation may not follow a pattern. We propose a Long Short Term Memory based\nEncoder-Decoder (LSTM-ED) scheme to obtain an unsupervised health index (HI)\nfor a system using multi-sensor time-series data. LSTM-ED is trained to\nreconstruct the time-series corresponding to healthy state of a system. The\nreconstruction error is used to compute HI which is then used for RUL\nestimation. We evaluate our approach on publicly available Turbofan Engine and\nMilling Machine datasets. We also present results on a real-world industry\ndataset from a pulverizer mill where we find significant correlation between\nLSTM-ED based HI and maintenance costs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 12:59:31 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Malhotra", "Pankaj", ""], ["TV", "Vishnu", ""], ["Ramakrishnan", "Anusha", ""], ["Anand", "Gaurangi", ""], ["Vig", "Lovekesh", ""], ["Agarwal", "Puneet", ""], ["Shroff", "Gautam", ""]]}, {"id": "1608.06203", "submitter": "Sewoong Oh", "authors": "Ashish Khetan, Sewoong Oh", "title": "Computational and Statistical Tradeoffs in Learning to Rank", "comments": "30 pages 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For massive and heterogeneous modern datasets, it is of fundamental interest\nto provide guarantees on the accuracy of estimation when computational\nresources are limited. In the application of learning to rank, we provide a\nhierarchy of rank-breaking mechanisms ordered by the complexity in thus\ngenerated sketch of the data. This allows the number of data points collected\nto be gracefully traded off against computational resources available, while\nguaranteeing the desired level of accuracy. Theoretical guarantees on the\nproposed generalized rank-breaking implicitly provide such trade-offs, which\ncan be explicitly characterized under certain canonical scenarios on the\nstructure of the data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 15:58:31 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Khetan", "Ashish", ""], ["Oh", "Sewoong", ""]]}, {"id": "1608.06235", "submitter": "Yunpeng Pan", "authors": "Yunpeng Pan, Xinyan Yan, Evangelos Theodorou and Byron Boots", "title": "Adaptive Probabilistic Trajectory Optimization via Efficient Approximate\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic systems must be able to quickly and robustly make decisions when\noperating in uncertain and dynamic environments. While Reinforcement Learning\n(RL) can be used to compute optimal policies with little prior knowledge about\nthe environment, it suffers from slow convergence. An alternative approach is\nModel Predictive Control (MPC), which optimizes policies quickly, but also\nrequires accurate models of the system dynamics and environment. In this paper\nwe propose a new approach, adaptive probabilistic trajectory optimization, that\ncombines the benefits of RL and MPC. Our method uses scalable approximate\ninference to learn and updates probabilistic models in an online incremental\nfashion while also computing optimal control policies via successive local\napproximations. We present two variations of our algorithm based on the Sparse\nSpectrum Gaussian Process (SSGP) model, and we test our algorithm on three\nlearning tasks, demonstrating the effectiveness and efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 17:49:50 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 23:11:23 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Pan", "Yunpeng", ""], ["Yan", "Xinyan", ""], ["Theodorou", "Evangelos", ""], ["Boots", "Byron", ""]]}, {"id": "1608.06253", "submitter": "Christina Lioma Assoc. Prof", "authors": "Brian Brost and Yevgeny Seldin and Ingemar J. Cox and Christina Lioma", "title": "Multi-Dueling Bandits and Their Application to Online Ranker Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New ranking algorithms are continually being developed and refined,\nnecessitating the development of efficient methods for evaluating these\nrankers. Online ranker evaluation focuses on the challenge of efficiently\ndetermining, from implicit user feedback, which ranker out of a finite set of\nrankers is the best. Online ranker evaluation can be modeled by dueling ban-\ndits, a mathematical model for online learning under limited feedback from\npairwise comparisons. Comparisons of pairs of rankers is performed by\ninterleaving their result sets and examining which documents users click on.\nThe dueling bandits model addresses the key issue of which pair of rankers to\ncompare at each iteration, thereby providing a solution to the\nexploration-exploitation trade-off. Recently, methods for simultaneously\ncomparing more than two rankers have been developed. However, the question of\nwhich rankers to compare at each iteration was left open. We address this\nquestion by proposing a generalization of the dueling bandits model that uses\nsimultaneous comparisons of an unrestricted number of rankers. We evaluate our\nalgorithm on synthetic data and several standard large-scale online ranker\nevaluation datasets. Our experimental results show that the algorithm yields\norders of magnitude improvement in performance compared to stateof- the-art\ndueling bandit algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 18:20:18 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Brost", "Brian", ""], ["Seldin", "Yevgeny", ""], ["Cox", "Ingemar J.", ""], ["Lioma", "Christina", ""]]}, {"id": "1608.06315", "submitter": "David Sussillo", "authors": "David Sussillo, Rafal Jozefowicz, L. F. Abbott, Chethan Pandarinath", "title": "LFADS - Latent Factor Analysis via Dynamical Systems", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroscience is experiencing a data revolution in which many hundreds or\nthousands of neurons are recorded simultaneously. Currently, there is little\nconsensus on how such data should be analyzed. Here we introduce LFADS (Latent\nFactor Analysis via Dynamical Systems), a method to infer latent dynamics from\nsimultaneously recorded, single-trial, high-dimensional neural spiking data.\nLFADS is a sequential model based on a variational auto-encoder. By making a\ndynamical systems hypothesis regarding the generation of the observed data,\nLFADS reduces observed spiking to a set of low-dimensional temporal factors,\nper-trial initial conditions, and inferred inputs. We compare LFADS to existing\nmethods on synthetic data and show that it significantly out-performs them in\ninferring neural firing rates and latent dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 21:15:00 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Sussillo", "David", ""], ["Jozefowicz", "Rafal", ""], ["Abbott", "L. F.", ""], ["Pandarinath", "Chethan", ""]]}, {"id": "1608.06374", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Thomas S. Huang", "title": "Deep Double Sparsity Encoder: Learning to Sparsify Not Only Features But\n  Also Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper emphasizes the significance to jointly exploit the problem\nstructure and the parameter structure, in the context of deep modeling. As a\nspecific and interesting example, we describe the deep double sparsity encoder\n(DDSE), which is inspired by the double sparsity model for dictionary learning.\nDDSE simultaneously sparsities the output features and the learned model\nparameters, under one unified framework. In addition to its intuitive model\ninterpretation, DDSE also possesses compact model size and low complexity.\nExtensive simulations compare DDSE with several carefully-designed baselines,\nand verify the consistently superior performance of DDSE. We further apply DDSE\nto the novel application domain of brain encoding, with promising preliminary\nresults achieved.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 03:50:01 GMT"}, {"version": "v2", "created": "Sun, 2 Oct 2016 03:01:51 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Wang", "Zhangyang", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1608.06408", "submitter": "Sougata Chaudhuri", "authors": "Sougata Chaudhuri and Ambuj Tewari", "title": "Online Learning to Rank with Top-k Feedback", "comments": "Under review in JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two settings of online learning to rank where feedback is\nrestricted to top ranked items. The problem is cast as an online game between a\nlearner and sequence of users, over $T$ rounds. In both settings, the learners\nobjective is to present ranked list of items to the users. The learner's\nperformance is judged on the entire ranked list and true relevances of the\nitems. However, the learner receives highly restricted feedback at end of each\nround, in form of relevances of only the top $k$ ranked items, where $k \\ll m$.\nThe first setting is \\emph{non-contextual}, where the list of items to be\nranked is fixed. The second setting is \\emph{contextual}, where lists of items\nvary, in form of traditional query-document lists. No stochastic assumption is\nmade on the generation process of relevances of items and contexts. We provide\nefficient ranking strategies for both the settings. The strategies achieve\n$O(T^{2/3})$ regret, where regret is based on popular ranking measures in first\nsetting and ranking surrogates in second setting. We also provide impossibility\nresults for certain ranking measures and a certain class of surrogates, when\nfeedback is restricted to the top ranked item, i.e. $k=1$. We empirically\ndemonstrate the performance of our algorithms on simulated and real world\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 07:40:08 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Chaudhuri", "Sougata", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1608.06409", "submitter": "Timothy O'Shea", "authors": "Timothy J O'Shea, Kiran Karra, T. Charles Clancy", "title": "Learning to Communicate: Channel Auto-encoders, Domain Specific\n  Regularizers, and Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of learning efficient and adaptive ways to communicate\nbinary information over an impaired channel. We treat the problem as\nreconstruction optimization through impairment layers in a channel autoencoder\nand introduce several new domain-specific regularizing layers to emulate common\nchannel impairments. We also apply a radio transformer network based attention\nmodel on the input of the decoder to help recover canonical signal\nrepresentations. We demonstrate some promising initial capacity results from\nthis architecture and address several remaining challenges before such a system\ncould become practical.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 07:41:31 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["O'Shea", "Timothy J", ""], ["Karra", "Kiran", ""], ["Clancy", "T. Charles", ""]]}, {"id": "1608.06581", "submitter": "Robert Adolf", "authors": "Robert Adolf, Saketh Rama, Brandon Reagen, Gu-Yeon Wei, David Brooks", "title": "Fathom: Reference Workloads for Modern Deep Learning Methods", "comments": "Proceedings of the IEEE International Symposium on Workload\n  Characterization, 2016", "journal-ref": null, "doi": "10.1109/IISWC.2016.7581275", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been popularized by its recent successes on challenging\nartificial intelligence problems. One of the reasons for its dominance is also\nan ongoing challenge: the need for immense amounts of computational power.\nHardware architects have responded by proposing a wide array of promising\nideas, but to date, the majority of the work has focused on specific algorithms\nin somewhat narrow application domains. While their specificity does not\ndiminish these approaches, there is a clear need for more flexible solutions.\nWe believe the first step is to examine the characteristics of cutting edge\nmodels from across the deep learning community.\n  Consequently, we have assembled Fathom: a collection of eight archetypal deep\nlearning workloads for study. Each of these models comes from a seminal work in\nthe deep learning community, ranging from the familiar deep convolutional\nneural network of Krizhevsky et al., to the more exotic memory networks from\nFacebook's AI research group. Fathom has been released online, and this paper\nfocuses on understanding the fundamental performance characteristics of each\nmodel. We use a set of application-level modeling tools built around the\nTensorFlow deep learning framework in order to analyze the behavior of the\nFathom workloads. We present a breakdown of where time is spent, the\nsimilarities between the performance profiles of our models, an analysis of\nbehavior in inference and training, and the effects of parallelism on scaling.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 17:11:07 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Adolf", "Robert", ""], ["Rama", "Saketh", ""], ["Reagen", "Brandon", ""], ["Wei", "Gu-Yeon", ""], ["Brooks", "David", ""]]}, {"id": "1608.06602", "submitter": "Burak \\c{C}akmak", "authors": "Burak \\c{C}akmak, Manfred Opper, Bernard H. Fleury and Ole Winther", "title": "Self-Averaging Expectation Propagation", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of approximate Bayesian inference for a general\nclass of observation models by means of the expectation propagation (EP)\nframework for large systems under some statistical assumptions. Our approach\ntries to overcome the numerical bottleneck of EP caused by the inversion of\nlarge matrices. Assuming that the measurement matrices are realizations of\nspecific types of ensembles we use the concept of freeness from random matrix\ntheory to show that the EP cavity variances exhibit an asymptotic\nself-averaging property. They can be pre-computed using specific generating\nfunctions, i.e. the R- and/or S-transforms in free probability, which do not\nrequire matrix inversions. Our approach extends the framework of (generalized)\napproximate message passing -- assumes zero-mean iid entries of the measurement\nmatrix -- to a general class of random matrix ensembles. The generalization is\nvia a simple formulation of the R- and/or S-transforms of the limiting\neigenvalue distribution of the Gramian of the measurement matrix. We\ndemonstrate the performance of our approach on a signal recovery problem of\nnonlinear compressed sensing and compare it with that of EP.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 18:49:03 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["\u00c7akmak", "Burak", ""], ["Opper", "Manfred", ""], ["Fleury", "Bernard H.", ""], ["Winther", "Ole", ""]]}, {"id": "1608.06608", "submitter": "Yang Zhang", "authors": "Yang Zhang, Rupam Acharyya, Ji Liu, Boqing Gong", "title": "Infinite-Label Learning with Semantic Output Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new statistical machine learning paradigm, named infinite-label\nlearning, to annotate a data point with more than one relevant labels from a\ncandidate set, which pools both the finite labels observed at training and a\npotentially infinite number of previously unseen labels. The infinite-label\nlearning fundamentally expands the scope of conventional multi-label learning,\nand better models the practical requirements in various real-world\napplications, such as image tagging, ads-query association, and article\ncategorization. However, how can we learn a labeling function that is capable\nof assigning to a data point the labels omitted from the training set? To\nanswer the question, we seek some clues from the recent work on zero-shot\nlearning, where the key is to represent a class/label by a vector of semantic\ncodes, as opposed to treating them as atomic labels. We validate the\ninfinite-label learning by a PAC bound in theory and some empirical studies on\nboth synthetic and real data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 19:14:47 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 02:47:58 GMT"}, {"version": "v3", "created": "Sat, 21 Oct 2017 00:56:08 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Zhang", "Yang", ""], ["Acharyya", "Rupam", ""], ["Liu", "Ji", ""], ["Gong", "Boqing", ""]]}, {"id": "1608.06651", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel, Maarten de Rijke, Marcel Worring", "title": "Unsupervised, Efficient and Semantic Expertise Retrieval", "comments": "WWW2016, Proceedings of the 25th International Conference on World\n  Wide Web. 2016", "journal-ref": null, "doi": "10.1145/2872427.2882974", "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an unsupervised discriminative model for the task of retrieving\nexperts in online document collections. We exclusively employ textual evidence\nand avoid explicit feature engineering by learning distributed word\nrepresentations in an unsupervised way. We compare our model to\nstate-of-the-art unsupervised statistical vector space and probabilistic\ngenerative approaches. Our proposed log-linear model achieves the retrieval\nperformance levels of state-of-the-art document-centric methods with the low\ninference cost of so-called profile-centric approaches. It yields a\nstatistically significant improved ranking over vector space and generative\nmodels in most cases, matching the performance of supervised methods on various\nbenchmarks. That is, by using solely text we can do as well as methods that\nwork with external evidence and/or relevance feedback. A contrastive analysis\nof rankings produced by discriminative and generative approaches shows that\nthey have complementary strengths due to the ability of the unsupervised\ndiscriminative model to perform semantic matching.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 20:55:09 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 04:57:54 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Van Gysel", "Christophe", ""], ["de Rijke", "Maarten", ""], ["Worring", "Marcel", ""]]}, {"id": "1608.06664", "submitter": "Shih-Chieh Su", "authors": "Shih-Chieh Su, Joseph Vaughn and Jean-Laurent Huynh", "title": "Topic Grids for Homogeneous Data Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the topic grids to detect anomaly and analyze the behavior based\non the access log content. Content-based behavioral risk is quantified in the\nhigh dimensional space where the topics are generated from the log. The topics\nare being projected homogeneously into a space that is perception- and\ninteraction-friendly to the human experts.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 22:44:42 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Su", "Shih-Chieh", ""], ["Vaughn", "Joseph", ""], ["Huynh", "Jean-Laurent", ""]]}, {"id": "1608.06665", "submitter": "Walid Gomaa", "authors": "Mohamed Khamis, Walid Gomaa, Basem Galal", "title": "Deep learning is competing random forest in computational docking", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational docking is the core process of computer-aided drug design; it\naims at predicting the best orientation and conformation of a small drug\nmolecule when bound to a target large protein receptor. The docking quality is\ntypically measured by a scoring function: a mathematical predictive model that\nproduces a score representing the binding free energy and hence the stability\nof the resulting complex molecule. We analyze the performance of both learning\ntechniques on the scoring power, the ranking power, docking power, and\nscreening power using the PDBbind 2013 database. For the scoring and ranking\npowers, the proposed learning scoring functions depend on a wide range of\nfeatures (energy terms, pharmacophore, intermolecular) that entirely\ncharacterize the protein-ligand complexes. For the docking and screening\npowers, the proposed learning scoring functions depend on the intermolecular\nfeatures of the RF-Score to utilize a larger number of training complexes. For\nthe scoring power, the DL\\_RF scoring function achieves Pearson's correlation\ncoefficient between the predicted and experimentally measured binding\naffinities of 0.799 versus 0.758 of the RF scoring function. For the ranking\npower, the DL scoring function ranks the ligands bound to fixed target protein\nwith accuracy 54% for the high-level ranking and with accuracy 78% for the\nlow-level ranking while the RF scoring function achieves (46% and 62%)\nrespectively. For the docking power, the DL\\_RF scoring function has a success\nrate when the three best-scored ligand binding poses are considered within 2\n\\AA\\ root-mean-square-deviation from the native pose of 36.0% versus 30.2% of\nthe RF scoring function. For the screening power, the DL scoring function has\nan average enrichment factor and success rate at the top 1% level of (2.69 and\n6.45%) respectively versus (1.61 and 4.84%) respectively of the RF scoring\nfunction.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 22:52:22 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Khamis", "Mohamed", ""], ["Gomaa", "Walid", ""], ["Galal", "Basem", ""]]}, {"id": "1608.06807", "submitter": "Emanuele Sansone", "authors": "Emanuele Sansone, Francesco G.B. De Natale, Zhi-Hua Zhou", "title": "Efficient Training for Positive Unlabeled Learning", "comments": "Submitted to IEEE TPAMI", "journal-ref": "31 July 2018", "doi": "10.1109/TPAMI.2018.2860995", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positive unlabeled (PU) learning is useful in various practical situations,\nwhere there is a need to learn a classifier for a class of interest from an\nunlabeled data set, which may contain anomalies as well as samples from unknown\nclasses. The learning task can be formulated as an optimization problem under\nthe framework of statistical learning theory. Recent studies have theoretically\nanalyzed its properties and generalization performance, nevertheless, little\neffort has been made to consider the problem of scalability, especially when\nlarge sets of unlabeled data are available. In this work we propose a novel\nscalable PU learning algorithm that is theoretically proven to provide the\noptimal solution, while showing superior computational and memory performance.\nExperimental evaluation confirms the theoretical evidence and shows that the\nproposed method can be successfully applied to a large variety of real-world\nproblems involving PU learning.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 13:38:16 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 20:14:08 GMT"}, {"version": "v3", "created": "Wed, 12 Oct 2016 14:53:15 GMT"}, {"version": "v4", "created": "Wed, 14 Mar 2018 08:04:58 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Sansone", "Emanuele", ""], ["De Natale", "Francesco G. B.", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1608.06863", "submitter": "Victoria Peterson Mrs", "authors": "Victoria Peterson, Hugo Leonardo Rufiner, Ruben Daniel Spies", "title": "Kullback-Leibler Penalized Sparse Discriminant Analysis for\n  Event-Related Potential Classification", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A brain computer interface (BCI) is a system which provides direct\ncommunication between the mind of a person and the outside world by using only\nbrain activity (EEG). The event-related potential (ERP)-based BCI problem\nconsists of a binary pattern recognition. Linear discriminant analysis (LDA) is\nwidely used to solve this type of classification problems, but it fails when\nthe number of features is large relative to the number of observations. In this\nwork we propose a penalized version of the sparse discriminant analysis (SDA),\ncalled Kullback-Leibler penalized sparse discriminant analysis (KLSDA). This\nmethod inherits both the discriminative feature selection and classification\nproperties of SDA and it also improves SDA performance through the addition of\nKullback-Leibler class discrepancy information. The KLSDA method is design to\nautomatically select the optimal regularization parameters. Numerical\nexperiments with two real ERP-EEG datasets show that this new method\noutperforms standard SDA.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 15:32:51 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Peterson", "Victoria", ""], ["Rufiner", "Hugo Leonardo", ""], ["Spies", "Ruben Daniel", ""]]}, {"id": "1608.06879", "submitter": "Sashank J. Reddi", "authors": "Sashank J. Reddi, Jakub Kone\\v{c}n\\'y, Peter Richt\\'arik, Barnab\\'as\n  P\\'ocz\\'os, Alex Smola", "title": "AIDE: Fast and Communication Efficient Distributed Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present two new communication-efficient methods for\ndistributed minimization of an average of functions. The first algorithm is an\ninexact variant of the DANE algorithm that allows any local algorithm to return\nan approximate solution to a local subproblem. We show that such a strategy\ndoes not affect the theoretical guarantees of DANE significantly. In fact, our\napproach can be viewed as a robustification strategy since the method is\nsubstantially better behaved than DANE on data partition arising in practice.\nIt is well known that DANE algorithm does not match the communication\ncomplexity lower bounds. To bridge this gap, we propose an accelerated variant\nof the first method, called AIDE, that not only matches the communication lower\nbounds but can also be implemented using a purely first-order oracle. Our\nempirical results show that AIDE is superior to other communication efficient\nalgorithms in settings that naturally arise in machine learning applications.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 16:04:12 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Reddi", "Sashank J.", ""], ["Kone\u010dn\u00fd", "Jakub", ""], ["Richt\u00e1rik", "Peter", ""], ["P\u00f3cz\u00f3s", "Barnab\u00e1s", ""], ["Smola", "Alex", ""]]}, {"id": "1608.06884", "submitter": "Hao Wang", "authors": "Hao Wang and Dit-Yan Yeung", "title": "Towards Bayesian Deep Learning: A Framework and Some Existing Methods", "comments": "To appear in IEEE Transactions on Knowledge and Data Engineering\n  (TKDE), 2016. This is a slightly shorter version of the survey\n  arXiv:1604.01662", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While perception tasks such as visual object recognition and text\nunderstanding play an important role in human intelligence, the subsequent\ntasks that involve inference, reasoning and planning require an even higher\nlevel of intelligence. The past few years have seen major advances in many\nperception tasks using deep learning models. For higher-level inference,\nhowever, probabilistic graphical models with their Bayesian nature are still\nmore powerful and flexible. To achieve integrated intelligence that involves\nboth perception and inference, it is naturally desirable to tightly integrate\ndeep learning and Bayesian models within a principled probabilistic framework,\nwhich we call Bayesian deep learning. In this unified framework, the perception\nof text or images using deep learning can boost the performance of higher-level\ninference and in return, the feedback from the inference process is able to\nenhance the perception of text or images. This paper proposes a general\nframework for Bayesian deep learning and reviews its recent applications on\nrecommender systems, topic models, and control. In this paper, we also discuss\nthe relationship and differences between Bayesian deep learning and other\nrelated topics like Bayesian treatment of neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 16:15:22 GMT"}, {"version": "v2", "created": "Sat, 3 Sep 2016 15:32:04 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Wang", "Hao", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1608.06984", "submitter": "Max Ren", "authors": "Thurston Sexton and Max Yi Ren", "title": "Learning an Optimization Algorithm through Human Design Iterations", "comments": "accepted to Journal of Mechanical Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving optimal design problems through crowdsourcing faces a dilemma: On one\nhand, human beings have been shown to be more effective than algorithms at\nsearching for good solutions of certain real-world problems with\nhigh-dimensional or discrete solution spaces; on the other hand, the cost of\nsetting up crowdsourcing environments, the uncertainty in the crowd's\ndomain-specific competence, and the lack of commitment of the crowd, all\ncontribute to the lack of real-world application of design crowdsourcing. We\nare thus motivated to investigate a solution-searching mechanism where an\noptimization algorithm is tuned based on human demonstrations on solution\nsearching, so that the search can be continued after human participants abandon\nthe problem. To do so, we model the iterative search process as a Bayesian\nOptimization (BO) algorithm, and propose an inverse BO (IBO) algorithm to find\nthe maximum likelihood estimators of the BO parameters based on human\nsolutions. We show through a vehicle design and control problem that the search\nperformance of BO can be improved by recovering its parameters based on an\neffective human search. Thus, IBO has the potential to improve the success rate\nof design crowdsourcing activities, by requiring only good search strategies\ninstead of good solutions from the crowd.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 23:22:06 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 17:40:37 GMT"}, {"version": "v3", "created": "Tue, 6 Dec 2016 17:30:21 GMT"}, {"version": "v4", "created": "Wed, 26 Apr 2017 18:00:36 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Sexton", "Thurston", ""], ["Ren", "Max Yi", ""]]}, {"id": "1608.06993", "submitter": "Gao Huang", "authors": "Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger", "title": "Densely Connected Convolutional Networks", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that convolutional networks can be substantially\ndeeper, more accurate, and efficient to train if they contain shorter\nconnections between layers close to the input and those close to the output. In\nthis paper, we embrace this observation and introduce the Dense Convolutional\nNetwork (DenseNet), which connects each layer to every other layer in a\nfeed-forward fashion. Whereas traditional convolutional networks with L layers\nhave L connections - one between each layer and its subsequent layer - our\nnetwork has L(L+1)/2 direct connections. For each layer, the feature-maps of\nall preceding layers are used as inputs, and its own feature-maps are used as\ninputs into all subsequent layers. DenseNets have several compelling\nadvantages: they alleviate the vanishing-gradient problem, strengthen feature\npropagation, encourage feature reuse, and substantially reduce the number of\nparameters. We evaluate our proposed architecture on four highly competitive\nobject recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).\nDenseNets obtain significant improvements over the state-of-the-art on most of\nthem, whilst requiring less computation to achieve high performance. Code and\npre-trained models are available at https://github.com/liuzhuang13/DenseNet .\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 00:44:55 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 14:50:55 GMT"}, {"version": "v3", "created": "Sat, 3 Dec 2016 08:35:43 GMT"}, {"version": "v4", "created": "Sun, 27 Aug 2017 02:56:24 GMT"}, {"version": "v5", "created": "Sun, 28 Jan 2018 17:12:02 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Huang", "Gao", ""], ["Liu", "Zhuang", ""], ["van der Maaten", "Laurens", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1608.07001", "submitter": "Yangtao Wang", "authors": "Yangtao Wang, Lihui Chen, Xiaoli Li", "title": "Incremental Minimax Optimization based Fuzzy Clustering for Large\n  Multi-view Data", "comments": "32 pages, 1 figures, submitted to Fuzzy Sets and Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental clustering approaches have been proposed for handling large data\nwhen given data set is too large to be stored. The key idea of these approaches\nis to find representatives to represent each cluster in each data chunk and\nfinal data analysis is carried out based on those identified representatives\nfrom all the chunks. However, most of the incremental approaches are used for\nsingle view data. As large multi-view data generated from multiple sources\nbecomes prevalent nowadays, there is a need for incremental clustering\napproaches to handle both large and multi-view data. In this paper we propose a\nnew incremental clustering approach called incremental minimax optimization\nbased fuzzy clustering (IminimaxFCM) to handle large multi-view data. In\nIminimaxFCM, representatives with multiple views are identified to represent\neach cluster by integrating multiple complementary views using minimax\noptimization. The detailed problem formulation, updating rules derivation, and\nthe in-depth analysis of the proposed IminimaxFCM are provided. Experimental\nstudies on several real world multi-view data sets have been conducted. We\nobserved that IminimaxFCM outperforms related incremental fuzzy clustering in\nterms of clustering accuracy, demonstrating the great potential of IminimaxFCM\nfor large multi-view data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 01:56:20 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Wang", "Yangtao", ""], ["Chen", "Lihui", ""], ["Li", "Xiaoli", ""]]}, {"id": "1608.07005", "submitter": "Yangtao Wang", "authors": "Yangtao Wang, Lihui Chen", "title": "Multi-View Fuzzy Clustering with Minimax Optimization for Effective\n  Clustering of Data from Multiple Sources", "comments": "34 pages, submitted to Expert Systems with Applications. arXiv admin\n  note: text overlap with arXiv:1608.07001", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view data clustering refers to categorizing a data set by making good\nuse of related information from multiple representations of the data. It\nbecomes important nowadays because more and more data can be collected in a\nvariety of ways, in different settings and from different sources, so each data\nset can be represented by different sets of features to form different views of\nit. Many approaches have been proposed to improve clustering performance by\nexploring and integrating heterogeneous information underlying different views.\nIn this paper, we propose a new multi-view fuzzy clustering approach called\nMinimaxFCM by using minimax optimization based on well-known Fuzzy c means. In\nMinimaxFCM the consensus clustering results are generated based on minimax\noptimization in which the maximum disagreements of different weighted views are\nminimized. Moreover, the weight of each view can be learned automatically in\nthe clustering process. In addition, there is only one parameter to be set\nbesides the fuzzifier. The detailed problem formulation, updating rules\nderivation, and the in-depth analysis of the proposed MinimaxFCM are provided\nhere. Experimental studies on nine multi-view data sets including real world\nimage and document data sets have been conducted. We observed that MinimaxFCM\noutperforms related multi-view clustering approaches in terms of clustering\naccuracy, demonstrating the great potential of MinimaxFCM for multi-view data\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 02:15:37 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Wang", "Yangtao", ""], ["Chen", "Lihui", ""]]}, {"id": "1608.07019", "submitter": "Haozhe Xie", "authors": "Haozhe Xie, Jie Li, Qiaosheng Zhang and Yadong Wang", "title": "Comparison among dimensionality reduction techniques based on Random\n  Projection for cancer classification", "comments": null, "journal-ref": "Computational biology and chemistry, 65: 165-172, 2016", "doi": "10.1016/j.compbiolchem.2016.09.010", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Projection (RP) technique has been widely applied in many scenarios\nbecause it can reduce high-dimensional features into low-dimensional space\nwithin short time and meet the need of real-time analysis of massive data.\nThere is an urgent need of dimensionality reduction with fast increase of big\ngenomics data. However, the performance of RP is usually lower. We attempt to\nimprove classification accuracy of RP through combining other reduction\ndimension methods such as Principle Component Analysis (PCA), Linear\nDiscriminant Analysis (LDA), and Feature Selection (FS). We compared\nclassification accuracy and running time of different combination methods on\nthree microarray datasets and a simulation dataset. Experimental results show a\nremarkable improvement of 14.77% in classification accuracy of FS followed by\nRP compared to RP on BC-TCGA dataset. LDA followed by RP also helps RP to yield\na more discriminative subspace with an increase of 13.65% on classification\naccuracy on the same dataset. FS followed by RP outperforms other combination\nmethods in classification accuracy on most of the datasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 05:14:57 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 13:56:03 GMT"}, {"version": "v3", "created": "Thu, 23 Feb 2017 02:52:17 GMT"}, {"version": "v4", "created": "Tue, 30 May 2017 01:59:19 GMT"}, {"version": "v5", "created": "Sat, 17 Jun 2017 04:12:57 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Xie", "Haozhe", ""], ["Li", "Jie", ""], ["Zhang", "Qiaosheng", ""], ["Wang", "Yadong", ""]]}, {"id": "1608.07051", "submitter": "Dawei Chen", "authors": "Dawei Chen, Cheng Soon Ong, Lexing Xie", "title": "Learning Points and Routes to Recommend Trajectories", "comments": null, "journal-ref": null, "doi": "10.1145/2983323.2983672", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of recommending tours to travellers is an important and broadly\nstudied area. Suggested solutions include various approaches of\npoints-of-interest (POI) recommendation and route planning. We consider the\ntask of recommending a sequence of POIs, that simultaneously uses information\nabout POIs and routes. Our approach unifies the treatment of various sources of\ninformation by representing them as features in machine learning algorithms,\nenabling us to learn from past behaviour. Information about POIs are used to\nlearn a POI ranking model that accounts for the start and end points of tours.\nData about previous trajectories are used for learning transition patterns\nbetween POIs that enable us to recommend probable routes. In addition, a\nprobabilistic model is proposed to combine the results of POI ranking and the\nPOI to POI transitions. We propose a new F$_1$ score on pairs of POIs that\ncapture the order of visits. Empirical results show that our approach improves\non recent methods, and demonstrate that combining points and routes enables\nbetter trajectory recommendations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 08:39:47 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Chen", "Dawei", ""], ["Ong", "Cheng Soon", ""], ["Xie", "Lexing", ""]]}, {"id": "1608.07159", "submitter": "Seyed Hossein Ghafarian", "authors": "Hossein Ghafarian and Hadi Sadoghi Yazdi", "title": "Active Robust Learning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many practical applications of learning algorithms, unlabeled data is\ncheap and abundant whereas labeled data is expensive. Active learning\nalgorithms developed to achieve better performance with lower cost. Usually\nRepresentativeness and Informativeness are used in active learning algoirthms.\nAdvanced recent active learning methods consider both of these criteria.\nDespite its vast literature, very few active learning methods consider noisy\ninstances, i.e. label noisy and outlier instances. Also, these methods didn't\nconsider accuracy in computing representativeness and informativeness. Based on\nthe idea that inaccuracy in these measures and not taking noisy instances into\nconsideration are two sides of a coin and are inherently related, a new loss\nfunction is proposed. This new loss function helps to decrease the effect of\nnoisy instances while at the same time, reduces bias. We defined \"instance\ncomplexity\" as a new notion of complexity for instances of a learning problem.\nIt is proved that noisy instances in the data if any, are the ones with maximum\ninstance complexity. Based on this loss function which has two functions for\nclassifying ordinary and noisy instances, a new classifier, named\n\"Simple-Complex Classifier\" is proposed. In this classifier there are a simple\nand a complex function, with the complex function responsible for selecting\nnoisy instances. The resulting optimization problem for both learning and\nactive learning is highly non-convex and very challenging. In order to solve\nit, a convex relaxation is proposed.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 14:14:16 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Ghafarian", "Hossein", ""], ["Yazdi", "Hadi Sadoghi", ""]]}, {"id": "1608.07179", "submitter": "Yuichi Yoshida", "authors": "Kohei Hayashi, Yuichi Yoshida", "title": "Minimizing Quadratic Functions in Constant Time", "comments": "An extended abstract will appear in the proceedings of NIPS'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sampling-based optimization method for quadratic functions is proposed. Our\nmethod approximately solves the following $n$-dimensional quadratic\nminimization problem in constant time, which is independent of $n$:\n$z^*=\\min_{\\mathbf{v} \\in \\mathbb{R}^n}\\langle\\mathbf{v}, A \\mathbf{v}\\rangle +\nn\\langle\\mathbf{v}, \\mathrm{diag}(\\mathbf{d})\\mathbf{v}\\rangle +\nn\\langle\\mathbf{b}, \\mathbf{v}\\rangle$, where $A \\in \\mathbb{R}^{n \\times n}$\nis a matrix and $\\mathbf{d},\\mathbf{b} \\in \\mathbb{R}^n$ are vectors. Our\ntheoretical analysis specifies the number of samples $k(\\delta, \\epsilon)$ such\nthat the approximated solution $z$ satisfies $|z - z^*| = O(\\epsilon n^2)$ with\nprobability $1-\\delta$. The empirical performance (accuracy and runtime) is\npositively confirmed by numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 14:43:17 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Hayashi", "Kohei", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1608.07187", "submitter": "Aylin Caliskan", "authors": "Aylin Caliskan, Joanna J. Bryson, and Arvind Narayanan", "title": "Semantics derived automatically from language corpora contain human-like\n  biases", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": "10.1126/science.aal4230", "report-no": null, "categories": "cs.AI cs.CL cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial intelligence and machine learning are in a period of astounding\ngrowth. However, there are concerns that these technologies may be used, either\nwith or without intention, to perpetuate the prejudice and unfairness that\nunfortunately characterizes many human institutions. Here we show for the first\ntime that human-like semantic biases result from the application of standard\nmachine learning to ordinary language---the same sort of language humans are\nexposed to every day. We replicate a spectrum of standard human biases as\nexposed by the Implicit Association Test and other well-known psychological\nstudies. We replicate these using a widely used, purely statistical\nmachine-learning model---namely, the GloVe word embedding---trained on a corpus\nof text from the Web. Our results indicate that language itself contains\nrecoverable and accurate imprints of our historic biases, whether these are\nmorally neutral as towards insects or flowers, problematic as towards race or\ngender, or even simply veridical, reflecting the {\\em status quo} for the\ndistribution of gender with respect to careers or first names. These\nregularities are captured by machine learning along with the rest of semantics.\nIn addition to our empirical findings concerning language, we also contribute\nnew methods for evaluating bias in text, the Word Embedding Association Test\n(WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results\nhave implications not only for AI and machine learning, but also for the fields\nof psychology, sociology, and human ethics, since they raise the possibility\nthat mere exposure to everyday language can account for the biases we replicate\nhere.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 15:07:17 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 18:23:06 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 19:03:45 GMT"}, {"version": "v4", "created": "Thu, 25 May 2017 17:50:31 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Caliskan", "Aylin", ""], ["Bryson", "Joanna J.", ""], ["Narayanan", "Arvind", ""]]}, {"id": "1608.07249", "submitter": "Shaohuai Shi", "authors": "Shaohuai Shi, Qiang Wang, Pengfei Xu, Xiaowen Chu", "title": "Benchmarking State-of-the-Art Deep Learning Software Tools", "comments": "Revision history: 1. Revise ResNet-50 configuration in MXNet. 2. Add\n  faster implementation of ResNet-56 in TensorFlow with multiple GPUs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been shown as a successful machine learning method for a\nvariety of tasks, and its popularity results in numerous open-source deep\nlearning software tools. Training a deep network is usually a very\ntime-consuming process. To address the computational challenge in deep\nlearning, many tools exploit hardware features such as multi-core CPUs and\nmany-core GPUs to shorten the training time. However, different tools exhibit\ndifferent features and running performance when training different types of\ndeep networks on different hardware platforms, which makes it difficult for end\nusers to select an appropriate pair of software and hardware. In this paper, we\naim to make a comparative study of the state-of-the-art GPU-accelerated deep\nlearning software tools, including Caffe, CNTK, MXNet, TensorFlow, and Torch.\nWe first benchmark the running performance of these tools with three popular\ntypes of neural networks on two CPU platforms and three GPU platforms. We then\nbenchmark some distributed versions on multiple GPUs. Our contribution is\ntwo-fold. First, for end users of deep learning tools, our benchmarking results\ncan serve as a guide to selecting appropriate hardware platforms and software\ntools. Second, for software developers of deep learning tools, our in-depth\nanalysis points out possible future directions to further optimize the running\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 18:48:16 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 06:25:05 GMT"}, {"version": "v3", "created": "Sat, 3 Sep 2016 16:40:32 GMT"}, {"version": "v4", "created": "Sun, 11 Sep 2016 06:13:13 GMT"}, {"version": "v5", "created": "Mon, 19 Sep 2016 07:09:07 GMT"}, {"version": "v6", "created": "Wed, 25 Jan 2017 09:27:52 GMT"}, {"version": "v7", "created": "Fri, 17 Feb 2017 11:02:08 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Shi", "Shaohuai", ""], ["Wang", "Qiang", ""], ["Xu", "Pengfei", ""], ["Chu", "Xiaowen", ""]]}, {"id": "1608.07251", "submitter": "Qingyang Li", "authors": "Qingyang Li, Tao Yang, Liang Zhan, Derrek Paul Hibar, Neda Jahanshad,\n  Yalin Wang, Jieping Ye, Paul M. Thompson, Jie Wang", "title": "Large-scale Collaborative Imaging Genetics Studies of Risk Genetic\n  Factors for Alzheimer's Disease Across Multiple Institutions", "comments": "Published on the 19th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI). 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-wide association studies (GWAS) offer new opportunities to identify\ngenetic risk factors for Alzheimer's disease (AD). Recently, collaborative\nefforts across different institutions emerged that enhance the power of many\nexisting techniques on individual institution data. However, a major barrier to\ncollaborative studies of GWAS is that many institutions need to preserve\nindividual data privacy. To address this challenge, we propose a novel\ndistributed framework, termed Local Query Model (LQM) to detect risk SNPs for\nAD across multiple research institutions. To accelerate the learning process,\nwe propose a Distributed Enhanced Dual Polytope Projection (D-EDPP) screening\nrule to identify irrelevant features and remove them from the optimization. To\nthe best of our knowledge, this is the first successful run of the\ncomputationally intensive model selection procedure to learn a consistent model\nacross different institutions without compromising their privacy while ranking\nthe SNPs that may collectively affect AD. Empirical studies are conducted on\n809 subjects with 5.9 million SNP features which are distributed across three\nindividual institutions. D-EDPP achieved a 66-fold speed-up by effectively\nidentifying irrelevant features.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 23:21:49 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Li", "Qingyang", ""], ["Yang", "Tao", ""], ["Zhan", "Liang", ""], ["Hibar", "Derrek Paul", ""], ["Jahanshad", "Neda", ""], ["Wang", "Yalin", ""], ["Ye", "Jieping", ""], ["Thompson", "Paul M.", ""], ["Wang", "Jie", ""]]}, {"id": "1608.07310", "submitter": "Panayotis Mertikopoulos", "authors": "Panayotis Mertikopoulos and Zhengyuan Zhou", "title": "Learning in games with continuous action sets and unknown payoff\n  functions", "comments": "36 pages, 2 figures; completely reworked structure of first version\n  and dropped individual concavity assumptions", "journal-ref": null, "doi": "10.1007/s10107-017-1228-2", "report-no": null, "categories": "math.OC cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the convergence of no-regret learning in games with\ncontinuous action sets. For concreteness, we focus on learning via \"dual\naveraging\", a widely used class of no-regret learning schemes where players\ntake small steps along their individual payoff gradients and then \"mirror\" the\noutput back to their action sets. In terms of feedback, we assume that players\ncan only estimate their payoff gradients up to a zero-mean error with bounded\nvariance. To study the convergence of the induced sequence of play, we\nintroduce the notion of variational stability, and we show that stable\nequilibria are locally attracting with high probability whereas globally stable\nequilibria are globally attracting with probability 1. We also discuss some\napplications to mixed-strategy learning in finite games, and we provide\nexplicit estimates of the method's convergence speed.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 21:01:23 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 03:57:37 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Mertikopoulos", "Panayotis", ""], ["Zhou", "Zhengyuan", ""]]}, {"id": "1608.07328", "submitter": "Farshad Lahouti", "authors": "Farshad Lahouti, Babak Hassibi", "title": "Fundamental Limits of Budget-Fidelity Trade-off in Label Crowdsourcing", "comments": "Accepted for NIPS 2016, Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital crowdsourcing (CS) is a modern approach to perform certain large\nprojects using small contributions of a large crowd. In CS, a taskmaster\ntypically breaks down the project into small batches of tasks and assigns them\nto so-called workers with imperfect skill levels. The crowdsourcer then\ncollects and analyzes the results for inference and serving the purpose of the\nproject. In this work, the CS problem, as a human-in-the-loop computation\nproblem, is modeled and analyzed in an information theoretic rate-distortion\nframework. The purpose is to identify the ultimate fidelity that one can\nachieve by any form of query from the crowd and any decoding (inference)\nalgorithm with a given budget. The results are established by a joint source\nchannel (de)coding scheme, which represent the query scheme and inference, over\nparallel noisy channels, which model workers with imperfect skill levels. We\nalso present and analyze a query scheme dubbed $k$-ary incidence coding and\nstudy optimized query pricing in this setting.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 22:43:46 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Lahouti", "Farshad", ""], ["Hassibi", "Babak", ""]]}, {"id": "1608.07400", "submitter": "Robin Devooght", "authors": "Robin Devooght and Hugues Bersini", "title": "Collaborative Filtering with Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that collaborative filtering can be viewed as a sequence prediction\nproblem, and that given this interpretation, recurrent neural networks offer\nvery competitive approach. In particular we study how the long short-term\nmemory (LSTM) can be applied to collaborative filtering, and how it compares to\nstandard nearest neighbors and matrix factorization methods on movie\nrecommendation. We show that the LSTM is competitive in all aspects, and\nlargely outperforms other methods in terms of item coverage and short term\npredictions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 09:20:21 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 07:41:44 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Devooght", "Robin", ""], ["Bersini", "Hugues", ""]]}, {"id": "1608.07441", "submitter": "Maxime Bucher", "authors": "Maxime Bucher (Palaiseau), St\\'ephane Herbin (Palaiseau), Fr\\'ed\\'eric\n  Jurie", "title": "Hard Negative Mining for Metric Learning Based Zero-Shot Classification", "comments": null, "journal-ref": "ECCV 16 WS TASK-CV: Transferring and Adapting Source Knowledge in\n  Computer Vision, Oct 2016, Amsterdam, Netherlands. ECCV 16 WS TASK-CV:\n  Transferring and Adapting Source Knowledge in Computer Vision", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot learning has been shown to be an efficient strategy for domain\nadaptation. In this context, this paper builds on the recent work of Bucher et\nal. [1], which proposed an approach to solve Zero-Shot classification problems\n(ZSC) by introducing a novel metric learning based objective function. This\nobjective function allows to learn an optimal embedding of the attributes\njointly with a measure of similarity between images and attributes. This paper\nextends their approach by proposing several schemes to control the generation\nof the negative pairs, resulting in a significant improvement of the\nperformance and giving above state-of-the-art results on three challenging ZSC\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 12:42:43 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Bucher", "Maxime", "", "Palaiseau"], ["Herbin", "St\u00e9phane", "", "Palaiseau"], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1608.07502", "submitter": "Ting Chen Ting Chen", "authors": "Ting Chen, Lu-An Tang, Yizhou Sun, Zhengzhang Chen, Kai Zhang", "title": "Entity Embedding-based Anomaly Detection for Heterogeneous Categorical\n  Events", "comments": "Published as a conference paper in IJCAI'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection plays an important role in modern data-driven security\napplications, such as detecting suspicious access to a socket from a process.\nIn many cases, such events can be described as a collection of categorical\nvalues that are considered as entities of different types, which we call\nheterogeneous categorical events. Due to the lack of intrinsic distance\nmeasures among entities, and the exponentially large event space, most existing\nwork relies heavily on heuristics to calculate abnormal scores for events.\nDifferent from previous work, we propose a principled and unified probabilistic\nmodel APE (Anomaly detection via Probabilistic pairwise interaction and Entity\nembedding) that directly models the likelihood of events. In this model, we\nembed entities into a common latent space using their observed co-occurrence in\ndifferent events. More specifically, we first model the compatibility of each\npair of entities according to their embeddings. Then we utilize the weighted\npairwise interactions of different entity types to define the event\nprobability. Using Noise-Contrastive Estimation with \"context-dependent\" noise\ndistribution, our model can be learned efficiently regardless of the large\nevent space. Experimental results on real enterprise surveillance data show\nthat our methods can accurately detect abnormal events compared to other\nstate-of-the-art abnormal detection techniques.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 16:15:43 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Chen", "Ting", ""], ["Tang", "Lu-An", ""], ["Sun", "Yizhou", ""], ["Chen", "Zhengzhang", ""], ["Zhang", "Kai", ""]]}, {"id": "1608.07536", "submitter": "Valentina Gregori", "authors": "Valentina Gregori and Barbara Caputo", "title": "Leveraging over intact priors for boosting control and dexterity of\n  prosthetic hands by amputees", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-invasive myoelectric prostheses require a long training time to obtain\nsatisfactory control dexterity. These training times could possibly be reduced\nby leveraging over training efforts by previous subjects. So-called domain\nadaptation algorithms formalize this strategy and have indeed been shown to\nsignificantly reduce the amount of required training data for intact subjects\nfor myoelectric movements classification. It is not clear, however, whether\nthese results extend also to amputees and, if so, whether prior information\nfrom amputees and intact subjects is equally useful. To overcome this problem,\nwe evaluated several domain adaptation algorithms on data coming from both\namputees and intact subjects. Our findings indicate that: (1) the use of\nprevious experience from other subjects allows us to reduce the training time\nby about an order of magnitude; (2) this improvement holds regardless of\nwhether an amputee exploits previous information from other amputees or from\nintact subjects.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 17:47:58 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Gregori", "Valentina", ""], ["Caputo", "Barbara", ""]]}, {"id": "1608.07605", "submitter": "Cem Aksoylar", "authors": "Cem Aksoylar, Jing Qian, Venkatesh Saligrama", "title": "Clustering and Community Detection with Imbalanced Clusters", "comments": "Extended version of arXiv:1309.2303 with new applications. Accepted\n  to IEEE TSIPN", "journal-ref": null, "doi": "10.1109/TSIPN.2016.2601022", "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering methods which are frequently used in clustering and\ncommunity detection applications are sensitive to the specific graph\nconstructions particularly when imbalanced clusters are present. We show that\nratio cut (RCut) or normalized cut (NCut) objectives are not tailored to\nimbalanced cluster sizes since they tend to emphasize cut sizes over cut\nvalues. We propose a graph partitioning problem that seeks minimum cut\npartitions under minimum size constraints on partitions to deal with imbalanced\ncluster sizes. Our approach parameterizes a family of graphs by adaptively\nmodulating node degrees on a fixed node set, yielding a set of parameter\ndependent cuts reflecting varying levels of imbalance. The solution to our\nproblem is then obtained by optimizing over these parameters. We present\nrigorous limit cut analysis results to justify our approach and demonstrate the\nsuperiority of our method through experiments on synthetic and real datasets\nfor data clustering, semi-supervised learning and community detection.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 20:49:06 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Aksoylar", "Cem", ""], ["Qian", "Jing", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1608.07619", "submitter": "Shih-Chieh Su", "authors": "Shih-Chieh Su", "title": "Interacting with Massive Behavioral Data", "comments": "KDD 2016 Workshop on Interactive Data Exploration and Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short paper, we propose the split-diffuse (SD) algorithm that takes\nthe output of an existing word embedding algorithm, and distributes the data\npoints uniformly across the visualization space. The result improves the\nperceivability and the interactability by the human.\n  We apply the SD algorithm to analyze the user behavior through access logs\nwithin the cyber security domain. The result, named the topic grids, is a set\nof grids on various topics generated from the logs. On the same set of grids,\ndifferent behavioral metrics can be shown on different targets over different\nperiods of time, to provide visualization and interaction to the human experts.\n  Analysis, investigation, and other types of interaction can be performed on\nthe topic grids more efficiently than on the output of existing dimension\nreduction methods. In addition to the cyber security domain, the topic grids\ncan be further applied to other domains like e-commerce, credit card\ntransaction, customer service to analyze the behavior in a large scale.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 22:17:56 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Su", "Shih-Chieh", ""]]}, {"id": "1608.07625", "submitter": "Shih-Chieh Su", "authors": "Shih-Chieh Su", "title": "Large Scale Behavioral Analytics via Topical Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the split-diffuse (SD) algorithm that takes the output of an\nexisting dimension reduction algorithm, and distributes the data points\nuniformly across the visualization space. The result, called the topic grids,\nis a set of grids on various topics which are generated from the free-form text\ncontent of any domain of interest. The topic grids efficiently utilizes the\nvisualization space to provide visual summaries for massive data. Topical\nanalysis, comparison and interaction can be performed on the topic grids in a\nmore perceivable way.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 23:07:43 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Su", "Shih-Chieh", ""]]}, {"id": "1608.07630", "submitter": "Daniel Hsu", "authors": "Ji Xu, Daniel Hsu, Arian Maleki", "title": "Global analysis of Expectation Maximization for mixtures of two\n  Gaussians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectation Maximization (EM) is among the most popular algorithms for\nestimating parameters of statistical models. However, EM, which is an iterative\nalgorithm based on the maximum likelihood principle, is generally only\nguaranteed to find stationary points of the likelihood objective, and these\npoints may be far from any maximizer. This article addresses this disconnect\nbetween the statistical principles behind EM and its algorithmic properties.\nSpecifically, it provides a global analysis of EM for specific models in which\nthe observations comprise an i.i.d. sample from a mixture of two Gaussians.\nThis is achieved by (i) studying the sequence of parameters from idealized\nexecution of EM in the infinite sample limit, and fully characterizing the\nlimit points of the sequence in terms of the initial parameters; and then (ii)\nbased on this convergence analysis, establishing statistical consistency (or\nlack thereof) for the actual sequence of parameters produced by EM.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 23:53:43 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Xu", "Ji", ""], ["Hsu", "Daniel", ""], ["Maleki", "Arian", ""]]}, {"id": "1608.07636", "submitter": "Hossein Hosseini", "authors": "Hossein Hosseini, Sreeram Kannan, Baosen Zhang and Radha Poovendran", "title": "Learning Temporal Dependence from Time-Series Data with Latent Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the setting where a collection of time series, modeled as random\nprocesses, evolve in a causal manner, and one is interested in learning the\ngraph governing the relationships of these processes. A special case of wide\ninterest and applicability is the setting where the noise is Gaussian and\nrelationships are Markov and linear. We study this setting with two additional\nfeatures: firstly, each random process has a hidden (latent) state, which we\nuse to model the internal memory possessed by the variables (similar to hidden\nMarkov models). Secondly, each variable can depend on its latent memory state\nthrough a random lag (rather than a fixed lag), thus modeling memory recall\nwith differing lags at distinct times. Under this setting, we develop an\nestimator and prove that under a genericity assumption, the parameters of the\nmodel can be learned consistently. We also propose a practical adaption of this\nestimator, which demonstrates significant performance gains in both synthetic\nand real-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 00:25:54 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Hosseini", "Hossein", ""], ["Kannan", "Sreeram", ""], ["Zhang", "Baosen", ""], ["Poovendran", "Radha", ""]]}, {"id": "1608.07639", "submitter": "Yuval Atzmon", "authors": "Yuval Atzmon, Jonathan Berant, Vahid Kezami, Amir Globerson and Gal\n  Chechik", "title": "Learning to generalize to new compositions in image understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks have recently been used for learning to describe\nimages using natural language. However, it has been observed that these models\ngeneralize poorly to scenes that were not observed during training, possibly\ndepending too strongly on the statistics of the text in the training data. Here\nwe propose to describe images using short structured representations, aiming to\ncapture the crux of a description. These structured representations allow us to\ntease-out and evaluate separately two types of generalization: standard\ngeneralization to new images with similar scenes, and generalization to new\ncombinations of known entities. We compare two learning approaches on the\nMS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show,\nAttend and Tell), and a simple structured prediction model on top of a deep\nnetwork. We find that the structured model generalizes to new compositions\nsubstantially better than the LSTM, ~7 times the accuracy of predicting\nstructured representations. By providing a concrete method to quantify\ngeneralization for unseen combinations, we argue that structured\nrepresentations and compositional splits are a useful benchmark for image\ncaptioning, and advocate compositional models that capture linguistic and\nvisual structure.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 00:34:00 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Atzmon", "Yuval", ""], ["Berant", "Jonathan", ""], ["Kezami", "Vahid", ""], ["Globerson", "Amir", ""], ["Chechik", "Gal", ""]]}, {"id": "1608.07685", "submitter": "Han Xiao", "authors": "Han Xiao, Minlie Huang, Xiaoyan Zhu", "title": "KSR: A Semantic Representation of Knowledge Graph within a Novel\n  Unsupervised Paradigm", "comments": "submitting to IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge representation is a long-history topic in AI, which is very\nimportant. A variety of models have been proposed for knowledge graph\nembedding, which projects symbolic entities and relations into continuous\nvector space. However, most related methods merely focus on the data-fitting of\nknowledge graph, and ignore the interpretable semantic expression. Thus,\ntraditional embedding methods are not friendly for applications that require\nsemantic analysis, such as question answering and entity retrieval. To this\nend, this paper proposes a semantic representation method for knowledge graph\n\\textbf{(KSR)}, which imposes a two-level hierarchical generative process that\nglobally extracts many aspects and then locally assigns a specific category in\neach aspect for every triple. Since both aspects and categories are\nsemantics-relevant, the collection of categories in each aspect is treated as\nthe semantic representation of this triple. Extensive experiments show that our\nmodel outperforms other state-of-the-art baselines substantially.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 09:53:38 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 02:48:01 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 02:06:16 GMT"}, {"version": "v4", "created": "Thu, 30 Nov 2017 09:59:23 GMT"}, {"version": "v5", "created": "Fri, 1 Dec 2017 01:54:44 GMT"}, {"version": "v6", "created": "Sat, 16 Dec 2017 11:20:48 GMT"}, {"version": "v7", "created": "Fri, 11 May 2018 04:16:05 GMT"}, {"version": "v8", "created": "Wed, 1 Apr 2020 03:14:54 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Xiao", "Han", ""], ["Huang", "Minlie", ""], ["Zhu", "Xiaoyan", ""]]}, {"id": "1608.07690", "submitter": "Thomas Tanay", "authors": "Thomas Tanay and Lewis Griffin", "title": "A Boundary Tilting Persepective on the Phenomenon of Adversarial\n  Examples", "comments": "arXiv admin note: text overlap with arXiv:1412.6572 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been shown to suffer from a surprising weakness:\ntheir classification outputs can be changed by small, non-random perturbations\nof their inputs. This adversarial example phenomenon has been explained as\noriginating from deep networks being \"too linear\" (Goodfellow et al., 2014). We\nshow here that the linear explanation of adversarial examples presents a number\nof limitations: the formal argument is not convincing, linear classifiers do\nnot always suffer from the phenomenon, and when they do their adversarial\nexamples are different from the ones affecting deep networks.\n  We propose a new perspective on the phenomenon. We argue that adversarial\nexamples exist when the classification boundary lies close to the submanifold\nof sampled data, and present a mathematical analysis of this new perspective in\nthe linear case. We define the notion of adversarial strength and show that it\ncan be reduced to the deviation angle between the classifier considered and the\nnearest centroid classifier. Then, we show that the adversarial strength can be\nmade arbitrarily high independently of the classification performance due to a\nmechanism that we call boundary tilting. This result leads us to defining a new\ntaxonomy of adversarial examples. Finally, we show that the adversarial\nstrength observed in practice is directly dependent on the level of\nregularisation used and the strongest adversarial examples, symptomatic of\noverfitting, can be avoided by using a proper level of regularisation.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 10:44:54 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Tanay", "Thomas", ""], ["Griffin", "Lewis", ""]]}, {"id": "1608.07710", "submitter": "Yangming Zhou", "authors": "Yangming Zhou and Guoping Qiu", "title": "Random Forest for Label Ranking", "comments": "28 pages, 4 figures,accepted to Expert Systems With Applications in\n  June 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label ranking aims to learn a mapping from instances to rankings over a\nfinite number of predefined labels. Random forest is a powerful and one of the\nmost successful general-purpose machine learning algorithms of modern times. In\nthis paper, we present a powerful random forest label ranking method which uses\nrandom decision trees to retrieve nearest neighbors. We have developed a novel\ntwo-step rank aggregation strategy to effectively aggregate neighboring\nrankings discovered by the random forest into a final predicted ranking.\nCompared with existing methods, the new random forest method has many\nadvantages including its intrinsically scalable tree data structure, highly\nparallel-able computational architecture and much superior performance. We\npresent extensive experimental results to demonstrate that our new method\nachieves the highly competitive performance compared with state-of-the-art\nmethods for datasets with complete ranking and datasets with only partial\nranking information.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 13:32:42 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 11:05:45 GMT"}, {"version": "v3", "created": "Sat, 16 Jun 2018 03:22:49 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Zhou", "Yangming", ""], ["Qiu", "Guoping", ""]]}, {"id": "1608.07719", "submitter": "Joao Papa", "authors": "Leandro Aparecido Passos Junior and Joao Paulo Papa", "title": "Temperature-Based Deep Boltzmann Machines", "comments": "Submitted to Neural Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have been paramount in the last years, mainly due to\ntheir outstanding results in a number of applications, that range from speech\nrecognition to face-based user identification. Despite other techniques\nemployed for such purposes, Deep Boltzmann Machines are among the most used\nones, which are composed of layers of Restricted Boltzmann Machines (RBMs)\nstacked on top of each other. In this work, we evaluate the concept of\ntemperature in DBMs, which play a key role in Boltzmann-related distributions,\nbut it has never been considered in this context up to date. Therefore, the\nmain contribution of this paper is to take into account this information and to\nevaluate its influence in DBMs considering the task of binary image\nreconstruction. We expect this work can foster future research considering the\nusage of different temperatures during learning in DBMs.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 15:31:21 GMT"}, {"version": "v2", "created": "Sun, 4 Sep 2016 00:55:52 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Junior", "Leandro Aparecido Passos", ""], ["Papa", "Joao Paulo", ""]]}, {"id": "1608.07739", "submitter": "Jordan Frecon", "authors": "Jordan Frecon, Nelly Pustelnik, Nicolas Dobigeon, Herwig Wendt and\n  Patrice Abry", "title": "Bayesian selection for the l2-Potts model regularization parameter: 1D\n  piecewise constant signal denoising", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2715000", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Piecewise constant denoising can be solved either by deterministic\noptimization approaches, based on the Potts model, or by stochastic Bayesian\nprocedures. The former lead to low computational time but require the selection\nof a regularization parameter, whose value significantly impacts the achieved\nsolution, and whose automated selection remains an involved and challenging\nproblem. Conversely, fully Bayesian formalisms encapsulate the regularization\nparameter selection into hierarchical models, at the price of high\ncomputational costs. This contribution proposes an operational strategy that\ncombines hierarchical Bayesian and Potts model formulations, with the double\naim of automatically tuning the regularization parameter and of maintaining\ncomputational effciency. The proposed procedure relies on formally connecting a\nBayesian framework to a l2-Potts functional. Behaviors and performance for the\nproposed piecewise constant denoising and regularization parameter tuning\ntechniques are studied qualitatively and assessed quantitatively, and shown to\ncompare favorably against those of a fully Bayesian hierarchical procedure,\nboth in accuracy and in computational load.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2016 19:59:29 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 13:56:44 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Frecon", "Jordan", ""], ["Pustelnik", "Nelly", ""], ["Dobigeon", "Nicolas", ""], ["Wendt", "Herwig", ""], ["Abry", "Patrice", ""]]}, {"id": "1608.07888", "submitter": "Ian Gemp", "authors": "Ian Gemp and Sridhar Mahadevan", "title": "Online Monotone Optimization", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new framework for analyzing and designing no-regret\nalgorithms for dynamic (possibly adversarial) systems. The proposed framework\ngeneralizes the popular online convex optimization framework and extends it to\nits natural limit allowing it to capture a notion of regret that is intuitive\nfor more general problems such as those encountered in game theory and\nvariational inequalities. The framework hinges on a special choice of a\nsystem-wide loss function we have developed. Using this framework, we prove\nthat a simple update scheme provides a no-regret algorithm for monotone\nsystems. While previous results in game theory prove individual agents can\nenjoy unilateral no-regret guarantees, our result proves monotonicity\nsufficient for guaranteeing no-regret when considering the adjustments of\nmultiple agent strategies in parallel. Furthermore, to our knowledge, this is\nthe first framework to provide a suitable notion of regret for variational\ninequalities. Most importantly, our proposed framework ensures monotonicity a\nsufficient condition for employing multiple online learners safely in parallel.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 01:58:27 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Gemp", "Ian", ""], ["Mahadevan", "Sridhar", ""]]}, {"id": "1608.07892", "submitter": "Junqi Jin", "authors": "Junqi Jin, Ziang Yan, Kun Fu, Nan Jiang, Changshui Zhang", "title": "Optimizing Recurrent Neural Networks Architectures under Time\n  Constraints", "comments": "Withdrawn due to incompleteness and some overlaps with existing\n  literatures, I will resubmit adding further results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network (RNN)'s architecture is a key factor influencing its\nperformance. We propose algorithms to optimize hidden sizes under running time\nconstraint. We convert the discrete optimization into a subset selection\nproblem. By novel transformations, the objective function becomes submodular\nand constraint becomes supermodular. A greedy algorithm with bounds is\nsuggested to solve the transformed problem. And we show how transformations\ninfluence the bounds. To speed up optimization, surrogate functions are\nproposed which balance exploration and exploitation. Experiments show that our\nalgorithms can find more accurate models or faster models than manually tuned\nstate-of-the-art and random search. We also compare popular RNN architectures\nusing our algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 02:14:48 GMT"}, {"version": "v2", "created": "Sun, 19 Mar 2017 13:37:52 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 03:45:44 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Jin", "Junqi", ""], ["Yan", "Ziang", ""], ["Fu", "Kun", ""], ["Jiang", "Nan", ""], ["Zhang", "Changshui", ""]]}, {"id": "1608.07895", "submitter": "Olfa Nasraoui", "authors": "Olfa Nasraoui and Patrick Shafto", "title": "Human-Algorithm Interaction Biases in the Big Data Cycle: A Markov Chain\n  Iterated Learning Framework", "comments": "This research was supported by National Science Foundation grant\n  NSF-1549981", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early supervised machine learning algorithms have relied on reliable expert\nlabels to build predictive models. However, the gates of data generation have\nrecently been opened to a wider base of users who started participating\nincreasingly with casual labeling, rating, annotating, etc. The increased\nonline presence and participation of humans has led not only to a\ndemocratization of unchecked inputs to algorithms, but also to a wide\ndemocratization of the \"consumption\" of machine learning algorithms' outputs by\ngeneral users. Hence, these algorithms, many of which are becoming essential\nbuilding blocks of recommender systems and other information filters, started\ninteracting with users at unprecedented rates. The result is machine learning\nalgorithms that consume more and more data that is unchecked, or at the very\nleast, not fitting conventional assumptions made by various machine learning\nalgorithms. These include biased samples, biased labels, diverging training and\ntesting sets, and cyclical interaction between algorithms, humans, information\nconsumed by humans, and data consumed by algorithms. Yet, the continuous\ninteraction between humans and algorithms is rarely taken into account in\nmachine learning algorithm design and analysis. In this paper, we present a\npreliminary theoretical model and analysis of the mutual interaction between\nhumans and algorithms, based on an iterated learning framework that is inspired\nfrom the study of human language evolution. We also define the concepts of\nhuman and algorithm blind spots and outline machine learning approaches to mend\niterated bias through two novel notions: antidotes and reactive learning.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 02:37:21 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Nasraoui", "Olfa", ""], ["Shafto", "Patrick", ""]]}, {"id": "1608.07934", "submitter": "Hadi Zare", "authors": "Hadi Zare and Mojtaba Niazi", "title": "Relevant based structure learning for feature selection", "comments": "29 pages, 11 figures", "journal-ref": "Eng. Appl. Artif. Intel. 55 (2016) 93-102", "doi": "10.1016/j.engappai.2016.06.001", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is an important task in many problems occurring in pattern\nrecognition, bioinformatics, machine learning and data mining applications. The\nfeature selection approach enables us to reduce the computation burden and the\nfalling accuracy effect of dealing with huge number of features in typical\nlearning problems. There is a variety of techniques for feature selection in\nsupervised learning problems based on different selection metrics. In this\npaper, we propose a novel unified framework for feature selection built on the\ngraphical models and information theoretic tools. The proposed approach\nexploits the structure learning among features to select more relevant and less\nredundant features to the predictive modeling problem according to a primary\nnovel likelihood based criterion. In line with the selection of the optimal\nsubset of features through the proposed method, it provides us the Bayesian\nnetwork classifier without the additional cost of model training on the\nselected subset of features. The optimal properties of our method are\nestablished through empirical studies and computational complexity analysis.\nFurthermore the proposed approach is evaluated on a bunch of benchmark datasets\nbased on the well-known classification algorithms. Extensive experiments\nconfirm the significant improvement of the proposed approach compared to the\nearlier works.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 07:21:20 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Zare", "Hadi", ""], ["Niazi", "Mojtaba", ""]]}, {"id": "1608.07949", "submitter": "Sahar Imtiaz", "authors": "Sahar Imtiaz, Hadi Ghauch, M. Mahboob Ur Rahman, George Koudouridis,\n  and James Gross", "title": "Learning-Based Resource Allocation Scheme for TDD-Based CRAN System", "comments": "10 pages, 9 figures, accepted for publication in MSWiM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explosive growth in the use of smart wireless devices has necessitated the\nprovision of higher data rates and always-on connectivity, which are the main\nmotivators for designing the fifth generation (5G) systems. To achieve higher\nsystem efficiency, massive antenna deployment with tight coordination is one\npotential strategy for designing 5G systems, but has two types of associated\nsystem overhead. First is the synchronization overhead, which can be reduced by\nimplementing a cloud radio access network (CRAN)-based architecture design,\nthat separates the baseband processing and radio access functionality to\nachieve better system synchronization. Second is the overhead for acquiring\nchannel state information (CSI) of the users present in the system, which,\nhowever, increases tremendously when instantaneous CSI is used to serve\nhigh-mobility users. To serve a large number of users, a CRAN system with a\ndense deployment of remote radio heads (RRHs) is considered, such that each\nuser has a line-of-sight (LOS) link with the corresponding RRH. Since, the\ntrajectory of movement for high-mobility users is predictable; therefore,\nfairly accurate position estimates for those users can be obtained, and can be\nused for resource allocation to serve the considered users. The resource\nallocation is dependent upon various correlated system parameters, and these\ncorrelations can be learned using well-known \\emph{machine learning}\nalgorithms. This paper proposes a novel \\emph{learning-based resource\nallocation scheme} for time division duplex (TDD) based 5G CRAN systems with\ndense RRH deployment, by using only the users' position estimates for resource\nallocation, thus avoiding the need for CSI acquisition. This reduces the\noverall system overhead significantly, while still achieving near-optimal\nsystem performance; thus, better (effective) system efficiency is achieved.\n(See the paper for full abstract)\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 08:33:25 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Imtiaz", "Sahar", ""], ["Ghauch", "Hadi", ""], ["Rahman", "M. Mahboob Ur", ""], ["Koudouridis", "George", ""], ["Gross", "James", ""]]}, {"id": "1608.08052", "submitter": "Balamurugan P", "authors": "Nicolas Flammarion and Balamurugan Palaniappan and Francis Bach", "title": "Robust Discriminative Clustering with Sparse Regularizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering high-dimensional data often requires some form of dimensionality\nreduction, where clustered variables are separated from \"noise-looking\"\nvariables. We cast this problem as finding a low-dimensional projection of the\ndata which is well-clustered. This yields a one-dimensional projection in the\nsimplest situation with two clusters, and extends naturally to a multi-label\nscenario for more than two clusters. In this paper, (a) we first show that this\njoint clustering and dimension reduction formulation is equivalent to\npreviously proposed discriminative clustering frameworks, thus leading to\nconvex relaxations of the problem, (b) we propose a novel sparse extension,\nwhich is still cast as a convex relaxation and allows estimation in higher\ndimensions, (c) we propose a natural extension for the multi-label scenario,\n(d) we provide a new theoretical analysis of the performance of these\nformulations with a simple probabilistic model, leading to scalings over the\nform $d=O(\\sqrt{n})$ for the affine invariant case and $d=O(n)$ for the sparse\ncase, where $n$ is the number of examples and $d$ the ambient dimension, and\nfinally, (e) we propose an efficient iterative algorithm with running-time\ncomplexity proportional to $O(nd^2)$, improving on earlier algorithms which had\nquadratic complexity in the number of examples.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 14:00:21 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Flammarion", "Nicolas", ""], ["Palaniappan", "Balamurugan", ""], ["Bach", "Francis", ""]]}, {"id": "1608.08063", "submitter": "Remi Flamary", "authors": "R\\'emi Flamary, Marco Cuturi, Nicolas Courty, Alain Rakotomamonjy", "title": "Wasserstein Discriminant Analysis", "comments": null, "journal-ref": null, "doi": "10.1007/s10994-018-5717-1", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wasserstein Discriminant Analysis (WDA) is a new supervised method that can\nimprove classification of high-dimensional data by computing a suitable linear\nmap onto a lower dimensional subspace. Following the blueprint of classical\nLinear Discriminant Analysis (LDA), WDA selects the projection matrix that\nmaximizes the ratio of two quantities: the dispersion of projected points\ncoming from different classes, divided by the dispersion of projected points\ncoming from the same class. To quantify dispersion, WDA uses regularized\nWasserstein distances, rather than cross-variance measures which have been\nusually considered, notably in LDA. Thanks to the the underlying principles of\noptimal transport, WDA is able to capture both global (at distribution scale)\nand local (at samples scale) interactions between classes. Regularized\nWasserstein distances can be computed using the Sinkhorn matrix scaling\nalgorithm; We show that the optimization of WDA can be tackled using automatic\ndifferentiation of Sinkhorn iterations. Numerical experiments show promising\nresults both in terms of prediction and visualization on toy examples and real\nlife datasets such as MNIST and on deep features obtained from a subset of the\nCaltech dataset.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 14:18:40 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 08:42:15 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Flamary", "R\u00e9mi", ""], ["Cuturi", "Marco", ""], ["Courty", "Nicolas", ""], ["Rakotomamonjy", "Alain", ""]]}, {"id": "1608.08182", "submitter": "Bo Li", "authors": "Bo Li, Yining Wang, Aarti Singh, Yevgeniy Vorobeychik", "title": "Data Poisoning Attacks on Factorization-Based Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation and collaborative filtering systems are important in modern\ninformation and e-commerce applications. As these systems are becoming\nincreasingly popular in the industry, their outputs could affect business\ndecision making, introducing incentives for an adversarial party to compromise\nthe availability or integrity of such systems. We introduce a data poisoning\nattack on collaborative filtering systems. We demonstrate how a powerful\nattacker with full knowledge of the learner can generate malicious data so as\nto maximize his/her malicious objectives, while at the same time mimicking\nnormal user behavior to avoid being detected. While the complete knowledge\nassumption seems extreme, it enables a robust assessment of the vulnerability\nof collaborative filtering schemes to highly motivated attacks. We present\nefficient solutions for two popular factorization-based collaborative filtering\nalgorithms: the \\emph{alternative minimization} formulation and the\n\\emph{nuclear norm minimization} method. Finally, we test the effectiveness of\nour proposed algorithms on real-world data and discuss potential defensive\nstrategies.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 19:09:27 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 22:26:13 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Li", "Bo", ""], ["Wang", "Yining", ""], ["Singh", "Aarti", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "1608.08225", "submitter": "Max Tegmark", "authors": "Henry W. Lin (Harvard), Max Tegmark (MIT), David Rolnick (MIT)", "title": "Why does deep and cheap learning work so well?", "comments": "Replaced to match version published in Journal of Statistical\n  Physics: https://link.springer.com/article/10.1007/s10955-017-1836-5 Improved\n  refs & discussion, typos fixed. 16 pages, 3 figs", "journal-ref": null, "doi": "10.1007/s10955-017-1836-5", "report-no": null, "categories": "cond-mat.dis-nn cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how the success of deep learning could depend not only on mathematics\nbut also on physics: although well-known mathematical theorems guarantee that\nneural networks can approximate arbitrary functions well, the class of\nfunctions of practical interest can frequently be approximated through \"cheap\nlearning\" with exponentially fewer parameters than generic ones. We explore how\nproperties frequently encountered in physics such as symmetry, locality,\ncompositionality, and polynomial log-probability translate into exceptionally\nsimple neural networks. We further argue that when the statistical process\ngenerating the data is of a certain hierarchical form prevalent in physics and\nmachine-learning, a deep neural network can be more efficient than a shallow\none. We formalize these claims using information theory and discuss the\nrelation to the renormalization group. We prove various \"no-flattening\ntheorems\" showing when efficient linear deep networks cannot be accurately\napproximated by shallow ones without efficiency loss, for example, we show that\n$n$ variables cannot be multiplied using fewer than 2^n neurons in a single\nhidden layer.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 20:00:14 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 00:38:45 GMT"}, {"version": "v3", "created": "Tue, 2 May 2017 02:15:43 GMT"}, {"version": "v4", "created": "Thu, 3 Aug 2017 18:32:53 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Lin", "Henry W.", "", "Harvard"], ["Tegmark", "Max", "", "MIT"], ["Rolnick", "David", "", "MIT"]]}, {"id": "1608.08266", "submitter": "Nicola Di Mauro", "authors": "Antonio Vergari and Nicola Di Mauro and Floriana Esposito", "title": "Visualizing and Understanding Sum-Product Networks", "comments": "Machine Learning Journal paper (First Online), 24 pages", "journal-ref": null, "doi": "10.1007/s10994-018-5760-y", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sum-Product Networks (SPNs) are recently introduced deep tractable\nprobabilistic models by which several kinds of inference queries can be\nanswered exactly and in a tractable time. Up to now, they have been largely\nused as black box density estimators, assessed only by comparing their\nlikelihood scores only. In this paper we explore and exploit the inner\nrepresentations learned by SPNs. We do this with a threefold aim: first we want\nto get a better understanding of the inner workings of SPNs; secondly, we seek\nadditional ways to evaluate one SPN model and compare it against other\nprobabilistic models, providing diagnostic tools to practitioners; lastly, we\nwant to empirically evaluate how good and meaningful the extracted\nrepresentations are, as in a classic Representation Learning framework. In\norder to do so we revise their interpretation as deep neural networks and we\npropose to exploit several visualization techniques on their node activations\nand network outputs under different types of inference queries. To investigate\nthese models as feature extractors, we plug some SPNs, learned in a greedy\nunsupervised fashion on image datasets, in supervised classification learning\ntasks. We extract several embedding types from node activations by filtering\nnodes by their type, by their associated feature abstraction level and by their\nscope. In a thorough empirical comparison we prove them to be competitive\nagainst those generated from popular feature extractors as Restricted Boltzmann\nMachines. Finally, we investigate embeddings generated from random\nprobabilistic marginal queries as means to compare other tractable\nprobabilistic models on a common ground, extending our experiments to Mixtures\nof Trees.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 22:10:17 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 16:27:28 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Vergari", "Antonio", ""], ["Di Mauro", "Nicola", ""], ["Esposito", "Floriana", ""]]}, {"id": "1608.08337", "submitter": "Avleen Bijral", "authors": "Avleen S. Bijral", "title": "Data Dependent Convergence for Distributed Stochastic Optimization", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this dissertation we propose alternative analysis of distributed\nstochastic gradient descent (SGD) algorithms that rely on spectral properties\nof the data covariance. As a consequence we can relate questions pertaining to\nspeedups and convergence rates for distributed SGD to the data distribution\ninstead of the regularity properties of the objective functions. More precisely\nwe show that this rate depends on the spectral norm of the sample covariance\nmatrix. An estimate of this norm can provide practitioners with guidance\ntowards a potential gain in algorithm performance. For example many sparse\ndatasets with low spectral norm prove to be amenable to gains in distributed\nsettings. Towards establishing this data dependence we first study a\ndistributed consensus-based SGD algorithm and show that the rate of convergence\ninvolves the spectral norm of the sample covariance matrix when the underlying\ndata is assumed to be independent and identically distributed (homogenous).\nThis dependence allows us to identify network regimes that prove to be\nbeneficial for datasets with low sample covariance spectral norm. Existing\nconsensus based analyses prove to be sub-optimal in the homogenous setting. Our\nanalysis method also allows us to find data-dependent convergence rates as we\nlimit the amount of communication. Spreading a fixed amount of data across more\nnodes slows convergence; in the asymptotic regime we show that adding more\nmachines can help when minimizing twice-differentiable losses. Since the\nmini-batch results don't follow from the consensus results we propose a\ndifferent data dependent analysis thereby providing theoretical validation for\nwhy certain datasets are more amenable to mini-batching. We also provide\nempirical evidence for results in this thesis.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 05:58:38 GMT"}], "update_date": "2016-09-03", "authors_parsed": [["Bijral", "Avleen S.", ""]]}, {"id": "1608.08435", "submitter": "Rajasekar Venkatesan", "authors": "Rajasekar Venkatesan, Meng Joo Er", "title": "Multi-Label Classification Method Based on Extreme Learning Machines", "comments": "6 pages, 7 figures, 7 tables, ICARCV", "journal-ref": null, "doi": "10.1109/ICARCV.2014.7064375", "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an Extreme Learning Machine (ELM) based technique for\nMulti-label classification problems is proposed and discussed. In multi-label\nclassification, each of the input data samples belongs to one or more than one\nclass labels. The traditional binary and multi-class classification problems\nare the subset of the multi-label problem with the number of labels\ncorresponding to each sample limited to one. The proposed ELM based multi-label\nclassification technique is evaluated with six different benchmark multi-label\ndatasets from different domains such as multimedia, text and biology. A\ndetailed comparison of the results is made by comparing the proposed method\nwith the results from nine state of the arts techniques for five different\nevaluation metrics. The nine methods are chosen from different categories of\nmulti-label methods. The comparative results shows that the proposed Extreme\nLearning Machine based multi-label classification technique is a better\nalternative than the existing state of the art methods for multi-label\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 13:08:06 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Venkatesan", "Rajasekar", ""], ["Er", "Meng Joo", ""]]}, {"id": "1608.08574", "submitter": "Babatunde Olabenjo", "authors": "Babatunde Olabenjo", "title": "Applying Naive Bayes Classification to Google Play Apps Categorization", "comments": "Experiment Results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are over one million apps on Google Play Store and over half a million\npublishers. Having such a huge number of apps and developers can pose a\nchallenge to app users and new publishers on the store. Discovering apps can be\nchallenging if apps are not correctly published in the right category, and, in\nturn, reduce earnings for app developers. Additionally, with over 41 categories\non Google Play Store, deciding on the right category to publish an app can be\nchallenging for developers due to the number of categories they have to choose\nfrom. Machine Learning has been very useful, especially in classification\nproblems such sentiment analysis, document classification and spam detection.\nThese strategies can also be applied to app categorization on Google Play Store\nto suggest appropriate categories for app publishers using details from their\napplication.\n  In this project, we built two variations of the Naive Bayes classifier using\nopen metadata from top developer apps on Google Play Store in other to classify\nnew apps on the store. These classifiers are then evaluated using various\nevaluation methods and their results compared against each other. The results\nshow that the Naive Bayes algorithm performs well for our classification\nproblem and can potentially automate app categorization for Android app\npublishers on Google Play Store\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 17:46:55 GMT"}], "update_date": "2016-09-03", "authors_parsed": [["Olabenjo", "Babatunde", ""]]}, {"id": "1608.08614", "submitter": "Minyoung Huh", "authors": "Minyoung Huh, Pulkit Agrawal, Alexei A. Efros", "title": "What makes ImageNet good for transfer learning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The tremendous success of ImageNet-trained deep features on a wide range of\ntransfer tasks begs the question: what are the properties of the ImageNet\ndataset that are critical for learning good, general-purpose features? This\nwork provides an empirical investigation of various facets of this question: Is\nmore pre-training data always better? How does feature quality depend on the\nnumber of training examples per class? Does adding more object classes improve\nperformance? For the same data budget, how should the data be split into\nclasses? Is fine-grained recognition necessary for learning good features?\nGiven the same number of training classes, is it better to have coarse classes\nor fine-grained classes? Which is better: more classes or more examples per\nclass? To answer these and related questions, we pre-trained CNN features on\nvarious subsets of the ImageNet dataset and evaluated transfer performance on\nPASCAL detection, PASCAL action classification, and SUN scene classification\ntasks. Our overall findings suggest that most changes in the choice of\npre-training data long thought to be critical do not significantly affect\ntransfer performance.? Given the same number of training classes, is it better\nto have coarse classes or fine-grained classes? Which is better: more classes\nor more examples per class?\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 19:45:09 GMT"}, {"version": "v2", "created": "Sat, 10 Dec 2016 13:37:06 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Huh", "Minyoung", ""], ["Agrawal", "Pulkit", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1608.08710", "submitter": "Hao Li", "authors": "Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, Hans Peter Graf", "title": "Pruning Filters for Efficient ConvNets", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of CNNs in various applications is accompanied by a significant\nincrease in the computation and parameter storage costs. Recent efforts toward\nreducing these overheads involve pruning and compressing the weights of various\nlayers without hurting original accuracy. However, magnitude-based pruning of\nweights reduces a significant number of parameters from the fully connected\nlayers and may not adequately reduce the computation costs in the convolutional\nlayers due to irregular sparsity in the pruned networks. We present an\nacceleration method for CNNs, where we prune filters from CNNs that are\nidentified as having a small effect on the output accuracy. By removing whole\nfilters in the network together with their connecting feature maps, the\ncomputation costs are reduced significantly. In contrast to pruning weights,\nthis approach does not result in sparse connectivity patterns. Hence, it does\nnot need the support of sparse convolution libraries and can work with existing\nefficient BLAS libraries for dense matrix multiplications. We show that even\nsimple filter pruning techniques can reduce inference costs for VGG-16 by up to\n34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the\noriginal accuracy by retraining the networks.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 02:29:59 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 02:12:36 GMT"}, {"version": "v3", "created": "Fri, 10 Mar 2017 17:57:56 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Li", "Hao", ""], ["Kadav", "Asim", ""], ["Durdanovic", "Igor", ""], ["Samet", "Hanan", ""], ["Graf", "Hans Peter", ""]]}, {"id": "1608.08716", "submitter": "Aishwarya Agrawal", "authors": "C. Lawrence Zitnick, Aishwarya Agrawal, Stanislaw Antol, Margaret\n  Mitchell, Dhruv Batra, Devi Parikh", "title": "Measuring Machine Intelligence Through Visual Question Answering", "comments": "AI Magazine, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machines have become more intelligent, there has been a renewed interest\nin methods for measuring their intelligence. A common approach is to propose\ntasks for which a human excels, but one which machines find difficult. However,\nan ideal task should also be easy to evaluate and not be easily gameable. We\nbegin with a case study exploring the recently popular task of image captioning\nand its limitations as a task for measuring machine intelligence. An\nalternative and more promising task is Visual Question Answering that tests a\nmachine's ability to reason about language and vision. We describe a dataset\nunprecedented in size created for the task that contains over 760,000 human\ngenerated questions about images. Using around 10 million human generated\nanswers, machines may be easily evaluated.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 02:56:00 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Zitnick", "C. Lawrence", ""], ["Agrawal", "Aishwarya", ""], ["Antol", "Stanislaw", ""], ["Mitchell", "Margaret", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1608.08761", "submitter": "Tingting Xie", "authors": "Tingting Xie, Yuxing Peng, Changjian Wang", "title": "hi-RF: Incremental Learning Random Forest for large-scale multi-class\n  Data Classification", "comments": "Accepted by AIIE2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, dynamically growing data and incrementally growing number of\nclasses pose new challenges to large-scale data classification research. Most\ntraditional methods struggle to balance the precision and computational burden\nwhen data and its number of classes increased. However, some methods are with\nweak precision, and the others are time-consuming. In this paper, we propose an\nincremental learning method, namely, heterogeneous incremental Nearest Class\nMean Random Forest (hi-RF), to handle this issue. It is a heterogeneous method\nthat either replaces trees or updates trees leaves in the random forest\nadaptively, to reduce the computational time in comparable performance, when\ndata of new classes arrive. Specifically, to keep the accuracy, one proportion\nof trees are replaced by new NCM decision trees; to reduce the computational\nload, the rest trees are updated their leaves probabilities only. Most of all,\nout-of-bag estimation and out-of-bag boosting are proposed to balance the\naccuracy and the computational efficiency. Fair experiments were conducted and\ndemonstrated its comparable precision with much less computational time.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 08:18:48 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 17:34:03 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Xie", "Tingting", ""], ["Peng", "Yuxing", ""], ["Wang", "Changjian", ""]]}, {"id": "1608.08852", "submitter": "Martin Genzel", "authors": "Martin Genzel and Gitta Kutyniok", "title": "A Mathematical Framework for Feature Selection from Real-World Data with\n  Non-Linear Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the challenge of feature selection based on a\nrelatively small collection of sample pairs $\\{(x_i, y_i)\\}_{1 \\leq i \\leq m}$.\nThe observations $y_i \\in \\mathbb{R}$ are thereby supposed to follow a noisy\nsingle-index model, depending on a certain set of signal variables. A major\ndifficulty is that these variables usually cannot be observed directly, but\nrather arise as hidden factors in the actual data vectors $x_i \\in\n\\mathbb{R}^d$ (feature variables). We will prove that a successful variable\nselection is still possible in this setup, even when the applied estimator does\nnot have any knowledge of the underlying model parameters and only takes the\n'raw' samples $\\{(x_i, y_i)\\}_{1 \\leq i \\leq m}$ as input. The model\nassumptions of our results will be fairly general, allowing for non-linear\nobservations, arbitrary convex signal structures as well as strictly convex\nloss functions. This is particularly appealing for practical purposes, since in\nmany applications, already standard methods, e.g., the Lasso or logistic\nregression, yield surprisingly good outcomes. Apart from a general discussion\nof the practical scope of our theoretical findings, we will also derive a\nrigorous guarantee for a specific real-world problem, namely sparse feature\nextraction from (proteomics-based) mass spectrometry data.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 13:53:09 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Genzel", "Martin", ""], ["Kutyniok", "Gitta", ""]]}, {"id": "1608.08898", "submitter": "Rajasekar Venkatesan", "authors": "Meng Joo Er, Rajasekar Venkatesan and Ning Wang", "title": "A High Speed Multi-label Classifier based on Extreme Learning Machines", "comments": "12 pages, 2 figures, 10 tables", "journal-ref": null, "doi": "10.1007/978-3-319-28373-9_37", "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a high speed neural network classifier based on extreme\nlearning machines for multi-label classification problem is proposed and\ndis-cussed. Multi-label classification is a superset of traditional binary and\nmulti-class classification problems. The proposed work extends the extreme\nlearning machine technique to adapt to the multi-label problems. As opposed to\nthe single-label problem, both the number of labels the sample belongs to, and\neach of those target labels are to be identified for multi-label classification\nresulting in in-creased complexity. The proposed high speed multi-label\nclassifier is applied to six benchmark datasets comprising of different\napplication areas such as multi-media, text and biology. The training time and\ntesting time of the classifier are compared with those of the state-of-the-arts\nmethods. Experimental studies show that for all the six datasets, our proposed\ntechnique have faster execution speed and better performance, thereby\noutperforming all the existing multi-label clas-sification methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 14:56:12 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Er", "Meng Joo", ""], ["Venkatesan", "Rajasekar", ""], ["Wang", "Ning", ""]]}, {"id": "1608.08905", "submitter": "Rajasekar Venkatesan", "authors": "Rajasekar Venkatesan, Meng Joo Er, Shiqian Wu, Mahardhika Pratama", "title": "A Novel Online Real-time Classifier for Multi-label Data Streams", "comments": "8 pages, 7 tables, 3 figures. arXiv admin note: text overlap with\n  arXiv:1609.00086", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel extreme learning machine based online multi-label\nclassifier for real-time data streams is proposed. Multi-label classification\nis one of the actively researched machine learning paradigm that has gained\nmuch attention in the recent years due to its rapidly increasing real world\napplications. In contrast to traditional binary and multi-class classification,\nmulti-label classification involves association of each of the input samples\nwith a set of target labels simultaneously. There are no real-time online\nneural network based multi-label classifier available in the literature. In\nthis paper, we exploit the inherent nature of high speed exhibited by the\nextreme learning machines to develop a novel online real-time classifier for\nmulti-label data streams. The developed classifier is experimented with\ndatasets from different application domains for consistency, performance and\nspeed. The experimental studies show that the proposed method outperforms the\nexisting state-of-the-art techniques in terms of speed and accuracy and can\nclassify multi-label data streams in real-time.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 15:14:06 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Venkatesan", "Rajasekar", ""], ["Er", "Meng Joo", ""], ["Wu", "Shiqian", ""], ["Pratama", "Mahardhika", ""]]}, {"id": "1608.08925", "submitter": "Nathan Kallus", "authors": "Nathan Kallus", "title": "Recursive Partitioning for Personalization using Observational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning to choose from m discrete treatment options\n(e.g., news item or medical drug) the one with best causal effect for a\nparticular instance (e.g., user or patient) where the training data consists of\npassive observations of covariates, treatment, and the outcome of the\ntreatment. The standard approach to this problem is regress and compare: split\nthe training data by treatment, fit a regression model in each split, and, for\na new instance, predict all m outcomes and pick the best. By reformulating the\nproblem as a single learning task rather than m separate ones, we propose a new\napproach based on recursively partitioning the data into regimes where\ndifferent treatments are optimal. We extend this approach to an optimal\npartitioning approach that finds a globally optimal partition, achieving a\ncompact, interpretable, and impactful personalization model. We develop new\ntools for validating and evaluating personalization models on observational\ndata and use these to demonstrate the power of our novel approaches in a\npersonalized medicine and a job training application.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 16:20:59 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 22:59:40 GMT"}, {"version": "v3", "created": "Tue, 1 Aug 2017 13:12:19 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Kallus", "Nathan", ""]]}, {"id": "1608.08940", "submitter": "Luis Argerich", "authors": "Luis Argerich, Joaqu\\'in Torr\\'e Zaffaroni, Mat\\'ias J Cano", "title": "Hash2Vec, Feature Hashing for Word Embeddings", "comments": "ASAI 2016, 45JAIIO", "journal-ref": "45 JAIIO - ASAI 2016 - ISSN: 2451-7585 - Pages 33-40", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose the application of feature hashing to create word\nembeddings for natural language processing. Feature hashing has been used\nsuccessfully to create document vectors in related tasks like document\nclassification. In this work we show that feature hashing can be applied to\nobtain word embeddings in linear time with the size of the data. The results\nshow that this algorithm, that does not need training, is able to capture the\nsemantic meaning of words. We compare the results against GloVe showing that\nthey are similar. As far as we know this is the first application of feature\nhashing to the word embeddings problem and the results indicate this is a\nscalable technique with practical results for NLP applications.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 17:01:09 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Argerich", "Luis", ""], ["Zaffaroni", "Joaqu\u00edn Torr\u00e9", ""], ["Cano", "Mat\u00edas J", ""]]}, {"id": "1608.08967", "submitter": "Seyed-Mohsen Moosavi-Dezfooli", "authors": "Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard", "title": "Robustness of classifiers: from adversarial to random noise", "comments": "Accepted to NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent works have shown that state-of-the-art classifiers are\nvulnerable to worst-case (i.e., adversarial) perturbations of the datapoints.\nOn the other hand, it has been empirically observed that these same classifiers\nare relatively robust to random noise. In this paper, we propose to study a\n\\textit{semi-random} noise regime that generalizes both the random and\nworst-case noise regimes. We propose the first quantitative analysis of the\nrobustness of nonlinear classifiers in this general noise regime. We establish\nprecise theoretical bounds on the robustness of classifiers in this general\nregime, which depend on the curvature of the classifier's decision boundary.\nOur bounds confirm and quantify the empirical observations that classifiers\nsatisfying curvature constraints are robust to random noise. Moreover, we\nquantify the robustness of classifiers in terms of the subspace dimension in\nthe semi-random noise regime, and show that our bounds remarkably interpolate\nbetween the worst-case and random noise regimes. We perform experiments and\nshow that the derived bounds provide very accurate estimates when applied to\nvarious state-of-the-art deep neural networks and datasets. This result\nsuggests bounds on the curvature of the classifiers' decision boundaries that\nwe support experimentally, and more generally offers important insights onto\nthe geometry of high dimensional classification problems.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 17:54:34 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Frossard", "Pascal", ""]]}, {"id": "1608.08974", "submitter": "Yash Goyal", "authors": "Yash Goyal, Akrit Mohapatra, Devi Parikh, Dhruv Batra", "title": "Towards Transparent AI Systems: Interpreting Visual Question Answering\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown striking progress and obtained\nstate-of-the-art results in many AI research fields in the recent years.\nHowever, it is often unsatisfying to not know why they predict what they do. In\nthis paper, we address the problem of interpreting Visual Question Answering\n(VQA) models. Specifically, we are interested in finding what part of the input\n(pixels in images or words in questions) the VQA model focuses on while\nanswering the question. To tackle this problem, we use two visualization\ntechniques -- guided backpropagation and occlusion -- to find important words\nin the question and important regions in the image. We then present qualitative\nand quantitative analyses of these importance maps. We found that even without\nexplicit attention mechanisms, VQA models may sometimes be implicitly attending\nto relevant regions in the image, and often to appropriate words in the\nquestion.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 18:11:29 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 19:51:06 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Goyal", "Yash", ""], ["Mohapatra", "Akrit", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1608.08984", "submitter": "Jonathan Ortigosa-Hern\\'andez", "authors": "Jonathan Ortigosa-Hern\\'andez, I\\~naki Inza, Jose A. Lozano", "title": "Towards Competitive Classifiers for Unbalanced Classification Problems:\n  A Study on the Performance Scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although a great methodological effort has been invested in proposing\ncompetitive solutions to the class-imbalance problem, little effort has been\nmade in pursuing a theoretical understanding of this matter.\n  In order to shed some light on this topic, we perform, through a novel\nframework, an exhaustive analysis of the adequateness of the most commonly used\nperformance scores to assess this complex scenario. We conclude that using\nunweighted H\\\"older means with exponent $p \\leq 1$ to average the recalls of\nall the classes produces adequate scores which are capable of determining\nwhether a classifier is competitive.\n  Then, we review the major solutions presented in the class-imbalance\nliterature. Since any learning task can be defined as an optimisation problem\nwhere a loss function, usually connected to a particular score, is minimised,\nour goal, here, is to find whether the learning tasks found in the literature\nare also oriented to maximise the previously detected adequate scores. We\nconclude that they usually maximise the unweighted H\\\"older mean with $p = 1$\n(a-mean).\n  Finally, we provide bounds on the values of the studied performance scores\nwhich guarantee a classifier with a higher recall than the random classifier in\neach and every class.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 18:34:51 GMT"}], "update_date": "2016-09-04", "authors_parsed": [["Ortigosa-Hern\u00e1ndez", "Jonathan", ""], ["Inza", "I\u00f1aki", ""], ["Lozano", "Jose A.", ""]]}, {"id": "1608.09000", "submitter": "Gustavo Soares", "authors": "Reudismam Rolim, Gustavo Soares, Loris D'Antoni, Oleksandr Polozov,\n  Sumit Gulwani, Rohit Gheyi, Ryo Suzuki, Bjoern Hartmann", "title": "Learning Syntactic Program Transformations from Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IDEs, such as Visual Studio, automate common transformations, such as Rename\nand Extract Method refactorings. However, extending these catalogs of\ntransformations is complex and time-consuming. A similar phenomenon appears in\nintelligent tutoring systems where instructors have to write cumbersome code\ntransformations that describe \"common faults\" to fix similar student\nsubmissions to programming assignments. We present REFAZER, a technique for\nautomatically generating program transformations. REFAZER builds on the\nobservation that code edits performed by developers can be used as examples for\nlearning transformations. Example edits may share the same structure but\ninvolve different variables and subexpressions, which must be generalized in a\ntransformation at the right level of abstraction. To learn transformations,\nREFAZER leverages state-of-the-art programming-by-example methodology using the\nfollowing key components: (a) a novel domain-specific language (DSL) for\ndescribing program transformations, (b) domain-specific deductive algorithms\nfor synthesizing transformations in the DSL, and (c) functions for ranking the\nsynthesized transformations. We instantiate and evaluate REFAZER in two\ndomains. First, given examples of edits used by students to fix incorrect\nprogramming assignment submissions, we learn transformations that can fix other\nstudents' submissions with similar faults. In our evaluation conducted on 4\nprogramming tasks performed by 720 students, our technique helped to fix\nincorrect submissions for 87% of the students. In the second domain, we use\nrepetitive edits applied by developers to the same project to synthesize a\nprogram transformation that applies these edits to other locations in the code.\nIn our evaluation conducted on 59 scenarios of repetitive edits taken from 3 C#\nopen-source projects, REFAZER learns the intended program transformation in 83%\nof the cases.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 19:06:06 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Rolim", "Reudismam", ""], ["Soares", "Gustavo", ""], ["D'Antoni", "Loris", ""], ["Polozov", "Oleksandr", ""], ["Gulwani", "Sumit", ""], ["Gheyi", "Rohit", ""], ["Suzuki", "Ryo", ""], ["Hartmann", "Bjoern", ""]]}, {"id": "1608.09014", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin and Karthik Sridharan", "title": "A Tutorial on Online Supervised Learning with Applications to Node\n  Classification in Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the elegant observation of T. Cover '65 which, perhaps, is not as\nwell-known to the broader community as it should be. The first goal of the\ntutorial is to explain---through the prism of this elementary result---how to\nsolve certain sequence prediction problems by modeling sets of solutions rather\nthan the unknown data-generating mechanism. We extend Cover's observation in\nseveral directions and focus on computational aspects of the proposed\nalgorithms. The applicability of the methods is illustrated on several\nexamples, including node classification in a network.\n  The second aim of this tutorial is to demonstrate the following phenomenon:\nit is possible to predict as well as a combinatorial \"benchmark\" for which we\nhave a certain multiplicative approximation algorithm, even if the exact\ncomputation of the benchmark given all the data is NP-hard. The proposed\nprediction methods, therefore, circumvent some of the computational\ndifficulties associated with finding the best model given the data. These\ndifficulties arise rather quickly when one attempts to develop a probabilistic\nmodel for graph-based or other problems with a combinatorial structure.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 19:55:35 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}]