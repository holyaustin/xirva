[{"id": "0807.0093", "submitter": "Karsten Borgwardt", "authors": "S.V.N. Vishwanathan, Karsten M. Borgwardt, Imre Risi Kondor, Nicol N.\n  Schraudolph", "title": "Graph Kernels", "comments": "http://jmlr.csail.mit.edu/papers/v11/vishwanathan10a.html", "journal-ref": "Journal of Machine Learning Research 11 (Apr): 1201-1242, 2010", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework to study graph kernels, special cases of which\ninclude the random walk graph kernel \\citep{GaeFlaWro03,BorOngSchVisetal05},\nmarginalized graph kernel \\citep{KasTsuIno03,KasTsuIno04,MahUedAkuPeretal04},\nand geometric kernel on graphs \\citep{Gaertner02}. Through extensions of linear\nalgebra to Reproducing Kernel Hilbert Spaces (RKHS) and reduction to a\nSylvester equation, we construct an algorithm that improves the time complexity\nof kernel computation from $O(n^6)$ to $O(n^3)$. When the graphs are sparse,\nconjugate gradient solvers or fixed-point iterations bring our algorithm into\nthe sub-cubic domain. Experiments on graphs from bioinformatics and other\napplication domains show that it is often more than a thousand times faster\nthan previous approaches. We then explore connections between diffusion kernels\n\\citep{KonLaf02}, regularization on graphs \\citep{SmoKon03}, and graph kernels,\nand use these connections to propose new graph kernels. Finally, we show that\nrational kernels \\citep{CorHafMoh02,CorHafMoh03,CorHafMoh04} when specialized\nto graphs reduce to the random walk graph kernel.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2008 09:46:14 GMT"}], "update_date": "2010-11-30", "authors_parsed": [["Vishwanathan", "S. V. N.", ""], ["Borgwardt", "Karsten M.", ""], ["Kondor", "Imre Risi", ""], ["Schraudolph", "Nicol N.", ""]]}, {"id": "0807.1005", "submitter": "Peter Grunwald", "authors": "Tim van Erven, Peter Grunwald and Steven de Rooij", "title": "Catching Up Faster by Switching Sooner: A Prequential Solution to the\n  AIC-BIC Dilemma", "comments": "A preliminary version of a part of this paper appeared at the NIPS\n  2007 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model averaging, model selection and its approximations such as BIC\nare generally statistically consistent, but sometimes achieve slower rates og\nconvergence than other methods such as AIC and leave-one-out cross-validation.\nOn the other hand, these other methods can br inconsistent. We identify the\n\"catch-up phenomenon\" as a novel explanation for the slow convergence of\nBayesian methods. Based on this analysis we define the switch distribution, a\nmodification of the Bayesian marginal distribution. We show that, under broad\nconditions,model selection and prediction based on the switch distribution is\nboth consistent and achieves optimal convergence rates, thereby resolving the\nAIC-BIC dilemma. The method is practical; we give an efficient implementation.\nThe switch distribution has a data compression interpretation, and can thus be\nviewed as a \"prequential\" or MDL method; yet it is different from the MDL\nmethods that are usually considered in the literature. We compare the switch\ndistribution to Bayes factor model selection and leave-one-out\ncross-validation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2008 12:57:23 GMT"}], "update_date": "2008-09-17", "authors_parsed": [["van Erven", "Tim", ""], ["Grunwald", "Peter", ""], ["de Rooij", "Steven", ""]]}, {"id": "0807.1494", "submitter": "Matteo Gagliolo", "authors": "Matteo Gagliolo and Juergen Schmidhuber", "title": "Algorithm Selection as a Bandit Problem with Unbounded Losses", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": "10.1007/978-3-642-13800-3_7", "report-no": "IDSIA-07-08", "categories": "cs.AI cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithm selection is typically based on models of algorithm performance,\nlearned during a separate offline training sequence, which can be prohibitively\nexpensive. In recent work, we adopted an online approach, in which a\nperformance model is iteratively updated and used to guide selection on a\nsequence of problem instances. The resulting exploration-exploitation trade-off\nwas represented as a bandit problem with expert advice, using an existing\nsolver for this game, but this required the setting of an arbitrary bound on\nalgorithm runtimes, thus invalidating the optimal regret of the solver. In this\npaper, we propose a simpler framework for representing algorithm selection as a\nbandit problem, with partial information, and an unknown bound on losses. We\nadapt an existing solver to this game, proving a bound on its expected regret,\nwhich holds also for the resulting algorithm selection technique. We present\npreliminary experiments with a set of SAT solvers on a mixed SAT-UNSAT\nbenchmark.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2008 16:47:36 GMT"}], "update_date": "2013-01-31", "authors_parsed": [["Gagliolo", "Matteo", ""], ["Schmidhuber", "Juergen", ""]]}, {"id": "0807.1997", "submitter": "Zhi-Hua Zhou", "authors": "Zhi-Hua Zhou, Yu-Yin Sun, Yu-Feng Li", "title": "Multi-Instance Learning by Treating Instances As Non-I.I.D. Samples", "comments": "ICML, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-instance learning attempts to learn from a training set consisting of\nlabeled bags each containing many unlabeled instances. Previous studies\ntypically treat the instances in the bags as independently and identically\ndistributed. However, the instances in a bag are rarely independent, and\ntherefore a better performance can be expected if the instances are treated in\nan non-i.i.d. way that exploits the relations among instances. In this paper,\nwe propose a simple yet effective multi-instance learning method, which regards\neach bag as a graph and uses a specific kernel to distinguish the graphs by\nconsidering the features of the nodes as well as the features of the edges that\nconvey some relations among instances. The effectiveness of the proposed method\nis validated by experiments.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jul 2008 20:19:18 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2009 17:22:40 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2009 08:43:03 GMT"}, {"version": "v4", "created": "Wed, 13 May 2009 16:22:00 GMT"}], "update_date": "2009-05-13", "authors_parsed": [["Zhou", "Zhi-Hua", ""], ["Sun", "Yu-Yin", ""], ["Li", "Yu-Feng", ""]]}, {"id": "0807.2677", "submitter": "Jayakrishnan Unnikrishnan", "authors": "Jayakrishnan Unnikrishnan and Venugopal Veeravalli", "title": "Algorithms for Dynamic Spectrum Access with Learning for Cognitive Radio", "comments": "Published in IEEE Transactions on Signal Processing, February 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of dynamic spectrum sensing and access in cognitive\nradio systems as a partially observed Markov decision process (POMDP). A group\nof cognitive users cooperatively tries to exploit vacancies in primary\n(licensed) channels whose occupancies follow a Markovian evolution. We first\nconsider the scenario where the cognitive users have perfect knowledge of the\ndistribution of the signals they receive from the primary users. For this\nproblem, we obtain a greedy channel selection and access policy that maximizes\nthe instantaneous reward, while satisfying a constraint on the probability of\ninterfering with licensed transmissions. We also derive an analytical universal\nupper bound on the performance of the optimal policy. Through simulation, we\nshow that our scheme achieves good performance relative to the upper bound and\nimproved performance relative to an existing scheme.\n  We then consider the more practical scenario where the exact distribution of\nthe signal from the primary is unknown. We assume a parametric model for the\ndistribution and develop an algorithm that can learn the true distribution,\nstill guaranteeing the constraint on the interference probability. We show that\nthis algorithm outperforms the naive design that assumes a worst case value for\nthe parameter. We also provide a proof for the convergence of the learning\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2008 23:59:28 GMT"}, {"version": "v2", "created": "Fri, 21 Nov 2008 04:08:02 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2009 20:30:19 GMT"}, {"version": "v4", "created": "Sat, 6 Feb 2010 21:48:51 GMT"}], "update_date": "2010-02-06", "authors_parsed": [["Unnikrishnan", "Jayakrishnan", ""], ["Veeravalli", "Venugopal", ""]]}, {"id": "0807.2983", "submitter": "Marc Tommasi", "authors": "Fran\\c{c}ois Denis (LIF), Amaury Habrard (LIF), R\\'emi Gilleron (LIFL,\n  INRIA Futurs), Marc Tommasi (LIFL, INRIA Futurs, GRAPPA), \\'Edouard Gilbert\n  (INRIA Futurs)", "title": "On Probability Distributions for Trees: Representations, Inference and\n  Learning", "comments": null, "journal-ref": "Dans NIPS Workshop on Representations and Inference on Probability\n  Distributions (2007)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study probability distributions over free algebras of trees. Probability\ndistributions can be seen as particular (formal power) tree series [Berstel et\nal 82, Esik et al 03], i.e. mappings from trees to a semiring K . A widely\nstudied class of tree series is the class of rational (or recognizable) tree\nseries which can be defined either in an algebraic way or by means of\nmultiplicity tree automata. We argue that the algebraic representation is very\nconvenient to model probability distributions over a free algebra of trees.\nFirst, as in the string case, the algebraic representation allows to design\nlearning algorithms for the whole class of probability distributions defined by\nrational tree series. Note that learning algorithms for rational tree series\ncorrespond to learning algorithms for weighted tree automata where both the\nstructure and the weights are learned. Second, the algebraic representation can\nbe easily extended to deal with unranked trees (like XML trees where a symbol\nmay have an unbounded number of children). Both properties are particularly\nrelevant for applications: nondeterministic automata are required for the\ninference problem to be relevant (recall that Hidden Markov Models are\nequivalent to nondeterministic string automata); nowadays applications for Web\nInformation Extraction, Web Services and document processing consider unranked\ntrees.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2008 14:41:44 GMT"}], "update_date": "2008-07-21", "authors_parsed": [["Denis", "Fran\u00e7ois", "", "LIF"], ["Habrard", "Amaury", "", "LIF"], ["Gilleron", "R\u00e9mi", "", "LIFL,\n  INRIA Futurs"], ["Tommasi", "Marc", "", "LIFL, INRIA Futurs, GRAPPA"], ["Gilbert", "\u00c9douard", "", "INRIA Futurs"]]}, {"id": "0807.3396", "submitter": "Kamakshi Sivaramakrishnan", "authors": "Kamakshi Sivaramakrishnan and Tsachy Weissman", "title": "Universal Denoising of Discrete-time Continuous-Amplitude Signals", "comments": "56 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reconstructing a discrete-time signal (sequence)\nwith continuous-valued components corrupted by a known memoryless channel. When\nperformance is measured using a per-symbol loss function satisfying mild\nregularity conditions, we develop a sequence of denoisers that, although\nindependent of the distribution of the underlying `clean' sequence, is\nuniversally optimal in the limit of large sequence length. This sequence of\ndenoisers is universal in the sense of performing as well as any sliding window\ndenoising scheme which may be optimized for the underlying clean signal. Our\nresults are initially developed in a ``semi-stochastic'' setting, where the\nnoiseless signal is an unknown individual sequence, and the only source of\nrandomness is due to the channel noise. It is subsequently shown that in the\nfully stochastic setting, where the noiseless sequence is a stationary\nstochastic process, our schemes universally attain optimum performance. The\nproposed schemes draw from nonparametric density estimation techniques and are\npractically implementable. We demonstrate efficacy of the proposed schemes in\ndenoising gray-scale images in the conventional additive white Gaussian noise\nsetting, with additional promising results for less conventional noise\ndistributions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2008 07:42:11 GMT"}], "update_date": "2008-07-23", "authors_parsed": [["Sivaramakrishnan", "Kamakshi", ""], ["Weissman", "Tsachy", ""]]}, {"id": "0807.4198", "submitter": "Brian Vogel", "authors": "Brian K. Vogel", "title": "Positive factor networks: A graphical framework for modeling\n  non-negative sequential data", "comments": "Minor editing of the abstract, introduction, and concluding sections\n  to improve readability and remove redundant wording, based on feedback from a\n  reviewer. No changes were made to the material presented nor to the results.\n  Added an acknowledgment section to thank the reviewer. Corrected minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel graphical framework for modeling non-negative sequential\ndata with hierarchical structure. Our model corresponds to a network of coupled\nnon-negative matrix factorization (NMF) modules, which we refer to as a\npositive factor network (PFN). The data model is linear, subject to\nnon-negativity constraints, so that observation data consisting of an additive\ncombination of individually representable observations is also representable by\nthe network. This is a desirable property for modeling problems in\ncomputational auditory scene analysis, since distinct sound sources in the\nenvironment are often well-modeled as combining additively in the corresponding\nmagnitude spectrogram. We propose inference and learning algorithms that\nleverage existing NMF algorithms and that are straightforward to implement. We\npresent a target tracking example and provide results for synthetic observation\ndata which serve to illustrate the interesting properties of PFNs and motivate\ntheir potential usefulness in applications such as music transcription, source\nseparation, and speech recognition. We show how a target process characterized\nby a hierarchical state transition model can be represented as a PFN. Our\nresults illustrate that a PFN which is defined in terms of a single target\nobservation can then be used to effectively track the states of multiple\nsimultaneous targets. Our results show that the quality of the inferred target\nstates degrades gradually as the observation noise is increased. We also\npresent results for an example in which meaningful hierarchical features are\nextracted from a spectrogram. Such a hierarchical representation could be\nuseful for music transcription and source separation applications. We also\npropose a network for language modeling.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2008 22:50:46 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2009 00:30:26 GMT"}], "update_date": "2009-07-16", "authors_parsed": [["Vogel", "Brian K.", ""]]}]