[{"id": "1108.0017", "submitter": "Parasaran Raman", "authors": "Jeff M. Phillips, Parasaran Raman, and Suresh Venkatasubramanian", "title": "Generating a Diverse Set of High-Quality Clusterings", "comments": "12 Pages, 5 Figures, 2nd MultiClust Workshop at ECML PKDD 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new framework for generating multiple good quality partitions\n(clusterings) of a single data set. Our approach decomposes this problem into\ntwo components, generating many high-quality partitions, and then grouping\nthese partitions to obtain k representatives. The decomposition makes the\napproach extremely modular and allows us to optimize various criteria that\ncontrol the choice of representative partitions.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2011 21:07:51 GMT"}], "update_date": "2011-08-02", "authors_parsed": [["Phillips", "Jeff M.", ""], ["Raman", "Parasaran", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1108.0039", "submitter": "Atilim Gunes Baydin", "authors": "Atilim Gunes Baydin, Ramon Lopez de Mantaras, Simeon Simoff, Carles\n  Sierra", "title": "CBR with Commonsense Reasoning and Structure Mapping: An Application to\n  Mediation", "comments": "15 pages, 3 figures; updated copyright notice", "journal-ref": "Case-Based Reasoning Research and Development, LNCS (LNAI), Volume\n  6880 (2011), 378-392, Springer Berlin / Heidelberg, ISBN: 978-3-642-23290-9", "doi": "10.1007/978-3-642-23291-6_28", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation is an important method in dispute resolution. We implement a case\nbased reasoning approach to mediation integrating analogical and commonsense\nreasoning components that allow an artificial mediation agent to satisfy\nrequirements expected from a human mediator, in particular: utilizing\nexperience with cases in different domains; and structurally transforming the\nset of issues for a better solution. We utilize a case structure based on\nontologies reflecting the perceptions of the parties in dispute. The analogical\nreasoning component, employing the Structure Mapping Theory from psychology,\nprovides a flexibility to respond innovatively in unusual circumstances, in\ncontrast with conventional approaches confined into specialized problem\ndomains. We aim to build a mediation case base incorporating real world\ninstances ranging from interpersonal or intergroup disputes to international\nconflicts.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2011 05:12:17 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2011 20:11:51 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Baydin", "Atilim Gunes", ""], ["de Mantaras", "Ramon Lopez", ""], ["Simoff", "Simeon", ""], ["Sierra", "Carles", ""]]}, {"id": "1108.0775", "submitter": "Francis Bach", "authors": "Francis Bach (LIENS, INRIA Paris - Rocquencourt), Rodolphe Jenatton\n  (LIENS, INRIA Paris - Rocquencourt), Julien Mairal, Guillaume Obozinski\n  (LIENS, INRIA Paris - Rocquencourt)", "title": "Optimization with Sparsity-Inducing Penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse estimation methods are aimed at using or obtaining parsimonious\nrepresentations of data or models. They were first dedicated to linear variable\nselection but numerous extensions have now emerged such as structured sparsity\nor kernel selection. It turns out that many of the related estimation problems\ncan be cast as convex optimization problems by regularizing the empirical risk\nwith appropriate non-smooth norms. The goal of this paper is to present from a\ngeneral perspective optimization tools and techniques dedicated to such\nsparsity-inducing penalties. We cover proximal methods, block-coordinate\ndescent, reweighted $\\ell_2$-penalized techniques, working-set and homotopy\nmethods, as well as non-convex formulations and extensions, and provide an\nextensive set of experiments to compare various algorithms from a computational\npoint of view.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2011 07:55:19 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2011 09:59:21 GMT"}], "update_date": "2011-11-24", "authors_parsed": [["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"], ["Jenatton", "Rodolphe", "", "LIENS, INRIA Paris - Rocquencourt"], ["Mairal", "Julien", "", "LIENS, INRIA Paris - Rocquencourt"], ["Obozinski", "Guillaume", "", "LIENS, INRIA Paris - Rocquencourt"]]}, {"id": "1108.0895", "submitter": "Ping Li", "authors": "Ping Li and Christian Konig", "title": "Accurate Estimators for Improving Minwise Hashing and b-Bit Minwise\n  Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minwise hashing is the standard technique in the context of search and\ndatabases for efficiently estimating set (e.g., high-dimensional 0/1 vector)\nsimilarities. Recently, b-bit minwise hashing was proposed which significantly\nimproves upon the original minwise hashing in practice by storing only the\nlowest b bits of each hashed value, as opposed to using 64 bits. b-bit hashing\nis particularly effective in applications which mainly concern sets of high\nsimilarities (e.g., the resemblance >0.5). However, there are other important\napplications in which not just pairs of high similarities matter. For example,\nmany learning algorithms require all pairwise similarities and it is expected\nthat only a small fraction of the pairs are similar. Furthermore, many\napplications care more about containment (e.g., how much one object is\ncontained by another object) than the resemblance. In this paper, we show that\nthe estimators for minwise hashing and b-bit minwise hashing used in the\ncurrent practice can be systematically improved and the improvements are most\nsignificant for set pairs of low resemblance and high containment.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2011 17:08:11 GMT"}], "update_date": "2011-08-04", "authors_parsed": [["Li", "Ping", ""], ["Konig", "Christian", ""]]}, {"id": "1108.1636", "submitter": "Peng Zhang", "authors": "Peng Zhang, Yuanyuan Ren, and Bo Zhang", "title": "A new embedding quality assessment method for manifold learning", "comments": "16 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold learning is a hot research topic in the field of computer science. A\ncrucial issue with current manifold learning methods is that they lack a\nnatural quantitative measure to assess the quality of learned embeddings, which\ngreatly limits their applications to real-world problems. In this paper, a new\nembedding quality assessment method for manifold learning, named as\nNormalization Independent Embedding Quality Assessment (NIEQA), is proposed.\nCompared with current assessment methods which are limited to isometric\nembeddings, the NIEQA method has a much larger application range due to two\nfeatures. First, it is based on a new measure which can effectively evaluate\nhow well local neighborhood geometry is preserved under normalization, hence it\ncan be applied to both isometric and normalized embeddings. Second, it can\nprovide both local and global evaluations to output an overall assessment.\nTherefore, NIEQA can serve as a natural tool in model selection and evaluation\ntasks for manifold learning. Experimental results on benchmark data sets\nvalidate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2011 08:59:05 GMT"}], "update_date": "2011-08-09", "authors_parsed": [["Zhang", "Peng", ""], ["Ren", "Yuanyuan", ""], ["Zhang", "Bo", ""]]}, {"id": "1108.1766", "submitter": "Steve Hanneke", "authors": "Steve Hanneke", "title": "Activized Learning: Transforming Passive to Active with Improved Label\n  Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the theoretical advantages of active learning over passive learning.\nSpecifically, we prove that, in noise-free classifier learning for VC classes,\nany passive learning algorithm can be transformed into an active learning\nalgorithm with asymptotically strictly superior label complexity for all\nnontrivial target functions and distributions. We further provide a general\ncharacterization of the magnitudes of these improvements in terms of a novel\ngeneralization of the disagreement coefficient. We also extend these results to\nactive learning in the presence of label noise, and find that even under broad\nclasses of noise distributions, we can typically guarantee strict improvements\nover the known results for passive learning.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2011 18:04:02 GMT"}], "update_date": "2011-08-09", "authors_parsed": [["Hanneke", "Steve", ""]]}, {"id": "1108.2054", "submitter": "Fabrizio Angiulli", "authors": "Fabrizio Angiulli and Fabio Fassetti", "title": "Uncertain Nearest Neighbor Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work deals with the problem of classifying uncertain data. With this aim\nthe Uncertain Nearest Neighbor (UNN) rule is here introduced, which represents\nthe generalization of the deterministic nearest neighbor rule to the case in\nwhich uncertain objects are available. The UNN rule relies on the concept of\nnearest neighbor class, rather than on that of nearest neighbor object. The\nnearest neighbor class of a test object is the class that maximizes the\nprobability of providing its nearest neighbor. It is provided evidence that the\nformer concept is much more powerful than the latter one in the presence of\nuncertainty, in that it correctly models the right semantics of the nearest\nneighbor decision rule when applied to the uncertain scenario. An effective and\nefficient algorithm to perform uncertain nearest neighbor classification of a\ngeneric (un)certain test object is designed, based on properties that greatly\nreduce the temporal cost associated with nearest neighbor class probability\ncomputation. Experimental results are presented, showing that the UNN rule is\neffective and efficient in classifying uncertain data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2011 21:28:42 GMT"}], "update_date": "2011-08-11", "authors_parsed": [["Angiulli", "Fabrizio", ""], ["Fassetti", "Fabio", ""]]}, {"id": "1108.2283", "submitter": "Federico Schl\\\"uter", "authors": "Federico Schl\\\"uter", "title": "A survey on independence-based Markov networks learning", "comments": "35 pages, 1 figure", "journal-ref": "Schl\\\"uter, F. (2011). A survey on independence-based Markov\n  networks learning. Artificial Intelligence Review, 1-25", "doi": "10.1007/s10462-012-9346-y", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work reports the most relevant technical aspects in the problem of\nlearning the \\emph{Markov network structure} from data. Such problem has become\nincreasingly important in machine learning, and many other application fields\nof machine learning. Markov networks, together with Bayesian networks, are\nprobabilistic graphical models, a widely used formalism for handling\nprobability distributions in intelligent systems. Learning graphical models\nfrom data have been extensively applied for the case of Bayesian networks, but\nfor Markov networks learning it is not tractable in practice. However, this\nsituation is changing with time, given the exponential growth of computers\ncapacity, the plethora of available digital data, and the researching on new\nlearning technologies. This work stresses on a technology called\nindependence-based learning, which allows the learning of the independence\nstructure of those networks from data in an efficient and sound manner,\nwhenever the dataset is sufficiently large, and data is a representative\nsampling of the target distribution. In the analysis of such technology, this\nwork surveys the current state-of-the-art algorithms for learning Markov\nnetworks structure, discussing its current limitations, and proposing a series\nof open problems where future works may produce some advances in the area in\nterms of quality and efficiency. The paper concludes by opening a discussion\nabout how to develop a general formalism for improving the quality of the\nstructures learned, when data is scarce.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2011 20:25:08 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2013 19:15:05 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Schl\u00fcter", "Federico", ""]]}, {"id": "1108.2486", "submitter": "Duncan Blythe", "authors": "Duncan Blythe, Paul von B\\\"unau, Frank Meinecke, Klaus-Robert M\\\"uller", "title": "Feature Extraction for Change-Point Detection using Stationary Subspace\n  Analysis", "comments": "24 pages, 20 figures, journal preprint", "journal-ref": null, "doi": "10.1109/TNNLS.2012.2185811", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting changes in high-dimensional time series is difficult because it\ninvolves the comparison of probability densities that need to be estimated from\nfinite samples. In this paper, we present the first feature extraction method\ntailored to change point detection, which is based on an extended version of\nStationary Subspace Analysis. We reduce the dimensionality of the data to the\nmost non-stationary directions, which are most informative for detecting state\nchanges in the time series. In extensive simulations on synthetic data we show\nthat the accuracy of three change point detection algorithms is significantly\nincreased by a prior feature extraction step. These findings are confirmed in\nan application to industrial fault monitoring.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2011 18:54:02 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Blythe", "Duncan", ""], ["von B\u00fcnau", "Paul", ""], ["Meinecke", "Frank", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1108.2580", "submitter": "Danny Bickson", "authors": "Yao Wu, Qiang Yan, Danny Bickson, Yucheng Low, Qing Yang", "title": "Efficient Multicore Collaborative Filtering", "comments": "In ACM KDD CUP Workshop 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the solution method taken by LeBuSiShu team for track1\nin ACM KDD CUP 2011 contest (resulting in the 5th place). We identified two\nmain challenges: the unique item taxonomy characteristics as well as the large\ndata set size.To handle the item taxonomy, we present a novel method called\nMatrix Factorization Item Taxonomy Regularization (MFITR). MFITR obtained the\n2nd best prediction result out of more then ten implemented algorithms. For\nrapidly computing multiple solutions of various algorithms, we have implemented\nan open source parallel collaborative filtering library on top of the GraphLab\nmachine learning framework. We report some preliminary performance results\nobtained using the BlackLight supercomputer.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2011 07:22:08 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2011 11:14:42 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Wu", "Yao", ""], ["Yan", "Qiang", ""], ["Bickson", "Danny", ""], ["Low", "Yucheng", ""], ["Yang", "Qing", ""]]}, {"id": "1108.2820", "submitter": "Marina Sapir", "authors": "Marina Sapir", "title": "Ensemble Risk Modeling Method for Robust Learning on Scarce Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical risk modeling, typical data are \"scarce\": they have relatively\nsmall number of training instances (N), censoring, and high dimensionality (M).\nWe show that the problem may be effectively simplified by reducing it to\nbipartite ranking, and introduce new bipartite ranking algorithm, Smooth Rank,\nfor robust learning on scarce data. The algorithm is based on ensemble learning\nwith unsupervised aggregation of predictors. The advantage of our approach is\nconfirmed in comparison with two \"gold standard\" risk modeling methods on 10\nreal life survival analysis datasets, where the new approach has the best\nresults on all but two datasets with the largest ratio N/M. For systematic\nstudy of the effects of data scarcity on modeling by all three methods, we\nconducted two types of computational experiments: on real life data with\nrandomly drawn training sets of different sizes, and on artificial data with\nincreasing number of features. Both experiments demonstrated that Smooth Rank\nhas critical advantage over the popular methods on the scarce data; it does not\nsuffer from overfitting where other methods do.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2011 20:47:30 GMT"}, {"version": "v2", "created": "Sat, 28 Jan 2012 07:51:50 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Sapir", "Marina", ""]]}, {"id": "1108.2840", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an, Geoffrey J. Goodhill", "title": "Generalised elastic nets", "comments": "52 pages, 16 figures. Original manuscript dated August 14, 2003 and\n  not updated since. Current authors' email addresses:\n  mcarreira-perpinan@ucmerced.edu, g.goodhill@uq.edu.au", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The elastic net was introduced as a heuristic algorithm for combinatorial\noptimisation and has been applied, among other problems, to biological\nmodelling. It has an energy function which trades off a fitness term against a\ntension term. In the original formulation of the algorithm the tension term was\nimplicitly based on a first-order derivative. In this paper we generalise the\nelastic net model to an arbitrary quadratic tension term, e.g. derived from a\ndiscretised differential operator, and give an efficient learning algorithm. We\nrefer to these as generalised elastic nets (GENs). We give a theoretical\nanalysis of the tension term for 1D nets with periodic boundary conditions, and\nshow that the model is sensitive to the choice of finite difference scheme that\nrepresents the discretised derivative. We illustrate some of these issues in\nthe context of cortical map models, by relating the choice of tension term to a\ncortical interaction function. In particular, we prove that this interaction\ntakes the form of a Mexican hat for the original elastic net, and of\nprogressively more oscillatory Mexican hats for higher-order derivatives. The\nresults apply not only to generalised elastic nets but also to other methods\nusing discrete differential penalties, and are expected to be useful in other\nareas, such as data analysis, computer graphics and optimisation problems.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2011 03:47:14 GMT"}], "update_date": "2011-08-16", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Goodhill", "Geoffrey J.", ""]]}, {"id": "1108.3072", "submitter": "Ping Li", "authors": "Ping Li, Anshumali Shrivastava, Christian Konig", "title": "Training Logistic Regression and SVM on 200GB Data Using b-Bit Minwise\n  Hashing and Comparisons with Vowpal Wabbit (VW)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generated a dataset of 200 GB with 10^9 features, to test our recent b-bit\nminwise hashing algorithms for training very large-scale logistic regression\nand SVM. The results confirm our prior work that, compared with the VW hashing\nalgorithm (which has the same variance as random projections), b-bit minwise\nhashing is substantially more accurate at the same storage. For example, with\nmerely 30 hashed values per data point, b-bit minwise hashing can achieve\nsimilar accuracies as VW with 2^14 hashed values per data point.\n  We demonstrate that the preprocessing cost of b-bit minwise hashing is\nroughly on the same order of magnitude as the data loading time. Furthermore,\nby using a GPU, the preprocessing cost can be reduced to a small fraction of\nthe data loading time.\n  Minwise hashing has been widely used in industry, at least in the context of\nsearch. One reason for its popularity is that one can efficiently simulate\npermutations by (e.g.,) universal hashing. In other words, there is no need to\nstore the permutation matrix. In this paper, we empirically verify this\npractice, by demonstrating that even using the simplest 2-universal hashing\ndoes not degrade the learning performance.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2011 19:53:55 GMT"}], "update_date": "2011-08-16", "authors_parsed": [["Li", "Ping", ""], ["Shrivastava", "Anshumali", ""], ["Konig", "Christian", ""]]}, {"id": "1108.3154", "submitter": "Stephane Ross", "authors": "Stephane Ross, J. Andrew Bagnell", "title": "Stability Conditions for Online Learnability", "comments": "16 pages. Earlier version of this work submitted (but rejected) to\n  COLT 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stability is a general notion that quantifies the sensitivity of a learning\nalgorithm's output to small change in the training dataset (e.g. deletion or\nreplacement of a single training sample). Such conditions have recently been\nshown to be more powerful to characterize learnability in the general learning\nsetting under i.i.d. samples where uniform convergence is not necessary for\nlearnability, but where stability is both sufficient and necessary for\nlearnability. We here show that similar stability conditions are also\nsufficient for online learnability, i.e. whether there exists a learning\nalgorithm such that under any sequence of examples (potentially chosen\nadversarially) produces a sequence of hypotheses that has no regret in the\nlimit with respect to the best hypothesis in hindsight. We introduce online\nstability, a stability condition related to uniform-leave-one-out stability in\nthe batch setting, that is sufficient for online learnability. In particular we\nshow that popular classes of online learners, namely algorithms that fall in\nthe category of Follow-the-(Regularized)-Leader, Mirror Descent, gradient-based\nmethods and randomized algorithms like Weighted Majority and Hedge, are\nguaranteed to have no regret if they have such online stability property. We\nprovide examples that suggest the existence of an algorithm with such stability\ncondition might in fact be necessary for online learnability. For the more\nrestricted binary classification setting, we establish that such stability\ncondition is in fact both sufficient and necessary. We also show that for a\nlarge class of online learnable problems in the general learning setting,\nnamely those with a notion of sub-exponential covering, no-regret online\nalgorithms that have such stability condition exists.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 05:11:54 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2011 17:01:35 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Ross", "Stephane", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1108.3259", "submitter": "Souhaib Ben Taieb", "authors": "Souhaib Ben Taieb and Gianluca Bontempi and Amir Atiya and Antti\n  Sorjamaa", "title": "A review and comparison of strategies for multi-step ahead time series\n  forecasting based on the NN5 forecasting competition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-step ahead forecasting is still an open challenge in time series\nforecasting. Several approaches that deal with this complex problem have been\nproposed in the literature but an extensive comparison on a large number of\ntasks is still missing. This paper aims to fill this gap by reviewing existing\nstrategies for multi-step ahead forecasting and comparing them in theoretical\nand practical terms. To attain such an objective, we performed a large scale\ncomparison of these different strategies using a large experimental benchmark\n(namely the 111 series from the NN5 forecasting competition). In addition, we\nconsidered the effects of deseasonalization, input variable selection, and\nforecast combination on these strategies and on multi-step ahead forecasting at\nlarge. The following three findings appear to be consistently supported by the\nexperimental results: Multiple-Output strategies are the best performing\napproaches, deseasonalization leads to uniformly improved forecast accuracy,\nand input selection is more effective when performed in conjunction with\ndeseasonalization.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 14:55:20 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Taieb", "Souhaib Ben", ""], ["Bontempi", "Gianluca", ""], ["Atiya", "Amir", ""], ["Sorjamaa", "Antti", ""]]}, {"id": "1108.3298", "submitter": "Nando de Freitas", "authors": "Byron Knoll, Nando de Freitas", "title": "A Machine Learning Perspective on Predictive Coding with PAQ", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PAQ8 is an open source lossless data compression algorithm that currently\nachieves the best compression rates on many benchmarks. This report presents a\ndetailed description of PAQ8 from a statistical machine learning perspective.\nIt shows that it is possible to understand some of the modules of PAQ8 and use\nthis understanding to improve the method. However, intuitive statistical\nexplanations of the behavior of other modules remain elusive. We hope the\ndescription in this report will be a starting point for discussions that will\nincrease our understanding, lead to improvements to PAQ8, and facilitate a\ntransfer of knowledge from PAQ8 to other machine learning methods, such a\nrecurrent neural networks and stochastic memoizers. Finally, the report\npresents a broad range of new applications of PAQ to machine learning tasks\nincluding language modeling and adaptive text prediction, adaptive game\nplaying, classification, and compression using features from the field of deep\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 18:06:29 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Knoll", "Byron", ""], ["de Freitas", "Nando", ""]]}, {"id": "1108.3372", "submitter": "Miguel L\\'azaro Gredilla", "authors": "Miguel L\\'azaro-Gredilla, Steven Van Vaerenbergh, and Neil Lawrence", "title": "Overlapping Mixtures of Gaussian Processes for the Data Association\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a mixture of GPs to address the data association\nproblem, i.e. to label a group of observations according to the sources that\ngenerated them. Unlike several previously proposed GP mixtures, the novel\nmixture has the distinct characteristic of using no gating function to\ndetermine the association of samples and mixture components. Instead, all the\nGPs in the mixture are global and samples are clustered following\n\"trajectories\" across input space. We use a non-standard variational Bayesian\nalgorithm to efficiently recover sample labels and learn the hyperparameters.\nWe show how multi-object tracking problems can be disambiguated and also\nexplore the characteristics of the model in traditional regression settings.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 23:46:59 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["L\u00e1zaro-Gredilla", "Miguel", ""], ["Van Vaerenbergh", "Steven", ""], ["Lawrence", "Neil", ""]]}, {"id": "1108.3446", "submitter": "Josef Urban", "authors": "Jesse Alama, Tom Heskes, Daniel K\\\"uhlwein, Evgeni Tsivtsivadze, and\n  Josef Urban", "title": "Premise Selection for Mathematics by Corpus Analysis and Kernel Methods", "comments": "26 pages", "journal-ref": null, "doi": "10.1007/s10817-013-9286-5", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart premise selection is essential when using automated reasoning as a tool\nfor large-theory formal proof development. A good method for premise selection\nin complex mathematical libraries is the application of machine learning to\nlarge corpora of proofs. This work develops learning-based premise selection in\ntwo ways. First, a newly available minimal dependency analysis of existing\nhigh-level formal mathematical proofs is used to build a large knowledge base\nof proof dependencies, providing precise data for ATP-based re-verification and\nfor training premise selection algorithms. Second, a new machine learning\nalgorithm for premise selection based on kernel methods is proposed and\nimplemented. To evaluate the impact of both techniques, a benchmark consisting\nof 2078 large-theory mathematical problems is constructed,extending the older\nMPTP Challenge benchmark. The combined effect of the techniques results in a\n50% improvement on the benchmark over the Vampire/SInE state-of-the-art system\nfor automated reasoning in large theories.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2011 11:18:55 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2012 18:52:58 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Alama", "Jesse", ""], ["Heskes", "Tom", ""], ["K\u00fchlwein", "Daniel", ""], ["Tsivtsivadze", "Evgeni", ""], ["Urban", "Josef", ""]]}, {"id": "1108.3476", "submitter": "Massimiliano Pontil", "authors": "Andreas Maurer and Massimiliano Pontil", "title": "Structured Sparsity and Generalization", "comments": null, "journal-ref": "Journal of Machine Learning Research, 13:671-690, 2012", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data dependent generalization bound for a large class of\nregularized algorithms which implement structured sparsity constraints. The\nbound can be applied to standard squared-norm regularization, the Lasso, the\ngroup Lasso, some versions of the group Lasso with overlapping groups, multiple\nkernel learning and other regularization schemes. In all these cases\ncompetitive results are obtained. A novel feature of our bound is that it can\nbe applied in an infinite dimensional setting such as the Lasso in a separable\nHilbert space or multiple kernel learning with a countable number of kernels.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2011 13:36:11 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2011 08:47:01 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Maurer", "Andreas", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1108.4142", "submitter": "Aleksandrs Slivkins", "authors": "Moshe Babaioff, Shaddin Dughmi, Robert Kleinberg and Aleksandrs\n  Slivkins", "title": "Dynamic Pricing with Limited Supply", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of dynamic pricing with limited supply. A seller has\n$k$ identical items for sale and is facing $n$ potential buyers (\"agents\") that\nare arriving sequentially. Each agent is interested in buying one item. Each\nagent's value for an item is an IID sample from some fixed distribution with\nsupport $[0,1]$. The seller offers a take-it-or-leave-it price to each arriving\nagent (possibly different for different agents), and aims to maximize his\nexpected revenue.\n  We focus on \"prior-independent\" mechanisms -- ones that do not use any\ninformation about the distribution. They are desirable because knowing the\ndistribution is unrealistic in many practical scenarios. We study how the\nrevenue of such mechanisms compares to the revenue of the optimal offline\nmechanism that knows the distribution (\"offline benchmark\").\n  We present a prior-independent dynamic pricing mechanism whose revenue is at\nmost $O((k \\log n)^{2/3})$ less than the offline benchmark, for every\ndistribution that is regular. In fact, this guarantee holds without *any*\nassumptions if the benchmark is relaxed to fixed-price mechanisms. Further, we\nprove a matching lower bound. The performance guarantee for the same mechanism\ncan be improved to $O(\\sqrt{k} \\log n)$, with a distribution-dependent\nconstant, if $k/n$ is sufficiently small. We show that, in the worst case over\nall demand distributions, this is essentially the best rate that can be\nobtained with a distribution-specific constant.\n  On a technical level, we exploit the connection to multi-armed bandits (MAB).\nWhile dynamic pricing with unlimited supply can easily be seen as an MAB\nproblem, the intuition behind MAB approaches breaks when applied to the setting\nwith limited supply. Our high-level conceptual contribution is that even the\nlimited supply setting can be fruitfully treated as a bandit problem.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2011 20:28:09 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2012 18:42:22 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2013 20:08:07 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Babaioff", "Moshe", ""], ["Dughmi", "Shaddin", ""], ["Kleinberg", "Robert", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "1108.4545", "submitter": "Tshilidzi Marwala", "authors": "Meir Perez and Tshilidzi Marwala", "title": "The fuzzy gene filter: A classifier performance assesment", "comments": "Intelligent Systems and Control / 742: Computational Bioscience (ISC\n  2011) July 11 - 13, 2011 Cambridge, United Kingdom Editor(s): J.F. Whidborne,\n  P. Willis, G. Montana", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fuzzy Gene Filter (FGF) is an optimised Fuzzy Inference System designed\nto rank genes in order of differential expression, based on expression data\ngenerated in a microarray experiment. This paper examines the effectiveness of\nthe FGF for feature selection using various classification architectures. The\nFGF is compared to three of the most common gene ranking algorithms: t-test,\nWilcoxon test and ROC curve analysis. Four classification schemes are used to\ncompare the performance of the FGF vis-a-vis the standard approaches: K Nearest\nNeighbour (KNN), Support Vector Machine (SVM), Naive Bayesian Classifier (NBC)\nand Artificial Neural Network (ANN). A nested stratified Leave-One-Out Cross\nValidation scheme is used to identify the optimal number top ranking genes, as\nwell as the optimal classifier parameters. Two microarray data sets are used\nfor the comparison: a prostate cancer data set and a lymphoma data set.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2011 10:34:07 GMT"}], "update_date": "2011-08-24", "authors_parsed": [["Perez", "Meir", ""], ["Marwala", "Tshilidzi", ""]]}, {"id": "1108.4551", "submitter": "Tshilidzi Marwala", "authors": "Mlungisi Duma, Bhekisipho Twala, Tshilidzi Marwala", "title": "Improving the performance of the ripper in insurance risk classification\n  : A comparitive study using feature selection", "comments": "ICINCO 2011: 8th International Conference on Informatics in Control,\n  Automation and Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ripper algorithm is designed to generate rule sets for large datasets\nwith many features. However, it was shown that the algorithm struggles with\nclassification performance in the presence of missing data. The algorithm\nstruggles to classify instances when the quality of the data deteriorates as a\nresult of increasing missing data. In this paper, a feature selection technique\nis used to help improve the classification performance of the Ripper model.\nPrincipal component analysis and evidence automatic relevance determination\ntechniques are used to improve the performance. A comparison is done to see\nwhich technique helps the algorithm improve the most. Training datasets with\ncompletely observable data were used to construct the model and testing\ndatasets with missing values were used for measuring accuracy. The results\nshowed that principal component analysis is a better feature selection for the\nRipper in improving the classification performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2011 10:52:18 GMT"}], "update_date": "2011-08-24", "authors_parsed": [["Duma", "Mlungisi", ""], ["Twala", "Bhekisipho", ""], ["Marwala", "Tshilidzi", ""]]}, {"id": "1108.4559", "submitter": "Tomer Koren", "authors": "Elad Hazan and Tomer Koren", "title": "Optimal Algorithms for Ridge and Lasso Regression with Partially\n  Observed Attributes", "comments": "This is a full version of the paper arXiv:1206.4678 appearing in ICML\n  2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the most common variants of linear regression, including Ridge,\nLasso and Support-vector regression, in a setting where the learner is allowed\nto observe only a fixed number of attributes of each example at training time.\nWe present simple and efficient algorithms for these problems: for Lasso and\nRidge regression they need the same total number of attributes (up to\nconstants) as do full-information algorithms, for reaching a certain accuracy.\nFor Support-vector regression, we require exponentially less attributes\ncompared to the state of the art. By that, we resolve an open problem recently\nposed by Cesa-Bianchi et al. (2010). Experiments show the theoretical bounds to\nbe justified by superior performance compared to the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2011 11:52:35 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2012 18:59:15 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Hazan", "Elad", ""], ["Koren", "Tomer", ""]]}, {"id": "1108.4961", "submitter": "Gabor Bartok", "authors": "Andr\\'as Antos, G\\'abor Bart\\'ok, Csaba Szepesv\\'ari", "title": "Non-trivial two-armed partial-monitoring games are bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online learning in partial-monitoring games against an oblivious\nadversary. We show that when the number of actions available to the learner is\ntwo and the game is nontrivial then it is reducible to a bandit-like game and\nthus the minimax regret is $\\Theta(\\sqrt{T})$.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2011 22:38:40 GMT"}], "update_date": "2011-08-26", "authors_parsed": [["Antos", "Andr\u00e1s", ""], ["Bart\u00f3k", "G\u00e1bor", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1108.5397", "submitter": "Charles Bergeron PhD", "authors": "Charles Bergeron, Theresa Hepburn, C. Matthew Sundling, Michael Krein,\n  Bill Katt, Nagamani Sukumar, Curt M. Breneman, Kristin P. Bennett", "title": "Prediction of peptide bonding affinity: kernel methods for nonlinear\n  modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents regression models obtained from a process of blind\nprediction of peptide binding affinity from provided descriptors for several\ndistinct datasets as part of the 2006 Comparative Evaluation of Prediction\nAlgorithms (COEPRA) contest. This paper finds that kernel partial least\nsquares, a nonlinear partial least squares (PLS) algorithm, outperforms PLS,\nand that the incorporation of transferable atom equivalent features improves\npredictive capability.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2011 21:21:51 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["Bergeron", "Charles", ""], ["Hepburn", "Theresa", ""], ["Sundling", "C. Matthew", ""], ["Krein", "Michael", ""], ["Katt", "Bill", ""], ["Sukumar", "Nagamani", ""], ["Breneman", "Curt M.", ""], ["Bennett", "Kristin P.", ""]]}, {"id": "1108.5491", "submitter": "Massimo Melucci", "authors": "Massimo Melucci", "title": "Improving Ranking Using Quantum Probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.ET cs.LG physics.data-an", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The paper shows that ranking information units by quantum probability differs\nfrom ranking them by classical probability provided the same data used for\nparameter estimation. As probability of detection (also known as recall or\npower) and probability of false alarm (also known as fallout or size) measure\nthe quality of ranking, we point out and show that ranking by quantum\nprobability yields higher probability of detection than ranking by classical\nprobability provided a given probability of false alarm and the same parameter\nestimation data. As quantum probability provided more effective detectors than\nclassical probability within other domains that data management, we conjecture\nthat, the system that can implement subspace-based detectors shall be more\neffective than a system which implements a set-based detectors, the\neffectiveness being calculated as expected recall estimated over the\nprobability of detection and expected fallout estimated over the probability of\nfalse alarm.\n", "versions": [{"version": "v1", "created": "Sun, 28 Aug 2011 02:55:18 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["Melucci", "Massimo", ""]]}, {"id": "1108.5514", "submitter": "Yu Zhang", "authors": "Yu Zhang, Mihaela van der Schaar", "title": "Strategic Learning and Robust Protocol Design for Online Communities\n  with Selfish Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on analyzing the free-riding behavior of self-interested\nusers in online communities. Hence, traditional optimization methods for\ncommunities composed of compliant users such as network utility maximization\ncannot be applied here. In our prior work, we show how social reciprocation\nprotocols can be designed in online communities which have populations\nconsisting of a continuum of users and are stationary under stochastic\npermutations. Under these assumptions, we are able to prove that users\nvoluntarily comply with the pre-determined social norms and cooperate with\nother users in the community by providing their services. In this paper, we\ngeneralize the study by analyzing the interactions of self-interested users in\nonline communities with finite populations and are not stationary. To optimize\ntheir long-term performance based on their knowledge, users adapt their\nstrategies to play their best response by solving individual stochastic control\nproblems. The best-response dynamic introduces a stochastic dynamic process in\nthe community, in which the strategies of users evolve over time. We then\ninvestigate the long-term evolution of a community, and prove that the\ncommunity will converge to stochastically stable equilibria which are stable\nagainst stochastic permutations. Understanding the evolution of a community\nprovides protocol designers with guidelines for designing social norms in which\nno user has incentives to adapt its strategy and deviate from the prescribed\nprotocol, thereby ensuring that the adopted protocol will enable the community\nto achieve the optimal social welfare.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2011 04:18:19 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["Zhang", "Yu", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1108.5575", "submitter": "Massimo Melucci", "authors": "Massimo Melucci", "title": "Getting Beyond the State of the Art of Information Retrieval with\n  Quantum Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG physics.data-an", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  According to the probability ranking principle, the document set with the\nhighest values of probability of relevance optimizes information retrieval\neffectiveness given the probabilities are estimated as accurately as possible.\nThe key point of this principle is the separation of the document set into two\nsubsets with a given level of fallout and with the highest recall. If subsets\nof set measures are replaced by subspaces and space measures, we obtain an\nalternative theory stemming from Quantum Theory. That theory is named after\nvector probability because vectors represent event like sets do in classical\nprobability. The paper shows that the separation into vector subspaces is more\neffective than the separation into subsets with the same available evidence.\nThe result is proved mathematically and verified experimentally. In general,\nthe paper suggests that quantum theory is not only a source of rhetoric\ninspiration, but is a sufficient condition to improve retrieval effectiveness\nin a principled way.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2011 14:37:39 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["Melucci", "Massimo", ""]]}, {"id": "1108.5668", "submitter": "Gabriel Dulac-Arnold", "authors": "Gabriel Dulac-Arnold, Ludovic Denoyer, Philippe Preux and Patrick\n  Gallinari", "title": "Datum-Wise Classification: A Sequential Approach to Sparsity", "comments": "ECML2011", "journal-ref": "Lecture Notes in Computer Science, 2011, Volume 6911/2011, 375-390", "doi": "10.1007/978-3-642-23780-5_34", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel classification technique whose aim is to select an\nappropriate representation for each datapoint, in contrast to the usual\napproach of selecting a representation encompassing the whole dataset. This\ndatum-wise representation is found by using a sparsity inducing empirical risk,\nwhich is a relaxation of the standard L 0 regularized risk. The classification\nproblem is modeled as a sequential decision process that sequentially chooses,\nfor each datapoint, which features to use before classifying. Datum-Wise\nClassification extends naturally to multi-class tasks, and we describe a\nspecific case where our inference has equivalent complexity to a traditional\nlinear classifier, while still using a variable number of features. We compare\nour classifier to classical L 1 regularized linear models (L 1-SVM and LARS) on\na set of common binary and multi-class datasets and show that for an equal\naverage number of features used we can get improved performance using our\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2011 17:46:08 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Dulac-Arnold", "Gabriel", ""], ["Denoyer", "Ludovic", ""], ["Preux", "Philippe", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1108.5784", "submitter": "Massimo Melucci", "authors": "Massimo Melucci", "title": "Probability Ranking in Vector Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The Probability Ranking Principle states that the document set with the\nhighest values of probability of relevance optimizes information retrieval\neffectiveness given the probabilities are estimated as accurately as possible.\nThe key point of the principle is the separation of the document set into two\nsubsets with a given level of fallout and with the highest recall. The paper\nintroduces the separation between two vector subspaces and shows that the\nseparation yields a more effective performance than the optimal separation into\nsubsets with the same available evidence, the performance being measured with\nrecall and fallout. The result is proved mathematically and exemplified\nexperimentally.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2011 00:31:44 GMT"}], "update_date": "2011-08-31", "authors_parsed": [["Melucci", "Massimo", ""]]}, {"id": "1108.6088", "submitter": "Alexander Rakhlin", "authors": "Dean Foster and Alexander Rakhlin", "title": "No Internal Regret via Neighborhood Watch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm which attains O(\\sqrt{T}) internal (and thus\nexternal) regret for finite games with partial monitoring under the local\nobservability condition. Recently, this condition has been shown by (Bartok,\nPal, and Szepesvari, 2011) to imply the O(\\sqrt{T}) rate for partial monitoring\ngames against an i.i.d. opponent, and the authors conjectured that the same\nholds for non-stochastic adversaries. Our result is in the affirmative, and it\ncompletes the characterization of possible rates for finite partial-monitoring\ngames, an open question stated by (Cesa-Bianchi, Lugosi, and Stoltz, 2006). Our\nregret guarantees also hold for the more general model of partial monitoring\nwith random signals.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2011 21:48:37 GMT"}], "update_date": "2011-09-01", "authors_parsed": [["Foster", "Dean", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "1108.6211", "submitter": "Alessandro Lazaric", "authors": "Alessandro Lazaric (INRIA Lille - Nord Europe), Marcello Restelli", "title": "Transfer from Multiple MDPs", "comments": "2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer reinforcement learning (RL) methods leverage on the experience\ncollected on a set of source tasks to speed-up RL algorithms. A simple and\neffective approach is to transfer samples from source tasks and include them\ninto the training set used to solve a given target task. In this paper, we\ninvestigate the theoretical properties of this transfer method and we introduce\nnovel algorithms adapting the transfer process on the basis of the similarity\nbetween source and target tasks. Finally, we report illustrative experimental\nresults in a continuous chain problem.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2011 12:46:11 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2011 09:19:00 GMT"}], "update_date": "2011-09-02", "authors_parsed": [["Lazaric", "Alessandro", "", "INRIA Lille - Nord Europe"], ["Restelli", "Marcello", ""]]}, {"id": "1108.6296", "submitter": "Feng Yan", "authors": "Zenglin Xu, Feng Yan, Yuan (Alan) Qi", "title": "Infinite Tucker Decomposition: Nonparametric Bayesian Models for\n  Multiway Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Tensor decomposition is a powerful computational tool for multiway data\nanalysis. Many popular tensor decomposition approaches---such as the Tucker\ndecomposition and CANDECOMP/PARAFAC (CP)---amount to multi-linear\nfactorization. They are insufficient to model (i) complex interactions between\ndata entities, (ii) various data types (e.g. missing data and binary data), and\n(iii) noisy observations and outliers. To address these issues, we propose\ntensor-variate latent nonparametric Bayesian models, coupled with efficient\ninference methods, for multiway data analysis. We name these models InfTucker.\nUsing these InfTucker, we conduct Tucker decomposition in an infinite feature\nspace. Unlike classical tensor decomposition models, our new approaches handle\nboth continuous and binary data in a probabilistic framework. Unlike previous\nBayesian models on matrices and tensors, our models are based on latent\nGaussian or $t$ processes with nonlinear covariance functions. To efficiently\nlearn the InfTucker from data, we develop a variational inference technique on\ntensors. Compared with classical implementation, the new technique reduces both\ntime and space complexities by several orders of magnitude. Our experimental\nresults on chemometrics and social network datasets demonstrate that our new\nmodels achieved significantly higher prediction accuracy than the most\nstate-of-art tensor decomposition\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2011 17:36:26 GMT"}, {"version": "v2", "created": "Sat, 14 Jan 2012 16:11:56 GMT"}], "update_date": "2012-01-17", "authors_parsed": [["Xu", "Zenglin", "", "Alan"], ["Yan", "Feng", "", "Alan"], ["Yuan", "", "", "Alan"], ["Qi", "", ""]]}]