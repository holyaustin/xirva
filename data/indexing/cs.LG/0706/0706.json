[{"id": "0706.0585", "submitter": "Zhendong Zhao", "authors": "Zhendong Zhao, Lei Yuan, Yuxuan Wang, Forrest Sheng Bao, Shunyi Zhang\n  Yanfei Sun", "title": "A Novel Model of Working Set Selection for SMO Decomposition Methods", "comments": "8 pages, 12 figures, it was submitted to IEEE International\n  conference of Tools on Artificial Intelligence", "journal-ref": null, "doi": "10.1109/ICTAI.2007.99", "report-no": null, "categories": "cs.LG cs.AI", "license": null, "abstract": "  In the process of training Support Vector Machines (SVMs) by decomposition\nmethods, working set selection is an important technique, and some exciting\nschemes were employed into this field. To improve working set selection, we\npropose a new model for working set selection in sequential minimal\noptimization (SMO) decomposition methods. In this model, it selects B as\nworking set without reselection. Some properties are given by simple proof, and\nexperiments demonstrate that the proposed method is in general faster than\nexisting methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2007 05:55:07 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Zhao", "Zhendong", ""], ["Yuan", "Lei", ""], ["Wang", "Yuxuan", ""], ["Bao", "Forrest Sheng", ""], ["Sun", "Shunyi Zhang Yanfei", ""]]}, {"id": "0706.2040", "submitter": "Edoardo Airoldi", "authors": "Edoardo M Airoldi", "title": "Getting started in probabilistic graphical models", "comments": "12 pages, 1 figure", "journal-ref": "Airoldi EM (2007) Getting started in probabilistic graphical\n  models. PLoS Comput Biol 3(12): e252", "doi": "10.1371/journal.pcbi.0030252", "report-no": null, "categories": "q-bio.QM cs.LG physics.soc-ph stat.ME stat.ML", "license": null, "abstract": "  Probabilistic graphical models (PGMs) have become a popular tool for\ncomputational analysis of biological data in a variety of domains. But, what\nexactly are they and how do they work? How can we use PGMs to discover patterns\nthat are biologically relevant? And to what extent can PGMs help us formulate\nnew hypotheses that are testable at the bench? This note sketches out some\nanswers and illustrates the main ideas behind the statistical approach to\nbiological pattern discovery.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2007 14:52:06 GMT"}, {"version": "v2", "created": "Sat, 10 Nov 2007 19:25:59 GMT"}], "update_date": "2010-02-22", "authors_parsed": [["Airoldi", "Edoardo M", ""]]}, {"id": "0706.3188", "submitter": "Vladimir Vovk", "authors": "Glenn Shafer and Vladimir Vovk", "title": "A tutorial on conformal prediction", "comments": "58 pages, 9 figures", "journal-ref": "Journal of Machine Learning Research 9 (2008) 371-421.\n  http://www.jmlr.org/papers/v9/shafer08a.html", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": null, "abstract": "  Conformal prediction uses past experience to determine precise levels of\nconfidence in new predictions. Given an error probability $\\epsilon$, together\nwith a method that makes a prediction $\\hat{y}$ of a label $y$, it produces a\nset of labels, typically containing $\\hat{y}$, that also contains $y$ with\nprobability $1-\\epsilon$. Conformal prediction can be applied to any method for\nproducing $\\hat{y}$: a nearest-neighbor method, a support-vector machine, ridge\nregression, etc.\n  Conformal prediction is designed for an on-line setting in which labels are\npredicted successively, each one being revealed before the next is predicted.\nThe most novel and valuable feature of conformal prediction is that if the\nsuccessive examples are sampled independently from the same distribution, then\nthe successive predictions will be right $1-\\epsilon$ of the time, even though\nthey are based on an accumulating dataset rather than on independent datasets.\n  In addition to the model under which successive examples are sampled\nindependently, other on-line compression models can also use conformal\nprediction. The widely used Gaussian linear model is one of these.\n  This tutorial presents a self-contained account of the theory of conformal\nprediction and works through several numerical examples. A more comprehensive\ntreatment of the topic is provided in \"Algorithmic Learning in a Random World\",\nby Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005).\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2007 16:40:06 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Shafer", "Glenn", ""], ["Vovk", "Vladimir", ""]]}, {"id": "0706.3679", "submitter": "Yann Guermeur", "authors": "Yann Guermeur (LORIA)", "title": "Scale-sensitive Psi-dimensions: the Capacity Measures for Classifiers\n  Taking Values in R^Q", "comments": null, "journal-ref": "ASMDA 2007 (2007) 1-8", "doi": null, "report-no": null, "categories": "cs.LG", "license": null, "abstract": "  Bounds on the risk play a crucial role in statistical learning theory. They\nusually involve as capacity measure of the model studied the VC dimension or\none of its extensions. In classification, such \"VC dimensions\" exist for models\ntaking values in {0, 1}, {1,..., Q} and R. We introduce the generalizations\nappropriate for the missing case, the one of models with values in R^Q. This\nprovides us with a new guaranteed risk for M-SVMs which appears superior to the\nexisting one.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2007 17:28:57 GMT"}], "update_date": "2007-06-26", "authors_parsed": [["Guermeur", "Yann", "", "LORIA"]]}]