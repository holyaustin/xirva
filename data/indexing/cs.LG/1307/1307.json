[{"id": "1307.0032", "submitter": "Ioannis Mitliagkas", "authors": "Ioannis Mitliagkas, Constantine Caramanis, Prateek Jain", "title": "Memory Limited, Streaming PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider streaming, one-pass principal component analysis (PCA), in the\nhigh-dimensional regime, with limited memory. Here, $p$-dimensional samples are\npresented sequentially, and the goal is to produce the $k$-dimensional subspace\nthat best approximates these points. Standard algorithms require $O(p^2)$\nmemory; meanwhile no algorithm can do better than $O(kp)$ memory, since this is\nwhat the output itself requires. Memory (or storage) complexity is most\nmeaningful when understood in the context of computational and sample\ncomplexity. Sample complexity for high-dimensional PCA is typically studied in\nthe setting of the {\\em spiked covariance model}, where $p$-dimensional points\nare generated from a population covariance equal to the identity (white noise)\nplus a low-dimensional perturbation (the spike) which is the signal to be\nrecovered. It is now well-understood that the spike can be recovered when the\nnumber of samples, $n$, scales proportionally with the dimension, $p$. Yet, all\nalgorithms that provably achieve this, have memory complexity $O(p^2)$.\nMeanwhile, algorithms with memory-complexity $O(kp)$ do not have provable\nbounds on sample complexity comparable to $p$. We present an algorithm that\nachieves both: it uses $O(kp)$ memory (meaning storage of any kind) and is able\nto compute the $k$-dimensional spike with $O(p \\log p)$ sample-complexity --\nthe first algorithm of its kind. While our theoretical analysis focuses on the\nspiked covariance model, our simulations show that our algorithm is successful\non much more general models for the data.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 21:38:17 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Mitliagkas", "Ioannis", ""], ["Caramanis", "Constantine", ""], ["Jain", "Prateek", ""]]}, {"id": "1307.0048", "submitter": "Kun  Yang", "authors": "Kun Yang", "title": "Simple one-pass algorithm for penalized linear regression with\n  cross-validation on MapReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a one-pass algorithm on MapReduce for penalized\nlinear regression\n  \\[f_\\lambda(\\alpha, \\beta) = \\|Y - \\alpha\\mathbf{1} - X\\beta\\|_2^2 +\np_{\\lambda}(\\beta)\\] where $\\alpha$ is the intercept which can be omitted\ndepending on application; $\\beta$ is the coefficients and $p_{\\lambda}$ is the\npenalized function with penalizing parameter $\\lambda$. $f_\\lambda(\\alpha,\n\\beta)$ includes interesting classes such as Lasso, Ridge regression and\nElastic-net. Compared to latest iterative distributed algorithms requiring\nmultiple MapReduce jobs, our algorithm achieves huge performance improvement;\nmoreover, our algorithm is exact compared to the approximate algorithms such as\nparallel stochastic gradient decent. Moreover, what our algorithm distinguishes\nwith others is that it trains the model with cross validation to choose optimal\n$\\lambda$ instead of user specified one.\n  Key words: penalized linear regression, lasso, elastic-net, ridge, MapReduce\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 23:32:11 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 05:52:11 GMT"}, {"version": "v3", "created": "Thu, 14 Apr 2016 01:55:55 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Yang", "Kun", ""]]}, {"id": "1307.0127", "submitter": "Tor Lattimore", "authors": "Tor Lattimore and Marcus Hutter and Peter Sunehag", "title": "Concentration and Confidence for Discrete Bayesian Sequence Predictors", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian sequence prediction is a simple technique for predicting future\nsymbols sampled from an unknown measure on infinite sequences over a countable\nalphabet. While strong bounds on the expected cumulative error are known, there\nare only limited results on the distribution of this error. We prove tight\nhigh-probability bounds on the cumulative error, which is measured in terms of\nthe Kullback-Leibler (KL) divergence. We also consider the problem of\nconstructing upper confidence bounds on the KL and Hellinger errors similar to\nthose constructed from Hoeffding-like bounds in the i.i.d. case. The new\nresults are applied to show that Bayesian sequence prediction can be used in\nthe Knows What It Knows (KWIK) framework with bounds that match the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2013 16:36:30 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Lattimore", "Tor", ""], ["Hutter", "Marcus", ""], ["Sunehag", "Peter", ""]]}, {"id": "1307.0252", "submitter": "Eric Bair", "authors": "Eric Bair", "title": "Semi-supervised clustering methods", "comments": "28 pages, 5 figures", "journal-ref": "WIREs Comp Stat, 2013, 5(5): 349-361", "doi": "10.1002/wics.1270", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis methods seek to partition a data set into homogeneous\nsubgroups. It is useful in a wide variety of applications, including document\nprocessing and modern genetics. Conventional clustering methods are\nunsupervised, meaning that there is no outcome variable nor is anything known\nabout the relationship between the observations in the data set. In many\nsituations, however, information about the clusters is available in addition to\nthe values of the features. For example, the cluster labels of some\nobservations may be known, or certain observations may be known to belong to\nthe same cluster. In other cases, one may wish to identify clusters that are\nassociated with a particular outcome variable. This review describes several\nclustering algorithms (known as \"semi-supervised clustering\" methods) that can\nbe applied in these situations. The majority of these methods are modifications\nof the popular k-means clustering method, and several of them will be described\nin detail. A brief description of some other semi-supervised clustering\nalgorithms is also provided.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 00:51:07 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Bair", "Eric", ""]]}, {"id": "1307.0253", "submitter": "Bhavana Dalvi", "authors": "Bhavana Dalvi, William W. Cohen, Jamie Callan", "title": "Exploratory Learning", "comments": "16 pages; European Conference on Machine Learning and Principles and\n  Practice of Knowledge Discovery in Databases, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multiclass semi-supervised learning (SSL), it is sometimes the case that\nthe number of classes present in the data is not known, and hence no labeled\nexamples are provided for some classes. In this paper we present variants of\nwell-known semi-supervised multiclass learning methods that are robust when the\ndata contains an unknown number of classes. In particular, we present an\n\"exploratory\" extension of expectation-maximization (EM) that explores\ndifferent numbers of classes while learning. \"Exploratory\" SSL greatly improves\nperformance on three datasets in terms of F1 on the classes with seed examples\ni.e., the classes which are expected to be in the data. Our Exploratory EM\nalgorithm also outperforms a SSL method based non-parametric Bayesian\nclustering.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 01:09:25 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Dalvi", "Bhavana", ""], ["Cohen", "William W.", ""], ["Callan", "Jamie", ""]]}, {"id": "1307.0261", "submitter": "Bhavana Dalvi", "authors": "Bhavana Dalvi, William W. Cohen, and Jamie Callan", "title": "WebSets: Extracting Sets of Entities from the Web Using Unsupervised\n  Information Extraction", "comments": "10 pages; International Conference on Web Search and Data Mining 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a open-domain information extraction method for extracting\nconcept-instance pairs from an HTML corpus. Most earlier approaches to this\nproblem rely on combining clusters of distributionally similar terms and\nconcept-instance pairs obtained with Hearst patterns. In contrast, our method\nrelies on a novel approach for clustering terms found in HTML tables, and then\nassigning concept names to these clusters using Hearst patterns. The method can\nbe efficiently applied to a large corpus, and experimental results on several\ndatasets show that our method can accurately extract large numbers of\nconcept-instance pairs.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 02:49:08 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Dalvi", "Bhavana", ""], ["Cohen", "William W.", ""], ["Callan", "Jamie", ""]]}, {"id": "1307.0317", "submitter": "Jaka \\v{S}peh", "authors": "Jaka \\v{S}peh, Andrej Muhi\\v{c}, Jan Rupnik", "title": "Algorithms of the LDA model [REPORT]", "comments": "5 pages, 4 figures, report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We review three algorithms for Latent Dirichlet Allocation (LDA). Two of them\nare variational inference algorithms: Variational Bayesian inference and Online\nVariational Bayesian inference and one is Markov Chain Monte Carlo (MCMC)\nalgorithm -- Collapsed Gibbs sampling. We compare their time complexity and\nperformance. We find that online variational Bayesian inference is the fastest\nalgorithm and still returns reasonably good results.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 10:03:58 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["\u0160peh", "Jaka", ""], ["Muhi\u010d", "Andrej", ""], ["Rupnik", "Jan", ""]]}, {"id": "1307.0366", "submitter": "Caroline Uhler", "authors": "Garvesh Raskutti and Caroline Uhler", "title": "Learning directed acyclic graphs based on sparsest permutations", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a Bayesian network or directed acyclic\ngraph (DAG) model from observational data. A number of constraint-based,\nscore-based and hybrid algorithms have been developed for this purpose. For\nconstraint-based methods, statistical consistency guarantees typically rely on\nthe faithfulness assumption, which has been show to be restrictive especially\nfor graphs with cycles in the skeleton. However, there is only limited work on\nconsistency guarantees for score-based and hybrid algorithms and it has been\nunclear whether consistency guarantees can be proven under weaker conditions\nthan the faithfulness assumption. In this paper, we propose the sparsest\npermutation (SP) algorithm. This algorithm is based on finding the causal\nordering of the variables that yields the sparsest DAG. We prove that this new\nscore-based method is consistent under strictly weaker conditions than the\nfaithfulness assumption. We also demonstrate through simulations on small DAGs\nthat the SP algorithm compares favorably to the constraint-based PC and SGS\nalgorithms as well as the score-based Greedy Equivalence Search and hybrid\nMax-Min Hill-Climbing method. In the Gaussian setting, we prove that our\nalgorithm boils down to finding the permutation of the variables with sparsest\nCholesky decomposition for the inverse covariance matrix. Using this\nconnection, we show that in the oracle setting, where the true covariance\nmatrix is known, the SP algorithm is in fact equivalent to $\\ell_0$-penalized\nmaximum likelihood estimation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 13:41:40 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2013 01:07:21 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2014 12:16:10 GMT"}, {"version": "v4", "created": "Sat, 27 Jul 2019 12:33:21 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Raskutti", "Garvesh", ""], ["Uhler", "Caroline", ""]]}, {"id": "1307.0414", "submitter": "Ian Goodfellow", "authors": "Ian J. Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron Courville,\n  Mehdi Mirza, Ben Hamner, Will Cukierski, Yichuan Tang, David Thaler,\n  Dong-Hyun Lee, Yingbo Zhou, Chetan Ramaiah, Fangxiang Feng, Ruifan Li,\n  Xiaojie Wang, Dimitris Athanasakis, John Shawe-Taylor, Maxim Milakov, John\n  Park, Radu Ionescu, Marius Popescu, Cristian Grozea, James Bergstra, Jingjing\n  Xie, Lukasz Romaszko, Bing Xu, Zhang Chuang, and Yoshua Bengio", "title": "Challenges in Representation Learning: A report on three machine\n  learning contests", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ICML 2013 Workshop on Challenges in Representation Learning focused on\nthree challenges: the black box learning challenge, the facial expression\nrecognition challenge, and the multimodal learning challenge. We describe the\ndatasets created for these challenges and summarize the results of the\ncompetitions. We provide suggestions for organizers of future challenges and\nsome comments on what kind of knowledge can be gained from machine learning\ncompetitions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 15:53:22 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Goodfellow", "Ian J.", ""], ["Erhan", "Dumitru", ""], ["Carrier", "Pierre Luc", ""], ["Courville", "Aaron", ""], ["Mirza", "Mehdi", ""], ["Hamner", "Ben", ""], ["Cukierski", "Will", ""], ["Tang", "Yichuan", ""], ["Thaler", "David", ""], ["Lee", "Dong-Hyun", ""], ["Zhou", "Yingbo", ""], ["Ramaiah", "Chetan", ""], ["Feng", "Fangxiang", ""], ["Li", "Ruifan", ""], ["Wang", "Xiaojie", ""], ["Athanasakis", "Dimitris", ""], ["Shawe-Taylor", "John", ""], ["Milakov", "Maxim", ""], ["Park", "John", ""], ["Ionescu", "Radu", ""], ["Popescu", "Marius", ""], ["Grozea", "Cristian", ""], ["Bergstra", "James", ""], ["Xie", "Jingjing", ""], ["Romaszko", "Lukasz", ""], ["Xu", "Bing", ""], ["Chuang", "Zhang", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1307.0426", "submitter": "Thomas Lampert", "authors": "Thomas A. Lampert, Andr\\'e Stumpf, Pierre Gan\\c{c}arski", "title": "An Empirical Study into Annotator Agreement, Ground Truth Estimation,\n  and Algorithm Evaluation", "comments": "16 pages", "journal-ref": "IEEE Transactions on Image Processing 25(6), 2557-2572, 2016", "doi": "10.1109/TIP.2016.2544703", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although agreement between annotators has been studied in the past from a\nstatistical viewpoint, little work has attempted to quantify the extent to\nwhich this phenomenon affects the evaluation of computer vision (CV) object\ndetection algorithms. Many researchers utilise ground truth (GT) in experiments\nand more often than not this GT is derived from one annotator's opinion. How\ndoes the difference in opinion affect an algorithm's evaluation? Four examples\nof typical CV problems are chosen, and a methodology is applied to each to\nquantify the inter-annotator variance and to offer insight into the mechanisms\nbehind agreement and the use of GT. It is found that when detecting linear\nobjects annotator agreement is very low. The agreement in object position,\nlinear or otherwise, can be partially explained through basic image properties.\nAutomatic object detectors are compared to annotator agreement and it is found\nthat a clear relationship exists. Several methods for calculating GTs from a\nnumber of annotations are applied and the resulting differences in the\nperformance of the object detectors are quantified. It is found that the rank\nof a detector is highly dependent upon the method used to form the GT. It is\nalso found that although the STAPLE and LSML GT estimation methods appear to\nrepresent the mean of the performance measured using the individual\nannotations, when there are few annotations, or there is a large variance in\nthem, these estimates tend to degrade. Furthermore, one of the most commonly\nadopted annotation combination methods--consensus voting--accentuates more\nobvious features, which results in an overestimation of the algorithm's\nperformance. Finally, it is concluded that in some datasets it may not be\npossible to state with any confidence that one algorithm outperforms another\nwhen evaluating upon one GT and a method for calculating confidence bounds is\ndiscussed.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 16:16:40 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2013 14:39:01 GMT"}, {"version": "v3", "created": "Tue, 26 Apr 2016 11:05:18 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Lampert", "Thomas A.", ""], ["Stumpf", "Andr\u00e9", ""], ["Gan\u00e7arski", "Pierre", ""]]}, {"id": "1307.0471", "submitter": "Patrick Rebentrost", "authors": "Patrick Rebentrost, Masoud Mohseni, Seth Lloyd", "title": "Quantum support vector machine for big data classification", "comments": "5 pages", "journal-ref": "Phys. Rev. Lett. 113, 130503 (2014)", "doi": "10.1103/PhysRevLett.113.130503", "report-no": null, "categories": "quant-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised machine learning is the classification of new data based on\nalready classified training examples. In this work, we show that the support\nvector machine, an optimized binary classifier, can be implemented on a quantum\ncomputer, with complexity logarithmic in the size of the vectors and the number\nof training examples. In cases when classical sampling algorithms require\npolynomial time, an exponential speed-up is obtained. At the core of this\nquantum big data algorithm is a non-sparse matrix exponentiation technique for\nefficiently performing a matrix inversion of the training data inner-product\n(kernel) matrix.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 18:35:53 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2013 05:07:59 GMT"}, {"version": "v3", "created": "Thu, 10 Jul 2014 04:33:52 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Rebentrost", "Patrick", ""], ["Mohseni", "Masoud", ""], ["Lloyd", "Seth", ""]]}, {"id": "1307.0473", "submitter": "Maxim Raginsky", "authors": "Maxim Raginsky and Angelia Nedi\\'c", "title": "Online discrete optimization in social networks in the presence of\n  Knightian uncertainty", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a model of collective real-time decision-making (or learning) in a\nsocial network operating in an uncertain environment, for which no a priori\nprobabilistic model is available. Instead, the environment's impact on the\nagents in the network is seen through a sequence of cost functions, revealed to\nthe agents in a causal manner only after all the relevant actions are taken.\nThere are two kinds of costs: individual costs incurred by each agent and\nlocal-interaction costs incurred by each agent and its neighbors in the social\nnetwork. Moreover, agents have inertia: each agent has a default mixed strategy\nthat stays fixed regardless of the state of the environment, and must expend\neffort to deviate from this strategy in order to respond to cost signals coming\nfrom the environment. We construct a decentralized strategy, wherein each agent\nselects its action based only on the costs directly affecting it and on the\ndecisions made by its neighbors in the network. In this setting, we quantify\nsocial learning in terms of regret, which is given by the difference between\nthe realized network performance over a given time horizon and the best\nperformance that could have been achieved in hindsight by a fictitious\ncentralized entity with full knowledge of the environment's evolution. We show\nthat our strategy achieves the regret that scales polylogarithmically with the\ntime horizon and polynomially with the number of agents and the maximum number\nof neighbors of any agent in the social network.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 18:46:06 GMT"}, {"version": "v2", "created": "Thu, 29 Jan 2015 04:20:48 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Raginsky", "Maxim", ""], ["Nedi\u0107", "Angelia", ""]]}, {"id": "1307.0578", "submitter": "Ava Bargi", "authors": "Ava Bargi, Richard Yi Da Xu, Massimo Piccardi", "title": "A non-parametric conditional factor regression model for\n  high-dimensional input and response", "comments": "9 pages, 3 figures, NIPS submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a non-parametric conditional factor regression\n(NCFR)model for domains with high-dimensional input and response. NCFR enhances\nlinear regression in two ways: a) introducing low-dimensional latent factors\nleading to dimensionality reduction and b) integrating an Indian Buffet Process\nas a prior for the latent factors to derive unlimited sparse dimensions.\nExperimental results comparing NCRF to several alternatives give evidence to\nremarkable prediction performance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 02:54:09 GMT"}], "update_date": "2013-07-03", "authors_parsed": [["Bargi", "Ava", ""], ["Da Xu", "Richard Yi", ""], ["Piccardi", "Massimo", ""]]}, {"id": "1307.0589", "submitter": "Steven Ness", "authors": "Steven Ness, Helena Symonds, Paul Spong, George Tzanetakis", "title": "The Orchive : Data mining a massive bioacoustic archive", "comments": "ICML 2013 Workshop on Machine Learning for Bioacoustics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Orchive is a large collection of over 20,000 hours of audio recordings\nfrom the OrcaLab research facility located off the northern tip of Vancouver\nIsland. It contains recorded orca vocalizations from the 1980 to the present\ntime and is one of the largest resources of bioacoustic data in the world. We\nhave developed a web-based interface that allows researchers to listen to these\nrecordings, view waveform and spectral representations of the audio, label\nclips with annotations, and view the results of machine learning classifiers\nbased on automatic audio features extraction. In this paper we describe such\nclassifiers that discriminate between background noise, orca calls, and the\nvoice notes that are present in most of the tapes. Furthermore we show\nclassification results for individual calls based on a previously existing orca\ncall catalog. We have also experimentally investigated the scalability of\nclassifiers over the entire Orchive.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 04:59:19 GMT"}], "update_date": "2013-07-03", "authors_parsed": [["Ness", "Steven", ""], ["Symonds", "Helena", ""], ["Spong", "Paul", ""], ["Tzanetakis", "George", ""]]}, {"id": "1307.0643", "submitter": "Tam\\'as Sz\\'antai", "authors": "Edith Kov\\'acs and Tam\\'as Sz\\'antai", "title": "Discovering the Markov network structure", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new proof is given for the supermodularity of information\ncontent. Using the decomposability of the information content an algorithm is\ngiven for discovering the Markov network graph structure endowed by the\npairwise Markov property of a given probability distribution. A discrete\nprobability distribution is given for which the equivalence of\nHammersley-Clifford theorem is fulfilled although some of the possible vector\nrealizations are taken on with zero probability. Our algorithm for discovering\nthe pairwise Markov network is illustrated on this example, too.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 09:35:16 GMT"}], "update_date": "2013-07-03", "authors_parsed": [["Kov\u00e1cs", "Edith", ""], ["Sz\u00e1ntai", "Tam\u00e1s", ""]]}, {"id": "1307.0781", "submitter": "Cem Tekin", "authors": "Cem Tekin, Mihaela van der Schaar", "title": "Distributed Online Big Data Classification Using Context Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed, online data mining systems have emerged as a result of\napplications requiring analysis of large amounts of correlated and\nhigh-dimensional data produced by multiple distributed data sources. We propose\na distributed online data classification framework where data is gathered by\ndistributed data sources and processed by a heterogeneous set of distributed\nlearners which learn online, at run-time, how to classify the different data\nstreams either by using their locally available classification functions or by\nhelping each other by classifying each other's data. Importantly, since the\ndata is gathered at different locations, sending the data to another learner to\nprocess incurs additional costs such as delays, and hence this will be only\nbeneficial if the benefits obtained from a better classification will exceed\nthe costs. We model the problem of joint classification by the distributed and\nheterogeneous learners from multiple data sources as a distributed contextual\nbandit problem where each data is characterized by a specific context. We\ndevelop a distributed online learning algorithm for which we can prove\nsublinear regret. Compared to prior work in distributed online data mining, our\nwork is the first to provide analytic regret results characterizing the\nperformance of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 18:09:59 GMT"}], "update_date": "2013-07-03", "authors_parsed": [["Tekin", "Cem", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1307.0803", "submitter": "Marinka Zitnik", "authors": "Marinka \\v{Z}itnik and Bla\\v{z} Zupan", "title": "Data Fusion by Matrix Factorization", "comments": "Short preprint, 13 pages, 3 Figures, 3 Tables. Full paper in\n  10.1109/TPAMI.2014.2343973", "journal-ref": "Marinka Zitnik and Blaz Zupan. IEEE Transactions on Pattern\n  Analysis and Machine Intelligence, 37(1):41-53 (2015)", "doi": "10.1109/TPAMI.2014.2343973", "report-no": null, "categories": "cs.LG cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most problems in science and engineering we can obtain data sets that\ndescribe the observed system from various perspectives and record the behavior\nof its individual components. Heterogeneous data sets can be collectively mined\nby data fusion. Fusion can focus on a specific target relation and exploit\ndirectly associated data together with contextual data and data about system's\nconstraints. In the paper we describe a data fusion approach with penalized\nmatrix tri-factorization (DFMF) that simultaneously factorizes data matrices to\nreveal hidden associations. The approach can directly consider any data that\ncan be expressed in a matrix, including those from feature-based\nrepresentations, ontologies, associations and networks. We demonstrate the\nutility of DFMF for gene function prediction task with eleven different data\nsources and for prediction of pharmacologic actions by fusing six data sources.\nOur data fusion algorithm compares favorably to alternative data integration\napproaches and achieves higher accuracy than can be obtained from any single\ndata source alone.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 19:35:21 GMT"}, {"version": "v2", "created": "Fri, 6 Feb 2015 16:15:38 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["\u017ditnik", "Marinka", ""], ["Zupan", "Bla\u017e", ""]]}, {"id": "1307.0813", "submitter": "Marc Deisenroth", "authors": "Marc Peter Deisenroth, Peter Englert, Jan Peters and Dieter Fox", "title": "Multi-Task Policy Search", "comments": "8 pages, double column. IEEE International Conference on Robotics and\n  Automation, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning policies that generalize across multiple tasks is an important and\nchallenging research topic in reinforcement learning and robotics. Training\nindividual policies for every single potential task is often impractical,\nespecially for continuous task variations, requiring more principled approaches\nto share and transfer knowledge among similar tasks. We present a novel\napproach for learning a nonlinear feedback policy that generalizes across\nmultiple tasks. The key idea is to define a parametrized policy as a function\nof both the state and the task, which allows learning a single policy that\ngeneralizes across multiple known and unknown tasks. Applications of our novel\napproach to reinforcement and imitation learning in real-robot experiments are\nshown.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 07:59:32 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2014 09:17:52 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Deisenroth", "Marc Peter", ""], ["Englert", "Peter", ""], ["Peters", "Jan", ""], ["Fox", "Dieter", ""]]}, {"id": "1307.0846", "submitter": "Evgeni Tsivtsivadze", "authors": "Evgeni Tsivtsivadze and Tom Heskes", "title": "Semi-supervised Ranking Pursuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel sparse preference learning/ranking algorithm. Our\nalgorithm approximates the true utility function by a weighted sum of basis\nfunctions using the squared loss on pairs of data points, and is a\ngeneralization of the kernel matching pursuit method. It can operate both in a\nsupervised and a semi-supervised setting and allows efficient search for\nmultiple, near-optimal solutions. Furthermore, we describe the extension of the\nalgorithm suitable for combined ranking and regression tasks. In our\nexperiments we demonstrate that the proposed algorithm outperforms several\nstate-of-the-art learning methods when taking into account unlabeled data and\nperforms comparably in a supervised learning scenario, while providing sparser\nsolutions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 20:51:40 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Tsivtsivadze", "Evgeni", ""], ["Heskes", "Tom", ""]]}, {"id": "1307.0995", "submitter": "Ji Won Yoon Ph.D.", "authors": "Ji Won Yoon", "title": "An Efficient Model Selection for Gaussian Mixture Model in a Bayesian\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to cluster or partition data, we often use\nExpectation-and-Maximization (EM) or Variational approximation with a Gaussian\nMixture Model (GMM), which is a parametric probability density function\nrepresented as a weighted sum of $\\hat{K}$ Gaussian component densities.\nHowever, model selection to find underlying $\\hat{K}$ is one of the key\nconcerns in GMM clustering, since we can obtain the desired clusters only when\n$\\hat{K}$ is known. In this paper, we propose a new model selection algorithm\nto explore $\\hat{K}$ in a Bayesian framework. The proposed algorithm builds the\ndensity of the model order which any information criterions such as AIC and BIC\nbasically fail to reconstruct. In addition, this algorithm reconstructs the\ndensity quickly as compared to the time-consuming Monte Carlo simulation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2013 12:54:25 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Yoon", "Ji Won", ""]]}, {"id": "1307.1058", "submitter": "NIkolai Zolotykh", "authors": "Max A. Alekseyev, Marina G. Basova, Nikolai Yu. Zolotykh", "title": "On the minimal teaching sets of two-dimensional threshold functions", "comments": "11 pages, 4 figures", "journal-ref": "SIAM J. Discrete Math. 29(1), pp. 157-165 (2015)", "doi": "10.1137/140978090", "report-no": null, "categories": "math.CO cs.LG math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that a minimal teaching set of any threshold function on the\ntwodimensional rectangular grid consists of 3 or 4 points. We derive exact\nformulae for the numbers of functions corresponding to these values and further\nrefine them in the case of a minimal teaching set of size 3. We also prove that\nthe average cardinality of the minimal teaching sets of threshold functions is\nasymptotically 7/2.\n  We further present corollaries of these results concerning some special\narrangements of lines in the plane.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2013 16:05:10 GMT"}, {"version": "v2", "created": "Sat, 19 Jul 2014 07:10:33 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Alekseyev", "Max A.", ""], ["Basova", "Marina G.", ""], ["Zolotykh", "Nikolai Yu.", ""]]}, {"id": "1307.1078", "submitter": "Uwe Aickelin", "authors": "Jenna Reps, Jan Feyereisl, Jonathan M. Garibaldi, Uwe Aickelin, Jack\n  E. Gibson, Richard B. Hubbard", "title": "Investigating the Detection of Adverse Drug Events in a UK General\n  Practice Electronic Health-Care Database", "comments": "UKCI 2011, the 11th Annual Workshop on Computational Intelligence,\n  Manchester, pp 167-173", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-mining techniques have frequently been developed for Spontaneous\nreporting databases. These techniques aim to find adverse drug events\naccurately and efficiently. Spontaneous reporting databases are prone to\nmissing information, under reporting and incorrect entries. This often results\nin a detection lag or prevents the detection of some adverse drug events. These\nlimitations do not occur in electronic health-care databases. In this paper,\nexisting methods developed for spontaneous reporting databases are implemented\non both a spontaneous reporting database and a general practice electronic\nhealth-care database and compared. The results suggests that the application of\nexisting methods to the general practice database may help find signals that\nhave gone undetected when using the spontaneous reporting system database. In\naddition the general practice database provides far more supplementary\ninformation, that if incorporated in analysis could provide a wealth of\ninformation for identifying adverse events more accurately.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2013 16:55:32 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Reps", "Jenna", ""], ["Feyereisl", "Jan", ""], ["Garibaldi", "Jonathan M.", ""], ["Aickelin", "Uwe", ""], ["Gibson", "Jack E.", ""], ["Hubbard", "Richard B.", ""]]}, {"id": "1307.1079", "submitter": "Uwe Aickelin", "authors": "Ian Dent, Uwe Aickelin, Tom Rodden", "title": "Application of a clustering framework to UK domestic electricity data", "comments": "UKCI 2011, the 11th Annual Workshop on Computational Intelligence,\n  Manchester, pp 161-166", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper takes an approach to clustering domestic electricity load profiles\nthat has been successfully used with data from Portugal and applies it to UK\ndata. Clustering techniques are applied and it is found that the preferred\ntechnique in the Portuguese work (a two stage process combining Self Organised\nMaps and Kmeans) is not appropriate for the UK data. The work shows that up to\nnine clusters of households can be identified with the differences in usage\nprofiles being visually striking. This demonstrates the appropriateness of\nbreaking the electricity usage patterns down to more detail than the two load\nprofiles currently published by the electricity industry. The paper details\ninitial results using data collected in Milton Keynes around 1990. Further work\nis described and will concentrate on building accurate and meaningful clusters\nof similar electricity users in order to better direct demand side management\ninitiatives to the most relevant target customers.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2013 17:03:31 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Dent", "Ian", ""], ["Aickelin", "Uwe", ""], ["Rodden", "Tom", ""]]}, {"id": "1307.1192", "submitter": "Paul Grigas", "authors": "Robert M. Freund, Paul Grigas, Rahul Mazumder", "title": "AdaBoost and Forward Stagewise Regression are First-Order Convex\n  Optimization Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting methods are highly popular and effective supervised learning methods\nwhich combine weak learners into a single accurate model with good statistical\nperformance. In this paper, we analyze two well-known boosting methods,\nAdaBoost and Incremental Forward Stagewise Regression (FS$_\\varepsilon$), by\nestablishing their precise connections to the Mirror Descent algorithm, which\nis a first-order method in convex optimization. As a consequence of these\nconnections we obtain novel computational guarantees for these boosting\nmethods. In particular, we characterize convergence bounds of AdaBoost, related\nto both the margin and log-exponential loss function, for any step-size\nsequence. Furthermore, this paper presents, for the first time, precise\ncomputational complexity results for FS$_\\varepsilon$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 03:17:23 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Freund", "Robert M.", ""], ["Grigas", "Paul", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1307.1275", "submitter": "Ruifan Li", "authors": "Fangxiang Feng and Ruifan Li and Xiaojie Wang", "title": "Constructing Hierarchical Image-tags Bimodal Representations for Word\n  Tags Alternative Choice", "comments": "6 pages, 1 figure, Presented at the Workshop on Representation\n  Learning, ICML 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper describes our solution to the multi-modal learning challenge of\nICML. This solution comprises constructing three-level representations in three\nconsecutive stages and choosing correct tag words with a data-specific\nstrategy. Firstly, we use typical methods to obtain level-1 representations.\nEach image is represented using MPEG-7 and gist descriptors with additional\nfeatures released by the contest organizers. And the corresponding word tags\nare represented by bag-of-words model with a dictionary of 4000 words.\nSecondly, we learn the level-2 representations using two stacked RBMs for each\nmodality. Thirdly, we propose a bimodal auto-encoder to learn the\nsimilarities/dissimilarities between the pairwise image-tags as level-3\nrepresentations. Finally, during the test phase, based on one observation of\nthe dataset, we come up with a data-specific strategy to choose the correct tag\nwords leading to a leap of an improved overall performance. Our final average\naccuracy on the private test set is 100%, which ranks the first place in this\nchallenge.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 11:10:45 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Feng", "Fangxiang", ""], ["Li", "Ruifan", ""], ["Wang", "Xiaojie", ""]]}, {"id": "1307.1380", "submitter": "Uwe Aickelin", "authors": "Ian Dent, Uwe Aickelin, Tom Rodden", "title": "The Application of a Data Mining Framework to Energy Usage Profiling in\n  Domestic Residences using UK data", "comments": "Buildings Do Not Use Energy, People Do Research Student Conference,\n  Bath, UK, 2011. arXiv admin note: text overlap with arXiv:1307.1079", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for defining representative load profiles for\ndomestic electricity users in the UK. It considers bottom up and clustering\nmethods and then details the research plans for implementing and improving\nexisting framework approaches based on the overall usage profile. The work\nfocuses on adapting and applying analysis framework approaches to UK energy\ndata in order to determine the effectiveness of creating a few (single figures)\narchetypical users with the intention of improving on the current methods of\ndetermining usage profiles. The work is currently in progress and the paper\ndetails initial results using data collected in Milton Keynes around 1990.\nVarious possible enhancements to the work are considered including a split\nbased on temperature to reflect the varying UK weather conditions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 15:45:09 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Dent", "Ian", ""], ["Aickelin", "Uwe", ""], ["Rodden", "Tom", ""]]}, {"id": "1307.1385", "submitter": "Uwe Aickelin", "authors": "Ian Dent, Christian Wagner, Uwe Aickelin, Tom Rodden", "title": "Creating Personalised Energy Plans. From Groups to Individuals using\n  Fuzzy C Means Clustering", "comments": "Digital Engagement 11, Newcastle, November 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Changes in the UK electricity market mean that domestic users will be\nrequired to modify their usage behaviour in order that supplies can be\nmaintained. Clustering allows usage profiles collected at the household level\nto be clustered into groups and assigned a stereotypical profile which can be\nused to target marketing campaigns. Fuzzy C Means clustering extends this by\nallowing each household to be a member of many groups and hence provides the\nopportunity to make personalised offers to the household dependent on their\ndegree of membership of each group. In addition, feedback can be provided on\nhow user's changing behaviour is moving them towards more \"green\" or cost\neffective stereotypical usage.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 15:55:33 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Dent", "Ian", ""], ["Wagner", "Christian", ""], ["Aickelin", "Uwe", ""], ["Rodden", "Tom", ""]]}, {"id": "1307.1387", "submitter": "Uwe Aickelin", "authors": "Hala Helmi, Jon M. Garibaldi and Uwe Aickelin", "title": "Examining the Classification Accuracy of TSVMs with ?Feature Selection\n  in Comparison with the GLAD Algorithm", "comments": "UKCI 2011, the 11th Annual Workshop on Computational Intelligence,\n  Manchester, pp 7-12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene expression data sets are used to classify and predict patient diagnostic\ncategories. As we know, it is extremely difficult and expensive to obtain gene\nexpression labelled examples. Moreover, conventional supervised approaches\ncannot function properly when labelled data (training examples) are\ninsufficient using Support Vector Machines (SVM) algorithms. Therefore, in this\npaper, we suggest Transductive Support Vector Machines (TSVMs) as\nsemi-supervised learning algorithms, learning with both labelled samples data\nand unlabelled samples to perform the classification of microarray data. To\nprune the superfluous genes and samples we used a feature selection method\ncalled Recursive Feature Elimination (RFE), which is supposed to enhance the\noutput of classification and avoid the local optimization problem. We examined\nthe classification prediction accuracy of the TSVM-RFE algorithm in comparison\nwith the Genetic Learning Across Datasets (GLAD) algorithm, as both are\nsemi-supervised learning methods. Comparing these two methods, we found that\nthe TSVM-RFE surpassed both a SVM using RFE and GLAD.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 16:06:25 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Helmi", "Hala", ""], ["Garibaldi", "Jon M.", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1307.1391", "submitter": "Uwe Aickelin", "authors": "Feng Gu, Jan Feyereisl, Robert Oates, Jenna Reps, Julie Greensmith,\n  Uwe Aickelin", "title": "Quiet in Class: Classification, Noise and the Dendritic Cell Algorithm", "comments": "Proceedings of the 10th International Conference on Artificial Immune\n  Systems (ICARIS 2011), LNCS Volume 6825, Cambridge, UK, pp 173-186, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theoretical analyses of the Dendritic Cell Algorithm (DCA) have yielded\nseveral criticisms about its underlying structure and operation. As a result,\nseveral alterations and fixes have been suggested in the literature to correct\nfor these findings. A contribution of this work is to investigate the effects\nof replacing the classification stage of the DCA (which is known to be flawed)\nwith a traditional machine learning technique. This work goes on to question\nthe merits of those unique properties of the DCA that are yet to be thoroughly\nanalysed. If none of these properties can be found to have a benefit over\ntraditional approaches, then \"fixing\" the DCA is arguably less efficient than\nsimply creating a new algorithm. This work examines the dynamic filtering\nproperty of the DCA and questions the utility of this unique feature for the\nanomaly detection problem. It is found that this feature, while advantageous\nfor noisy, time-ordered classification, is not as useful as a traditional\nstatic filter for processing a synthetic dataset. It is concluded that there\nare still unique features of the DCA left to investigate. Areas that may be of\nbenefit to the Artificial Immune Systems community are suggested.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 16:19:21 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Gu", "Feng", ""], ["Feyereisl", "Jan", ""], ["Oates", "Robert", ""], ["Reps", "Jenna", ""], ["Greensmith", "Julie", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1307.1394", "submitter": "Uwe Aickelin", "authors": "Yihui Liu, Uwe Aickelin", "title": "Detect adverse drug reactions for drug Alendronate", "comments": "Second International Conference on Business Computing and Global\n  Informatization (BCGIN), pp 820-823, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adverse drug reaction (ADR) is widely concerned for public health issue. In\nthis study we propose an original approach to detect the ADRs using feature\nmatrix and feature selection. The experiments are carried out on the drug\nSimvastatin. Major side effects for the drug are detected and better\nperformance is achieved compared to other computerized methods. The detected\nADRs are based on the computerized method, further investigation is needed.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 16:24:17 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Liu", "Yihui", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1307.1411", "submitter": "Uwe Aickelin", "authors": "Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack\n  E. Gibson, Richard B. Hubbard", "title": "Discovering Sequential Patterns in a UK General Practice Database", "comments": "2012 IEEE-EMBS International Conference on Biomedical and Health\n  Informatics, pp 960-963, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wealth of computerised medical information becoming readily available\npresents the opportunity to examine patterns of illnesses, therapies and\nresponses. These patterns may be able to predict illnesses that a patient is\nlikely to develop, allowing the implementation of preventative actions. In this\npaper sequential rule mining is applied to a General Practice database to find\nrules involving a patients age, gender and medical history. By incorporating\nthese rules into current health-care a patient can be highlighted as\nsusceptible to a future illness based on past or current illnesses, gender and\nyear of birth. This knowledge has the ability to greatly improve health-care\nand reduce health-care costs.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 17:01:44 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Reps", "Jenna", ""], ["Garibaldi", "Jonathan M.", ""], ["Aickelin", "Uwe", ""], ["Soria", "Daniele", ""], ["Gibson", "Jack E.", ""], ["Hubbard", "Richard B.", ""]]}, {"id": "1307.1493", "submitter": "Stefan Wager", "authors": "Stefan Wager, Sida Wang, and Percy Liang", "title": "Dropout Training as Adaptive Regularization", "comments": "11 pages. Advances in Neural Information Processing Systems (NIPS),\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout and other feature noising schemes control overfitting by artificially\ncorrupting the training data. For generalized linear models, dropout performs a\nform of adaptive regularization. Using this viewpoint, we show that the dropout\nregularizer is first-order equivalent to an L2 regularizer applied after\nscaling the features by an estimate of the inverse diagonal Fisher information\nmatrix. We also establish a connection to AdaGrad, an online learning\nalgorithm, and find that a close relative of AdaGrad operates by repeatedly\nsolving linear dropout-regularized problems. By casting dropout as\nregularization, we develop a natural semi-supervised algorithm that uses\nunlabeled data to create a better adaptive regularizer. We apply this idea to\ndocument classification tasks, and show that it consistently boosts the\nperformance of dropout training, improving on state-of-the-art results on the\nIMDB reviews dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 21:33:56 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2013 17:56:35 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Wager", "Stefan", ""], ["Wang", "Sida", ""], ["Liang", "Percy", ""]]}, {"id": "1307.1584", "submitter": "Uwe Aickelin", "authors": "Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack\n  E. Gibson, Richard B. Hubbard", "title": "Comparing Data-mining Algorithms Developed for Longitudinal\n  Observational Databases", "comments": "UKCI 2012, the 12th Annual Workshop on Computational Intelligence,\n  Heriot-Watt University, pp 1-8, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal observational databases have become a recent interest in the\npost marketing drug surveillance community due to their ability of presenting a\nnew perspective for detecting negative side effects. Algorithms mining\nlongitudinal observation databases are not restricted by many of the\nlimitations associated with the more conventional methods that have been\ndeveloped for spontaneous reporting system databases. In this paper we\ninvestigate the robustness of four recently developed algorithms that mine\nlongitudinal observational databases by applying them to The Health Improvement\nNetwork (THIN) for six drugs with well document known negative side effects.\nOur results show that none of the existing algorithms was able to consistently\nidentify known adverse drug reactions above events related to the cause of the\ndrug and no algorithm was superior.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 11:24:55 GMT"}], "update_date": "2013-07-08", "authors_parsed": [["Reps", "Jenna", ""], ["Garibaldi", "Jonathan M.", ""], ["Aickelin", "Uwe", ""], ["Soria", "Daniele", ""], ["Gibson", "Jack E.", ""], ["Hubbard", "Richard B.", ""]]}, {"id": "1307.1599", "submitter": "Uwe Aickelin", "authors": "Chris Roadknight, Uwe Aickelin, Guoping Qiu, John Scholefield, Lindy\n  Durrant", "title": "Supervised Learning and Anti-learning of Colorectal Cancer Classes and\n  Survival Rates from Cellular Biology Parameters", "comments": "IEEE International Conference on Systems, Man, and Cybernetics, pp\n  797-802, 2012", "journal-ref": null, "doi": "10.1109/ICSMC.2012.6377825", "report-no": null, "categories": "cs.LG cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a dataset relating to cellular and physical\nconditions of patients who are operated upon to remove colorectal tumours. This\ndata provides a unique insight into immunological status at the point of tumour\nremoval, tumour classification and post-operative survival. Attempts are made\nto learn relationships between attributes (physical and immunological) and the\nresulting tumour stage and survival. Results for conventional machine learning\napproaches can be considered poor, especially for predicting tumour stages for\nthe most important types of cancer. This poor performance is further\ninvestigated and compared with a synthetic, dataset based on the logical\nexclusive-OR function and it is shown that there is a significant level of\n'anti-learning' present in all supervised methods used and this can be\nexplained by the highly dimensional, complex and sparsely representative\ndataset. For predicting the stage of cancer from the immunological attributes,\nanti-learning approaches outperform a range of popular algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 12:53:28 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Roadknight", "Chris", ""], ["Aickelin", "Uwe", ""], ["Qiu", "Guoping", ""], ["Scholefield", "John", ""], ["Durrant", "Lindy", ""]]}, {"id": "1307.1601", "submitter": "Uwe Aickelin", "authors": "Chris Roadknight, Uwe Aickelin, Alex Ladas, Daniele Soria, John\n  Scholefield and Lindy Durrant", "title": "Biomarker Clustering of Colorectal Cancer Data to Complement Clinical\n  Classification", "comments": "Federated Conference on Computer Science and Information Systems\n  (FedCSIS), pp 187-191, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a dataset relating to cellular and physical\nconditions of patients who are operated upon to remove colorectal tumours. This\ndata provides a unique insight into immunological status at the point of tumour\nremoval, tumour classification and post-operative survival. Attempts are made\nto cluster this dataset and important subsets of it in an effort to\ncharacterize the data and validate existing standards for tumour\nclassification. It is apparent from optimal clustering that existing tumour\nclassification is largely unrelated to immunological factors within a patient\nand that there may be scope for re-evaluating treatment options and survival\nestimates based on a combination of tumour physiology and patient\nhistochemistry.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 12:56:24 GMT"}], "update_date": "2013-07-08", "authors_parsed": [["Roadknight", "Chris", ""], ["Aickelin", "Uwe", ""], ["Ladas", "Alex", ""], ["Soria", "Daniele", ""], ["Scholefield", "John", ""], ["Durrant", "Lindy", ""]]}, {"id": "1307.1662", "submitter": "Rami Al-Rfou", "authors": "Rami Al-Rfou, Bryan Perozzi, Steven Skiena", "title": "Polyglot: Distributed Word Representations for Multilingual NLP", "comments": "10 pages, 2 figures, Proceedings of Conference on Computational\n  Natural Language Learning CoNLL'2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed word representations (word embeddings) have recently contributed\nto competitive performance in language modeling and several NLP tasks. In this\nwork, we train word embeddings for more than 100 languages using their\ncorresponding Wikipedias. We quantitatively demonstrate the utility of our word\nembeddings by using them as the sole features for training a part of speech\ntagger for a subset of these languages. We find their performance to be\ncompetitive with near state-of-art methods in English, Danish and Swedish.\nMoreover, we investigate the semantic features captured by these embeddings\nthrough the proximity of word groupings. We will release these embeddings\npublicly to help researchers in the development and enhancement of multilingual\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 16:52:09 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 17:31:33 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Al-Rfou", "Rami", ""], ["Perozzi", "Bryan", ""], ["Skiena", "Steven", ""]]}, {"id": "1307.1674", "submitter": "Raman Arora", "authors": "Raman Arora, Andrew Cotter, and Nathan Srebro", "title": "Stochastic Optimization of PCA with Capped MSG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study PCA as a stochastic optimization problem and propose a novel\nstochastic approximation algorithm which we refer to as \"Matrix Stochastic\nGradient\" (MSG), as well as a practical variant, Capped MSG. We study the\nmethod both theoretically and empirically.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 17:39:40 GMT"}], "update_date": "2013-07-08", "authors_parsed": [["Arora", "Raman", ""], ["Cotter", "Andrew", ""], ["Srebro", "Nathan", ""]]}, {"id": "1307.1759", "submitter": "Jayakrishnan Unnikrishnan", "authors": "Wei Chen, Dayu Huang, Ankur A. Kulkarni, Jayakrishnan Unnikrishnan,\n  Quanyan Zhu, Prashant Mehta, Sean Meyn, Adam Wierman", "title": "Approximate dynamic programming using fluid and diffusion approximations\n  with applications to power management", "comments": "Submitted to SIAM Journal on Control and Optimization (SICON), July\n  2013", "journal-ref": null, "doi": "10.1109/CDC.2009.5399685", "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuro-dynamic programming is a class of powerful techniques for approximating\nthe solution to dynamic programming equations. In their most computationally\nattractive formulations, these techniques provide the approximate solution only\nwithin a prescribed finite-dimensional function class. Thus, the question that\nalways arises is how should the function class be chosen? The goal of this\npaper is to propose an approach using the solutions to associated fluid and\ndiffusion approximations. In order to illustrate this approach, the paper\nfocuses on an application to dynamic speed scaling for power management in\ncomputer processors.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2013 07:25:35 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2013 05:57:37 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Chen", "Wei", ""], ["Huang", "Dayu", ""], ["Kulkarni", "Ankur A.", ""], ["Unnikrishnan", "Jayakrishnan", ""], ["Zhu", "Quanyan", ""], ["Mehta", "Prashant", ""], ["Meyn", "Sean", ""], ["Wierman", "Adam", ""]]}, {"id": "1307.1769", "submitter": "Lior Rokach", "authors": "Lior Rokach, Alon Schclar, Ehud Itach", "title": "Ensemble Methods for Multi-label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble methods have been shown to be an effective tool for solving\nmulti-label classification tasks. In the RAndom k-labELsets (RAKEL) algorithm,\neach member of the ensemble is associated with a small randomly-selected subset\nof k labels. Then, a single label classifier is trained according to each\ncombination of elements in the subset. In this paper we adopt a similar\napproach, however, instead of randomly choosing subsets, we select the minimum\nrequired subsets of k labels that cover all labels and meet additional\nconstraints such as coverage of inter-label correlations. Construction of the\ncover is achieved by formulating the subset selection as a minimum set covering\nproblem (SCP) and solving it by using approximation algorithms. Every cover\nneeds only to be prepared once by offline algorithms. Once prepared, a cover\nmay be applied to the classification of any given multi-label dataset whose\nproperties conform with those of the cover. The contribution of this paper is\ntwo-fold. First, we introduce SCP as a general framework for constructing label\ncovers while allowing the user to incorporate cover construction constraints.\nWe demonstrate the effectiveness of this framework by proposing two\nconstruction constraints whose enforcement produces covers that improve the\nprediction performance of random selection. Second, we provide theoretical\nbounds that quantify the probabilities of random selection to produce covers\nthat meet the proposed construction criteria. The experimental results indicate\nthat the proposed methods improve multi-label classification accuracy and\nstability compared with the RAKEL algorithm and to other state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2013 10:17:44 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Rokach", "Lior", ""], ["Schclar", "Alon", ""], ["Itach", "Ehud", ""]]}, {"id": "1307.1827", "submitter": "Sivan Sabato", "authors": "Daniel Hsu and Sivan Sabato", "title": "Loss minimization and parameter estimation with heavy tails", "comments": "Final version as published in JMLR", "journal-ref": "Journal of Machine Learning Research, 17(18):1--40, 2016", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies applications and generalizations of a simple estimation\ntechnique that provides exponential concentration under heavy-tailed\ndistributions, assuming only bounded low-order moments. We show that the\ntechnique can be used for approximate minimization of smooth and strongly\nconvex losses, and specifically for least squares linear regression. For\ninstance, our $d$-dimensional estimator requires just\n$\\tilde{O}(d\\log(1/\\delta))$ random samples to obtain a constant factor\napproximation to the optimal least squares loss with probability $1-\\delta$,\nwithout requiring the covariates or noise to be bounded or subgaussian. We\nprovide further applications to sparse linear regression and low-rank\ncovariance matrix estimation with similar allowances on the noise and covariate\ndistributions. The core technique is a generalization of the median-of-means\nestimator to arbitrary metric spaces.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2013 01:38:16 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2013 19:48:02 GMT"}, {"version": "v3", "created": "Fri, 9 Aug 2013 16:50:15 GMT"}, {"version": "v4", "created": "Tue, 5 Nov 2013 16:32:14 GMT"}, {"version": "v5", "created": "Fri, 28 Feb 2014 02:57:37 GMT"}, {"version": "v6", "created": "Tue, 8 Jul 2014 19:55:25 GMT"}, {"version": "v7", "created": "Mon, 18 Apr 2016 09:05:38 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Hsu", "Daniel", ""], ["Sabato", "Sivan", ""]]}, {"id": "1307.1954", "submitter": "Matthew Blaschko", "authors": "Wojciech Zaremba (INRIA Saclay - Ile de France, CVN), Arthur Gretton,\n  Matthew Blaschko (INRIA Saclay - Ile de France, CVN)", "title": "B-tests: Low Variance Kernel Two-Sample Tests", "comments": "Neural Information Processing Systems (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A family of maximum mean discrepancy (MMD) kernel two-sample tests is\nintroduced. Members of the test family are called Block-tests or B-tests, since\nthe test statistic is an average over MMDs computed on subsets of the samples.\nThe choice of block size allows control over the tradeoff between test power\nand computation time. In this respect, the $B$-test family combines favorable\nproperties of previously proposed MMD two-sample tests: B-tests are more\npowerful than a linear time test where blocks are just pairs of samples, yet\nthey are more computationally efficient than a quadratic time test where a\nsingle large block incorporating all the samples is used to compute a\nU-statistic. A further important advantage of the B-tests is their\nasymptotically Normal null distribution: this is by contrast with the\nU-statistic, which is degenerate under the null hypothesis, and for which\nestimates of the null distribution are computationally demanding. Recent\nresults on kernel selection for hypothesis testing transfer seamlessly to the\nB-tests, yielding a means to optimize test power via kernel choice.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 06:10:58 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2013 09:51:44 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2014 20:39:40 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Zaremba", "Wojciech", "", "INRIA Saclay - Ile de France, CVN"], ["Gretton", "Arthur", "", "INRIA Saclay - Ile de France, CVN"], ["Blaschko", "Matthew", "", "INRIA Saclay - Ile de France, CVN"]]}, {"id": "1307.1998", "submitter": "Uwe Aickelin", "authors": "Alexandros Ladas, Uwe Aickelin, Jon Garibaldi, Eamonn Ferguson", "title": "Using Clustering to extract Personality Information from socio economic\n  data", "comments": "UKCI 2012, the 12th Annual Workshop on Computational Intelligence,\n  Heriot-Watt University, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has become apparent that models that have been applied widely in\neconomics, including Machine Learning techniques and Data Mining methods,\nshould take into consideration principles that derive from the theories of\nPersonality Psychology in order to discover more comprehensive knowledge\nregarding complicated economic behaviours. In this work, we present a method to\nextract Behavioural Groups by using simple clustering techniques that can\npotentially reveal aspects of the Personalities for their members. We believe\nthat this is very important because the psychological information regarding the\nPersonalities of individuals is limited in real world applications and because\nit can become a useful tool in improving the traditional models of Knowledge\nEconomy.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 09:25:07 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Ladas", "Alexandros", ""], ["Aickelin", "Uwe", ""], ["Garibaldi", "Jon", ""], ["Ferguson", "Eamonn", ""]]}, {"id": "1307.2111", "submitter": "Uwe Aickelin", "authors": "Ian Dent, Tony Craig, Uwe Aickelin, Tom Rodden", "title": "Finding the creatures of habit; Clustering households based on their\n  flexibility in using electricity", "comments": "Digital Futures 2012, Aberdeen, UK, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Changes in the UK electricity market, particularly with the roll out of smart\nmeters, will provide greatly increased opportunities for initiatives intended\nto change households' electricity usage patterns for the benefit of the overall\nsystem. Users show differences in their regular behaviours and clustering\nhouseholds into similar groupings based on this variability provides for\nefficient targeting of initiatives. Those people who are stuck into a regular\npattern of activity may be the least receptive to an initiative to change\nbehaviour. A sample of 180 households from the UK are clustered into four\ngroups as an initial test of the concept and useful, actionable groupings are\nfound.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 14:47:42 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Dent", "Ian", ""], ["Craig", "Tony", ""], ["Aickelin", "Uwe", ""], ["Rodden", "Tom", ""]]}, {"id": "1307.2118", "submitter": "David  McAllester", "authors": "David McAllester", "title": "A PAC-Bayesian Tutorial with A Dropout Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial gives a concise overview of existing PAC-Bayesian theory\nfocusing on three generalization bounds. The first is an Occam bound which\nhandles rules with finite precision parameters and which states that\ngeneralization loss is near training loss when the number of bits needed to\nwrite the rule is small compared to the sample size. The second is a\nPAC-Bayesian bound providing a generalization guarantee for posterior\ndistributions rather than for individual rules. The PAC-Bayesian bound\nnaturally handles infinite precision rule parameters, $L_2$ regularization,\n{\\em provides a bound for dropout training}, and defines a natural notion of a\nsingle distinguished PAC-Bayesian posterior distribution. The third bound is a\ntraining-variance bound --- a kind of bias-variance analysis but with bias\nreplaced by expected training loss. The training-variance bound dominates the\nother bounds but is more difficult to interpret. It seems to suggest variance\nreduction methods such as bagging and may ultimately provide a more meaningful\nanalysis of dropouts.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 15:03:03 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["McAllester", "David", ""]]}, {"id": "1307.2150", "submitter": "Yaroslav Halchenko", "authors": "Yaroslav O. Halchenko, Michael Hanke, James V. Haxby, Stephen Jose\n  Hanson, Christoph S. Herrmann", "title": "Transmodal Analysis of Neural Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing neuronal activity in the brain, both in time and in space, is a\ncentral challenge to advance the understanding of brain function. Because of\nthe inability of any single neuroimaging techniques to cover all aspects at\nonce, there is a growing interest to combine signals from multiple modalities\nin order to benefit from the advantages of each acquisition method. Due to the\ncomplexity and unknown parameterization of any suggested complete model of BOLD\nresponse in functional magnetic resonance imaging (fMRI), the development of a\nreliable ultimate fusion approach remains difficult. But besides the primary\ngoal of superior temporal and spatial resolution, conjoint analysis of data\nfrom multiple imaging modalities can alternatively be used to segregate neural\ninformation from physiological and acquisition noise. In this paper we suggest\na novel methodology which relies on constructing a quantifiable mapping of data\nfrom one modality (electroencephalography; EEG) into another (fMRI), called\ntransmodal analysis of neural signals (TRANSfusion). TRANSfusion attempts to\nmap neural data embedded within the EEG signal into its reflection in fMRI\ndata. Assessing the mapping performance on unseen data allows to localize brain\nareas where a significant portion of the signal could be reliably\nreconstructed, hence the areas neural activity of which is reflected in both\nEEG and fMRI data. Consecutive analysis of the learnt model allows to localize\nareas associated with specific frequency bands of EEG, or areas functionally\nrelated (connected or coherent) to any given EEG sensor. We demonstrate the\nperformance of TRANSfusion on artificial and real data from an auditory\nexperiment. We further speculate on possible alternative uses: cross-modal data\nfiltering and EEG-driven interpolation of fMRI signals to obtain arbitrarily\nhigh temporal sampling of BOLD.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 16:30:29 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Halchenko", "Yaroslav O.", ""], ["Hanke", "Michael", ""], ["Haxby", "James V.", ""], ["Hanson", "Stephen Jose", ""], ["Herrmann", "Christoph S.", ""]]}, {"id": "1307.2307", "submitter": "Kun Zhang", "authors": "Kun Zhang, Heng Peng, Laiwan Chan, Aapo Hyvarinen", "title": "Bridging Information Criteria and Parameter Shrinkage for Model\n  Selection", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection based on classical information criteria, such as BIC, is\ngenerally computationally demanding, but its properties are well studied. On\nthe other hand, model selection based on parameter shrinkage by $\\ell_1$-type\npenalties is computationally efficient. In this paper we make an attempt to\ncombine their strengths, and propose a simple approach that penalizes the\nlikelihood with data-dependent $\\ell_1$ penalties as in adaptive Lasso and\nexploits a fixed penalization parameter. Even for finite samples, its model\nselection results approximately coincide with those based on information\ncriteria; in particular, we show that in some special cases, this approach and\nthe corresponding information criterion produce exactly the same model. One can\nalso consider this approach as a way to directly determine the penalization\nparameter in adaptive Lasso to achieve information criteria-like model\nselection. As extensions, we apply this idea to complex models including\nGaussian mixture model and mixture of factor analyzers, whose model selection\nis traditionally difficult to do; by adopting suitable penalties, we provide\ncontinuous approximators to the corresponding information criteria, which are\neasy to optimize and enable efficient model selection.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 23:52:55 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Zhang", "Kun", ""], ["Peng", "Heng", ""], ["Chan", "Laiwan", ""], ["Hyvarinen", "Aapo", ""]]}, {"id": "1307.2312", "submitter": "Diane Oyen", "authors": "Diane Oyen and Terran Lane", "title": "Bayesian Discovery of Multiple Bayesian Networks via Transfer Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian network structure learning algorithms with limited data are being\nused in domains such as systems biology and neuroscience to gain insight into\nthe underlying processes that produce observed data. Learning reliable networks\nfrom limited data is difficult, therefore transfer learning can improve the\nrobustness of learned networks by leveraging data from related tasks. Existing\ntransfer learning algorithms for Bayesian network structure learning give a\nsingle maximum a posteriori estimate of network models. Yet, many other models\nmay be equally likely, and so a more informative result is provided by Bayesian\nstructure discovery. Bayesian structure discovery algorithms estimate posterior\nprobabilities of structural features, such as edges. We present transfer\nlearning for Bayesian structure discovery which allows us to explore the shared\nand unique structural features among related tasks. Efficient computation\nrequires that our transfer learning objective factors into local calculations,\nwhich we prove is given by a broad class of transfer biases. Theoretically, we\nshow the efficiency of our approach. Empirically, we show that compared to\nsingle task learning, transfer learning is better able to positively identify\ntrue edges. We apply the method to whole-brain neuroimaging data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 00:58:10 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Oyen", "Diane", ""], ["Lane", "Terran", ""]]}, {"id": "1307.2579", "submitter": "Jonathan Huang", "authors": "Chris Piech, Jonathan Huang, Zhenghao Chen, Chuong Do, Andrew Ng,\n  Daphne Koller", "title": "Tuned Models of Peer Assessment in MOOCs", "comments": "Proceedings of The 6th International Conference on Educational Data\n  Mining (EDM 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In massive open online courses (MOOCs), peer grading serves as a critical\ntool for scaling the grading of complex, open-ended assignments to courses with\ntens or hundreds of thousands of students. But despite promising initial\ntrials, it does not always deliver accurate results compared to human experts.\nIn this paper, we develop algorithms for estimating and correcting for grader\nbiases and reliabilities, showing significant improvement in peer grading\naccuracy on real data with 63,199 peer grades from Coursera's HCI course\nofferings --- the largest peer grading networks analysed to date. We relate\ngrader biases and reliabilities to other student factors such as student\nengagement, performance as well as commenting style. We also show that our\nmodel can lead to more intelligent assignment of graders to gradees.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 20:03:51 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Piech", "Chris", ""], ["Huang", "Jonathan", ""], ["Chen", "Zhenghao", ""], ["Do", "Chuong", ""], ["Ng", "Andrew", ""], ["Koller", "Daphne", ""]]}, {"id": "1307.2611", "submitter": "Diane Oyen", "authors": "Diane Oyen, Alexandru Niculescu-Mizil, Rachel Ostroff, Alex Stewart,\n  Vincent P. Clark", "title": "Controlling the Precision-Recall Tradeoff in Differential Dependency\n  Network Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models have gained a lot of attention recently as a tool for\nlearning and representing dependencies among variables in multivariate data.\nOften, domain scientists are looking specifically for differences among the\ndependency networks of different conditions or populations (e.g. differences\nbetween regulatory networks of different species, or differences between\ndependency networks of diseased versus healthy populations). The standard\nmethod for finding these differences is to learn the dependency networks for\neach condition independently and compare them. We show that this approach is\nprone to high false discovery rates (low precision) that can render the\nanalysis useless. We then show that by imposing a bias towards learning similar\ndependency networks for each condition the false discovery rates can be reduced\nto acceptable levels, at the cost of finding a reduced number of differences.\nAlgorithms developed in the transfer learning literature can be used to vary\nthe strength of the imposed similarity bias and provide a natural mechanism to\nsmoothly adjust this differential precision-recall tradeoff to cater to the\nrequirements of the analysis conducted. We present real case studies\n(oncological and neurological) where domain experts use the proposed technique\nto extract useful differential networks that shed light on the biological\nprocesses involved in cancer and brain function.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 22:07:55 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Oyen", "Diane", ""], ["Niculescu-Mizil", "Alexandru", ""], ["Ostroff", "Rachel", ""], ["Stewart", "Alex", ""], ["Clark", "Vincent P.", ""]]}, {"id": "1307.2674", "submitter": "Hongwei Li", "authors": "Hongwei Li, Bin Yu and Dengyong Zhou", "title": "Error Rate Bounds in Crowdsourcing Models", "comments": "13 pages, 3 figures, downloadable supplementary files", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is an effective tool for human-powered computation on many\ntasks challenging for computers. In this paper, we provide finite-sample\nexponential bounds on the error rate (in probability and in expectation) of\nhyperplane binary labeling rules under the Dawid-Skene crowdsourcing model. The\nbounds can be applied to analyze many common prediction methods, including the\nmajority voting and weighted majority voting. These bound results could be\nuseful for controlling the error rate and designing better algorithms. We show\nthat the oracle Maximum A Posterior (MAP) rule approximately optimizes our\nupper bound on the mean error rate for any hyperplane binary labeling rule, and\npropose a simple data-driven weighted majority voting (WMV) rule (called\none-step WMV) that attempts to approximate the oracle MAP and has a provable\ntheoretical guarantee on the error rate. Moreover, we use simulated and real\ndata to demonstrate that the data-driven EM-MAP rule is a good approximation to\nthe oracle MAP rule, and to demonstrate that the mean error rate of the\ndata-driven EM-MAP rule is also bounded by the mean error rate bound of the\noracle MAP rule with estimated parameters plugging into the bound.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 05:19:10 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Li", "Hongwei", ""], ["Yu", "Bin", ""], ["Zhou", "Dengyong", ""]]}, {"id": "1307.2855", "submitter": "Zeyuan Allen Zhu", "authors": "Lorenzo Orecchia, Zeyuan Allen Zhu", "title": "Flow-Based Algorithms for Local Graph Clustering", "comments": "A shorter version of this paper has appeared in the proceedings of\n  the 25th ACM-SIAM Symposium on Discrete Algorithms (SODA) 2014", "journal-ref": null, "doi": "10.1137/1.9781611973402.94", "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a subset S of vertices of an undirected graph G, the cut-improvement\nproblem asks us to find a subset S that is similar to A but has smaller\nconductance. A very elegant algorithm for this problem has been given by\nAndersen and Lang [AL08] and requires solving a small number of\nsingle-commodity maximum flow computations over the whole graph G. In this\npaper, we introduce LocalImprove, the first cut-improvement algorithm that is\nlocal, i.e. that runs in time dependent on the size of the input set A rather\nthan on the size of the entire graph. Moreover, LocalImprove achieves this\nlocal behaviour while essentially matching the same theoretical guarantee as\nthe global algorithm of Andersen and Lang.\n  The main application of LocalImprove is to the design of better\nlocal-graph-partitioning algorithms. All previously known local algorithms for\ngraph partitioning are random-walk based and can only guarantee an output\nconductance of O(\\sqrt{OPT}) when the target set has conductance OPT \\in [0,1].\nVery recently, Zhu, Lattanzi and Mirrokni [ZLM13] improved this to O(OPT /\n\\sqrt{CONN}) where the internal connectivity parameter CONN \\in [0,1] is\ndefined as the reciprocal of the mixing time of the random walk over the graph\ninduced by the target set. In this work, we show how to use LocalImprove to\nobtain a constant approximation O(OPT) as long as CONN/OPT = Omega(1). This\nyields the first flow-based algorithm. Moreover, its performance strictly\noutperforms the ones based on random walks and surprisingly matches that of the\nbest known global algorithm, which is SDP-based, in this parameter regime\n[MMV12].\n  Finally, our results show that spectral methods are not the only viable\napproach to the construction of local graph partitioning algorithm and open\ndoor to the study of algorithms with even better approximation and locality\nguarantees.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 17:04:35 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2013 19:44:03 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Orecchia", "Lorenzo", ""], ["Zhu", "Zeyuan Allen", ""]]}, {"id": "1307.2965", "submitter": "Quan Wang", "authors": "Quan Wang, Dijia Wu, Le Lu, Meizhu Liu, Kim L. Boyer and Shaohua Kevin\n  Zhou", "title": "Semantic Context Forests for Learning-Based Knee Cartilage Segmentation\n  in 3D MR Images", "comments": "MICCAI 2013: Workshop on Medical Computer Vision", "journal-ref": null, "doi": "10.1007/978-3-319-05530-5_11", "report-no": null, "categories": "cs.CV cs.LG q-bio.TO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic segmentation of human knee cartilage from 3D MR images is a\nuseful yet challenging task due to the thin sheet structure of the cartilage\nwith diffuse boundaries and inhomogeneous intensities. In this paper, we\npresent an iterative multi-class learning method to segment the femoral, tibial\nand patellar cartilage simultaneously, which effectively exploits the spatial\ncontextual constraints between bone and cartilage, and also between different\ncartilages. First, based on the fact that the cartilage grows in only certain\narea of the corresponding bone surface, we extract the distance features of not\nonly to the surface of the bone, but more informatively, to the densely\nregistered anatomical landmarks on the bone surface. Second, we introduce a set\nof iterative discriminative classifiers that at each iteration, probability\ncomparison features are constructed from the class confidence maps derived by\npreviously learned classifiers. These features automatically embed the semantic\ncontext information between different cartilages of interest. Validated on a\ntotal of 176 volumes from the Osteoarthritis Initiative (OAI) dataset, the\nproposed approach demonstrates high robustness and accuracy of segmentation in\ncomparison with existing state-of-the-art MR cartilage segmentation methods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 03:29:51 GMT"}, {"version": "v2", "created": "Tue, 22 Apr 2014 16:01:12 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Wang", "Quan", ""], ["Wu", "Dijia", ""], ["Lu", "Le", ""], ["Liu", "Meizhu", ""], ["Boyer", "Kim L.", ""], ["Zhou", "Shaohua Kevin", ""]]}, {"id": "1307.2971", "submitter": "Ana Georgina Flesia MS", "authors": "Ana Georgina Flesia, Josef Baumgartner, Javier Gimenez, Jorge Martinez", "title": "Accuracy of MAP segmentation with hidden Potts and Markov mesh prior\n  models via Path Constrained Viterbi Training, Iterated Conditional Modes and\n  Graph Cut based algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study statistical classification accuracy of two different\nMarkov field environments for pixelwise image segmentation, considering the\nlabels of the image as hidden states and solving the estimation of such labels\nas a solution of the MAP equation. The emission distribution is assumed the\nsame in all models, and the difference lays in the Markovian prior hypothesis\nmade over the labeling random field. The a priori labeling knowledge will be\nmodeled with a) a second order anisotropic Markov Mesh and b) a classical\nisotropic Potts model. Under such models, we will consider three different\nsegmentation procedures, 2D Path Constrained Viterbi training for the Hidden\nMarkov Mesh, a Graph Cut based segmentation for the first order isotropic Potts\nmodel, and ICM (Iterated Conditional Modes) for the second order isotropic\nPotts model.\n  We provide a unified view of all three methods, and investigate goodness of\nfit for classification, studying the influence of parameter estimation,\ncomputational gain, and extent of automation in the statistical measures\nOverall Accuracy, Relative Improvement and Kappa coefficient, allowing robust\nand accurate statistical analysis on synthetic and real-life experimental data\ncoming from the field of Dental Diagnostic Radiography. All algorithms, using\nthe learned parameters, generate good segmentations with little interaction\nwhen the images have a clear multimodal histogram. Suboptimal learning proves\nto be frail in the case of non-distinctive modes, which limits the complexity\nof usable models, and hence the achievable error rate as well.\n  All Matlab code written is provided in a toolbox available for download from\nour website, following the Reproducible Research Paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 04:49:11 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Flesia", "Ana Georgina", ""], ["Baumgartner", "Josef", ""], ["Gimenez", "Javier", ""], ["Martinez", "Jorge", ""]]}, {"id": "1307.3102", "submitter": "Vitaly Feldman", "authors": "Maria Florina Balcan, Vitaly Feldman", "title": "Statistical Active Learning Algorithms for Noise Tolerance and\n  Differential Privacy", "comments": "Extended abstract appears in NIPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a framework for designing efficient active learning algorithms\nthat are tolerant to random classification noise and are\ndifferentially-private. The framework is based on active learning algorithms\nthat are statistical in the sense that they rely on estimates of expectations\nof functions of filtered random examples. It builds on the powerful statistical\nquery framework of Kearns (1993).\n  We show that any efficient active statistical learning algorithm can be\nautomatically converted to an efficient active learning algorithm which is\ntolerant to random classification noise as well as other forms of\n\"uncorrelated\" noise. The complexity of the resulting algorithms has\ninformation-theoretically optimal quadratic dependence on $1/(1-2\\eta)$, where\n$\\eta$ is the noise rate.\n  We show that commonly studied concept classes including thresholds,\nrectangles, and linear separators can be efficiently actively learned in our\nframework. These results combined with our generic conversion lead to the first\ncomputationally-efficient algorithms for actively learning some of these\nconcept classes in the presence of random classification noise that provide\nexponential improvement in the dependence on the error $\\epsilon$ over their\npassive counterparts. In addition, we show that our algorithms can be\nautomatically converted to efficient active differentially-private algorithms.\nThis leads to the first differentially-private active learning algorithms with\nexponential label savings over the passive case.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 13:31:21 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2013 02:13:05 GMT"}, {"version": "v3", "created": "Fri, 3 Jan 2014 05:38:04 GMT"}, {"version": "v4", "created": "Wed, 5 Nov 2014 06:41:07 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Balcan", "Maria Florina", ""], ["Feldman", "Vitaly", ""]]}, {"id": "1307.3176", "submitter": "L.A. Prashanth", "authors": "Nathaniel Korda, Prashanth L.A. and R\\'emi Munos", "title": "Fast gradient descent for drifting least squares regression, with\n  application to bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning algorithms require to often recompute least squares\nregression estimates of parameters. We study improving the computational\ncomplexity of such algorithms by using stochastic gradient descent (SGD) type\nschemes in place of classic regression solvers. We show that SGD schemes\nefficiently track the true solutions of the regression problems, even in the\npresence of a drift. This finding coupled with an $O(d)$ improvement in\ncomplexity, where $d$ is the dimension of the data, make them attractive for\nimplementation in the big data settings. In the case when strong convexity in\nthe regression problem is guaranteed, we provide bounds on the error both in\nexpectation and high probability (the latter is often needed to provide\ntheoretical guarantees for higher level algorithms), despite the drifting least\nsquares solution. As an example of this case we prove that the regret\nperformance of an SGD version of the PEGE linear bandit algorithm\n[Rusmevichientong and Tsitsiklis 2010] is worse that that of PEGE itself only\nby a factor of $O(\\log^4 n)$. When strong convexity of the regression problem\ncannot be guaranteed, we investigate using an adaptive regularisation. We make\nan empirical study of an adaptively regularised, SGD version of LinUCB [Li et\nal. 2010] in a news article recommendation application, which uses the large\nscale news recommendation dataset from Yahoo! front page. These experiments\nshow a large gain in computational complexity, with a consistently low tracking\nerror and click-through-rate (CTR) performance that is $75\\%$ close.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 16:36:29 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 00:27:18 GMT"}, {"version": "v3", "created": "Thu, 24 Jul 2014 14:29:52 GMT"}, {"version": "v4", "created": "Thu, 20 Nov 2014 12:40:48 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Korda", "Nathaniel", ""], ["A.", "Prashanth L.", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1307.3301", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman and Jan Vondrak", "title": "Optimal Bounds on Approximation of Submodular and XOS Functions by\n  Juntas", "comments": "Extended abstract appears in proceedings of FOCS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the approximability of several classes of real-valued\nfunctions by functions of a small number of variables ({\\em juntas}). Our main\nresults are tight bounds on the number of variables required to approximate a\nfunction $f:\\{0,1\\}^n \\rightarrow [0,1]$ within $\\ell_2$-error $\\epsilon$ over\nthe uniform distribution: 1. If $f$ is submodular, then it is $\\epsilon$-close\nto a function of $O(\\frac{1}{\\epsilon^2} \\log \\frac{1}{\\epsilon})$ variables.\nThis is an exponential improvement over previously known results. We note that\n$\\Omega(\\frac{1}{\\epsilon^2})$ variables are necessary even for linear\nfunctions. 2. If $f$ is fractionally subadditive (XOS) it is $\\epsilon$-close\nto a function of $2^{O(1/\\epsilon^2)}$ variables. This result holds for all\nfunctions with low total $\\ell_1$-influence and is a real-valued analogue of\nFriedgut's theorem for boolean functions. We show that $2^{\\Omega(1/\\epsilon)}$\nvariables are necessary even for XOS functions.\n  As applications of these results, we provide learning algorithms over the\nuniform distribution. For XOS functions, we give a PAC learning algorithm that\nruns in time $2^{poly(1/\\epsilon)} poly(n)$. For submodular functions we give\nan algorithm in the more demanding PMAC learning model (Balcan and Harvey,\n2011) which requires a multiplicative $1+\\gamma$ factor approximation with\nprobability at least $1-\\epsilon$ over the target distribution. Our uniform\ndistribution algorithm runs in time $2^{poly(1/(\\gamma\\epsilon))} poly(n)$.\nThis is the first algorithm in the PMAC model that over the uniform\ndistribution can achieve a constant approximation factor arbitrarily close to 1\nfor all submodular functions. As follows from the lower bounds in (Feldman et\nal., 2013) both of these algorithms are close to optimal. We also give\napplications for proper learning, testing and agnostic learning with value\nqueries of these classes.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 00:41:01 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2013 19:06:49 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2015 07:13:28 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Feldman", "Vitaly", ""], ["Vondrak", "Jan", ""]]}, {"id": "1307.3337", "submitter": "E.N.Sathishkumar", "authors": "T.Chandrasekhar, K.Thangavel, E.Elayaraja, E.N.Sathishkumar", "title": "Unsupervised Gene Expression Data using Enhanced Clustering Method", "comments": "5 pages, 1 figures, conference", "journal-ref": "International Conference on Emerging Trends in Computing,\n  Communication and Nanotechnology (ICE-CCN), 25-26 March 2013, Page(s): 518 -\n  522", "doi": "10.1109/ICE-CCN.2013.6528554", "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarrays are made it possible to simultaneously monitor the expression\nprofiles of thousands of genes under various experimental conditions.\nIdentification of co-expressed genes and coherent patterns is the central goal\nin microarray or gene expression data analysis and is an important task in\nbioinformatics research. Feature selection is a process to select features\nwhich are more informative. It is one of the important steps in knowledge\ndiscovery. The problem is that not all features are important. Some of the\nfeatures may be redundant, and others may be irrelevant and noisy. In this work\nthe unsupervised Gene selection method and Enhanced Center Initialization\nAlgorithm (ECIA) with K-Means algorithms have been applied for clustering of\nGene Expression Data. This proposed clustering algorithm overcomes the\ndrawbacks in terms of specifying the optimal number of clusters and\ninitialization of good cluster centroids. Gene Expression Data show that could\nidentify compact clusters with performs well in terms of the Silhouette\nCoefficients cluster measure.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 06:20:59 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Chandrasekhar", "T.", ""], ["Thangavel", "K.", ""], ["Elayaraja", "E.", ""], ["Sathishkumar", "E. N.", ""]]}, {"id": "1307.3457", "submitter": "Bubacarr Bah", "authors": "Bubacarr Bah, Ali Sadeghian and Volkan Cevher", "title": "Energy-aware adaptive bi-Lipschitz embeddings", "comments": "4 pages, 2 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dimensionality reducing matrix design based on training data\nwith constraints on its Frobenius norm and number of rows. Our design criteria\nis aimed at preserving the distances between the data points in the\ndimensionality reduced space as much as possible relative to their distances in\noriginal data space. This approach can be considered as a deterministic\nBi-Lipschitz embedding of the data points. We introduce a scalable learning\nalgorithm, dubbed AMUSE, and provide a rigorous estimation guarantee by\nleveraging game theoretic tools. We also provide a generalization\ncharacterization of our matrix based on our sample data. We use compressive\nsensing problems as an example application of our problem, where the Frobenius\nnorm design constraint translates into the sensing energy.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 13:49:14 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Bah", "Bubacarr", ""], ["Sadeghian", "Ali", ""], ["Cevher", "Volkan", ""]]}, {"id": "1307.3549", "submitter": "Chandrasekhar.T", "authors": "T.Chandrasekhar, K.Thangavel, E.Elayaraja", "title": "Performance Analysis of Clustering Algorithms for Gene Expression Data", "comments": "4 pages,4 figures. arXiv admin note: substantial text overlap with\n  arXiv:1112.4261, arXiv:1201.4914, arXiv:1307.3337", "journal-ref": "International Journal of Scientific & Engineering Research Volume\n  3, Issue 12, December-2012, page 1-4", "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarray technology is a process that allows thousands of genes\nsimultaneously monitor to various experimental conditions. It is used to\nidentify the co-expressed genes in specific cells or tissues that are actively\nused to make proteins, This method is used to analysis the gene expression, an\nimportant task in bioinformatics research. Cluster analysis of gene expression\ndata has proved to be a useful tool for identifying co-expressed genes,\nbiologically relevant groupings of genes and samples. In this paper we analysed\nK-Means with Automatic Generations of Merge Factor for ISODATA- AGMFI, to group\nthe microarray data sets on the basic of ISODATA. AGMFI is to generate initial\nvalues for merge and Spilt factor, maximum merge times instead of selecting\nefficient values as in ISODATA. The initial seeds for each cluster were\nnormally chosen either sequentially or randomly. The quality of the final\nclusters was found to be influenced by these initial seeds. For the real life\nproblems, the suitable number of clusters cannot be predicted. To overcome the\nabove drawback the current research focused on developing the clustering\nalgorithms without giving the initial number of clusters.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 06:43:27 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Chandrasekhar", "T.", ""], ["Thangavel", "K.", ""], ["Elayaraja", "E.", ""]]}, {"id": "1307.3617", "submitter": "Varun Kanade", "authors": "Varun Kanade, Elchanan Mossel", "title": "MCMC Learning", "comments": "28 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of learning under the uniform distribution is rich and deep, with\nconnections to cryptography, computational complexity, and the analysis of\nboolean functions to name a few areas. This theory however is very limited due\nto the fact that the uniform distribution and the corresponding Fourier basis\nare rarely encountered as a statistical model.\n  A family of distributions that vastly generalizes the uniform distribution on\nthe Boolean cube is that of distributions represented by Markov Random Fields\n(MRF). Markov Random Fields are one of the main tools for modeling high\ndimensional data in many areas of statistics and machine learning.\n  In this paper we initiate the investigation of extending central ideas,\nmethods and algorithms from the theory of learning under the uniform\ndistribution to the setup of learning concepts given examples from MRF\ndistributions. In particular, our results establish a novel connection between\nproperties of MCMC sampling of MRFs and learning under the MRF distribution.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2013 07:00:00 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 08:11:27 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Kanade", "Varun", ""], ["Mossel", "Elchanan", ""]]}, {"id": "1307.3673", "submitter": "Omar Alonso", "authors": "Alexandros Ntoulas, Omar Alonso, Vasilis Kandylas", "title": "A Data Management Approach for Dataset Selection Using Human Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the number of applications that use machine learning algorithms increases,\nthe need for labeled data useful for training such algorithms intensifies.\n  Getting labels typically involves employing humans to do the annotation,\nwhich directly translates to training and working costs. Crowdsourcing\nplatforms have made labeling cheaper and faster, but they still involve\nsignificant costs, especially for the cases where the potential set of\ncandidate data to be labeled is large. In this paper we describe a methodology\nand a prototype system aiming at addressing this challenge for Web-scale\nproblems in an industrial setting. We discuss ideas on how to efficiently\nselect the data to use for training of machine learning algorithms in an\nattempt to reduce cost. We show results achieving good performance with reduced\ncost by carefully selecting which instances to label. Our proposed algorithm is\npresented as part of a framework for managing and generating training datasets,\nwhich includes, among other components, a human computation element.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2013 19:29:33 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Ntoulas", "Alexandros", ""], ["Alonso", "Omar", ""], ["Kandylas", "Vasilis", ""]]}, {"id": "1307.3675", "submitter": "Chris Dyer", "authors": "Chris Dyer", "title": "Minimum Error Rate Training and the Convex Hull Semiring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the line search used in the minimum error rate training algorithm\nMERT as the \"inside score\" of a weighted proof forest under a semiring defined\nin terms of well-understood operations from computational geometry. This\nconception leads to a straightforward complexity analysis of the dynamic\nprogramming MERT algorithms of Macherey et al. (2008) and Kumar et al. (2009)\nand practical approaches to implementation.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2013 19:38:09 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Dyer", "Chris", ""]]}, {"id": "1307.3687", "submitter": "Junzhou Zhao", "authors": "Junzhou Zhao", "title": "On Analyzing Estimation Errors due to Constrained Connections in Online\n  Review Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained connection is the phenomenon that a reviewer can only review a\nsubset of products/services due to narrow range of interests or limited\nattention capacity. In this work, we study how constrained connections can\naffect estimation performance in online review systems (ORS). We find that\nreviewers' constrained connections will cause poor estimation performance, both\nfrom the measurements of estimation accuracy and Bayesian Cramer Rao lower\nbound.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2013 01:37:48 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Zhao", "Junzhou", ""]]}, {"id": "1307.3785", "submitter": "Christos Dimitrakakis", "authors": "Aristide C. Y. Tossou and Christos Dimitrakakis", "title": "Probabilistic inverse reinforcement learning in unknown environments", "comments": "UAI 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning by demonstration from agents acting in\nunknown stochastic Markov environments or games. Our aim is to estimate agent\npreferences in order to construct improved policies for the same task that the\nagents are trying to solve. To do so, we extend previous probabilistic\napproaches for inverse reinforcement learning in known MDPs to the case of\nunknown dynamics or opponents. We do this by deriving two simplified\nprobabilistic models of the demonstrator's policy and utility. For\ntractability, we use maximum a posteriori estimation rather than full Bayesian\ninference. Under a flat prior, this results in a convex optimisation problem.\nWe find that the resulting algorithms are highly competitive against a variety\nof other methods for inverse reinforcement learning that do have knowledge of\nthe dynamics.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2013 22:06:12 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Tossou", "Aristide C. Y.", ""], ["Dimitrakakis", "Christos", ""]]}, {"id": "1307.3824", "submitter": "Keki Burjorjee", "authors": "Keki M. Burjorjee", "title": "The Fundamental Learning Problem that Genetic Algorithms with Uniform\n  Crossover Solve Efficiently and Repeatedly As Evolution Proceeds", "comments": "For an easy introduction to implicit concurrency (with animations),\n  visit\n  http://blog.hackingevolution.net/2013/03/24/implicit-concurrency-in-genetic-algorithms/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CC cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes theoretical bonafides for implicit concurrent\nmultivariate effect evaluation--implicit concurrency for short---a broad and\nversatile computational learning efficiency thought to underlie\ngeneral-purpose, non-local, noise-tolerant optimization in genetic algorithms\nwith uniform crossover (UGAs). We demonstrate that implicit concurrency is\nindeed a form of efficient learning by showing that it can be used to obtain\nclose-to-optimal bounds on the time and queries required to approximately\ncorrectly solve a constrained version (k=7, \\eta=1/5) of a recognizable\ncomputational learning problem: learning parities with noisy membership\nqueries. We argue that a UGA that treats the noisy membership query oracle as a\nfitness function can be straightforwardly used to approximately correctly learn\nthe essential attributes in O(log^1.585 n) queries and O(n log^1.585 n) time,\nwhere n is the total number of attributes. Our proof relies on an accessible\nsymmetry argument and the use of statistical hypothesis testing to reject a\nglobal null hypothesis at the 10^-100 level of significance. It is, to the best\nof our knowledge, the first relatively rigorous identification of efficient\ncomputational learning in an evolutionary algorithm on a non-trivial learning\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 06:32:52 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Burjorjee", "Keki M.", ""]]}, {"id": "1307.3846", "submitter": "S\\'ebastien Brati\\`eres", "authors": "Sebastien Bratieres, Novi Quadrianto, Zoubin Ghahramani", "title": "Bayesian Structured Prediction Using Gaussian Processes", "comments": "8 pages with figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a conceptually novel structured prediction model, GPstruct,\nwhich is kernelized, non-parametric and Bayesian, by design. We motivate the\nmodel with respect to existing approaches, among others, conditional random\nfields (CRFs), maximum margin Markov networks (M3N), and structured support\nvector machines (SVMstruct), which embody only a subset of its properties. We\npresent an inference procedure based on Markov Chain Monte Carlo. The framework\ncan be instantiated for a wide range of structured objects such as linear\nchains, trees, grids, and other general graphs. As a proof of concept, the\nmodel is benchmarked on several natural language processing tasks and a video\ngesture segmentation task involving a linear chain structure. We show\nprediction accuracies for GPstruct which are comparable to or exceeding those\nof CRFs and SVMstruct.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 07:57:56 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Bratieres", "Sebastien", ""], ["Quadrianto", "Novi", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1307.3949", "submitter": "Steffen Borgwardt", "authors": "Steffen Borgwardt", "title": "On Soft Power Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in data analysis begin with a set of points in a Euclidean\nspace that is partitioned into clusters. Common tasks then are to devise a\nclassifier deciding which of the clusters a new point is associated to, finding\noutliers with respect to the clusters, or identifying the type of clustering\nused for the partition.\n  One of the common kinds of clusterings are (balanced) least-squares\nassignments with respect to a given set of sites. For these, there is a\n'separating power diagram' for which each cluster lies in its own cell.\n  In the present paper, we aim for efficient algorithms for outlier detection\nand the computation of thresholds that measure how similar a clustering is to a\nleast-squares assignment for fixed sites. For this purpose, we devise a new\nmodel for the computation of a 'soft power diagram', which allows a soft\nseparation of the clusters with 'point counting properties'; e.g. we are able\nto prescribe how many points we want to classify as outliers.\n  As our results hold for a more general non-convex model of free sites, we\ndescribe it and our proofs in this more general way. Its locally optimal\nsolutions satisfy the aforementioned point counting properties. For our target\napplications that use fixed sites, our algorithms are efficiently solvable to\nglobal optimality by linear programming.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 14:04:39 GMT"}, {"version": "v2", "created": "Tue, 24 Jun 2014 14:21:00 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Borgwardt", "Steffen", ""]]}, {"id": "1307.3964", "submitter": "Alejandro Edera", "authors": "Alejandro Edera, Federico Schl\\\"uter, Facundo Bromberg", "title": "Learning Markov networks with context-specific independences", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the Markov network structure from data is a problem that has\nreceived considerable attention in machine learning, and in many other\napplication fields. This work focuses on a particular approach for this purpose\ncalled independence-based learning. Such approach guarantees the learning of\nthe correct structure efficiently, whenever data is sufficient for representing\nthe underlying distribution. However, an important issue of such approach is\nthat the learned structures are encoded in an undirected graph. The problem\nwith graphs is that they cannot encode some types of independence relations,\nsuch as the context-specific independences. They are a particular case of\nconditional independences that is true only for a certain assignment of its\nconditioning set, in contrast to conditional independences that must hold for\nall its assignments. In this work we present CSPC, an independence-based\nalgorithm for learning structures that encode context-specific independences,\nand encoding them in a log-linear model, instead of a graph. The central idea\nof CSPC is combining the theoretical guarantees provided by the\nindependence-based approach with the benefits of representing complex\nstructures by using features in a log-linear model. We present experiments in a\nsynthetic case, showing that CSPC is more accurate than the state-of-the-art IB\nalgorithms when the underlying distribution contains CSIs.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 14:31:44 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Edera", "Alejandro", ""], ["Schl\u00fcter", "Federico", ""], ["Bromberg", "Facundo", ""]]}, {"id": "1307.4048", "submitter": "Pavan Kumar D S", "authors": "D. S. Pavan Kumar, N. Vishnu Prasad, Vikas Joshi, S. Umesh", "title": "Modified SPLICE and its Extension to Non-Stereo Data for Noise Robust\n  Speech Recognition", "comments": "Submitted to Automatic Speech Recognition and Understanding (ASRU)\n  2013 Workshop", "journal-ref": null, "doi": "10.1109/ASRU.2013.6707725", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a modification to the training process of the popular SPLICE\nalgorithm has been proposed for noise robust speech recognition. The\nmodification is based on feature correlations, and enables this stereo-based\nalgorithm to improve the performance in all noise conditions, especially in\nunseen cases. Further, the modified framework is extended to work for\nnon-stereo datasets where clean and noisy training utterances, but not stereo\ncounterparts, are required. Finally, an MLLR-based computationally efficient\nrun-time noise adaptation method in SPLICE framework has been proposed. The\nmodified SPLICE shows 8.6% absolute improvement over SPLICE in Test C of\nAurora-2 database, and 2.93% overall. Non-stereo method shows 10.37% and 6.93%\nabsolute improvements over Aurora-2 and Aurora-4 baseline models respectively.\nRun-time adaptation shows 9.89% absolute improvement in modified framework as\ncompared to SPLICE for Test C, and 4.96% overall w.r.t. standard MLLR\nadaptation on HMMs.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 18:39:10 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Kumar", "D. S. Pavan", ""], ["Prasad", "N. Vishnu", ""], ["Joshi", "Vikas", ""], ["Umesh", "S.", ""]]}, {"id": "1307.4145", "submitter": "Jie  Wang", "authors": "Jie Wang, Jiayu Zhou, Jun Liu, Peter Wonka, Jieping Ye", "title": "A Safe Screening Rule for Sparse Logistic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The l1-regularized logistic regression (or sparse logistic regression) is a\nwidely used method for simultaneous classification and feature selection.\nAlthough many recent efforts have been devoted to its efficient implementation,\nits application to high dimensional data still poses significant challenges. In\nthis paper, we present a fast and effective sparse logistic regression\nscreening rule (Slores) to identify the 0 components in the solution vector,\nwhich may lead to a substantial reduction in the number of features to be\nentered to the optimization. An appealing feature of Slores is that the data\nset needs to be scanned only once to run the screening and its computational\ncost is negligible compared to that of solving the sparse logistic regression\nproblem. Moreover, Slores is independent of solvers for sparse logistic\nregression, thus Slores can be integrated with any existing solver to improve\nthe efficiency. We have evaluated Slores using high-dimensional data sets from\ndifferent applications. Extensive experimental results demonstrate that Slores\noutperforms the existing state-of-the-art screening rules and the efficiency of\nsolving sparse logistic regression is improved by one magnitude in general.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2013 02:03:51 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2013 22:57:16 GMT"}], "update_date": "2013-07-22", "authors_parsed": [["Wang", "Jie", ""], ["Zhou", "Jiayu", ""], ["Liu", "Jun", ""], ["Wonka", "Peter", ""], ["Ye", "Jieping", ""]]}, {"id": "1307.4156", "submitter": "Jie  Wang", "authors": "Jie Wang, Jun Liu, Jieping Ye", "title": "Efficient Mixed-Norm Regularization: Algorithms and Safe Screening\n  Methods", "comments": "arXiv admin note: text overlap with arXiv:1009.4766", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse learning has recently received increasing attention in many areas\nincluding machine learning, statistics, and applied mathematics. The mixed-norm\nregularization based on the l1q norm with q>1 is attractive in many\napplications of regression and classification in that it facilitates group\nsparsity in the model. The resulting optimization problem is, however,\nchallenging to solve due to the inherent structure of the mixed-norm\nregularization. Existing work deals with special cases with q=1, 2, infinity,\nand they cannot be easily extended to the general case. In this paper, we\npropose an efficient algorithm based on the accelerated gradient method for\nsolving the general l1q-regularized problem. One key building block of the\nproposed algorithm is the l1q-regularized Euclidean projection (EP_1q). Our\ntheoretical analysis reveals the key properties of EP_1q and illustrates why\nEP_1q for the general q is significantly more challenging to solve than the\nspecial cases. Based on our theoretical analysis, we develop an efficient\nalgorithm for EP_1q by solving two zero finding problems. To further improve\nthe efficiency of solving large dimensional mixed-norm regularized problems, we\npropose a screening method which is able to quickly identify the inactive\ngroups, i.e., groups that have 0 components in the solution. This may lead to\nsubstantial reduction in the number of groups to be entered to the\noptimization. An appealing feature of our screening method is that the data set\nneeds to be scanned only once to run the screening. Compared to that of solving\nthe mixed-norm regularized problems, the computational cost of our screening\ntest is negligible. The key of the proposed screening method is an accurate\nsensitivity analysis of the dual optimal solution when the regularization\nparameter varies. Experimental results demonstrate the efficiency of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2013 03:09:13 GMT"}], "update_date": "2013-07-17", "authors_parsed": [["Wang", "Jie", ""], ["Liu", "Jun", ""], ["Ye", "Jieping", ""]]}, {"id": "1307.4514", "submitter": "Aur\\'elien Bellet", "authors": "Aur\\'elien Bellet", "title": "Supervised Metric Learning with Generalization Guarantees", "comments": "PhD thesis defended on December 11, 2012 (Laboratoire Hubert Curien,\n  University of Saint-Etienne)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The crucial importance of metrics in machine learning algorithms has led to\nan increasing interest in optimizing distance and similarity functions, an area\nof research known as metric learning. When data consist of feature vectors, a\nlarge body of work has focused on learning a Mahalanobis distance. Less work\nhas been devoted to metric learning from structured objects (such as strings or\ntrees), most of it focusing on optimizing a notion of edit distance. We\nidentify two important limitations of current metric learning approaches.\nFirst, they allow to improve the performance of local algorithms such as\nk-nearest neighbors, but metric learning for global algorithms (such as linear\nclassifiers) has not been studied so far. Second, the question of the\ngeneralization ability of metric learning methods has been largely ignored. In\nthis thesis, we propose theoretical and algorithmic contributions that address\nthese limitations. Our first contribution is the derivation of a new kernel\nfunction built from learned edit probabilities. Our second contribution is a\nnovel framework for learning string and tree edit similarities inspired by the\nrecent theory of (e,g,t)-good similarity functions. Using uniform stability\narguments, we establish theoretical guarantees for the learned similarity that\ngive a bound on the generalization error of a linear classifier built from that\nsimilarity. In our third contribution, we extend these ideas to metric learning\nfrom feature vectors by proposing a bilinear similarity learning method that\nefficiently optimizes the (e,g,t)-goodness. Generalization guarantees are\nderived for our approach, highlighting that our method minimizes a tighter\nbound on the generalization error of the classifier. Our last contribution is a\nframework for establishing generalization bounds for a large class of existing\nmetric learning algorithms based on a notion of algorithmic robustness.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 06:42:00 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2013 17:42:26 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Bellet", "Aur\u00e9lien", ""]]}, {"id": "1307.4564", "submitter": "Nicol\\`o Cesa-Bianchi", "authors": "Noga Alon, Nicol\\`o Cesa-Bianchi, Claudio Gentile, Yishay Mansour", "title": "From Bandits to Experts: A Tale of Domination and Independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the partial observability model for multi-armed bandits,\nintroduced by Mannor and Shamir. Our main result is a characterization of\nregret in the directed observability model in terms of the dominating and\nindependence numbers of the observability graph. We also show that in the\nundirected case, the learner can achieve optimal regret without even accessing\nthe observability graph before selecting an action. Both results are shown\nusing variants of the Exp3 algorithm operating on the observability graph in a\ntime-efficient manner.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 10:24:00 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Alon", "Noga", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""], ["Gentile", "Claudio", ""], ["Mansour", "Yishay", ""]]}, {"id": "1307.4653", "submitter": "Massimiliano Pontil", "authors": "Bernardino Romera-Paredes and Massimiliano Pontil", "title": "A New Convex Relaxation for Tensor Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning a tensor from a set of linear measurements.\nA prominent methodology for this problem is based on a generalization of trace\nnorm regularization, which has been used extensively for learning low rank\nmatrices, to the tensor setting. In this paper, we highlight some limitations\nof this approach and propose an alternative convex relaxation on the Euclidean\nball. We then describe a technique to solve the associated regularization\nproblem, which builds upon the alternating direction method of multipliers.\nExperiments on one synthetic dataset and two real datasets indicate that the\nproposed method improves significantly over tensor trace norm regularization in\nterms of estimation error, while remaining computationally tractable.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 14:38:47 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Romera-Paredes", "Bernardino", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1307.4847", "submitter": "Zheng Wen", "authors": "Zheng Wen and Benjamin Van Roy", "title": "Efficient Reinforcement Learning in Deterministic Systems with Value\n  Function Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reinforcement learning over episodes of a\nfinite-horizon deterministic system and as a solution propose optimistic\nconstraint propagation (OCP), an algorithm designed to synthesize efficient\nexploration and value function generalization. We establish that when the true\nvalue function lies within a given hypothesis class, OCP selects optimal\nactions over all but at most K episodes, where K is the eluder dimension of the\ngiven hypothesis class. We establish further efficiency and asymptotic\nperformance guarantees that apply even if the true value function does not lie\nin the given hypothesis class, for the special case where the hypothesis class\nis the span of pre-specified indicator functions over disjoint sets. We also\ndiscuss the computational complexity of OCP and present computational results\ninvolving two illustrative examples.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2013 07:22:39 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2013 23:05:18 GMT"}, {"version": "v3", "created": "Sun, 8 May 2016 20:10:21 GMT"}, {"version": "v4", "created": "Wed, 6 Jul 2016 23:56:50 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Wen", "Zheng", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1307.4891", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel and Helmut B\\\"olcskei", "title": "Robust Subspace Clustering via Thresholding", "comments": "final version, to appear in the IEEE Transactions on Information\n  Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of clustering noisy and incompletely observed high-dimensional\ndata points into a union of low-dimensional subspaces and a set of outliers is\nconsidered. The number of subspaces, their dimensions, and their orientations\nare assumed unknown. We propose a simple low-complexity subspace clustering\nalgorithm, which applies spectral clustering to an adjacency matrix obtained by\nthresholding the correlations between data points. In other words, the\nadjacency matrix is constructed from the nearest neighbors of each data point\nin spherical distance. A statistical performance analysis shows that the\nalgorithm exhibits robustness to additive noise and succeeds even when the\nsubspaces intersect. Specifically, our results reveal an explicit tradeoff\nbetween the affinity of the subspaces and the tolerable noise level. We\nfurthermore prove that the algorithm succeeds even when the data points are\nincompletely observed with the number of missing entries allowed to be (up to a\nlog-factor) linear in the ambient dimension. We also propose a simple scheme\nthat provably detects outliers, and we present numerical results on real and\nsynthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2013 10:08:47 GMT"}, {"version": "v2", "created": "Wed, 9 Apr 2014 13:23:04 GMT"}, {"version": "v3", "created": "Wed, 25 Jun 2014 13:18:37 GMT"}, {"version": "v4", "created": "Fri, 21 Aug 2015 13:53:51 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Heckel", "Reinhard", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1307.5101", "submitter": "Hsiang-Fu Yu", "authors": "Hsiang-Fu Yu and Prateek Jain and Purushottam Kar and Inderjit S.\n  Dhillon", "title": "Large-scale Multi-label Learning with Missing Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-label classification problem has generated significant interest in\nrecent years. However, existing approaches do not adequately address two key\nchallenges: (a) the ability to tackle problems with a large number (say\nmillions) of labels, and (b) the ability to handle data with missing labels. In\nthis paper, we directly address both these problems by studying the multi-label\nproblem in a generic empirical risk minimization (ERM) framework. Our\nframework, despite being simple, is surprisingly able to encompass several\nrecent label-compression based methods which can be derived as special cases of\nour method. To optimize the ERM problem, we develop techniques that exploit the\nstructure of specific loss functions - such as the squared loss function - to\noffer efficient algorithms. We further show that our learning framework admits\nformal excess risk bounds even in the presence of missing labels. Our risk\nbounds are tight and demonstrate better generalization performance for low-rank\npromoting trace-norm regularization when compared to (rank insensitive)\nFrobenius norm regularization. Finally, we present extensive empirical results\non a variety of benchmark datasets and show that our methods perform\nsignificantly better than existing label compression based methods and can\nscale up to very large datasets such as the Wikipedia dataset.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2013 23:55:55 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2013 22:33:17 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2013 16:57:43 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Yu", "Hsiang-Fu", ""], ["Jain", "Prateek", ""], ["Kar", "Purushottam", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1307.5118", "submitter": "Tingting Zhao Tingting Zhao", "authors": "Syogo Mori, Voot Tangkaratt, Tingting Zhao, Jun Morimoto, and Masashi\n  Sugiyama", "title": "Model-Based Policy Gradients with Parameter-Based Exploration by\n  Least-Squares Conditional Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of reinforcement learning (RL) is to let an agent learn an optimal\ncontrol policy in an unknown environment so that future expected rewards are\nmaximized. The model-free RL approach directly learns the policy based on data\nsamples. Although using many samples tends to improve the accuracy of policy\nlearning, collecting a large number of samples is often expensive in practice.\nOn the other hand, the model-based RL approach first estimates the transition\nmodel of the environment and then learns the policy based on the estimated\ntransition model. Thus, if the transition model is accurately learned from a\nsmall amount of data, the model-based approach can perform better than the\nmodel-free approach. In this paper, we propose a novel model-based RL method by\ncombining a recently proposed model-free policy search method called policy\ngradients with parameter-based exploration and the state-of-the-art transition\nmodel estimator called least-squares conditional density estimation. Through\nexperiments, we demonstrate the practical usefulness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 03:00:39 GMT"}], "update_date": "2013-07-22", "authors_parsed": [["Mori", "Syogo", ""], ["Tangkaratt", "Voot", ""], ["Zhao", "Tingting", ""], ["Morimoto", "Jun", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1307.5161", "submitter": "Xavier Boix", "authors": "Gemma Roig, Xavier Boix, Luc Van Gool", "title": "Random Binary Mappings for Kernel Learning and Efficient SVM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Machines (SVMs) are powerful learners that have led to\nstate-of-the-art results in various computer vision problems. SVMs suffer from\nvarious drawbacks in terms of selecting the right kernel, which depends on the\nimage descriptors, as well as computational and memory efficiency. This paper\nintroduces a novel kernel, which serves such issues well. The kernel is learned\nby exploiting a large amount of low-complex, randomized binary mappings of the\ninput feature. This leads to an efficient SVM, while also alleviating the task\nof kernel selection. We demonstrate the capabilities of our kernel on 6\nstandard vision benchmarks, in which we combine several common image\ndescriptors, namely histograms (Flowers17 and Daimler), attribute-like\ndescriptors (UCI, OSR, and a-VOC08), and Sparse Quantization (ImageNet).\nResults show that our kernel learning adapts well to the different descriptors\ntypes, achieving the performance of the kernels specifically tuned for each\nimage descriptor, and with similar evaluation cost as efficient SVM methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 08:47:32 GMT"}, {"version": "v2", "created": "Fri, 28 Mar 2014 08:49:17 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Roig", "Gemma", ""], ["Boix", "Xavier", ""], ["Van Gool", "Luc", ""]]}, {"id": "1307.5302", "submitter": "Dino Sejdinovic", "authors": "Dino Sejdinovic, Heiko Strathmann, Maria Lomeli Garcia, Christophe\n  Andrieu, Arthur Gretton", "title": "Kernel Adaptive Metropolis-Hastings", "comments": "Proceedings of the 31st International Conference on Machine Learning,\n  Beijing, China, 2014; JMLR: W&CP volume 32(2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Kernel Adaptive Metropolis-Hastings algorithm is introduced, for the\npurpose of sampling from a target distribution with strongly nonlinear support.\nThe algorithm embeds the trajectory of the Markov chain into a reproducing\nkernel Hilbert space (RKHS), such that the feature space covariance of the\nsamples informs the choice of proposal. The procedure is computationally\nefficient and straightforward to implement, since the RKHS moves can be\nintegrated out analytically: our proposal distribution in the original space is\na normal distribution whose mean and covariance depend on where the current\nsample lies in the support of the target distribution, and adapts to its local\ncovariance structure. Furthermore, the procedure requires neither gradients nor\nany other higher order information about the target, making it particularly\nattractive for contexts such as Pseudo-Marginal MCMC. Kernel Adaptive\nMetropolis-Hastings outperforms competing fixed and adaptive samplers on\nmultivariate, highly nonlinear target distributions, arising in both real-world\nand synthetic examples. Code may be downloaded at\nhttps://github.com/karlnapf/kameleon-mcmc.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 18:26:34 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2014 18:06:06 GMT"}, {"version": "v3", "created": "Thu, 12 Jun 2014 22:30:05 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Sejdinovic", "Dino", ""], ["Strathmann", "Heiko", ""], ["Garcia", "Maria Lomeli", ""], ["Andrieu", "Christophe", ""], ["Gretton", "Arthur", ""]]}, {"id": "1307.5438", "submitter": "Yaqin Zhou", "authors": "Xiang-yang Li, Shaojie Tang and Yaqin Zhou", "title": "Towards Distribution-Free Multi-Armed Bandits with Combinatorial\n  Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a generalized version of classical multi-armed bandits\n(MABs) problem by allowing for arbitrary constraints on constituent bandits at\neach decision point. The motivation of this study comes from many situations\nthat involve repeatedly making choices subject to arbitrary constraints in an\nuncertain environment: for instance, regularly deciding which advertisements to\ndisplay online in order to gain high click-through-rate without knowing user\npreferences, or what route to drive home each day under uncertain weather and\ntraffic conditions. Assume that there are $K$ unknown random variables (RVs),\ni.e., arms, each evolving as an \\emph{i.i.d} stochastic process over time. At\neach decision epoch, we select a strategy, i.e., a subset of RVs, subject to\narbitrary constraints on constituent RVs.\n  We then gain a reward that is a linear combination of observations on\nselected RVs.\n  The performance of prior results for this problem heavily depends on the\ndistribution of strategies generated by corresponding learning policy. For\nexample, if the reward-difference between the best and second best strategy\napproaches zero, prior result may lead to arbitrarily large regret.\n  Meanwhile, when there are exponential number of possible strategies at each\ndecision point, naive extension of a prior distribution-free policy would cause\npoor performance in terms of regret, computation and space complexity.\n  To this end, we propose an efficient Distribution-Free Learning (DFL) policy\nthat achieves zero regret, regardless of the probability distribution of the\nresultant strategies.\n  Our learning policy has both $O(K)$ time complexity and $O(K)$ space\ncomplexity. In successive generations, we show that even if finding the optimal\nstrategy at each decision point is NP-hard, our policy still allows for\napproximated solutions while retaining near zero-regret.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2013 16:40:46 GMT"}, {"version": "v2", "created": "Sun, 11 May 2014 03:45:24 GMT"}, {"version": "v3", "created": "Sun, 5 Oct 2014 04:20:27 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Li", "Xiang-yang", ""], ["Tang", "Shaojie", ""], ["Zhou", "Yaqin", ""]]}, {"id": "1307.5449", "submitter": "Yonatan Gur", "authors": "O. Besbes, Y. Gur, and A. Zeevi", "title": "Non-stationary Stochastic Optimization", "comments": null, "journal-ref": null, "doi": "10.1287/opre.2015.1408", "report-no": null, "categories": "math.PR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a non-stationary variant of a sequential stochastic optimization\nproblem, in which the underlying cost functions may change along the horizon.\nWe propose a measure, termed variation budget, that controls the extent of said\nchange, and study how restrictions on this budget impact achievable\nperformance. We identify sharp conditions under which it is possible to achieve\nlong-run-average optimality and more refined performance measures such as rate\noptimality that fully characterize the complexity of such problems. In doing\nso, we also establish a strong connection between two rather disparate strands\nof literature: adversarial online convex optimization; and the more traditional\nstochastic approximation paradigm (couched in a non-stationary setting). This\nconnection is the key to deriving well performing policies in the latter, by\nleveraging structure of optimal policies in the former. Finally, tight bounds\non the minimax regret allow us to quantify the \"price of non-stationarity,\"\nwhich mathematically captures the added complexity embedded in a temporally\nchanging environment versus a stationary one.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2013 18:46:01 GMT"}, {"version": "v2", "created": "Mon, 22 Dec 2014 22:45:18 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Besbes", "O.", ""], ["Gur", "Y.", ""], ["Zeevi", "A.", ""]]}, {"id": "1307.5494", "submitter": "Laura Balzano", "authors": "Laura Balzano and Stephen J. Wright", "title": "On GROUSE and Incremental SVD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GROUSE (Grassmannian Rank-One Update Subspace Estimation) is an incremental\nalgorithm for identifying a subspace of Rn from a sequence of vectors in this\nsubspace, where only a subset of components of each vector is revealed at each\niteration. Recent analysis has shown that GROUSE converges locally at an\nexpected linear rate, under certain assumptions. GROUSE has a similar flavor to\nthe incremental singular value decomposition algorithm, which updates the SVD\nof a matrix following addition of a single column. In this paper, we modify the\nincremental SVD approach to handle missing data, and demonstrate that this\nmodified approach is equivalent to GROUSE, for a certain choice of an\nalgorithmic parameter.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2013 03:47:16 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Balzano", "Laura", ""], ["Wright", "Stephen J.", ""]]}, {"id": "1307.5497", "submitter": "Chunhua Shen", "authors": "Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel", "title": "A scalable stage-wise approach to large-margin multi-class loss based\n  boosting", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable and effective classification model to train multi-class\nboosting for multi-class classification problems. Shen and Hao introduced a\ndirect formulation of multi- class boosting in the sense that it directly\nmaximizes the multi- class margin [C. Shen and Z. Hao, \"A direct formulation\nfor totally-corrective multi- class boosting\", in Proc. IEEE Conf. Comp. Vis.\nPatt. Recogn., 2011]. The major problem of their approach is its high\ncomputational complexity for training, which hampers its application on\nreal-world problems. In this work, we propose a scalable and simple stage-wise\nmulti-class boosting method, which also directly maximizes the multi-class\nmargin. Our approach of- fers a few advantages: 1) it is simple and\ncomputationally efficient to train. The approach can speed up the training time\nby more than two orders of magnitude without sacrificing the classification\naccuracy. 2) Like traditional AdaBoost, it is less sensitive to the choice of\nparameters and empirically demonstrates excellent generalization performance.\nExperimental results on challenging multi-class machine learning and vision\ntasks demonstrate that the proposed approach substantially improves the\nconvergence rate and accuracy of the final visual detector at no additional\ncomputational cost compared to existing multi-class boosting.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2013 06:06:13 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Paisitkriangkrai", "Sakrapee", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1307.5599", "submitter": "Naresh Kumar Mallenahalli Prof. Dr.", "authors": "M. Naresh Kumar", "title": "Performance comparison of State-of-the-art Missing Value Imputation\n  Algorithms on Some Bench mark Datasets", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision making from data involves identifying a set of attributes that\ncontribute to effective decision making through computational intelligence. The\npresence of missing values greatly influences the selection of right set of\nattributes and this renders degradation in classification accuracies of the\nclassifiers. As missing values are quite common in data collection phase during\nfield experiments or clinical trails appropriate handling would improve the\nclassifier performance. In this paper we present a review of recently developed\nmissing value imputation algorithms and compare their performance on some bench\nmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 06:50:21 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Kumar", "M. Naresh", ""]]}, {"id": "1307.5697", "submitter": "Aziz Erkal Selman", "authors": "Martin Grohe, Kristian Kersting, Martin Mladenov, Erkal Selman", "title": "Dimension Reduction via Colour Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colour refinement is a basic algorithmic routine for graph isomorphism\ntesting, appearing as a subroutine in almost all practical isomorphism solvers.\nIt partitions the vertices of a graph into \"colour classes\" in such a way that\nall vertices in the same colour class have the same number of neighbours in\nevery colour class. Tinhofer (Disc. App. Math., 1991), Ramana, Scheinerman, and\nUllman (Disc. Math., 1994) and Godsil (Lin. Alg. and its App., 1997)\nestablished a tight correspondence between colour refinement and fractional\nisomorphisms of graphs, which are solutions to the LP relaxation of a natural\nILP formulation of graph isomorphism.\n  We introduce a version of colour refinement for matrices and extend existing\nquasilinear algorithms for computing the colour classes. Then we generalise the\ncorrespondence between colour refinement and fractional automorphisms and\ndevelop a theory of fractional automorphisms and isomorphisms of matrices.\n  We apply our results to reduce the dimensions of systems of linear equations\nand linear programs. Specifically, we show that any given LP L can efficiently\nbe transformed into a (potentially) smaller LP L' whose number of variables and\nconstraints is the number of colour classes of the colour refinement algorithm,\napplied to a matrix associated with the LP. The transformation is such that we\ncan easily (by a linear mapping) map both feasible and optimal solutions back\nand forth between the two LPs. We demonstrate empirically that colour\nrefinement can indeed greatly reduce the cost of solving linear programs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 13:34:44 GMT"}, {"version": "v2", "created": "Wed, 30 Apr 2014 13:28:47 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Grohe", "Martin", ""], ["Kersting", "Kristian", ""], ["Mladenov", "Martin", ""], ["Selman", "Erkal", ""]]}, {"id": "1307.5730", "submitter": "Baogang Hu", "authors": "Xiaowan Zhang and Bao-Gang Hu", "title": "A New Strategy of Cost-Free Learning in the Class Imbalance Problem", "comments": "Classification, class imbalance, cost-free learning, cost-sensitive\n  learning, abstaining, mutual information, ROC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we define cost-free learning (CFL) formally in comparison with\ncost-sensitive learning (CSL). The main difference between them is that a CFL\napproach seeks optimal classification results without requiring any cost\ninformation, even in the class imbalance problem. In fact, several CFL\napproaches exist in the related studies, such as sampling and some\ncriteria-based pproaches. However, to our best knowledge, none of the existing\nCFL and CSL approaches are able to process the abstaining classifications\nproperly when no information is given about errors and rejects. Based on\ninformation theory, we propose a novel CFL which seeks to maximize normalized\nmutual information of the targets and the decision outputs of classifiers.\nUsing the strategy, we can deal with binary/multi-class classifications\nwith/without abstaining. Significant features are observed from the new\nstrategy. While the degree of class imbalance is changing, the proposed\nstrategy is able to balance the errors and rejects accordingly and\nautomatically. Another advantage of the strategy is its ability of deriving\noptimal rejection thresholds for abstaining classifications and the\n\"equivalent\" costs in binary classifications. The connection between rejection\nthresholds and ROC curve is explored. Empirical investigation is made on\nseveral benchmark data sets in comparison with other existing approaches. The\nclassification results demonstrate a promising perspective of the strategy in\nmachine learning.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 14:36:03 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Zhang", "Xiaowan", ""], ["Hu", "Bao-Gang", ""]]}, {"id": "1307.5870", "submitter": "Cun Mu", "authors": "Cun Mu, Bo Huang, John Wright, Donald Goldfarb", "title": "Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery", "comments": "Slight modifications are made in this second version (mainly, Lemma\n  5)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering a low-rank tensor from incomplete information is a recurring\nproblem in signal processing and machine learning. The most popular convex\nrelaxation of this problem minimizes the sum of the nuclear norms of the\nunfoldings of the tensor. We show that this approach can be substantially\nsuboptimal: reliably recovering a $K$-way tensor of length $n$ and Tucker rank\n$r$ from Gaussian measurements requires $\\Omega(r n^{K-1})$ observations. In\ncontrast, a certain (intractable) nonconvex formulation needs only $O(r^K +\nnrK)$ observations. We introduce a very simple, new convex relaxation, which\npartially bridges this gap. Our new formulation succeeds with $O(r^{\\lfloor K/2\n\\rfloor}n^{\\lceil K/2 \\rceil})$ observations. While these results pertain to\nGaussian measurements, simulations strongly suggest that the new norm also\noutperforms the sum of nuclear norms for tensor completion from a random subset\nof entries.\n  Our lower bound for the sum-of-nuclear-norms model follows from a new result\non recovering signals with multiple sparse structures (e.g. sparse, low rank),\nwhich perhaps surprisingly demonstrates the significant suboptimality of the\ncommonly used recovery approach via minimizing the sum of individual sparsity\ninducing norms (e.g. $l_1$, nuclear norm). Our new formulation for low-rank\ntensor recovery however opens the possibility in reducing the sample complexity\nby exploiting several structures jointly.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 20:23:29 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2013 05:59:52 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Mu", "Cun", ""], ["Huang", "Bo", ""], ["Wright", "John", ""], ["Goldfarb", "Donald", ""]]}, {"id": "1307.5934", "submitter": "Zizhuo Wang", "authors": "Xiao Alison Chen, Zizhuo Wang", "title": "A Near-Optimal Dynamic Learning Algorithm for Online Matching Problems\n  with Concave Returns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an online matching problem with concave returns. This problem is\na significant generalization of the Adwords allocation problem and has vast\napplications in online advertising. In this problem, a sequence of items arrive\nsequentially and each has to be allocated to one of the bidders, who bid a\ncertain value for each item. At each time, the decision maker has to allocate\nthe current item to one of the bidders without knowing the future bids and the\nobjective is to maximize the sum of some concave functions of each bidder's\naggregate value. In this work, we propose an algorithm that achieves\nnear-optimal performance for this problem when the bids arrive in a random\norder and the input data satisfies certain conditions. The key idea of our\nalgorithm is to learn the input data pattern dynamically: we solve a sequence\nof carefully chosen partial allocation problems and use their optimal solutions\nto assist with the future decision. Our analysis belongs to the primal-dual\nparadigm, however, the absence of linearity of the objective function and the\ndynamic feature of the algorithm makes our analysis quite unique.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 03:24:28 GMT"}, {"version": "v2", "created": "Mon, 16 Feb 2015 17:39:58 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2015 09:43:57 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Chen", "Xiao Alison", ""], ["Wang", "Zizhuo", ""]]}, {"id": "1307.5944", "submitter": "Eric Hall", "authors": "Eric C. Hall and Rebecca M. Willett", "title": "Online Optimization in Dynamic Environments", "comments": "arXiv admin note: text overlap with arXiv:1301.1254", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing - Signal\n  Processing for Big Data, vol. 9, no 4. 2015", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-velocity streams of high-dimensional data pose significant \"big data\"\nanalysis challenges across a range of applications and settings. Online\nlearning and online convex programming play a significant role in the rapid\nrecovery of important or anomalous information from these large datastreams.\nWhile recent advances in online learning have led to novel and rapidly\nconverging algorithms, these methods are unable to adapt to nonstationary\nenvironments arising in real-world problems. This paper describes a dynamic\nmirror descent framework which addresses this challenge, yielding low\ntheoretical regret bounds and accurate, adaptive, and computationally efficient\nalgorithms which are applicable to broad classes of problems. The methods are\ncapable of learning and adapting to an underlying and possibly time-varying\ndynamical model. Empirical results in the context of dynamic texture analysis,\nsolar flare detection, sequential compressed sensing of a dynamic scene,\ntraffic surveillance,tracking self-exciting point processes and network\nbehavior in the Enron email corpus support the core theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 04:13:44 GMT"}, {"version": "v2", "created": "Thu, 3 Jul 2014 21:21:17 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2016 17:14:35 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Hall", "Eric C.", ""], ["Willett", "Rebecca M.", ""]]}, {"id": "1307.6134", "submitter": "Paul Reverdy", "authors": "Paul Reverdy, Vaibhav Srivastava, Naomi E. Leonard", "title": "Modeling Human Decision-making in Generalized Gaussian Multi-armed\n  Bandits", "comments": "25 pages. Appendix G included in this version details minor\n  modifications that correct for an oversight in the previously-published\n  proofs. The remainder of the text reflects the previously-published version", "journal-ref": "Proceedings of the IEEE, vol. 102, iss. 4, p. 544-571, 2014", "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a formal model of human decision-making in explore-exploit tasks\nusing the context of multi-armed bandit problems, where the decision-maker must\nchoose among multiple options with uncertain rewards. We address the standard\nmulti-armed bandit problem, the multi-armed bandit problem with transition\ncosts, and the multi-armed bandit problem on graphs. We focus on the case of\nGaussian rewards in a setting where the decision-maker uses Bayesian inference\nto estimate the reward values. We model the decision-maker's prior knowledge\nwith the Bayesian prior on the mean reward. We develop the upper credible limit\n(UCL) algorithm for the standard multi-armed bandit problem and show that this\ndeterministic algorithm achieves logarithmic cumulative expected regret, which\nis optimal performance for uninformative priors. We show how good priors and\ngood assumptions on the correlation structure among arms can greatly enhance\ndecision-making performance, even over short time horizons. We extend to the\nstochastic UCL algorithm and draw several connections to human decision-making\nbehavior. We present empirical data from human experiments and show that human\nperformance is efficiently captured by the stochastic UCL algorithm with\nappropriate parameters. For the multi-armed bandit problem with transition\ncosts and the multi-armed bandit problem on graphs, we generalize the UCL\nalgorithm to the block UCL algorithm and the graphical block UCL algorithm,\nrespectively. We show that these algorithms also achieve logarithmic cumulative\nexpected regret and require a sub-logarithmic expected number of transitions\namong arms. We further illustrate the performance of these algorithms with\nnumerical examples. NB: Appendix G included in this version details minor\nmodifications that correct for an oversight in the previously-published proofs.\nThe remainder of the text reflects the published work.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 16:05:13 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2014 17:28:13 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2014 19:10:11 GMT"}, {"version": "v4", "created": "Fri, 19 Apr 2019 16:14:11 GMT"}, {"version": "v5", "created": "Fri, 20 Dec 2019 15:27:31 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Reverdy", "Paul", ""], ["Srivastava", "Vaibhav", ""], ["Leonard", "Naomi E.", ""]]}, {"id": "1307.6143", "submitter": "Niko Br\\\"ummer", "authors": "Niko Brummer", "title": "Generative, Fully Bayesian, Gaussian, Openset Pattern Classifier", "comments": "Research Report, BOSARIS 2012 Speaker Recognition Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report works out the details of a closed-form, fully Bayesian,\nmulticlass, openset, generative pattern classifier using multivariate Gaussian\nlikelihoods, with conjugate priors. The generative model has a common\nwithin-class covariance, which is proportional to the between-class covariance\nin the conjugate prior. The scalar proportionality constant is the only plugin\nparameter. All other model parameters are intergated out in closed form. An\nexpression is given for the model evidence, which can be used to make plugin\nestimates for the proportionality constant. Pattern recognition is done via the\npredictive likeihoods of classes for which training data is available, as well\nas a predicitve likelihood for any as yet unseen class.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 16:33:00 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2013 13:26:08 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Brummer", "Niko", ""]]}, {"id": "1307.6365", "submitter": "Josif Grabocka", "authors": "Josif Grabocka, Martin Wistuba, Lars Schmidt-Thieme", "title": "Time-Series Classification Through Histograms of Symbolic Polynomials", "comments": null, "journal-ref": null, "doi": "10.1109/TKDE.2014.2377746", "report-no": null, "categories": "cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-series classification has attracted considerable research attention due\nto the various domains where time-series data are observed, ranging from\nmedicine to econometrics. Traditionally, the focus of time-series\nclassification has been on short time-series data composed of a unique pattern\nwith intraclass pattern distortions and variations, while recently there have\nbeen attempts to focus on longer series composed of various local patterns.\nThis study presents a novel method which can detect local patterns in long\ntime-series via fitting local polynomial functions of arbitrary degrees. The\ncoefficients of the polynomial functions are converted to symbolic words via\nequivolume discretizations of the coefficients' distributions. The symbolic\npolynomial words enable the detection of similar local patterns by assigning\nthe same words to similar polynomials. Moreover, a histogram of the frequencies\nof the words is constructed from each time-series' bag of words. Each row of\nthe histogram enables a new representation for the series and symbolize the\nexistence of local patterns and their frequencies. Experimental evidence\ndemonstrates outstanding results of our method compared to the state-of-art\nbaselines, by exhibiting the best classification accuracies in all the datasets\nand having statistically significant improvements in the absolute majority of\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 10:07:50 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2013 03:40:27 GMT"}, {"version": "v3", "created": "Wed, 31 Jul 2013 10:58:02 GMT"}, {"version": "v4", "created": "Mon, 23 Dec 2013 22:26:35 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Grabocka", "Josif", ""], ["Wistuba", "Martin", ""], ["Schmidt-Thieme", "Lars", ""]]}, {"id": "1307.6515", "submitter": "Sivaraman Balakrishnan", "authors": "Sivaraman Balakrishnan, Srivatsan Narayanan, Alessandro Rinaldo, Aarti\n  Singh, Larry Wasserman", "title": "Cluster Trees on Manifolds", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the problem of estimating the cluster tree for a\ndensity $f$ supported on or near a smooth $d$-dimensional manifold $M$\nisometrically embedded in $\\mathbb{R}^D$. We analyze a modified version of a\n$k$-nearest neighbor based algorithm recently proposed by Chaudhuri and\nDasgupta. The main results of this paper show that under mild assumptions on\n$f$ and $M$, we obtain rates of convergence that depend on $d$ only but not on\nthe ambient dimension $D$. We also show that similar (albeit non-algorithmic)\nresults can be obtained for kernel density estimators. We sketch a construction\nof a sample complexity lower bound instance for a natural class of manifold\noblivious clustering algorithms. We further briefly consider the known manifold\ncase and show that in this case a spatially adaptive algorithm achieves better\nrates.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 18:17:53 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Balakrishnan", "Sivaraman", ""], ["Narayanan", "Srivatsan", ""], ["Rinaldo", "Alessandro", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1307.6616", "submitter": "Shaobo Lin", "authors": "Shaobo Lin, Chen Xu, Jingshan Zeng, Jian Fang", "title": "Does generalization performance of $l^q$ regularization learning depend\n  on $q$? A negative example", "comments": "35 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  $l^q$-regularization has been demonstrated to be an attractive technique in\nmachine learning and statistical modeling. It attempts to improve the\ngeneralization (prediction) capability of a machine (model) through\nappropriately shrinking its coefficients. The shape of a $l^q$ estimator\ndiffers in varying choices of the regularization order $q$. In particular,\n$l^1$ leads to the LASSO estimate, while $l^{2}$ corresponds to the smooth\nridge regression. This makes the order $q$ a potential tuning parameter in\napplications. To facilitate the use of $l^{q}$-regularization, we intend to\nseek for a modeling strategy where an elaborative selection on $q$ is\navoidable. In this spirit, we place our investigation within a general\nframework of $l^{q}$-regularized kernel learning under a sample dependent\nhypothesis space (SDHS). For a designated class of kernel functions, we show\nthat all $l^{q}$ estimators for $0< q < \\infty$ attain similar generalization\nerror bounds. These estimated bounds are almost optimal in the sense that up to\na logarithmic factor, the upper and lower bounds are asymptotically identical.\nThis finding tentatively reveals that, in some modeling contexts, the choice of\n$q$ might not have a strong impact in terms of the generalization capability.\nFrom this perspective, $q$ can be arbitrarily specified, or specified merely by\nother no generalization criteria like smoothness, computational complexity,\nsparsity, etc..\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 00:48:04 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Lin", "Shaobo", ""], ["Xu", "Chen", ""], ["Zeng", "Jingshan", ""], ["Fang", "Jian", ""]]}, {"id": "1307.6769", "submitter": "Tamara Broderick", "authors": "Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C. Wilson,\n  Michael I. Jordan", "title": "Streaming Variational Bayes", "comments": "25 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SDA-Bayes, a framework for (S)treaming, (D)istributed,\n(A)synchronous computation of a Bayesian posterior. The framework makes\nstreaming updates to the estimated posterior according to a user-specified\napproximation batch primitive. We demonstrate the usefulness of our framework,\nwith variational Bayes (VB) as the primitive, by fitting the latent Dirichlet\nallocation model to two large-scale document collections. We demonstrate the\nadvantages of our algorithm over stochastic variational inference (SVI) by\ncomparing the two after a single pass through a known amount of data---a case\nwhere SVI may be applied---and in the streaming setting, where SVI does not\napply.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 15:03:40 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2013 23:29:01 GMT"}], "update_date": "2013-11-22", "authors_parsed": [["Broderick", "Tamara", ""], ["Boyd", "Nicholas", ""], ["Wibisono", "Andre", ""], ["Wilson", "Ashia C.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1307.6814", "submitter": "Shveta Bhatia", "authors": "Shveta Kundra Bhatia, V.S. Dixit", "title": "A Propound Method for the Improvement of Cluster Quality", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 4, No 2, July 2012 ISSN (Online): 1694-0814", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper Knockout Refinement Algorithm (KRA) is proposed to refine\noriginal clusters obtained by applying SOM and K-Means clustering algorithms.\nKRA Algorithm is based on Contingency Table concepts. Metrics are computed for\nthe Original and Refined Clusters. Quality of Original and Refined Clusters are\ncompared in terms of metrics. The proposed algorithm (KRA) is tested in the\neducational domain and results show that it generates better quality clusters\nin terms of improved metric values.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 17:07:39 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Bhatia", "Shveta Kundra", ""], ["Dixit", "V. S.", ""]]}, {"id": "1307.6887", "submitter": "Mohammad Gheshlaghi Azar", "authors": "Mohammad Gheshlaghi Azar and Alessandro Lazaric and Emma Brunskill", "title": "Sequential Transfer in Multi-armed Bandit with Finite Set of Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from prior tasks and transferring that experience to improve future\nperformance is critical for building lifelong learning agents. Although results\nin supervised and reinforcement learning show that transfer may significantly\nimprove the learning performance, most of the literature on transfer is focused\non batch learning tasks. In this paper we study the problem of\n\\textit{sequential transfer in online learning}, notably in the multi-armed\nbandit framework, where the objective is to minimize the cumulative regret over\na sequence of tasks by incrementally transferring knowledge from prior tasks.\nWe introduce a novel bandit algorithm based on a method-of-moments approach for\nthe estimation of the possible tasks and derive regret bounds for it.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 22:17:12 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Azar", "Mohammad Gheshlaghi", ""], ["Lazaric", "Alessandro", ""], ["Brunskill", "Emma", ""]]}, {"id": "1307.7024", "submitter": "Shiliang Sun", "authors": "Shiliang Sun", "title": "Multi-view Laplacian Support Vector Machines", "comments": "Lecture Notes in Computer Science, 2011, 7121: 209-222", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach, multi-view Laplacian support vector machines\n(SVMs), for semi-supervised learning under the multi-view scenario. It\nintegrates manifold regularization and multi-view regularization into the usual\nformulation of SVMs and is a natural extension of SVMs from supervised learning\nto multi-view semi-supervised learning. The function optimization problem in a\nreproducing kernel Hilbert space is converted to an optimization in a\nfinite-dimensional Euclidean space. After providing a theoretical bound for the\ngeneralization performance of the proposed method, we further give a\nformulation of the empirical Rademacher complexity which affects the bound\nsignificantly. From this bound and the empirical Rademacher complexity, we can\ngain insights into the roles played by different regularization terms to the\ngeneralization performance. Experimental results on synthetic and real-world\ndata sets are presented, which validate the effectiveness of the proposed\nmulti-view Laplacian SVMs approach.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 13:02:14 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Sun", "Shiliang", ""]]}, {"id": "1307.7028", "submitter": "Shiliang Sun", "authors": "Shiliang Sun", "title": "Infinite Mixtures of Multivariate Gaussian Processes", "comments": "Proceedings of the International Conference on Machine Learning and\n  Cybernetics, 2013, pages 1011-1016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new model called infinite mixtures of multivariate\nGaussian processes, which can be used to learn vector-valued functions and\napplied to multitask learning. As an extension of the single multivariate\nGaussian process, the mixture model has the advantages of modeling multimodal\ndata and alleviating the computationally cubic complexity of the multivariate\nGaussian process. A Dirichlet process prior is adopted to allow the (possibly\ninfinite) number of mixture components to be automatically inferred from\ntraining data, and Markov chain Monte Carlo sampling techniques are used for\nparameter and latent variable inference. Preliminary experimental results on\nmultivariate regression show the feasibility of the proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 13:24:31 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Sun", "Shiliang", ""]]}, {"id": "1307.7050", "submitter": "Khalid Raza", "authors": "Khalid Raza, Atif N Hasan", "title": "A Comprehensive Evaluation of Machine Learning Techniques for Cancer\n  Class Prediction Based on Microarray Data", "comments": "8 pages, 3 figures and 7 tables", "journal-ref": "International Journal of Bioinformatics Research and Applications,\n  Inderscience, 11(5): 397-416 (2015)", "doi": "10.1504/IJBRA.2015.071940", "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prostate cancer is among the most common cancer in males and its\nheterogeneity is well known. Its early detection helps making therapeutic\ndecision. There is no standard technique or procedure yet which is full-proof\nin predicting cancer class. The genomic level changes can be detected in gene\nexpression data and those changes may serve as standard model for any random\ncancer data for class prediction. Various techniques were implied on prostate\ncancer data set in order to accurately predict cancer class including machine\nlearning techniques. Huge number of attributes and few number of sample in\nmicroarray data leads to poor machine learning, therefore the most challenging\npart is attribute reduction or non significant gene reduction. In this work we\nhave compared several machine learning techniques for their accuracy in\npredicting the cancer class. Machine learning is effective when number of\nattributes (genes) are larger than the number of samples which is rarely\npossible with gene expression data. Attribute reduction or gene filtering is\nabsolutely required in order to make the data more meaningful as most of the\ngenes do not participate in tumor development and are irrelevant for cancer\nprediction. Here we have applied combination of statistical techniques such as\ninter-quartile range and t-test, which has been effective in filtering\nsignificant genes and minimizing noise from data. Further we have done a\ncomprehensive evaluation of ten state-of-the-art machine learning techniques\nfor their accuracy in class prediction of prostate cancer. Out of these\ntechniques, Bayes Network out performed with an accuracy of 94.11% followed by\nNavie Bayes with an accuracy of 91.17%. To cross validate our results, we\nmodified our training dataset in six different way and found that average\nsensitivity, specificity, precision and accuracy of Bayes Network is highest\namong all other techniques used.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 14:44:16 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Raza", "Khalid", ""], ["Hasan", "Atif N", ""]]}, {"id": "1307.7192", "submitter": "Mehrdad Mahdavi", "authors": "Mehrdad Mahdavi and Rong Jin", "title": "MixedGrad: An O(1/T) Convergence Rate Algorithm for Stochastic Smooth\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the optimal convergence rate for stochastic\noptimization of smooth functions is $O(1/\\sqrt{T})$, which is same as\nstochastic optimization of Lipschitz continuous convex functions. This is in\ncontrast to optimizing smooth functions using full gradients, which yields a\nconvergence rate of $O(1/T^2)$. In this work, we consider a new setup for\noptimizing smooth functions, termed as {\\bf Mixed Optimization}, which allows\nto access both a stochastic oracle and a full gradient oracle. Our goal is to\nsignificantly improve the convergence rate of stochastic optimization of smooth\nfunctions by having an additional small number of accesses to the full gradient\noracle. We show that, with an $O(\\ln T)$ calls to the full gradient oracle and\nan $O(T)$ calls to the stochastic oracle, the proposed mixed optimization\nalgorithm is able to achieve an optimization error of $O(1/T)$.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 23:27:23 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Mahdavi", "Mehrdad", ""], ["Jin", "Rong", ""]]}, {"id": "1307.7286", "submitter": "Harjinder Kaur Miss", "authors": "Harjinder Kaur, Gurpreet Singh, Jaspreet Minhas", "title": "A Review of Machine Learning based Anomaly Detection Techniques", "comments": "3 pages. arXiv admin note: text overlap with arXiv:1204.6416 by other\n  authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrusion detection is so much popular since the last two decades where\nintrusion is attempted to break into or misuse the system. It is mainly of two\ntypes based on the intrusions, first is Misuse or signature based detection and\nthe other is Anomaly detection. In this paper Machine learning based methods\nwhich are one of the types of Anomaly detection techniques is discussed.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2013 18:00:43 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Kaur", "Harjinder", ""], ["Singh", "Gurpreet", ""], ["Minhas", "Jaspreet", ""]]}, {"id": "1307.7303", "submitter": "Martin Mueller", "authors": "Martin E. Mueller and Madhura D. Thosar", "title": "Learning to Understand by Evolving Theories", "comments": "KRR Workshop at ICLP 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe an approach that enables an autonomous system to\ninfer the semantics of a command (i.e. a symbol sequence representing an\naction) in terms of the relations between changes in the observations and the\naction instances. We present a method of how to induce a theory (i.e. a\nsemantic description) of the meaning of a command in terms of a minimal set of\nbackground knowledge. The only thing we have is a sequence of observations from\nwhich we extract what kinds of effects were caused by performing the command.\nThis way, we yield a description of the semantics of the action and, hence, a\ndefinition.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2013 20:33:34 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Mueller", "Martin E.", ""], ["Thosar", "Madhura D.", ""]]}, {"id": "1307.7429", "submitter": "Seyyed Reza Khaze", "authors": "Amin Babazadeh Sangar, Seyyed Reza Khaze, Laya Ebrahimi", "title": "Participation anticipating in elections using data mining methods", "comments": null, "journal-ref": "International Journal on Cybernetics & Informatics ( IJCI) Vol.2,\n  No.2, April2013", "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anticipating the political behavior of people will be considerable help for\nelection candidates to assess the possibility of their success and to be\nacknowledged about the public motivations to select them. In this paper, we\nprovide a general schematic of the architecture of participation anticipating\nsystem in presidential election by using KNN, Classification Tree and Na\\\"ive\nBayes and tools orange based on crisp which had hopeful output. To test and\nassess the proposed model, we begin to use the case study by selecting 100\nqualified persons who attend in 11th presidential election of Islamic republic\nof Iran and anticipate their participation in Kohkiloye & Boyerahmad. We\nindicate that KNN can perform anticipation and classification processes with\nhigh accuracy in compared with two other algorithms to anticipate\nparticipation.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 01:15:25 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Sangar", "Amin Babazadeh", ""], ["Khaze", "Seyyed Reza", ""], ["Ebrahimi", "Laya", ""]]}, {"id": "1307.7432", "submitter": "Seyyed Reza Khaze", "authors": "Farhad Soleimanian Gharehchopogh, Seyyed Reza Khaze", "title": "Data mining application for cyber space users tendency in blog writing:\n  a case study", "comments": null, "journal-ref": "International Journal of Computer Applications (IJCA), Vol.47,\n  No.18, June2012", "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blogs are the recent emerging media which relies on information technology\nand technological advance. Since the mass media in some less-developed and\ndeveloping countries are in government service and their policies are developed\nbased on governmental interests, so blogs are provided for ideas and exchanging\nopinions. In this paper, we highlighted performed simulations from obtained\ninformation from 100 users and bloggers in Kohkiloye and Boyer Ahmad Province\nand using Weka 3.6 tool and c4.5 algorithm by applying decision tree with more\nthan %82 precision for getting future tendency anticipation of users to\nblogging and using in strategically areas.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 01:29:48 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Gharehchopogh", "Farhad Soleimanian", ""], ["Khaze", "Seyyed Reza", ""]]}, {"id": "1307.7577", "submitter": "Jun Liu", "authors": "Jun Liu, Zheng Zhao, Jie Wang, Jieping Ye", "title": "Safe Screening With Variational Inequalities and Its Application to\n  LASSO", "comments": "Accepted by International Conference on Machine Learning 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse learning techniques have been routinely used for feature selection as\nthe resulting model usually has a small number of non-zero entries. Safe\nscreening, which eliminates the features that are guaranteed to have zero\ncoefficients for a certain value of the regularization parameter, is a\ntechnique for improving the computational efficiency. Safe screening is gaining\nincreasing attention since 1) solving sparse learning formulations usually has\na high computational cost especially when the number of features is large and\n2) one needs to try several regularization parameters to select a suitable\nmodel. In this paper, we propose an approach called \"Sasvi\" (Safe screening\nwith variational inequalities). Sasvi makes use of the variational inequality\nthat provides the sufficient and necessary optimality condition for the dual\nproblem. Several existing approaches for Lasso screening can be casted as\nrelaxed versions of the proposed Sasvi, thus Sasvi provides a stronger safe\nscreening rule. We further study the monotone properties of Sasvi for Lasso,\nbased on which a sure removal regularization parameter can be identified for\neach feature. Experimental results on both synthetic and real data sets are\nreported to demonstrate the effectiveness of the proposed Sasvi for Lasso\nscreening.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 13:45:58 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2013 15:00:04 GMT"}, {"version": "v3", "created": "Mon, 12 May 2014 19:46:39 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Liu", "Jun", ""], ["Zhao", "Zheng", ""], ["Wang", "Jie", ""], ["Ye", "Jieping", ""]]}, {"id": "1307.7793", "submitter": "Yongsub  Lim", "authors": "Yongsub Lim, Kyomin Jung, Pushmeet Kohli", "title": "Multi-dimensional Parametric Mincuts for Constrained MAP Inference", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose novel algorithms for inferring the Maximum a\nPosteriori (MAP) solution of discrete pairwise random field models under\nmultiple constraints. We show how this constrained discrete optimization\nproblem can be formulated as a multi-dimensional parametric mincut problem via\nits Lagrangian dual, and prove that our algorithm isolates all constraint\ninstances for which the problem can be solved exactly. These multiple solutions\nenable us to even deal with `soft constraints' (higher order penalty\nfunctions). Moreover, we propose two practical variants of our algorithm to\nsolve problems with hard constraints. We also show how our method can be\napplied to solve various constrained discrete optimization problems such as\nsubmodular minimization and shortest path computation. Experimental evaluation\nusing the foreground-background image segmentation problem with statistic\nconstraints reveals that our method is faster and its results are closer to the\nground truth labellings compared with the popular continuous relaxation based\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 03:02:44 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Lim", "Yongsub", ""], ["Jung", "Kyomin", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1307.7795", "submitter": "Aaron Darling", "authors": "Ramanuja Simha and Hagit Shatkay", "title": "Protein (Multi-)Location Prediction: Using Location Inter-Dependencies\n  in a Probabilistic Framework", "comments": "Peer-reviewed and presented as part of the 13th Workshop on\n  Algorithms in Bioinformatics (WABI2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.LG q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing the location of a protein within the cell is important for\nunderstanding its function, role in biological processes, and potential use as\na drug target. Much progress has been made in developing computational methods\nthat predict single locations for proteins, assuming that proteins localize to\na single location. However, it has been shown that proteins localize to\nmultiple locations. While a few recent systems have attempted to predict\nmultiple locations of proteins, they typically treat locations as independent\nor capture inter-dependencies by treating each locations-combination present in\nthe training set as an individual location-class. We present a new method and a\npreliminary system we have developed that directly incorporates\ninter-dependencies among locations into the multiple-location-prediction\nprocess, using a collection of Bayesian network classifiers. We evaluate our\nsystem on a dataset of single- and multi-localized proteins. Our results,\nobtained by incorporating inter-dependencies are significantly higher than\nthose obtained by classifiers that do not use inter-dependencies. The\nperformance of our system on multi-localized proteins is comparable to a top\nperforming system (YLoc+), without restricting predictions to be based only on\nlocation-combinations present in the training set.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 03:19:05 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Simha", "Ramanuja", ""], ["Shatkay", "Hagit", ""]]}, {"id": "1307.7852", "submitter": "Jingdong Wang", "authors": "Jingdong Wang, Jing Wang, Gang Zeng, Zhuowen Tu, Rui Gan, and Shipeng\n  Li", "title": "Scalable $k$-NN graph construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The $k$-NN graph has played a central role in increasingly popular\ndata-driven techniques for various learning and vision tasks; yet, finding an\nefficient and effective way to construct $k$-NN graphs remains a challenge,\nespecially for large-scale high-dimensional data. In this paper, we propose a\nnew approach to construct approximate $k$-NN graphs with emphasis in:\nefficiency and accuracy. We hierarchically and randomly divide the data points\ninto subsets and build an exact neighborhood graph over each subset, achieving\na base approximate neighborhood graph; we then repeat this process for several\ntimes to generate multiple neighborhood graphs, which are combined to yield a\nmore accurate approximate neighborhood graph. Furthermore, we propose a\nneighborhood propagation scheme to further enhance the accuracy. We show both\ntheoretical and empirical accuracy and efficiency of our approach to $k$-NN\ngraph construction and demonstrate significant speed-up in dealing with large\nscale visual data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 07:33:31 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Wang", "Jingdong", ""], ["Wang", "Jing", ""], ["Zeng", "Gang", ""], ["Tu", "Zhuowen", ""], ["Gan", "Rui", ""], ["Li", "Shipeng", ""]]}, {"id": "1307.7948", "submitter": "Kristi Kuljus", "authors": "Kristi Kuljus and J\\\"uri Lember", "title": "On the accuracy of the Viterbi alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a hidden Markov model, the underlying Markov chain is usually hidden.\nOften, the maximum likelihood alignment (Viterbi alignment) is used as its\nestimate. Although having the biggest likelihood, the Viterbi alignment can\nbehave very untypically by passing states that are at most unexpected. To avoid\nsuch situations, the Viterbi alignment can be modified by forcing it not to\npass these states. In this article, an iterative procedure for improving the\nViterbi alignment is proposed and studied. The iterative approach is compared\nwith a simple bunch approach where a number of states with low probability are\nall replaced at the same time. It can be seen that the iterative way of\nadjusting the Viterbi alignment is more efficient and it has several advantages\nover the bunch approach. The same iterative algorithm for improving the Viterbi\nalignment can be used in the case of peeping, that is when it is possible to\nreveal hidden states. In addition, lower bounds for classification\nprobabilities of the Viterbi alignment under different conditions on the model\nparameters are studied.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 12:40:16 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Kuljus", "Kristi", ""], ["Lember", "J\u00fcri", ""]]}, {"id": "1307.7973", "submitter": "Antoine Bordes", "authors": "Jason Weston, Antoine Bordes, Oksana Yakhnenko, Nicolas Usunier", "title": "Connecting Language and Knowledge Bases with Embedding Models for\n  Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach for relation extraction from free text\nwhich is trained to jointly use information from the text and from existing\nknowledge. Our model is based on two scoring functions that operate by learning\nlow-dimensional embeddings of words and of entities and relationships from a\nknowledge base. We empirically show on New York Times articles aligned with\nFreebase relations that our approach is able to efficiently use the extra\ninformation provided by a large subset of Freebase data (4M entities, 23k\nrelationships) to improve over existing methods that rely on text features\nalone.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 13:37:09 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Weston", "Jason", ""], ["Bordes", "Antoine", ""], ["Yakhnenko", "Oksana", ""], ["Usunier", "Nicolas", ""]]}, {"id": "1307.7981", "submitter": "Niko Br\\\"ummer", "authors": "Niko Br\\\"ummer and George Doddington", "title": "Likelihood-ratio calibration using prior-weighted proper scoring rules", "comments": "Accepted, Interspeech 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior-weighted logistic regression has become a standard tool for calibration\nin speaker recognition. Logistic regression is the optimization of the expected\nvalue of the logarithmic scoring rule. We generalize this via a parametric\nfamily of proper scoring rules. Our theoretical analysis shows how different\nmembers of this family induce different relative weightings over a spectrum of\napplications of which the decision thresholds range from low to high. Special\nattention is given to the interaction between prior weighting and proper\nscoring rule parameters. Experiments on NIST SRE'12 suggest that for\napplications with low false-alarm rate requirements, scoring rules tailored to\nemphasize higher score thresholds may give better accuracy than logistic\nregression.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 13:59:13 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Br\u00fcmmer", "Niko", ""], ["Doddington", "George", ""]]}, {"id": "1307.7993", "submitter": "Weiguang Wang", "authors": "Weiguang Wang, Yingbin Liang, Eric P. Xing", "title": "Sharp Threshold for Multivariate Multi-Response Linear Regression via\n  Block Regularized Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a multivariate multi-response (MVMR) linear\nregression problem, which contains multiple linear regression models with\ndifferently distributed design matrices, and different regression and output\nvectors. The goal is to recover the support union of all regression vectors\nusing $l_1/l_2$-regularized Lasso. We characterize sufficient and necessary\nconditions on sample complexity \\emph{as a sharp threshold} to guarantee\nsuccessful recovery of the support union. Namely, if the sample size is above\nthe threshold, then $l_1/l_2$-regularized Lasso correctly recovers the support\nunion; and if the sample size is below the threshold, $l_1/l_2$-regularized\nLasso fails to recover the support union. In particular, the threshold\nprecisely captures the impact of the sparsity of regression vectors and the\nstatistical properties of the design matrices on sample complexity. Therefore,\nthe threshold function also captures the advantages of joint support union\nrecovery using multi-task Lasso over individual support recovery using\nsingle-task Lasso.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 14:24:52 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Wang", "Weiguang", ""], ["Liang", "Yingbin", ""], ["Xing", "Eric P.", ""]]}, {"id": "1307.8012", "submitter": "Robert Lyon", "authors": "R. J. Lyon, J. M. Brooke, J. D. Knowles, B. W. Stappers", "title": "A Study on Classification in Imbalanced and Partially-Labelled Data\n  Streams", "comments": "6 Pages, 2 figures, to be published in Proceedings 2013 IEEE\n  International Conference on Systems, Man, and Cybernetics (SMC)", "journal-ref": null, "doi": "10.1109/SMC.2013.260", "report-no": null, "categories": "astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The domain of radio astronomy is currently facing significant computational\nchallenges, foremost amongst which are those posed by the development of the\nworld's largest radio telescope, the Square Kilometre Array (SKA). Preliminary\nspecifications for this instrument suggest that the final design will\nincorporate between 2000 and 3000 individual 15 metre receiving dishes, which\ntogether can be expected to produce a data rate of many TB/s. Given such a high\ndata rate, it becomes crucial to consider how this information will be\nprocessed and stored to maximise its scientific utility. In this paper, we\nconsider one possible data processing scenario for the SKA, for the purposes of\nan all-sky pulsar survey. In particular we treat the selection of promising\nsignals from the SKA processing pipeline as a data stream classification\nproblem. We consider the feasibility of classifying signals that arrive via an\nunlabelled and heavily class imbalanced data stream, using currently available\nalgorithms and frameworks. Our results indicate that existing stream learners\nexhibit unacceptably low recall on real astronomical data when used in standard\nconfiguration; however, good false positive performance and comparable accuracy\nto static learners, suggests they have definite potential as an on-line\nsolution to this particular big data challenge.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 15:11:59 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lyon", "R. J.", ""], ["Brooke", "J. M.", ""], ["Knowles", "J. D.", ""], ["Stappers", "B. W.", ""]]}, {"id": "1307.8049", "submitter": "Stefanie Jegelka", "authors": "Xinghao Pan, Joseph E. Gonzalez, Stefanie Jegelka, Tamara Broderick,\n  Michael I. Jordan", "title": "Optimistic Concurrency Control for Distributed Unsupervised Learning", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on distributed machine learning algorithms has focused primarily on\none of two extremes - algorithms that obey strict concurrency constraints or\nalgorithms that obey few or no such constraints. We consider an intermediate\nalternative in which algorithms optimistically assume that conflicts are\nunlikely and if conflicts do arise a conflict-resolution protocol is invoked.\nWe view this \"optimistic concurrency control\" paradigm as particularly\nappropriate for large-scale machine learning algorithms, particularly in the\nunsupervised setting. We demonstrate our approach in three problem areas:\nclustering, feature learning and online facility location. We evaluate our\nmethods via large-scale experiments in a cluster computing environment.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 17:07:58 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Pan", "Xinghao", ""], ["Gonzalez", "Joseph E.", ""], ["Jegelka", "Stefanie", ""], ["Broderick", "Tamara", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1307.8136", "submitter": "Brian Kent", "authors": "Brian P. Kent, Alessandro Rinaldo, Timothy Verstynen", "title": "DeBaCl: A Python Package for Interactive DEnsity-BAsed CLustering", "comments": "28 pages, 9 figures, for associated software see\n  https://github.com/CoAxLab/DeBaCl", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The level set tree approach of Hartigan (1975) provides a probabilistically\nbased and highly interpretable encoding of the clustering behavior of a\ndataset. By representing the hierarchy of data modes as a dendrogram of the\nlevel sets of a density estimator, this approach offers many advantages for\nexploratory analysis and clustering, especially for complex and\nhigh-dimensional data. Several R packages exist for level set tree estimation,\nbut their practical usefulness is limited by computational inefficiency,\nabsence of interactive graphical capabilities and, from a theoretical\nperspective, reliance on asymptotic approximations. To make it easier for\npractitioners to capture the advantages of level set trees, we have written the\nPython package DeBaCl for DEnsity-BAsed CLustering. In this article we\nillustrate how DeBaCl's level set tree estimates can be used for difficult\nclustering tasks and interactive graphical data analysis. The package is\nintended to promote the practical use of level set trees through improvements\nin computational efficiency and a high degree of user customization. In\naddition, the flexible algorithms implemented in DeBaCl enjoy finite sample\naccuracy, as demonstrated in recent literature on density clustering. Finally,\nwe show the level set tree framework can be easily extended to deal with\nfunctional data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 20:19:26 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Kent", "Brian P.", ""], ["Rinaldo", "Alessandro", ""], ["Verstynen", "Timothy", ""]]}, {"id": "1307.8187", "submitter": "Haipeng Luo", "authors": "Haipeng Luo and Robert E. Schapire", "title": "Towards Minimax Online Learning with Unknown Time Horizon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online learning when the time horizon is unknown. We apply a\nminimax analysis, beginning with the fixed horizon case, and then moving on to\ntwo unknown-horizon settings, one that assumes the horizon is chosen randomly\naccording to some known distribution, and the other which allows the adversary\nfull control over the horizon. For the random horizon setting with restricted\nlosses, we derive a fully optimal minimax algorithm. And for the adversarial\nhorizon setting, we prove a nontrivial lower bound which shows that the\nadversary obtains strictly more power than when the horizon is fixed and known.\nBased on the minimax solution of the random horizon setting, we then propose a\nnew adaptive algorithm which \"pretends\" that the horizon is drawn from a\ndistribution from a special family, but no matter how the actual horizon is\nchosen, the worst-case regret is of the optimal rate. Furthermore, our\nalgorithm can be combined and applied in many ways, for instance, to online\nconvex optimization, follow the perturbed leader, exponential weights algorithm\nand first order bounds. Experiments show that our algorithm outperforms many\nother existing algorithms in an online linear optimization setting.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 01:49:50 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2013 18:49:58 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Luo", "Haipeng", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1307.8305", "submitter": "Tobias Glasmachers", "authors": "Tobias Glasmachers", "title": "The Planning-ahead SMO Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sequential minimal optimization (SMO) algorithm and variants thereof are\nthe de facto standard method for solving large quadratic programs for support\nvector machine (SVM) training. In this paper we propose a simple yet powerful\nmodification. The main emphasis is on an algorithm improving the SMO step size\nby planning-ahead. The theoretical analysis ensures its convergence to the\noptimum. Experiments involving a large number of datasets were carried out to\ndemonstrate the superiority of the new algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 12:38:20 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Glasmachers", "Tobias", ""]]}, {"id": "1307.8371", "submitter": "Pranjal Awasthi", "authors": "Pranjal Awasthi, Maria Florina Balcan, Philip M. Long", "title": "The Power of Localization for Efficiently Learning Linear Separators\n  with Noise", "comments": "Contains improved label complexity analysis communicated to us by\n  Steve Hanneke", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach for designing computationally efficient learning\nalgorithms that are tolerant to noise, and demonstrate its effectiveness by\ndesigning algorithms with improved noise tolerance guarantees for learning\nlinear separators.\n  We consider both the malicious noise model and the adversarial label noise\nmodel. For malicious noise, where the adversary can corrupt both the label and\nthe features, we provide a polynomial-time algorithm for learning linear\nseparators in $\\Re^d$ under isotropic log-concave distributions that can\ntolerate a nearly information-theoretically optimal noise rate of $\\eta =\n\\Omega(\\epsilon)$. For the adversarial label noise model, where the\ndistribution over the feature vectors is unchanged, and the overall probability\nof a noisy label is constrained to be at most $\\eta$, we also give a\npolynomial-time algorithm for learning linear separators in $\\Re^d$ under\nisotropic log-concave distributions that can handle a noise rate of $\\eta =\n\\Omega\\left(\\epsilon\\right)$.\n  We show that, in the active learning model, our algorithms achieve a label\ncomplexity whose dependence on the error parameter $\\epsilon$ is\npolylogarithmic. This provides the first polynomial-time active learning\nalgorithm for learning linear separators in the presence of malicious noise or\nadversarial label noise.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 16:11:26 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2013 18:51:51 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2013 21:49:27 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2013 19:18:31 GMT"}, {"version": "v5", "created": "Mon, 16 Dec 2013 17:20:36 GMT"}, {"version": "v6", "created": "Fri, 3 Jan 2014 17:20:00 GMT"}, {"version": "v7", "created": "Fri, 7 Mar 2014 17:15:52 GMT"}, {"version": "v8", "created": "Wed, 12 Oct 2016 17:42:40 GMT"}, {"version": "v9", "created": "Sun, 3 Jun 2018 18:22:37 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Balcan", "Maria Florina", ""], ["Long", "Philip M.", ""]]}, {"id": "1307.8430", "submitter": "Bryan Conroy", "authors": "Bryan R. Conroy, Jennifer M. Walz, Brian Cheung, Paul Sajda", "title": "Fast Simultaneous Training of Generalized Linear Models (FaSTGLZ)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient algorithm for simultaneously training sparse\ngeneralized linear models across many related problems, which may arise from\nbootstrapping, cross-validation and nonparametric permutation testing. Our\napproach leverages the redundancies across problems to obtain significant\ncomputational improvements relative to solving the problems sequentially by a\nconventional algorithm. We demonstrate our fast simultaneous training of\ngeneralized linear models (FaSTGLZ) algorithm on a number of real-world\ndatasets, and we run otherwise computationally intensive bootstrapping and\npermutation test analyses that are typically necessary for obtaining\nstatistically rigorous classification results and meaningful interpretation.\nCode is freely available at http://liinc.bme.columbia.edu/fastglz.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 19:18:11 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Conroy", "Bryan R.", ""], ["Walz", "Jennifer M.", ""], ["Cheung", "Brian", ""], ["Sajda", "Paul", ""]]}]