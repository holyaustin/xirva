[{"id": "1707.00010", "submitter": "Muhammad Bilal Zafar", "authors": "Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, Krishna\n  P. Gummadi, Adrian Weller", "title": "From Parity to Preference-based Notions of Fairness in Classification", "comments": "To appear in Proceedings of the 31st Conference on Neural Information\n  Processing Systems (NIPS 2017). Code available at:\n  https://github.com/mbilalzafar/fair-classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adoption of automated, data-driven decision making in an ever expanding\nrange of applications has raised concerns about its potential unfairness\ntowards certain social groups. In this context, a number of recent studies have\nfocused on defining, detecting, and removing unfairness from data-driven\ndecision systems. However, the existing notions of fairness, based on parity\n(equality) in treatment or outcomes for different social groups, tend to be\nquite stringent, limiting the overall decision making accuracy. In this paper,\nwe draw inspiration from the fair-division and envy-freeness literature in\neconomics and game theory and propose preference-based notions of fairness --\ngiven the choice between various sets of decision treatments or outcomes, any\ngroup of users would collectively prefer its treatment or outcomes, regardless\nof the (dis)parity as compared to the other groups. Then, we introduce\ntractable proxies to design margin-based classifiers that satisfy these\npreference-based notions of fairness. Finally, we experiment with a variety of\nsynthetic and real-world datasets and show that preference-based fairness\nallows for greater decision accuracy than parity-based fairness.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 18:01:49 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 19:00:49 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Zafar", "Muhammad Bilal", ""], ["Valera", "Isabel", ""], ["Rodriguez", "Manuel Gomez", ""], ["Gummadi", "Krishna P.", ""], ["Weller", "Adrian", ""]]}, {"id": "1707.00044", "submitter": "Yahav Bechavod", "authors": "Yahav Bechavod and Katrina Ligett", "title": "Penalizing Unfairness in Binary Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for mitigating unfairness in learned classifiers.\nIn particular, we focus on binary classification tasks over individuals from\ntwo populations, where, as our criterion for fairness, we wish to achieve\nsimilar false positive rates in both populations, and similar false negative\nrates in both populations. As a proof of concept, we implement our approach and\nempirically evaluate its ability to achieve both fairness and accuracy, using\ndatasets from the fields of criminal risk assessment, credit, lending, and\ncollege admissions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 20:59:44 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 22:21:52 GMT"}, {"version": "v3", "created": "Thu, 8 Mar 2018 17:58:40 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Bechavod", "Yahav", ""], ["Ligett", "Katrina", ""]]}, {"id": "1707.00075", "submitter": "Alex Beutel", "authors": "Alex Beutel, Jilin Chen, Zhe Zhao, Ed H. Chi", "title": "Data Decisions and Theoretical Implications when Adversarially Learning\n  Fair Representations", "comments": "Presented as a poster at the 2017 Workshop on Fairness,\n  Accountability, and Transparency in Machine Learning (FAT/ML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we learn a classifier that is \"fair\" for a protected or sensitive\ngroup, when we do not know if the input to the classifier belongs to the\nprotected group? How can we train such a classifier when data on the protected\ngroup is difficult to attain? In many settings, finding out the sensitive input\nattribute can be prohibitively expensive even during model training, and\nsometimes impossible during model serving. For example, in recommender systems,\nif we want to predict if a user will click on a given recommendation, we often\ndo not know many attributes of the user, e.g., race or age, and many attributes\nof the content are hard to determine, e.g., the language or topic. Thus, it is\nnot feasible to use a different classifier calibrated based on knowledge of the\nsensitive attribute.\n  Here, we use an adversarial training procedure to remove information about\nthe sensitive attribute from the latent representation learned by a neural\nnetwork. In particular, we study how the choice of data for the adversarial\ntraining effects the resulting fairness properties. We find two interesting\nresults: a small amount of data is needed to train these adversarial models,\nand the data distribution empirically drives the adversary's notion of\nfairness.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 01:09:33 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 01:31:36 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Beutel", "Alex", ""], ["Chen", "Jilin", ""], ["Zhao", "Zhe", ""], ["Chi", "Ed H.", ""]]}, {"id": "1707.00095", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Francis Li, and Alexander Wong", "title": "Exploring the Imposition of Synaptic Precision Restrictions For\n  Evolutionary Synthesis of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key contributing factor to incredible success of deep neural networks has\nbeen the significant rise on massively parallel computing devices allowing\nresearchers to greatly increase the size and depth of deep neural networks,\nleading to significant improvements in modeling accuracy. Although deeper,\nlarger, or complex deep neural networks have shown considerable promise, the\ncomputational complexity of such networks is a major barrier to utilization in\nresource-starved scenarios. We explore the synaptogenesis of deep neural\nnetworks in the formation of efficient deep neural network architectures within\nan evolutionary deep intelligence framework, where a probabilistic generative\nmodeling strategy is introduced to stochastically synthesize increasingly\nefficient yet effective offspring deep neural networks over generations,\nmimicking evolutionary processes such as heredity, random mutation, and natural\nselection in a probabilistic manner. In this study, we primarily explore the\nimposition of synaptic precision restrictions and its impact on the\nevolutionary synthesis of deep neural networks to synthesize more efficient\nnetwork architectures tailored for resource-starved scenarios. Experimental\nresults show significant improvements in synaptic efficiency (~10X decrease for\nGoogLeNet-based DetectNet) and inference speed (>5X increase for\nGoogLeNet-based DetectNet) while preserving modeling accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 04:56:08 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Li", "Francis", ""], ["Wong", "Alexander", ""]]}, {"id": "1707.00117", "submitter": "Wenbo Hu", "authors": "Wenbo Hu, Lifeng Hua, Lei Li, Hang Su, Tian Wang, Ning Chen, Bo Zhang", "title": "SAM: Semantic Attribute Modulation for Language Modeling and Style\n  Variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Semantic Attribute Modulation (SAM) for language\nmodeling and style variation. The semantic attribute modulation includes\nvarious document attributes, such as titles, authors, and document categories.\nWe consider two types of attributes, (title attributes and category\nattributes), and a flexible attribute selection scheme by automatically scoring\nthem via an attribute attention mechanism. The semantic attributes are embedded\ninto the hidden semantic space as the generation inputs. With the attributes\nproperly harnessed, our proposed SAM can generate interpretable texts with\nregard to the input attributes. Qualitative analysis, including word semantic\nanalysis and attention values, shows the interpretability of SAM. On several\ntypical text datasets, we empirically demonstrate the superiority of the\nSemantic Attribute Modulated language model with different combinations of\ndocument attributes. Moreover, we present a style variation for the lyric\ngeneration using SAM, which shows a strong connection between the style\nvariation and the semantic attributes.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 09:00:28 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 14:59:04 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 03:53:00 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Hu", "Wenbo", ""], ["Hua", "Lifeng", ""], ["Li", "Lei", ""], ["Su", "Hang", ""], ["Wang", "Tian", ""], ["Chen", "Ning", ""], ["Zhang", "Bo", ""]]}, {"id": "1707.00130", "submitter": "Pei-Hao Su", "authors": "Pei-Hao Su, Pawel Budzianowski, Stefan Ultes, Milica Gasic, and Steve\n  Young", "title": "Sample-efficient Actor-Critic Reinforcement Learning with Supervised\n  Data for Dialogue Management", "comments": "Accepted as a long paper in SigDial 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) methods have significant potential for\ndialogue policy optimisation. However, they suffer from a poor performance in\nthe early stages of learning. This is especially problematic for on-line\nlearning with real users. Two approaches are introduced to tackle this problem.\nFirstly, to speed up the learning process, two sample-efficient neural networks\nalgorithms: trust region actor-critic with experience replay (TRACER) and\nepisodic natural actor-critic with experience replay (eNACER) are presented.\nFor TRACER, the trust region helps to control the learning step size and avoid\ncatastrophic model changes. For eNACER, the natural gradient identifies the\nsteepest ascent direction in policy space to speed up the convergence. Both\nmodels employ off-policy learning with experience replay to improve\nsample-efficiency. Secondly, to mitigate the cold start issue, a corpus of\ndemonstration data is utilised to pre-train the models prior to on-line\nreinforcement learning. Combining these two approaches, we demonstrate a\npractical approach to learn deep RL-based dialogue policies and demonstrate\ntheir effectiveness in a task-oriented information seeking domain.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 09:56:31 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 08:55:16 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Su", "Pei-Hao", ""], ["Budzianowski", "Pawel", ""], ["Ultes", "Stefan", ""], ["Gasic", "Milica", ""], ["Young", "Steve", ""]]}, {"id": "1707.00143", "submitter": "Cong Fu", "authors": "Cong Fu, Chao Xiang, Changxu Wang, Deng Cai", "title": "Fast Approximate Nearest Neighbor Search With The Navigating\n  Spreading-out Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate nearest neighbor search (ANNS) is a fundamental problem in\ndatabases and data mining. A scalable ANNS algorithm should be both\nmemory-efficient and fast. Some early graph-based approaches have shown\nattractive theoretical guarantees on search time complexity, but they all\nsuffer from the problem of high indexing time complexity. Recently, some\ngraph-based methods have been proposed to reduce indexing complexity by\napproximating the traditional graphs; these methods have achieved revolutionary\nperformance on million-scale datasets. Yet, they still can not scale to\nbillion-node databases. In this paper, to further improve the search-efficiency\nand scalability of graph-based methods, we start by introducing four aspects:\n(1) ensuring the connectivity of the graph; (2) lowering the average out-degree\nof the graph for fast traversal; (3) shortening the search path; and (4)\nreducing the index size. Then, we propose a novel graph structure called\nMonotonic Relative Neighborhood Graph (MRNG) which guarantees very low search\ncomplexity (close to logarithmic time). To further lower the indexing\ncomplexity and make it practical for billion-node ANNS problems, we propose a\nnovel graph structure named Navigating Spreading-out Graph (NSG) by\napproximating the MRNG. The NSG takes the four aspects into account\nsimultaneously. Extensive experiments show that NSG outperforms all the\nexisting algorithms significantly. In addition, NSG shows superior performance\nin the E-commercial search scenario of Taobao (Alibaba Group) and has been\nintegrated into their search engine at billion-node scale.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 11:52:37 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 06:44:39 GMT"}, {"version": "v3", "created": "Fri, 19 Jan 2018 08:40:37 GMT"}, {"version": "v4", "created": "Sat, 10 Feb 2018 02:57:14 GMT"}, {"version": "v5", "created": "Tue, 29 May 2018 03:56:20 GMT"}, {"version": "v6", "created": "Sun, 3 Jun 2018 15:18:10 GMT"}, {"version": "v7", "created": "Tue, 25 Sep 2018 06:10:31 GMT"}, {"version": "v8", "created": "Thu, 4 Oct 2018 07:21:17 GMT"}, {"version": "v9", "created": "Tue, 11 Dec 2018 20:25:58 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Fu", "Cong", ""], ["Xiang", "Chao", ""], ["Wang", "Changxu", ""], ["Cai", "Deng", ""]]}, {"id": "1707.00183", "submitter": "Tambet Matiisen", "authors": "Tambet Matiisen, Avital Oliver, Taco Cohen, John Schulman", "title": "Teacher-Student Curriculum Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Teacher-Student Curriculum Learning (TSCL), a framework for\nautomatic curriculum learning, where the Student tries to learn a complex task\nand the Teacher automatically chooses subtasks from a given set for the Student\nto train on. We describe a family of Teacher algorithms that rely on the\nintuition that the Student should practice more those tasks on which it makes\nthe fastest progress, i.e. where the slope of the learning curve is highest. In\naddition, the Teacher algorithms address the problem of forgetting by also\nchoosing tasks where the Student's performance is getting worse. We demonstrate\nthat TSCL matches or surpasses the results of carefully hand-crafted curricula\nin two tasks: addition of decimal numbers with LSTM and navigation in\nMinecraft. Using our automatically generated curriculum enabled to solve a\nMinecraft maze that could not be solved at all when training directly on\nsolving the maze, and the learning was an order of magnitude faster than\nuniform sampling of subtasks.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 18:13:17 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 20:57:09 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Matiisen", "Tambet", ""], ["Oliver", "Avital", ""], ["Cohen", "Taco", ""], ["Schulman", "John", ""]]}, {"id": "1707.00192", "submitter": "Yixin Fang", "authors": "Yixin Fang, Jinfeng Xu, Lei Yang", "title": "On Scalable Inference with Stochastic Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications involving large dataset or online updating, stochastic\ngradient descent (SGD) provides a scalable way to compute parameter estimates\nand has gained increasing popularity due to its numerical convenience and\nmemory efficiency. While the asymptotic properties of SGD-based estimators have\nbeen established decades ago, statistical inference such as interval estimation\nremains much unexplored. The traditional resampling method such as the\nbootstrap is not computationally feasible since it requires to repeatedly draw\nindependent samples from the entire dataset. The plug-in method is not\napplicable when there are no explicit formulas for the covariance matrix of the\nestimator. In this paper, we propose a scalable inferential procedure for\nstochastic gradient descent, which, upon the arrival of each observation,\nupdates the SGD estimate as well as a large number of randomly perturbed SGD\nestimates. The proposed method is easy to implement in practice. We establish\nits theoretical properties for a general class of models that includes\ngeneralized linear models and quantile regression models as special cases. The\nfinite-sample performance and numerical utility is evaluated by simulation\nstudies and two real data applications.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 18:54:34 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Fang", "Yixin", ""], ["Xu", "Jinfeng", ""], ["Yang", "Lei", ""]]}, {"id": "1707.00206", "submitter": "Zhiting Hu", "authors": "Junxian He, Zhiting Hu, Taylor Berg-Kirkpatrick, Ying Huang, Eric P.\n  Xing", "title": "Efficient Correlated Topic Modeling with Topic Embedding", "comments": "KDD 2017 oral. The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlated topic modeling has been limited to small model and problem sizes\ndue to their high computational cost and poor scaling. In this paper, we\npropose a new model which learns compact topic embeddings and captures topic\ncorrelations through the closeness between the topic vectors. Our method\nenables efficient inference in the low-dimensional embedding space, reducing\nprevious cubic or quadratic time complexity to linear w.r.t the topic size. We\nfurther speedup variational inference with a fast sampler to exploit sparsity\nof topic occurrence. Extensive experiments show that our approach is capable of\nhandling model and data scales which are several orders of magnitude larger\nthan existing correlation results, without sacrificing modeling quality by\nproviding competitive or superior performance in document classification and\nretrieval.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 21:10:15 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["He", "Junxian", ""], ["Hu", "Zhiting", ""], ["Berg-Kirkpatrick", "Taylor", ""], ["Huang", "Ying", ""], ["Xing", "Eric P.", ""]]}, {"id": "1707.00260", "submitter": "Shiliang Sun", "authors": "Shiliang Sun, John Paisley, Qiuyang Liu", "title": "Location Dependent Dirichlet Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dirichlet processes (DP) are widely applied in Bayesian nonparametric\nmodeling. However, in their basic form they do not directly integrate\ndependency information among data arising from space and time. In this paper,\nwe propose location dependent Dirichlet processes (LDDP) which incorporate\nnonparametric Gaussian processes in the DP modeling framework to model such\ndependencies. We develop the LDDP in the context of mixture modeling, and\ndevelop a mean field variational inference algorithm for this mixture model.\nThe effectiveness of the proposed modeling framework is shown on an image\nsegmentation task.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 08:33:39 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Sun", "Shiliang", ""], ["Paisley", "John", ""], ["Liu", "Qiuyang", ""]]}, {"id": "1707.00297", "submitter": "Minyar Sassi", "authors": "Mohamed Ali Zoghlami, Olfa Arfaoui, Minyar Sassi Hidri, Rahma Ben Ayed", "title": "Classification non supervis\\'ee des donn\\'ees h\\'et\\'erog\\`enes \\`a\n  large \\'echelle", "comments": "6 pages, in French, 8 figures", "journal-ref": "Conf\\'erence Internationale Francophone sur la Science de\n  Donn\\'ees - Les 23\\`emes Rencontres annuelles de la Soci\\'et\\'e Francophone\n  de Classification (AAFD & SFC), Marrakech, Maroc, pp. 37-42, 2016", "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When it comes to cluster massive data, response time, disk access and quality\nof formed classes becoming major issues for companies. It is in this context\nthat we have come to define a clustering framework for large scale\nheterogeneous data that contributes to the resolution of these issues. The\nproposed framework is based on, firstly, the descriptive analysis based on MCA,\nand secondly, the MapReduce paradigm in a large scale environment. The results\nare encouraging and prove the efficiency of the hybrid deployment on response\nquality and time component as on qualitative and quantitative data.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 14:26:51 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Zoghlami", "Mohamed Ali", ""], ["Arfaoui", "Olfa", ""], ["Hidri", "Minyar Sassi", ""], ["Ayed", "Rahma Ben", ""]]}, {"id": "1707.00300", "submitter": "Dianhui Wang", "authors": "Dianhui Wang, Caihao Cui", "title": "Stochastic Configuration Networks Ensemble for Large-Scale Data\n  Analytics", "comments": "20 pages, 7 figures, 9 tables; this paper has been submitted to\n  Information Sciences for publication in December 2016, and accepted on July\n  3, 2017", "journal-ref": null, "doi": "10.1016/j.ins.2017.07.003", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a fast decorrelated neuro-ensemble with heterogeneous\nfeatures for large-scale data analytics, where stochastic configuration\nnetworks (SCNs) are employed as base learner models and the well-known negative\ncorrelation learning (NCL) strategy is adopted to evaluate the output weights.\nBy feeding a large number of samples into the SCN base models, we obtain a huge\nsized linear equation system which is difficult to be solved by means of\ncomputing a pseudo-inverse used in the least squares method. Based on the group\nof heterogeneous features, the block Jacobi and Gauss-Seidel methods are\nemployed to iteratively evaluate the output weights, and a convergence analysis\nis given with a demonstration on the uniqueness of these iterative solutions.\nExperiments with comparisons on two large-scale datasets are carried out, and\nthe system robustness with respect to the regularizing factor used in NCL is\ngiven. Results indicate that the proposed ensemble learning techniques have\ngood potential for resolving large-scale data modelling problems.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 14:43:23 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 02:09:46 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Wang", "Dianhui", ""], ["Cui", "Caihao", ""]]}, {"id": "1707.00309", "submitter": "R Devon Hjelm", "authors": "Karan Grewal and R Devon Hjelm and Yoshua Bengio", "title": "Variance Regularizing Adversarial Learning", "comments": "Method is out of date and some results are incorrect", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach for training adversarial models by replacing\nthe discriminator score with a bi-modal Gaussian distribution over the\nreal/fake indicator variables. In order to do this, we train the Gaussian\nclassifier to match the target bi-modal distribution implicitly through\nmeta-adversarial training. We hypothesize that this approach ensures a non-zero\ngradient to the generator, even in the limit of a perfect classifier. We test\nour method against standard benchmark image datasets as well as show the\nclassifier output distribution is smooth and has overlap between the real and\nfake modes.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 15:36:07 GMT"}, {"version": "v2", "created": "Sun, 19 Aug 2018 20:21:33 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Grewal", "Karan", ""], ["Hjelm", "R Devon", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1707.00351", "submitter": "Minyar Sassi", "authors": "Rania Mkhinini Gahar, Olfa Arfaoui, Minyar Sassi Hidri, Nejib Ben-Hadj\n  Alouane", "title": "Dimensionality reduction with missing values imputation", "comments": "6 pages, 2 figures, The first Computer science University of Tunis El\n  Manar, PhD Symposium (CUPS'17), Tunisia, May 22-25, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a new statical approach for high-dimensionality\nreduction of heterogenous data that limits the curse of dimensionality and\ndeals with missing values. To handle these latter, we propose to use the Random\nForest imputation's method. The main purpose here is to extract useful\ninformation and so reducing the search space to facilitate the data exploration\nprocess. Several illustrative numeric examples, using data coming from publicly\navailable machine learning repositories are also included. The experimental\ncomponent of the study shows the efficiency of the proposed analytical\napproach.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 20:47:11 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Gahar", "Rania Mkhinini", ""], ["Arfaoui", "Olfa", ""], ["Hidri", "Minyar Sassi", ""], ["Alouane", "Nejib Ben-Hadj", ""]]}, {"id": "1707.00372", "submitter": "Jong Chul Ye", "authors": "Jong Chul Ye, Yoseob Han, Eunju Cha", "title": "Deep Convolutional Framelets: A General Deep Learning Framework for\n  Inverse Problems", "comments": "This will appear in SIAM Journal on Imaging Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning approaches with various network architectures have\nachieved significant performance improvement over existing iterative\nreconstruction methods in various imaging problems. However, it is still\nunclear why these deep learning architectures work for specific inverse\nproblems. To address these issues, here we show that the long-searched-for\nmissing link is the convolution framelets for representing a signal by\nconvolving local and non-local bases. The convolution framelets was originally\ndeveloped to generalize the theory of low-rank Hankel matrix approaches for\ninverse problems, and this paper further extends the idea so that we can obtain\na deep neural network using multilayer convolution framelets with perfect\nreconstruction (PR) under rectilinear linear unit nonlinearity (ReLU). Our\nanalysis also shows that the popular deep network components such as residual\nblock, redundant filter channels, and concatenated ReLU (CReLU) do indeed help\nto achieve the PR, while the pooling and unpooling layers should be augmented\nwith high-pass branches to meet the PR condition. Moreover, by changing the\nnumber of filter channels and bias, we can control the shrinkage behaviors of\nthe neural network. This discovery leads us to propose a novel theory for deep\nconvolutional framelets neural network. Using numerical experiments with\nvarious inverse problems, we demonstrated that our deep convolution framelets\nnetwork shows consistent improvement over existing deep architectures.This\ndiscovery suggests that the success of deep learning is not from a magical\npower of a black-box, but rather comes from the power of a novel signal\nrepresentation using non-local basis combined with data-driven local basis,\nwhich is indeed a natural extension of classical signal processing theory.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 00:16:04 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 08:20:27 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 13:27:07 GMT"}, {"version": "v4", "created": "Thu, 30 Nov 2017 07:12:01 GMT"}, {"version": "v5", "created": "Thu, 25 Jan 2018 09:37:10 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Ye", "Jong Chul", ""], ["Han", "Yoseob", ""], ["Cha", "Eunju", ""]]}, {"id": "1707.00389", "submitter": "Il Yong Chun", "authors": "Il Yong Chun, Jeffrey A. Fessler", "title": "Convolutional Dictionary Learning: Acceleration and Convergence", "comments": "21 pages, 7 figures, submitted to IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2017.2761545", "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional dictionary learning (CDL or sparsifying CDL) has many\napplications in image processing and computer vision. There has been growing\ninterest in developing efficient algorithms for CDL, mostly relying on the\naugmented Lagrangian (AL) method or the variant alternating direction method of\nmultipliers (ADMM). When their parameters are properly tuned, AL methods have\nshown fast convergence in CDL. However, the parameter tuning process is not\ntrivial due to its data dependence and, in practice, the convergence of AL\nmethods depends on the AL parameters for nonconvex CDL problems. To moderate\nthese problems, this paper proposes a new practically feasible and convergent\nBlock Proximal Gradient method using a Majorizer (BPG-M) for CDL. The\nBPG-M-based CDL is investigated with different block updating schemes and\nmajorization matrix designs, and further accelerated by incorporating some\nmomentum coefficient formulas and restarting techniques. All of the methods\ninvestigated incorporate a boundary artifacts removal (or, more generally,\nsampling) operator in the learning model. Numerical experiments show that,\nwithout needing any parameter tuning process, the proposed BPG-M approach\nconverges more stably to desirable solutions of lower objective values than the\nexisting state-of-the-art ADMM algorithm and its memory-efficient variant do.\nCompared to the ADMM approaches, the BPG-M method using a multi-block updating\nscheme is particularly useful in single-threaded CDL algorithm handling large\ndatasets, due to its lower memory requirement and no polynomial computational\ncomplexity. Image denoising experiments show that, for relatively strong\nadditive white Gaussian noise, the filters learned by BPG-M-based CDL\noutperform those trained by the ADMM approach.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 03:42:22 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 07:12:16 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Chun", "Il Yong", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1707.00391", "submitter": "Suresh Venkatasubramanian", "authors": "Amanda Bower and Sarah N. Kitchen and Laura Niss and Martin J. Strauss\n  and Alexander Vargas and Suresh Venkatasubramanian", "title": "Fair Pipelines", "comments": "Presented as a poster at the 2017 Workshop on Fairness,\n  Accountability, and Transparency in Machine Learning (FAT/ML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work facilitates ensuring fairness of machine learning in the real world\nby decoupling fairness considerations in compound decisions. In particular,\nthis work studies how fairness propagates through a compound decision-making\nprocesses, which we call a pipeline. Prior work in algorithmic fairness only\nfocuses on fairness with respect to one decision. However, many decision-making\nprocesses require more than one decision. For instance, hiring is at least a\ntwo stage model: deciding who to interview from the applicant pool and then\ndeciding who to hire from the interview pool. Perhaps surprisingly, we show\nthat the composition of fair components may not guarantee a fair pipeline under\na $(1+\\varepsilon)$-equal opportunity definition of fair. However, we identify\ncircumstances that do provide that guarantee. We also propose numerous\ndirections for future work on more general compound machine learning decisions.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 03:53:29 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Bower", "Amanda", ""], ["Kitchen", "Sarah N.", ""], ["Niss", "Laura", ""], ["Strauss", "Martin J.", ""], ["Vargas", "Alexander", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1707.00415", "submitter": "Tao Qin Dr.", "authors": "Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, Tie-Yan Liu", "title": "Dual Supervised Learning", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many supervised learning tasks are emerged in dual forms, e.g.,\nEnglish-to-French translation vs. French-to-English translation, speech\nrecognition vs. text to speech, and image classification vs. image generation.\nTwo dual tasks have intrinsic connections with each other due to the\nprobabilistic correlation between their models. This connection is, however,\nnot effectively utilized today, since people usually train the models of two\ndual tasks separately and independently. In this work, we propose training the\nmodels of two dual tasks simultaneously, and explicitly exploiting the\nprobabilistic correlation between them to regularize the training process. For\nease of reference, we call the proposed approach \\emph{dual supervised\nlearning}. We demonstrate that dual supervised learning can improve the\npractical performances of both tasks, for various applications including\nmachine translation, image processing, and sentiment analysis.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 06:19:31 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Xia", "Yingce", ""], ["Qin", "Tao", ""], ["Chen", "Wei", ""], ["Bian", "Jiang", ""], ["Yu", "Nenghai", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1707.00418", "submitter": "Chih-Kuan Yeh", "authors": "Chih-Kuan Yeh, Wei-Chieh Wu, Wei-Jen Ko, Yu-Chiang Frank Wang", "title": "Learning Deep Latent Spaces for Multi-Label Classification", "comments": "published in AAAI-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification is a practical yet challenging task in machine\nlearning related fields, since it requires the prediction of more than one\nlabel category for each input instance. We propose a novel deep neural networks\n(DNN) based model, Canonical Correlated AutoEncoder (C2AE), for solving this\ntask. Aiming at better relating feature and label domain data for improved\nclassification, we uniquely perform joint feature and label embedding by\nderiving a deep latent space, followed by the introduction of label-correlation\nsensitive loss function for recovering the predicted label outputs. Our C2AE is\nachieved by integrating the DNN architectures of canonical correlation analysis\nand autoencoder, which allows end-to-end learning and prediction with the\nability to exploit label dependency. Moreover, our C2AE can be easily extended\nto address the learning problem with missing labels. Our experiments on\nmultiple datasets with different scales confirm the effectiveness and\nrobustness of our proposed method, which is shown to perform favorably against\nstate-of-the-art methods for multi-label classification.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 06:37:01 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Yeh", "Chih-Kuan", ""], ["Wu", "Wei-Chieh", ""], ["Ko", "Wei-Jen", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "1707.00424", "submitter": "Pratik Chaudhari", "authors": "Pratik Chaudhari, Carlo Baldassi, Riccardo Zecchina, Stefano Soatto,\n  Ameet Talwalkar, Adam Oberman", "title": "Parle: parallelizing stochastic gradient descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm called Parle for parallel training of deep\nnetworks that converges 2-4x faster than a data-parallel implementation of SGD,\nwhile achieving significantly improved error rates that are nearly\nstate-of-the-art on several benchmarks including CIFAR-10 and CIFAR-100,\nwithout introducing any additional hyper-parameters. We exploit the phenomenon\nof flat minima that has been shown to lead to improved generalization error for\ndeep networks. Parle requires very infrequent communication with the parameter\nserver and instead performs more computation on each client, which makes it\nwell-suited to both single-machine, multi-GPU settings and distributed\nimplementations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 07:14:56 GMT"}, {"version": "v2", "created": "Sun, 10 Sep 2017 04:22:49 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Chaudhari", "Pratik", ""], ["Baldassi", "Carlo", ""], ["Zecchina", "Riccardo", ""], ["Soatto", "Stefano", ""], ["Talwalkar", "Ameet", ""], ["Oberman", "Adam", ""]]}, {"id": "1707.00506", "submitter": "Ana Freire", "authors": "Akshay Kumar Chaturvedi, Filipa Peleja, Ana Freire", "title": "Recommender System for News Articles using Supervised Learning", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade we have observed a mass increase of information, in\nparticular information that is shared through smartphones. Consequently, the\namount of information that is available does not allow the average user to be\naware of all his options. In this context, recommender systems use a number of\ntechniques to help a user find the desired product. Hence, nowadays recommender\nsystems play an important role. Recommender Systems' aim to identify products\nthat best fits user preferences. These techniques are advantageous to both\nusers and vendors, as it enables the user to rapidly find what he needs and the\nvendors to promote their products and sales. As the industry became aware of\nthe gains that could be accomplished by using these algorithms, also a very\ninteresting problem for many researchers, recommender systems became a very\nactive area since the mid 90's. Having in mind that this is an ongoing problem\nthe present thesis intends to observe the value of using a recommender\nalgorithm to find users likes by observing her domain preferences. In a\nbalanced probabilistic method, this thesis will show how news topics can be\nused to recommend news articles. In this thesis, we used different machine\nlearning methods to determine the user ratings for an article. To tackle this\nproblem, supervised learning methods such as linear regression, Naive Bayes and\nlogistic regression are used. All the aforementioned models have a different\nnature which has an impact on the solution of the given problem. Furthermore,\nnumber of experiments are presented and discussed to identify the feature set\nthat fits best to the problem.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 12:27:17 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Chaturvedi", "Akshay Kumar", ""], ["Peleja", "Filipa", ""], ["Freire", "Ana", ""]]}, {"id": "1707.00524", "submitter": "Hayan Yin", "authors": "Haiyan Yin, Jianda Chen, Sinno Jialin Pan", "title": "Hashing over Predicted Future Frames for Informed Exploration of Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep reinforcement learning (RL) tasks, an efficient exploration mechanism\nshould be able to encourage an agent to take actions that lead to less frequent\nstates which may yield higher accumulative future return. However, both knowing\nabout the future and evaluating the frequentness of states are non-trivial\ntasks, especially for deep RL domains, where a state is represented by\nhigh-dimensional image frames. In this paper, we propose a novel informed\nexploration framework for deep RL, where we build the capability for an RL\nagent to predict over the future transitions and evaluate the frequentness for\nthe predicted future frames in a meaningful manner. To this end, we train a\ndeep prediction model to predict future frames given a state-action pair, and a\nconvolutional autoencoder model to hash over the seen frames. In addition, to\nutilize the counts derived from the seen frames to evaluate the frequentness\nfor the predicted frames, we tackle the challenge of matching the predicted\nfuture frames and their corresponding seen frames at the latent feature level.\nIn this way, we derive a reliable metric for evaluating the novelty of the\nfuture direction pointed by each action, and hence inform the agent to explore\nthe least frequent one.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 13:07:40 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 12:09:47 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Yin", "Haiyan", ""], ["Chen", "Jianda", ""], ["Pan", "Sinno Jialin", ""]]}, {"id": "1707.00536", "submitter": "Peng Yang", "authors": "Peng Yang, Peilin Zhao, Xin Gao, Yong Liu", "title": "Robust Cost-Sensitive Learning for Recommendation with Implicit Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation is the task of improving customer experience through\npersonalized recommendation based on users' past feedback. In this paper, we\ninvestigate the most common scenario: the user-item (U-I) matrix of implicit\nfeedback. Even though many recommendation approaches are designed based on\nimplicit feedback, they attempt to project the U-I matrix into a low-rank\nlatent space, which is a strict restriction that rarely holds in practice. In\naddition, although misclassification costs from imbalanced classes are\nsignificantly different, few methods take the cost of classification error into\naccount. To address aforementioned issues, we propose a robust framework by\ndecomposing the U-I matrix into two components: (1) a low-rank matrix that\ncaptures the common preference, and (2) a sparse matrix that detects the\nuser-specific preference of individuals. A cost-sensitive learning model is\nembedded into the framework. Specifically, this model exploits different costs\nin the loss function for the observed and unobserved instances. We show that\nthe resulting non-smooth convex objective can be optimized efficiently by an\naccelerated projected gradient method with closed-form solutions. Morever, the\nproposed algorithm can be scaled up to large-sized datasets after a relaxation.\nThe theoretical result shows that even with a small fraction of 1's in the U-I\nmatrix $M\\in\\mathbb{R}^{n\\times m}$, the cost-sensitive error of the proposed\nmodel is upper bounded by $O(\\frac{\\alpha}{\\sqrt{mn}})$, where $\\alpha$ is a\nbias over imbalanced classes. Finally, empirical experiments are extensively\ncarried out to evaluate the effectiveness of our proposed algorithm.\nEncouraging experimental results show that our algorithm outperforms several\nstate-of-the-art algorithms on benchmark recommendation datasets.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 13:27:56 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 07:02:49 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Yang", "Peng", ""], ["Zhao", "Peilin", ""], ["Gao", "Xin", ""], ["Liu", "Yong", ""]]}, {"id": "1707.00577", "submitter": "Junhong Lin", "authors": "Junhong Lin, Lorenzo Rosasco", "title": "Generalization Properties of Doubly Stochastic Learning Algorithms", "comments": "24 pages. To appear in Journal of Complexity", "journal-ref": null, "doi": "10.1016/j.jco.2018.02.004", "report-no": null, "categories": "stat.ML cs.LG math.FA math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Doubly stochastic learning algorithms are scalable kernel methods that\nperform very well in practice. However, their generalization properties are not\nwell understood and their analysis is challenging since the corresponding\nlearning sequence may not be in the hypothesis space induced by the kernel. In\nthis paper, we provide an in-depth theoretical analysis for different variants\nof doubly stochastic learning algorithms within the setting of nonparametric\nregression in a reproducing kernel Hilbert space and considering the square\nloss. Particularly, we derive convergence results on the generalization error\nfor the studied algorithms either with or without an explicit penalty term. To\nthe best of our knowledge, the derived results for the unregularized variants\nare the first of this kind, while the results for the regularized variants\nimprove those in the literature. The novelties in our proof are a sample error\nbound that requires controlling the trace norm of a cumulative operator, and a\nrefined analysis of bounding initial error.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 14:46:05 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 21:22:12 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Lin", "Junhong", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1707.00622", "submitter": "Morteza Ashraphijuo", "authors": "Morteza Ashraphijuo, Xiaodong Wang, Vaneet Aggarwal", "title": "Rank Determination for Low-Rank Data Completion", "comments": null, "journal-ref": "Journal of Machine Learning Research, 18(98), pp. 1-29, Sept 2017", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, fundamental conditions on the sampling patterns have been obtained\nfor finite completability of low-rank matrices or tensors given the\ncorresponding ranks. In this paper, we consider the scenario where the rank is\nnot given and we aim to approximate the unknown rank based on the location of\nsampled entries and some given completion. We consider a number of data models,\nincluding single-view matrix, multi-view matrix, CP tensor, tensor-train tensor\nand Tucker tensor. For each of these data models, we provide an upper bound on\nthe rank when an arbitrary low-rank completion is given. We characterize these\nbounds both deterministically, i.e., with probability one given that the\nsampling pattern satisfies certain combinatorial properties, and\nprobabilistically, i.e., with high probability given that the sampling\nprobability is above some threshold. Moreover, for both single-view matrix and\nCP tensor, we are able to show that the obtained upper bound is exactly equal\nto the unknown rank if the lowest-rank completion is given. Furthermore, we\nprovide numerical experiments for the case of single-view matrix, where we use\nnuclear norm minimization to find a low-rank completion of the sampled data and\nwe observe that in most of the cases the proposed upper bound on the rank is\nequal to the true rank.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 16:08:07 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Ashraphijuo", "Morteza", ""], ["Wang", "Xiaodong", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1707.00683", "submitter": "Florian Strub", "authors": "Harm de Vries, Florian Strub, J\\'er\\'emie Mary, Hugo Larochelle,\n  Olivier Pietquin, Aaron Courville", "title": "Modulating early visual processing by language", "comments": "Advances in Neural Information Processing Systems 30 (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly assumed that language refers to high-level visual concepts\nwhile leaving low-level visual processing unaffected. This view dominates the\ncurrent literature in computational models for language-vision tasks, where\nvisual and linguistic input are mostly processed independently before being\nfused into a single representation. In this paper, we deviate from this classic\npipeline and propose to modulate the \\emph{entire visual processing} by\nlinguistic input. Specifically, we condition the batch normalization parameters\nof a pretrained residual network (ResNet) on a language embedding. This\napproach, which we call MOdulated RESnet (\\MRN), significantly improves strong\nbaselines on two visual question answering tasks. Our ablation study shows that\nmodulating from the early stages of the visual processing is beneficial.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 04:06:01 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 02:58:44 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 20:04:53 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["de Vries", "Harm", ""], ["Strub", "Florian", ""], ["Mary", "J\u00e9r\u00e9mie", ""], ["Larochelle", "Hugo", ""], ["Pietquin", "Olivier", ""], ["Courville", "Aaron", ""]]}, {"id": "1707.00684", "submitter": "Tomoyoshi Shimobaba Dr.", "authors": "Tomoyoshi Shimobaba, Naoki Kuwata, Mizuha Homma, Takayuki Takahashi,\n  Yuki Nagahama, Marie Sano, Satoki Hasegawa, Ryuji Hirayama, Takashi Kakue,\n  Atsushi Shiraki, Naoki Takada, Tomoyoshi Ito", "title": "Deep-learning-based data page classification for holographic memory", "comments": null, "journal-ref": null, "doi": "10.1364/ao.56.007327", "report-no": null, "categories": "cs.CV cs.LG cs.NE physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep-learning-based classification of data pages used in\nholographic memory. We numerically investigated the classification performance\nof a conventional multi-layer perceptron (MLP) and a deep neural network, under\nthe condition that reconstructed page data are contaminated by some noise and\nare randomly laterally shifted. The MLP was found to have a classification\naccuracy of 91.58%, whereas the deep neural network was able to classify data\npages at an accuracy of 99.98%. The accuracy of the deep neural network is two\norders of magnitude better than the MLP.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 05:47:37 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Shimobaba", "Tomoyoshi", ""], ["Kuwata", "Naoki", ""], ["Homma", "Mizuha", ""], ["Takahashi", "Takayuki", ""], ["Nagahama", "Yuki", ""], ["Sano", "Marie", ""], ["Hasegawa", "Satoki", ""], ["Hirayama", "Ryuji", ""], ["Kakue", "Takashi", ""], ["Shiraki", "Atsushi", ""], ["Takada", "Naoki", ""], ["Ito", "Tomoyoshi", ""]]}, {"id": "1707.00703", "submitter": "Emmanuel Dufourq Mr", "authors": "Emmanuel Dufourq, Bruce A. Bassett", "title": "Automated Problem Identification: Regression vs Classification via\n  Evolutionary Deep Networks", "comments": "9 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Regression or classification? This is perhaps the most basic question faced\nwhen tackling a new supervised learning problem. We present an Evolutionary\nDeep Learning (EDL) algorithm that automatically solves this by identifying the\nquestion type with high accuracy, along with a proposed deep architecture.\nTypically, a significant amount of human insight and preparation is required\nprior to executing machine learning algorithms. For example, when creating deep\nneural networks, the number of parameters must be selected in advance and\nfurthermore, a lot of these choices are made based upon pre-existing knowledge\nof the data such as the use of a categorical cross entropy loss function.\nHumans are able to study a dataset and decide whether it represents a\nclassification or a regression problem, and consequently make decisions which\nwill be applied to the execution of the neural network. We propose the\nAutomated Problem Identification (API) algorithm, which uses an evolutionary\nalgorithm interface to TensorFlow to manipulate a deep neural network to decide\nif a dataset represents a classification or a regression problem. We test API\non 16 different classification, regression and sentiment analysis datasets with\nup to 10,000 features and up to 17,000 unique target values. API achieves an\naverage accuracy of $96.3\\%$ in identifying the problem type without hardcoding\nany insights about the general characteristics of regression or classification\nproblems. For example, API successfully identifies classification problems even\nwith 1000 target values. Furthermore, the algorithm recommends which loss\nfunction to use and also recommends a neural network architecture. Our work is\ntherefore a step towards fully automated machine learning.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 18:00:08 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Dufourq", "Emmanuel", ""], ["Bassett", "Bruce A.", ""]]}, {"id": "1707.00724", "submitter": "Daniel Brown", "authors": "Daniel S. Brown and Scott Niekum", "title": "Efficient Probabilistic Performance Bounds for Inverse Reinforcement\n  Learning", "comments": "In proceedings AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of reinforcement learning there has been recent progress towards\nsafety and high-confidence bounds on policy performance. However, to our\nknowledge, no practical methods exist for determining high-confidence policy\nperformance bounds in the inverse reinforcement learning setting---where the\ntrue reward function is unknown and only samples of expert behavior are given.\nWe propose a sampling method based on Bayesian inverse reinforcement learning\nthat uses demonstrations to determine practical high-confidence upper bounds on\nthe $\\alpha$-worst-case difference in expected return between any evaluation\npolicy and the optimal policy under the expert's unknown reward function. We\nevaluate our proposed bound on both a standard grid navigation task and a\nsimulated driving task and achieve tighter and more accurate bounds than a\nfeature count-based baseline. We also give examples of how our proposed bound\ncan be utilized to perform risk-aware policy selection and risk-aware policy\nimprovement. Because our proposed bound requires several orders of magnitude\nfewer demonstrations than existing high-confidence bounds, it is the first\npractical method that allows agents that learn from demonstration to express\nconfidence in the quality of their learned policy.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 18:40:13 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 19:00:28 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 23:28:40 GMT"}, {"version": "v4", "created": "Tue, 21 Nov 2017 21:27:17 GMT"}, {"version": "v5", "created": "Sat, 23 Jun 2018 02:11:52 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Brown", "Daniel S.", ""], ["Niekum", "Scott", ""]]}, {"id": "1707.00762", "submitter": "Bart van Merri\\\"enboer", "authors": "Bart van Merri\\\"enboer, Amartya Sanyal, Hugo Larochelle and Yoshua\n  Bengio", "title": "Multiscale sequence modeling with a learned dictionary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generalization of neural network sequence models. Instead of\npredicting one symbol at a time, our multi-scale model makes predictions over\nmultiple, potentially overlapping multi-symbol tokens. A variation of the\nbyte-pair encoding (BPE) compression algorithm is used to learn the dictionary\nof tokens that the model is trained with. When applied to language modelling,\nour model has the flexibility of character-level models while maintaining many\nof the performance benefits of word-level models. Our experiments show that\nthis model performs better than a regular LSTM on language modeling tasks,\nespecially for smaller models.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 21:16:49 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 14:45:00 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["van Merri\u00ebnboer", "Bart", ""], ["Sanyal", "Amartya", ""], ["Larochelle", "Hugo", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1707.00768", "submitter": "Alexander B. Jung", "authors": "Alexander B. Jung", "title": "Learning to Avoid Errors in GANs by Manipulating Input Spaces", "comments": "9 pages, 6 figures (plus 21 pages Appendix), code available at:\n  https://github.com/aleju/gan-error-avoidance", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances, large scale visual artifacts are still a common\noccurrence in images generated by GANs. Previous work has focused on improving\nthe generator's capability to accurately imitate the data distribution\n$p_{data}$. In this paper, we instead explore methods that enable GANs to\nactively avoid errors by manipulating the input space. The core idea is to\napply small changes to each noise vector in order to shift them away from areas\nin the input space that tend to result in errors. We derive three different\narchitectures from that idea. The main one of these consists of a simple\nresidual module that leads to significantly less visual artifacts, while only\nslightly decreasing diversity. The module is trivial to add to existing GANs\nand costs almost zero computation and memory.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 21:58:23 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Jung", "Alexander B.", ""]]}, {"id": "1707.00780", "submitter": "Chao Lan", "authors": "Chao Lan and Jun Huan", "title": "Discriminatory Transfer", "comments": "Presented as a poster at the 2017 Workshop on Fairness,\n  Accountability, and Transparency in Machine Learning (FAT/ML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe standard transfer learning can improve prediction accuracies of\ntarget tasks at the cost of lowering their prediction fairness -- a phenomenon\nwe named discriminatory transfer. We examine prediction fairness of a standard\nhypothesis transfer algorithm and a standard multi-task learning algorithm, and\nshow they both suffer discriminatory transfer on the real-world Communities and\nCrime data set. The presented case study introduces an interaction between\nfairness and transfer learning, as an extension of existing fairness studies\nthat focus on single task learning.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 23:20:39 GMT"}, {"version": "v2", "created": "Sat, 8 Jul 2017 15:02:09 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 15:29:17 GMT"}, {"version": "v4", "created": "Mon, 7 Aug 2017 21:00:19 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Lan", "Chao", ""], ["Huan", "Jun", ""]]}, {"id": "1707.00783", "submitter": "Jonathan Wells", "authors": "Jonathan R. Wells and Kai Ming Ting", "title": "A simple efficient density estimator that enables fast systematic search", "comments": "Corrected typos in the reference section and added an acknowledgement\n  on the first page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a simple and efficient density estimator that enables\nfast systematic search. To show its advantage over commonly used kernel density\nestimator, we apply it to outlying aspects mining. Outlying aspects mining\ndiscovers feature subsets (or subspaces) that describe how a query stand out\nfrom a given dataset. The task demands a systematic search of subspaces. We\nidentify that existing outlying aspects miners are restricted to datasets with\nsmall data size and dimensions because they employ kernel density estimator,\nwhich is computationally expensive, for subspace assessments. We show that a\nrecent outlying aspects miner can run orders of magnitude faster by simply\nreplacing its density estimator with the proposed density estimator, enabling\nit to deal with large datasets with thousands of dimensions that would\notherwise be impossible.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 23:42:46 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 05:23:11 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Wells", "Jonathan R.", ""], ["Ting", "Kai Ming", ""]]}, {"id": "1707.00784", "submitter": "Kelli Humbird", "authors": "K. D. Humbird, J. L. Peterson, R. G. McClarren", "title": "Deep neural network initialization with decision trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work a novel, automated process for constructing and initializing\ndeep feed-forward neural networks based on decision trees is presented. The\nproposed algorithm maps a collection of decision trees trained on the data into\na collection of initialized neural networks, with the structures of the\nnetworks determined by the structures of the trees. The tree-informed\ninitialization acts as a warm-start to the neural network training process,\nresulting in efficiently trained, accurate networks. These models, referred to\nas \"deep jointly-informed neural networks\" (DJINN), demonstrate high predictive\nperformance for a variety of regression and classification datasets, and\ndisplay comparable performance to Bayesian hyper-parameter optimization at a\nlower computational cost. By combining the user-friendly features of decision\ntree models with the flexibility and scalability of deep neural networks, DJINN\nis an attractive algorithm for training predictive models on a wide range of\ncomplex datasets.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 23:45:54 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2018 00:45:26 GMT"}, {"version": "v3", "created": "Tue, 3 Jul 2018 03:53:39 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Humbird", "K. D.", ""], ["Peterson", "J. L.", ""], ["McClarren", "R. G.", ""]]}, {"id": "1707.00797", "submitter": "Dilin Wang", "authors": "Qiang Liu, Dilin Wang", "title": "Learning Deep Energy Models: Contrastive Divergence vs. Amortized MLE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a number of new algorithms for learning deep energy models and\ndemonstrate their properties. We show that our SteinCD performs well in term of\ntest likelihood, while SteinGAN performs well in terms of generating realistic\nlooking images. Our results suggest promising directions for learning better\nmodels by combining GAN-style methods with traditional energy-based learning.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 02:05:52 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Liu", "Qiang", ""], ["Wang", "Dilin", ""]]}, {"id": "1707.00802", "submitter": "Liu Xun", "authors": "Xun Liu, Wei Xue, Lei Xiao, Bo Zhang", "title": "PBODL : Parallel Bayesian Online Deep Learning for Click-Through Rate\n  Prediction in Tencent Advertising System", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a parallel bayesian online deep learning framework (PBODL) for\nclick-through rate (CTR) prediction within today's Tencent advertising system,\nwhich provides quick and accurate learning of user preferences. We first\nexplain the framework with a deep probit regression model, which is trained\nwith probabilistic back-propagation in the mode of assumed Gaussian density\nfiltering. Then we extend the model family to a variety of bayesian online\nmodels with increasing feature embedding capabilities, such as Sparse-MLP,\nFM-MLP and FFM-MLP. Finally, we implement a parallel training system based on a\nstream computing infrastructure and parameter servers. Experiments with public\navailable datasets and Tencent industrial datasets show that models within our\nframework perform better than several common online models, such as\nAdPredictor, FTRL-Proximal and MatchBox. Online A/B test within Tencent\nadvertising system further proves that our framework could achieve CTR and CPM\nlift by learning more quickly and accurately.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 02:40:41 GMT"}, {"version": "v2", "created": "Sun, 9 Jul 2017 08:42:32 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Liu", "Xun", ""], ["Xue", "Wei", ""], ["Xiao", "Lei", ""], ["Zhang", "Bo", ""]]}, {"id": "1707.00819", "submitter": "Sebastian Weichwald", "authors": "Paul K. Rubenstein, Sebastian Weichwald, Stephan Bongers, Joris M.\n  Mooij, Dominik Janzing, Moritz Grosse-Wentrup, Bernhard Sch\\\"olkopf", "title": "Causal Consistency of Structural Equation Models", "comments": "equal contribution between Rubenstein and Weichwald; accepted\n  manuscript", "journal-ref": "Proceedings of the Annual Conference on Uncertainty in Artificial\n  Intelligence, UAI 2017 ( http://auai.org/uai2017/proceedings/papers/11.pdf )", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex systems can be modelled at various levels of detail. Ideally, causal\nmodels of the same system should be consistent with one another in the sense\nthat they agree in their predictions of the effects of interventions. We\nformalise this notion of consistency in the case of Structural Equation Models\n(SEMs) by introducing exact transformations between SEMs. This provides a\ngeneral language to consider, for instance, the different levels of description\nin the following three scenarios: (a) models with large numbers of variables\nversus models in which the `irrelevant' or unobservable variables have been\nmarginalised out; (b) micro-level models versus macro-level models in which the\nmacro-variables are aggregate features of the micro-variables; (c) dynamical\ntime series models versus models of their stationary behaviour. Our analysis\nstresses the importance of well specified interventions in the causal modelling\nprocess and sheds light on the interpretation of cyclic SEMs.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 06:05:31 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Rubenstein", "Paul K.", ""], ["Weichwald", "Sebastian", ""], ["Bongers", "Stephan", ""], ["Mooij", "Joris M.", ""], ["Janzing", "Dominik", ""], ["Grosse-Wentrup", "Moritz", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1707.00821", "submitter": "Nader Bshouty", "authors": "Nader H. Bshouty and Catherine A. Haddad-Zaknoon", "title": "The Maximum Cosine Framework for Deriving Perceptron Based Linear\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a mathematical framework, called the Maximum\nCosine Framework or MCF, for deriving new linear classifiers. The method is\nbased on selecting an appropriate bound on the cosine of the angle between the\ntarget function and the algorithm's. To justify its correctness, we use the MCF\nto show how to regenerate the update rule of Aggressive ROMMA. Moreover, we\nconstruct a cosine bound from which we build the Maximum Cosine Perceptron\nalgorithm or, for short, the MCP algorithm. We prove that the MCP shares the\nsame mistake bound like the Perceptron. In addition, we demonstrate the\npromising performance of the MCP on a real dataset. Our experiments show that,\nunder the restriction of single pass learning, the MCP algorithm outperforms PA\nand Aggressive ROMMA.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 06:13:13 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Bshouty", "Nader H.", ""], ["Haddad-Zaknoon", "Catherine A.", ""]]}, {"id": "1707.00860", "submitter": "Sakyasingha Dasgupta", "authors": "Subhajit Chaudhury, Sakyasingha Dasgupta, Asim Munawar, Md. A. Salam\n  Khan, Ryuki Tachibana", "title": "Conditional generation of multi-modal data using constrained embedding\n  space mapping", "comments": "7 pages, 4 figures, ICML 2017 Workshop on Implicit Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a conditional generative model that maps low-dimensional\nembeddings of multiple modalities of data to a common latent space hence\nextracting semantic relationships between them. The embedding specific to a\nmodality is first extracted and subsequently a constrained optimization\nprocedure is performed to project the two embedding spaces to a common\nmanifold. The individual embeddings are generated back from this common latent\nspace. However, in order to enable independent conditional inference for\nseparately extracting the corresponding embeddings from the common latent space\nrepresentation, we deploy a proxy variable trick - wherein, the single shared\nlatent space is replaced by the respective separate latent spaces of each\nmodality. We design an objective function, such that, during training we can\nforce these separate spaces to lie close to each other, by minimizing the\ndistance between their probability distribution functions. Experimental results\ndemonstrate that the learned joint model can generalize to learning concepts of\ndouble MNIST digits with additional attributes of colors,from both textual and\nspeech input.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 09:00:38 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 00:51:04 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Chaudhury", "Subhajit", ""], ["Dasgupta", "Sakyasingha", ""], ["Munawar", "Asim", ""], ["Khan", "Md. A. Salam", ""], ["Tachibana", "Ryuki", ""]]}, {"id": "1707.00893", "submitter": "Philipp Jund", "authors": "Philipp Jund, Andreas Eitel, Nichola Abdo, Wolfram Burgard", "title": "Optimization Beyond the Convolution: Generalizing Spatial Relations with\n  End-to-End Metric Learning", "comments": "Accepted for publication at ICRA2018. Supplementary Video:\n  http://spatialrelations.cs.uni-freiburg.de/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To operate intelligently in domestic environments, robots require the ability\nto understand arbitrary spatial relations between objects and to generalize\nthem to objects of varying sizes and shapes. In this work, we present a novel\nend-to-end approach to generalize spatial relations based on distance metric\nlearning. We train a neural network to transform 3D point clouds of objects to\na metric space that captures the similarity of the depicted spatial relations,\nusing only geometric models of the objects. Our approach employs gradient-based\noptimization to compute object poses in order to imitate an arbitrary target\nrelation by reducing the distance to it under the learned metric. Our results\nbased on simulated and real-world experiments show that the proposed method\nenables robots to generalize spatial relations to unknown objects over a\ncontinuous spectrum.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 10:19:34 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 09:30:21 GMT"}, {"version": "v3", "created": "Sat, 24 Mar 2018 14:13:14 GMT"}, {"version": "v4", "created": "Mon, 1 Jul 2019 13:44:40 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Jund", "Philipp", ""], ["Eitel", "Andreas", ""], ["Abdo", "Nichola", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1707.00948", "submitter": "Anisa Allahdadi", "authors": "Anisa Allahdadi, Ricardo Morla", "title": "Anomaly Detection and Modeling in 802.11 Wireless Networks", "comments": null, "journal-ref": null, "doi": "10.1007/s10922-018-9455-2", "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IEEE 802.11 Wireless Networks are getting more and more popular at university\ncampuses, enterprises, shopping centers, airports and in so many other public\nplaces, providing Internet access to a large crowd openly and quickly. The\nwireless users are also getting more dependent on WiFi technology and therefore\ndemanding more reliability and higher performance for this vital technology.\nHowever, due to unstable radio conditions, faulty equipment, and dynamic user\nbehavior among other reasons, there are always unpredictable performance\nproblems in a wireless covered area. Detection and prediction of such problems\nis of great significance to network managers if they are to alleviate the\nconnectivity issues of the mobile users and provide a higher quality wireless\nservice. This paper aims to improve the management of the 802.11 wireless\nnetworks by characterizing and modeling wireless usage patterns in a set of\nanomalous scenarios that can occur in such networks. We apply time-invariant\n(Gaussian Mixture Models) and time-variant (Hidden Markov Models) modeling\napproaches to a dataset generated from a large production network and describe\nhow we use these models for anomaly detection. We then generate several common\nanomalies on a Testbed network and evaluate the proposed anomaly detection\nmethodologies in a controlled environment. The experimental results of the\nTestbed show that HMM outperforms GMM and yields a higher anomaly detection\nratio and a lower false alarm rate.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 12:49:41 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Allahdadi", "Anisa", ""], ["Morla", "Ricardo", ""]]}, {"id": "1707.01047", "submitter": "Vasilis Syrgkanis", "authors": "Robert Chen, Brendan Lucier, Yaron Singer, Vasilis Syrgkanis", "title": "Robust Optimization for Non-Convex Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider robust optimization problems, where the goal is to optimize in\nthe worst case over a class of objective functions. We develop a reduction from\nrobust improper optimization to Bayesian optimization: given an oracle that\nreturns $\\alpha$-approximate solutions for distributions over objectives, we\ncompute a distribution over solutions that is $\\alpha$-approximate in the worst\ncase. We show that de-randomizing this solution is NP-hard in general, but can\nbe done for a broad class of statistical learning tasks. We apply our results\nto robust neural network training and submodular optimization. We evaluate our\napproach experimentally on corrupted character classification, and robust\ninfluence maximization in networks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 15:56:42 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Chen", "Robert", ""], ["Lucier", "Brendan", ""], ["Singer", "Yaron", ""], ["Syrgkanis", "Vasilis", ""]]}, {"id": "1707.01069", "submitter": "Robert Bamler", "authors": "Robert Bamler and Stephan Mandt", "title": "Structured Black Box Variational Inference for Latent Time Series Models", "comments": "5 pages, 1 figure; presented at the ICML 2017 Time Series Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous latent time series models are prevalent in Bayesian modeling;\nexamples include the Kalman filter, dynamic collaborative filtering, or dynamic\ntopic models. These models often benefit from structured, non mean field\nvariational approximations that capture correlations between time steps. Black\nbox variational inference with reparameterization gradients (BBVI) allows us to\nexplore a rich new class of Bayesian non-conjugate latent time series models;\nhowever, a naive application of BBVI to such structured variational models\nwould scale quadratically in the number of time steps. We describe a BBVI\nalgorithm analogous to the forward-backward algorithm which instead scales\nlinearly in time. It allows us to efficiently sample from the variational\ndistribution and estimate the gradients of the ELBO. Finally, we show results\non the recently proposed dynamic word embedding model, which was trained using\nour method.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 17:03:59 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Bamler", "Robert", ""], ["Mandt", "Stephan", ""]]}, {"id": "1707.01093", "submitter": "Ofir Lindenbaum", "authors": "Ofir Lindenbaum and Moshe Salhov and Arie Yeredor and Amir Averbuch", "title": "Kernel Scaling for Manifold Learning and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Kernel methods play a critical role in many machine learning algorithms. They\nare useful in manifold learning, classification, clustering and other data\nanalysis tasks. Setting the kernel's scale parameter, also referred to as the\nkernel's bandwidth, highly affects the performance of the task in hand. We\npropose to set a scale parameter that is tailored to one of two types of tasks:\nclassification and manifold learning. For manifold learning, we seek a scale\nwhich is best at capturing the manifold's intrinsic dimension. For\nclassification, we propose three methods for estimating the scale, which\noptimize the classification results in different senses. The proposed\nframeworks are simulated on artificial and on real datasets. The results show a\nhigh correlation between optimal classification rates and the estimated scales.\nFinally, we demonstrate the approach on a seismic event classification task.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 15:30:31 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 02:41:46 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Lindenbaum", "Ofir", ""], ["Salhov", "Moshe", ""], ["Yeredor", "Arie", ""], ["Averbuch", "Amir", ""]]}, {"id": "1707.01155", "submitter": "Jakub Kone\\v{c}n\\'y", "authors": "Jakub Kone\\v{c}n\\'y", "title": "Stochastic, Distributed and Federated Optimization for Machine Learning", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study optimization algorithms for the finite sum problems frequently\narising in machine learning applications. First, we propose novel variants of\nstochastic gradient descent with a variance reduction property that enables\nlinear convergence for strongly convex objectives. Second, we study distributed\nsetting, in which the data describing the optimization problem does not fit\ninto a single computing node. In this case, traditional methods are\ninefficient, as the communication costs inherent in distributed optimization\nbecome the bottleneck. We propose a communication-efficient framework which\niteratively forms local subproblems that can be solved with arbitrary local\noptimization algorithms. Finally, we introduce the concept of Federated\nOptimization/Learning, where we try to solve the machine learning problems\nwithout having data stored in any centralized manner. The main motivation comes\nfrom industry when handling user-generated data. The current prevalent practice\nis that companies collect vast amounts of user data and store them in\ndatacenters. An alternative we propose is not to collect the data in first\nplace, and instead occasionally use the computational power of users' devices\nto solve the very same optimization problems, while alleviating privacy\nconcerns at the same time. In such setting, minimization of communication\nrounds is the primary goal, and we demonstrate that solving the optimization\nproblems in such circumstances is conceptually tractable.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 21:15:14 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Kone\u010dn\u00fd", "Jakub", ""]]}, {"id": "1707.01164", "submitter": "Jianbo Chen", "authors": "Jianbo Chen, Mitchell Stern, Martin J. Wainwright, Michael I. Jordan", "title": "Kernel Feature Selection via Conditional Covariance Minimization", "comments": "The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for feature selection that employs kernel-based measures\nof independence to find a subset of covariates that is maximally predictive of\nthe response. Building on past work in kernel dimension reduction, we show how\nto perform feature selection via a constrained optimization problem involving\nthe trace of the conditional covariance operator. We prove various consistency\nresults for this procedure, and also demonstrate that our method compares\nfavorably with other state-of-the-art algorithms on a variety of synthetic and\nreal data sets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 22:00:58 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 19:34:49 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Chen", "Jianbo", ""], ["Stern", "Mitchell", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1707.01166", "submitter": "Jun Qi", "authors": "Jun Qi, Xu Liu, Javier Tejedor and Shunsuke Kamijo", "title": "Unsupervised Submodular Rank Aggregation on Score-based Permutations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised rank aggregation on score-based permutations, which is widely\nused in many applications, has not been deeply explored yet. This work studies\nthe use of submodular optimization for rank aggregation on score-based\npermutations in an unsupervised way. Specifically, we propose an unsupervised\napproach based on the Lovasz Bregman divergence for setting up linear\nstructured convex and nested structured concave objective functions. In\naddition, stochastic optimization methods are applied in the training process\nand efficient algorithms for inference can be guaranteed. The experimental\nresults from Information Retrieval, Combining Distributed Neural Networks,\nInfluencers in Social Networks, and Distributed Automatic Speech Recognition\ntasks demonstrate the effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 22:21:38 GMT"}, {"version": "v2", "created": "Fri, 1 Sep 2017 03:49:19 GMT"}, {"version": "v3", "created": "Wed, 6 Sep 2017 20:57:59 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Qi", "Jun", ""], ["Liu", "Xu", ""], ["Tejedor", "Javier", ""], ["Kamijo", "Shunsuke", ""]]}, {"id": "1707.01209", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "Model compression as constrained optimization, with application to\n  neural nets. Part I: general framework", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressing neural nets is an active research problem, given the large size\nof state-of-the-art nets for tasks such as object recognition, and the\ncomputational limits imposed by mobile devices. We give a general formulation\nof model compression as constrained optimization. This includes many types of\ncompression: quantization, low-rank decomposition, pruning, lossless\ncompression and others. Then, we give a general algorithm to optimize this\nnonconvex problem based on the augmented Lagrangian and alternating\noptimization. This results in a \"learning-compression\" algorithm, which\nalternates a learning step of the uncompressed model, independent of the\ncompression type, with a compression step of the model parameters, independent\nof the learning task. This simple, efficient algorithm is guaranteed to find\nthe best compressed model for the task in a local sense under standard\nassumptions.\n  We present separately in several companion papers the development of this\ngeneral framework into specific algorithms for model compression based on\nquantization, pruning and other variations, including experimental results on\ncompressing neural nets and other models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 04:36:26 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1707.01212", "submitter": "Karthik Gurumoorthy", "authors": "Karthik S. Gurumoorthy, Amit Dhurandhar, Guillermo Cecchi, and Charu\n  Aggarwal", "title": "Efficient Data Representation by Selecting Prototypes with Importance\n  Weights", "comments": "Accepted for publication in International Conference on Data Mining\n  (ICDM) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prototypical examples that best summarizes and compactly represents an\nunderlying complex data distribution communicate meaningful insights to humans\nin domains where simple explanations are hard to extract. In this paper we\npresent algorithms with strong theoretical guarantees to mine these data sets\nand select prototypes a.k.a. representatives that optimally describes them. Our\nwork notably generalizes the recent work by Kim et al. (2016) where in addition\nto selecting prototypes, we also associate non-negative weights which are\nindicative of their importance. This extension provides a single coherent\nframework under which both prototypes and criticisms (i.e. outliers) can be\nfound. Furthermore, our framework works for any symmetric positive definite\nkernel thus addressing one of the key open questions laid out in Kim et al.\n(2016). By establishing that our objective function enjoys a key property of\nthat of weak submodularity, we present a fast ProtoDash algorithm and also\nderive approximation guarantees for the same. We demonstrate the efficacy of\nour method on diverse domains such as retail, digit recognition (MNIST) and on\npublicly available 40 health questionnaires obtained from the Center for\nDisease Control (CDC) website maintained by the US Dept. of Health. We validate\nthe results quantitatively as well as qualitatively based on expert feedback\nand recently published scientific studies on public health, thus showcasing the\npower of our technique in providing actionability (for retail), utility (for\nMNIST) and insight (on CDC datasets) which arguably are the hallmarks of an\neffective data mining method.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 05:17:10 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 15:12:08 GMT"}, {"version": "v3", "created": "Sat, 3 Feb 2018 10:10:45 GMT"}, {"version": "v4", "created": "Mon, 12 Aug 2019 05:35:59 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Gurumoorthy", "Karthik S.", ""], ["Dhurandhar", "Amit", ""], ["Cecchi", "Guillermo", ""], ["Aggarwal", "Charu", ""]]}, {"id": "1707.01213", "submitter": "Naiyan Wang", "authors": "Zehao Huang, Naiyan Wang", "title": "Data-Driven Sparse Structure Selection for Deep Neural Networks", "comments": "ECCV Camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have liberated its extraordinary power on\nvarious tasks. However, it is still very challenging to deploy state-of-the-art\nmodels into real-world applications due to their high computational complexity.\nHow can we design a compact and effective network without massive experiments\nand expert knowledge? In this paper, we propose a simple and effective\nframework to learn and prune deep models in an end-to-end manner. In our\nframework, a new type of parameter -- scaling factor is first introduced to\nscale the outputs of specific structures, such as neurons, groups or residual\nblocks. Then we add sparsity regularizations on these factors, and solve this\noptimization problem by a modified stochastic Accelerated Proximal Gradient\n(APG) method. By forcing some of the factors to zero, we can safely remove the\ncorresponding structures, thus prune the unimportant parts of a CNN. Comparing\nwith other structure selection methods that may need thousands of trials or\niterative fine-tuning, our method is trained fully end-to-end in one training\npass without bells and whistles. We evaluate our method, Sparse Structure\nSelection with several state-of-the-art CNNs, and demonstrate very promising\nresults with adaptive depth and width selection.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 05:21:50 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 22:38:02 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 05:14:37 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Huang", "Zehao", ""], ["Wang", "Naiyan", ""]]}, {"id": "1707.01217", "submitter": "Jian Shen", "authors": "Jian Shen, Yanru Qu, Weinan Zhang, Yong Yu", "title": "Wasserstein Distance Guided Representation Learning for Domain\n  Adaptation", "comments": "The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation aims at generalizing a high-performance learner on a target\ndomain via utilizing the knowledge distilled from a source domain which has a\ndifferent but related data distribution. One solution to domain adaptation is\nto learn domain invariant feature representations while the learned\nrepresentations should also be discriminative in prediction. To learn such\nrepresentations, domain adaptation frameworks usually include a domain\ninvariant representation learning approach to measure and reduce the domain\ndiscrepancy, as well as a discriminator for classification. Inspired by\nWasserstein GAN, in this paper we propose a novel approach to learn domain\ninvariant feature representations, namely Wasserstein Distance Guided\nRepresentation Learning (WDGRL). WDGRL utilizes a neural network, denoted by\nthe domain critic, to estimate empirical Wasserstein distance between the\nsource and target samples and optimizes the feature extractor network to\nminimize the estimated Wasserstein distance in an adversarial manner. The\ntheoretical advantages of Wasserstein distance for domain adaptation lie in its\ngradient property and promising generalization bound. Empirical studies on\ncommon sentiment and image classification adaptation datasets demonstrate that\nour proposed WDGRL outperforms the state-of-the-art domain invariant\nrepresentation learning approaches.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 05:34:13 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 05:39:24 GMT"}, {"version": "v3", "created": "Tue, 23 Jan 2018 12:15:15 GMT"}, {"version": "v4", "created": "Fri, 9 Mar 2018 07:20:13 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Shen", "Jian", ""], ["Qu", "Yanru", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""]]}, {"id": "1707.01219", "submitter": "Naiyan Wang", "authors": "Zehao Huang, Naiyan Wang", "title": "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite deep neural networks have demonstrated extraordinary power in various\napplications, their superior performances are at expense of high storage and\ncomputational costs. Consequently, the acceleration and compression of neural\nnetworks have attracted much attention recently. Knowledge Transfer (KT), which\naims at training a smaller student network by transferring knowledge from a\nlarger teacher model, is one of the popular solutions. In this paper, we\npropose a novel knowledge transfer method by treating it as a distribution\nmatching problem. Particularly, we match the distributions of neuron\nselectivity patterns between teacher and student networks. To achieve this\ngoal, we devise a new KT loss function by minimizing the Maximum Mean\nDiscrepancy (MMD) metric between these distributions. Combined with the\noriginal loss function, our method can significantly improve the performance of\nstudent networks. We validate the effectiveness of our method across several\ndatasets, and further combine it with other KT methods to explore the best\npossible results. Last but not least, we fine-tune the model to other tasks\nsuch as object detection. The results are also encouraging, which confirm the\ntransferability of the learned features.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 05:44:02 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 22:35:22 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Huang", "Zehao", ""], ["Wang", "Naiyan", ""]]}, {"id": "1707.01220", "submitter": "Naiyan Wang", "authors": "Yuntao Chen, Naiyan Wang, Zhaoxiang Zhang", "title": "DarkRank: Accelerating Deep Metric Learning via Cross Sample\n  Similarities Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have witnessed rapid evolution of deep neural network architecture design\nin the past years. These latest progresses greatly facilitate the developments\nin various areas such as computer vision and natural language processing.\nHowever, along with the extraordinary performance, these state-of-the-art\nmodels also bring in expensive computational cost. Directly deploying these\nmodels into applications with real-time requirement is still infeasible.\nRecently, Hinton etal. have shown that the dark knowledge within a powerful\nteacher model can significantly help the training of a smaller and faster\nstudent network. These knowledge are vastly beneficial to improve the\ngeneralization ability of the student model. Inspired by their work, we\nintroduce a new type of knowledge -- cross sample similarities for model\ncompression and acceleration. This knowledge can be naturally derived from deep\nmetric learning model. To transfer them, we bring the \"learning to rank\"\ntechnique into deep metric learning formulation. We test our proposed DarkRank\nmethod on various metric learning tasks including pedestrian re-identification,\nimage retrieval and image clustering. The results are quite encouraging. Our\nmethod can improve over the baseline method by a large margin. Moreover, it is\nfully compatible with other existing methods. When combined, the performance\ncan be further boosted.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 05:47:11 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 22:44:44 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Chen", "Yuntao", ""], ["Wang", "Naiyan", ""], ["Zhang", "Zhaoxiang", ""]]}, {"id": "1707.01242", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart", "title": "Learning Geometric Concepts with Nasty Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the efficient learnability of geometric concept classes -\nspecifically, low-degree polynomial threshold functions (PTFs) and\nintersections of halfspaces - when a fraction of the data is adversarially\ncorrupted. We give the first polynomial-time PAC learning algorithms for these\nconcept classes with dimension-independent error guarantees in the presence of\nnasty noise under the Gaussian distribution. In the nasty noise model, an\nomniscient adversary can arbitrarily corrupt a small fraction of both the\nunlabeled data points and their labels. This model generalizes well-studied\nnoise models, including the malicious noise model and the agnostic (adversarial\nlabel noise) model. Prior to our work, the only concept class for which\nefficient malicious learning algorithms were known was the class of\norigin-centered halfspaces.\n  Specifically, our robust learning algorithm for low-degree PTFs succeeds\nunder a number of tame distributions -- including the Gaussian distribution\nand, more generally, any log-concave distribution with (approximately) known\nlow-degree moments. For LTFs under the Gaussian distribution, we give a\npolynomial-time algorithm that achieves error $O(\\epsilon)$, where $\\epsilon$\nis the noise rate. At the core of our PAC learning results is an efficient\nalgorithm to approximate the low-degree Chow-parameters of any bounded function\nin the presence of nasty noise. To achieve this, we employ an iterative\nspectral method for outlier detection and removal, inspired by recent work in\nrobust unsupervised learning. Our aforementioned algorithm succeeds for a range\nof distributions satisfying mild concentration bounds and moment assumptions.\nThe correctness of our robust learning algorithm for intersections of\nhalfspaces makes essential use of a novel robust inverse independence lemma\nthat may be of broader interest.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 07:41:40 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1707.01250", "submitter": "Shlomo Berkovsky", "authors": "Amit Tiroshi, Tsvi Kuflik, Shlomo Berkovsky, Mohamed Ali Kaafar", "title": "Graph Based Recommendations: From Data Representation to Feature\n  Extraction and Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling users for the purpose of identifying their preferences and then\npersonalizing services on the basis of these models is a complex task,\nprimarily due to the need to take into consideration various explicit and\nimplicit signals, missing or uncertain information, contextual aspects, and\nmore. In this study, a novel generic approach for uncovering latent preference\npatterns from user data is proposed and evaluated. The approach relies on\nrepresenting the data using graphs, and then systematically extracting\ngraph-based features and using them to enrich the original user models. The\nextracted features encapsulate complex relationships between users, items, and\nmetadata. The enhanced user models can then serve as an input to any\nrecommendation algorithm. The proposed approach is domain-independent\n(demonstrated on data from movies, music, and business recommender systems),\nand is evaluated using several state-of-the-art machine learning methods, on\ndifferent recommendation tasks, and using different evaluation metrics. The\nresults show a unanimous improvement in the recommendation accuracy across\ntasks and domains. In addition, the evaluation provides a deeper analysis\nregarding the performance of the approach in special scenarios, including high\nsparsity and variability of ratings.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 08:08:21 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Tiroshi", "Amit", ""], ["Kuflik", "Tsvi", ""], ["Berkovsky", "Shlomo", ""], ["Kaafar", "Mohamed Ali", ""]]}, {"id": "1707.01322", "submitter": "Elizabeth Polgreen", "authors": "Elizabeth Polgreen, Viraj Wijesuriya, Sofie Haesaert and Alessandro\n  Abate", "title": "Automated Experiment Design for Data-Efficient Verification of\n  Parametric Markov Decision Processes", "comments": "QEST 2017, 18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.LO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for statistical verification of quantitative\nproperties over a partially unknown system with actions, utilising a\nparameterised model (in this work, a parametric Markov decision process) and\ndata collected from experiments performed on the underlying system. We obtain\nthe confidence that the underlying system satisfies a given property, and show\nthat the method uses data efficiently and thus is robust to the amount of data\navailable. These characteristics are achieved by firstly exploiting parameter\nsynthesis to establish a feasible set of parameters for which the underlying\nsystem will satisfy the property; secondly, by actively synthesising\nexperiments to increase amount of information in the collected data that is\nrelevant to the property; and finally propagating this information over the\nmodel parameters, obtaining a confidence that reflects our belief whether or\nnot the system parameters lie in the feasible set, thereby solving the\nverification problem.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 11:12:15 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Polgreen", "Elizabeth", ""], ["Wijesuriya", "Viraj", ""], ["Haesaert", "Sofie", ""], ["Abate", "Alessandro", ""]]}, {"id": "1707.01357", "submitter": "Stefan Lattner", "authors": "Stefan Lattner and Maarten Grachten", "title": "Improving Content-Invariance in Gated Autoencoders for 2D and 3D Object\n  Rotation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-invariance in mapping codes learned by GAEs is a useful feature for\nvarious relation learning tasks. In this paper we show that the\ncontent-invariance of mapping codes for images of 2D and 3D rotated objects can\nbe substantially improved by extending the standard GAE loss (symmetric\nreconstruction error) with a regularization term that penalizes the symmetric\ncross-reconstruction error. This error term involves reconstruction of pairs\nwith mapping codes obtained from other pairs exhibiting similar\ntransformations. Although this would principally require knowledge of the\ntransformations exhibited by training pairs, our experiments show that a\nbootstrapping approach can sidestep this issue, and that the regularization\nterm can effectively be used in an unsupervised setting.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 12:28:43 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Lattner", "Stefan", ""], ["Grachten", "Maarten", ""]]}, {"id": "1707.01428", "submitter": "Jeffery Kinnison", "authors": "Jeff Kinnison, Nathaniel Kremer-Herman, Douglas Thain and Walter\n  Scheirer", "title": "SHADHO: Massively Scalable Hardware-Aware Distributed Hyperparameter\n  Optimization", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision is experiencing an AI renaissance, in which machine learning\nmodels are expediting important breakthroughs in academic research and\ncommercial applications. Effectively training these models, however, is not\ntrivial due in part to hyperparameters: user-configured values that control a\nmodel's ability to learn from data. Existing hyperparameter optimization\nmethods are highly parallel but make no effort to balance the search across\nheterogeneous hardware or to prioritize searching high-impact spaces. In this\npaper, we introduce a framework for massively Scalable Hardware-Aware\nDistributed Hyperparameter Optimization (SHADHO). Our framework calculates the\nrelative complexity of each search space and monitors performance on the\nlearning task over all trials. These metrics are then used as heuristics to\nassign hyperparameters to distributed workers based on their hardware. We first\ndemonstrate that our framework achieves double the throughput of a standard\ndistributed hyperparameter optimization framework by optimizing SVM for MNIST\nusing 150 distributed workers. We then conduct model search with SHADHO over\nthe course of one week using 74 GPUs across two compute clusters to optimize\nU-Net for a cell segmentation task, discovering 515 models that achieve a lower\nvalidation loss than standard U-Net.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 15:16:27 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 16:26:17 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Kinnison", "Jeff", ""], ["Kremer-Herman", "Nathaniel", ""], ["Thain", "Douglas", ""], ["Scheirer", "Walter", ""]]}, {"id": "1707.01461", "submitter": "Shiv Shankar", "authors": "Shiv Shankar, Sunita Sarawagi", "title": "Labeled Memory Networks for Online Model Adaptation", "comments": "Accepted at AAAI 2018, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmenting a neural network with memory that can grow without growing the\nnumber of trained parameters is a recent powerful concept with many exciting\napplications. We propose a design of memory augmented neural networks (MANNs)\ncalled Labeled Memory Networks (LMNs) suited for tasks requiring online\nadaptation in classification models. LMNs organize the memory with classes as\nthe primary key.The memory acts as a second boosted stage following a regular\nneural network thereby allowing the memory and the primary network to play\ncomplementary roles. Unlike existing MANNs that write to memory for every\ninstance and use LRU based memory replacement, LMNs write only for instances\nwith non-zero loss and use label-based memory replacement. We demonstrate\nsignificant accuracy gains on various tasks including word-modelling and\nfew-shot learning. In this paper, we establish their potential in online\nadapting a batch trained neural network to domain-relevant labeled data at\ndeployment time. We show that LMNs are better than other MANNs designed for\nmeta-learning. We also found them to be more accurate and faster than\nstate-of-the-art methods of retuning model parameters for adapting to\ndomain-specific labeled data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 16:49:28 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 09:40:19 GMT"}, {"version": "v3", "created": "Sat, 2 Dec 2017 07:31:20 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Shankar", "Shiv", ""], ["Sarawagi", "Sunita", ""]]}, {"id": "1707.01475", "submitter": "Th\\'eo Trouillon", "authors": "Th\\'eo Trouillon and Maximilian Nickel", "title": "Complex and Holographic Embeddings of Knowledge Graphs: A Comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embeddings of knowledge graphs have received significant attention due to\ntheir excellent performance for tasks like link prediction and entity\nresolution. In this short paper, we are providing a comparison of two\nstate-of-the-art knowledge graph embeddings for which their equivalence has\nrecently been established, i.e., ComplEx and HolE [Nickel, Rosasco, and Poggio,\n2016; Trouillon et al., 2016; Hayashi and Shimbo, 2017]. First, we briefly\nreview both models and discuss how their scoring functions are equivalent. We\nthen analyze the discrepancy of results reported in the original articles, and\nshow experimentally that they are likely due to the use of different loss\nfunctions. In further experiments, we evaluate the ability of both models to\nembed symmetric and antisymmetric patterns. Finally, we discuss advantages and\ndisadvantages of both models and under which conditions one would be preferable\nto the other.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 17:17:34 GMT"}, {"version": "v2", "created": "Sun, 23 Jul 2017 04:30:21 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Trouillon", "Th\u00e9o", ""], ["Nickel", "Maximilian", ""]]}, {"id": "1707.01476", "submitter": "Tim Dettmers", "authors": "Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel", "title": "Convolutional 2D Knowledge Graph Embeddings", "comments": "Extended AAAI2018 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction for knowledge graphs is the task of predicting missing\nrelationships between entities. Previous work on link prediction has focused on\nshallow, fast models which can scale to large knowledge graphs. However, these\nmodels learn less expressive features than deep, multi-layer models -- which\npotentially limits performance. In this work, we introduce ConvE, a multi-layer\nconvolutional network model for link prediction, and report state-of-the-art\nresults for several established datasets. We also show that the model is highly\nparameter efficient, yielding the same performance as DistMult and R-GCN with\n8x and 17x fewer parameters. Analysis of our model suggests that it is\nparticularly effective at modelling nodes with high indegree -- which are\ncommon in highly-connected, complex knowledge graphs such as Freebase and\nYAGO3. In addition, it has been noted that the WN18 and FB15k datasets suffer\nfrom test set leakage, due to inverse relations from the training set being\npresent in the test set -- however, the extent of this issue has so far not\nbeen quantified. We find this problem to be severe: a simple rule-based model\ncan achieve state-of-the-art results on both WN18 and FB15k. To ensure that\nmodels are evaluated on datasets where simply exploiting inverse relations\ncannot yield competitive results, we investigate and validate several commonly\nused datasets -- deriving robust variants where necessary. We then perform\nexperiments on these robust datasets for our own and several previously\nproposed models and find that ConvE achieves state-of-the-art Mean Reciprocal\nRank across most datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 17:18:17 GMT"}, {"version": "v2", "created": "Sat, 8 Jul 2017 15:31:20 GMT"}, {"version": "v3", "created": "Sun, 7 Jan 2018 13:58:02 GMT"}, {"version": "v4", "created": "Mon, 26 Mar 2018 19:57:43 GMT"}, {"version": "v5", "created": "Fri, 6 Apr 2018 09:28:55 GMT"}, {"version": "v6", "created": "Wed, 4 Jul 2018 09:53:46 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Dettmers", "Tim", ""], ["Minervini", "Pasquale", ""], ["Stenetorp", "Pontus", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1707.01477", "submitter": "Michael Veale", "authors": "Reuben Binns, Michael Veale, Max Van Kleek, Nigel Shadbolt", "title": "Like trainer, like bot? Inheritance of bias in algorithmic content\n  moderation", "comments": "12 pages, 3 figures, 9th International Conference on Social\n  Informatics (SocInfo 2017), Oxford, UK, 13--15 September 2017 (forthcoming in\n  Springer Lecture Notes in Computer Science)", "journal-ref": null, "doi": "10.1007/978-3-319-67256-4_32", "report-no": null, "categories": "cs.CY cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internet has become a central medium through which `networked publics'\nexpress their opinions and engage in debate. Offensive comments and personal\nattacks can inhibit participation in these spaces. Automated content moderation\naims to overcome this problem using machine learning classifiers trained on\nlarge corpora of texts manually annotated for offence. While such systems could\nhelp encourage more civil debate, they must navigate inherently normatively\ncontestable boundaries, and are subject to the idiosyncratic norms of the human\nraters who provide the training data. An important objective for platforms\nimplementing such measures might be to ensure that they are not unduly biased\ntowards or against particular norms of offence. This paper provides some\nexploratory methods by which the normative biases of algorithmic content\nmoderation systems can be measured, by way of a case study using an existing\ndataset of comments labelled for offence. We train classifiers on comments\nlabelled by different demographic subsets (men and women) to understand how\ndifferences in conceptions of offence between these groups might affect the\nperformance of the resulting models on various test sets. We conclude by\ndiscussing some of the ethical choices facing the implementers of algorithmic\nmoderation systems, given various desired levels of diversity of viewpoints\namongst discussion participants.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 17:19:45 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Binns", "Reuben", ""], ["Veale", "Michael", ""], ["Van Kleek", "Max", ""], ["Shadbolt", "Nigel", ""]]}, {"id": "1707.01495", "submitter": "Marcin Andrychowicz", "authors": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel\n  Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba", "title": "Hindsight Experience Replay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dealing with sparse rewards is one of the biggest challenges in Reinforcement\nLearning (RL). We present a novel technique called Hindsight Experience Replay\nwhich allows sample-efficient learning from rewards which are sparse and binary\nand therefore avoid the need for complicated reward engineering. It can be\ncombined with an arbitrary off-policy RL algorithm and may be seen as a form of\nimplicit curriculum.\n  We demonstrate our approach on the task of manipulating objects with a\nrobotic arm. In particular, we run experiments on three different tasks:\npushing, sliding, and pick-and-place, in each case using only binary rewards\nindicating whether or not the task is completed. Our ablation studies show that\nHindsight Experience Replay is a crucial ingredient which makes training\npossible in these challenging environments. We show that our policies trained\non a physics simulation can be deployed on a physical robot and successfully\ncomplete the task.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 17:55:53 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 18:35:33 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 10:04:20 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Andrychowicz", "Marcin", ""], ["Wolski", "Filip", ""], ["Ray", "Alex", ""], ["Schneider", "Jonas", ""], ["Fong", "Rachel", ""], ["Welinder", "Peter", ""], ["McGrew", "Bob", ""], ["Tobin", "Josh", ""], ["Abbeel", "Pieter", ""], ["Zaremba", "Wojciech", ""]]}, {"id": "1707.01543", "submitter": "Yuting Wei", "authors": "Yuting Wei, Fanny Yang, Martin J. Wainwright", "title": "Early stopping for kernel boosting algorithms: A general analysis with\n  localized complexities", "comments": "Compared to the nips version, this version includes results for the\n  random design case and the proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early stopping of iterative algorithms is a widely-used form of\nregularization in statistics, commonly used in conjunction with boosting and\nrelated gradient-type algorithms. Although consistency results have been\nestablished in some settings, such estimators are less well-understood than\ntheir analogues based on penalized regularization. In this paper, for a\nrelatively broad class of loss functions and boosting algorithms (including\nL2-boost, LogitBoost and AdaBoost, among others), we exhibit a direct\nconnection between the performance of a stopped iterate and the localized\nGaussian complexity of the associated function class. This connection allows us\nto show that local fixed point analysis of Gaussian or Rademacher complexities,\nnow standard in the analysis of penalized estimators, can be used to derive\noptimal stopping rules. We derive such stopping rules in detail for various\nkernel classes, and illustrate the correspondence of our theory with practice\nfor Sobolev kernel classes.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 19:12:12 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 20:17:06 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Wei", "Yuting", ""], ["Yang", "Fanny", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1707.01561", "submitter": "Sixun Ouyang", "authors": "Felipe Costa, Sixun Ouyang, Peter Dolog, Aonghus Lawlor", "title": "Automatic Generation of Natural Language Explanations", "comments": "7 pages, 5 figures, 2nd workshop on Deep Learning for Recommender\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task for recommender system is to generate explanations\naccording to a user's preferences. Most of the current methods for explainable\nrecommendations use structured sentences to provide descriptions along with the\nrecommendations they produce. However, those methods have neglected the\nreview-oriented way of writing a text, even though it is known that these\nreviews have a strong influence over user's decision.\n  In this paper, we propose a method for the automatic generation of natural\nlanguage explanations, for predicting how a user would write about an item,\nbased on user ratings from different items' features. We design a\ncharacter-level recurrent neural network (RNN) model, which generates an item's\nreview explanations using long-short term memories (LSTM). The model generates\ntext reviews given a combination of the review and ratings score that express\nopinions about different factors or aspects of an item. Our network is trained\non a sub-sample from the large real-world dataset BeerAdvocate. Our empirical\nevaluation using natural language processing metrics shows the generated text's\nquality is close to a real user written review, identifying negation,\nmisspellings, and domain specific vocabulary.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 14:52:41 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Costa", "Felipe", ""], ["Ouyang", "Sixun", ""], ["Dolog", "Peter", ""], ["Lawlor", "Aonghus", ""]]}, {"id": "1707.01591", "submitter": "Arya Farahi", "authors": "Alex Chojnacki, Chengyu Dai, Arya Farahi, Guangsha Shi, Jared Webb,\n  Daniel T. Zhang, Jacob Abernethy, Eric Schwartz", "title": "A Data Science Approach to Understanding Residential Water Contamination\n  in Flint", "comments": "Applied Data Science track paper at KDD 2017. For associated\n  promotional video, see https://www.youtube.com/watch?v=0g66ImaV8Ag", "journal-ref": null, "doi": "10.1145/3097983.3098078", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the residents of Flint learned that lead had contaminated their water\nsystem, the local government made water-testing kits available to them free of\ncharge. The city government published the results of these tests, creating a\nvaluable dataset that is key to understanding the causes and extent of the lead\ncontamination event in Flint. This is the nation's largest dataset on lead in a\nmunicipal water system.\n  In this paper, we predict the lead contamination for each household's water\nsupply, and we study several related aspects of Flint's water troubles, many of\nwhich generalize well beyond this one city. For example, we show that elevated\nlead risks can be (weakly) predicted from observable home attributes. Then we\nexplore the factors associated with elevated lead. These risk assessments were\ndeveloped in part via a crowd sourced prediction challenge at the University of\nMichigan. To inform Flint residents of these assessments, they have been\nincorporated into a web and mobile application funded by \\texttt{Google.org}.\nWe also explore questions of self-selection in the residential testing program,\nexamining which factors are linked to when and how frequently residents\nvoluntarily sample their water.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 22:12:14 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Chojnacki", "Alex", ""], ["Dai", "Chengyu", ""], ["Farahi", "Arya", ""], ["Shi", "Guangsha", ""], ["Webb", "Jared", ""], ["Zhang", "Daniel T.", ""], ["Abernethy", "Jacob", ""], ["Schwartz", "Eric", ""]]}, {"id": "1707.01606", "submitter": "Ekraam Sabir", "authors": "Ayush Jaiswal, Ekraam Sabir, Wael AbdAlmageed, Premkumar Natarajan", "title": "Multimedia Semantic Integrity Assessment Using Joint Embedding Of Images\n  And Text", "comments": "*Ayush Jaiswal and Ekraam Sabir contributed equally to the work in\n  this paper", "journal-ref": "In Proceedings of the 2017 ACM on Multimedia Conference, pp.\n  1465-1471. ACM, 2017", "doi": "10.1145/3123266.3123385", "report-no": null, "categories": "cs.MM cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real world multimedia data is often composed of multiple modalities such as\nan image or a video with associated text (e.g. captions, user comments, etc.)\nand metadata. Such multimodal data packages are prone to manipulations, where a\nsubset of these modalities can be altered to misrepresent or repurpose data\npackages, with possible malicious intent. It is, therefore, important to\ndevelop methods to assess or verify the integrity of these multimedia packages.\nUsing computer vision and natural language processing methods to directly\ncompare the image (or video) and the associated caption to verify the integrity\nof a media package is only possible for a limited set of objects and scenes. In\nthis paper, we present a novel deep learning-based approach for assessing the\nsemantic integrity of multimedia packages containing images and captions, using\na reference set of multimedia packages. We construct a joint embedding of\nimages and captions with deep multimodal representation learning on the\nreference dataset in a framework that also provides image-caption consistency\nscores (ICCSs). The integrity of query media packages is assessed as the\ninlierness of the query ICCSs with respect to the reference dataset. We present\nthe MultimodAl Information Manipulation dataset (MAIM), a new dataset of media\npackages from Flickr, which we make available to the research community. We use\nboth the newly created dataset as well as Flickr30K and MS COCO datasets to\nquantitatively evaluate our proposed approach. The reference dataset does not\ncontain unmanipulated versions of tampered query packages. Our method is able\nto achieve F1 scores of 0.75, 0.89 and 0.94 on MAIM, Flickr30K and MS COCO,\nrespectively, for detecting semantically incoherent media packages.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 01:25:17 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 00:47:21 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 18:02:08 GMT"}, {"version": "v4", "created": "Fri, 29 Jun 2018 00:34:27 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Jaiswal", "Ayush", ""], ["Sabir", "Ekraam", ""], ["AbdAlmageed", "Wael", ""], ["Natarajan", "Premkumar", ""]]}, {"id": "1707.01623", "submitter": "Ji-Sung Kim", "authors": "Ji-Sung Kim, Xin Gao, Andrey Rzhetsky", "title": "RIDDLE: Race and ethnicity Imputation from Disease history with Deep\n  LEarning", "comments": null, "journal-ref": "PLOS Computational Biology 14(4): e1006106 (2018)", "doi": "10.1371/journal.pcbi.1006106", "report-no": null, "categories": "q-bio.QM cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anonymized electronic medical records are an increasingly popular source of\nresearch data. However, these datasets often lack race and ethnicity\ninformation. This creates problems for researchers modeling human disease, as\nrace and ethnicity are powerful confounders for many health exposures and\ntreatment outcomes; race and ethnicity are closely linked to\npopulation-specific genetic variation. We showed that deep neural networks\ngenerate more accurate estimates for missing racial and ethnic information than\ncompeting methods (e.g., logistic regression, random forest). RIDDLE yielded\nsignificantly better classification performance across all metrics that were\nconsidered: accuracy, cross-entropy loss (error), and area under the curve for\nreceiver operating characteristic plots (all $p < 10^{-6}$). We made specific\nefforts to interpret the trained neural network models to identify, quantify,\nand visualize medical features which are predictive of race and ethnicity. We\nused these characterizations of informative features to perform a systematic\ncomparison of differential disease patterns by race and ethnicity. The fact\nthat clinical histories are informative for imputing race and ethnicity could\nreflect (1) a skewed distribution of blue- and white-collar professions across\nracial and ethnic groups, (2) uneven accessibility and subjective importance of\nprophylactic health, (3) possible variation in lifestyle, such as dietary\nhabits, and (4) differences in background genetic variation which predispose to\ndiseases.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 03:03:57 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 21:21:47 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Kim", "Ji-Sung", ""], ["Gao", "Xin", ""], ["Rzhetsky", "Andrey", ""]]}, {"id": "1707.01647", "submitter": "Alex Kim", "authors": "HyoungSeok Kim, JiHoon Kang, WooMyoung Park, SukHyun Ko, YoonHo Cho,\n  DaeSung Yu, YoungSook Song, JungWon Choi", "title": "Convergence Analysis of Optimization Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.OC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The regret bound of an optimization algorithms is one of the basic criteria\nfor evaluating the performance of the given algorithm. By inspecting the\ndifferences between the regret bounds of traditional algorithms and adaptive\none, we provide a guide for choosing an optimizer with respect to the given\ndata set and the loss function. For analysis, we assume that the loss function\nis convex and its gradient is Lipschitz continuous.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 06:10:53 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Kim", "HyoungSeok", ""], ["Kang", "JiHoon", ""], ["Park", "WooMyoung", ""], ["Ko", "SukHyun", ""], ["Cho", "YoonHo", ""], ["Yu", "DaeSung", ""], ["Song", "YoungSook", ""], ["Choi", "JungWon", ""]]}, {"id": "1707.01698", "submitter": "Stijn Heldens", "authors": "Stijn Heldens, Claudio Martella, Nelly Litvak, Maarten van Steen", "title": "Automated Lane Detection in Crowds using Proximity Graphs", "comments": "Presented at the 6th International Workshop on Urban Computing\n  (UrbComp 2017) held in conjunction with the 23th ACM SIGKDD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying the behavior of crowds is vital for understanding and predicting\nhuman interactions in public areas. Research has shown that, under certain\nconditions, large groups of people can form collective behavior patterns: local\ninteractions between individuals results in global movements patterns. To\ndetect these patterns in a crowd, we assume each person is carrying an on-body\ndevice that acts a local proximity sensor, e.g., smartphone or bluetooth badge,\nand represent the texture of the crowd as a proximity graph. Our goal is\nextract information about crowds from these proximity graphs. In this work, we\nfocus on one particular type of pattern: lane formation. We present a formal\ndefinition of a lane, proposed a simple probabilistic model that simulates\nlanes moving through a stationary crowd, and present an automated\nlane-detection method. Our preliminary results show that our method is able to\ndetect lanes of different shapes and sizes. We see our work as an initial step\ntowards rich pattern recognition using proximity graphs.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 09:23:07 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Heldens", "Stijn", ""], ["Martella", "Claudio", ""], ["Litvak", "Nelly", ""], ["van Steen", "Maarten", ""]]}, {"id": "1707.01700", "submitter": "Joris Gu\\'erin", "authors": "Joris Gu\\'erin, Olivier Gibaru, St\\'ephane Thiery and Eric Nyiri", "title": "CNN features are also great at unsupervised classification", "comments": "10 pages, 2 figures, 4 tables. Proceedings of AIFU 2018, Melbourne,\n  Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at providing insight on the transferability of deep CNN\nfeatures to unsupervised problems. We study the impact of different pretrained\nCNN feature extractors on the problem of image set clustering for object\nclassification as well as fine-grained classification. We propose a rather\nstraightforward pipeline combining deep-feature extraction using a CNN\npretrained on ImageNet and a classic clustering algorithm to classify sets of\nimages. This approach is compared to state-of-the-art algorithms in\nimage-clustering and provides better results. These results strengthen the\nbelief that supervised training of deep CNN on large datasets, with a large\nvariability of classes, extracts better features than most carefully designed\nengineering approaches, even for unsupervised tasks. We also validate our\napproach on a robotic application, consisting in sorting and storing objects\nsmartly based on clustering.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 09:24:35 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 11:11:15 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Gu\u00e9rin", "Joris", ""], ["Gibaru", "Olivier", ""], ["Thiery", "St\u00e9phane", ""], ["Nyiri", "Eric", ""]]}, {"id": "1707.01825", "submitter": "Jacob Whitehill", "authors": "Jacob Whitehill", "title": "Climbing the Kaggle Leaderboard by Exploiting the Log-Loss Oracle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of data-mining competitions (e.g., Kaggle, KDDCup, ILSVRC\nChallenge), we show how access to an oracle that reports a contestant's\nlog-loss score on the test set can be exploited to deduce the ground-truth of\nsome of the test examples. By applying this technique iteratively to batches of\n$m$ examples (for small $m$), all of the test labels can eventually be\ninferred. In this paper, (1) We demonstrate this attack on the first stage of a\nrecent Kaggle competition (Intel & MobileODT Cancer Screening) and use it to\nachieve a log-loss of $0.00000$ (and thus attain a rank of #4 out of 848\ncontestants), without ever training a classifier to solve the actual task. (2)\nWe prove an upper bound on the batch size $m$ as a function of the\nfloating-point resolution of the probability estimates that the contestant\nsubmits for the labels. (3) We derive, and demonstrate in simulation, a more\nflexible attack that can be used even when the oracle reports the accuracy on\nan unknown (but fixed) subset of the test set's labels. These results underline\nthe importance of evaluating contestants based only on test data that the\noracle does not examine.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 14:54:54 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Whitehill", "Jacob", ""]]}, {"id": "1707.01826", "submitter": "Fanghui Liu", "authors": "Fanghui Liu, Xiaolin Huang, Chen Gong, Jie Yang, Johan A.K. Suykens", "title": "Indefinite Kernel Logistic Regression with Concave-inexact-convex\n  Procedure", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2018.2851305", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In kernel methods, the kernels are often required to be positive definite,\nwhich restricts the use of many indefinite kernels. To consider those\nnon-positive definite kernels, in this paper, we aim to build an indefinite\nkernel learning framework for kernel logistic regression. The proposed\nindefinite kernel logistic regression (IKLR) model is analysed in the\nReproducing Kernel Kre\\u{\\i}n Spaces (RKKS) and then becomes non-convex. Using\nthe positive decomposition of a non-positive definite kernel, the derived IKLR\nmodel can be decomposed into the difference of two convex functions.\nAccordingly, a concave-convex procedure is introduced to solve the non-convex\noptimization problem. Since the concave-convex procedure has to solve a\nsub-problem in each iteration, we propose a concave-inexact-convex procedure\n(CCICP) algorithm with an inexact solving scheme to accelerate the solving\nprocess. Besides, we propose a stochastic variant of CCICP to efficiently\nobtain a proximal solution, which achieves the similar purpose with the inexact\nsolving scheme in CCICP. The convergence analyses of the above two variants of\nconcave-convex procedure are conducted. By doing so, our method works\neffectively not only under a deterministic setting but also under a stochastic\nsetting. Experimental results on several benchmarks suggest that the proposed\nIKLR model performs favorably against the standard (positive-definite) kernel\nlogistic regression and other competitive indefinite learning based algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 14:55:08 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 13:42:21 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Liu", "Fanghui", ""], ["Huang", "Xiaolin", ""], ["Gong", "Chen", ""], ["Yang", "Jie", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1707.01875", "submitter": "Christos Dimitrakakis", "authors": "Yang Liu, Goran Radanovic, Christos Dimitrakakis, Debmalya Mandal,\n  David C. Parkes", "title": "Calibrated Fairness in Bandits", "comments": "To be presented at the FAT-ML'17 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study fairness within the stochastic, \\emph{multi-armed bandit} (MAB)\ndecision making framework. We adapt the fairness framework of \"treating similar\nindividuals similarly\" to this setting. Here, an `individual' corresponds to an\narm and two arms are `similar' if they have a similar quality distribution.\nFirst, we adopt a {\\em smoothness constraint} that if two arms have a similar\nquality distribution then the probability of selecting each arm should be\nsimilar. In addition, we define the {\\em fairness regret}, which corresponds to\nthe degree to which an algorithm is not calibrated, where perfect calibration\nrequires that the probability of selecting an arm is equal to the probability\nwith which the arm has the best quality realization. We show that a variation\non Thompson sampling satisfies smooth fairness for total variation distance,\nand give an $\\tilde{O}((kT)^{2/3})$ bound on fairness regret. This complements\nprior work, which protects an on-average better arm from being less favored. We\nalso explain how to extend our algorithm to the dueling bandit setting.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 17:24:01 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Liu", "Yang", ""], ["Radanovic", "Goran", ""], ["Dimitrakakis", "Christos", ""], ["Mandal", "Debmalya", ""], ["Parkes", "David C.", ""]]}, {"id": "1707.01926", "submitter": "Yaguang Li", "authors": "Yaguang Li, Rose Yu, Cyrus Shahabi and Yan Liu", "title": "Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic\n  Forecasting", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatiotemporal forecasting has various applications in neuroscience, climate\nand transportation domain. Traffic forecasting is one canonical example of such\nlearning task. The task is challenging due to (1) complex spatial dependency on\nroad networks, (2) non-linear temporal dynamics with changing road conditions\nand (3) inherent difficulty of long-term forecasting. To address these\nchallenges, we propose to model the traffic flow as a diffusion process on a\ndirected graph and introduce Diffusion Convolutional Recurrent Neural Network\n(DCRNN), a deep learning framework for traffic forecasting that incorporates\nboth spatial and temporal dependency in the traffic flow. Specifically, DCRNN\ncaptures the spatial dependency using bidirectional random walks on the graph,\nand the temporal dependency using the encoder-decoder architecture with\nscheduled sampling. We evaluate the framework on two real-world large scale\nroad network traffic datasets and observe consistent improvement of 12% - 15%\nover state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 18:20:59 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 16:51:18 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 19:52:51 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Li", "Yaguang", ""], ["Yu", "Rose", ""], ["Shahabi", "Cyrus", ""], ["Liu", "Yan", ""]]}, {"id": "1707.01932", "submitter": "Eric Jang", "authors": "Eric Jang, Sudheendra Vijayanarasimhan, Peter Pastor, Julian Ibarz,\n  Sergey Levine", "title": "End-to-End Learning of Semantic Grasping", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of semantic robotic grasping, in which a robot picks up\nan object of a user-specified class using only monocular images. Inspired by\nthe two-stream hypothesis of visual reasoning, we present a semantic grasping\nframework that learns object detection, classification, and grasp planning in\nan end-to-end fashion. A \"ventral stream\" recognizes object class while a\n\"dorsal stream\" simultaneously interprets the geometric relationships necessary\nto execute successful grasps. We leverage the autonomous data collection\ncapabilities of robots to obtain a large self-supervised dataset for training\nthe dorsal stream, and use semi-supervised label propagation to train the\nventral stream with only a modest amount of human supervision. We\nexperimentally show that our approach improves upon grasping systems whose\ncomponents are not learned end-to-end, including a baseline method that uses\nbounding box detection. Furthermore, we show that jointly training our model\nwith auxiliary data consisting of non-semantic grasping data, as well as\nsemantically labeled images without grasp actions, has the potential to\nsubstantially improve semantic grasping performance.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 18:41:22 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 07:41:54 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 08:57:52 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Jang", "Eric", ""], ["Vijayanarasimhan", "Sudheendra", ""], ["Pastor", "Peter", ""], ["Ibarz", "Julian", ""], ["Levine", "Sergey", ""]]}, {"id": "1707.01939", "submitter": "Mahdi Nazemi", "authors": "Mahdi Nazemi, Shahin Nazarian, Massoud Pedram", "title": "High-Performance FPGA Implementation of Equivariant Adaptive Separation\n  via Independence Algorithm for Independent Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent Component Analysis (ICA) is a dimensionality reduction technique\nthat can boost efficiency of machine learning models that deal with probability\ndensity functions, e.g. Bayesian neural networks. Algorithms that implement\nadaptive ICA converge slower than their nonadaptive counterparts, however, they\nare capable of tracking changes in underlying distributions of input features.\nThis intrinsically slow convergence of adaptive methods combined with existing\nhardware implementations that operate at very low clock frequencies necessitate\nfundamental improvements in both algorithm and hardware design. This paper\npresents an algorithm that allows efficient hardware implementation of ICA.\nCompared to previous work, our FPGA implementation of adaptive ICA improves\nclock frequency by at least one order of magnitude and throughput by at least\ntwo orders of magnitude. Our proposed algorithm is not limited to ICA and can\nbe used in various machine learning problems that use stochastic gradient\ndescent optimization.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 19:02:11 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Nazemi", "Mahdi", ""], ["Nazarian", "Shahin", ""], ["Pedram", "Massoud", ""]]}, {"id": "1707.01943", "submitter": "David Alvarez-Melis", "authors": "David Alvarez-Melis, Tommi S. Jaakkola", "title": "A causal framework for explaining the predictions of black-box\n  sequence-to-sequence models", "comments": "12 Pages, EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We interpret the predictions of any black-box structured input-structured\noutput model around a specific input-output pair. Our method returns an\n\"explanation\" consisting of groups of input-output tokens that are causally\nrelated. These dependencies are inferred by querying the black-box model with\nperturbed inputs, generating a graph over tokens from the responses, and\nsolving a partitioning problem to select the most relevant components. We focus\nthe general approach on sequence-to-sequence problems, adopting a variational\nautoencoder to yield meaningful input perturbations. We test our method across\nseveral NLP sequence generation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 19:36:43 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 16:31:23 GMT"}, {"version": "v3", "created": "Tue, 14 Nov 2017 20:12:41 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Alvarez-Melis", "David", ""], ["Jaakkola", "Tommi S.", ""]]}, {"id": "1707.01945", "submitter": "Deanna Needell", "authors": "Deanna Needell, Rayan Saab, Tina Woolf", "title": "Simple Classification using Binary Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary, or one-bit, representations of data arise naturally in many\napplications, and are appealing in both hardware implementations and algorithm\ndesign. In this work, we study the problem of data classification from binary\ndata and propose a framework with low computation and resource costs. We\nillustrate the utility of the proposed approach through stylized and realistic\nnumerical experiments, and provide a theoretical analysis for a simple case. We\nhope that our framework and analysis will serve as a foundation for studying\nsimilar types of approaches.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 19:45:29 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Needell", "Deanna", ""], ["Saab", "Rayan", ""], ["Woolf", "Tina", ""]]}, {"id": "1707.02019", "submitter": "Massimo Caccia", "authors": "Massimo Caccia and Bruno R\\'emillard", "title": "Option Pricing and Hedging for Discrete Time Autoregressive Hidden\n  Markov Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR cs.LG q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we solve the discrete time mean-variance hedging problem when\nasset returns follow a multivariate autoregressive hidden Markov model. Time\ndependent volatility and serial dependence are well established properties of\nfinancial time series and our model covers both. To illustrate the relevance of\nour proposed methodology, we first compare the proposed model with the\nwell-known hidden Markov model via likelihood ratio tests and a novel\ngoodness-of-fit test on the S\\&P 500 daily returns. Secondly, we present\nout-of-sample hedging results on S\\&P 500 vanilla options as well as a trading\nstrategy based on theoretical prices, which we compare to simpler models\nincluding the classical Black-Scholes delta-hedging approach.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 02:04:25 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Caccia", "Massimo", ""], ["R\u00e9millard", "Bruno", ""]]}, {"id": "1707.02029", "submitter": "Saswat Padhi", "authors": "Saswat Padhi and Rahul Sharma and Todd Millstein", "title": "LoopInvGen: A Loop Invariant Generator based on Precondition Inference", "comments": "Tool Description ( for technical details, see our PLDI paper at\n  https://doi.org/10.1145/2908080.2908099 ), SyGuS-COMP'19 Competition\n  Contribution, 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe the LoopInvGen tool for generating loop invariants that can\nprovably guarantee correctness of a program with respect to a given\nspecification. LoopInvGen is an efficient implementation of the inference\ntechnique originally proposed in our earlier work on PIE\n(https://doi.org/10.1145/2908080.2908099).\n  In contrast to existing techniques, LoopInvGen is not restricted to a fixed\nset of features -- atomic predicates that are composed together to build\ncomplex loop invariants. Instead, we start with no initial features, and use\nprogram synthesis techniques to grow the set on demand. This not only enables a\nless onerous and more expressive approach, but also appears to be significantly\nfaster than the existing tools over the SyGuS-COMP 2018 benchmarks from the INV\ntrack.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 03:51:39 GMT"}, {"version": "v2", "created": "Sat, 23 Jun 2018 12:00:06 GMT"}, {"version": "v3", "created": "Sun, 8 Jul 2018 07:15:41 GMT"}, {"version": "v4", "created": "Thu, 31 Oct 2019 10:16:52 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Padhi", "Saswat", ""], ["Sharma", "Rahul", ""], ["Millstein", "Todd", ""]]}, {"id": "1707.02038", "submitter": "Daniel Russo", "authors": "Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng\n  Wen", "title": "A Tutorial on Thompson Sampling", "comments": null, "journal-ref": "Foundations and Trends in Machine Learning, Vol. 11, No. 1, pp.\n  1-96, 2018", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling is an algorithm for online decision problems where actions\nare taken sequentially in a manner that must balance between exploiting what is\nknown to maximize immediate performance and investing to accumulate new\ninformation that may improve future performance. The algorithm addresses a\nbroad range of problems in a computationally efficient manner and is therefore\nenjoying wide use. This tutorial covers the algorithm and its application,\nillustrating concepts through a range of examples, including Bernoulli bandit\nproblems, shortest path problems, product recommendation, assortment, active\nlearning with neural networks, and reinforcement learning in Markov decision\nprocesses. Most of these problems involve complex information structures, where\ninformation revealed by taking an action informs beliefs about other actions.\nWe will also discuss when and why Thompson sampling is or is not effective and\nrelations to alternative algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 05:22:16 GMT"}, {"version": "v2", "created": "Sun, 19 Nov 2017 02:29:27 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 22:22:04 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Russo", "Daniel", ""], ["Van Roy", "Benjamin", ""], ["Kazerouni", "Abbas", ""], ["Osband", "Ian", ""], ["Wen", "Zheng", ""]]}, {"id": "1707.02158", "submitter": "Bora Edizel", "authors": "Bora Edizel, Amin Mantrach, Xiao Bai", "title": "Deep Character-Level Click-Through Rate Prediction for Sponsored Search", "comments": "SIGIR2017, 10 pages", "journal-ref": null, "doi": "10.1145/3077136.3080811", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the click-through rate of an advertisement is a critical component\nof online advertising platforms. In sponsored search, the click-through rate\nestimates the probability that a displayed advertisement is clicked by a user\nafter she submits a query to the search engine. Commercial search engines\ntypically rely on machine learning models trained with a large number of\nfeatures to make such predictions. This is inevitably requires a lot of\nengineering efforts to define, compute, and select the appropriate features. In\nthis paper, we propose two novel approaches (one working at character level and\nthe other working at word level) that use deep convolutional neural networks to\npredict the click-through rate of a query-advertisement pair. Specially, the\nproposed architectures only consider the textual content appearing in a\nquery-advertisement pair as input, and produce as output a click-through rate\nprediction. By comparing the character-level model with the word-level model,\nwe show that language representation can be learnt from scratch at character\nlevel when trained on enough data. Through extensive experiments using billions\nof query-advertisement pairs of a popular commercial search engine, we\ndemonstrate that both approaches significantly outperform a baseline model\nbuilt on well-selected text features and a state-of-the-art word2vec-based\napproach. Finally, by combining the predictions of the deep models introduced\nin this study with the prediction of the model in production of the same\ncommercial search engine, we significantly improve the accuracy and the\ncalibration of the click-through rate prediction of the production system.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 13:15:45 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Edizel", "Bora", ""], ["Mantrach", "Amin", ""], ["Bai", "Xiao", ""]]}, {"id": "1707.02198", "submitter": "Cicero dos Santos", "authors": "Cicero Nogueira dos Santos, Kahini Wadhawan, Bowen Zhou", "title": "Learning Loss Functions for Semi-supervised Learning via Discriminative\n  Adversarial Networks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose discriminative adversarial networks (DAN) for semi-supervised\nlearning and loss function learning. Our DAN approach builds upon generative\nadversarial networks (GANs) and conditional GANs but includes the key\ndifferentiator of using two discriminators instead of a generator and a\ndiscriminator. DAN can be seen as a framework to learn loss functions for\npredictors that also implements semi-supervised learning in a straightforward\nmanner. We propose instantiations of DAN for two different prediction tasks:\nclassification and ranking. Our experimental results on three datasets of\ndifferent tasks demonstrate that DAN is a promising framework for both\nsemi-supervised learning and learning loss functions for predictors. For all\ntasks, the semi-supervised capability of DAN can significantly boost the\npredictor performance for small labeled sets with minor architecture changes\nacross tasks. Moreover, the loss functions automatically learned by DANs are\nvery competitive and usually outperform the standard pairwise and negative\nlog-likelihood loss functions for both semi-supervised and supervised learning.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 14:38:33 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Santos", "Cicero Nogueira dos", ""], ["Wadhawan", "Kahini", ""], ["Zhou", "Bowen", ""]]}, {"id": "1707.02201", "submitter": "Josh Merel", "authors": "Josh Merel, Yuval Tassa, Dhruva TB, Sriram Srinivasan, Jay Lemmon,\n  Ziyu Wang, Greg Wayne, Nicolas Heess", "title": "Learning human behaviors from motion capture by adversarial imitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid progress in deep reinforcement learning has made it increasingly\nfeasible to train controllers for high-dimensional humanoid bodies. However,\nmethods that use pure reinforcement learning with simple reward functions tend\nto produce non-humanlike and overly stereotyped movement behaviors. In this\nwork, we extend generative adversarial imitation learning to enable training of\ngeneric neural network policies to produce humanlike movement patterns from\nlimited demonstrations consisting only of partially observed state features,\nwithout access to actions, even when the demonstrations come from a body with\ndifferent and unknown physical parameters. We leverage this approach to build\nsub-skill policies from motion capture data and show that they can be reused to\nsolve tasks when controlled by a higher level controller.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 14:46:45 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 14:02:39 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Merel", "Josh", ""], ["Tassa", "Yuval", ""], ["TB", "Dhruva", ""], ["Srinivasan", "Sriram", ""], ["Lemmon", "Jay", ""], ["Wang", "Ziyu", ""], ["Wayne", "Greg", ""], ["Heess", "Nicolas", ""]]}, {"id": "1707.02260", "submitter": "L. Elisa Celis", "authors": "L. Elisa Celis and Nisheeth K. Vishnoi", "title": "Fair Personalization", "comments": "To appear at FAT/ML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalization is pervasive in the online space as, when combined with\nlearning, it leads to higher efficiency and revenue by allowing the most\nrelevant content to be served to each user. However, recent studies suggest\nthat such personalization can propagate societal or systemic biases, which has\nled to calls for regulatory mechanisms and algorithms to combat inequality.\nHere we propose a rigorous algorithmic framework that allows for the\npossibility to control biased or discriminatory personalization with respect to\nsensitive attributes of users without losing all of the benefits of\npersonalization.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 16:40:06 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Celis", "L. Elisa", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1707.02267", "submitter": "Stephen James", "authors": "Stephen James, Andrew J. Davison, Edward Johns", "title": "Transferring End-to-End Visuomotor Control from Simulation to Real World\n  for a Multi-Stage Task", "comments": "1st Conference on Robot Learning (CoRL 2017), Mountain View, United\n  States", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end control for robot manipulation and grasping is emerging as an\nattractive alternative to traditional pipelined approaches. However, end-to-end\nmethods tend to either be slow to train, exhibit little or no generalisability,\nor lack the ability to accomplish long-horizon or multi-stage tasks. In this\npaper, we show how two simple techniques can lead to end-to-end (image to\nvelocity) execution of a multi-stage task, which is analogous to a simple\ntidying routine, without having seen a single real image. This involves\nlocating, reaching for, and grasping a cube, then locating a basket and\ndropping the cube inside. To achieve this, robot trajectories are computed in a\nsimulator, to collect a series of control velocities which accomplish the task.\nThen, a CNN is trained to map observed images to velocities, using domain\nrandomisation to enable generalisation to real world images. Results show that\nwe are able to successfully accomplish the task in the real world with the\nability to generalise to novel environments, including those with dynamic\nlighting conditions, distractor objects, and moving objects, including the\nbasket itself. We believe our approach to be simple, highly scalable, and\ncapable of learning long-horizon tasks that have until now not been shown with\nthe state-of-the-art in end-to-end robot control.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 16:55:55 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 09:35:10 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["James", "Stephen", ""], ["Davison", "Andrew J.", ""], ["Johns", "Edward", ""]]}, {"id": "1707.02290", "submitter": "Chunhua Shen", "authors": "Hao Lu, Zhiguo Cao, Yang Xiao, Bohan Zhuang, Chunhua Shen", "title": "TasselNet: Counting maize tassels in the wild via local counts\n  regression network", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately counting maize tassels is important for monitoring the growth\nstatus of maize plants. This tedious task, however, is still mainly done by\nmanual efforts. In the context of modern plant phenotyping, automating this\ntask is required to meet the need of large-scale analysis of genotype and\nphenotype. In recent years, computer vision technologies have experienced a\nsignificant breakthrough due to the emergence of large-scale datasets and\nincreased computational resources. Naturally image-based approaches have also\nreceived much attention in plant-related studies. Yet a fact is that most\nimage-based systems for plant phenotyping are deployed under controlled\nlaboratory environment. When transferring the application scenario to\nunconstrained in-field conditions, intrinsic and extrinsic variations in the\nwild pose great challenges for accurate counting of maize tassels, which goes\nbeyond the ability of conventional image processing techniques. This calls for\nfurther robust computer vision approaches to address in-field variations. This\npaper studies the in-field counting problem of maize tassels. To our knowledge,\nthis is the first time that a plant-related counting problem is considered\nusing computer vision technologies under unconstrained field-based environment.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 02:47:06 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Lu", "Hao", ""], ["Cao", "Zhiguo", ""], ["Xiao", "Yang", ""], ["Zhuang", "Bohan", ""], ["Shen", "Chunhua", ""]]}, {"id": "1707.02293", "submitter": "Andres Masegosa R", "authors": "Andres Masegosa, Thomas D. Nielsen, Helge Langseth, Dario Ramos-Lopez,\n  Antonio Salmeron, Anders L. Madsen", "title": "Bayesian Models of Data Streams with Hierarchical Power Priors", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making inferences from data streams is a pervasive problem in many modern\ndata analysis applications. But it requires to address the problem of\ncontinuous model updating and adapt to changes or drifts in the underlying data\ngenerating distribution. In this paper, we approach these problems from a\nBayesian perspective covering general conjugate exponential models. Our\nproposal makes use of non-conjugate hierarchical priors to explicitly model\ntemporal changes of the model parameters. We also derive a novel variational\ninference scheme which overcomes the use of non-conjugate priors while\nmaintaining the computational efficiency of variational methods over conjugate\nmodels. The approach is validated on three real data sets over three latent\nvariable models.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 09:44:15 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Masegosa", "Andres", ""], ["Nielsen", "Thomas D.", ""], ["Langseth", "Helge", ""], ["Ramos-Lopez", "Dario", ""], ["Salmeron", "Antonio", ""], ["Madsen", "Anders L.", ""]]}, {"id": "1707.02353", "submitter": "Alex Zhavoronkov", "authors": "Konstantin Chekanov, Polina Mamoshina, Roman V. Yampolskiy, Radu\n  Timofte, Morten Scheibye-Knudsen, Alex Zhavoronkov", "title": "Evaluating race and sex diversity in the world's largest companies using\n  deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diversity is one of the fundamental properties for the survival of species,\npopulations, and organizations. Recent advances in deep learning allow for the\nrapid and automatic assessment of organizational diversity and possible\ndiscrimination by race, sex, age and other parameters. Automating the process\nof assessing the organizational diversity using the deep neural networks and\neliminating the human factor may provide a set of real-time unbiased reports to\nall stakeholders. In this pilot study we applied the deep-learned predictors of\nrace and sex to the executive management and board member profiles of the 500\nlargest companies from the 2016 Forbes Global 2000 list and compared the\npredicted ratios to the ratios within each company's country of origin and\nranked them by the sex-, age- and race- diversity index (DI). While the study\nhas many limitations and no claims are being made concerning the individual\ncompanies, it demonstrates a method for the rapid and impartial assessment of\norganizational diversity using deep neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 12:32:19 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Chekanov", "Konstantin", ""], ["Mamoshina", "Polina", ""], ["Yampolskiy", "Roman V.", ""], ["Timofte", "Radu", ""], ["Scheibye-Knudsen", "Morten", ""], ["Zhavoronkov", "Alex", ""]]}, {"id": "1707.02375", "submitter": "Yanan Sui", "authors": "Yanan Sui, Yisong Yue, Joel W. Burdick", "title": "Correlational Dueling Bandits with Application to Clinical Treatment in\n  Large Decision Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider sequential decision making under uncertainty, where the goal is\nto optimize over a large decision space using noisy comparative feedback. This\nproblem can be formulated as a $K$-armed Dueling Bandits problem where $K$ is\nthe total number of decisions. When $K$ is very large, existing dueling bandits\nalgorithms suffer huge cumulative regret before converging on the optimal arm.\nThis paper studies the dueling bandits problem with a large number of arms that\nexhibit a low-dimensional correlation structure. Our problem is motivated by a\nclinical decision making process in large decision space. We propose an\nefficient algorithm CorrDuel which optimizes the exploration/exploitation\ntradeoff in this large decision space of clinical treatments. More broadly, our\napproach can be applied to other sequential decision problems with large and\nstructured decision spaces. We derive regret bounds, and evaluate performance\nin simulation experiments as well as on a live clinical trial of therapeutic\nspinal cord stimulation. To our knowledge, this marks the first time an online\nlearning algorithm was applied towards spinal cord injury treatments. Our\nexperimental results show the effectiveness and efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 00:25:00 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Sui", "Yanan", ""], ["Yue", "Yisong", ""], ["Burdick", "Joel W.", ""]]}, {"id": "1707.02377", "submitter": "Minmin Chen", "authors": "Minmin Chen", "title": "Efficient Vector Representation for Documents through Corruption", "comments": "5th International Conference on Learning Representations, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient document representation learning framework, Document\nVector through Corruption (Doc2VecC). Doc2VecC represents each document as a\nsimple average of word embeddings. It ensures a representation generated as\nsuch captures the semantic meanings of the document during learning. A\ncorruption model is included, which introduces a data-dependent regularization\nthat favors informative or rare words while forcing the embeddings of common\nand non-discriminative ones to be close to zero. Doc2VecC produces\nsignificantly better word embeddings than Word2Vec. We compare Doc2VecC with\nseveral state-of-the-art document representation learning algorithms. The\nsimple model architecture introduced by Doc2VecC matches or out-performs the\nstate-of-the-art in generating high-quality document representations for\nsentiment analysis, document classification as well as semantic relatedness\ntasks. The simplicity of the model enables training on billions of words per\nhour on a single machine. At the same time, the model is very efficient in\ngenerating representations of unseen documents at test time.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 00:57:01 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Chen", "Minmin", ""]]}, {"id": "1707.02391", "submitter": "Aditi Raghunathan", "authors": "Aditi Raghunathan, Ravishankar Krishnaswamy, Prateek Jain", "title": "Learning Mixture of Gaussians with Streaming Data", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of learning a mixture of Gaussians with\nstreaming data: given a stream of $N$ points in $d$ dimensions generated by an\nunknown mixture of $k$ spherical Gaussians, the goal is to estimate the model\nparameters using a single pass over the data stream. We analyze a streaming\nversion of the popular Lloyd's heuristic and show that the algorithm estimates\nall the unknown centers of the component Gaussians accurately if they are\nsufficiently separated. Assuming each pair of centers are $C\\sigma$ distant\nwith $C=\\Omega((k\\log k)^{1/4}\\sigma)$ and where $\\sigma^2$ is the maximum\nvariance of any Gaussian component, we show that asymptotically the algorithm\nestimates the centers optimally (up to constants); our center separation\nrequirement matches the best known result for spherical Gaussians\n\\citep{vempalawang}. For finite samples, we show that a bias term based on the\ninitial estimate decreases at $O(1/{\\rm poly}(N))$ rate while variance\ndecreases at nearly optimal rate of $\\sigma^2 d/N$.\n  Our analysis requires seeding the algorithm with a good initial estimate of\nthe true cluster centers for which we provide an online PCA based clustering\nalgorithm. Indeed, the asymptotic per-step time complexity of our algorithm is\nthe optimal $d\\cdot k$ while space complexity of our algorithm is $O(dk\\log\nk)$.\n  In addition to the bias and variance terms which tend to $0$, the\nhard-thresholding based updates of streaming Lloyd's algorithm is agnostic to\nthe data distribution and hence incurs an approximation error that cannot be\navoided. However, by using a streaming version of the classical\n(soft-thresholding-based) EM method that exploits the Gaussian distribution\nexplicitly, we show that for a mixture of two Gaussians the true means can be\nestimated consistently, with estimation error decreasing at nearly optimal\nrate, and tending to $0$ for $N\\rightarrow \\infty$.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 03:35:26 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Raghunathan", "Aditi", ""], ["Krishnaswamy", "Ravishankar", ""], ["Jain", "Prateek", ""]]}, {"id": "1707.02392", "submitter": "Panos Achlioptas", "authors": "Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, Leonidas Guibas", "title": "Learning Representations and Generative Models for 3D Point Clouds", "comments": null, "journal-ref": "35th International Conference on Machine Learning (ICML), 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional geometric data offer an excellent domain for studying\nrepresentation learning and generative modeling. In this paper, we look at\ngeometric data represented as point clouds. We introduce a deep AutoEncoder\n(AE) network with state-of-the-art reconstruction quality and generalization\nability. The learned representations outperform existing methods on 3D\nrecognition tasks and enable shape editing via simple algebraic manipulations,\nsuch as semantic part editing, shape analogies and shape interpolation, as well\nas shape completion. We perform a thorough study of different generative models\nincluding GANs operating on the raw point clouds, significantly improved GANs\ntrained in the fixed latent space of our AEs, and Gaussian Mixture Models\n(GMMs). To quantitatively evaluate generative models we introduce measures of\nsample fidelity and diversity based on matchings between sets of point clouds.\nInterestingly, our evaluation of generalization, fidelity and diversity reveals\nthat GMMs trained in the latent space of our AEs yield the best results\noverall.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 03:44:49 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 05:35:30 GMT"}, {"version": "v3", "created": "Tue, 12 Jun 2018 04:27:00 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Achlioptas", "Panos", ""], ["Diamanti", "Olga", ""], ["Mitliagkas", "Ioannis", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1707.02412", "submitter": "Hailin Chen", "authors": "Hailin Chen, Shengping Cui, Sebastian Li", "title": "Application of Transfer Learning Approaches in Multimodal Wearable Human\n  Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through this project, we researched on transfer learning methods and their\napplications on real world problems. By implementing and modifying various\nmethods in transfer learning for our problem, we obtained an insight in the\nadvantages and disadvantages of these methods, as well as experiences in\ndeveloping neural network models for knowledge transfer. Due to time\nconstraint, we only applied a representative method for each major approach in\ntransfer learning. As pointed out in the literature review, each method has its\nown assumptions, strengths and shortcomings. Thus we believe that an\nensemble-learning approach combining the different methods should yield a\nbetter performance, which can be our future research focus.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 08:04:39 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Chen", "Hailin", ""], ["Cui", "Shengping", ""], ["Li", "Sebastian", ""]]}, {"id": "1707.02444", "submitter": "Chulhee Yun", "authors": "Chulhee Yun, Suvrit Sra, Ali Jadbabaie", "title": "Global optimality conditions for deep neural networks", "comments": "14 pages. A camera-ready version that will appear at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the error landscape of deep linear and nonlinear neural networks\nwith the squared error loss. Minimizing the loss of a deep linear neural\nnetwork is a nonconvex problem, and despite recent progress, our understanding\nof this loss surface is still incomplete. For deep linear networks, we present\nnecessary and sufficient conditions for a critical point of the risk function\nto be a global minimum. Surprisingly, our conditions provide an efficiently\ncheckable test for global optimality, while such tests are typically\nintractable in nonconvex optimization. We further extend these results to deep\nnonlinear neural networks and prove similar sufficient conditions for global\noptimality, albeit in a more limited function space setting.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 14:04:37 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 03:37:54 GMT"}, {"version": "v3", "created": "Sat, 24 Mar 2018 05:26:13 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Yun", "Chulhee", ""], ["Sra", "Suvrit", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1707.02469", "submitter": "Pau Vilimelis Aceituno", "authors": "Pau Vilimelis Aceituno, Yan Gang, Yang-Yu Liu", "title": "Tailoring Artificial Neural Networks for Optimal Learning", "comments": "19 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the most important paradigms of recurrent neural networks, the echo\nstate network (ESN) has been applied to a wide range of fields, from robotics\nto medicine, finance, and language processing. A key feature of the ESN\nparadigm is its reservoir --- a directed and weighted network of neurons that\nprojects the input time series into a high dimensional space where linear\nregression or classification can be applied. Despite extensive studies, the\nimpact of the reservoir network on the ESN performance remains unclear.\nCombining tools from physics, dynamical systems and network science, we attempt\nto open the black box of ESN and offer insights to understand the behavior of\ngeneral artificial neural networks. Through spectral analysis of the reservoir\nnetwork we reveal a key factor that largely determines the ESN memory capacity\nand hence affects its performance. Moreover, we find that adding short loops to\nthe reservoir network can tailor ESN for specific tasks and optimize learning.\nWe validate our findings by applying ESN to forecast both synthetic and real\nbenchmark time series. Our results provide a new way to design task-specific\nESN. More importantly, it demonstrates the power of combining tools from\nphysics, dynamical systems and network science to offer new insights in\nunderstanding the mechanisms of general artificial neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 17:17:29 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 10:55:15 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2019 20:01:57 GMT"}, {"version": "v4", "created": "Tue, 25 Feb 2020 17:45:30 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Aceituno", "Pau Vilimelis", ""], ["Gang", "Yan", ""], ["Liu", "Yang-Yu", ""]]}, {"id": "1707.02510", "submitter": "Siddhartha Saxena", "authors": "Siddhartha Saxena, Shibhansh Dohare and Jaivardhan Kapoor", "title": "Variational Inference via Transformations on Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference methods often focus on the problem of efficient model\noptimization, with little emphasis on the choice of the approximating\nposterior. In this paper, we review and implement the various methods that\nenable us to develop a rich family of approximating posteriors. We show that\none particular method employing transformations on distributions results in\ndeveloping very rich and complex posterior approximation. We analyze its\nperformance on the MNIST dataset by implementing with a Variational Autoencoder\nand demonstrate its effectiveness in learning better posterior distributions.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 01:25:05 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Saxena", "Siddhartha", ""], ["Dohare", "Shibhansh", ""], ["Kapoor", "Jaivardhan", ""]]}, {"id": "1707.02515", "submitter": "Liting Sun", "authors": "Liting Sun, Cheng Peng, Wei Zhan, Masayoshi Tomizuka", "title": "A Fast Integrated Planning and Control Framework for Autonomous Driving\n  via Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For safe and efficient planning and control in autonomous driving, we need a\ndriving policy which can achieve desirable driving quality in long-term horizon\nwith guaranteed safety and feasibility. Optimization-based approaches, such as\nModel Predictive Control (MPC), can provide such optimal policies, but their\ncomputational complexity is generally unacceptable for real-time\nimplementation. To address this problem, we propose a fast integrated planning\nand control framework that combines learning- and optimization-based approaches\nin a two-layer hierarchical structure. The first layer, defined as the \"policy\nlayer\", is established by a neural network which learns the long-term optimal\ndriving policy generated by MPC. The second layer, called the \"execution\nlayer\", is a short-term optimization-based controller that tracks the reference\ntrajecotries given by the \"policy layer\" with guaranteed short-term safety and\nfeasibility. Moreover, with efficient and highly-representative features, a\nsmall-size neural network is sufficient in the \"policy layer\" to handle many\ncomplicated driving scenarios. This renders online imitation learning with\nDataset Aggregation (DAgger) so that the performance of the \"policy layer\" can\nbe improved rapidly and continuously online. Several exampled driving scenarios\nare demonstrated to verify the effectiveness and efficiency of the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 02:00:21 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Sun", "Liting", ""], ["Peng", "Cheng", ""], ["Zhan", "Wei", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "1707.02530", "submitter": "Anurag Kumar", "authors": "Anurag Kumar, Bhiksha Raj", "title": "Deep CNN Framework for Audio Event Recognition using Weakly Labeled Web\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of audio event recognition models requires labeled training\ndata, which are generally hard to obtain. One promising source of recordings of\naudio events is the large amount of multimedia data on the web. In particular,\nif the audio content analysis must itself be performed on web audio, it is\nimportant to train the recognizers themselves from such data. Training from\nthese web data, however, poses several challenges, the most important being the\navailability of labels : labels, if any, that may be obtained for the data are\ngenerally {\\em weak}, and not of the kind conventionally required for training\ndetectors or classifiers. We propose that learning algorithms that can exploit\nweak labels offer an effective method to learn from web data. We then propose a\nrobust and efficient deep convolutional neural network (CNN) based framework to\nlearn audio event recognizers from weakly labeled data. The proposed method can\ntrain from and analyze recordings of variable length in an efficient manner and\noutperforms a network trained with {\\em strongly labeled} web data by a\nconsiderable margin.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 06:16:23 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 07:33:03 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Kumar", "Anurag", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1707.02568", "submitter": "Jiequn Han", "authors": "Jiequn Han, Arnulf Jentzen, Weinan E", "title": "Solving high-dimensional partial differential equations using deep\n  learning", "comments": "13 pages, 6 figures", "journal-ref": "Proceedings of the National Academy of Sciences, 115(34),\n  8505-8510 (2018)", "doi": "10.1073/pnas.1718942115", "report-no": null, "categories": "math.NA cs.LG math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing algorithms for solving high-dimensional partial differential\nequations (PDEs) has been an exceedingly difficult task for a long time, due to\nthe notoriously difficult problem known as the \"curse of dimensionality\". This\npaper introduces a deep learning-based approach that can handle general\nhigh-dimensional parabolic PDEs. To this end, the PDEs are reformulated using\nbackward stochastic differential equations and the gradient of the unknown\nsolution is approximated by neural networks, very much in the spirit of deep\nreinforcement learning with the gradient acting as the policy function.\nNumerical results on examples including the nonlinear Black-Scholes equation,\nthe Hamilton-Jacobi-Bellman equation, and the Allen-Cahn equation suggest that\nthe proposed algorithm is quite effective in high dimensions, in terms of both\naccuracy and cost. This opens up new possibilities in economics, finance,\noperational research, and physics, by considering all participating agents,\nassets, resources, or particles together at the same time, instead of making ad\nhoc assumptions on their inter-relationships.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 12:05:15 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 01:28:53 GMT"}, {"version": "v3", "created": "Tue, 3 Jul 2018 10:08:29 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Han", "Jiequn", ""], ["Jentzen", "Arnulf", ""], ["E", "Weinan", ""]]}, {"id": "1707.02575", "submitter": "Sun Chong Wang", "authors": "Sun-Chong Wang", "title": "Neural Machine Translation between Herbal Prescriptions and Diseases", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current study applies deep learning to herbalism. Toward the goal, we\nacquired the de-identified health insurance reimbursements that were claimed in\na 10-year period from 2004 to 2013 in the National Health Insurance Database of\nTaiwan, the total number of reimbursement records equaling 340 millions. Two\nartificial intelligence techniques were applied to the dataset: residual\nconvolutional neural network multitask classifier and attention-based recurrent\nneural network. The former works to translate from herbal prescriptions to\ndiseases; and the latter from diseases to herbal prescriptions. Analysis of the\nclassification results indicates that herbal prescriptions are specific to:\nanatomy, pathophysiology, sex and age of the patient, and season and year of\nthe prescription. Further analysis identifies temperature and gross domestic\nproduct as the meteorological and socioeconomic factors that are associated\nwith herbal prescriptions. Analysis of the neural machine transitional result\nindicates that the recurrent neural network learnt not only syntax but also\nsemantics of diseases and herbal prescriptions.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 12:51:47 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Wang", "Sun-Chong", ""]]}, {"id": "1707.02610", "submitter": "Eleni Triantafillou", "authors": "Eleni Triantafillou, Richard Zemel, Raquel Urtasun", "title": "Few-Shot Learning Through an Information Retrieval Lens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning refers to understanding new concepts from only a few\nexamples. We propose an information retrieval-inspired approach for this\nproblem that is motivated by the increased importance of maximally leveraging\nall the available information in this low-data regime. We define a training\nobjective that aims to extract as much information as possible from each\ntraining batch by effectively optimizing over all relative orderings of the\nbatch points simultaneously. In particular, we view each batch point as a\n`query' that ranks the remaining ones based on its predicted relevance to them\nand we define a model within the framework of structured prediction to optimize\nmean Average Precision over these rankings. Our method achieves impressive\nresults on the standard few-shot classification benchmarks while is also\ncapable of few-shot retrieval.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 18:03:07 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 15:51:56 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Triantafillou", "Eleni", ""], ["Zemel", "Richard", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1707.02617", "submitter": "Raul Rojas Prof.", "authors": "Raul Rojas", "title": "Deepest Neural Networks", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that a long chain of perceptrons (that is, a multilayer\nperceptron, or MLP, with many hidden layers of width one) can be a universal\nclassifier. The classification procedure is not necessarily computationally\nefficient, but the technique throws some light on the kind of computations\npossible with narrow and deep MLPs.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 18:34:45 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Rojas", "Raul", ""]]}, {"id": "1707.02649", "submitter": "Shahin Shahrampour", "authors": "Shahin Shahrampour, Vahid Tarokh", "title": "Nonlinear Sequential Accepts and Rejects for Identification of Top Arms\n  in Stochastic Bandits", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the M-best-arm identification problem in multi-armed bandits. A\nplayer has a limited budget to explore K arms (M<K), and once pulled, each arm\nyields a reward drawn (independently) from a fixed, unknown distribution. The\ngoal is to find the top M arms in the sense of expected reward. We develop an\nalgorithm which proceeds in rounds to deactivate arms iteratively. At each\nround, the budget is divided by a nonlinear function of remaining arms, and the\narms are pulled correspondingly. Based on a decision rule, the deactivated arm\nat each round may be accepted or rejected. The algorithm outputs the accepted\narms that should ideally be the top M arms. We characterize the decay rate of\nthe misidentification probability and establish that the nonlinear budget\nallocation proves to be useful for different problem environments (described by\nthe number of competitive arms). We provide comprehensive numerical experiments\nshowing that our algorithm outperforms the state-of-the-art using suitable\nnonlinearity.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 22:32:09 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Shahrampour", "Shahin", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1707.02657", "submitter": "Edilson Anselmo Corr\\^ea J\\'unior", "authors": "Edilson A. Corr\\^ea Jr, Vanessa Q. Marinho, Leandro B. dos Santos,\n  Thales F. C. Bertaglia, Marcos V. Treviso, Henrico B. Brum", "title": "PELESent: Cross-domain polarity classification using distant supervision", "comments": "Accepted for publication in BRACIS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The enormous amount of texts published daily by Internet users has fostered\nthe development of methods to analyze this content in several natural language\nprocessing areas, such as sentiment analysis. The main goal of this task is to\nclassify the polarity of a message. Even though many approaches have been\nproposed for sentiment analysis, some of the most successful ones rely on the\navailability of large annotated corpus, which is an expensive and\ntime-consuming process. In recent years, distant supervision has been used to\nobtain larger datasets. So, inspired by these techniques, in this paper we\nextend such approaches to incorporate popular graphic symbols used in\nelectronic messages, the emojis, in order to create a large sentiment corpus\nfor Portuguese. Trained on almost one million tweets, several models were\ntested in both same domain and cross-domain corpora. Our methods obtained very\ncompetitive results in five annotated corpora from mixed domains (Twitter and\nproduct reviews), which proves the domain-independent property of such\napproach. In addition, our results suggest that the combination of emoticons\nand emojis is able to properly capture the sentiment of a message.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 23:13:58 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Corr\u00eaa", "Edilson A.", "Jr"], ["Marinho", "Vanessa Q.", ""], ["Santos", "Leandro B. dos", ""], ["Bertaglia", "Thales F. C.", ""], ["Treviso", "Marcos V.", ""], ["Brum", "Henrico B.", ""]]}, {"id": "1707.02670", "submitter": "Peng Xu", "authors": "Christopher De Sa, Bryan He, Ioannis Mitliagkas, Christopher R\\'e,\n  Peng Xu", "title": "Accelerated Stochastic Power Iteration", "comments": "37 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is one of the most powerful tools in\nmachine learning. The simplest method for PCA, the power iteration, requires\n$\\mathcal O(1/\\Delta)$ full-data passes to recover the principal component of a\nmatrix with eigen-gap $\\Delta$. Lanczos, a significantly more complex method,\nachieves an accelerated rate of $\\mathcal O(1/\\sqrt{\\Delta})$ passes. Modern\napplications, however, motivate methods that only ingest a subset of available\ndata, known as the stochastic setting. In the online stochastic setting, simple\nalgorithms like Oja's iteration achieve the optimal sample complexity $\\mathcal\nO(\\sigma^2/\\Delta^2)$. Unfortunately, they are fully sequential, and also\nrequire $\\mathcal O(\\sigma^2/\\Delta^2)$ iterations, far from the $\\mathcal\nO(1/\\sqrt{\\Delta})$ rate of Lanczos. We propose a simple variant of the power\niteration with an added momentum term, that achieves both the optimal sample\nand iteration complexity. In the full-pass setting, standard analysis shows\nthat momentum achieves the accelerated rate, $\\mathcal O(1/\\sqrt{\\Delta})$. We\ndemonstrate empirically that naively applying momentum to a stochastic method,\ndoes not result in acceleration. We perform a novel, tight variance analysis\nthat reveals the \"breaking-point variance\" beyond which this acceleration does\nnot occur. By combining this insight with modern variance reduction techniques,\nwe construct stochastic PCA algorithms, for the online and offline setting,\nthat achieve an accelerated iteration complexity $\\mathcal O(1/\\sqrt{\\Delta})$.\nDue to the embarassingly parallel nature of our methods, this acceleration\ntranslates directly to wall-clock time if deployed in a parallel environment.\nOur approach is very general, and applies to many non-convex optimization\nproblems that can now be accelerated using the same technique.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 01:13:33 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["De Sa", "Christopher", ""], ["He", "Bryan", ""], ["Mitliagkas", "Ioannis", ""], ["R\u00e9", "Christopher", ""], ["Xu", "Peng", ""]]}, {"id": "1707.02702", "submitter": "Shuang Song", "authors": "Shuang Song, Kamalika Chaudhuri", "title": "Composition Properties of Inferential Privacy for Time-Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of mobile devices and the internet of things,\ndeveloping principled solutions for privacy in time series applications has\nbecome increasingly important. While differential privacy is the gold standard\nfor database privacy, many time series applications require a different kind of\nguarantee, and a number of recent works have used some form of inferential\nprivacy to address these situations.\n  However, a major barrier to using inferential privacy in practice is its lack\nof graceful composition -- even if the same or related sensitive data is used\nin multiple releases that are safe individually, the combined release may have\npoor privacy properties. In this paper, we study composition properties of a\nform of inferential privacy called Pufferfish when applied to time-series data.\nWe show that while general Pufferfish mechanisms may not compose gracefully, a\nspecific Pufferfish mechanism, called the Markov Quilt Mechanism, which was\nrecently introduced, has strong composition properties comparable to that of\npure differential privacy when applied to time series data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 05:50:59 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Song", "Shuang", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1707.02711", "submitter": "Thomas Wiatowski", "authors": "Thomas Wiatowski and Philipp Grohs and Helmut B\\\"olcskei", "title": "Topology Reduction in Deep Convolutional Feature Extraction Networks", "comments": "Corrected errors in arguments on spectral decay of Sobolev functions.\n  Replaced part of the decay results (Sections 5-7) by corresponding statements\n  for effectively band-limited functions", "journal-ref": "Proc. of SPIE (Wavelets and Sparsity XVII), San Diego, USA, Vol.\n  10394, pp. 1039418:1-1039418:12, Aug. 2017, (invited paper)", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG math.FA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) used in practice employ potentially\nhundreds of layers and $10$,$000$s of nodes. Such network sizes entail\nsignificant computational complexity due to the large number of convolutions\nthat need to be carried out; in addition, a large number of parameters needs to\nbe learned and stored. Very deep and wide CNNs may therefore not be well suited\nto applications operating under severe resource constraints as is the case,\ne.g., in low-power embedded and mobile platforms. This paper aims at\nunderstanding the impact of CNN topology, specifically depth and width, on the\nnetwork's feature extraction capabilities. We address this question for the\nclass of scattering networks that employ either Weyl-Heisenberg filters or\nwavelets, the modulus non-linearity, and no pooling. The exponential feature\nmap energy decay results in Wiatowski et al., 2017, are generalized to\n$\\mathcal{O}(a^{-N})$, where an arbitrary decay factor $a>1$ can be realized\nthrough suitable choice of the Weyl-Heisenberg prototype function or the mother\nwavelet. We then show how networks of fixed (possibly small) depth $N$ can be\ndesigned to guarantee that $((1-\\varepsilon)\\cdot 100)\\%$ of the input signal's\nenergy are contained in the feature vector. Based on the notion of\noperationally significant nodes, we characterize, partly rigorously and partly\nheuristically, the topology-reducing effects of (effectively) band-limited\ninput signals, band-limited filters, and feature map symmetries. Finally, for\nnetworks based on Weyl-Heisenberg filters, we determine the prototype function\nbandwidth that minimizes---for fixed network depth $N$---the average number of\noperationally significant nodes per layer.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 06:35:48 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 08:59:37 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Wiatowski", "Thomas", ""], ["Grohs", "Philipp", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1707.02727", "submitter": "Shuisheng Zhou", "authors": "Li Chen, Shuisheng Zhou, Zhuan Zhang", "title": "Stochastic Variance Reduction Gradient for a Non-convex Problem Using\n  Graduated Optimization", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, nonconvex optimization problems with multiple local\noptimums are often encountered. Graduated Optimization Algorithm (GOA) is a\npopular heuristic method to obtain global optimums of nonconvex problems\nthrough progressively minimizing a series of convex approximations to the\nnonconvex problems more and more accurate. Recently, such an algorithm GradOpt\nbased on GOA is proposed with amazing theoretical and experimental results, but\nit mainly studies the problem which consists of one nonconvex part. This paper\naims to find the global solution of a nonconvex objective with a convex part\nplus a nonconvex part based on GOA. By graduating approximating non-convex part\nof the problem and minimizing them with the Stochastic Variance Reduced\nGradient (SVRG) or proximal SVRG, two new algorithms, SVRG-GOA and PSVRG-GOA,\nare proposed. We prove that the new algorithms have lower iteration complexity\n($O(1/\\varepsilon)$) than GradOpt ($O(1/\\varepsilon^2)$). Some tricks, such as\nenlarging shrink factor, using project step, stochastic gradient, and\nmini-batch skills, are also given to accelerate the convergence speed of the\nproposed algorithms. Experimental results illustrate that the new algorithms\nwith the similar performance can converge to 'global' optimums of the nonconvex\nproblems, and they converge faster than the GradOpt and the nonconvex proximal\nSVRG.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 07:43:36 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Chen", "Li", ""], ["Zhou", "Shuisheng", ""], ["Zhang", "Zhuan", ""]]}, {"id": "1707.02729", "submitter": "Peter Sch\\\"uller", "authors": "Peter Sch\\\"uller and Mishal Benz", "title": "Best-Effort Inductive Logic Programming via Fine-grained Cost-based\n  Hypothesis Generation", "comments": "Submitted to Machine Learning special issue on Inductive Logic\n  Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the Inspire system which participated in the first competition on\nInductive Logic Programming (ILP). Inspire is based on Answer Set Programming\n(ASP). The distinguishing feature of Inspire is an ASP encoding for hypothesis\nspace generation: given a set of facts representing the mode bias, and a set of\ncost configuration parameters, each answer set of this encoding represents a\nsingle rule that is considered for finding a hypothesis that entails the given\nexamples. Compared with state-of-the-art methods that use the length of the\nrule body as a metric for rule complexity, our approach permits a much more\nfine-grained specification of the shape of hypothesis candidate rules. The\nInspire system iteratively increases the rule cost limit and thereby increases\nthe search space until it finds a suitable hypothesis. The system searches for\na hypothesis that entails a single example at a time, utilizing an ASP encoding\nderived from the encoding used in XHAIL. We perform experiments with the\ndevelopment and test set of the ILP competition. For comparison we also adapted\nthe ILASP system to process competition instances. Experimental results show\nthat the cost parameters for the hypothesis search space are an important\nfactor for finding hypotheses to competition instances within tight resource\nbounds.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 07:50:04 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 23:08:07 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Sch\u00fcller", "Peter", ""], ["Benz", "Mishal", ""]]}, {"id": "1707.02747", "submitter": "Ziyu Wang", "authors": "Ziyu Wang, Josh Merel, Scott Reed, Greg Wayne, Nando de Freitas,\n  Nicolas Heess", "title": "Robust Imitation of Diverse Behaviors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have recently shown great promise in imitation\nlearning for motor control. Given enough data, even supervised approaches can\ndo one-shot imitation learning; however, they are vulnerable to cascading\nfailures when the agent trajectory diverges from the demonstrations. Compared\nto purely supervised methods, Generative Adversarial Imitation Learning (GAIL)\ncan learn more robust controllers from fewer demonstrations, but is inherently\nmode-seeking and more difficult to train. In this paper, we show how to combine\nthe favourable aspects of these two approaches. The base of our model is a new\ntype of variational autoencoder on demonstration trajectories that learns\nsemantic policy embeddings. We show that these embeddings can be learned on a 9\nDoF Jaco robot arm in reaching tasks, and then smoothly interpolated with a\nresulting smooth interpolation of reaching behavior. Leveraging these policy\nrepresentations, we develop a new version of GAIL that (1) is much more robust\nthan the purely-supervised controller, especially with few demonstrations, and\n(2) avoids mode collapse, capturing many diverse behaviors when GAIL on its own\ndoes not. We demonstrate our approach on learning diverse gaits from\ndemonstration on a 2D biped and a 62 DoF 3D humanoid in the MuJoCo physics\nenvironment.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 08:46:14 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 09:31:26 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Wang", "Ziyu", ""], ["Merel", "Josh", ""], ["Reed", "Scott", ""], ["Wayne", "Greg", ""], ["de Freitas", "Nando", ""], ["Heess", "Nicolas", ""]]}, {"id": "1707.02757", "submitter": "Damian Straszak", "authors": "Javad B. Ebrahimi and Damian Straszak and Nisheeth K. Vishnoi", "title": "Subdeterminant Maximization via Nonconvex Relaxations and\n  Anti-concentration", "comments": "in FOCS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several fundamental problems that arise in optimization and computer science\ncan be cast as follows: Given vectors $v_1,\\ldots,v_m \\in \\mathbb{R}^d$ and a\nconstraint family ${\\cal B}\\subseteq 2^{[m]}$, find a set $S \\in \\cal{B}$ that\nmaximizes the squared volume of the simplex spanned by the vectors in $S$. A\nmotivating example is the data-summarization problem in machine learning where\none is given a collection of vectors that represent data such as documents or\nimages. The volume of a set of vectors is used as a measure of their diversity,\nand partition or matroid constraints over $[m]$ are imposed in order to ensure\nresource or fairness constraints. Recently, Nikolov and Singh presented a\nconvex program and showed how it can be used to estimate the value of the most\ndiverse set when ${\\cal B}$ corresponds to a partition matroid. This result was\nrecently extended to regular matroids in works of Straszak and Vishnoi, and\nAnari and Oveis Gharan. The question of whether these estimation algorithms can\nbe converted into the more useful approximation algorithms -- that also output\na set -- remained open.\n  The main contribution of this paper is to give the first approximation\nalgorithms for both partition and regular matroids. We present novel\nformulations for the subdeterminant maximization problem for these matroids;\nthis reduces them to the problem of finding a point that maximizes the absolute\nvalue of a nonconvex function over a Cartesian product of probability\nsimplices. The technical core of our results is a new anti-concentration\ninequality for dependent random variables that allows us to relate the optimal\nvalue of these nonconvex functions to their value at a random point. Unlike\nprior work on the constrained subdeterminant maximization problem, our proofs\ndo not rely on real-stability or convexity and could be of independent interest\nboth in algorithms and complexity.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 09:04:51 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 12:13:21 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Ebrahimi", "Javad B.", ""], ["Straszak", "Damian", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1707.02796", "submitter": "Zackory Erickson", "authors": "Zackory Erickson, Sonia Chernova, and Charles C. Kemp", "title": "Semi-Supervised Haptic Material Recognition for Robots using Generative\n  Adversarial Networks", "comments": "11 pages, 6 figures, 6 tables, 1st Conference on Robot Learning (CoRL\n  2017)", "journal-ref": "PMLR 78:157-166 (2017)", "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Material recognition enables robots to incorporate knowledge of material\nproperties into their interactions with everyday objects. For example, material\nrecognition opens up opportunities for clearer communication with a robot, such\nas \"bring me the metal coffee mug\", and recognizing plastic versus metal is\ncrucial when using a microwave or oven. However, collecting labeled training\ndata with a robot is often more difficult than unlabeled data. We present a\nsemi-supervised learning approach for material recognition that uses generative\nadversarial networks (GANs) with haptic features such as force, temperature,\nand vibration. Our approach achieves state-of-the-art results and enables a\nrobot to estimate the material class of household objects with ~90% accuracy\nwhen 92% of the training data are unlabeled. We explore how well this approach\ncan recognize the material of new objects and we discuss challenges facing\ngeneralization. To motivate learning from unlabeled training data, we also\ncompare results against several common supervised learning classifiers. In\naddition, we have released the dataset used for this work which consists of\ntime-series haptic measurements from a robot that conducted thousands of\ninteractions with 72 household objects.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 11:04:42 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 21:39:48 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Erickson", "Zackory", ""], ["Chernova", "Sonia", ""], ["Kemp", "Charles C.", ""]]}, {"id": "1707.02812", "submitter": "Suranjana Samanta", "authors": "Suranjana Samanta, Sameep Mehta", "title": "Towards Crafting Text Adversarial Samples", "comments": "11 pages, 5 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial samples are strategically modified samples, which are crafted\nwith the purpose of fooling a classifier at hand. An attacker introduces\nspecially crafted adversarial samples to a deployed classifier, which are being\nmis-classified by the classifier. However, the samples are perceived to be\ndrawn from entirely different classes and thus it becomes hard to detect the\nadversarial samples. Most of the prior works have been focused on synthesizing\nadversarial samples in the image domain. In this paper, we propose a new method\nof crafting adversarial text samples by modification of the original samples.\nModifications of the original text samples are done by deleting or replacing\nthe important or salient words in the text or by introducing new words in the\ntext sample. Our algorithm works best for the datasets which have\nsub-categories within each of the classes of examples. While crafting\nadversarial samples, one of the key constraint is to generate meaningful\nsentences which can at pass off as legitimate from language (English)\nviewpoint. Experimental results on IMDB movie review dataset for sentiment\nanalysis and Twitter dataset for gender detection show the efficiency of our\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 11:58:08 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Samanta", "Suranjana", ""], ["Mehta", "Sameep", ""]]}, {"id": "1707.02914", "submitter": "Xuehang Zheng", "authors": "Xuehang Zheng, Zening Lu, Saiprasad Ravishankar, Yong Long, Jeffrey A.\n  Fessler", "title": "Low Dose CT Image Reconstruction With Learned Sparsifying Transform", "comments": "This is a revised and corrected version of the IEEE IVMSP Workshop\n  paper DOI: 10.1109/IVMSPW.2016.7528219", "journal-ref": null, "doi": "10.1109/IVMSPW.2016.7528219", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in computed tomography (CT) is to reduce X-ray dose to a\nlow or even ultra-low level while maintaining the high quality of reconstructed\nimages. We propose a new method for CT reconstruction that combines penalized\nweighted-least squares reconstruction (PWLS) with regularization based on a\nsparsifying transform (PWLS-ST) learned from a dataset of numerous CT images.\nWe adopt an alternating algorithm to optimize the PWLS-ST cost function that\nalternates between a CT image update step and a sparse coding step. We adopt a\nrelaxed linearized augmented Lagrangian method with ordered-subsets (relaxed\nOS-LALM) to accelerate the CT image update step by reducing the number of\nforward and backward projections. Numerical experiments on the XCAT phantom\nshow that for low dose levels, the proposed PWLS-ST method dramatically\nimproves the quality of reconstructed images compared to PWLS reconstruction\nwith a nonadaptive edge-preserving regularizer (PWLS-EP).\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 15:42:05 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Zheng", "Xuehang", ""], ["Lu", "Zening", ""], ["Ravishankar", "Saiprasad", ""], ["Long", "Yong", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1707.02920", "submitter": "Rouhollah Rahmatizadeh", "authors": "Rouhollah Rahmatizadeh, Pooya Abolghasemi, Ladislau B\\\"ol\\\"oni, Sergey\n  Levine", "title": "Vision-Based Multi-Task Manipulation for Inexpensive Robots Using\n  End-To-End Learning from Demonstration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a technique for multi-task learning from demonstration that trains\nthe controller of a low-cost robotic arm to accomplish several complex picking\nand placing tasks, as well as non-prehensile manipulation. The controller is a\nrecurrent neural network using raw images as input and generating robot arm\ntrajectories, with the parameters shared across the tasks. The controller also\ncombines VAE-GAN-based reconstruction with autoregressive multimodal action\nprediction. Our results demonstrate that it is possible to learn complex\nmanipulation tasks, such as picking up a towel, wiping an object, and\ndepositing the towel to its previous position, entirely from raw images with\ndirect behavior cloning. We show that weight sharing and reconstruction-based\nregularization substantially improve generalization and robustness, and\ntraining on multiple tasks simultaneously increases the success rate on all\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 16:05:53 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 16:19:28 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Rahmatizadeh", "Rouhollah", ""], ["Abolghasemi", "Pooya", ""], ["B\u00f6l\u00f6ni", "Ladislau", ""], ["Levine", "Sergey", ""]]}, {"id": "1707.02975", "submitter": "Song Wang Dr.", "authors": "Song Wang, Jun Sun, Satoshi Naoi", "title": "On Study of the Reliable Fully Convolutional Networks with Tree Arranged\n  Outputs (TAO-FCN) for Handwritten String Recognition", "comments": "Rejected by ICDAR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The handwritten string recognition is still a challengeable task, though the\npowerful deep learning tools were introduced. In this paper, based on TAO-FCN,\nwe proposed an end-to-end system for handwritten string recognition. Compared\nwith the conventional methods, there is no preprocess nor manually designed\nrules employed. With enough labelled data, it is easy to apply the proposed\nmethod to different applications. Although the performance of the proposed\nmethod may not be comparable with the state-of-the-art approaches, it's\nusability and robustness are more meaningful for practical applications.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 07:34:29 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Wang", "Song", ""], ["Sun", "Jun", ""], ["Naoi", "Satoshi", ""]]}, {"id": "1707.03034", "submitter": "Mohak Bhardwaj", "authors": "Mohak Bhardwaj, Sanjiban Choudhury, Sebastian Scherer", "title": "Learning Heuristic Search via Imitation", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic motion planning problems are typically solved by constructing a\nsearch tree of valid maneuvers from a start to a goal configuration. Limited\nonboard computation and real-time planning constraints impose a limit on how\nlarge this search tree can grow. Heuristics play a crucial role in such\nsituations by guiding the search towards potentially good directions and\nconsequently minimizing search effort. Moreover, it must infer such directions\nin an efficient manner using only the information uncovered by the search up\nuntil that time. However, state of the art methods do not address the problem\nof computing a heuristic that explicitly minimizes search effort. In this\npaper, we do so by training a heuristic policy that maps the partial\ninformation from the search to decide which node of the search tree to expand.\nUnfortunately, naively training such policies leads to slow convergence and\npoor local minima. We present SaIL, an efficient algorithm that trains\nheuristic policies by imitating \"clairvoyant oracles\" - oracles that have full\ninformation about the world and demonstrate decisions that minimize search\neffort. We leverage the fact that such oracles can be efficiently computed\nusing dynamic programming and derive performance guarantees for the learnt\nheuristic. We validate the approach on a spectrum of environments which show\nthat SaIL consistently outperforms state of the art algorithms. Our approach\npaves the way forward for learning heuristics that demonstrate an anytime\nnature - finding feasible solutions quickly and incrementally refining it over\ntime.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 19:36:14 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Bhardwaj", "Mohak", ""], ["Choudhury", "Sanjiban", ""], ["Scherer", "Sebastian", ""]]}, {"id": "1707.03073", "submitter": "Li Zhang", "authors": "Yu Bai and Sally Goldman and Li Zhang", "title": "TAPAS: Two-pass Approximate Adaptive Sampling for Softmax", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TAPAS is a novel adaptive sampling method for the softmax model. It uses a\ntwo pass sampling strategy where the examples used to approximate the gradient\nof the partition function are first sampled according to a squashed population\ndistribution and then resampled adaptively using the context and current model.\nWe describe an efficient distributed implementation of TAPAS. We show, on both\nsynthetic data and a large real dataset, that TAPAS has low computational\noverhead and works well for minimizing the rank loss for multi-class\nclassification problems with a very large label space.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 22:01:44 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 06:50:13 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Bai", "Yu", ""], ["Goldman", "Sally", ""], ["Zhang", "Li", ""]]}, {"id": "1707.03092", "submitter": "Mohammadhussein Rafieisakhaei", "authors": "Dan Yu, Mohammadhussein Rafieisakhaei and Suman Chakravorty", "title": "A Separation-Based Design to Data-Driven Control for Large-Scale\n  Partially Observed Systems", "comments": "3 pages, 6 figures, In Robotics: Science and Systems (RSS) 2017\n  Workshop of \"POMDPs in Robotics: State of The Art, Challenges, and\n  Opportunities\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the partially observed stochastic optimal control problem\nfor systems with state dynamics governed by Partial Differential Equations\n(PDEs) that leads to an extremely large problem. First, an open-loop\ndeterministic trajectory optimization problem is solved using a black box\nsimulation model of the dynamical system. Next, a Linear Quadratic Gaussian\n(LQG) controller is designed for the nominal trajectory-dependent linearized\nsystem, which is identified using input-output experimental data consisting of\nthe impulse responses of the optimized nominal system. A computational\nnonlinear heat example is used to illustrate the performance of the approach.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 01:01:55 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Yu", "Dan", ""], ["Rafieisakhaei", "Mohammadhussein", ""], ["Chakravorty", "Suman", ""]]}, {"id": "1707.03110", "submitter": "Sumarsih Purbarani", "authors": "Sumarsih Condroayu Purbarani, Hadaiq Rolis Sanabila, Wisnu Jatmiko", "title": "Distance-to-Mean Continuous Conditional Random Fields to Enhance\n  Prediction Problem in Traffic Flow Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increase of vehicle in highways may cause traffic congestion as well as\nin the normal roadways. Predicting the traffic flow in highways especially, is\ndemanded to solve this congestion problem. Predictions on time-series\nmultivariate data, such as in the traffic flow dataset, have been largely\naccomplished through various approaches. The approach with conventional\nprediction algorithms, such as with Support Vector Machine (SVM), is only\ncapable of accommodating predictions that are independent in each time unit.\nHence, the sequential relationships in this time series data is hardly\nexplored. Continuous Conditional Random Field (CCRF) is one of Probabilistic\nGraphical Model (PGM) algorithms which can accommodate this problem. The\nneighboring aspects of sequential data such as in the time series data can be\nexpressed by CCRF so that its predictions are more reliable. In this article, a\nnovel approach called DM-CCRF is adopted by modifying the CCRF prediction\nalgorithm to strengthen the probability of the predictions made by the baseline\nregressor. The result shows that DM-CCRF is superior in performance compared to\nCCRF. This is validated by the error decrease of the baseline up to 9%\nsignificance. This is twice the standard CCRF performance which can only\ndecrease baseline error by 4.582% at most.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 02:36:28 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Purbarani", "Sumarsih Condroayu", ""], ["Sanabila", "Hadaiq Rolis", ""], ["Jatmiko", "Wisnu", ""]]}, {"id": "1707.03134", "submitter": "Gautam Ramachandra", "authors": "Gautam Ramachandra", "title": "Least Square Variational Bayesian Autoencoder with Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years Variation Autoencoders have become one of the most popular\nunsupervised learning of complicated distributions.Variational Autoencoder\n(VAE) provides more efficient reconstructive performance over a traditional\nautoencoder. Variational auto enocders make better approximaiton than MCMC. The\nVAE defines a generative process in terms of ancestral sampling through a\ncascade of hidden stochastic layers. They are a directed graphic models.\nVariational autoencoder is trained to maximise the variational lower bound.\nHere we are trying maximise the likelihood and also at the same time we are\ntrying to make a good approximation of the data. Its basically trading of the\ndata log-likelihood and the KL divergence from the true posterior. This paper\ndescribes the scenario in which we wish to find a point-estimate to the\nparameters $\\theta$ of some parametric model in which we generate each\nobservations by first sampling a local latent variable and then sampling the\nassociated observation. Here we use least square loss function with\nregularization in the the reconstruction of the image, the least square loss\nfunction was found to give better reconstructed images and had a faster\ntraining time.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 05:24:01 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Ramachandra", "Gautam", ""]]}, {"id": "1707.03141", "submitter": "Nikhil Mishra", "authors": "Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, Pieter Abbeel", "title": "A Simple Neural Attentive Meta-Learner", "comments": "iclr 2018 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks excel in regimes with large amounts of data, but tend to\nstruggle when data is scarce or when they need to adapt quickly to changes in\nthe task. In response, recent work in meta-learning proposes training a\nmeta-learner on a distribution of similar tasks, in the hopes of generalization\nto novel but related tasks by learning a high-level strategy that captures the\nessence of the problem it is asked to solve. However, many recent meta-learning\napproaches are extensively hand-designed, either using architectures\nspecialized to a particular application, or hard-coding algorithmic components\nthat constrain how the meta-learner solves the task. We propose a class of\nsimple and generic meta-learner architectures that use a novel combination of\ntemporal convolutions and soft attention; the former to aggregate information\nfrom past experience and the latter to pinpoint specific pieces of information.\nIn the most extensive set of meta-learning experiments to date, we evaluate the\nresulting Simple Neural AttentIve Learner (or SNAIL) on several\nheavily-benchmarked tasks. On all tasks, in both supervised and reinforcement\nlearning, SNAIL attains state-of-the-art performance by significant margins.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 06:21:31 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 16:08:03 GMT"}, {"version": "v3", "created": "Sun, 25 Feb 2018 04:55:20 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Mishra", "Nikhil", ""], ["Rohaninejad", "Mostafa", ""], ["Chen", "Xi", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1707.03157", "submitter": "Marek Smieja", "authors": "Marek \\'Smieja, Krzysztof Hajto and Jacek Tabor", "title": "Efficient mixture model for clustering of sparse high dimensional binary\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a mixture model, SparseMix, for clustering of sparse\nhigh dimensional binary data, which connects model-based with centroid-based\nclustering. Every group is described by a representative and a probability\ndistribution modeling dispersion from this representative. In contrast to\nclassical mixture models based on EM algorithm, SparseMix:\n  -is especially designed for the processing of sparse data,\n  -can be efficiently realized by an on-line Hartigan optimization algorithm,\n  -is able to automatically reduce unnecessary clusters.\n  We perform extensive experimental studies on various types of data, which\nconfirm that SparseMix builds partitions with higher compatibility with\nreference grouping than related methods. Moreover, constructed representatives\noften better reveal the internal structure of data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 07:48:57 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["\u015amieja", "Marek", ""], ["Hajto", "Krzysztof", ""], ["Tabor", "Jacek", ""]]}, {"id": "1707.03167", "submitter": "Nick Schneider", "authors": "Nick Schneider, Florian Piewak, Christoph Stiller, Uwe Franke", "title": "RegNet: Multimodal Sensor Registration Using Deep Neural Networks", "comments": "published in IEEE Intelligent Vehicles Symposium, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present RegNet, the first deep convolutional neural network\n(CNN) to infer a 6 degrees of freedom (DOF) extrinsic calibration between\nmultimodal sensors, exemplified using a scanning LiDAR and a monocular camera.\nCompared to existing approaches, RegNet casts all three conventional\ncalibration steps (feature extraction, feature matching and global regression)\ninto a single real-time capable CNN. Our method does not require any human\ninteraction and bridges the gap between classical offline and target-less\nonline calibration approaches as it provides both a stable initial estimation\nas well as a continuous online correction of the extrinsic parameters. During\ntraining we randomly decalibrate our system in order to train RegNet to infer\nthe correspondence between projected depth measurements and RGB image and\nfinally regress the extrinsic calibration. Additionally, with an iterative\nexecution of multiple CNNs, that are trained on different magnitudes of\ndecalibration, our approach compares favorably to state-of-the-art methods in\nterms of a mean calibration error of 0.28 degrees for the rotational and 6 cm\nfor the translation components even for large decalibrations up to 1.5 m and 20\ndegrees.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 08:21:58 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Schneider", "Nick", ""], ["Piewak", "Florian", ""], ["Stiller", "Christoph", ""], ["Franke", "Uwe", ""]]}, {"id": "1707.03184", "submitter": "Atul Kumar", "authors": "Atul Kumar, Sameep Mehta", "title": "A Survey on Resilient Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning based system are increasingly being used for sensitive tasks\nsuch as security surveillance, guiding autonomous vehicle, taking investment\ndecisions, detecting and blocking network intrusion and malware etc. However,\nrecent research has shown that machine learning models are venerable to attacks\nby adversaries at all phases of machine learning (eg, training data collection,\ntraining, operation). All model classes of machine learning systems can be\nmisled by providing carefully crafted inputs making them wrongly classify\ninputs. Maliciously created input samples can affect the learning process of a\nML system by either slowing down the learning process, or affecting the\nperformance of the learned mode, or causing the system make error(s) only in\nattacker's planned scenario. Because of these developments, understanding\nsecurity of machine learning algorithms and systems is emerging as an important\nresearch area among computer security and machine learning researchers and\npractitioners. We present a survey of this emerging area in machine learning.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 09:15:46 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Kumar", "Atul", ""], ["Mehta", "Sameep", ""]]}, {"id": "1707.03190", "submitter": "Yuanyuan Liu", "authors": "Yuanyuan Liu, Fanhua Shang, James Cheng", "title": "Accelerated Variance Reduced Stochastic ADMM", "comments": "16 pages, 5 figures, Appears in Proceedings of the 31th AAAI\n  Conference on Artificial Intelligence (AAAI), San Francisco, California, USA,\n  pp. 2287--2293, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many variance reduced stochastic alternating direction method of\nmultipliers (ADMM) methods (e.g.\\ SAG-ADMM, SDCA-ADMM and SVRG-ADMM) have made\nexciting progress such as linear convergence rates for strongly convex\nproblems. However, the best known convergence rate for general convex problems\nis O(1/T) as opposed to O(1/T^2) of accelerated batch algorithms, where $T$ is\nthe number of iterations. Thus, there still remains a gap in convergence rates\nbetween existing stochastic ADMM and batch algorithms. To bridge this gap, we\nintroduce the momentum acceleration trick for batch optimization into the\nstochastic variance reduced gradient based ADMM (SVRG-ADMM), which leads to an\naccelerated (ASVRG-ADMM) method. Then we design two different momentum term\nupdate rules for strongly convex and general convex cases. We prove that\nASVRG-ADMM converges linearly for strongly convex problems. Besides having a\nlow per-iteration complexity as existing stochastic ADMM methods, ASVRG-ADMM\nimproves the convergence rate on general convex problems from O(1/T) to\nO(1/T^2). Our experimental results show the effectiveness of ASVRG-ADMM.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 09:29:46 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Liu", "Yuanyuan", ""], ["Shang", "Fanhua", ""], ["Cheng", "James", ""]]}, {"id": "1707.03191", "submitter": "Sergio Consoli", "authors": "Sergio Consoli, Jacek Kustra, Pieter Vos, Monique Hendriks, Dimitrios\n  Mavroeidis", "title": "Towards an automated method based on Iterated Local Search optimization\n  for tuning the parameters of Support Vector Machines", "comments": "3 pages, Benelearn 2017 conference, Eindhoven", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide preliminary details and formulation of an optimization strategy\nunder current development that is able to automatically tune the parameters of\na Support Vector Machine over new datasets. The optimization strategy is a\nheuristic based on Iterated Local Search, a modification of classic hill\nclimbing which iterates calls to a local search routine.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 09:29:58 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Consoli", "Sergio", ""], ["Kustra", "Jacek", ""], ["Vos", "Pieter", ""], ["Hendriks", "Monique", ""], ["Mavroeidis", "Dimitrios", ""]]}, {"id": "1707.03213", "submitter": "Xingyuan Dai", "authors": "Xingyuan Dai, Rui Fu, Yilun Lin, Li Li, Fei-Yue Wang", "title": "DeepTrend: A Deep Hierarchical Neural Network for Traffic Flow\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the temporal pattern in traffic flow time series,\nand implement a deep learning model for traffic flow prediction. Detrending\nbased methods decompose original flow series into trend and residual series, in\nwhich trend describes the fixed temporal pattern in traffic flow and residual\nseries is used for prediction. Inspired by the detrending method, we propose\nDeepTrend, a deep hierarchical neural network used for traffic flow prediction\nwhich considers and extracts the time-variant trend. DeepTrend has two stacked\nlayers: extraction layer and prediction layer. Extraction layer, a fully\nconnected layer, is used to extract the time-variant trend in traffic flow by\nfeeding the original flow series concatenated with corresponding simple average\ntrend series. Prediction layer, an LSTM layer, is used to make flow prediction\nby feeding the obtained trend from the output of extraction layer and\ncalculated residual series. To make the model more effective, DeepTrend needs\nfirst pre-trained layer-by-layer and then fine-tuned in the entire network.\nExperiments show that DeepTrend can noticeably boost the prediction performance\ncompared with some traditional prediction models and LSTM with detrending based\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 10:51:42 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Dai", "Xingyuan", ""], ["Fu", "Rui", ""], ["Lin", "Yilun", ""], ["Li", "Li", ""], ["Wang", "Fei-Yue", ""]]}, {"id": "1707.03311", "submitter": "Gil Shabat", "authors": "Yariv Aizenbud, Amir Averbuch, Gil Shabat and Guy Ziv", "title": "Similarity Search Over Graphs Using Localized Spectral Analysis", "comments": "Published in SampTA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a new similarity detection algorithm. Given an input set\nof multi-dimensional data points, where each data point is assumed to be\nmulti-dimensional, and an additional reference data point for similarity\nfinding, the algorithm uses kernel method that embeds the data points into a\nlow dimensional manifold. Unlike other kernel methods, which consider the\nentire data for the embedding, our method selects a specific set of kernel\neigenvectors. The eigenvectors are chosen to separate between the data points\nand the reference data point so that similar data points can be easily\nidentified as being distinct from most of the members in the dataset.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 15:03:57 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Aizenbud", "Yariv", ""], ["Averbuch", "Amir", ""], ["Shabat", "Gil", ""], ["Ziv", "Guy", ""]]}, {"id": "1707.03324", "submitter": "Zhiqiang Zhou", "authors": "Guanghui Lan and Zhiqiang Zhou", "title": "Dynamic Stochastic Approximation for Multi-stage Stochastic Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider multi-stage stochastic optimization problems with\nconvex objectives and conic constraints at each stage. We present a new\nstochastic first-order method, namely the dynamic stochastic approximation\n(DSA) algorithm, for solving these types of stochastic optimization problems.\nWe show that DSA can achieve an optimal ${\\cal O}(1/\\epsilon^4)$ rate of\nconvergence in terms of the total number of required scenarios when applied to\na three-stage stochastic optimization problem. We further show that this rate\nof convergence can be improved to ${\\cal O}(1/\\epsilon^2)$ when the objective\nfunction is strongly convex. We also discuss variants of DSA for solving more\ngeneral multi-stage stochastic optimization problems with the number of stages\n$T > 3$. The developed DSA algorithms only need to go through the scenario tree\nonce in order to compute an $\\epsilon$-solution of the multi-stage stochastic\noptimization problem. As a result, the memory required by DSA only grows\nlinearly with respect to the number of stages. To the best of our knowledge,\nthis is the first time that stochastic approximation type methods are\ngeneralized for multi-stage stochastic optimization with $T \\ge 3$.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 15:29:55 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 22:30:13 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Lan", "Guanghui", ""], ["Zhou", "Zhiqiang", ""]]}, {"id": "1707.03340", "submitter": "Bao Wang", "authors": "Bao Wang, Duo Zhang, Duanhao Zhang, P.Jeffery Brantingham, Andrea L.\n  Bertozzi", "title": "Deep Learning for Real Time Crime Forecasting", "comments": "4 pages, 6 figures, NOLTA, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate real time crime prediction is a fundamental issue for public safety,\nbut remains a challenging problem for the scientific community. Crime\noccurrences depend on many complex factors. Compared to many predictable\nevents, crime is sparse. At different spatio-temporal scales, crime\ndistributions display dramatically different patterns. These distributions are\nof very low regularity in both space and time. In this work, we adapt the\nstate-of-the-art deep learning spatio-temporal predictor, ST-ResNet [Zhang et\nal, AAAI, 2017], to collectively predict crime distribution over the Los\nAngeles area. Our models are two staged. First, we preprocess the raw crime\ndata. This includes regularization in both space and time to enhance\npredictable signals. Second, we adapt hierarchical structures of residual\nconvolutional units to train multi-factor crime prediction models. Experiments\nover a half year period in Los Angeles reveal highly accurate predictive power\nof our models.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 17:36:53 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Wang", "Bao", ""], ["Zhang", "Duo", ""], ["Zhang", "Duanhao", ""], ["Brantingham", "P. Jeffery", ""], ["Bertozzi", "Andrea L.", ""]]}, {"id": "1707.03372", "submitter": "Stephen Mussmann", "authors": "Stephen Mussmann, Daniel Levy, Stefano Ermon", "title": "Fast Amortized Inference and Learning in Log-linear Models with Randomly\n  Perturbed Nearest Neighbor Search", "comments": "In UAI proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference in log-linear models scales linearly in the size of output space in\nthe worst-case. This is often a bottleneck in natural language processing and\ncomputer vision tasks when the output space is feasibly enumerable but very\nlarge. We propose a method to perform inference in log-linear models with\nsublinear amortized cost. Our idea hinges on using Gumbel random variable\nperturbations and a pre-computed Maximum Inner Product Search data structure to\naccess the most-likely elements in sublinear amortized time. Our method yields\nprovable runtime and accuracy guarantees. Further, we present empirical\nexperiments on ImageNet and Word Embeddings showing significant speedups for\nsampling, inference, and learning in log-linear models.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 17:23:10 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Mussmann", "Stephen", ""], ["Levy", "Daniel", ""], ["Ermon", "Stefano", ""]]}, {"id": "1707.03374", "submitter": "Abhishek Gupta", "authors": "YuXuan Liu, Abhishek Gupta, Pieter Abbeel, Sergey Levine", "title": "Imitation from Observation: Learning to Imitate Behaviors from Raw Video\n  via Context Translation", "comments": "Accepted at ICRA 2018, Brisbane. YuXuan Liu and Abhishek Gupta had\n  equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning is an effective approach for autonomous systems to acquire\ncontrol policies when an explicit reward function is unavailable, using\nsupervision provided as demonstrations from an expert, typically a human\noperator. However, standard imitation learning methods assume that the agent\nreceives examples of observation-action tuples that could be provided, for\ninstance, to a supervised learning algorithm. This stands in contrast to how\nhumans and animals imitate: we observe another person performing some behavior\nand then figure out which actions will realize that behavior, compensating for\nchanges in viewpoint, surroundings, object positions and types, and other\nfactors. We term this kind of imitation learning \"imitation-from-observation,\"\nand propose an imitation learning method based on video prediction with context\ntranslation and deep reinforcement learning. This lifts the assumption in\nimitation learning that the demonstration should consist of observations in the\nsame environment configuration, and enables a variety of interesting\napplications, including learning robotic skills that involve tool use simply by\nobserving videos of human tool use. Our experimental results show the\neffectiveness of our approach in learning a wide range of real-world robotic\ntasks modeled after common household chores from videos of a human\ndemonstrator, including sweeping, ladling almonds, pushing objects as well as a\nnumber of tasks in simulation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 17:23:53 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 21:00:13 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Liu", "YuXuan", ""], ["Gupta", "Abhishek", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1707.03377", "submitter": "Qunzhi Zhang", "authors": "Qunzhi Zhang and Didier Sornette", "title": "Learning like humans with Deep Symbolic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cond-mat.dis-nn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Deep Symbolic Network (DSN) model, which aims at becoming\nthe white-box version of Deep Neural Networks (DNN). The DSN model provides a\nsimple, universal yet powerful structure, similar to DNN, to represent any\nknowledge of the world, which is transparent to humans. The conjecture behind\nthe DSN model is that any type of real world objects sharing enough common\nfeatures are mapped into human brains as a symbol. Those symbols are connected\nby links, representing the composition, correlation, causality, or other\nrelationships between them, forming a deep, hierarchical symbolic network\nstructure. Powered by such a structure, the DSN model is expected to learn like\nhumans, because of its unique characteristics. First, it is universal, using\nthe same structure to store any knowledge. Second, it can learn symbols from\nthe world and construct the deep symbolic networks automatically, by utilizing\nthe fact that real world objects have been naturally separated by\nsingularities. Third, it is symbolic, with the capacity of performing causal\ndeduction and generalization. Fourth, the symbols and the links between them\nare transparent to us, and thus we will know what it has learned or not - which\nis the key for the security of an AI system. Fifth, its transparency enables it\nto learn with relatively small data. Sixth, its knowledge can be accumulated.\nLast but not least, it is more friendly to unsupervised learning than DNN. We\npresent the details of the model, the algorithm powering its automatic learning\nability, and describe its usefulness in different use cases. The purpose of\nthis paper is to generate broad interest to develop it within an open source\nproject centered on the Deep Symbolic Network (DSN) model towards the\ndevelopment of general AI.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 17:29:51 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 12:00:38 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Zhang", "Qunzhi", ""], ["Sornette", "Didier", ""]]}, {"id": "1707.03386", "submitter": "Ali Mousavi", "authors": "Ali Mousavi, Gautam Dasarathy, Richard G. Baraniuk", "title": "DeepCodec: Adaptive Sensing and Recovery via Deep Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a novel computational sensing framework for sensing\nand recovering structured signals. When trained on a set of representative\nsignals, our framework learns to take undersampled measurements and recover\nsignals from them using a deep convolutional neural network. In other words, it\nlearns a transformation from the original signals to a near-optimal number of\nundersampled measurements and the inverse transformation from measurements to\nsignals. This is in contrast to traditional compressive sensing (CS) systems\nthat use random linear measurements and convex optimization or iterative\nalgorithms for signal recovery. We compare our new framework with\n$\\ell_1$-minimization from the phase transition point of view and demonstrate\nthat it outperforms $\\ell_1$-minimization in the regions of phase transition\nplot where $\\ell_1$-minimization cannot recover the exact solution. In\naddition, we experimentally demonstrate how learning measurements enhances the\noverall recovery performance, speeds up training of recovery framework, and\nleads to having fewer parameters to learn.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 17:49:20 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Mousavi", "Ali", ""], ["Dasarathy", "Gautam", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1707.03389", "submitter": "Irina Higgins", "authors": "Irina Higgins, Nicolas Sonnerat, Loic Matthey, Arka Pal, Christopher P\n  Burgess, Matko Bosnjak, Murray Shanahan, Matthew Botvinick, Demis Hassabis,\n  Alexander Lerchner", "title": "SCAN: Learning Hierarchical Compositional Visual Concepts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seemingly infinite diversity of the natural world arises from a\nrelatively small set of coherent rules, such as the laws of physics or\nchemistry. We conjecture that these rules give rise to regularities that can be\ndiscovered through primarily unsupervised experiences and represented as\nabstract concepts. If such representations are compositional and hierarchical,\nthey can be recombined into an exponentially large set of new concepts. This\npaper describes SCAN (Symbol-Concept Association Network), a new framework for\nlearning such abstractions in the visual domain. SCAN learns concepts through\nfast symbol association, grounding them in disentangled visual primitives that\nare discovered in an unsupervised manner. Unlike state of the art multimodal\ngenerative model baselines, our approach requires very few pairings between\nsymbols and images and makes no assumptions about the form of symbol\nrepresentations. Once trained, SCAN is capable of multimodal bi-directional\ninference, generating a diverse set of image samples from symbolic descriptions\nand vice versa. It also allows for traversal and manipulation of the implicit\nhierarchy of visual concepts through symbolic instructions and learnt logical\nrecombination operations. Such manipulations enable SCAN to break away from its\ntraining data distribution and imagine novel visual concepts through\nsymbolically instructed recombination of previously learnt concepts.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 17:58:45 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 16:46:31 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2018 17:25:01 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Higgins", "Irina", ""], ["Sonnerat", "Nicolas", ""], ["Matthey", "Loic", ""], ["Pal", "Arka", ""], ["Burgess", "Christopher P", ""], ["Bosnjak", "Matko", ""], ["Shanahan", "Murray", ""], ["Botvinick", "Matthew", ""], ["Hassabis", "Demis", ""], ["Lerchner", "Alexander", ""]]}, {"id": "1707.03426", "submitter": "Niloofar Yousefi", "authors": "Niloofar Yousefi, Cong Li, Mansooreh Mollaghasemi, Georgios\n  Anagnostopoulos and Michael Georgiopoulos", "title": "Multi-Task Learning Using Neighborhood Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new and effective algorithm for learning kernels in a\nMulti-Task Learning (MTL) setting. Although, we consider a MTL scenario here,\nour approach can be easily applied to standard single task learning, as well.\nAs shown by our empirical results, our algorithm consistently outperforms the\ntraditional kernel learning algorithms such as uniform combination solution,\nconvex combinations of base kernels as well as some kernel alignment-based\nmodels, which have been proven to give promising results in the past. We\npresent a Rademacher complexity bound based on which a new Multi-Task Multiple\nKernel Learning (MT-MKL) model is derived. In particular, we propose a Support\nVector Machine-regularized model in which, for each task, an optimal kernel is\nlearned based on a neighborhood-defining kernel that is not restricted to be\npositive semi-definite. Comparative experimental results are showcased that\nunderline the merits of our neighborhood-defining framework in both\nclassification and regression problems.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 18:43:41 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Yousefi", "Niloofar", ""], ["Li", "Cong", ""], ["Mollaghasemi", "Mansooreh", ""], ["Anagnostopoulos", "Georgios", ""], ["Georgiopoulos", "Michael", ""]]}, {"id": "1707.03450", "submitter": "Cristobal Silva", "authors": "Iv\\'an Castro, Crist\\'obal Silva, Felipe Tobar", "title": "Initialising Kernel Adaptive Filters via Probabilistic Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic framework for both (i) determining the initial\nsettings of kernel adaptive filters (KAFs) and (ii) constructing fully-adaptive\nKAFs whereby in addition to weights and dictionaries, kernel parameters are\nlearnt sequentially. This is achieved by formulating the estimator as a\nprobabilistic model and defining dedicated prior distributions over the kernel\nparameters, weights and dictionary, enforcing desired properties such as\nsparsity. The model can then be trained using a subset of data to initialise\nstandard KAFs or updated sequentially each time a new observation becomes\navailable. Due to the nonlinear/non-Gaussian properties of the model, learning\nand inference is achieved using gradient-based maximum-a-posteriori\noptimisation and Markov chain Monte Carlo methods, and can be confidently used\nto compute predictions. The proposed framework was validated on nonlinear time\nseries of both synthetic and real-world nature, where it outperformed standard\nKAFs in terms of mean square error and the sparsity of the learnt dictionaries.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 20:03:31 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Castro", "Iv\u00e1n", ""], ["Silva", "Crist\u00f3bal", ""], ["Tobar", "Felipe", ""]]}, {"id": "1707.03469", "submitter": "Evgeny Burnaev", "authors": "Alexander Kuleshov, Alexander Bernstein, Evgeny Burnaev, Yury Yanovich", "title": "Machine Learning in Appearance-based Robot Self-localization", "comments": "7 pages, 3 figures, ICMLA 2017 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An appearance-based robot self-localization problem is considered in the\nmachine learning framework. The appearance space is composed of all possible\nimages, which can be captured by a robot's visual system under all robot\nlocalizations. Using recent manifold learning and deep learning techniques, we\npropose a new geometrically motivated solution based on training data\nconsisting of a finite set of images captured in known locations of the robot.\nThe solution includes estimation of the robot localization mapping from the\nappearance space to the robot localization space, as well as estimation of the\ninverse mapping for modeling visual image features. The latter allows solving\nthe robot localization problem as the Kalman filtering problem.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 15:32:53 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 22:17:24 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Kuleshov", "Alexander", ""], ["Bernstein", "Alexander", ""], ["Burnaev", "Evgeny", ""], ["Yanovich", "Yury", ""]]}, {"id": "1707.03497", "submitter": "Junhyuk Oh", "authors": "Junhyuk Oh, Satinder Singh, Honglak Lee", "title": "Value Prediction Network", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel deep reinforcement learning (RL) architecture,\ncalled Value Prediction Network (VPN), which integrates model-free and\nmodel-based RL methods into a single neural network. In contrast to typical\nmodel-based RL methods, VPN learns a dynamics model whose abstract states are\ntrained to make option-conditional predictions of future values (discounted sum\nof rewards) rather than of future observations. Our experimental results show\nthat VPN has several advantages over both model-free and model-based baselines\nin a stochastic environment where careful planning is required but building an\naccurate observation-prediction model is difficult. Furthermore, VPN\noutperforms Deep Q-Network (DQN) on several Atari games even with\nshort-lookahead planning, demonstrating its potential as a new way of learning\na good state representation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 23:32:36 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 23:55:47 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Oh", "Junhyuk", ""], ["Singh", "Satinder", ""], ["Lee", "Honglak", ""]]}, {"id": "1707.03502", "submitter": "Jindong Wang", "authors": "Jindong Wang, Yiqiang Chen, Shuji Hao, Xiaohui Peng and Lisha Hu", "title": "Deep Learning for Sensor-based Activity Recognition: A Survey", "comments": "10 pages, 2 figures, and 5 tables; submitted to Pattern Recognition\n  Letters (second revision)", "journal-ref": null, "doi": "10.1016/j.patrec.2018.02.010", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensor-based activity recognition seeks the profound high-level knowledge\nabout human activities from multitudes of low-level sensor readings.\nConventional pattern recognition approaches have made tremendous progress in\nthe past years. However, those methods often heavily rely on heuristic\nhand-crafted feature extraction, which could hinder their generalization\nperformance. Additionally, existing methods are undermined for unsupervised and\nincremental learning tasks. Recently, the recent advancement of deep learning\nmakes it possible to perform automatic high-level feature extraction thus\nachieves promising performance in many areas. Since then, deep learning based\nmethods have been widely adopted for the sensor-based activity recognition\ntasks. This paper surveys the recent advance of deep learning based\nsensor-based activity recognition. We summarize existing literature from three\naspects: sensor modality, deep model, and application. We also present detailed\ninsights on existing work and propose grand challenges for future research.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 00:21:04 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 03:11:15 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Wang", "Jindong", ""], ["Chen", "Yiqiang", ""], ["Hao", "Shuji", ""], ["Peng", "Xiaohui", ""], ["Hu", "Lisha", ""]]}, {"id": "1707.03505", "submitter": "Benjamin Grimmer", "authors": "Damek Davis, Benjamin Grimmer", "title": "Proximally Guided Stochastic Subgradient Method for Nonsmooth, Nonconvex\n  Problems", "comments": "Updated 9/17/2018: Major Revision -added high probability bounds,\n  improved convergence analysis in general, new experimental results. Updated\n  7/26/2017: Added references to introduction and a couple simple extensions as\n  Sections 3.2 and 4. Updated 8/23/2017: Added NSF acknowledgements. Updated\n  10/16/2017: Added experimental results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a stochastic projected subgradient method for\nweakly convex (i.e., uniformly prox-regular) nonsmooth, nonconvex functions---a\nwide class of functions which includes the additive and convex composite\nclasses. At a high-level, the method is an inexact proximal point iteration in\nwhich the strongly convex proximal subproblems are quickly solved with a\nspecialized stochastic projected subgradient method. The primary contribution\nof this paper is a simple proof that the proposed algorithm converges at the\nsame rate as the stochastic gradient method for smooth nonconvex problems. This\nresult appears to be the first convergence rate analysis of a stochastic (or\neven deterministic) subgradient method for the class of weakly convex\nfunctions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 00:35:43 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 00:30:49 GMT"}, {"version": "v3", "created": "Thu, 24 Aug 2017 04:45:32 GMT"}, {"version": "v4", "created": "Tue, 17 Oct 2017 06:51:31 GMT"}, {"version": "v5", "created": "Tue, 18 Sep 2018 03:46:41 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Davis", "Damek", ""], ["Grimmer", "Benjamin", ""]]}, {"id": "1707.03538", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen and Faicel Chamroukhi", "title": "An Introduction to the Practical and Theoretical Aspects of\n  Mixture-of-Experts Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture-of-experts (MoE) models are a powerful paradigm for modeling of data\narising from complex data generating processes (DGPs). In this article, we\ndemonstrate how different MoE models can be constructed to approximate the\nunderlying DGPs of arbitrary types of data. Due to the probabilistic nature of\nMoE models, we propose the maximum quasi-likelihood (MQL) estimator as a method\nfor estimating MoE model parameters from data, and we provide conditions under\nwhich MQL estimators are consistent and asymptotically normal. The blockwise\nminorization-maximizatoin (blockwise-MM) algorithm framework is proposed as an\nall-purpose method for constructing algorithms for obtaining MQL estimators. An\nexample derivation of a blockwise-MM algorithm is provided. We then present a\nmethod for constructing information criteria for estimating the number of\ncomponents in MoE models and provide justification for the classic Bayesian\ninformation criterion (BIC). We explain how MoE models can be used to conduct\nclassification, clustering, and regression and we illustrate these applications\nvia a pair of worked examples.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 04:44:14 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Nguyen", "Hien D.", ""], ["Chamroukhi", "Faicel", ""]]}, {"id": "1707.03548", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Yong Xu, Ling Shao, Jian Yang", "title": "Discriminative Block-Diagonal Representation Learning for Image\n  Recognition", "comments": "Accepted by TNNLS, and the matlab codes are available at\n  https://sites.google.com/site/darrenzz219/Home/publication", "journal-ref": null, "doi": "10.1109/TNNLS.2017.2712801", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing block-diagonal representation researches mainly focuses on casting\nblock-diagonal regularization on training data, while only little attention is\ndedicated to concurrently learning both block-diagonal representations of\ntraining and test data. In this paper, we propose a discriminative\nblock-diagonal low-rank representation (BDLRR) method for recognition. In\nparticular, the elaborate BDLRR is formulated as a joint optimization problem\nof shrinking the unfavorable representation from off-block-diagonal elements\nand strengthening the compact block-diagonal representation under the\nsemi-supervised framework of low-rank representation. To this end, we first\nimpose penalty constraints on the negative representation to eliminate the\ncorrelation between different classes such that the incoherence criterion of\nthe extra-class representation is boosted. Moreover, a constructed subspace\nmodel is developed to enhance the self-expressive power of training samples and\nfurther build the representation bridge between the training and test samples,\nsuch that the coherence of the learned intra-class representation is\nconsistently heightened. Finally, the resulting optimization problem is solved\nelegantly by employing an alternative optimization strategy, and a simple\nrecognition algorithm on the learned representation is utilized for final\nprediction. Extensive experimental results demonstrate that the proposed method\nachieves superb recognition results on four face image datasets, three\ncharacter datasets, and the fifteen scene multi-categories dataset. It not only\nshows superior potential on image recognition but also outperforms\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 05:33:57 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Zhang", "Zheng", ""], ["Xu", "Yong", ""], ["Shao", "Ling", ""], ["Yang", "Jian", ""]]}, {"id": "1707.03569", "submitter": "Georgios Balikas", "authors": "Georgios Balikas, Simon Moura, Massih-Reza Amini", "title": "Multitask Learning for Fine-Grained Twitter Sentiment Analysis", "comments": "International ACM SIGIR Conference on Research and Development in\n  Information Retrieval 2017", "journal-ref": null, "doi": "10.1145/3077136.3080702", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional sentiment analysis approaches tackle problems like ternary\n(3-category) and fine-grained (5-category) classification by learning the tasks\nseparately. We argue that such classification tasks are correlated and we\npropose a multitask approach based on a recurrent neural network that benefits\nby jointly learning them. Our study demonstrates the potential of multitask\nmodels on this type of problems and improves the state-of-the-art results in\nthe fine-grained sentiment classification problem.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 07:17:50 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Balikas", "Georgios", ""], ["Moura", "Simon", ""], ["Amini", "Massih-Reza", ""]]}, {"id": "1707.03631", "submitter": "Sungrae Park", "authors": "Sungrae Park, Jun-Keon Park, Su-Jin Shin, Il-Chul Moon", "title": "Adversarial Dropout for Supervised and Semi-supervised Learning", "comments": "submitted to AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the training with adversarial examples, which are generated by\nadding a small but worst-case perturbation on input examples, has been proved\nto improve generalization performance of neural networks. In contrast to the\nindividually biased inputs to enhance the generality, this paper introduces\nadversarial dropout, which is a minimal set of dropouts that maximize the\ndivergence between the outputs from the network with the dropouts and the\ntraining supervisions. The identified adversarial dropout are used to\nreconfigure the neural network to train, and we demonstrated that training on\nthe reconfigured sub-network improves the generalization performance of\nsupervised and semi-supervised learning tasks on MNIST and CIFAR-10. We\nanalyzed the trained model to reason the performance improvement, and we found\nthat adversarial dropout increases the sparsity of neural networks more than\nthe standard dropout does.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 10:25:57 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 00:36:45 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Park", "Sungrae", ""], ["Park", "Jun-Keon", ""], ["Shin", "Su-Jin", ""], ["Moon", "Il-Chul", ""]]}, {"id": "1707.03634", "submitter": "Yi Luo", "authors": "Yi Luo, Zhuo Chen, Nima Mesgarani", "title": "Speaker-independent Speech Separation with Deep Attractor Network", "comments": "IEEE/ACM Transactions on Audio, Speech and Language Processing\n  (TASLP), Volume 26 Issue 4, April 2018, Page 787-796", "journal-ref": "IEEE/ACM Transactions on Audio, Speech and Language Processing\n  (TASLP), Volume 26 Issue 4, April 2018, Page 787-796", "doi": "10.1109/TASLP.2018.2795749", "report-no": null, "categories": "cs.SD cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite the recent success of deep learning for many speech processing tasks,\nsingle-microphone, speaker-independent speech separation remains challenging\nfor two main reasons. The first reason is the arbitrary order of the target and\nmasker speakers in the mixture permutation problem, and the second is the\nunknown number of speakers in the mixture output dimension problem. We propose\na novel deep learning framework for speech separation that addresses both of\nthese issues. We use a neural network to project the time-frequency\nrepresentation of the mixture signal into a high-dimensional embedding space. A\nreference point attractor is created in the embedding space to represent each\nspeaker which is defined as the centroid of the speaker in the embedding space.\nThe time-frequency embeddings of each speaker are then forced to cluster around\nthe corresponding attractor point which is used to determine the time-frequency\nassignment of the speaker. We propose three methods for finding the attractors\nfor each source in the embedding space and compare their advantages and\nlimitations. The objective function for the network is standard signal\nreconstruction error which enables end-to-end operation during both training\nand test phases. We evaluated our system using the Wall Street Journal dataset\nWSJ0 on two and three speaker mixtures and report comparable or better\nperformance than other state-of-the-art deep learning methods for speech\nseparation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 10:32:32 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 00:00:32 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2018 02:31:09 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Luo", "Yi", ""], ["Chen", "Zhuo", ""], ["Mesgarani", "Nima", ""]]}, {"id": "1707.03663", "submitter": "Niladri Chatterji", "authors": "Xiang Cheng, Niladri S. Chatterji, Peter L. Bartlett and Michael I.\n  Jordan", "title": "Underdamped Langevin MCMC: A non-asymptotic analysis", "comments": "23 pages; Correction to Corollary 7", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the underdamped Langevin diffusion when the log of the target\ndistribution is smooth and strongly concave. We present a MCMC algorithm based\non its discretization and show that it achieves $\\varepsilon$ error (in\n2-Wasserstein distance) in $\\mathcal{O}(\\sqrt{d}/\\varepsilon)$ steps. This is a\nsignificant improvement over the best known rate for overdamped Langevin MCMC,\nwhich is $\\mathcal{O}(d/\\varepsilon^2)$ steps under the same\nsmoothness/concavity assumptions.\n  The underdamped Langevin MCMC scheme can be viewed as a version of\nHamiltonian Monte Carlo (HMC) which has been observed to outperform overdamped\nLangevin MCMC methods in a number of application areas. We provide quantitative\nrates that support this empirical wisdom.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 12:08:55 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 04:23:36 GMT"}, {"version": "v3", "created": "Thu, 2 Nov 2017 20:55:26 GMT"}, {"version": "v4", "created": "Sat, 11 Nov 2017 20:06:25 GMT"}, {"version": "v5", "created": "Mon, 1 Jan 2018 23:24:29 GMT"}, {"version": "v6", "created": "Tue, 16 Jan 2018 19:28:32 GMT"}, {"version": "v7", "created": "Fri, 26 Jan 2018 21:56:51 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Cheng", "Xiang", ""], ["Chatterji", "Niladri S.", ""], ["Bartlett", "Peter L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1707.03682", "submitter": "Yuzhi Wang", "authors": "Yuzhi Wang and Anqi Yang and Xiaoming Chen and Pengjun Wang and Yu\n  Wang and Huazhong Yang", "title": "A Deep Learning Approach for Blind Drift Calibration of Sensor Networks", "comments": null, "journal-ref": "IEEE Sensors Journal, 17 (2017), 4158-4171", "doi": "10.1109/JSEN.2017.2703885", "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal drift of sensory data is a severe problem impacting the data quality\nof wireless sensor networks (WSNs). With the proliferation of large-scale and\nlong-term WSNs, it is becoming more important to calibrate sensors when the\nground truth is unavailable. This problem is called \"blind calibration\". In\nthis paper, we propose a novel deep learning method named projection-recovery\nnetwork (PRNet) to blindly calibrate sensor measurements online. The PRNet\nfirst projects the drifted data to a feature space, and uses a powerful deep\nconvolutional neural network to recover the estimated drift-free measurements.\nWe deploy a 24-sensor testbed and provide comprehensive empirical evidence\nshowing that the proposed method significantly improves the sensing accuracy\nand drifted sensor detection. Compared with previous methods, PRNet can\ncalibrate 2x of drifted sensors at the recovery rate of 80% under the same\nlevel of accuracy requirement. We also provide helpful insights for designing\ndeep neural networks for sensor calibration. We hope our proposed simple and\neffective approach will serve as a solid baseline in blind drift calibration of\nsensor networks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 17:10:13 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Wang", "Yuzhi", ""], ["Yang", "Anqi", ""], ["Chen", "Xiaoming", ""], ["Wang", "Pengjun", ""], ["Wang", "Yu", ""], ["Yang", "Huazhong", ""]]}, {"id": "1707.03718", "submitter": "Abhishek Chaurasia", "authors": "Abhishek Chaurasia and Eugenio Culurciello", "title": "LinkNet: Exploiting Encoder Representations for Efficient Semantic\n  Segmentation", "comments": "5 pages, 5 figures, GitHub: https://github.com/e-lab/LinkNet", "journal-ref": null, "doi": "10.1109/VCIP.2017.8305148", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixel-wise semantic segmentation for visual scene understanding not only\nneeds to be accurate, but also efficient in order to find any use in real-time\napplication. Existing algorithms even though are accurate but they do not focus\non utilizing the parameters of neural network efficiently. As a result they are\nhuge in terms of parameters and number of operations; hence slow too. In this\npaper, we propose a novel deep neural network architecture which allows it to\nlearn without any significant increase in number of parameters. Our network\nuses only 11.5 million parameters and 21.2 GFLOPs for processing an image of\nresolution 3x640x360. It gives state-of-the-art performance on CamVid and\ncomparable results on Cityscapes dataset. We also compare our networks\nprocessing time on NVIDIA GPU and embedded system device with existing\nstate-of-the-art architectures for different image resolutions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 20:37:17 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Chaurasia", "Abhishek", ""], ["Culurciello", "Eugenio", ""]]}, {"id": "1707.03770", "submitter": "Adithya M Devraj", "authors": "Adithya M. Devraj and Sean P. Meyn", "title": "Fastest Convergence for Q-learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Zap Q-learning algorithm introduced in this paper is an improvement of\nWatkins' original algorithm and recent competitors in several respects. It is a\nmatrix-gain algorithm designed so that its asymptotic variance is optimal.\nMoreover, an ODE analysis suggests that the transient behavior is a close match\nto a deterministic Newton-Raphson implementation. This is made possible by a\ntwo time-scale update equation for the matrix gain sequence.\n  The analysis suggests that the approach will lead to stable and efficient\ncomputation even for non-ideal parameterized settings. Numerical experiments\nconfirm the quick convergence, even in such non-ideal cases.\n  A secondary goal of this paper is tutorial. The first half of the paper\ncontains a survey on reinforcement learning algorithms, with a focus on minimum\nvariance algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 15:44:22 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 18:38:35 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Devraj", "Adithya M.", ""], ["Meyn", "Sean P.", ""]]}, {"id": "1707.03804", "submitter": "Hao Tan", "authors": "Hao Tan, Mohit Bansal", "title": "Source-Target Inference Models for Spatial Instruction Understanding", "comments": "Accepted to AAAI 2018 (8 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models that can execute natural language instructions for situated robotic\ntasks such as assembly and navigation have several useful applications in\nhomes, offices, and remote scenarios. We study the semantics of\nspatially-referred configuration and arrangement instructions, based on the\nchallenging Bisk-2016 blank-labeled block dataset. This task involves finding a\nsource block and moving it to the target position (mentioned via a reference\nblock and offset), where the blocks have no names or colors and are just\nreferred to via spatial location features. We present novel models for the\nsubtasks of source block classification and target position regression, based\non joint-loss language and spatial-world representation learning, as well as\nCNN-based and dual attention models to compute the alignment between the world\nblocks and the instruction phrases. For target position prediction, we compare\ntwo inference approaches: annealed sampling via policy gradient versus\nexpectation inference via supervised regression. Our models achieve the new\nstate-of-the-art on this task, with an improvement of 47% on source block\naccuracy and 22% on target position distance.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 17:15:57 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 16:57:02 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "1707.03815", "submitter": "Aleksandar Bojchevski", "authors": "Aleksandar Bojchevski, Stephan G\\\"unnemann", "title": "Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via\n  Ranking", "comments": "Updated: ICLR 2018 camera-ready version", "journal-ref": "International Conference on Learning Representations, ICLR 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods that learn representations of nodes in a graph play a critical role\nin network analysis since they enable many downstream learning tasks. We\npropose Graph2Gauss - an approach that can efficiently learn versatile node\nembeddings on large scale (attributed) graphs that show strong performance on\ntasks such as link prediction and node classification. Unlike most approaches\nthat represent nodes as point vectors in a low-dimensional continuous space, we\nembed each node as a Gaussian distribution, allowing us to capture uncertainty\nabout the representation. Furthermore, we propose an unsupervised method that\nhandles inductive learning scenarios and is applicable to different types of\ngraphs: plain/attributed, directed/undirected. By leveraging both the network\nstructure and the associated node attributes, we are able to generalize to\nunseen nodes without additional training. To learn the embeddings we adopt a\npersonalized ranking formulation w.r.t. the node distances that exploits the\nnatural ordering of the nodes imposed by the network structure. Experiments on\nreal world networks demonstrate the high performance of our approach,\noutperforming state-of-the-art network embedding methods on several different\ntasks. Additionally, we demonstrate the benefits of modeling uncertainty - by\nanalyzing it we can estimate neighborhood diversity and detect the intrinsic\nlatent dimensionality of a graph.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 17:54:04 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 17:04:01 GMT"}, {"version": "v3", "created": "Mon, 30 Oct 2017 13:32:40 GMT"}, {"version": "v4", "created": "Tue, 27 Feb 2018 10:20:09 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Bojchevski", "Aleksandar", ""], ["G\u00fcnnemann", "Stephan", ""]]}, {"id": "1707.03821", "submitter": "David Tolpin", "authors": "Michael Dymshits, Ben Myara, David Tolpin", "title": "Process Monitoring on Sequences of System Call Count Vectors", "comments": "5 pages, 4 figures, ICCST 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a methodology for efficient monitoring of processes running on\nhosts in a corporate network. The methodology is based on collecting streams of\nsystem calls produced by all or selected processes on the hosts, and sending\nthem over the network to a monitoring server, where machine learning algorithms\nare used to identify changes in process behavior due to malicious activity,\nhardware failures, or software errors. The methodology uses a sequence of\nsystem call count vectors as the data format which can handle large and varying\nvolumes of data.\n  Unlike previous approaches, the methodology introduced in this paper is\nsuitable for distributed collection and processing of data in large corporate\nnetworks. We evaluate the methodology both in a laboratory setting on a\nreal-life setup and provide statistics characterizing performance and accuracy\nof the methodology.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 13:14:43 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Dymshits", "Michael", ""], ["Myara", "Ben", ""], ["Tolpin", "David", ""]]}, {"id": "1707.03848", "submitter": "Yan Zhang", "authors": "Yan Zhang, G. M. Dilshan Godaliyadda, Nicola Ferrier, Emine B. Gulsoy,\n  Charles A. Bouman, Charudatta Phatak", "title": "Reduced Electron Exposure for Energy-Dispersive Spectroscopy using\n  Dynamic Sampling", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytical electron microscopy and spectroscopy of biological specimens,\npolymers, and other beam sensitive materials has been a challenging area due to\nirradiation damage. There is a pressing need to develop novel imaging and\nspectroscopic imaging methods that will minimize such sample damage as well as\nreduce the data acquisition time. The latter is useful for high-throughput\nanalysis of materials structure and chemistry. In this work, we present a novel\nmachine learning based method for dynamic sparse sampling of EDS data using a\nscanning electron microscope. Our method, based on the supervised learning\napproach for dynamic sampling algorithm and neural networks based\nclassification of EDS data, allows a dramatic reduction in the total sampling\nof up to 90%, while maintaining the fidelity of the reconstructed elemental\nmaps and spectroscopic data. We believe this approach will enable imaging and\nelemental mapping of materials that would otherwise be inaccessible to these\nanalysis techniques.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 15:05:14 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Zhang", "Yan", ""], ["Godaliyadda", "G. M. Dilshan", ""], ["Ferrier", "Nicola", ""], ["Gulsoy", "Emine B.", ""], ["Bouman", "Charles A.", ""], ["Phatak", "Charudatta", ""]]}, {"id": "1707.03854", "submitter": "Aditi Raghunathan", "authors": "Aditi Raghunathan, Greg Valiant, James Zou", "title": "Estimating the unseen from multiple populations", "comments": "13 pages, 3 figures, appearing at the International Conference on\n  Machine Learning 2017 (ICML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given samples from a distribution, how many new elements should we expect to\nfind if we continue sampling this distribution? This is an important and\nactively studied problem, with many applications ranging from unseen species\nestimation to genomics. We generalize this extrapolation and related unseen\nestimation problems to the multiple population setting, where population $j$\nhas an unknown distribution $D_j$ from which we observe $n_j$ samples. We\nderive an optimal estimator for the total number of elements we expect to find\namong new samples across the populations. Surprisingly, we prove that our\nestimator's accuracy is independent of the number of populations. We also\ndevelop an efficient optimization algorithm to solve the more general problem\nof estimating multi-population frequency distributions. We validate our methods\nand theory through extensive experiments. Finally, on a real dataset of human\ngenomes across multiple ancestries, we demonstrate how our approach for unseen\nestimation can enable cohort designs that can discover interesting mutations\nwith greater efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 18:26:19 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Raghunathan", "Aditi", ""], ["Valiant", "Greg", ""], ["Zou", "James", ""]]}, {"id": "1707.03904", "submitter": "Bhuwan Dhingra", "authors": "Bhuwan Dhingra, Kathryn Mazaitis and William W. Cohen", "title": "Quasar: Datasets for Question Answering by Search and Reading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two new large-scale datasets aimed at evaluating systems designed\nto comprehend a natural language query and extract its answer from a large\ncorpus of text. The Quasar-S dataset consists of 37000 cloze-style\n(fill-in-the-gap) queries constructed from definitions of software entity tags\non the popular website Stack Overflow. The posts and comments on the website\nserve as the background corpus for answering the cloze questions. The Quasar-T\ndataset consists of 43000 open-domain trivia questions and their answers\nobtained from various internet sources. ClueWeb09 serves as the background\ncorpus for extracting these answers. We pose these datasets as a challenge for\ntwo related subtasks of factoid Question Answering: (1) searching for relevant\npieces of text that include the correct answer to a query, and (2) reading the\nretrieved text to answer the query. We also describe a retrieval system for\nextracting relevant sentences and documents from the corpus given a query, and\ninclude these in the release for researchers wishing to only focus on (2). We\nevaluate several baselines on both datasets, ranging from simple heuristics to\npowerful neural models, and show that these lag behind human performance by\n16.4% and 32.1% for Quasar-S and -T respectively. The datasets are available at\nhttps://github.com/bdhingra/quasar .\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 20:53:26 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 01:48:08 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Dhingra", "Bhuwan", ""], ["Mazaitis", "Kathryn", ""], ["Cohen", "William W.", ""]]}, {"id": "1707.03905", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev, Pavel Erofeev, Artem Papanov", "title": "Influence of Resampling on Accuracy of Imbalanced Classification", "comments": "5 pages, 2 figures, Eighth International Conference on Machine Vision\n  (December 8, 2015)", "journal-ref": "Proc. SPIE9875, 2015", "doi": "10.1117/12.2228523", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world binary classification tasks (e.g. detection of certain\nobjects from images), an available dataset is imbalanced, i.e., it has much\nless representatives of a one class (a minor class), than of another.\nGenerally, accurate prediction of the minor class is crucial but it's hard to\nachieve since there is not much information about the minor class. One approach\nto deal with this problem is to preliminarily resample the dataset, i.e., add\nnew elements to the dataset or remove existing ones. Resampling can be done in\nvarious ways which raises the problem of choosing the most appropriate one. In\nthis paper we experimentally investigate impact of resampling on classification\naccuracy, compare resampling methods and highlight key points and difficulties\nof resampling.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 20:55:22 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Erofeev", "Pavel", ""], ["Papanov", "Artem", ""]]}, {"id": "1707.03909", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev, Pavel Erofeev, Dmitry Smolyakov", "title": "Model Selection for Anomaly Detection", "comments": "6 pages, 3 figures, Eighth International Conference on Machine Vision\n  (December 8, 2015)", "journal-ref": "Proc. SPIE 9875, 2015", "doi": "10.1117/12.2228794", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection based on one-class classification algorithms is broadly\nused in many applied domains like image processing (e.g. detection of whether a\npatient is \"cancerous\" or \"healthy\" from mammography image), network intrusion\ndetection, etc. Performance of an anomaly detection algorithm crucially depends\non a kernel, used to measure similarity in a feature space. The standard\napproaches (e.g. cross-validation) for kernel selection, used in two-class\nclassification problems, can not be used directly due to the specific nature of\na data (absence of a second, abnormal, class data). In this paper we generalize\nseveral kernel selection methods from binary-class case to the case of\none-class classification and perform extensive comparison of these approaches\nusing both synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 21:03:36 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Erofeev", "Pavel", ""], ["Smolyakov", "Dmitry", ""]]}, {"id": "1707.03938", "submitter": "Michael Janner", "authors": "Michael Janner, Karthik Narasimhan, Regina Barzilay", "title": "Representation Learning for Grounded Spatial Reasoning", "comments": "Accepted to TACL 2017, code:\n  https://github.com/jannerm/spatial-reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation of spatial references is highly contextual, requiring\njoint inference over both language and the environment. We consider the task of\nspatial reasoning in a simulated environment, where an agent can act and\nreceive rewards. The proposed model learns a representation of the world\nsteered by instruction text. This design allows for precise alignment of local\nneighborhoods with corresponding verbalizations, while also handling global\nreferences in the instructions. We train our model with reinforcement learning\nusing a variant of generalized value iteration. The model outperforms\nstate-of-the-art approaches on several metrics, yielding a 45% reduction in\ngoal localization error.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 00:17:45 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 02:20:54 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Janner", "Michael", ""], ["Narasimhan", "Karthik", ""], ["Barzilay", "Regina", ""]]}, {"id": "1707.03979", "submitter": "Marc Pickett", "authors": "Marc Pickett, Ayush Sekhari, James Davidson", "title": "A Brief Study of In-Domain Transfer and Learning from Fewer Samples\n  using A Few Simple Priors", "comments": "Accepted for ICML 2017 Workshop on Picky Learners", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain knowledge can often be encoded in the structure of a network, such as\nconvolutional layers for vision, which has been shown to increase\ngeneralization and decrease sample complexity, or the number of samples\nrequired for successful learning. In this study, we ask whether sample\ncomplexity can be reduced for systems where the structure of the domain is\nunknown beforehand, and the structure and parameters must both be learned from\nthe data. We show that sample complexity reduction through learning structure\nis possible for at least two simple cases. In studying these cases, we also\ngain insight into how this might be done for more complex domains.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 04:56:24 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Pickett", "Marc", ""], ["Sekhari", "Ayush", ""], ["Davidson", "James", ""]]}, {"id": "1707.03986", "submitter": "Cheng Li", "authors": "Yue He, Kaidi Cao, Cheng Li and Chen Change Loy", "title": "Merge or Not? Learning to Group Faces via Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Given a large number of unlabeled face images, face grouping aims at\nclustering the images into individual identities present in the data. This task\nremains a challenging problem despite the remarkable capability of deep\nlearning approaches in learning face representation. In particular, grouping\nresults can still be egregious given profile faces and a large number of\nuninteresting faces and noisy detections. Often, a user needs to correct the\nerroneous grouping manually. In this study, we formulate a novel face grouping\nframework that learns clustering strategy from ground-truth simulated behavior.\nThis is achieved through imitation learning (a.k.a apprenticeship learning or\nlearning by watching) via inverse reinforcement learning (IRL). In contrast to\nexisting clustering approaches that group instances by similarity, our\nframework makes sequential decision to dynamically decide when to merge two\nface instances/groups driven by short- and long-term rewards. Extensive\nexperiments on three benchmark datasets show that our framework outperforms\nunsupervised and supervised baselines.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 06:16:43 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["He", "Yue", ""], ["Cao", "Kaidi", ""], ["Li", "Cheng", ""], ["Loy", "Chen Change", ""]]}, {"id": "1707.04025", "submitter": "Marco Loog", "authors": "Marco Loog, Jesse H. Krijthe, Are C. Jensen", "title": "On Measuring and Quantifying Performance: Error Rates, Surrogate Loss,\n  and an Example in SSL", "comments": null, "journal-ref": "In Handbook of Pattern Recognition and Computer Vision (pp. 53-68)\n  (2016)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various approaches to learning, notably in domain adaptation, active\nlearning, learning under covariate shift, semi-supervised learning, learning\nwith concept drift, and the like, one often wants to compare a baseline\nclassifier to one or more advanced (or at least different) strategies. In this\nchapter, we basically argue that if such classifiers, in their respective\ntraining phases, optimize a so-called surrogate loss that it may also be\nvaluable to compare the behavior of this loss on the test set, next to the\nregular classification error rate. It can provide us with an additional view on\nthe classifiers' relative performances that error rates cannot capture. As an\nexample, limited but convincing empirical results demonstrates that we may be\nable to find semi-supervised learning strategies that can guarantee performance\nimprovements with increasing numbers of unlabeled data in terms of\nlog-likelihood. In contrast, the latter may be impossible to guarantee for the\nclassification error rate.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 08:29:00 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Loog", "Marco", ""], ["Krijthe", "Jesse H.", ""], ["Jensen", "Are C.", ""]]}, {"id": "1707.04035", "submitter": "Simone Scardapane", "authors": "Simone Scardapane, Steven Van Vaerenbergh, Simone Totaro, Aurelio\n  Uncini", "title": "Kafnets: kernel-based non-parametric activation functions for neural\n  networks", "comments": "Preprint submitted to Neural Networks (Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are generally built by interleaving (adaptable) linear layers\nwith (fixed) nonlinear activation functions. To increase their flexibility,\nseveral authors have proposed methods for adapting the activation functions\nthemselves, endowing them with varying degrees of flexibility. None of these\napproaches, however, have gained wide acceptance in practice, and research in\nthis topic remains open. In this paper, we introduce a novel family of flexible\nactivation functions that are based on an inexpensive kernel expansion at every\nneuron. Leveraging over several properties of kernel-based models, we propose\nmultiple variations for designing and initializing these kernel activation\nfunctions (KAFs), including a multidimensional scheme allowing to nonlinearly\ncombine information from different paths in the network. The resulting KAFs can\napproximate any mapping defined over a subset of the real line, either convex\nor nonconvex. Furthermore, they are smooth over their entire domain, linear in\ntheir parameters, and they can be regularized using any known scheme, including\nthe use of $\\ell_1$ penalties to enforce sparseness. To the best of our\nknowledge, no other known model satisfies all these properties simultaneously.\nIn addition, we provide a relatively complete overview on alternative\ntechniques for adapting the activation functions, which is currently lacking in\nthe literature. A large set of experiments validates our proposal.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 09:22:01 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 11:33:32 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Scardapane", "Simone", ""], ["Van Vaerenbergh", "Steven", ""], ["Totaro", "Simone", ""], ["Uncini", "Aurelio", ""]]}, {"id": "1707.04041", "submitter": "Christoph David Hofer M.Sc.", "authors": "Christoph Hofer and Roland Kwitt and Marc Niethammer and Andreas Uhl", "title": "Deep Learning with Topological Signatures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring topological and geometrical information from data can offer an\nalternative perspective on machine learning problems. Methods from topological\ndata analysis, e.g., persistent homology, enable us to obtain such information,\ntypically in the form of summary representations of topological features.\nHowever, such topological signatures often come with an unusual structure\n(e.g., multisets of intervals) that is highly impractical for most machine\nlearning techniques. While many strategies have been proposed to map these\ntopological signatures into machine learning compatible representations, they\nsuffer from being agnostic to the target learning task. In contrast, we propose\na technique that enables us to input topological signatures to deep neural\nnetworks and learn a task-optimal representation during training. Our approach\nis realized as a novel input layer with favorable theoretical properties.\nClassification experiments on 2D object shapes and social network graphs\ndemonstrate the versatility of the approach and, in case of the latter, we even\noutperform the state-of-the-art by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 09:36:05 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 11:38:53 GMT"}, {"version": "v3", "created": "Fri, 16 Feb 2018 07:12:19 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Hofer", "Christoph", ""], ["Kwitt", "Roland", ""], ["Niethammer", "Marc", ""], ["Uhl", "Andreas", ""]]}, {"id": "1707.04046", "submitter": "Ben Usman", "authors": "Ben Usman, Kate Saenko, Brian Kulis", "title": "Stable Distribution Alignment Using the Dual of the Adversarial Distance", "comments": "ICLR 2018 Conference Invite to Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods that align distributions by minimizing an adversarial distance\nbetween them have recently achieved impressive results. However, these\napproaches are difficult to optimize with gradient descent and they often do\nnot converge well without careful hyperparameter tuning and proper\ninitialization. We investigate whether turning the adversarial min-max problem\ninto an optimization problem by replacing the maximization part with its dual\nimproves the quality of the resulting alignment and explore its connections to\nMaximum Mean Discrepancy. Our empirical results suggest that using the dual\nformulation for the restricted family of linear discriminators results in a\nmore stable convergence to a desirable solution when compared with the\nperformance of a primal min-max GAN-like objective and an MMD objective under\nthe same restrictions. We test our hypothesis on the problem of aligning two\nsynthetic point clouds on a plane and on a real-image domain adaptation problem\non digits. In both cases, the dual formulation yields an iterative procedure\nthat gives more stable and monotonic improvement over time.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 09:50:14 GMT"}, {"version": "v2", "created": "Sat, 28 Oct 2017 02:28:56 GMT"}, {"version": "v3", "created": "Mon, 8 Jan 2018 19:32:26 GMT"}, {"version": "v4", "created": "Tue, 30 Jan 2018 20:49:21 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Usman", "Ben", ""], ["Saenko", "Kate", ""], ["Kulis", "Brian", ""]]}, {"id": "1707.04131", "submitter": "Jonas Rauber", "authors": "Jonas Rauber, Wieland Brendel, Matthias Bethge", "title": "Foolbox: A Python toolbox to benchmark the robustness of machine\n  learning models", "comments": "Code and examples available at https://github.com/bethgelab/foolbox\n  and documentation available at http://foolbox.readthedocs.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even todays most advanced machine learning models are easily fooled by almost\nimperceptible perturbations of their inputs. Foolbox is a new Python package to\ngenerate such adversarial perturbations and to quantify and compare the\nrobustness of machine learning models. It is build around the idea that the\nmost comparable robustness measure is the minimum perturbation needed to craft\nan adversarial example. To this end, Foolbox provides reference implementations\nof most published adversarial attack methods alongside some new ones, all of\nwhich perform internal hyperparameter tuning to find the minimum adversarial\nperturbation. Additionally, Foolbox interfaces with most popular deep learning\nframeworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows\ndifferent adversarial criteria such as targeted misclassification and top-k\nmisclassification as well as different distance measures. The code is licensed\nunder the MIT license and is openly available at\nhttps://github.com/bethgelab/foolbox . The most up-to-date documentation can be\nfound at http://foolbox.readthedocs.io .\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 13:59:15 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 03:22:01 GMT"}, {"version": "v3", "created": "Tue, 20 Mar 2018 10:10:10 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Rauber", "Jonas", ""], ["Brendel", "Wieland", ""], ["Bethge", "Matthias", ""]]}, {"id": "1707.04175", "submitter": "Razvan Pascanu", "authors": "Yee Whye Teh, Victor Bapst, Wojciech Marian Czarnecki, John Quan,\n  James Kirkpatrick, Raia Hadsell, Nicolas Heess, Razvan Pascanu", "title": "Distral: Robust Multitask Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most deep reinforcement learning algorithms are data inefficient in complex\nand rich environments, limiting their applicability to many scenarios. One\ndirection for improving data efficiency is multitask learning with shared\nneural network parameters, where efficiency may be improved through transfer\nacross related tasks. In practice, however, this is not usually observed,\nbecause gradients from different tasks can interfere negatively, making\nlearning unstable and sometimes even less data efficient. Another issue is the\ndifferent reward schemes between tasks, which can easily lead to one task\ndominating the learning of a shared model. We propose a new approach for joint\ntraining of multiple tasks, which we refer to as Distral (Distill & transfer\nlearning). Instead of sharing parameters between the different workers, we\npropose to share a \"distilled\" policy that captures common behaviour across\ntasks. Each worker is trained to solve its own task while constrained to stay\nclose to the shared policy, while the shared policy is trained by distillation\nto be the centroid of all task policies. Both aspects of the learning process\nare derived by optimizing a joint objective function. We show that our approach\nsupports efficient transfer on complex 3D environments, outperforming several\nrelated methods. Moreover, the proposed learning process is more robust and\nmore stable---attributes that are critical in deep reinforcement learning.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 15:24:20 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Teh", "Yee Whye", ""], ["Bapst", "Victor", ""], ["Czarnecki", "Wojciech Marian", ""], ["Quan", "John", ""], ["Kirkpatrick", "James", ""], ["Hadsell", "Raia", ""], ["Heess", "Nicolas", ""], ["Pascanu", "Razvan", ""]]}, {"id": "1707.04199", "submitter": "Anders Oland", "authors": "Anders Oland and Aayush Bansal and Roger B. Dannenberg and Bhiksha Raj", "title": "Be Careful What You Backpropagate: A Case For Linear Output Activations\n  & Gradient Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we show that saturating output activation functions, such as\nthe softmax, impede learning on a number of standard classification tasks.\nMoreover, we present results showing that the utility of softmax does not stem\nfrom the normalization, as some have speculated. In fact, the normalization\nmakes things worse. Rather, the advantage is in the exponentiation of error\ngradients. This exponential gradient boosting is shown to speed up convergence\nand improve generalization. To this end, we demonstrate faster convergence and\nbetter performance on diverse classification tasks: image classification using\nCIFAR-10 and ImageNet, and semantic segmentation using PASCAL VOC 2012. In the\nlatter case, using the state-of-the-art neural network architecture, the model\nconverged 33% faster with our method (roughly two days of training less) than\nwith the standard softmax activation, and with a slightly better performance to\nboot.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 16:19:09 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Oland", "Anders", ""], ["Bansal", "Aayush", ""], ["Dannenberg", "Roger B.", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1707.04218", "submitter": "Yanpeng Li", "authors": "Yanpeng Li", "title": "Learning Features from Co-occurrences: A Theoretical Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing a word by its co-occurrences with other words in context is an\neffective way to capture the meaning of the word. However, the theory behind\nremains a challenge. In this work, taking the example of a word classification\ntask, we give a theoretical analysis of the approaches that represent a word X\nby a function f(P(C|X)), where C is a context feature, P(C|X) is the\nconditional probability estimated from a text corpus, and the function f maps\nthe co-occurrence measure to a prediction score. We investigate the impact of\ncontext feature C and the function f. We also explain the reasons why using the\nco-occurrences with multiple context features may be better than just using a\nsingle one. In addition, some of the results shed light on the theory of\nfeature learning and machine learning in general.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 16:46:50 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Li", "Yanpeng", ""]]}, {"id": "1707.04236", "submitter": "Felipe Tobar", "authors": "Felipe Tobar", "title": "Improving Sparsity in Kernel Adaptive Filters Using a Unit-Norm\n  Dictionary", "comments": "Accepted at the IEEE Digital Signal Processing conference 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel adaptive filters, a class of adaptive nonlinear time-series models,\nare known by their ability to learn expressive autoregressive patterns from\nsequential data. However, for trivial monotonic signals, they struggle to\nperform accurate predictions and at the same time keep computational complexity\nwithin desired boundaries. This is because new observations are incorporated to\nthe dictionary when they are far from what the algorithm has seen in the past.\nWe propose a novel approach to kernel adaptive filtering that compares new\nobservations against dictionary samples in terms of their unit-norm\n(normalised) versions, meaning that new observations that look like previous\nsamples but have a different magnitude are not added to the dictionary. We\nachieve this by proposing the unit-norm Gaussian kernel and define a\nsparsification criterion for this novel kernel. This new methodology is\nvalidated on two real-world datasets against standard KAF in terms of the\nnormalised mean square error and the dictionary size.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 17:37:46 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Tobar", "Felipe", ""]]}, {"id": "1707.04291", "submitter": "An Yan", "authors": "An Yan, Michael J. Lee, Andrew J. Ko", "title": "Predicting Abandonment in Online Coding Tutorials", "comments": "Accepted to IEEE Symposium on Visual Languages and Human-Centric\n  Computing (VL/HCC), 2017", "journal-ref": null, "doi": "10.1109/VLHCC.2017.8103467", "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learners regularly abandon online coding tutorials when they get bored or\nfrustrated, but there are few techniques for anticipating this abandonment to\nintervene. In this paper, we examine the feasibility of predicting abandonment\nwith machine-learned classifiers. Using interaction logs from an online\nprogramming game, we extracted a collection of features that are potentially\nrelated to learner abandonment and engagement, then developed classifiers for\neach level. Across the first five levels of the game, our classifiers\nsuccessfully predicted 61% to 76% of learners who did not complete the next\nlevel, achieving an average AUC of 0.68. In these classifiers, features\nnegatively associated with abandonment included account activation and\nhelp-seeking behaviors, whereas features positively associated with abandonment\nincluded features indicating difficulty and disengagement. These findings\nhighlight the feasibility of providing timely intervention to learners likely\nto quit.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 19:55:00 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Yan", "An", ""], ["Lee", "Michael J.", ""], ["Ko", "Andrew J.", ""]]}, {"id": "1707.04300", "submitter": "Gautam Dasarathy", "authors": "Gautam Dasarathy, Elchanan Mossel, Robert Nowak, Sebastien Roch", "title": "Coalescent-based species tree estimation: a stochastic Farris transform", "comments": "Submitted. 49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR math.ST q-bio.PE stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconstruction of a species phylogeny from genomic data faces two\nsignificant hurdles: 1) the trees describing the evolution of each individual\ngene--i.e., the gene trees--may differ from the species phylogeny and 2) the\nmolecular sequences corresponding to each gene often provide limited\ninformation about the gene trees themselves. In this paper we consider an\napproach to species tree reconstruction that addresses both these hurdles.\nSpecifically, we propose an algorithm for phylogeny reconstruction under the\nmultispecies coalescent model with a standard model of site substitution. The\nmultispecies coalescent is commonly used to model gene tree discordance due to\nincomplete lineage sorting, a well-studied population-genetic effect.\n  In previous work, an information-theoretic trade-off was derived in this\ncontext between the number of loci, $m$, needed for an accurate reconstruction\nand the length of the locus sequences, $k$. It was shown that to reconstruct an\ninternal branch of length $f$, one needs $m$ to be of the order of $1/[f^{2}\n\\sqrt{k}]$. That previous result was obtained under the molecular clock\nassumption, i.e., under the assumption that mutation rates (as well as\npopulation sizes) are constant across the species phylogeny.\n  Here we generalize this result beyond the restrictive molecular clock\nassumption, and obtain a new reconstruction algorithm that has the same data\nrequirement (up to log factors). Our main contribution is a novel reduction to\nthe molecular clock case under the multispecies coalescent. As a corollary, we\nalso obtain a new identifiability result of independent interest: for any\nspecies tree with $n \\geq 3$ species, the rooted species tree can be identified\nfrom the distribution of its unrooted weighted gene trees even in the absence\nof a molecular clock.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 20:22:35 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Dasarathy", "Gautam", ""], ["Mossel", "Elchanan", ""], ["Nowak", "Robert", ""], ["Roch", "Sebastien", ""]]}, {"id": "1707.04319", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an and Yerlan Idelbayev", "title": "Model compression as constrained optimization, with application to\n  neural nets. Part II: quantization", "comments": "33 pages, 15 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of deep neural net compression by quantization: given\na large, reference net, we want to quantize its real-valued weights using a\ncodebook with $K$ entries so that the training loss of the quantized net is\nminimal. The codebook can be optimally learned jointly with the net, or fixed,\nas for binarization or ternarization approaches. Previous work has quantized\nthe weights of the reference net, or incorporated rounding operations in the\nbackpropagation algorithm, but this has no guarantee of converging to a\nloss-optimal, quantized net. We describe a new approach based on the recently\nproposed framework of model compression as constrained optimization\n\\citep{Carreir17a}. This results in a simple iterative \"learning-compression\"\nalgorithm, which alternates a step that learns a net of continuous weights with\na step that quantizes (or binarizes/ternarizes) the weights, and is guaranteed\nto converge to local optimum of the loss for quantized nets. We develop\nalgorithms for an adaptive codebook or a (partially) fixed codebook. The latter\nincludes binarization, ternarization, powers-of-two and other important\nparticular cases. We show experimentally that we can achieve much higher\ncompression rates than previous quantization work (even using just 1 bit per\nweight) with negligible loss degradation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 20:58:40 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Idelbayev", "Yerlan", ""]]}, {"id": "1707.04324", "submitter": "Hirsh Agarwal", "authors": "Hirsh R. Agarwal, Andrew Huang", "title": "Tensor-Based Backpropagation in Neural Networks with Non-Sequential\n  Input", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have been able to achieve groundbreaking accuracy at tasks\nconventionally considered only doable by humans. Using stochastic gradient\ndescent, optimization in many dimensions is made possible, albeit at a\nrelatively high computational cost. By splitting training data into batches,\nnetworks can be distributed and trained vastly more efficiently and with\nminimal accuracy loss. We have explored the mathematics behind efficiently\nimplementing tensor-based batch backpropagation algorithms. A common approach\nto batch training is iterating over batch items individually. Explicitly using\ntensor operations to backpropagate allows training to be performed\nnon-linearly, increasing computational efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 21:01:49 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Agarwal", "Hirsh R.", ""], ["Huang", "Andrew", ""]]}, {"id": "1707.04327", "submitter": "Adnan Darwiche", "authors": "Adnan Darwiche", "title": "Human-Level Intelligence or Animal-Like Abilities?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vision systems of the eagle and the snake outperform everything that we\ncan make in the laboratory, but snakes and eagles cannot build an eyeglass or a\ntelescope or a microscope. (Judea Pearl)\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 21:17:14 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Darwiche", "Adnan", ""]]}, {"id": "1707.04347", "submitter": "Lin Chen", "authors": "Lin Chen, Moran Feldman, Amin Karbasi", "title": "Weakly Submodular Maximization Beyond Cardinality Constraints: Does\n  Randomization Help Greedy?", "comments": "Authors are listed in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.AI cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular functions are a broad class of set functions, which naturally\narise in diverse areas. Many algorithms have been suggested for the\nmaximization of these functions. Unfortunately, once the function deviates from\nsubmodularity, the known algorithms may perform arbitrarily poorly. Amending\nthis issue, by obtaining approximation results for set functions generalizing\nsubmodular functions, has been the focus of recent works.\n  One such class, known as weakly submodular functions, has received a lot of\nattention. A key result proved by Das and Kempe (2011) showed that the\napproximation ratio of the greedy algorithm for weakly submodular maximization\nsubject to a cardinality constraint degrades smoothly with the distance from\nsubmodularity. However, no results have been obtained for maximization subject\nto constraints beyond cardinality. In particular, it is not known whether the\ngreedy algorithm achieves any non-trivial approximation ratio for such\nconstraints.\n  In this paper, we prove that a randomized version of the greedy algorithm\n(previously used by Buchbinder et al. (2014) for a different problem) achieves\nan approximation ratio of $(1 + 1/\\gamma)^{-2}$ for the maximization of a\nweakly submodular function subject to a general matroid constraint, where\n$\\gamma$ is a parameter measuring the distance of the function from\nsubmodularity. Moreover, we also experimentally compare the performance of this\nversion of the greedy algorithm on real world problems against natural\nbenchmarks, and show that the algorithm we study performs well also in\npractice. To the best of our knowledge, this is the first algorithm with a\nnon-trivial approximation guarantee for maximizing a weakly submodular function\nsubject to a constraint other than the simple cardinality constraint. In\nparticular, it is the first algorithm with such a guarantee for the important\nand broad class of matroid constraints.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 22:48:43 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Chen", "Lin", ""], ["Feldman", "Moran", ""], ["Karbasi", "Amin", ""]]}, {"id": "1707.04385", "submitter": "Richard Nock", "authors": "Richard Nock and Zac Cranko and Aditya Krishna Menon and Lizhen Qu and\n  Robert C. Williamson", "title": "f-GANs in an Information Geometric Nutshell", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowozin \\textit{et al} showed last year how to extend the GAN\n\\textit{principle} to all $f$-divergences. The approach is elegant but falls\nshort of a full description of the supervised game, and says little about the\nkey player, the generator: for example, what does the generator actually\nconverge to if solving the GAN game means convergence in some space of\nparameters? How does that provide hints on the generator's design and compare\nto the flourishing but almost exclusively experimental literature on the\nsubject?\n  In this paper, we unveil a broad class of distributions for which such\nconvergence happens --- namely, deformed exponential families, a wide superset\nof exponential families --- and show tight connections with the three other key\nGAN parameters: loss, game and architecture. In particular, we show that\ncurrent deep architectures are able to factorize a very large number of such\ndensities using an especially compact design, hence displaying the power of\ndeep architectures and their concinnity in the $f$-GAN game. This result holds\ngiven a sufficient condition on \\textit{activation functions} --- which turns\nout to be satisfied by popular choices. The key to our results is a variational\ngeneralization of an old theorem that relates the KL divergence between regular\nexponential families and divergences between their natural parameters. We\ncomplete this picture with additional results and experimental insights on how\nthese results may be used to ground further improvements of GAN architectures,\nvia (i) a principled design of the activation functions in the generator and\n(ii) an explicit integration of proper composite losses' link function in the\ndiscriminator.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 05:07:52 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Nock", "Richard", ""], ["Cranko", "Zac", ""], ["Menon", "Aditya Krishna", ""], ["Qu", "Lizhen", ""], ["Williamson", "Robert C.", ""]]}, {"id": "1707.04402", "submitter": "Gregory Palmer", "authors": "Gregory Palmer, Karl Tuyls, Daan Bloembergen, Rahul Savani", "title": "Lenient Multi-Agent Deep Reinforcement Learning", "comments": "9 pages, 6 figures, AAMAS2018 Conference Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the success of single agent deep reinforcement learning (DRL) in\nrecent years can be attributed to the use of experience replay memories (ERM),\nwhich allow Deep Q-Networks (DQNs) to be trained efficiently through sampling\nstored state transitions. However, care is required when using ERMs for\nmulti-agent deep reinforcement learning (MA-DRL), as stored transitions can\nbecome outdated because agents update their policies in parallel [11]. In this\nwork we apply leniency [23] to MA-DRL. Lenient agents map state-action pairs to\ndecaying temperature values that control the amount of leniency applied towards\nnegative policy updates that are sampled from the ERM. This introduces optimism\nin the value-function update, and has been shown to facilitate cooperation in\ntabular fully-cooperative multi-agent reinforcement learning problems. We\nevaluate our Lenient-DQN (LDQN) empirically against the related Hysteretic-DQN\n(HDQN) algorithm [22] as well as a modified version we call scheduled-HDQN,\nthat uses average reward learning near terminal states. Evaluations take place\nin extended variations of the Coordinated Multi-Agent Object Transportation\nProblem (CMOTP) [8] which include fully-cooperative sub-tasks and stochastic\nrewards. We find that LDQN agents are more likely to converge to the optimal\npolicy in a stochastic reward CMOTP compared to standard and scheduled-HDQN\nagents.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 07:33:20 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 09:36:29 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Palmer", "Gregory", ""], ["Tuyls", "Karl", ""], ["Bloembergen", "Daan", ""], ["Savani", "Rahul", ""]]}, {"id": "1707.04487", "submitter": "Adrian Spurr", "authors": "Adrian Spurr, Emre Aksan, Otmar Hilliges", "title": "Guiding InfoGAN with Semi-Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new semi-supervised GAN architecture (ss-InfoGAN)\nfor image synthesis that leverages information from few labels (as little as\n0.22%, max. 10% of the dataset) to learn semantically meaningful and\ncontrollable data representations where latent variables correspond to label\ncategories. The architecture builds on Information Maximizing Generative\nAdversarial Networks (InfoGAN) and is shown to learn both continuous and\ncategorical codes and achieves higher quality of synthetic samples compared to\nfully unsupervised settings. Furthermore, we show that using small amounts of\nlabeled data speeds-up training convergence. The architecture maintains the\nability to disentangle latent variables for which no labels are available.\nFinally, we contribute an information-theoretic reasoning on how introducing\nsemi-supervision increases mutual information between synthetic and real data.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 12:44:22 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Spurr", "Adrian", ""], ["Aksan", "Emre", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1707.04582", "submitter": "Gregory Barello", "authors": "Takafumi Arakaki, G. Barello, Yashar Ahmadian", "title": "Capturing the diversity of biological tuning curves using generative\n  adversarial networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tuning curves characterizing the response selectivities of biological neurons\noften exhibit large degrees of irregularity and diversity across neurons.\nTheoretical network models that feature heterogeneous cell populations or\nrandom connectivity also give rise to diverse tuning curves. However, a general\nframework for fitting such models to experimentally measured tuning curves is\nlacking. We address this problem by proposing to view mechanistic network\nmodels as generative models whose parameters can be optimized to fit the\ndistribution of experimentally measured tuning curves. A major obstacle for\nfitting such models is that their likelihood function is not explicitly\navailable or is highly intractable to compute. Recent advances in machine\nlearning provide ways for fitting generative models without the need to\nevaluate the likelihood and its gradient. Generative Adversarial Networks (GAN)\nprovide one such framework which has been successful in traditional machine\nlearning tasks. We apply this approach in two separate experiments, showing how\nGANs can be used to fit commonly used mechanistic models in theoretical\nneuroscience to datasets of measured tuning curves. This fitting procedure\navoids the computationally expensive step of inferring latent variables, e.g.\nthe biophysical parameters of individual cells or the particular realization of\nthe full synaptic connectivity matrix, and directly learns model parameters\nwhich characterize the statistics of connectivity or of single-cell properties.\nAnother strength of this approach is that it fits the entire, joint\ndistribution of experimental tuning curves, instead of matching a few summary\nstatistics picked a priori by the user. More generally, this framework opens\nthe door to fitting theoretically motivated dynamical network models directly\nto simultaneously or non-simultaneously recorded neural responses.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 17:56:50 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 18:44:38 GMT"}, {"version": "v3", "created": "Wed, 19 Jul 2017 16:52:01 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Arakaki", "Takafumi", ""], ["Barello", "G.", ""], ["Ahmadian", "Yashar", ""]]}, {"id": "1707.04585", "submitter": "Aidan N. Gomez", "authors": "Aidan N. Gomez, Mengye Ren, Raquel Urtasun, Roger B. Grosse", "title": "The Reversible Residual Network: Backpropagation Without Storing\n  Activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep residual networks (ResNets) have significantly pushed forward the\nstate-of-the-art on image classification, increasing in performance as networks\ngrow both deeper and wider. However, memory consumption becomes a bottleneck,\nas one needs to store the activations in order to calculate gradients using\nbackpropagation. We present the Reversible Residual Network (RevNet), a variant\nof ResNets where each layer's activations can be reconstructed exactly from the\nnext layer's. Therefore, the activations for most layers need not be stored in\nmemory during backpropagation. We demonstrate the effectiveness of RevNets on\nCIFAR-10, CIFAR-100, and ImageNet, establishing nearly identical classification\naccuracy to equally-sized ResNets, even though the activation storage\nrequirements are independent of depth.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 03:05:43 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Gomez", "Aidan N.", ""], ["Ren", "Mengye", ""], ["Urtasun", "Raquel", ""], ["Grosse", "Roger B.", ""]]}, {"id": "1707.04588", "submitter": "Ga\\\"etan Hadjeres", "authors": "Ga\\\"etan Hadjeres and Frank Nielsen and Fran\\c{c}ois Pachet", "title": "GLSR-VAE: Geodesic Latent Space Regularization for Variational\n  AutoEncoder Architectures", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VAEs (Variational AutoEncoders) have proved to be powerful in the context of\ndensity modeling and have been used in a variety of contexts for creative\npurposes. In many settings, the data we model possesses continuous attributes\nthat we would like to take into account at generation time. We propose in this\npaper GLSR-VAE, a Geodesic Latent Space Regularization for the Variational\nAutoEncoder architecture and its generalizations which allows a fine control on\nthe embedding of the data into the latent space. When augmenting the VAE loss\nwith this regularization, changes in the learned latent space reflects changes\nof the attributes of the data. This deeper understanding of the VAE latent\nspace structure offers the possibility to modulate the attributes of the\ngenerated data in a continuous way. We demonstrate its efficiency on a\nmonophonic music generation task where we manage to generate variations of\ndiscrete sequences in an intended and playful way.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 12:28:25 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Hadjeres", "Ga\u00ebtan", ""], ["Nielsen", "Frank", ""], ["Pachet", "Fran\u00e7ois", ""]]}, {"id": "1707.04610", "submitter": "Tian Guo", "authors": "Tian Guo", "title": "Cloud-based or On-device: An Empirical Study of Mobile Deep Inference", "comments": "Accepted at The IEEE International Conference on Cloud Engineering\n  (IC2E) conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern mobile applications are benefiting significantly from the advancement\nin deep learning, e.g., implementing real-time image recognition and\nconversational system. Given a trained deep learning model, applications\nusually need to perform a series of matrix operations based on the input data,\nin order to infer possible output values. Because of computational complexity\nand size constraints, these trained models are often hosted in the cloud. To\nutilize these cloud-based models, mobile apps will have to send input data over\nthe network. While cloud-based deep learning can provide reasonable response\ntime for mobile apps, it restricts the use case scenarios, e.g. mobile apps\nneed to have network access. With mobile specific deep learning optimizations,\nit is now possible to employ on-device inference. However, because mobile\nhardware, such as GPU and memory size, can be very limited when compared to its\ndesktop counterpart, it is important to understand the feasibility of this new\non-device deep learning inference architecture. In this paper, we empirically\nevaluate the inference performance of three Convolutional Neural Networks\n(CNNs) using a benchmark Android application we developed. Our measurement and\nanalysis suggest that on-device inference can cost up to two orders of\nmagnitude greater response time and energy when compared to cloud-based\ninference, and that loading model and computing probability are two performance\nbottlenecks for on-device deep inferences.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 19:05:50 GMT"}, {"version": "v2", "created": "Sun, 15 Apr 2018 17:48:20 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Guo", "Tian", ""]]}, {"id": "1707.04615", "submitter": "John Wilmes", "authors": "Le Song, Santosh Vempala, John Wilmes, and Bo Xie", "title": "On the Complexity of Learning Neural Networks", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stunning empirical successes of neural networks currently lack rigorous\ntheoretical explanation. What form would such an explanation take, in the face\nof existing complexity-theoretic lower bounds? A first step might be to show\nthat data generated by neural networks with a single hidden layer, smooth\nactivation functions and benign input distributions can be learned efficiently.\nWe demonstrate here a comprehensive lower bound ruling out this possibility:\nfor a wide class of activation functions (including all currently used), and\ninputs drawn from any logconcave distribution, there is a family of\none-hidden-layer functions whose output is a sum gate, that are hard to learn\nin a precise sense: any statistical query algorithm (which includes all known\nvariants of stochastic gradient descent with any loss function) needs an\nexponential number of queries even using tolerance inversely proportional to\nthe input dimensionality. Moreover, this hard family of functions is realizable\nwith a small (sublinear in dimension) number of activation units in the single\nhidden layer. The lower bound is also robust to small perturbations of the true\nweights. Systematic experiments illustrate a phase transition in the training\nerror as predicted by the analysis.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 19:23:07 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Song", "Le", ""], ["Vempala", "Santosh", ""], ["Wilmes", "John", ""], ["Xie", "Bo", ""]]}, {"id": "1707.04619", "submitter": "Fathi Salem", "authors": "Atra Akandeh and Fathi M. Salem", "title": "Simplified Long Short-term Memory Recurrent Neural Networks: part I", "comments": "4 pages, 6 figures, 5 tables. Part I of a three part publications\n  that will appear in IKE'17 - The 16th Int'l Conference on Information &\n  Knowledge Engineering The 2017 World Congress in Computer Science, Computer\n  Engineering & Applied Computing | CSCE'17, July 17-20, 2017, Las Vegas,\n  Nevada, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present five variants of the standard Long Short-term Memory (LSTM)\nrecurrent neural networks by uniformly reducing blocks of adaptive parameters\nin the gating mechanisms. For simplicity, we refer to these models as LSTM1,\nLSTM2, LSTM3, LSTM4, and LSTM5, respectively. Such parameter-reduced variants\nenable speeding up data training computations and would be more suitable for\nimplementations onto constrained embedded platforms. We comparatively evaluate\nand verify our five variant models on the classical MNIST dataset and\ndemonstrate that these variant models are comparable to a standard\nimplementation of the LSTM model while using less number of parameters.\nMoreover, we observe that in some cases the standard LSTM's accuracy\nperformance will drop after a number of epochs when using the ReLU\nnonlinearity; in contrast, however, LSTM3, LSTM4 and LSTM5 will retain their\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 19:46:59 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Akandeh", "Atra", ""], ["Salem", "Fathi M.", ""]]}, {"id": "1707.04623", "submitter": "Fathi Salem", "authors": "Atra Akandeh and Fathi M. Salem", "title": "Simplified Long Short-term Memory Recurrent Neural Networks: part II", "comments": "4 pages, 6 figures, 5 tables; this is part II of three-part work, all\n  to appear in IKE'17- The 16th Int'l Conference on Information & Knowledge\n  Engineering, in The 2017 World Congress in Computer Science Computer\n  Engineering & Applied Computing | CSCE'17 July 17-20, 2017, Las Vegas,\n  Nevada, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is part II of three-part work. Here, we present a second set of\ninter-related five variants of simplified Long Short-term Memory (LSTM)\nrecurrent neural networks by further reducing adaptive parameters. Two of these\nmodels have been introduced in part I of this work. We evaluate and verify our\nmodel variants on the benchmark MNIST dataset and assert that these models are\ncomparable to the base LSTM model while use progressively less number of\nparameters. Moreover, we observe that in case of using the ReLU activation, the\ntest accuracy performance of the standard LSTM will drop after a number of\nepochs when learning parameter become larger. However all of the new model\nvariants sustain their performance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 19:59:18 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Akandeh", "Atra", ""], ["Salem", "Fathi M.", ""]]}, {"id": "1707.04626", "submitter": "Fathi Salem", "authors": "Atra Akandeh and Fathi M. Salem", "title": "Simplified Long Short-term Memory Recurrent Neural Networks: part III", "comments": "Here 5 pages (in the conference 4 pages), 10 figures, 5 tables; this\n  is part III of a three part work, all will appear in the IKE'17 - The 16th\n  Int'l Conference on Information & Knowledge Engineering. The 2017 World\n  Congress in Computer Science Computer Engineering & Applied Computing |\n  CSCE'17, July 17-20, 2017, Las Vegas, Nevada, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is part III of three-part work. In parts I and II, we have presented\neight variants for simplified Long Short Term Memory (LSTM) recurrent neural\nnetworks (RNNs). It is noted that fast computation, specially in constrained\ncomputing resources, are an important factor in processing big time-sequence\ndata. In this part III paper, we present and evaluate two new LSTM model\nvariants which dramatically reduce the computational load while retaining\ncomparable performance to the base (standard) LSTM RNNs. In these new variants,\nwe impose (Hadamard) pointwise state multiplications in the cell-memory network\nin addition to the gating signal networks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 20:12:37 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Akandeh", "Atra", ""], ["Salem", "Fathi M.", ""]]}, {"id": "1707.04638", "submitter": "Marinka Zitnik", "authors": "Marinka Zitnik and Jure Leskovec", "title": "Predicting multicellular function through multi-layer tissue networks", "comments": "In Proceedings of the 25th International Conference on Intelligent\n  Systems for Molecular Biology (ISMB), 2017", "journal-ref": "Bioinformatics 2017, 33 (14): i190-i198", "doi": "10.1093/bioinformatics/btx252", "report-no": null, "categories": "cs.LG cs.SI q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Understanding functions of proteins in specific human tissues is\nessential for insights into disease diagnostics and therapeutics, yet\nprediction of tissue-specific cellular function remains a critical challenge\nfor biomedicine.\n  Results: Here we present OhmNet, a hierarchy-aware unsupervised node feature\nlearning approach for multi-layer networks. We build a multi-layer network,\nwhere each layer represents molecular interactions in a different human tissue.\nOhmNet then automatically learns a mapping of proteins, represented as nodes,\nto a neural embedding based low-dimensional space of features. OhmNet\nencourages sharing of similar features among proteins with similar network\nneighborhoods and among proteins activated in similar tissues. The algorithm\ngeneralizes prior work, which generally ignores relationships between tissues,\nby modeling tissue organization with a rich multiscale tissue hierarchy. We use\nOhmNet to study multicellular function in a multi-layer protein interaction\nnetwork of 107 human tissues. In 48 tissues with known tissue-specific cellular\nfunctions, OhmNet provides more accurate predictions of cellular function than\nalternative approaches, and also generates more accurate hypotheses about\ntissue-specific protein actions. We show that taking into account the tissue\nhierarchy leads to improved predictive power. Remarkably, we also demonstrate\nthat it is possible to leverage the tissue hierarchy in order to effectively\ntransfer cellular functions to a functionally uncharacterized tissue. Overall,\nOhmNet moves from flat networks to multiscale models able to predict a range of\nphenotypes spanning cellular subsystems\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 21:03:53 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Zitnik", "Marinka", ""], ["Leskovec", "Jure", ""]]}, {"id": "1707.04639", "submitter": "Richard Yang", "authors": "Richard R. Yang, Mike Borowczak", "title": "Predictive Liability Models and Visualizations of High Dimensional\n  Retail Employee Data", "comments": null, "journal-ref": null, "doi": "10.1145/3194206.3195587", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Employee theft and dishonesty is a major contributor to loss in the retail\nindustry. Retailers have reported the need for more automated analytic tools to\nassess the liability of their employees. In this work, we train and optimize\nseveral machine learning models for regression prediction and analysis on this\ndata, which will help retailers identify and manage risky employees. Since the\ndata we use is very high dimensional, we use feature selection techniques to\nidentify the most contributing factors to an employee's assessed risk. We also\nuse dimension reduction and data embedding techniques to present this dataset\nin a easy to interpret format.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 21:10:32 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 03:19:56 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2018 01:59:19 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Yang", "Richard R.", ""], ["Borowczak", "Mike", ""]]}, {"id": "1707.04673", "submitter": "Asish Ghoshal", "authors": "Asish Ghoshal and Jean Honorio", "title": "Learning linear structural equation models in polynomial time and sample\n  complexity", "comments": null, "journal-ref": "International Conference on Artificial Intelligence and Statistics\n  (AISTATS) 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of learning structural equation models (SEMs) from data is a\nfundamental problem in causal inference. We develop a new algorithm --- which\nis computationally and statistically efficient and works in the\nhigh-dimensional regime --- for learning linear SEMs from purely observational\ndata with arbitrary noise distribution. We consider three aspects of the\nproblem: identifiability, computational efficiency, and statistical efficiency.\nWe show that when data is generated from a linear SEM over $p$ nodes and\nmaximum degree $d$, our algorithm recovers the directed acyclic graph (DAG)\nstructure of the SEM under an identifiability condition that is more general\nthan those considered in the literature, and without faithfulness assumptions.\nIn the population setting, our algorithm recovers the DAG structure in\n$\\mathcal{O}(p(d^2 + \\log p))$ operations. In the finite sample setting, if the\nestimated precision matrix is sparse, our algorithm has a smoothed complexity\nof $\\widetilde{\\mathcal{O}}(p^3 + pd^7)$, while if the estimated precision\nmatrix is dense, our algorithm has a smoothed complexity of\n$\\widetilde{\\mathcal{O}}(p^5)$. For sub-Gaussian noise, we show that our\nalgorithm has a sample complexity of $\\mathcal{O}(\\frac{d^8}{\\varepsilon^2}\n\\log (\\frac{p}{\\sqrt{\\delta}}))$ to achieve $\\varepsilon$ element-wise additive\nerror with respect to the true autoregression matrix with probability at most\n$1 - \\delta$, while for noise with bounded $(4m)$-th moment, with $m$ being a\npositive integer, our algorithm has a sample complexity of\n$\\mathcal{O}(\\frac{d^8}{\\varepsilon^2} (\\frac{p^2}{\\delta})^{1/m})$.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 01:10:57 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Ghoshal", "Asish", ""], ["Honorio", "Jean", ""]]}, {"id": "1707.04780", "submitter": "Decebal Constantin Mocanu", "authors": "Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H.\n  Nguyen, Madeleine Gibescu, Antonio Liotta", "title": "Scalable Training of Artificial Neural Networks with Adaptive Sparse\n  Connectivity inspired by Network Science", "comments": "18 pages", "journal-ref": "Nature Communications, 2018", "doi": "10.1038/s41467-018-04316-3", "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through the success of deep learning in various domains, artificial neural\nnetworks are currently among the most used artificial intelligence methods.\nTaking inspiration from the network properties of biological neural networks\n(e.g. sparsity, scale-freeness), we argue that (contrary to general practice)\nartificial neural networks, too, should not have fully-connected layers. Here\nwe propose sparse evolutionary training of artificial neural networks, an\nalgorithm which evolves an initial sparse topology (Erd\\H{o}s-R\\'enyi random\ngraph) of two consecutive layers of neurons into a scale-free topology, during\nlearning. Our method replaces artificial neural networks fully-connected layers\nwith sparse ones before training, reducing quadratically the number of\nparameters, with no decrease in accuracy. We demonstrate our claims on\nrestricted Boltzmann machines, multi-layer perceptrons, and convolutional\nneural networks for unsupervised and supervised learning on 15 datasets. Our\napproach has the potential to enable artificial neural networks to scale up\nbeyond what is currently possible.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 19:46:25 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 12:55:55 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Mocanu", "Decebal Constantin", ""], ["Mocanu", "Elena", ""], ["Stone", "Peter", ""], ["Nguyen", "Phuong H.", ""], ["Gibescu", "Madeleine", ""], ["Liotta", "Antonio", ""]]}, {"id": "1707.04791", "submitter": "Stephen Tu", "authors": "Stephen Tu, Ross Boczar, Andrew Packard, Benjamin Recht", "title": "Non-Asymptotic Analysis of Robust Control from Coarse-Grained\n  Identification", "comments": "A substantial revision, where we strengthen our existing upper bounds\n  and introduce a matching lower bound", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores the trade-off between the number of samples required to\naccurately build models of dynamical systems and the degradation of performance\nin various control objectives due to a coarse approximation. In particular, we\nshow that simple models can be easily fit from input/output data and are\nsufficient for achieving various control objectives. We derive bounds on the\nnumber of noisy input/output samples from a stable linear time-invariant system\nthat are sufficient to guarantee that the corresponding finite impulse response\napproximation is close to the true system in the $\\mathcal{H}_\\infty$-norm. We\ndemonstrate that these demands are lower than those derived in prior art which\naimed to accurately identify dynamical models. We also explore how different\nphysical input constraints, such as power constraints, affect the sample\ncomplexity. Finally, we show how our analysis fits within the established\nframework of robust control, by demonstrating how a controller designed for an\napproximate system provably meets performance objectives on the true system.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 21:45:20 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 10:22:37 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Tu", "Stephen", ""], ["Boczar", "Ross", ""], ["Packard", "Andrew", ""], ["Recht", "Benjamin", ""]]}, {"id": "1707.04822", "submitter": "Adams Wei Yu", "authors": "Adams Wei Yu, Lei Huang, Qihang Lin, Ruslan Salakhutdinov, Jaime\n  Carbonell", "title": "Block-Normalized Gradient Method: An Empirical Study for Training Deep\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a generic and simple strategy for utilizing\nstochastic gradient information in optimization. The technique essentially\ncontains two consecutive steps in each iteration: 1) computing and normalizing\neach block (layer) of the mini-batch stochastic gradient; 2) selecting\nappropriate step size to update the decision variable (parameter) towards the\nnegative of the block-normalized gradient. We conduct extensive empirical\nstudies on various non-convex neural network optimization problems, including\nmulti-layer perceptron, convolution neural networks and recurrent neural\nnetworks. The results indicate the block-normalized gradient can help\naccelerate the training of neural networks. In particular, we observe that the\nnormalized gradient methods having constant step size with occasionally decay,\nsuch as SGD with momentum, have better performance in the deep convolution\nneural networks, while those with adaptive step sizes, such as Adam, perform\nbetter in recurrent neural networks. Besides, we also observe this line of\nmethods can lead to solutions with better generalization properties, which is\nconfirmed by the performance improvement over strong baselines.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 04:47:22 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 09:45:02 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Yu", "Adams Wei", ""], ["Huang", "Lei", ""], ["Lin", "Qihang", ""], ["Salakhutdinov", "Ruslan", ""], ["Carbonell", "Jaime", ""]]}, {"id": "1707.04849", "submitter": "Evgeniy Vodolazskiy", "authors": "Michail Schlesinger, Evgeniy Vodolazskiy", "title": "Minimax deviation strategies for machine learning and recognition with\n  short learning samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article is devoted to the problem of small learning samples in machine\nlearning. The flaws of maximum likelihood learning and minimax learning are\nlooked into and the concept of minimax deviation learning is introduced that is\nfree of those flaws.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 09:15:08 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Schlesinger", "Michail", ""], ["Vodolazskiy", "Evgeniy", ""]]}, {"id": "1707.04853", "submitter": "Xu He", "authors": "Xu He and Herbert Jaeger", "title": "Overcoming Catastrophic Interference by Conceptors", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": "Jacobs University Technical Report Nr 35", "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catastrophic interference has been a major roadblock in the research of\ncontinual learning. Here we propose a variant of the back-propagation\nalgorithm, \"conceptor-aided back-prop\" (CAB), in which gradients are shielded\nby conceptors against degradation of previously learned tasks. Conceptors have\ntheir origin in reservoir computing, where they have been previously shown to\novercome catastrophic forgetting. CAB extends these results to deep feedforward\nnetworks. On the disjoint MNIST task CAB outperforms two other methods for\ncoping with catastrophic interference that have recently been proposed in the\ndeep learning field.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 10:12:13 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 21:37:54 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["He", "Xu", ""], ["Jaeger", "Herbert", ""]]}, {"id": "1707.04873", "submitter": "Han Cai", "authors": "Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, Jun Wang", "title": "Efficient Architecture Search by Network Transformation", "comments": "The Thirty-Second AAAI Conference on Artificial Intelligence\n  (AAAI-18). We change the title from \"Reinforcement Learning for Architecture\n  Search by Network Transformation\" to \"Efficient Architecture Search by\n  Network Transformation\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Techniques for automatically designing deep neural network architectures such\nas reinforcement learning based approaches have recently shown promising\nresults. However, their success is based on vast computational resources (e.g.\nhundreds of GPUs), making them difficult to be widely used. A noticeable\nlimitation is that they still design and train each network from scratch during\nthe exploration of the architecture space, which is highly inefficient. In this\npaper, we propose a new framework toward efficient architecture search by\nexploring the architecture space based on the current network and reusing its\nweights. We employ a reinforcement learning agent as the meta-controller, whose\naction is to grow the network depth or layer width with function-preserving\ntransformations. As such, the previously validated networks can be reused for\nfurther exploration, thus saves a large amount of computational cost. We apply\nour method to explore the architecture space of the plain convolutional neural\nnetworks (no skip-connections, branching etc.) on image benchmark datasets\n(CIFAR-10, SVHN) with restricted computational resources (5 GPUs). Our method\ncan design highly competitive networks that outperform existing networks using\nthe same design scheme. On CIFAR-10, our model without skip-connections\nachieves 4.23\\% test error rate, exceeding a vast majority of modern\narchitectures and approaching DenseNet. Furthermore, by applying our method to\nexplore the DenseNet architecture space, we are able to achieve more accurate\nnetworks with fewer parameters.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 12:39:02 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 08:38:04 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Cai", "Han", ""], ["Chen", "Tianyao", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""], ["Wang", "Jun", ""]]}, {"id": "1707.04879", "submitter": "Andros Tjandra", "authors": "Andros Tjandra, Sakriani Sakti, Satoshi Nakamura", "title": "Listening while Speaking: Speech Chain by Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the close relationship between speech perception and production,\nresearch in automatic speech recognition (ASR) and text-to-speech synthesis\n(TTS) has progressed more or less independently without exerting much mutual\ninfluence on each other. In human communication, on the other hand, a\nclosed-loop speech chain mechanism with auditory feedback from the speaker's\nmouth to her ear is crucial. In this paper, we take a step further and develop\na closed-loop speech chain model based on deep learning. The\nsequence-to-sequence model in close-loop architecture allows us to train our\nmodel on the concatenation of both labeled and unlabeled data. While ASR\ntranscribes the unlabeled speech features, TTS attempts to reconstruct the\noriginal speech waveform based on the text from ASR. In the opposite direction,\nASR also attempts to reconstruct the original text transcription given the\nsynthesized speech. To the best of our knowledge, this is the first deep\nlearning model that integrates human speech perception and production\nbehaviors. Our experimental results show that the proposed approach\nsignificantly improved the performance more than separate systems that were\nonly trained with labeled data.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 13:27:56 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Tjandra", "Andros", ""], ["Sakti", "Sakriani", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "1707.04926", "submitter": "Adel Javanmard", "authors": "Mahdi Soltanolkotabi and Adel Javanmard and Jason D. Lee", "title": "Theoretical insights into the optimization landscape of\n  over-parameterized shallow neural networks", "comments": "Section 3 on numerical experiments is added. Theorems 2.1 and 2.2 are\n  improved to apply to almost all input data (not just Gaussian inputs).\n  Related work section is expanded. The paper is accepted for publication in\n  IEEE transaction on Information Theory (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of learning a shallow artificial neural\nnetwork that best fits a training data set. We study this problem in the\nover-parameterized regime where the number of observations are fewer than the\nnumber of parameters in the model. We show that with quadratic activations the\noptimization landscape of training such shallow neural networks has certain\nfavorable characteristics that allow globally optimal models to be found\nefficiently using a variety of local search heuristics. This result holds for\nan arbitrary training data of input/output pairs. For differentiable activation\nfunctions we also show that gradient descent, when suitably initialized,\nconverges at a linear rate to a globally optimal model. This result focuses on\na realizable model where the inputs are chosen i.i.d. from a Gaussian\ndistribution and the labels are generated according to planted weight\ncoefficients.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 18:13:51 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 00:11:02 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Soltanolkotabi", "Mahdi", ""], ["Javanmard", "Adel", ""], ["Lee", "Jason D.", ""]]}, {"id": "1707.04940", "submitter": "Yuri G. Gordienko", "authors": "Yuriy Kochura, Sergii Stirenko, Yuri Gordienko", "title": "Comparative Performance Analysis of Neural Networks Architectures on H2O\n  Platform for Various Activation Functions", "comments": "4 pages, 6 figures, 6 tables; 2017 IEEE International Young\n  Scientists Forum on Applied Physics and Engineering (YSF-2017) (Lviv,\n  Ukraine)", "journal-ref": null, "doi": "10.1109/YSF.2017.8126654", "report-no": null, "categories": "cs.LG cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (deep structured learning, hierarchi- cal learning or deep\nmachine learning) is a branch of machine learning based on a set of algorithms\nthat attempt to model high- level abstractions in data by using multiple\nprocessing layers with complex structures or otherwise composed of multiple\nnon-linear transformations. In this paper, we present the results of testing\nneural networks architectures on H2O platform for various activation functions,\nstopping metrics, and other parameters of machine learning algorithm. It was\ndemonstrated for the use case of MNIST database of handwritten digits in\nsingle-threaded mode that blind selection of these parameters can hugely\nincrease (by 2-3 orders) the runtime without the significant increase of\nprecision. This result can have crucial influence for opitmization of available\nand new machine learning methods, especially for image recognition problems.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 19:57:28 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Kochura", "Yuriy", ""], ["Stirenko", "Sergii", ""], ["Gordienko", "Yuri", ""]]}, {"id": "1707.04958", "submitter": "Jonathan Rubin", "authors": "Jonathan Rubin, Cristhian Potes, Minnan Xu-Wilson, Junzi Dong, Asif\n  Rahman, Hiep Nguyen, David Moromisato", "title": "An Ensemble Boosting Model for Predicting Transfer to the Pediatric\n  Intensive Care Unit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work focuses on the problem of predicting the transfer of pediatric\npatients from the general ward of a hospital to the pediatric intensive care\nunit. Using data collected over 5.5 years from the electronic health records of\ntwo medical facilities, we develop classifiers based on adaptive boosting and\ngradient tree boosting. We further combine these learned classifiers into an\nensemble model and compare its performance to a modified pediatric early\nwarning score (PEWS) baseline that relies on expert defined guidelines. To\ngauge model generalizability, we perform an inter-facility evaluation where we\ntrain our algorithm on data from one facility and perform evaluation on a\nhidden test dataset from a separate facility. We show that improvements are\nwitnessed over the PEWS baseline in accuracy (0.77 vs. 0.69), sensitivity (0.80\nvs. 0.68), specificity (0.74 vs. 0.70) and AUROC (0.85 vs. 0.73).\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 23:01:35 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Rubin", "Jonathan", ""], ["Potes", "Cristhian", ""], ["Xu-Wilson", "Minnan", ""], ["Dong", "Junzi", ""], ["Rahman", "Asif", ""], ["Nguyen", "Hiep", ""], ["Moromisato", "David", ""]]}, {"id": "1707.05010", "submitter": "Phuoc Nguyen", "authors": "Phuoc Nguyen, Truyen Tran, Svetha Venkatesh", "title": "Deep Learning to Attend to Risk in ICU", "comments": "Accepted IJCAI17-KDH workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling physiological time-series in ICU is of high clinical importance.\nHowever, data collected within ICU are irregular in time and often contain\nmissing measurements. Since absence of a measure would signify its lack of\nimportance, the missingness is indeed informative and might reflect the\ndecision making by the clinician. Here we propose a deep learning architecture\nthat can effectively handle these challenges for predicting ICU mortality\noutcomes. The model is based on Long Short-Term Memory, and has layered\nattention mechanisms. At the sensing layer, the model decides whether to\nobserve and incorporate parts of the current measurements. At the reasoning\nlayer, evidences across time steps are weighted and combined. The model is\nevaluated on the PhysioNet 2012 dataset showing competitive and interpretable\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 06:23:20 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Nguyen", "Phuoc", ""], ["Tran", "Truyen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1707.05023", "submitter": "Gerard Biau", "authors": "G\\'erard Biau (LSTA, LPMA), Beno\\^it Cadre (ENS Rennes, IRMAR)", "title": "Optimization by gradient boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient boosting is a state-of-the-art prediction technique that\nsequentially produces a model in the form of linear combinations of simple\npredictors---typically decision trees---by solving an infinite-dimensional\nconvex optimization problem. We provide in the present paper a thorough\nanalysis of two widespread versions of gradient boosting, and introduce a\ngeneral framework for studying these algorithms from the point of view of\nfunctional optimization. We prove their convergence as the number of iterations\ntends to infinity and highlight the importance of having a strongly convex risk\nfunctional to minimize. We also present a reasonable statistical context\nensuring consistency properties of the boosting predictors as the sample size\ngrows. In our approach, the optimization procedures are run forever (that is,\nwithout resorting to an early stopping strategy), and statistical\nregularization is basically achieved via an appropriate $L^2$ penalization of\nthe loss and strong convexity arguments.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 07:44:26 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Biau", "G\u00e9rard", "", "LSTA, LPMA"], ["Cadre", "Beno\u00eet", "", "ENS Rennes, IRMAR"]]}, {"id": "1707.05101", "submitter": "Alexey Drutsa", "authors": "Alexey Drutsa", "title": "On consistency of optimal pricing algorithms in repeated posted-price\n  auctions with strategic buyer", "comments": "25 pages; 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study revenue optimization learning algorithms for repeated posted-price\nauctions where a seller interacts with a single strategic buyer that holds a\nfixed private valuation for a good and seeks to maximize his cumulative\ndiscounted surplus. For this setting, first, we propose a novel algorithm that\nnever decreases offered prices and has a tight strategic regret bound in\n$\\Theta(\\log\\log T)$ under some mild assumptions on the buyer surplus\ndiscounting. This result closes the open research question on the existence of\na no-regret horizon-independent weakly consistent pricing. The proposed\nalgorithm is inspired by our observation that a double decrease of offered\nprices in a weakly consistent algorithm is enough to cause a linear regret.\nThis motivates us to construct a novel transformation that maps a\nright-consistent algorithm to a weakly consistent one that never decreases\noffered prices.\n  Second, we outperform the previously known strategic regret upper bound of\nthe algorithm PRRFES, where the improvement is achieved by means of a finer\nconstant factor $C$ of the principal term $C\\log\\log T$ in this upper bound.\nFinally, we generalize results on strategic regret previously known for\ngeometric discounting of the buyer's surplus to discounting of other types,\nnamely: the optimality of the pricing PRRFES to the case of geometrically\nconcave decreasing discounting; and linear lower bound on the strategic regret\nof a wide range of horizon-independent weakly consistent algorithms to the case\nof arbitrary discounts.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 11:29:14 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 01:15:57 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Drutsa", "Alexey", ""]]}, {"id": "1707.05128", "submitter": "Jayadev Acharya", "authors": "Jayadev Acharya, Ziteng Sun, Huanyu Zhang", "title": "Differentially Private Testing of Identity and Closeness of Discrete\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problems of identity testing (goodness of fit), and\ncloseness testing (two sample test) of distributions over $k$ elements, under\ndifferential privacy. While the problems have a long history in statistics,\nfinite sample bounds for these problems have only been established recently.\n  In this work, we derive upper and lower bounds on the sample complexity of\nboth the problems under $(\\varepsilon, \\delta)$-differential privacy. We\nprovide optimal sample complexity algorithms for identity testing problem for\nall parameter ranges, and the first results for closeness testing. Our\ncloseness testing bounds are optimal in the sparse regime where the number of\nsamples is at most $k$.\n  Our upper bounds are obtained by privatizing non-private estimators for these\nproblems. The non-private estimators are chosen to have small sensitivity. We\npropose a general framework to establish lower bounds on the sample complexity\nof statistical tasks under differential privacy. We show a bound on\ndifferentially private algorithms in terms of a coupling between the two\nhypothesis classes we aim to test. By constructing carefully chosen priors over\nthe hypothesis classes, and using Le Cam's two point theorem we provide a\ngeneral mechanism for proving lower bounds. We believe that the framework can\nbe used to obtain strong lower bounds for other statistical tasks under\nprivacy.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 13:00:13 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 03:24:09 GMT"}, {"version": "v3", "created": "Tue, 31 Oct 2017 04:37:01 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Acharya", "Jayadev", ""], ["Sun", "Ziteng", ""], ["Zhang", "Huanyu", ""]]}, {"id": "1707.05147", "submitter": "Thomas Brouwer", "authors": "Thomas Brouwer, Jes Frellsen, Pietro Li\\'o", "title": "Comparative Study of Inference Methods for Bayesian Nonnegative Matrix\n  Factorisation", "comments": "European Conference on Machine Learning and Principles and Practice\n  of Knowledge Discovery in Databases (ECML PKDD 2017). The final publication\n  will be available at link.springer.com. arXiv admin note: text overlap with\n  arXiv:1610.08127", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the trade-offs of different inference approaches for\nBayesian matrix factorisation methods, which are commonly used for predicting\nmissing values, and for finding patterns in the data. In particular, we\nconsider Bayesian nonnegative variants of matrix factorisation and\ntri-factorisation, and compare non-probabilistic inference, Gibbs sampling,\nvariational Bayesian inference, and a maximum-a-posteriori approach. The\nvariational approach is new for the Bayesian nonnegative models. We compare\ntheir convergence, and robustness to noise and sparsity of the data, on both\nsynthetic and real-world datasets. Furthermore, we extend the models with the\nBayesian automatic relevance determination prior, allowing the models to\nperform automatic model selection, and demonstrate its efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 18:24:55 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Brouwer", "Thomas", ""], ["Frellsen", "Jes", ""], ["Li\u00f3", "Pietro", ""]]}, {"id": "1707.05173", "submitter": "Owain Evans", "authors": "William Saunders, Girish Sastry, Andreas Stuhlmueller, Owain Evans", "title": "Trial without Error: Towards Safe Reinforcement Learning via Human\n  Intervention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI systems are increasingly applied to complex tasks that involve interaction\nwith humans. During training, such systems are potentially dangerous, as they\nhaven't yet learned to avoid actions that could cause serious harm. How can an\nAI system explore and learn without making a single mistake that harms humans\nor otherwise causes serious damage? For model-free reinforcement learning,\nhaving a human \"in the loop\" and ready to intervene is currently the only way\nto prevent all catastrophes. We formalize human intervention for RL and show\nhow to reduce the human labor required by training a supervised learner to\nimitate the human's intervention decisions. We evaluate this scheme on Atari\ngames, with a Deep RL agent being overseen by a human for four hours. When the\nclass of catastrophes is simple, we are able to prevent all catastrophes\nwithout affecting the agent's learning (whereas an RL baseline fails due to\ncatastrophic forgetting). However, this scheme is less successful when\ncatastrophes are more complex: it reduces but does not eliminate catastrophes\nand the supervised learner fails on adversarial examples found by the agent.\nExtrapolating to more challenging environments, we show that our implementation\nwould not scale (due to the infeasible amount of human labor required). We\noutline extensions of the scheme that are necessary if we are to train\nmodel-free agents without a single catastrophe.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 14:13:40 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Saunders", "William", ""], ["Sastry", "Girish", ""], ["Stuhlmueller", "Andreas", ""], ["Evans", "Owain", ""]]}, {"id": "1707.05227", "submitter": "Marek Rei", "authors": "Marek Rei, Helen Yannakoudakis", "title": "Auxiliary Objectives for Neural Error Detection Models", "comments": "The 12th Workshop on Innovative Use of NLP for Building Educational\n  Applications (BEA 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the utility of different auxiliary objectives and training\nstrategies within a neural sequence labeling approach to error detection in\nlearner writing. Auxiliary costs provide the model with additional linguistic\ninformation, allowing it to learn general-purpose compositional features that\ncan then be exploited for other objectives. Our experiments show that a joint\nlearning approach trained with parallel labels on in-domain data improves\nperformance over the previous best error detection system. While the resulting\nmodel has the same number of parameters, the additional objectives allow it to\nbe optimised more efficiently and achieve better performance.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 15:24:09 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Rei", "Marek", ""], ["Yannakoudakis", "Helen", ""]]}, {"id": "1707.05233", "submitter": "Marek Rei", "authors": "Marek Rei", "title": "Detecting Off-topic Responses to Visual Prompts", "comments": "The 12th Workshop on Innovative Use of NLP for Building Educational\n  Applications (BEA 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated methods for essay scoring have made great progress in recent years,\nachieving accuracies very close to human annotators. However, a known weakness\nof such automated scorers is not taking into account the semantic relevance of\nthe submitted text. While there is existing work on detecting answer relevance\ngiven a textual prompt, very little previous research has been done to\nincorporate visual writing prompts. We propose a neural architecture and\nseveral extensions for detecting off-topic responses to visual prompts and\nevaluate it on a dataset of texts written by language learners.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 15:31:20 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Rei", "Marek", ""]]}, {"id": "1707.05236", "submitter": "Marek Rei", "authors": "Marek Rei, Mariano Felice, Zheng Yuan, Ted Briscoe", "title": "Artificial Error Generation with Machine Translation and Syntactic\n  Patterns", "comments": "The 12th Workshop on Innovative Use of NLP for Building Educational\n  Applications (BEA 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shortage of available training data is holding back progress in the area of\nautomated error detection. This paper investigates two alternative methods for\nartificially generating writing errors, in order to create additional\nresources. We propose treating error generation as a machine translation task,\nwhere grammatically correct text is translated to contain errors. In addition,\nwe explore a system for extracting textual patterns from an annotated corpus,\nwhich can then be used to insert errors into grammatically correct sentences.\nOur experiments show that the inclusion of artificially generated errors\nsignificantly improves error detection accuracy on both FCE and CoNLL 2014\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 15:38:09 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Rei", "Marek", ""], ["Felice", "Mariano", ""], ["Yuan", "Zheng", ""], ["Briscoe", "Ted", ""]]}, {"id": "1707.05246", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, Barbara Plank", "title": "Learning to select data for transfer learning with Bayesian Optimization", "comments": "EMNLP 2017. Code available at:\n  https://github.com/sebastianruder/learn-to-select-data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain similarity measures can be used to gauge adaptability and select\nsuitable data for transfer learning, but existing approaches define ad hoc\nmeasures that are deemed suitable for respective tasks. Inspired by work on\ncurriculum learning, we propose to \\emph{learn} data selection measures using\nBayesian Optimization and evaluate them across models, domains and tasks. Our\nlearned measures outperform existing domain similarity measures significantly\non three tasks: sentiment analysis, part-of-speech tagging, and parsing. We\nshow the importance of complementing similarity with diversity, and that\nlearned measures are -- to some degree -- transferable across models, domains,\nand even tasks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 15:53:18 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Ruder", "Sebastian", ""], ["Plank", "Barbara", ""]]}, {"id": "1707.05300", "submitter": "Carlos Florensa", "authors": "Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, Pieter\n  Abbeel", "title": "Reverse Curriculum Generation for Reinforcement Learning", "comments": "Published at the 1st Conference on Robot Learning (CoRL 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many relevant tasks require an agent to reach a certain state, or to\nmanipulate objects into a desired configuration. For example, we might want a\nrobot to align and assemble a gear onto an axle or insert and turn a key in a\nlock. These goal-oriented tasks present a considerable challenge for\nreinforcement learning, since their natural reward function is sparse and\nprohibitive amounts of exploration are required to reach the goal and receive\nsome learning signal. Past approaches tackle these problems by exploiting\nexpert demonstrations or by manually designing a task-specific reward shaping\nfunction to guide the learning agent. Instead, we propose a method to learn\nthese tasks without requiring any prior knowledge other than obtaining a single\nstate in which the task is achieved. The robot is trained in reverse, gradually\nlearning to reach the goal from a set of start states increasingly far from the\ngoal. Our method automatically generates a curriculum of start states that\nadapts to the agent's performance, leading to efficient training on\ngoal-oriented tasks. We demonstrate our approach on difficult simulated\nnavigation and fine-grained manipulation problems, not solvable by\nstate-of-the-art reinforcement learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 17:53:54 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 02:46:26 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 10:10:17 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Florensa", "Carlos", ""], ["Held", "David", ""], ["Wulfmeier", "Markus", ""], ["Zhang", "Michael", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1707.05363", "submitter": "Yi Zhou", "authors": "Zimo Li, Yi Zhou, Shuangjiu Xiao, Chong He, Zeng Huang, Hao Li", "title": "Auto-Conditioned Recurrent Networks for Extended Complex Human Motion\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a real-time method for synthesizing highly complex human motions\nusing a novel training regime we call the auto-conditioned Recurrent Neural\nNetwork (acRNN). Recently, researchers have attempted to synthesize new motion\nby using autoregressive techniques, but existing methods tend to freeze or\ndiverge after a couple of seconds due to an accumulation of errors that are fed\nback into the network. Furthermore, such methods have only been shown to be\nreliable for relatively simple human motions, such as walking or running. In\ncontrast, our approach can synthesize arbitrary motions with highly complex\nstyles, including dances or martial arts in addition to locomotion. The acRNN\nis able to accomplish this by explicitly accommodating for autoregressive noise\naccumulation during training. Our work is the first to our knowledge that\ndemonstrates the ability to generate over 18,000 continuous frames (300\nseconds) of new complex human motion w.r.t. different styles.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 18:45:29 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 22:55:25 GMT"}, {"version": "v3", "created": "Sat, 6 Jan 2018 00:39:13 GMT"}, {"version": "v4", "created": "Sat, 24 Feb 2018 03:36:57 GMT"}, {"version": "v5", "created": "Mon, 9 Jul 2018 21:23:27 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Li", "Zimo", ""], ["Zhou", "Yi", ""], ["Xiao", "Shuangjiu", ""], ["He", "Chong", ""], ["Huang", "Zeng", ""], ["Li", "Hao", ""]]}, {"id": "1707.05373", "submitter": "Moustapha Cisse", "authors": "Moustapha Cisse, Yossi Adi, Natalia Neverova and Joseph Keshet", "title": "Houdini: Fooling Deep Structured Prediction Models", "comments": "12 pages, 8 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating adversarial examples is a critical step for evaluating and\nimproving the robustness of learning machines. So far, most existing methods\nonly work for classification and are not designed to alter the true performance\nmeasure of the problem at hand. We introduce a novel flexible approach named\nHoudini for generating adversarial examples specifically tailored for the final\nperformance measure of the task considered, be it combinatorial and\nnon-decomposable. We successfully apply Houdini to a range of applications such\nas speech recognition, pose estimation and semantic segmentation. In all cases,\nthe attacks based on Houdini achieve higher success rate than those based on\nthe traditional surrogates used to train the models while using a less\nperceptible adversarial perturbation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 19:11:08 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Cisse", "Moustapha", ""], ["Adi", "Yossi", ""], ["Neverova", "Natalia", ""], ["Keshet", "Joseph", ""]]}, {"id": "1707.05390", "submitter": "William Cohen", "authors": "William W. Cohen, Fan Yang, Kathryn Rivard Mazaitis", "title": "TensorLog: Deep Learning Meets Probabilistic DBs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an implementation of a probabilistic first-order logic called\nTensorLog, in which classes of logical queries are compiled into differentiable\nfunctions in a neural-network infrastructure such as Tensorflow or Theano. This\nleads to a close integration of probabilistic logical reasoning with\ndeep-learning infrastructure: in particular, it enables high-performance deep\nlearning frameworks to be used for tuning the parameters of a probabilistic\nlogic. Experimental results show that TensorLog scales to problems involving\nhundreds of thousands of knowledge-base triples and tens of thousands of\nexamples.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 20:37:08 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Cohen", "William W.", ""], ["Yang", "Fan", ""], ["Mazaitis", "Kathryn Rivard", ""]]}, {"id": "1707.05392", "submitter": "Yipeng Hu", "authors": "Yipeng Hu, Eli Gibson, Li-Lin Lee, Weidi Xie, Dean C. Barratt, Tom\n  Vercauteren, J. Alison Noble", "title": "Freehand Ultrasound Image Simulation with Spatially-Conditioned\n  Generative Adversarial Networks", "comments": "Accepted to MICCAI RAMBO 2017", "journal-ref": null, "doi": "10.1007/978-3-319-67564-0_11", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sonography synthesis has a wide range of applications, including medical\nprocedure simulation, clinical training and multimodality image registration.\nIn this paper, we propose a machine learning approach to simulate ultrasound\nimages at given 3D spatial locations (relative to the patient anatomy), based\non conditional generative adversarial networks (GANs). In particular, we\nintroduce a novel neural network architecture that can sample anatomically\naccurate images conditionally on spatial position of the (real or mock)\nfreehand ultrasound probe. To ensure an effective and efficient spatial\ninformation assimilation, the proposed spatially-conditioned GANs take\ncalibrated pixel coordinates in global physical space as conditioning input,\nand utilise residual network units and shortcuts of conditioning data in the\nGANs' discriminator and generator, respectively. Using optically tracked B-mode\nultrasound images, acquired by an experienced sonographer on a fetus phantom,\nwe demonstrate the feasibility of the proposed method by two sets of\nquantitative results: distances were calculated between corresponding\nanatomical landmarks identified in the held-out ultrasound images and the\nsimulated data at the same locations unseen to the networks; a usability study\nwas carried out to distinguish the simulated data from the real images. In\nsummary, we present what we believe are state-of-the-art visually realistic\nultrasound images, simulated by the proposed GAN architecture that is stable to\ntrain and capable of generating plausibly diverse image samples.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 20:48:28 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Hu", "Yipeng", ""], ["Gibson", "Eli", ""], ["Lee", "Li-Lin", ""], ["Xie", "Weidi", ""], ["Barratt", "Dean C.", ""], ["Vercauteren", "Tom", ""], ["Noble", "J. Alison", ""]]}, {"id": "1707.05420", "submitter": "Junyu Xuan", "authors": "Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu", "title": "Cooperative Hierarchical Dirichlet Processes: Superposition vs.\n  Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cooperative hierarchical structure is a common and significant data\nstructure observed in, or adopted by, many research areas, such as: text mining\n(author-paper-word) and multi-label classification (label-instance-feature).\nRenowned Bayesian approaches for cooperative hierarchical structure modeling\nare mostly based on topic models. However, these approaches suffer from a\nserious issue in that the number of hidden topics/factors needs to be fixed in\nadvance and an inappropriate number may lead to overfitting or underfitting.\nOne elegant way to resolve this issue is Bayesian nonparametric learning, but\nexisting work in this area still cannot be applied to cooperative hierarchical\nstructure modeling.\n  In this paper, we propose a cooperative hierarchical Dirichlet process (CHDP)\nto fill this gap. Each node in a cooperative hierarchical structure is assigned\na Dirichlet process to model its weights on the infinite hidden factors/topics.\nTogether with measure inheritance from hierarchical Dirichlet process, two\nkinds of measure cooperation, i.e., superposition and maximization, are defined\nto capture the many-to-many relationships in the cooperative hierarchical\nstructure. Furthermore, two constructive representations for CHDP, i.e.,\nstick-breaking and international restaurant process, are designed to facilitate\nthe model inference. Experiments on synthetic and real-world data with\ncooperative hierarchical structures demonstrate the properties and the ability\nof CHDP for cooperative hierarchical structure modeling and its potential for\npractical application scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 00:42:10 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Xuan", "Junyu", ""], ["Lu", "Jie", ""], ["Zhang", "Guangquan", ""], ["Da Xu", "Richard Yi", ""]]}, {"id": "1707.05422", "submitter": "Silvia Villa", "authors": "Simon Matet, Lorenzo Rosasco, Silvia Villa and Bang Long Vu", "title": "Don't relax: early stopping for convex regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing efficient regularization algorithms when\nregularization is encoded by a (strongly) convex functional. Unlike classical\npenalization methods based on a relaxation approach, we propose an iterative\nmethod where regularization is achieved via early stopping. Our results show\nthat the proposed procedure achieves the same recovery accuracy as penalization\nmethods, while naturally integrating computational considerations. An empirical\nanalysis on a number of problems provides promising results with respect to the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 00:46:31 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Matet", "Simon", ""], ["Rosasco", "Lorenzo", ""], ["Villa", "Silvia", ""], ["Vu", "Bang Long", ""]]}, {"id": "1707.05470", "submitter": "Zi Yin", "authors": "Zi Yin, Keng-hao Chang, Ruofei Zhang", "title": "DeepProbe: Information Directed Sequence Understanding and Chatbot\n  Design via Recurrent Neural Networks", "comments": "Proceedings of the 23rd ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining, 2017", "journal-ref": null, "doi": "10.1145/3097983.3098148", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information extraction and user intention identification are central topics\nin modern query understanding and recommendation systems. In this paper, we\npropose DeepProbe, a generic information-directed interaction framework which\nis built around an attention-based sequence to sequence (seq2seq) recurrent\nneural network. DeepProbe can rephrase, evaluate, and even actively ask\nquestions, leveraging the generative ability and likelihood estimation made\npossible by seq2seq models. DeepProbe makes decisions based on a derived\nuncertainty (entropy) measure conditioned on user inputs, possibly with\nmultiple rounds of interactions. Three applications, namely a rewritter, a\nrelevance scorer and a chatbot for ad recommendation, were built around\nDeepProbe, with the first two serving as precursory building blocks for the\nthird. We first use the seq2seq model in DeepProbe to rewrite a user query into\none of standard query form, which is submitted to an ordinary recommendation\nsystem. Secondly, we evaluate DeepProbe's seq2seq model-based relevance\nscoring. Finally, we build a chatbot prototype capable of making active user\ninteractions, which can ask questions that maximize information gain, allowing\nfor a more efficient user intention idenfication process. We evaluate first two\napplications by 1) comparing with baselines by BLEU and AUC, and 2) human judge\nevaluation. Both demonstrate significant improvements compared with current\nstate-of-the-art systems, proving their values as useful tools on their own,\nand at the same time laying a good foundation for the ongoing chatbot\napplication.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 05:12:09 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 17:05:00 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Yin", "Zi", ""], ["Chang", "Keng-hao", ""], ["Zhang", "Ruofei", ""]]}, {"id": "1707.05489", "submitter": "Omid Poursaeed", "authors": "Omid Poursaeed, Tomas Matera, Serge Belongie", "title": "Vision-based Real Estate Price Estimation", "comments": null, "journal-ref": "Machine Vision and Applications, 29(4), 667-676, 2018", "doi": "10.1007/s00138-018-0922-2", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the advent of online real estate database companies like Zillow, Trulia\nand Redfin, the problem of automatic estimation of market values for houses has\nreceived considerable attention. Several real estate websites provide such\nestimates using a proprietary formula. Although these estimates are often close\nto the actual sale prices, in some cases they are highly inaccurate. One of the\nkey factors that affects the value of a house is its interior and exterior\nappearance, which is not considered in calculating automatic value estimates.\nIn this paper, we evaluate the impact of visual characteristics of a house on\nits market value. Using deep convolutional neural networks on a large dataset\nof photos of home interiors and exteriors, we develop a method for estimating\nthe luxury level of real estate photos. We also develop a novel framework for\nautomated value assessment using the above photos in addition to home\ncharacteristics including size, offered price and number of bedrooms. Finally,\nby applying our proposed method for price estimation to a new dataset of real\nestate photos and metadata, we show that it outperforms Zillow's estimates.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 06:29:02 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 19:47:00 GMT"}, {"version": "v3", "created": "Wed, 3 Oct 2018 15:39:27 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Poursaeed", "Omid", ""], ["Matera", "Tomas", ""], ["Belongie", "Serge", ""]]}, {"id": "1707.05497", "submitter": "Maryam Aliakbarpour", "authors": "Maryam Aliakbarpour, Ilias Diakonikolas, Ronitt Rubinfeld", "title": "Differentially Private Identity and Closeness Testing of Discrete\n  Distributions", "comments": "Submitted, May 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problems of identity and closeness testing over a discrete\npopulation from random samples. Our goal is to develop efficient testers while\nguaranteeing Differential Privacy to the individuals of the population. We\ndescribe an approach that yields sample-efficient differentially private\ntesters for these problems. Our theoretical results show that there exist\nprivate identity and closeness testers that are nearly as sample-efficient as\ntheir non-private counterparts. We perform an experimental evaluation of our\nalgorithms on synthetic data. Our experiments illustrate that our private\ntesters achieve small type I and type II errors with sample size sublinear in\nthe domain size of the underlying distributions.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 06:51:31 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Aliakbarpour", "Maryam", ""], ["Diakonikolas", "Ilias", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "1707.05499", "submitter": "Anirban Laha", "authors": "Disha Shrivastava, Saneem Ahmed CG, Anirban Laha, Karthik\n  Sankaranarayanan", "title": "A Machine Learning Approach for Evaluating Creative Artifacts", "comments": "Accepted at SIGKDD Workshop on Machine Learning for Creativity\n  (ML4Creativity), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much work has been done in understanding human creativity and defining\nmeasures to evaluate creativity. This is necessary mainly for the reason of\nhaving an objective and automatic way of quantifying creative artifacts. In\nthis work, we propose a regression-based learning framework which takes into\naccount quantitatively the essential criteria for creativity like novelty,\ninfluence, value and unexpectedness. As it is often the case with most creative\ndomains, there is no clear ground truth available for creativity. Our proposed\nlearning framework is applicable to all creative domains; yet we evaluate it on\na dataset of movies created from IMDb and Rotten Tomatoes due to availability\nof audience and critic scores, which can be used as proxy ground truth labels\nfor creativity. We report promising results and observations from our\nexperiments in the following ways : 1) Correlation of creative criteria with\ncritic scores, 2) Improvement in movie rating prediction with inclusion of\nvarious creative criteria, and 3) Identification of creative movies.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 06:59:45 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Shrivastava", "Disha", ""], ["CG", "Saneem Ahmed", ""], ["Laha", "Anirban", ""], ["Sankaranarayanan", "Karthik", ""]]}, {"id": "1707.05532", "submitter": "Florian Wenzel", "authors": "Florian Wenzel, Theo Galy-Fajou, Matthaeus Deutsch, Marius Kloft", "title": "Bayesian Nonlinear Support Vector Machines for Big Data", "comments": "accepted as conference paper at ECML-PKDD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast inference method for Bayesian nonlinear support vector\nmachines that leverages stochastic variational inference and inducing points.\nOur experiments show that the proposed method is faster than competing Bayesian\napproaches and scales easily to millions of data points. It provides additional\nfeatures over frequentist competitors such as accurate predictive uncertainty\nestimates and automatic hyperparameter search.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 09:16:50 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Wenzel", "Florian", ""], ["Galy-Fajou", "Theo", ""], ["Deutsch", "Matthaeus", ""], ["Kloft", "Marius", ""]]}, {"id": "1707.05533", "submitter": "Fabien Lauer", "authors": "Fabien Lauer (ABC)", "title": "Global optimization for low-dimensional switching linear regression and\n  bounded-error estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper provides global optimization algorithms for two particularly\ndifficult nonconvex problems raised by hybrid system identification: switching\nlinear regression and bounded-error estimation. While most works focus on local\noptimization heuristics without global optimality guarantees or with guarantees\nvalid only under restrictive conditions, the proposed approach always yields a\nsolution with a certificate of global optimality. This approach relies on a\nbranch-and-bound strategy for which we devise lower bounds that can be\nefficiently computed. In order to obtain scalable algorithms with respect to\nthe number of data, we directly optimize the model parameters in a continuous\noptimization setting without involving integer variables. Numerical experiments\nshow that the proposed algorithms offer a higher accuracy than convex\nrelaxations with a reasonable computational burden for hybrid system\nidentification. In addition, we discuss how bounded-error estimation is related\nto robust estimation in the presence of outliers and exact recovery under\nsparse noise, for which we also obtain promising numerical results.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 09:18:22 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 13:43:20 GMT"}, {"version": "v3", "created": "Thu, 23 Nov 2017 12:49:54 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Lauer", "Fabien", "", "ABC"]]}, {"id": "1707.05534", "submitter": "Erik Bodin", "authors": "Erik Bodin, Neill D. F. Campbell, Carl Henrik Ek", "title": "Latent Gaussian Process Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Latent Gaussian Process Regression which is a latent variable\nextension allowing modelling of non-stationary multi-modal processes using GPs.\nThe approach is built on extending the input space of a regression problem with\na latent variable that is used to modulate the covariance function over the\ntraining data. We show how our approach can be used to model multi-modal and\nnon-stationary processes. We exemplify the approach on a set of synthetic data\nand provide results on real data from motion capture and geostatistics.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 09:19:20 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 18:18:03 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Bodin", "Erik", ""], ["Campbell", "Neill D. F.", ""], ["Ek", "Carl Henrik", ""]]}, {"id": "1707.05562", "submitter": "Jordan Burgess", "authors": "Jordan Burgess, James Robert Lloyd, Zoubin Ghahramani", "title": "One-Shot Learning in Discriminative Neural Networks", "comments": "3 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of one-shot learning of visual categories. In this paper\nwe explore a Bayesian procedure for updating a pretrained convnet to classify a\nnovel image category for which data is limited. We decompose this convnet into\na fixed feature extractor and softmax classifier. We assume that the target\nweights for the new task come from the same distribution as the pretrained\nsoftmax weights, which we model as a multivariate Gaussian. By using this as a\nprior for the new weights, we demonstrate competitive performance with\nstate-of-the-art methods whilst also being consistent with 'normal' methods for\ntraining deep networks on large data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 11:17:22 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Burgess", "Jordan", ""], ["Lloyd", "James Robert", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1707.05587", "submitter": "Hermina Petric Maretic", "authors": "Hermina Petric Maretic, Dorina Thanou, Pascal Frossard", "title": "Graph learning under sparsity priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph signals offer a very generic and natural representation for data that\nlives on networks or irregular structures. The actual data structure is however\noften unknown a priori but can sometimes be estimated from the knowledge of the\napplication domain. If this is not possible, the data structure has to be\ninferred from the mere signal observations. This is exactly the problem that we\naddress in this paper, under the assumption that the graph signals can be\nrepresented as a sparse linear combination of a few atoms of a structured graph\ndictionary. The dictionary is constructed on polynomials of the graph\nLaplacian, which can sparsely represent a general class of graph signals\ncomposed of localized patterns on the graph. We formulate a graph learning\nproblem, whose solution provides an ideal fit between the signal observations\nand the sparse graph signal model. As the problem is non-convex, we propose to\nsolve it by alternating between a signal sparse coding and a graph update step.\nWe provide experimental results that outline the good graph recovery\nperformance of our method, which generally compares favourably to other recent\nnetwork inference algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 12:31:53 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Maretic", "Hermina Petric", ""], ["Thanou", "Dorina", ""], ["Frossard", "Pascal", ""]]}, {"id": "1707.05612", "submitter": "Fartash Faghri", "authors": "Fartash Faghri, David J. Fleet, Jamie Ryan Kiros and Sanja Fidler", "title": "VSE++: Improving Visual-Semantic Embeddings with Hard Negatives", "comments": "Accepted as spotlight presentation at British Machine Vision\n  Conference (BMVC) 2018. Code: https://github.com/fartashf/vsepp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new technique for learning visual-semantic embeddings for\ncross-modal retrieval. Inspired by hard negative mining, the use of hard\nnegatives in structured prediction, and ranking loss functions, we introduce a\nsimple change to common loss functions used for multi-modal embeddings. That,\ncombined with fine-tuning and use of augmented data, yields significant gains\nin retrieval performance. We showcase our approach, VSE++, on MS-COCO and\nFlickr30K datasets, using ablation studies and comparisons with existing\nmethods. On MS-COCO our approach outperforms state-of-the-art methods by 8.8%\nin caption retrieval and 11.3% in image retrieval (at R@1).\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 13:51:32 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 15:55:21 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 20:42:43 GMT"}, {"version": "v4", "created": "Sun, 29 Jul 2018 19:11:57 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Faghri", "Fartash", ""], ["Fleet", "David J.", ""], ["Kiros", "Jamie Ryan", ""], ["Fidler", "Sanja", ""]]}, {"id": "1707.05662", "submitter": "Vasilis Kontonis", "authors": "Dimitris Fotakis, Vasilis Kontonis, Piotr Krysta, and Paul Spirakis", "title": "Learning Powers of Poisson Binomial Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of simultaneously learning all powers of a Poisson\nBinomial Distribution (PBD). A PBD of order $n$ is the distribution of a sum of\n$n$ mutually independent Bernoulli random variables $X_i$, where\n$\\mathbb{E}[X_i] = p_i$. The $k$'th power of this distribution, for $k$ in a\nrange $[m]$, is the distribution of $P_k = \\sum_{i=1}^n X_i^{(k)}$, where each\nBernoulli random variable $X_i^{(k)}$ has $\\mathbb{E}[X_i^{(k)}] = (p_i)^k$.\nThe learning algorithm can query any power $P_k$ several times and succeeds in\nlearning all powers in the range, if with probability at least $1- \\delta$:\ngiven any $k \\in [m]$, it returns a probability distribution $Q_k$ with total\nvariation distance from $P_k$ at most $\\epsilon$. We provide almost matching\nlower and upper bounds on query complexity for this problem. We first show a\nlower bound on the query complexity on PBD powers instances with many distinct\nparameters $p_i$ which are separated, and we almost match this lower bound by\nexamining the query complexity of simultaneously learning all the powers of a\nspecial class of PBD's resembling the PBD's of our lower bound. We study the\nfundamental setting of a Binomial distribution, and provide an optimal\nalgorithm which uses $O(1/\\epsilon^2)$ samples. Diakonikolas, Kane and Stewart\n[COLT'16] showed a lower bound of $\\Omega(2^{1/\\epsilon})$ samples to learn the\n$p_i$'s within error $\\epsilon$. The question whether sampling from powers of\nPBDs can reduce this sampling complexity, has a negative answer since we show\nthat the exponential number of samples is inevitable. Having sampling access to\nthe powers of a PBD we then give a nearly optimal algorithm that learns its\n$p_i$'s. To prove our two last lower bounds we extend the classical minimax\nrisk definition from statistics to estimating functions of sequences of\ndistributions.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 15:02:43 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Fotakis", "Dimitris", ""], ["Kontonis", "Vasilis", ""], ["Krysta", "Piotr", ""], ["Spirakis", "Paul", ""]]}, {"id": "1707.05668", "submitter": "Erwan Lecarpentier", "authors": "Erwan Lecarpentier, Sebastian Rapp, Marc Melo, Emmanuel Rachelson", "title": "Empirical evaluation of a Q-Learning Algorithm for Model-free Autonomous\n  Soaring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous unpowered flight is a challenge for control and guidance systems:\nall the energy the aircraft might use during flight has to be harvested\ndirectly from the atmosphere. We investigate the design of an algorithm that\noptimizes the closed-loop control of a glider's bank and sideslip angles, while\nflying in the lower convective layer of the atmosphere in order to increase its\nmission endurance. Using a Reinforcement Learning approach, we demonstrate the\npossibility for real-time adaptation of the glider's behaviour to the\ntime-varying and noisy conditions associated with thermal soaring flight. Our\napproach is online, data-based and model-free, hence avoids the pitfalls of\naerological and aircraft modelling and allow us to deal with uncertainties and\nnon-stationarity. Additionally, we put a particular emphasis on keeping low\ncomputational requirements in order to make on-board execution feasible. This\narticle presents the stochastic, time-dependent aerological model used for\nsimulation, together with a standard aircraft model. Then we introduce an\nadaptation of a Q-learning algorithm and demonstrate its ability to control the\naircraft and improve its endurance by exploiting updrafts in non-stationary\nscenarios.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 15:06:01 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Lecarpentier", "Erwan", ""], ["Rapp", "Sebastian", ""], ["Melo", "Marc", ""], ["Rachelson", "Emmanuel", ""]]}, {"id": "1707.05721", "submitter": "Jun Qi", "authors": "Jun Qi", "title": "Submodular Mini-Batch Training in Generative Moment Matching Networks", "comments": "The paper has been withdrawn. See the abstract for the reason", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This article was withdrawn because (1) it was uploaded without the\nco-authors' knowledge or consent, and (2) there are allegations of plagiarism.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 16:04:08 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 18:34:12 GMT"}, {"version": "v3", "created": "Thu, 3 Aug 2017 14:32:30 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Qi", "Jun", ""]]}, {"id": "1707.05729", "submitter": "Ruben Martinez-Cantin", "authors": "Ruben Martinez-Cantin, Michael McCourt, Kevin Tee", "title": "Robust Bayesian Optimization with Student-t Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization has recently attracted the attention of the automatic\nmachine learning community for its excellent results in hyperparameter tuning.\nBO is characterized by the sample efficiency with which it can optimize\nexpensive black-box functions. The efficiency is achieved in a similar fashion\nto the learning to learn methods: surrogate models (typically in the form of\nGaussian processes) learn the target function and perform intelligent sampling.\nThis surrogate model can be applied even in the presence of noise; however, as\nwith most regression methods, it is very sensitive to outlier data. This can\nresult in erroneous predictions and, in the case of BO, biased and inefficient\nexploration. In this work, we present a GP model that is robust to outliers\nwhich uses a Student-t likelihood to segregate outliers and robustly conduct\nBayesian optimization. We present numerical results evaluating the proposed\nmethod in both artificial functions and real problems.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 16:22:07 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Martinez-Cantin", "Ruben", ""], ["McCourt", "Michael", ""], ["Tee", "Kevin", ""]]}, {"id": "1707.05733", "submitter": "Oier Mees", "authors": "Oier Mees, Andreas Eitel, Wolfram Burgard", "title": "Choosing Smartly: Adaptive Multimodal Fusion for Object Detection in\n  Changing Environments", "comments": "Published at the 2016 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems. Added a new baseline with respect to the IROS\n  version. Project page with code, pretrained models and our InOutDoorPeople\n  RGB-D dataset at http://adaptivefusion.cs.uni-freiburg.de/", "journal-ref": null, "doi": "10.1109/IROS.2016.7759048", "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is an essential task for autonomous robots operating in\ndynamic and changing environments. A robot should be able to detect objects in\nthe presence of sensor noise that can be induced by changing lighting\nconditions for cameras and false depth readings for range sensors, especially\nRGB-D cameras. To tackle these challenges, we propose a novel adaptive fusion\napproach for object detection that learns weighting the predictions of\ndifferent sensor modalities in an online manner. Our approach is based on a\nmixture of convolutional neural network (CNN) experts and incorporates multiple\nmodalities including appearance, depth and motion. We test our method in\nextensive robot experiments, in which we detect people in a combined indoor and\noutdoor scenario from RGB-D data, and we demonstrate that our method can adapt\nto harsh lighting changes and severe camera motion blur. Furthermore, we\npresent a new RGB-D dataset for people detection in mixed in- and outdoor\nenvironments, recorded with a mobile robot. Code, pretrained models and dataset\nare available at http://adaptivefusion.cs.uni-freiburg.de\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 16:36:56 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 12:43:54 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Mees", "Oier", ""], ["Eitel", "Andreas", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1707.05776", "submitter": "Piotr Bojanowski", "authors": "Piotr Bojanowski, Armand Joulin, David Lopez-Paz, Arthur Szlam", "title": "Optimizing the Latent Space of Generative Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have achieved remarkable results in\nthe task of generating realistic natural images. In most successful\napplications, GAN models share two common aspects: solving a challenging saddle\npoint optimization problem, interpreted as an adversarial game between a\ngenerator and a discriminator functions; and parameterizing the generator and\nthe discriminator as deep convolutional neural networks. The goal of this paper\nis to disentangle the contribution of these two factors to the success of GANs.\nIn particular, we introduce Generative Latent Optimization (GLO), a framework\nto train deep convolutional generators using simple reconstruction losses.\nThroughout a variety of experiments, we show that GLO enjoys many of the\ndesirable properties of GANs: synthesizing visually-appealing samples,\ninterpolating meaningfully between samples, and performing linear arithmetic\nwith noise vectors; all of this without the adversarial optimization scheme.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 17:58:34 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 13:19:44 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Bojanowski", "Piotr", ""], ["Joulin", "Armand", ""], ["Lopez-Paz", "David", ""], ["Szlam", "Arthur", ""]]}, {"id": "1707.05807", "submitter": "Ioannis Mitliagkas", "authors": "Ioannis Mitliagkas and Lester Mackey", "title": "Improving Gibbs Sampler Scan Quality with DoGS", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pairwise influence matrix of Dobrushin has long been used as an\nanalytical tool to bound the rate of convergence of Gibbs sampling. In this\nwork, we use Dobrushin influence as the basis of a practical tool to certify\nand efficiently improve the quality of a discrete Gibbs sampler. Our\nDobrushin-optimized Gibbs samplers (DoGS) offer customized variable selection\norders for a given sampling budget and variable subset of interest, explicit\nbounds on total variation distance to stationarity, and certifiable\nimprovements over the standard systematic and uniform random scan Gibbs\nsamplers. In our experiments with joint image segmentation and object\nrecognition, Markov chain Monte Carlo maximum likelihood estimation, and Ising\nmodel inference, DoGS consistently deliver higher-quality inferences with\nsignificantly smaller sampling budgets than standard Gibbs samplers.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 18:17:55 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Mitliagkas", "Ioannis", ""], ["Mackey", "Lester", ""]]}, {"id": "1707.05828", "submitter": "Maria Van Der Walt", "authors": "H.N. Mhaskar, S.V. Pereverzyev and M.D. van der Walt", "title": "A deep learning approach to diabetic blood glucose prediction", "comments": null, "journal-ref": "Front. Appl. Math. Stat., 14 July 2017", "doi": "10.3389/fams.2017.00014", "report-no": null, "categories": "cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the question of 30-minute prediction of blood glucose levels\nmeasured by continuous glucose monitoring devices, using clinical data. While\nmost studies of this nature deal with one patient at a time, we take a certain\npercentage of patients in the data set as training data, and test on the\nremainder of the patients; i.e., the machine need not re-calibrate on the new\npatients in the data set. We demonstrate how deep learning can outperform\nshallow networks in this example. One novelty is to demonstrate how a\nparsimonious deep representation can be constructed using domain knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 19:21:29 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Mhaskar", "H. N.", ""], ["Pereverzyev", "S. V.", ""], ["van der Walt", "M. D.", ""]]}, {"id": "1707.05840", "submitter": "Randall Balestriero", "authors": "Randall Balestriero", "title": "Multiscale Residual Mixture of PCA: Dynamic Dictionaries for Optimal\n  Basis Learning", "comments": "Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are interested in the problem of learning an over-complete\nbasis and a methodology such that the reconstruction or inverse problem does\nnot need optimization. We analyze the optimality of the presented approaches,\ntheir link to popular already known techniques s.a. Artificial Neural\nNetworks,k-means or Oja's learning rule. Finally, we will see that one approach\nto reach the optimal dictionary is a factorial and hierarchical approach. The\nderived approach lead to a formulation of a Deep Oja Network. We present\nresults on different tasks and present the resulting very efficient learning\nalgorithm which brings a new vision on the training of deep nets. Finally, the\ntheoretical work shows that deep frameworks are one way to efficiently have\nover-complete (combinatorially large) dictionary yet allowing easy\nreconstruction. We thus present the Deep Residual Oja Network (DRON). We\ndemonstrate that a recursive deep approach working on the residuals allow\nexponential decrease of the error w.r.t. the depth.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 19:56:05 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Balestriero", "Randall", ""]]}, {"id": "1707.05841", "submitter": "Randall Balestriero", "authors": "Randall Balestriero, Herve Glotin", "title": "Linear Time Complexity Deep Fourier Scattering Network and Extension to\n  Nonlinear Invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a scalable version of a state-of-the-art\ndeterministic time-invariant feature extraction approach based on consecutive\nchanges of basis and nonlinearities, namely, the scattering network. The first\nfocus of the paper is to extend the scattering network to allow the use of\nhigher order nonlinearities as well as extracting nonlinear and Fourier based\nstatistics leading to the required invariants of any inherently structured\ninput. In order to reach fast convolutions and to leverage the intrinsic\nstructure of wavelets, we derive our complete model in the Fourier domain. In\naddition of providing fast computations, we are now able to exploit sparse\nmatrices due to extremely high sparsity well localized in the Fourier domain.\nAs a result, we are able to reach a true linear time complexity with inputs in\nthe Fourier domain allowing fast and energy efficient solutions to machine\nlearning tasks. Validation of the features and computational results will be\npresented through the use of these invariant coefficients to perform\nclassification on audio recordings of bird songs captured in multiple different\nsoundscapes. In the end, the applicability of the presented solutions to deep\nartificial neural networks is discussed.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 20:04:35 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Balestriero", "Randall", ""], ["Glotin", "Herve", ""]]}, {"id": "1707.05878", "submitter": "Elena Mocanu", "authors": "Elena Mocanu, Decebal Constantin Mocanu, Phuong H. Nguyen, Antonio\n  Liotta, Michael E. Webber, Madeleine Gibescu, J.G. Slootweg", "title": "On-line Building Energy Optimization using Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unprecedented high volumes of data are becoming available with the growth of\nthe advanced metering infrastructure. These are expected to benefit planning\nand operation of the future power system, and to help the customers transition\nfrom a passive to an active role. In this paper, we explore for the first time\nin the smart grid context the benefits of using Deep Reinforcement Learning, a\nhybrid type of methods that combines Reinforcement Learning with Deep Learning,\nto perform on-line optimization of schedules for building energy management\nsystems. The learning procedure was explored using two methods, Deep Q-learning\nand Deep Policy Gradient, both of them being extended to perform multiple\nactions simultaneously. The proposed approach was validated on the large-scale\nPecan Street Inc. database. This highly-dimensional database includes\ninformation about photovoltaic power generation, electric vehicles as well as\nbuildings appliances. Moreover, these on-line energy scheduling strategies\ncould be used to provide real-time feedback to consumers to encourage more\nefficient use of electricity.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 22:00:53 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Mocanu", "Elena", ""], ["Mocanu", "Decebal Constantin", ""], ["Nguyen", "Phuong H.", ""], ["Liotta", "Antonio", ""], ["Webber", "Michael E.", ""], ["Gibescu", "Madeleine", ""], ["Slootweg", "J. G.", ""]]}, {"id": "1707.05909", "submitter": "Felipe Tobar", "authors": "Felipe Tobar, Gonzalo Rios, Tom\\'as Valdivia, Pablo Guerrero", "title": "Recovering Latent Signals from a Mixture of Measurements using a\n  Gaussian Process Prior", "comments": "Published on IEEE Signal Processing Letters on Dec. 2016", "journal-ref": "IEEE Signal Processing Letters, vol. 24, no. 2, pp. 231-235, Feb.\n  2017", "doi": "10.1109/LSP.2016.2637312", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sensing applications, sensors cannot always measure the latent quantity of\ninterest at the required resolution, sometimes they can only acquire a blurred\nversion of it due the sensor's transfer function. To recover latent signals\nwhen only noisy mixed measurements of the signal are available, we propose the\nGaussian process mixture of measurements (GPMM), which models the latent signal\nas a Gaussian process (GP) and allows us to perform Bayesian inference on such\nsignal conditional to a set of noisy mixture of measurements. We describe how\nto train GPMM, that is, to find the hyperparameters of the GP and the mixing\nweights, and how to perform inference on the latent signal under GPMM;\nadditionally, we identify the solution to the underdetermined linear system\nresulting from a sensing application as a particular case of GPMM. The proposed\nmodel is validated in the recovery of three signals: a smooth synthetic signal,\na real-world heart-rate time series and a step function, where GPMM\noutperformed the standard GP in terms of estimation error, uncertainty\nrepresentation and recovery of the spectral content of the latent signal.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 00:56:11 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Tobar", "Felipe", ""], ["Rios", "Gonzalo", ""], ["Valdivia", "Tom\u00e1s", ""], ["Guerrero", "Pablo", ""]]}, {"id": "1707.05926", "submitter": "Qiao Wang", "authors": "Qiao Wang, Zheng Wang, Xiaojun Ye", "title": "Equivalence between LINE and Matrix Factorization", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LINE [1], as an efficient network embedding method, has shown its\neffectiveness in dealing with large-scale undirected, directed, and/or weighted\nnetworks. Particularly, it proposes to preserve both the local structure\n(represented by First-order Proximity) and global structure (represented by\nSecond-order Proximity) of the network. In this study, we prove that LINE with\nthese two proximities (LINE(1st) and LINE(2nd)) are actually factoring two\ndifferent matrices separately. Specifically, LINE(1st) is factoring a matrix M\n(1), whose entries are the doubled Pointwise Mutual Information (PMI) of vertex\npairs in undirected networks, shifted by a constant. LINE(2nd) is factoring a\nmatrix M (2), whose entries are the PMI of vertex and context pairs in directed\nnetworks, shifted by a constant. We hope this finding would provide a basis for\nfurther extensions and generalizations of LINE.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 03:12:35 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 00:12:54 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Wang", "Qiao", ""], ["Wang", "Zheng", ""], ["Ye", "Xiaojun", ""]]}, {"id": "1707.05947", "submitter": "Wenlong Mou", "authors": "Wenlong Mou, Liwei Wang, Xiyu Zhai, Kai Zheng", "title": "Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical\n  Viewpoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithm-dependent generalization error bounds are central to statistical\nlearning theory. A learning algorithm may use a large hypothesis space, but the\nlimited number of iterations controls its model capacity and generalization\nerror. The impacts of stochastic gradient methods on generalization error for\nnon-convex learning problems not only have important theoretical consequences,\nbut are also critical to generalization errors of deep learning.\n  In this paper, we study the generalization errors of Stochastic Gradient\nLangevin Dynamics (SGLD) with non-convex objectives. Two theories are proposed\nwith non-asymptotic discrete-time analysis, using Stability and PAC-Bayesian\nresults respectively. The stability-based theory obtains a bound of\n$O\\left(\\frac{1}{n}L\\sqrt{\\beta T_k}\\right)$, where $L$ is uniform Lipschitz\nparameter, $\\beta$ is inverse temperature, and $T_k$ is aggregated step sizes.\nFor PAC-Bayesian theory, though the bound has a slower $O(1/\\sqrt{n})$ rate,\nthe contribution of each step is shown with an exponentially decaying factor by\nimposing $\\ell^2$ regularization, and the uniform Lipschitz constant is also\nreplaced by actual norms of gradients along trajectory. Our bounds have no\nimplicit dependence on dimensions, norms or other capacity measures of\nparameter, which elegantly characterizes the phenomenon of \"Fast Training\nGuarantees Generalization\" in non-convex settings. This is the first\nalgorithm-dependent result with reasonable dependence on aggregated step sizes\nfor non-convex learning, and has important implications to statistical learning\naspects of stochastic gradient methods in complicated models such as deep\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 06:17:57 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Mou", "Wenlong", ""], ["Wang", "Liwei", ""], ["Zhai", "Xiyu", ""], ["Zheng", "Kai", ""]]}, {"id": "1707.05970", "submitter": "Ishai Rosenberg", "authors": "Ishai Rosenberg, Asaf Shabtai, Lior Rokach, and Yuval Elovici", "title": "Generic Black-Box End-to-End Attack Against State of the Art API Call\n  Based Malware Classifiers", "comments": "Accepted as a conference paper at RAID 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a black-box attack against API call based machine\nlearning malware classifiers, focusing on generating adversarial sequences\ncombining API calls and static features (e.g., printable strings) that will be\nmisclassified by the classifier without affecting the malware functionality. We\nshow that this attack is effective against many classifiers due to the\ntransferability principle between RNN variants, feed forward DNNs, and\ntraditional machine learning classifiers such as SVM. We also implement GADGET,\na software framework to convert any malware binary to a binary undetected by\nmalware classifiers, using the proposed attack, without access to the malware\nsource code.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 08:16:31 GMT"}, {"version": "v2", "created": "Sat, 21 Oct 2017 04:57:55 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 11:05:26 GMT"}, {"version": "v4", "created": "Thu, 15 Feb 2018 20:39:31 GMT"}, {"version": "v5", "created": "Sun, 24 Jun 2018 21:03:21 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Rosenberg", "Ishai", ""], ["Shabtai", "Asaf", ""], ["Rokach", "Lior", ""], ["Elovici", "Yuval", ""]]}, {"id": "1707.05987", "submitter": "James Ridgway", "authors": "James Ridgway", "title": "Probably approximate Bayesian computation: nonasymptotic convergence of\n  ABC under misspecification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is a widely used inference method in\nBayesian statistics to bypass the point-wise computation of the likelihood. In\nthis paper we develop theoretical bounds for the distance between the\nstatistics used in ABC. We show that some versions of ABC are inherently robust\nto misspecification. The bounds are given in the form of oracle inequalities\nfor a finite sample size. The dependence on the dimension of the parameter\nspace and the number of statistics is made explicit. The results are shown to\nbe amenable to oracle inequalities in parameter space. We apply our theoretical\nresults to given prior distributions and data generating processes, including a\nnon-parametric regression model. In a second part of the paper, we propose a\nsequential Monte Carlo (SMC) to sample from the pseudo-posterior, improving\nupon the state of the art samplers.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 09:04:34 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 14:06:20 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Ridgway", "James", ""]]}, {"id": "1707.06065", "submitter": "Taesup Kim", "authors": "Taesup Kim, Inchul Song, Yoshua Bengio", "title": "Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in\n  Speech Recognition", "comments": "INTERSPEECH 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layer normalization is a recently introduced technique for normalizing the\nactivities of neurons in deep neural networks to improve the training speed and\nstability. In this paper, we introduce a new layer normalization technique\ncalled Dynamic Layer Normalization (DLN) for adaptive neural acoustic modeling\nin speech recognition. By dynamically generating the scaling and shifting\nparameters in layer normalization, DLN adapts neural acoustic models to the\nacoustic variability arising from various factors such as speakers, channel\nnoises, and environments. Unlike other adaptive acoustic models, our proposed\napproach does not require additional adaptation data or speaker information\nsuch as i-vectors. Moreover, the model size is fixed as it dynamically\ngenerates adaptation parameters. We apply our proposed DLN to deep\nbidirectional LSTM acoustic models and evaluate them on two benchmark datasets\nfor large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The\nexperimental results show that our DLN improves neural acoustic models in terms\nof transcription accuracy by dynamically adapting to various speakers and\nenvironments.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 13:04:09 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Kim", "Taesup", ""], ["Song", "Inchul", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1707.06142", "submitter": "Emmanuel Rachelson", "authors": "Luca Mossina, Emmanuel Rachelson", "title": "Naive Bayes Classification for Subset Selection", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article focuses on the question of learning how to automatically select\na subset of items among a bigger set. We introduce a methodology for the\ninference of ensembles of discrete values, based on the Naive Bayes assumption.\nOur motivation stems from practical use cases where one wishes to predict an\nunordered set of (possibly interdependent) values from a set of observed\nfeatures. This problem can be considered in the context of Multi-label\nClassification (MLC) where such values are seen as labels associated to\ncontinuous or discrete features. We introduce the \\nbx algorithm, an extension\nof Naive Bayes classification into the multi-label domain, discuss its\nproperties and evaluate our approach on real-world problems.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 15:10:48 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Mossina", "Luca", ""], ["Rachelson", "Emmanuel", ""]]}, {"id": "1707.06145", "submitter": "Xiang Li", "authors": "Xiang Li, Aoxiao Zhong, Ming Lin, Ning Guo, Mu Sun, Arkadiusz Sitek,\n  Jieping Ye, James Thrall, Quanzheng Li", "title": "Self-paced Convolutional Neural Network for Computer Aided Detection in\n  Medical Imaging Analysis", "comments": "accepted by 8th International Workshop on Machine Learning in Medical\n  Imaging (MLMI 2017)", "journal-ref": null, "doi": "10.1007/978-3-319-67389-9_25", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tissue characterization has long been an important component of Computer\nAided Diagnosis (CAD) systems for automatic lesion detection and further\nclinical planning. Motivated by the superior performance of deep learning\nmethods on various computer vision problems, there has been increasing work\napplying deep learning to medical image analysis. However, the development of a\nrobust and reliable deep learning model for computer-aided diagnosis is still\nhighly challenging due to the combination of the high heterogeneity in the\nmedical images and the relative lack of training samples. Specifically,\nannotation and labeling of the medical images is much more expensive and\ntime-consuming than other applications and often involves manual labor from\nmultiple domain experts. In this work, we propose a multi-stage, self-paced\nlearning framework utilizing a convolutional neural network (CNN) to classify\nComputed Tomography (CT) image patches. The key contribution of this approach\nis that we augment the size of training samples by refining the unlabeled\ninstances with a self-paced learning CNN. By implementing the framework on high\nperformance computing servers including the NVIDIA DGX1 machine, we obtained\nthe experimental result, showing that the self-pace boosted network\nconsistently outperformed the original network even with very scarce manual\nlabels. The performance gain indicates that applications with limited training\nsamples such as medical image analysis can benefit from using the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 15:15:36 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Li", "Xiang", ""], ["Zhong", "Aoxiao", ""], ["Lin", "Ming", ""], ["Guo", "Ning", ""], ["Sun", "Mu", ""], ["Sitek", "Arkadiusz", ""], ["Ye", "Jieping", ""], ["Thrall", "James", ""], ["Li", "Quanzheng", ""]]}, {"id": "1707.06170", "submitter": "Razvan Pascanu", "authors": "Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing,\n  Sebastien Racani\\`ere, David Reichert, Th\\'eophane Weber, Daan Wierstra,\n  Peter Battaglia", "title": "Learning model-based planning from scratch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional wisdom holds that model-based planning is a powerful approach to\nsequential decision-making. It is often very challenging in practice, however,\nbecause while a model can be used to evaluate a plan, it does not prescribe how\nto construct a plan. Here we introduce the \"Imagination-based Planner\", the\nfirst model-based, sequential decision-making agent that can learn to\nconstruct, evaluate, and execute plans. Before any action, it can perform a\nvariable number of imagination steps, which involve proposing an imagined\naction and evaluating it with its model-based imagination. All imagined actions\nand outcomes are aggregated, iteratively, into a \"plan context\" which\nconditions future real and imagined actions. The agent can even decide how to\nimagine: testing out alternative imagined actions, chaining sequences of\nactions together, or building a more complex \"imagination tree\" by navigating\nflexibly among the previously imagined states using a learned policy. And our\nagent can learn to plan economically, jointly optimizing for external rewards\nand computational costs associated with using its imagination. We show that our\narchitecture can learn to solve a challenging continuous control problem, and\nalso learn elaborate planning strategies in a discrete maze-solving task. Our\nwork opens a new direction toward learning the components of a model-based\nplanning system and how to use them.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 15:52:35 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Pascanu", "Razvan", ""], ["Li", "Yujia", ""], ["Vinyals", "Oriol", ""], ["Heess", "Nicolas", ""], ["Buesing", "Lars", ""], ["Racani\u00e8re", "Sebastien", ""], ["Reichert", "David", ""], ["Weber", "Th\u00e9ophane", ""], ["Wierstra", "Daan", ""], ["Battaglia", "Peter", ""]]}, {"id": "1707.06175", "submitter": "Taylor Mordan", "authors": "Taylor Mordan, Nicolas Thome, Matthieu Cord and Gilles Henaff", "title": "Deformable Part-based Fully Convolutional Network for Object Detection", "comments": "Accepted to BMVC 2017 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing region-based object detectors are limited to regions with fixed box\ngeometry to represent objects, even if those are highly non-rectangular. In\nthis paper we introduce DP-FCN, a deep model for object detection which\nexplicitly adapts to shapes of objects with deformable parts. Without\nadditional annotations, it learns to focus on discriminative elements and to\nalign them, and simultaneously brings more invariance for classification and\ngeometric information to refine localization. DP-FCN is composed of three main\nmodules: a Fully Convolutional Network to efficiently maintain spatial\nresolution, a deformable part-based RoI pooling layer to optimize positions of\nparts and build invariance, and a deformation-aware localization module\nexplicitly exploiting displacements of parts to improve accuracy of bounding\nbox regression. We experimentally validate our model and show significant\ngains. DP-FCN achieves state-of-the-art performances of 83.1% and 80.9% on\nPASCAL VOC 2007 and 2012 with VOC data only.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 16:03:05 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Mordan", "Taylor", ""], ["Thome", "Nicolas", ""], ["Cord", "Matthieu", ""], ["Henaff", "Gilles", ""]]}, {"id": "1707.06197", "submitter": "Liu Weiyi", "authors": "Weiyi Liu and Pin-Yu Chen and Hal Cooper and Min Hwan Oh and Sailung\n  Yeung and Toyotaro Suzumura", "title": "Can GAN Learn Topological Features of a Graph?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is first-line research expanding GANs into graph topology\nanalysis. By leveraging the hierarchical connectivity structure of a graph, we\nhave demonstrated that generative adversarial networks (GANs) can successfully\ncapture topological features of any arbitrary graph, and rank edge sets by\ndifferent stages according to their contribution to topology reconstruction.\nMoreover, in addition to acting as an indicator of graph reconstruction, we\nfind that these stages can also preserve important topological features in a\ngraph.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 17:06:21 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Liu", "Weiyi", ""], ["Chen", "Pin-Yu", ""], ["Cooper", "Hal", ""], ["Oh", "Min Hwan", ""], ["Yeung", "Sailung", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "1707.06203", "submitter": "Th\\'eophane  Weber", "authors": "Th\\'eophane Weber, S\\'ebastien Racani\\`ere, David P. Reichert, Lars\n  Buesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdom\\`enech Badia,\n  Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia,\n  Demis Hassabis, David Silver, Daan Wierstra", "title": "Imagination-Augmented Agents for Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Imagination-Augmented Agents (I2As), a novel architecture for\ndeep reinforcement learning combining model-free and model-based aspects. In\ncontrast to most existing model-based reinforcement learning and planning\nmethods, which prescribe how a model should be used to arrive at a policy, I2As\nlearn to interpret predictions from a learned environment model to construct\nimplicit plans in arbitrary ways, by using the predictions as additional\ncontext in deep policy networks. I2As show improved data efficiency,\nperformance, and robustness to model misspecification compared to several\nbaselines.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 17:12:56 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 17:26:18 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Weber", "Th\u00e9ophane", ""], ["Racani\u00e8re", "S\u00e9bastien", ""], ["Reichert", "David P.", ""], ["Buesing", "Lars", ""], ["Guez", "Arthur", ""], ["Rezende", "Danilo Jimenez", ""], ["Badia", "Adria Puigdom\u00e8nech", ""], ["Vinyals", "Oriol", ""], ["Heess", "Nicolas", ""], ["Li", "Yujia", ""], ["Pascanu", "Razvan", ""], ["Battaglia", "Peter", ""], ["Hassabis", "Demis", ""], ["Silver", "David", ""], ["Wierstra", "Daan", ""]]}, {"id": "1707.06213", "submitter": "Matthew Thorpe", "authors": "Dejan Slep\\v{c}ev and Matthew Thorpe", "title": "Analysis of $p$-Laplacian Regularization in Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a family of regression problems in a semi-supervised setting.\nThe task is to assign real-valued labels to a set of $n$ sample points,\nprovided a small training subset of $N$ labeled points. A goal of\nsemi-supervised learning is to take advantage of the (geometric) structure\nprovided by the large number of unlabeled data when assigning labels. We\nconsider random geometric graphs, with connection radius $\\epsilon(n)$, to\nrepresent the geometry of the data set. Functionals which model the task reward\nthe regularity of the estimator function and impose or reward the agreement\nwith the training data. Here we consider the discrete $p$-Laplacian\nregularization.\n  We investigate asymptotic behavior when the number of unlabeled points\nincreases, while the number of training points remains fixed. We uncover a\ndelicate interplay between the regularizing nature of the functionals\nconsidered and the nonlocality inherent to the graph constructions. We\nrigorously obtain almost optimal ranges on the scaling of $\\epsilon(n)$ for the\nasymptotic consistency to hold. We prove that the minimizers of the discrete\nfunctionals in random setting converge uniformly to the desired continuum\nlimit. Furthermore we discover that for the standard model used there is a\nrestrictive upper bound on how quickly $\\epsilon(n)$ must converge to zero as\n$n \\to \\infty$. We introduce a new model which is as simple as the original\nmodel, but overcomes this restriction.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 17:31:14 GMT"}, {"version": "v2", "created": "Sun, 15 Oct 2017 18:32:44 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Slep\u010dev", "Dejan", ""], ["Thorpe", "Matthew", ""]]}, {"id": "1707.06217", "submitter": "Ashwin Pananjady", "authors": "Ashwin Pananjady, Cheng Mao, Vidya Muthukumar, Martin J. Wainwright,\n  Thomas A. Courtade", "title": "Worst-case vs Average-case Design for Estimation from Fixed Pairwise\n  Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise comparison data arises in many domains, including tournament\nrankings, web search, and preference elicitation. Given noisy comparisons of a\nfixed subset of pairs of items, we study the problem of estimating the\nunderlying comparison probabilities under the assumption of strong stochastic\ntransitivity (SST). We also consider the noisy sorting subclass of the SST\nmodel. We show that when the assignment of items to the topology is arbitrary,\nthese permutation-based models, unlike their parametric counterparts, do not\nadmit consistent estimation for most comparison topologies used in practice. We\nthen demonstrate that consistent estimation is possible when the assignment of\nitems to the topology is randomized, thus establishing a dichotomy between\nworst-case and average-case designs. We propose two estimators in the\naverage-case setting and analyze their risk, showing that it depends on the\ncomparison topology only through the degree sequence of the topology. The rates\nachieved by these estimators are shown to be optimal for a large class of\ngraphs. Our results are corroborated by simulations on multiple comparison\ntopologies.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 17:47:05 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Pananjady", "Ashwin", ""], ["Mao", "Cheng", ""], ["Muthukumar", "Vidya", ""], ["Wainwright", "Martin J.", ""], ["Courtade", "Thomas A.", ""]]}, {"id": "1707.06226", "submitter": "Debanjan Ghosh", "authors": "Debanjan Ghosh, Alexander Richard Fabbri, Smaranda Muresan", "title": "The Role of Conversation Context for Sarcasm Detection in Online\n  Interactions", "comments": "SIGDial 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational models for sarcasm detection have often relied on the content\nof utterances in isolation. However, speaker's sarcastic intent is not always\nobvious without additional context. Focusing on social media discussions, we\ninvestigate two issues: (1) does modeling of conversation context help in\nsarcasm detection and (2) can we understand what part of conversation context\ntriggered the sarcastic reply. To address the first issue, we investigate\nseveral types of Long Short-Term Memory (LSTM) networks that can model both the\nconversation context and the sarcastic response. We show that the conditional\nLSTM network (Rocktaschel et al., 2015) and LSTM networks with sentence level\nattention on context and response outperform the LSTM model that reads only the\nresponse. To address the second issue, we present a qualitative analysis of\nattention weights produced by the LSTM models with attention and discuss the\nresults compared with human performance on the task.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 01:21:26 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Ghosh", "Debanjan", ""], ["Fabbri", "Alexander Richard", ""], ["Muresan", "Smaranda", ""]]}, {"id": "1707.06231", "submitter": "Carlos Eduardo Cancino-Chac\\'on", "authors": "Carlos Cancino-Chac\\'on, Maarten Grachten, Kat Agres", "title": "From Bach to the Beatles: The simulation of human tonal expectation\n  using ecologically-trained predictive models", "comments": "In Proceedings of the 18th International Society of Music Information\n  Retrieval Conference (ISMIR 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tonal structure is in part conveyed by statistical regularities between\nmusical events, and research has shown that computational models reflect tonal\nstructure in music by capturing these regularities in schematic constructs like\npitch histograms. Of the few studies that model the acquisition of perceptual\nlearning from musical data, most have employed self-organizing models that\nlearn a topology of static descriptions of musical contexts. Also, the stimuli\nused to train these models are often symbolic rather than acoustically faithful\nrepresentations of musical material. In this work we investigate whether\nsequential predictive models of musical memory (specifically, recurrent neural\nnetworks), trained on audio from commercial CD recordings, induce tonal\nknowledge in a similar manner to listeners (as shown in behavioral studies in\nmusic perception). Our experiments indicate that various types of recurrent\nneural networks produce musical expectations that clearly convey tonal\nstructure. Furthermore, the results imply that although implicit knowledge of\ntonal structure is a necessary condition for accurate musical expectation, the\nmost accurate predictive models also use other cues beyond the tonal structure\nof the musical context.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 16:37:23 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Cancino-Chac\u00f3n", "Carlos", ""], ["Grachten", "Maarten", ""], ["Agres", "Kat", ""]]}, {"id": "1707.06260", "submitter": "Timothy O'Shea", "authors": "Timothy J. O'Shea, Kiran Karra, T. Charles Clancy", "title": "Learning Approximate Neural Estimators for Wireless Channel State\n  Information", "comments": "Under conference submission as of June 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation is a critical component of synchronization in wireless and signal\nprocessing systems. There is a rich body of work on estimator derivation,\noptimization, and statistical characterization from analytic system models\nwhich are used pervasively today. We explore an alternative approach to\nbuilding estimators which relies principally on approximate regression using\nlarge datasets and large computationally efficient artificial neural network\nmodels capable of learning non-linear function mappings which provide compact\nand accurate estimates. For single carrier PSK modulation, we explore the\naccuracy and computational complexity of such estimators compared with the\ncurrent gold-standard analytically derived alternatives. We compare performance\nin various wireless operating conditions and consider the trade offs between\nthe two different classes of systems. Our results show the learned estimators\ncan provide improvements in areas such as short-time estimation and estimation\nunder non-trivial real world channel conditions such as fading or other\nnon-linear hardware or propagation effects.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 18:49:41 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["O'Shea", "Timothy J.", ""], ["Karra", "Kiran", ""], ["Clancy", "T. Charles", ""]]}, {"id": "1707.06261", "submitter": "Heinrich Jiang", "authors": "Heinrich Jiang", "title": "Non-Asymptotic Uniform Rates of Consistency for k-NN Regression", "comments": "In Proceedings of 33rd AAAI Conference on Artificial Intelligence\n  (AAAI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive high-probability finite-sample uniform rates of consistency for\n$k$-NN regression that are optimal up to logarithmic factors under mild\nassumptions. We moreover show that $k$-NN regression adapts to an unknown lower\nintrinsic dimension automatically. We then apply the $k$-NN regression rates to\nestablish new results about estimating the level sets and global maxima of a\nfunction from noisy observations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 18:56:04 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 00:19:36 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Jiang", "Heinrich", ""]]}, {"id": "1707.06263", "submitter": "Enzo Ferrante", "authors": "Enzo Ferrante and Puneet K Dokania and Rafael Marini and Nikos\n  Paragios", "title": "Deformable Registration through Learning of Context-Specific Metric\n  Aggregation", "comments": "Accepted for publication in the 8th International Workshop on Machine\n  Learning in Medical Imaging (MLMI 2017), in conjunction with MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel weakly supervised discriminative algorithm for learning\ncontext specific registration metrics as a linear combination of conventional\nsimilarity measures. Conventional metrics have been extensively used over the\npast two decades and therefore both their strengths and limitations are known.\nThe challenge is to find the optimal relative weighting (or parameters) of\ndifferent metrics forming the similarity measure of the registration algorithm.\nHand-tuning these parameters would result in sub optimal solutions and quickly\nbecome infeasible as the number of metrics increases. Furthermore, such\nhand-crafted combination can only happen at global scale (entire volume) and\ntherefore will not be able to account for the different tissue properties. We\npropose a learning algorithm for estimating these parameters locally,\nconditioned to the data semantic classes. The objective function of our\nformulation is a special case of non-convex function, difference of convex\nfunction, which we optimize using the concave convex procedure. As a proof of\nconcept, we show the impact of our approach on three challenging datasets for\ndifferent anatomical structures and modalities.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 19:06:38 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Ferrante", "Enzo", ""], ["Dokania", "Puneet K", ""], ["Marini", "Rafael", ""], ["Paragios", "Nikos", ""]]}, {"id": "1707.06265", "submitter": "Wei-Ning Hsu", "authors": "Wei-Ning Hsu, Yu Zhang, James Glass", "title": "Unsupervised Domain Adaptation for Robust Speech Recognition via\n  Variational Autoencoder-Based Data Augmentation", "comments": "Accepted to IEEE ASRU 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain mismatch between training and testing can lead to significant\ndegradation in performance in many machine learning scenarios. Unfortunately,\nthis is not a rare situation for automatic speech recognition deployments in\nreal-world applications. Research on robust speech recognition can be regarded\nas trying to overcome this domain mismatch issue. In this paper, we address the\nunsupervised domain adaptation problem for robust speech recognition, where\nboth source and target domain speech are presented, but word transcripts are\nonly available for the source domain speech. We present novel\naugmentation-based methods that transform speech in a way that does not change\nthe transcripts. Specifically, we first train a variational autoencoder on both\nsource and target domain data (without supervision) to learn a latent\nrepresentation of speech. We then transform nuisance attributes of speech that\nare irrelevant to recognition by modifying the latent representations, in order\nto augment labeled training data with additional data whose distribution is\nmore similar to the target domain. The proposed method is evaluated on the\nCHiME-4 dataset and reduces the absolute word error rate (WER) by as much as\n35% compared to the non-adapted baseline.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 19:10:44 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 16:31:05 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Hsu", "Wei-Ning", ""], ["Zhang", "Yu", ""], ["Glass", "James", ""]]}, {"id": "1707.06347", "submitter": "John Schulman", "authors": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg\n  Klimov", "title": "Proximal Policy Optimization Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new family of policy gradient methods for reinforcement\nlearning, which alternate between sampling data through interaction with the\nenvironment, and optimizing a \"surrogate\" objective function using stochastic\ngradient ascent. Whereas standard policy gradient methods perform one gradient\nupdate per data sample, we propose a novel objective function that enables\nmultiple epochs of minibatch updates. The new methods, which we call proximal\npolicy optimization (PPO), have some of the benefits of trust region policy\noptimization (TRPO), but they are much simpler to implement, more general, and\nhave better sample complexity (empirically). Our experiments test PPO on a\ncollection of benchmark tasks, including simulated robotic locomotion and Atari\ngame playing, and we show that PPO outperforms other online policy gradient\nmethods, and overall strikes a favorable balance between sample complexity,\nsimplicity, and wall-time.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 02:32:33 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 09:20:06 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Schulman", "John", ""], ["Wolski", "Filip", ""], ["Dhariwal", "Prafulla", ""], ["Radford", "Alec", ""], ["Klimov", "Oleg", ""]]}, {"id": "1707.06354", "submitter": "Jaime Fisac", "authors": "Jaime F. Fisac, Monica A. Gates, Jessica B. Hamrick, Chang Liu, Dylan\n  Hadfield-Menell, Malayandi Palaniappan, Dhruv Malik, S. Shankar Sastry,\n  Thomas L. Griffiths, and Anca D. Dragan", "title": "Pragmatic-Pedagogic Value Alignment", "comments": "Published at the International Symposium on Robotics Research (ISRR\n  2017)", "journal-ref": "International Symposium on Robotics Research, 2017", "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As intelligent systems gain autonomy and capability, it becomes vital to\nensure that their objectives match those of their human users; this is known as\nthe value-alignment problem. In robotics, value alignment is key to the design\nof collaborative robots that can integrate into human workflows, successfully\ninferring and adapting to their users' objectives as they go. We argue that a\nmeaningful solution to value alignment must combine multi-agent decision theory\nwith rich mathematical models of human cognition, enabling robots to tap into\npeople's natural collaborative capabilities. We present a solution to the\ncooperative inverse reinforcement learning (CIRL) dynamic game based on\nwell-established cognitive models of decision making and theory of mind. The\nsolution captures a key reciprocity relation: the human will not plan her\nactions in isolation, but rather reason pedagogically about how the robot might\nlearn from them; the robot, in turn, can anticipate this and interpret the\nhuman's actions pragmatically. To our knowledge, this work constitutes the\nfirst formal analysis of value alignment grounded in empirically validated\ncognitive models.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 03:07:19 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 20:44:09 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Fisac", "Jaime F.", ""], ["Gates", "Monica A.", ""], ["Hamrick", "Jessica B.", ""], ["Liu", "Chang", ""], ["Hadfield-Menell", "Dylan", ""], ["Palaniappan", "Malayandi", ""], ["Malik", "Dhruv", ""], ["Sastry", "S. Shankar", ""], ["Griffiths", "Thomas L.", ""], ["Dragan", "Anca D.", ""]]}, {"id": "1707.06422", "submitter": "Joris Mooij", "authors": "Sara Magliacane, Thijs van Ommen, Tom Claassen, Stephan Bongers,\n  Philip Versteeg, Joris M. Mooij", "title": "Domain Adaptation by Using Causal Inference to Predict Invariant\n  Conditional Distributions", "comments": "Camera-ready version, to be published in the proceedings of Neural\n  Information Processing Systems 2018 (NIPS*2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal common to domain adaptation and causal inference is to make\naccurate predictions when the distributions for the source (or training)\ndomain(s) and target (or test) domain(s) differ. In many cases, these different\ndistributions can be modeled as different contexts of a single underlying\nsystem, in which each distribution corresponds to a different perturbation of\nthe system, or in causal terms, an intervention. We focus on a class of such\ncausal domain adaptation problems, where data for one or more source domains\nare given, and the task is to predict the distribution of a certain target\nvariable from measurements of other variables in one or more target domains. We\npropose an approach for solving these problems that exploits causal inference\nand does not rely on prior knowledge of the causal graph, the type of\ninterventions or the intervention targets. We demonstrate our approach by\nevaluating a possible implementation on simulated and real world data.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 09:23:31 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 15:37:54 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 11:00:40 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Magliacane", "Sara", ""], ["van Ommen", "Thijs", ""], ["Claassen", "Tom", ""], ["Bongers", "Stephan", ""], ["Versteeg", "Philip", ""], ["Mooij", "Joris M.", ""]]}, {"id": "1707.06468", "submitter": "Fabian Pedregosa", "authors": "Fabian Pedregosa, R\\'emi Leblond, Simon Lacoste-Julien", "title": "Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite\n  Optimization", "comments": "Appears in Advances in Neural Information Processing Systems 30 (NIPS\n  2017), 28 pages", "journal-ref": "Advances in Neural Information Processing Systems 30 (NIPS 2017)", "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to their simplicity and excellent performance, parallel asynchronous\nvariants of stochastic gradient descent have become popular methods to solve a\nwide range of large-scale optimization problems on multi-core architectures.\nYet, despite their practical success, support for nonsmooth objectives is still\nlacking, making them unsuitable for many problems of interest in machine\nlearning, such as the Lasso, group Lasso or empirical risk minimization with\nconvex constraints.\n  In this work, we propose and analyze ProxASAGA, a fully asynchronous sparse\nmethod inspired by SAGA, a variance reduced incremental gradient algorithm. The\nproposed method is easy to implement and significantly outperforms the state of\nthe art on several nonsmooth, large-scale problems. We prove that our method\nachieves a theoretical linear speedup with respect to the sequential version\nunder assumptions on the sparsity of gradients and block-separability of the\nproximal term. Empirical benchmarks on a multi-core architecture illustrate\npractical speedups of up to 12x on a 20-core machine.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 12:14:31 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 05:24:08 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 16:49:49 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Pedregosa", "Fabian", ""], ["Leblond", "R\u00e9mi", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1707.06484", "submitter": "Fisher Yu", "authors": "Fisher Yu, Dequan Wang, Evan Shelhamer, Trevor Darrell", "title": "Deep Layer Aggregation", "comments": "Published at the Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual recognition requires rich representations that span levels from low to\nhigh, scales from small to large, and resolutions from fine to coarse. Even\nwith the depth of features in a convolutional network, a layer in isolation is\nnot enough: compounding and aggregating these representations improves\ninference of what and where. Architectural efforts are exploring many\ndimensions for network backbones, designing deeper or wider architectures, but\nhow to best aggregate layers and blocks across a network deserves further\nattention. Although skip connections have been incorporated to combine layers,\nthese connections have been \"shallow\" themselves, and only fuse by simple,\none-step operations. We augment standard architectures with deeper aggregation\nto better fuse information across layers. Our deep layer aggregation structures\niteratively and hierarchically merge the feature hierarchy to make networks\nwith better accuracy and fewer parameters. Experiments across architectures and\ntasks show that deep layer aggregation improves recognition and resolution\ncompared to existing branching and merging schemes. The code is at\nhttps://github.com/ucbdrive/dla.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 12:59:08 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 05:45:30 GMT"}, {"version": "v3", "created": "Fri, 4 Jan 2019 09:26:55 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Yu", "Fisher", ""], ["Wang", "Dequan", ""], ["Shelhamer", "Evan", ""], ["Darrell", "Trevor", ""]]}, {"id": "1707.06487", "submitter": "Yunfei Ye", "authors": "Yunfei Ye", "title": "A Nonlinear Kernel Support Matrix Machine for Matrix Learning", "comments": null, "journal-ref": null, "doi": "10.1007/s13042-018-0896-4", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many problems of supervised tensor learning (STL), real world data such as\nface images or MRI scans are naturally represented as matrices, which are also\ncalled as second order tensors. Most existing classifiers based on tensor\nrepresentation, such as support tensor machine (STM) need to solve iteratively\nwhich occupy much time and may suffer from local minima. In this paper, we\npresent a kernel support matrix machine (KSMM) to perform supervised learning\nwhen data are represented as matrices. KSMM is a general framework for the\nconstruction of matrix-based hyperplane to exploit structural information. We\nanalyze a unifying optimization problem for which we propose an asymptotically\nconvergent algorithm. Theoretical analysis for the generalization bounds is\nderived based on Rademacher complexity with respect to a probability\ndistribution. We demonstrate the merits of the proposed method by exhaustive\nexperiments on both simulation study and a number of real-word datasets from a\nvariety of application domains.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 13:00:04 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 13:12:27 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Ye", "Yunfei", ""]]}, {"id": "1707.06519", "submitter": "Chia-Hao Shen", "authors": "Chia-Hao Shen, Janet Y. Sung, Hung-Yi Lee", "title": "Language Transfer of Audio Word2Vec: Learning Audio Segment\n  Representations without Target Language Data", "comments": "arXiv admin note: text overlap with arXiv:1603.00982", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio Word2Vec offers vector representations of fixed dimensionality for\nvariable-length audio segments using Sequence-to-sequence Autoencoder (SA).\nThese vector representations are shown to describe the sequential phonetic\nstructures of the audio segments to a good degree, with real world applications\nsuch as query-by-example Spoken Term Detection (STD). This paper examines the\ncapability of language transfer of Audio Word2Vec. We train SA from one\nlanguage (source language) and use it to extract the vector representation of\nthe audio segments of another language (target language). We found that SA can\nstill catch phonetic structure from the audio segments of the target language\nif the source and target languages are similar. In query-by-example STD, we\nobtain the vector representations from the SA learned from a large amount of\nsource language data, and found them surpass the representations from naive\nencoder and SA directly learned from a small amount of target language data.\nThe result shows that it is possible to learn Audio Word2Vec model from\nhigh-resource languages and use it on low-resource languages. This further\nexpands the usability of Audio Word2Vec.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 10:54:00 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Shen", "Chia-Hao", ""], ["Sung", "Janet Y.", ""], ["Lee", "Hung-Yi", ""]]}, {"id": "1707.06527", "submitter": "Yanmin Qian", "authors": "Yanmin Qian, Xuankai Chang and Dong Yu", "title": "Single-Channel Multi-talker Speech Recognition with Permutation\n  Invariant Training", "comments": "11 pages, 6 figures, Submitted to IEEE/ACM Transactions on Audio,\n  Speech and Language Processing. arXiv admin note: text overlap with\n  arXiv:1704.01985", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although great progresses have been made in automatic speech recognition\n(ASR), significant performance degradation is still observed when recognizing\nmulti-talker mixed speech. In this paper, we propose and evaluate several\narchitectures to address this problem under the assumption that only a single\nchannel of mixed signal is available. Our technique extends permutation\ninvariant training (PIT) by introducing the front-end feature separation module\nwith the minimum mean square error (MSE) criterion and the back-end recognition\nmodule with the minimum cross entropy (CE) criterion. More specifically, during\ntraining we compute the average MSE or CE over the whole utterance for each\npossible utterance-level output-target assignment, pick the one with the\nminimum MSE or CE, and optimize for that assignment. This strategy elegantly\nsolves the label permutation problem observed in the deep learning based\nmulti-talker mixed speech separation and recognition systems. The proposed\narchitectures are evaluated and compared on an artificially mixed AMI dataset\nwith both two- and three-talker mixed speech. The experimental results indicate\nthat our proposed architectures can cut the word error rate (WER) by 45.0% and\n25.0% relatively against the state-of-the-art single-talker speech recognition\nsystem across all speakers when their energies are comparable, for two- and\nthree-talker mixed speech, respectively. To our knowledge, this is the first\nwork on the multi-talker mixed speech recognition on the challenging\nspeaker-independent spontaneous large vocabulary continuous speech task.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 03:48:54 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Qian", "Yanmin", ""], ["Chang", "Xuankai", ""], ["Yu", "Dong", ""]]}, {"id": "1707.06541", "submitter": "Jian Wu", "authors": "Jian Wu and Peter I. Frazier", "title": "Discretization-free Knowledge Gradient Methods for Bayesian Optimization", "comments": "This paper, which combines and extends two conference papers\n  (arXiv:1703.04389, arXiv:1606.04414), has been withdrawn by the authors\n  because it was submitted prematurely before proper attribution could be\n  provided", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies Bayesian ranking and selection (R&S) problems with\ncorrelated prior beliefs and continuous domains, i.e. Bayesian optimization\n(BO). Knowledge gradient methods [Frazier et al., 2008, 2009] have been widely\nstudied for discrete R&S problems, which sample the one-step Bayes-optimal\npoint. When used over continuous domains, previous work on the knowledge\ngradient [Scott et al., 2011, Wu and Frazier, 2016, Wu et al., 2017] often rely\non a discretized finite approximation. However, the discretization introduces\nerror and scales poorly as the dimension of domain grows. In this paper, we\ndevelop a fast discretization-free knowledge gradient method for Bayesian\noptimization. Our method is not restricted to the fully sequential setting, but\nuseful in all settings where knowledge gradient can be used over continuous\ndomains. We show how our method can be generalized to handle (i) batch of\npoints suggestion (parallel knowledge gradient); (ii) the setting where\nderivative information is available in the optimization process\n(derivative-enabled knowledge gradient). In numerical experiments, we\ndemonstrate that the discretization-free knowledge gradient method finds global\noptima significantly faster than previous Bayesian optimization algorithms on\nboth synthetic test functions and real-world applications, especially when\nfunction evaluations are noisy; and derivative-enabled knowledge gradient can\nfurther improve the performances, even outperforming the gradient-based\noptimizer such as BFGS when derivative information is available.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 14:28:05 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 18:31:35 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Wu", "Jian", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1707.06556", "submitter": "Aurelie Herbelot", "authors": "Aurelie Herbelot and Marco Baroni", "title": "High-risk learning: acquiring new word vectors from tiny data", "comments": "Accepted as short paper at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributional semantics models are known to struggle with small data. It is\ngenerally accepted that in order to learn 'a good vector' for a word, a model\nmust have sufficient examples of its usage. This contradicts the fact that\nhumans can guess the meaning of a word from a few occurrences only. In this\npaper, we show that a neural language model such as Word2Vec only necessitates\nminor modifications to its standard architecture to learn new terms from tiny\ndata, using background knowledge from a previously learnt semantic space. We\ntest our model on word definitions and on a nonce task involving 2-6 sentences'\nworth of context, showing a large increase in performance over state-of-the-art\nmodels on the definitional task.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 15:02:14 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Herbelot", "Aurelie", ""], ["Baroni", "Marco", ""]]}, {"id": "1707.06588", "submitter": "Yaniv Taigman", "authors": "Yaniv Taigman, Lior Wolf, Adam Polyak, Eliya Nachmani", "title": "VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new neural text to speech (TTS) method that is able to transform\ntext to speech in voices that are sampled in the wild. Unlike other systems,\nour solution is able to deal with unconstrained voice samples and without\nrequiring aligned phonemes or linguistic features. The network architecture is\nsimpler than those in the existing literature and is based on a novel shifting\nbuffer working memory. The same buffer is used for estimating the attention,\ncomputing the output audio, and for updating the buffer itself. The input\nsentence is encoded using a context-free lookup table that contains one entry\nper character or phoneme. The speakers are similarly represented by a short\nvector that can also be fitted to new identities, even with only a few samples.\nVariability in the generated speech is achieved by priming the buffer prior to\ngenerating the audio. Experimental results on several datasets demonstrate\nconvincing capabilities, making TTS accessible to a wider range of\napplications. In order to promote reproducibility, we release our source code\nand models.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 16:18:00 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 15:29:44 GMT"}, {"version": "v3", "created": "Thu, 1 Feb 2018 14:48:11 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Taigman", "Yaniv", ""], ["Wolf", "Lior", ""], ["Polyak", "Adam", ""], ["Nachmani", "Eliya", ""]]}, {"id": "1707.06613", "submitter": "Nicole Immorlica", "authors": "Cynthia Dwork, Nicole Immorlica, Adam Tauman Kalai, Max Leiserson", "title": "Decoupled classifiers for fair and efficient machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When it is ethical and legal to use a sensitive attribute (such as gender or\nrace) in machine learning systems, the question remains how to do so. We show\nthat the naive application of machine learning algorithms using sensitive\nfeatures leads to an inherent tradeoff in accuracy between groups. We provide a\nsimple and efficient decoupling technique, that can be added on top of any\nblack-box machine learning algorithm, to learn different classifiers for\ndifferent groups. Transfer learning is used to mitigate the problem of having\ntoo little data on any one group.\n  The method can apply to a range of fairness criteria. In particular, we\nrequire the application designer to specify as joint loss function that makes\nexplicit the trade-off between fairness and accuracy. Our reduction is shown to\nefficiently find the minimum loss as long as the objective has a certain\nnatural monotonicity property which may be of independent interest in the study\nof fairness in algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 17:08:48 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Dwork", "Cynthia", ""], ["Immorlica", "Nicole", ""], ["Kalai", "Adam Tauman", ""], ["Leiserson", "Max", ""]]}, {"id": "1707.06618", "submitter": "Quanquan Gu", "authors": "Pan Xu and Jinghui Chen and Difan Zou and Quanquan Gu", "title": "Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex\n  Optimization", "comments": "29 pages, 1 figure, 1 table. In NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework to analyze the global convergence of Langevin\ndynamics based algorithms for nonconvex finite-sum optimization with $n$\ncomponent functions. At the core of our analysis is a direct analysis of the\nergodicity of the numerical approximations to Langevin dynamics, which leads to\nfaster convergence rates. Specifically, we show that gradient Langevin dynamics\n(GLD) and stochastic gradient Langevin dynamics (SGLD) converge to the almost\nminimizer within $\\tilde O\\big(nd/(\\lambda\\epsilon) \\big)$ and $\\tilde\nO\\big(d^7/(\\lambda^5\\epsilon^5) \\big)$ stochastic gradient evaluations\nrespectively, where $d$ is the problem dimension, and $\\lambda$ is the spectral\ngap of the Markov chain generated by GLD. Both results improve upon the best\nknown gradient complexity results (Raginsky et al., 2017). Furthermore, for the\nfirst time we prove the global convergence guarantee for variance reduced\nstochastic gradient Langevin dynamics (SVRG-LD) to the almost minimizer within\n$\\tilde O\\big(\\sqrt{n}d^5/(\\lambda^4\\epsilon^{5/2})\\big)$ stochastic gradient\nevaluations, which outperforms the gradient complexities of GLD and SGLD in a\nwide regime. Our theoretical analyses shed some light on using Langevin\ndynamics based algorithms for nonconvex optimization with provable guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 17:18:11 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 18:08:04 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 02:57:22 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Xu", "Pan", ""], ["Chen", "Jinghui", ""], ["Zou", "Difan", ""], ["Gu", "Quanquan", ""]]}, {"id": "1707.06633", "submitter": "Martin V\\\"olker", "authors": "Felix Burget, Lukas Dominique Josef Fiederer, Daniel Kuhner, Martin\n  V\\\"olker, Johannes Aldinger, Robin Tibor Schirrmeister, Chau Do, Joschka\n  Boedecker, Bernhard Nebel, Tonio Ball, Wolfram Burgard", "title": "Acting Thoughts: Towards a Mobile Robotic Service Assistant for Users\n  with Limited Communication Skills", "comments": "* FB, LDJF, DK, MV and JA contributed equally to the work. Accepted\n  as a conference paper at the European Conference on Mobile Robotics 2017\n  (ECMR 2017), 6 pages, 3 figures", "journal-ref": "2017 European Conference on Mobile Robots (ECMR)", "doi": "10.1109/ECMR.2017.8098658", "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As autonomous service robots become more affordable and thus available also\nfor the general public, there is a growing need for user friendly interfaces to\ncontrol the robotic system. Currently available control modalities typically\nexpect users to be able to express their desire through either touch, speech or\ngesture commands. While this requirement is fulfilled for the majority of\nusers, paralyzed users may not be able to use such systems. In this paper, we\npresent a novel framework, that allows these users to interact with a robotic\nservice assistant in a closed-loop fashion, using only thoughts. The\nbrain-computer interface (BCI) system is composed of several interacting\ncomponents, i.e., non-invasive neuronal signal recording and decoding,\nhigh-level task planning, motion and manipulation planning as well as\nenvironment perception. In various experiments, we demonstrate its\napplicability and robustness in real world scenarios, considering\nfetch-and-carry tasks and tasks involving human-robot interaction. As our\nresults demonstrate, our system is capable of adapting to frequent changes in\nthe environment and reliably completing given tasks within a reasonable amount\nof time. Combined with high-level planning and autonomous robotic systems,\ninteresting new perspectives open up for non-invasive BCI-based human-robot\ninteractions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 17:51:12 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 06:30:43 GMT"}, {"version": "v3", "created": "Thu, 30 Nov 2017 08:25:20 GMT"}, {"version": "v4", "created": "Tue, 12 Jun 2018 14:52:41 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Burget", "Felix", ""], ["Fiederer", "Lukas Dominique Josef", ""], ["Kuhner", "Daniel", ""], ["V\u00f6lker", "Martin", ""], ["Aldinger", "Johannes", ""], ["Schirrmeister", "Robin Tibor", ""], ["Do", "Chau", ""], ["Boedecker", "Joschka", ""], ["Nebel", "Bernhard", ""], ["Ball", "Tonio", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1707.06658", "submitter": "Anirban Santara", "authors": "Anirban Santara, Abhishek Naik, Balaraman Ravindran, Dipankar Das,\n  Dheevatsa Mudigere, Sasikanth Avancha, Bharat Kaul", "title": "RAIL: Risk-Averse Imitation Learning", "comments": "Accepted for presentation in Deep Reinforcement Learning Symposium at\n  NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning algorithms learn viable policies by imitating an expert's\nbehavior when reward signals are not available. Generative Adversarial\nImitation Learning (GAIL) is a state-of-the-art algorithm for learning policies\nwhen the expert's behavior is available as a fixed set of trajectories. We\nevaluate in terms of the expert's cost function and observe that the\ndistribution of trajectory-costs is often more heavy-tailed for GAIL-agents\nthan the expert at a number of benchmark continuous-control tasks. Thus,\nhigh-cost trajectories, corresponding to tail-end events of catastrophic\nfailure, are more likely to be encountered by the GAIL-agents than the expert.\nThis makes the reliability of GAIL-agents questionable when it comes to\ndeployment in risk-sensitive applications like robotic surgery and autonomous\ndriving. In this work, we aim to minimize the occurrence of tail-end events by\nminimizing tail risk within the GAIL framework. We quantify tail risk by the\nConditional-Value-at-Risk (CVaR) of trajectories and develop the Risk-Averse\nImitation Learning (RAIL) algorithm. We observe that the policies learned with\nRAIL show lower tail-end risk than those of vanilla GAIL. Thus the proposed\nRAIL algorithm appears as a potent alternative to GAIL for improved reliability\nin risk-sensitive applications.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 18:01:45 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 05:40:58 GMT"}, {"version": "v3", "created": "Wed, 4 Oct 2017 09:34:42 GMT"}, {"version": "v4", "created": "Wed, 29 Nov 2017 12:44:19 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Santara", "Anirban", ""], ["Naik", "Abhishek", ""], ["Ravindran", "Balaraman", ""], ["Das", "Dipankar", ""], ["Mudigere", "Dheevatsa", ""], ["Avancha", "Sasikanth", ""], ["Kaul", "Bharat", ""]]}, {"id": "1707.06682", "submitter": "Regina Meszl\\'enyi", "authors": "Regina Meszl\\'enyi, Krisztian Buza and Zolt\\'an Vidny\\'anszky", "title": "Resting state fMRI functional connectivity-based classification using a\n  convolutional neural network architecture", "comments": "25 pages, 4 figures, 1 table, plus supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques have become increasingly popular in the field of\nresting state fMRI (functional magnetic resonance imaging) network based\nclassification. However, the application of convolutional networks has been\nproposed only very recently and has remained largely unexplored. In this paper\nwe describe a convolutional neural network architecture for functional\nconnectome classification called connectome-convolutional neural network\n(CCNN). Our results on simulated datasets and a publicly available dataset for\namnestic mild cognitive impairment classification demonstrate that our CCNN\nmodel can efficiently distinguish between subject groups. We also show that the\nconnectome-convolutional network is capable to combine information from diverse\nfunctional connectivity metrics and that models using a combination of\ndifferent connectivity descriptors are able to outperform classifiers using\nonly one metric. From this flexibility follows that our proposed CCNN model can\nbe easily adapted to a wide range of connectome based classification or\nregression tasks, by varying which connectivity descriptor combinations are\nused to train the network.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 19:12:58 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Meszl\u00e9nyi", "Regina", ""], ["Buza", "Krisztian", ""], ["Vidny\u00e1nszky", "Zolt\u00e1n", ""]]}, {"id": "1707.06728", "submitter": "Maria-Irina Nicolae", "authors": "Valentina Zantedeschi, Maria-Irina Nicolae and Ambrish Rawat", "title": "Efficient Defenses Against Adversarial Attacks", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the recent adoption of deep neural networks (DNN) accross a wide\nrange of applications, adversarial attacks against these models have proven to\nbe an indisputable threat. Adversarial samples are crafted with a deliberate\nintention of undermining a system. In the case of DNNs, the lack of better\nunderstanding of their working has prevented the development of efficient\ndefenses. In this paper, we propose a new defense method based on practical\nobservations which is easy to integrate into models and performs better than\nstate-of-the-art defenses. Our proposed solution is meant to reinforce the\nstructure of a DNN, making its prediction more stable and less likely to be\nfooled by adversarial samples. We conduct an extensive experimental study\nproving the efficiency of our method against multiple attacks, comparing it to\nnumerous defenses, both in white-box and black-box setups. Additionally, the\nimplementation of our method brings almost no overhead to the training\nprocedure, while maintaining the prediction performance of the original model\non clean samples.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 00:50:22 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 15:47:50 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Zantedeschi", "Valentina", ""], ["Nicolae", "Maria-Irina", ""], ["Rawat", "Ambrish", ""]]}, {"id": "1707.06742", "submitter": "Patrice Simard", "authors": "Patrice Y. Simard, Saleema Amershi, David M. Chickering, Alicia\n  Edelman Pelton, Soroush Ghorashi, Christopher Meek, Gonzalo Ramos, Jina Suh,\n  Johan Verwey, Mo Wang, and John Wernsing", "title": "Machine Teaching: A New Paradigm for Building Machine Learning Systems", "comments": "Also available at: http://aka.ms/machineteachingpaper", "journal-ref": null, "doi": null, "report-no": "MSR-TR-2017-26", "categories": "cs.LG cs.AI cs.HC cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current processes for building machine learning systems require\npractitioners with deep knowledge of machine learning. This significantly\nlimits the number of machine learning systems that can be created and has led\nto a mismatch between the demand for machine learning systems and the ability\nfor organizations to build them. We believe that in order to meet this growing\ndemand for machine learning systems we must significantly increase the number\nof individuals that can teach machines. We postulate that we can achieve this\ngoal by making the process of teaching machines easy, fast and above all,\nuniversally accessible.\n  While machine learning focuses on creating new algorithms and improving the\naccuracy of \"learners\", the machine teaching discipline focuses on the efficacy\nof the \"teachers\". Machine teaching as a discipline is a paradigm shift that\nfollows and extends principles of software engineering and programming\nlanguages. We put a strong emphasis on the teacher and the teacher's\ninteraction with data, as well as crucial components such as techniques and\ndesign principles of interaction and visualization.\n  In this paper, we present our position regarding the discipline of machine\nteaching and articulate fundamental machine teaching principles. We also\ndescribe how, by decoupling knowledge about machine learning algorithms from\nthe process of teaching, we can accelerate innovation and empower millions of\nnew uses for machine learning models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 02:37:04 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 05:45:05 GMT"}, {"version": "v3", "created": "Fri, 11 Aug 2017 00:16:49 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Simard", "Patrice Y.", ""], ["Amershi", "Saleema", ""], ["Chickering", "David M.", ""], ["Pelton", "Alicia Edelman", ""], ["Ghorashi", "Soroush", ""], ["Meek", "Christopher", ""], ["Ramos", "Gonzalo", ""], ["Suh", "Jina", ""], ["Verwey", "Johan", ""], ["Wang", "Mo", ""], ["Wernsing", "John", ""]]}, {"id": "1707.06756", "submitter": "Clayton Morrison", "authors": "Colin Reimer Dawson, Chaofan Huang, Clayton T. Morrison", "title": "An Infinite Hidden Markov Model With Similarity-Biased Transitions", "comments": "16 pages, 4 figures, accepted to ICML 2017, includes supplemental\n  appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a generalization of the Hierarchical Dirichlet Process Hidden\nMarkov Model (HDP-HMM) which is able to encode prior information that state\ntransitions are more likely between \"nearby\" states. This is accomplished by\ndefining a similarity function on the state space and scaling transition\nprobabilities by pair-wise similarities, thereby inducing correlations among\nthe transition distributions. We present an augmented data representation of\nthe model as a Markov Jump Process in which: (1) some jump attempts fail, and\n(2) the probability of success is proportional to the similarity between the\nsource and destination states. This augmentation restores conditional conjugacy\nand admits a simple Gibbs sampler. We evaluate the model and inference method\non a speaker diarization task and a \"harmonic parsing\" task using four-part\nchorale data, as well as on several synthetic datasets, achieving favorable\ncomparisons to existing models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 04:39:10 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Dawson", "Colin Reimer", ""], ["Huang", "Chaofan", ""], ["Morrison", "Clayton T.", ""]]}, {"id": "1707.06757", "submitter": "Kelum Gajamannage", "authors": "Kelum Gajamannage, Randy Paffenroth, Erik M. Bollt", "title": "A Nonlinear Dimensionality Reduction Framework Using Smooth Geodesics", "comments": "13 pages, 7 figures, submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing dimensionality reduction methods are adept at revealing hidden\nunderlying manifolds arising from high-dimensional data and thereby producing a\nlow-dimensional representation. However, the smoothness of the manifolds\nproduced by classic techniques over sparse and noisy data is not guaranteed. In\nfact, the embedding generated using such data may distort the geometry of the\nmanifold and thereby produce an unfaithful embedding. Herein, we propose a\nframework for nonlinear dimensionality reduction that generates a manifold in\nterms of smooth geodesics that is designed to treat problems in which manifold\nmeasurements are either sparse or corrupted by noise. Our method generates a\nnetwork structure for given high-dimensional data using a nearest neighbors\nsearch and then produces piecewise linear shortest paths that are defined as\ngeodesics. Then, we fit points in each geodesic by a smoothing spline to\nemphasize the smoothness. The robustness of this approach for sparse and noisy\ndatasets is demonstrated by the implementation of the method on synthetic and\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 05:04:07 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 17:38:33 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Gajamannage", "Kelum", ""], ["Paffenroth", "Randy", ""], ["Bollt", "Erik M.", ""]]}, {"id": "1707.06841", "submitter": "Youmna Farag", "authors": "Youmna Farag, Marek Rei, Ted Briscoe", "title": "An Error-Oriented Approach to Word Embedding Pre-Training", "comments": "10 pages, 2 figures, 4 tables, BEA 2017", "journal-ref": "The 12th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA 2017)", "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel word embedding pre-training approach that exploits writing\nerrors in learners' scripts. We compare our method to previous models that tune\nthe embeddings based on script scores and the discrimination between correct\nand corrupt word contexts in addition to the generic commonly-used embeddings\npre-trained on large corpora. The comparison is achieved by using the\naforementioned models to bootstrap a neural network that learns to predict a\nholistic score for scripts. Furthermore, we investigate augmenting our model\nwith error corrections and monitor the impact on performance. Our results show\nthat our error-oriented approach outperforms other comparable ones which is\nfurther demonstrated when training on more data. Additionally, extending the\nmodel with corrections provides further performance gains when data sparsity is\nan issue.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 11:06:12 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Farag", "Youmna", ""], ["Rei", "Marek", ""], ["Briscoe", "Ted", ""]]}, {"id": "1707.06887", "submitter": "Marc G. Bellemare", "authors": "Marc G. Bellemare, Will Dabney, R\\'emi Munos", "title": "A Distributional Perspective on Reinforcement Learning", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we argue for the fundamental importance of the value\ndistribution: the distribution of the random return received by a reinforcement\nlearning agent. This is in contrast to the common approach to reinforcement\nlearning which models the expectation of this return, or value. Although there\nis an established body of literature studying the value distribution, thus far\nit has always been used for a specific purpose such as implementing risk-aware\nbehaviour. We begin with theoretical results in both the policy evaluation and\ncontrol settings, exposing a significant distributional instability in the\nlatter. We then use the distributional perspective to design a new algorithm\nwhich applies Bellman's equation to the learning of approximate value\ndistributions. We evaluate our algorithm using the suite of games from the\nArcade Learning Environment. We obtain both state-of-the-art results and\nanecdotal evidence demonstrating the importance of the value distribution in\napproximate reinforcement learning. Finally, we combine theoretical and\nempirical evidence to highlight the ways in which the value distribution\nimpacts learning in the approximate setting.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 13:21:54 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Bellemare", "Marc G.", ""], ["Dabney", "Will", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1707.06903", "submitter": "Chu Wang", "authors": "Chu Wang, Iraj Saniee, William S. Kennedy, Chris A. White", "title": "A New Family of Near-metrics for Universal Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a family of near-metrics based on local graph diffusion to capture\nsimilarity for a wide class of data sets. These quasi-metametrics, as their\nnames suggest, dispense with one or two standard axioms of metric spaces,\nspecifically distinguishability and symmetry, so that similarity between data\npoints of arbitrary type and form could be measured broadly and effectively.\nThe proposed near-metric family includes the forward k-step diffusion and its\nreverse, typically on the graph consisting of data objects and their features.\nBy construction, this family of near-metrics is particularly appropriate for\ncategorical data, continuous data, and vector representations of images and\ntext extracted via deep learning approaches. We conduct extensive experiments\nto evaluate the performance of this family of similarity measures and compare\nand contrast with traditional measures of similarity used for each specific\napplication and with the ground truth when available. We show that for\nstructured data including categorical and continuous data, the near-metrics\ncorresponding to normalized forward k-step diffusion (k small) work as one of\nthe best performing similarity measures; for vector representations of text and\nimages including those extracted from deep learning, the near-metrics derived\nfrom normalized and reverse k-step graph diffusion (k very small) exhibit\noutstanding ability to distinguish data points from different classes.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 14:02:46 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 02:36:52 GMT"}, {"version": "v3", "created": "Tue, 17 Oct 2017 08:34:25 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Wang", "Chu", ""], ["Saniee", "Iraj", ""], ["Kennedy", "William S.", ""], ["White", "Chris A.", ""]]}, {"id": "1707.06962", "submitter": "Seongah Jeong", "authors": "Seongah Jeong, Xiang Li, Jiarui Yang, Quanzheng Li, Vahid Tarokh", "title": "Dictionary Learning and Sparse Coding-based Denoising for\n  High-Resolution Task Functional Connectivity MRI Analysis", "comments": "8 pages, 3 figures, MLMI2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel denoising framework for task functional Magnetic Resonance\nImaging (tfMRI) data to delineate the high-resolution spatial pattern of the\nbrain functional connectivity via dictionary learning and sparse coding (DLSC).\nIn order to address the limitations of the unsupervised DLSC-based fMRI\nstudies, we utilize the prior knowledge of task paradigm in the learning step\nto train a data-driven dictionary and to model the sparse representation. We\napply the proposed DLSC-based method to Human Connectome Project (HCP) motor\ntfMRI dataset. Studies on the functional connectivity of cerebrocerebellar\ncircuits in somatomotor networks show that the DLSC-based denoising framework\ncan significantly improve the prominent connectivity patterns, in comparison to\nthe temporal non-local means (tNLM)-based denoising method as well as the case\nwithout denoising, which is consistent and neuroscientifically meaningful\nwithin motor area. The promising results show that the proposed method can\nprovide an important foundation for the high-resolution functional connectivity\nanalysis, and provide a better approach for fMRI preprocessing.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 16:20:04 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Jeong", "Seongah", ""], ["Li", "Xiang", ""], ["Yang", "Jiarui", ""], ["Li", "Quanzheng", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1707.06992", "submitter": "Hossein Hosseini", "authors": "S. Hossein Hosseini and Afshin Ebrahimi", "title": "Ideological Sublations: Resolution of Dialectic in Population-based\n  Optimization", "comments": "An antenna selection model for massive MIMO was considered at the\n  current version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A population-based optimization algorithm was designed, inspired by two main\nthinking modes in philosophy, both based on dialectic concept and\nthesis-antithesis paradigm. They impose two different kinds of dialectics.\nIdealistic and materialistic antitheses are formulated as optimization models.\nBased on the models, the population is coordinated for dialectical\ninteractions. At the population-based context, the formulated optimization\nmodels are reduced to a simple detection problem for each thinker (particle).\nAccording to the assigned thinking mode to each thinker and her/his\nmeasurements of corresponding dialectic with other candidate particles, they\ndeterministically decide to interact with a thinker in maximum dialectic with\ntheir theses. The position of a thinker at maximum dialectic is known as an\navailable antithesis among the existing solutions. The dialectical interactions\nat each ideological community are distinguished by meaningful distributions of\nstep-sizes for each thinking mode. In fact, the thinking modes are regarded as\nexploration and exploitation elements of the proposed algorithm. The result is\na delicate balance without any requirement for adjustment of step-size\ncoefficients. Main parameter of the proposed algorithm is the number of\nparticles appointed to each thinking modes, or equivalently for each kind of\nmotions. An additional integer parameter is defined to boost the stability of\nthe final algorithm in some particular problems. The proposed algorithm is\nevaluated by a testbed of 12 single-objective continuous benchmark functions.\nMoreover, its performance and speed were highlighted in sparse reconstruction\nand antenna selection problems, at the context of compressed sensing and\nmassive MIMO, respectively. The results indicate fast and efficient performance\nin comparison with well-known evolutionary algorithms and dedicated\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 17:53:04 GMT"}, {"version": "v2", "created": "Sun, 3 Sep 2017 13:33:09 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Hosseini", "S. Hossein", ""], ["Ebrahimi", "Afshin", ""]]}, {"id": "1707.06997", "submitter": "Brett Beaulieu-Jones", "authors": "Brett K. Beaulieu-Jones", "title": "Machine Learning for Structured Clinical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research is a tertiary priority in the EHR, where the priorities are patient\ncare and billing. Because of this, the data is not standardized or formatted in\na manner easily adapted to machine learning approaches. Data may be missing for\na large variety of reasons ranging from individual input styles to differences\nin clinical decision making, for example, which lab tests to issue. Few\npatients are annotated at a research quality, limiting sample size and\npresenting a moving gold standard. Patient progression over time is key to\nunderstanding many diseases but many machine learning algorithms require a\nsnapshot, at a single time point, to create a usable vector form. Furthermore,\nalgorithms that produce black box results do not provide the interpretability\nrequired for clinical adoption. This chapter discusses these challenges and\nothers in applying machine learning techniques to the structured EHR (i.e.\nPatient Demographics, Family History, Medication Information, Vital Signs,\nLaboratory Tests, Genetic Testing). It does not cover feature extraction from\nadditional sources such as imaging data or free text patient notes but the\napproaches discussed can include features extracted from these sources.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 14:39:51 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Beaulieu-Jones", "Brett K.", ""]]}, {"id": "1707.07012", "submitter": "Quoc Le", "authors": "Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le", "title": "Learning Transferable Architectures for Scalable Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing neural network image classification models often requires\nsignificant architecture engineering. In this paper, we study a method to learn\nthe model architectures directly on the dataset of interest. As this approach\nis expensive when the dataset is large, we propose to search for an\narchitectural building block on a small dataset and then transfer the block to\na larger dataset. The key contribution of this work is the design of a new\nsearch space (the \"NASNet search space\") which enables transferability. In our\nexperiments, we search for the best convolutional layer (or \"cell\") on the\nCIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking\ntogether more copies of this cell, each with their own parameters to design a\nconvolutional architecture, named \"NASNet architecture\". We also introduce a\nnew regularization technique called ScheduledDropPath that significantly\nimproves generalization in the NASNet models. On CIFAR-10 itself, NASNet\nachieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet\nachieves, among the published works, state-of-the-art accuracy of 82.7% top-1\nand 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than\nthe best human-invented architectures while having 9 billion fewer FLOPS - a\nreduction of 28% in computational demand from the previous state-of-the-art\nmodel. When evaluated at different levels of computational cost, accuracies of\nNASNets exceed those of the state-of-the-art human-designed models. For\ninstance, a small version of NASNet also achieves 74% top-1 accuracy, which is\n3.1% better than equivalently-sized, state-of-the-art models for mobile\nplatforms. Finally, the learned features by NASNet used with the Faster-RCNN\nframework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 18:10:26 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 01:37:56 GMT"}, {"version": "v3", "created": "Fri, 1 Dec 2017 07:48:01 GMT"}, {"version": "v4", "created": "Wed, 11 Apr 2018 05:12:21 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Zoph", "Barret", ""], ["Vasudevan", "Vijay", ""], ["Shlens", "Jonathon", ""], ["Le", "Quoc V.", ""]]}, {"id": "1707.07113", "submitter": "Gilles Louppe", "authors": "Gilles Louppe, Joeri Hermans, Kyle Cranmer", "title": "Adversarial Variational Optimization of Non-Differentiable Simulators", "comments": "v4: Final version published at AISTATS 2019; v5: Fixed typo in Eqn 13", "journal-ref": "PMLR 89:1438-1447, 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex computer simulators are increasingly used across fields of science as\ngenerative models tying parameters of an underlying theory to experimental\nobservations. Inference in this setup is often difficult, as simulators rarely\nadmit a tractable density or likelihood function. We introduce Adversarial\nVariational Optimization (AVO), a likelihood-free inference algorithm for\nfitting a non-differentiable generative model incorporating ideas from\ngenerative adversarial networks, variational optimization and empirical Bayes.\nWe adapt the training procedure of generative adversarial networks by replacing\nthe differentiable generative network with a domain-specific simulator. We\nsolve the resulting non-differentiable minimax problem by minimizing\nvariational upper bounds of the two adversarial objectives. Effectively, the\nprocedure results in learning a proposal distribution over simulator\nparameters, such that the JS divergence between the marginal distribution of\nthe synthetic data and the empirical distribution of observed data is\nminimized. We evaluate and compare the method with simulators producing both\ndiscrete and continuous data.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 07:05:38 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 16:37:57 GMT"}, {"version": "v3", "created": "Thu, 5 Apr 2018 12:40:38 GMT"}, {"version": "v4", "created": "Fri, 5 Oct 2018 06:18:46 GMT"}, {"version": "v5", "created": "Thu, 16 Apr 2020 07:11:49 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Louppe", "Gilles", ""], ["Hermans", "Joeri", ""], ["Cranmer", "Kyle", ""]]}, {"id": "1707.07196", "submitter": "Panagiotis Traganitis", "authors": "Panagiotis A. Traganitis and Georgios B. Giannakis", "title": "Sketched Subspace Clustering", "comments": "P. A. Traganitis and G. B. Giannakis, \"Sketched Subspace Clustering,\"\n  IEEE Transactions on Signal Processing, vol. 66, to appear 2018", "journal-ref": null, "doi": "10.1109/TSP.2017.2781649", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The immense amount of daily generated and communicated data presents unique\nchallenges in their processing. Clustering, the grouping of data without the\npresence of ground-truth labels, is an important tool for drawing inferences\nfrom data. Subspace clustering (SC) is a relatively recent method that is able\nto successfully classify nonlinearly separable data in a multitude of settings.\nIn spite of their high clustering accuracy, SC methods incur prohibitively high\ncomputational complexity when processing large volumes of high-dimensional\ndata. Inspired by random sketching approaches for dimensionality reduction, the\npresent paper introduces a randomized scheme for SC, termed Sketch-SC, tailored\nfor large volumes of high-dimensional data. Sketch-SC accelerates the\ncomputationally heavy parts of state-of-the-art SC approaches by compressing\nthe data matrix across both dimensions using random projections, thus enabling\nfast and accurate large-scale SC. Performance analysis as well as extensive\nnumerical tests on real data corroborate the potential of Sketch-SC and its\ncompetitive performance relative to state-of-the-art scalable SC approaches.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 17:13:26 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 16:58:26 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Traganitis", "Panagiotis A.", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1707.07240", "submitter": "Bin Wang", "authors": "Bin Wang and Zhijian Ou", "title": "Language modeling with Neural trans-dimensional random fields", "comments": "6 pages, 2 figures and 3 tables, accepted to ASRU 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trans-dimensional random field language models (TRF LMs) have recently been\nintroduced, where sentences are modeled as a collection of random fields. The\nTRF approach has been shown to have the advantages of being computationally\nmore efficient in inference than LSTM LMs with close performance and being able\nto flexibly integrating rich features. In this paper we propose neural TRFs,\nbeyond of the previous discrete TRFs that only use linear potentials with\ndiscrete features. The idea is to use nonlinear potentials with continuous\nfeatures, implemented by neural networks (NNs), in the TRF framework. Neural\nTRFs combine the advantages of both NNs and TRFs. The benefits of word\nembedding, nonlinear feature learning and larger context modeling are inherited\nfrom the use of NNs. At the same time, the strength of efficient inference by\navoiding expensive softmax is preserved. A number of technical contributions,\nincluding employing deep convolutional neural networks (CNNs) to define the\npotentials and incorporating the joint stochastic approximation (JSA) strategy\nin the training algorithm, are developed in this work, which enable us to\nsuccessfully train neural TRF LMs. Various LMs are evaluated in terms of speech\nrecognition WERs by rescoring the 1000-best lists of WSJ'92 test data. The\nresults show that neural TRF LMs not only improve over discrete TRF LMs, but\nalso perform slightly better than LSTM LMs with only one fifth of parameters\nand 16x faster inference efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 03:06:47 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 01:25:18 GMT"}, {"version": "v3", "created": "Tue, 19 Sep 2017 08:28:42 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Wang", "Bin", ""], ["Ou", "Zhijian", ""]]}, {"id": "1707.07287", "submitter": "Pavel Gurevich", "authors": "Pavel Gurevich, Hannes Stuke", "title": "Pairing an arbitrary regressor with an artificial neural network\n  estimating aleatoric uncertainty", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest a general approach to quantification of different forms of\naleatoric uncertainty in regression tasks performed by artificial neural\nnetworks. It is based on the simultaneous training of two neural networks with\na joint loss function and a specific hyperparameter $\\lambda>0$ that allows for\nautomatically detecting noisy and clean regions in the input space and\ncontrolling their {\\em relative contribution} to the loss and its gradients.\nAfter the model has been trained, one of the networks performs predictions and\nthe other quantifies the uncertainty of these predictions by estimating the\nlocally averaged loss of the first one. Unlike in many classical uncertainty\nquantification methods, we do not assume any a priori knowledge of the ground\ntruth probability distribution, neither do we, in general, maximize the\nlikelihood of a chosen parametric family of distributions. We analyze the\nlearning process and the influence of clean and noisy regions of the input\nspace on the loss surface, depending on $\\lambda$. In particular, we show that\nsmall values of $\\lambda$ increase the relative contribution of clean regions\nto the loss and its gradients. This explains why choosing small $\\lambda$\nallows for better predictions compared with neural networks without uncertainty\ncounterparts and those based on classical likelihood maximization. Finally, we\ndemonstrate that one can naturally form ensembles of pairs of our networks and\nthus capture both aleatoric and epistemic uncertainty and avoid overfitting.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 12:07:58 GMT"}, {"version": "v2", "created": "Sat, 20 Jan 2018 13:54:40 GMT"}, {"version": "v3", "created": "Mon, 3 Sep 2018 07:07:53 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Gurevich", "Pavel", ""], ["Stuke", "Hannes", ""]]}, {"id": "1707.07299", "submitter": "Hamed Ghanbari", "authors": "H. Ghanbari, H. Zayyani, E. Yazdian", "title": "Joint DOA Estimation and Array Calibration Using Multiple Parametric\n  Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter proposes a multiple parametric dictionary learning algorithm for\ndirection of arrival (DOA) estimation in presence of array gain-phase error and\nmutual coupling. It jointly solves both the DOA estimation and array\nimperfection problems to yield a robust DOA estimation in presence of array\nimperfection errors and off-grid. In the proposed method, a multiple parametric\ndictionary learning-based algorithm with an steepest-descent iteration is used\nfor learning the parametric perturbation matrices and the steering matrix\nsimultaneously. It also exploits the multiple snapshots information to enhance\nthe performance of DOA estimation. Simulation results show the efficiency of\nthe proposed algorithm when both off-grid problem and array imperfection exist.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 14:01:22 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Ghanbari", "H.", ""], ["Zayyani", "H.", ""], ["Yazdian", "E.", ""]]}, {"id": "1707.07328", "submitter": "Robin Jia", "authors": "Robin Jia and Percy Liang", "title": "Adversarial Examples for Evaluating Reading Comprehension Systems", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard accuracy metrics indicate that reading comprehension systems are\nmaking rapid progress, but the extent to which these systems truly understand\nlanguage remains unclear. To reward systems with real language understanding\nabilities, we propose an adversarial evaluation scheme for the Stanford\nQuestion Answering Dataset (SQuAD). Our method tests whether systems can answer\nquestions about paragraphs that contain adversarially inserted sentences, which\nare automatically generated to distract computer systems without changing the\ncorrect answer or misleading humans. In this adversarial setting, the accuracy\nof sixteen published models drops from an average of $75\\%$ F1 score to $36\\%$;\nwhen the adversary is allowed to add ungrammatical sequences of words, average\naccuracy on four models decreases further to $7\\%$. We hope our insights will\nmotivate the development of new models that understand language more precisely.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 18:26:29 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Jia", "Robin", ""], ["Liang", "Percy", ""]]}, {"id": "1707.07341", "submitter": "Michael Hughes", "authors": "Michael C. Hughes and Leah Weiner and Gabriel Hope and Thomas H. McCoy\n  Jr. and Roy H. Perlis and Erik B. Sudderth and Finale Doshi-Velez", "title": "Prediction-Constrained Training for Semi-Supervised Mixture and Topic\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervisory signals have the potential to make low-dimensional data\nrepresentations, like those learned by mixture and topic models, more\ninterpretable and useful. We propose a framework for training latent variable\nmodels that explicitly balances two goals: recovery of faithful generative\nexplanations of high-dimensional data, and accurate prediction of associated\nsemantic labels. Existing approaches fail to achieve these goals due to an\nincomplete treatment of a fundamental asymmetry: the intended application is\nalways predicting labels from data, not data from labels. Our\nprediction-constrained objective for training generative models coherently\nintegrates loss-based supervisory signals while enabling effective\nsemi-supervised learning from partially labeled data. We derive learning\nalgorithms for semi-supervised mixture and topic models using stochastic\ngradient descent with automatic differentiation. We demonstrate improved\nprediction quality compared to several previous supervised topic models,\nachieving predictions competitive with high-dimensional logistic regression on\ntext sentiment analysis and electronic health records tasks while\nsimultaneously learning interpretable topics.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 20:19:06 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Hughes", "Michael C.", ""], ["Weiner", "Leah", ""], ["Hope", "Gabriel", ""], ["McCoy", "Thomas H.", "Jr."], ["Perlis", "Roy H.", ""], ["Sudderth", "Erik B.", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1707.07342", "submitter": "Kia Khezeli", "authors": "Kia Khezeli and Eilyan Bitar", "title": "An Online Learning Approach to Buying and Selling Demand Response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adopt the perspective of an aggregator, which seeks to coordinate its\npurchase of demand reductions from a fixed group of residential electricity\ncustomers, with its sale of the aggregate demand reduction in a two-settlement\nwholesale energy market. The aggregator procures reductions in demand by\noffering its customers a uniform price for reductions in consumption relative\nto their predetermined baselines. Prior to its realization of the aggregate\ndemand reduction, the aggregator must also determine how much energy to sell\ninto the two-settlement energy market. In the day-ahead market, the aggregator\ncommits to a forward contract, which calls for the delivery of energy in the\nreal-time market. The underlying aggregate demand curve, which relates the\naggregate demand reduction to the aggregator's offered price, is assumed to be\naffine and subject to unobservable, random shocks. Assuming that both the\nparameters of the demand curve and the distribution of the random shocks are\ninitially unknown to the aggregator, we investigate the extent to which the\naggregator might dynamically adapt its offered prices and forward contracts to\nmaximize its expected profit over a time window of $T$ days. Specifically, we\ndesign a dynamic pricing and contract offering policy that resolves the\naggregator's need to learn the unknown demand model with its desire to maximize\nits cumulative expected profit over time. In particular, the proposed pricing\npolicy is proven to incur a regret over $T$ days that is no greater than\n$O(\\log(T)\\sqrt{T})$.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 20:34:36 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 13:07:57 GMT"}, {"version": "v3", "created": "Thu, 28 Dec 2017 00:21:46 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Khezeli", "Kia", ""], ["Bitar", "Eilyan", ""]]}, {"id": "1707.07394", "submitter": "Shin Fujieda", "authors": "Shin Fujieda, Kohei Takayama and Toshiya Hachisuka", "title": "Wavelet Convolutional Neural Networks for Texture Classification", "comments": "9 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture classification is an important and challenging problem in many image\nprocessing applications. While convolutional neural networks (CNNs) achieved\nsignificant successes for image classification, texture classification remains\na difficult problem since textures usually do not contain enough information\nregarding the shape of object. In image processing, texture classification has\nbeen traditionally studied well with spectral analyses which exploit repeated\nstructures in many textures. Since CNNs process images as-is in the spatial\ndomain whereas spectral analyses process images in the frequency domain, these\nmodels have different characteristics in terms of performance. We propose a\nnovel CNN architecture, wavelet CNNs, which integrates a spectral analysis into\nCNNs. Our insight is that the pooling layer and the convolution layer can be\nviewed as a limited form of a spectral analysis. Based on this insight, we\ngeneralize both layers to perform a spectral analysis with wavelet transform.\nWavelet CNNs allow us to utilize spectral information which is lost in\nconventional CNNs but useful in texture classification. The experiments\ndemonstrate that our model achieves better accuracy in texture classification\nthan existing models. We also show that our model has significantly fewer\nparameters than CNNs, making our model easier to train with less memory.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 03:59:04 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Fujieda", "Shin", ""], ["Takayama", "Kohei", ""], ["Hachisuka", "Toshiya", ""]]}, {"id": "1707.07399", "submitter": "Miao Liu", "authors": "Miao Liu, Kavinayan Sivakumar, Shayegan Omidshafiei, Christopher\n  Amato, Jonathan P. How", "title": "Learning for Multi-robot Cooperation in Partially Observable Stochastic\n  Environments with Macro-actions", "comments": "Accepted to the 2017 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a data-driven approach for multi-robot coordination in\npartially-observable domains based on Decentralized Partially Observable Markov\nDecision Processes (Dec-POMDPs) and macro-actions (MAs). Dec-POMDPs provide a\ngeneral framework for cooperative sequential decision making under uncertainty\nand MAs allow temporally extended and asynchronous action execution. To date,\nmost methods assume the underlying Dec-POMDP model is known a priori or a full\nsimulator is available during planning time. Previous methods which aim to\naddress these issues suffer from local optimality and sensitivity to initial\nconditions. Additionally, few hardware demonstrations involving a large team of\nheterogeneous robots and with long planning horizons exist. This work addresses\nthese gaps by proposing an iterative sampling based Expectation-Maximization\nalgorithm (iSEM) to learn polices using only trajectory data containing\nobservations, MAs, and rewards. Our experiments show the algorithm is able to\nachieve better solution quality than the state-of-the-art learning-based\nmethods. We implement two variants of multi-robot Search and Rescue (SAR)\ndomains (with and without obstacles) on hardware to demonstrate the learned\npolicies can effectively control a team of distributed robots to cooperate in a\npartially observable stochastic environment.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 04:23:02 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 01:44:18 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Liu", "Miao", ""], ["Sivakumar", "Kavinayan", ""], ["Omidshafiei", "Shayegan", ""], ["Amato", "Christopher", ""], ["How", "Jonathan P.", ""]]}, {"id": "1707.07402", "submitter": "Khanh Nguyen", "authors": "Khanh Nguyen, Hal Daum\\'e III and Jordan Boyd-Graber", "title": "Reinforcement Learning for Bandit Neural Machine Translation with\n  Simulated Human Feedback", "comments": "11 pages, 5 figures, In Proceedings of Empirical Methods in Natural\n  Language Processing (EMNLP) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation is a natural candidate problem for reinforcement learning\nfrom human feedback: users provide quick, dirty ratings on candidate\ntranslations to guide a system to improve. Yet, current neural machine\ntranslation training focuses on expensive human-generated reference\ntranslations. We describe a reinforcement learning algorithm that improves\nneural machine translation systems from simulated human feedback. Our algorithm\ncombines the advantage actor-critic algorithm (Mnih et al., 2016) with the\nattention-based neural encoder-decoder architecture (Luong et al., 2015). This\nalgorithm (a) is well-designed for problems with a large action space and\ndelayed rewards, (b) effectively optimizes traditional corpus-level machine\ntranslation metrics, and (c) is robust to skewed, high-variance, granular\nfeedback modeled after actual human behaviors.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 04:35:19 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 17:19:01 GMT"}, {"version": "v3", "created": "Fri, 13 Oct 2017 06:10:55 GMT"}, {"version": "v4", "created": "Sat, 11 Nov 2017 05:01:23 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Nguyen", "Khanh", ""], ["Daum\u00e9", "Hal", "III"], ["Boyd-Graber", "Jordan", ""]]}, {"id": "1707.07409", "submitter": "Rajiv Sambasivan", "authors": "Rajiv Sambasivan, Sourish Das", "title": "Big Data Regression Using Tree Based Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling regression to large datasets is a common problem in many application\nareas. We propose a two step approach to scaling regression to large datasets.\nUsing a regression tree (CART) to segment the large dataset constitutes the\nfirst step of this approach. The second step of this approach is to develop a\nsuitable regression model for each segment. Since segment sizes are not very\nlarge, we have the ability to apply sophisticated regression techniques if\nrequired. A nice feature of this two step approach is that it can yield models\nthat have good explanatory power as well as good predictive performance.\nEnsemble methods like Gradient Boosted Trees can offer excellent predictive\nperformance but may not provide interpretable models. In the experiments\nreported in this study, we found that the predictive performance of the\nproposed approach matched the predictive performance of Gradient Boosted Trees.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 05:33:37 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 01:55:12 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Sambasivan", "Rajiv", ""], ["Das", "Sourish", ""]]}, {"id": "1707.07443", "submitter": "Cem Tekin", "authors": "A. \\\"Omer Sar{\\i}ta\\c{c} and Cem Tekin", "title": "Combinatorial Multi-armed Bandit with Probabilistically Triggered Arms:\n  A Case with Bounded Regret", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the combinatorial multi-armed bandit problem (CMAB)\nwith probabilistically triggered arms (PTAs). Under the assumption that the arm\ntriggering probabilities (ATPs) are positive for all arms, we prove that a\nclass of upper confidence bound (UCB) policies, named Combinatorial UCB with\nexploration rate $\\kappa$ (CUCB-$\\kappa$), and Combinatorial Thompson Sampling\n(CTS), which estimates the expected states of the arms via Thompson sampling,\nachieve bounded regret. In addition, we prove that CUCB-$0$ and CTS incur\n$O(\\sqrt{T})$ gap-independent regret. These results improve the results in\nprevious works, which show $O(\\log T)$ gap-dependent and $O(\\sqrt{T\\log T})$\ngap-independent regrets, respectively, under no assumptions on the ATPs. Then,\nwe numerically evaluate the performance of CUCB-$\\kappa$ and CTS in a\nreal-world movie recommendation problem, where the actions correspond to\nrecommending a set of movies, the arms correspond to the edges between the\nmovies and the users, and the goal is to maximize the total number of users\nthat are attracted by at least one movie. Our numerical results complement our\ntheoretical findings on bounded regret. Apart from this problem, our results\nalso directly apply to the online influence maximization (OIM) problem studied\nin numerous prior works.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 09:01:46 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Sar\u0131ta\u00e7", "A. \u00d6mer", ""], ["Tekin", "Cem", ""]]}, {"id": "1707.07469", "submitter": "Marta R. Costa-juss\\`a", "authors": "Han Yang, Marta R. Costa-juss\\`a and Jos\\'e A. R. Fonollosa", "title": "Character-level Intra Attention Network for Natural Language Inference", "comments": "EMNLP Workshop RepEval 2017: The Second Workshop on Evaluating Vector\n  Space Representations for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language inference (NLI) is a central problem in language\nunderstanding. End-to-end artificial neural networks have reached\nstate-of-the-art performance in NLI field recently.\n  In this paper, we propose Character-level Intra Attention Network (CIAN) for\nthe NLI task. In our model, we use the character-level convolutional network to\nreplace the standard word embedding layer, and we use the intra attention to\ncapture the intra-sentence semantics. The proposed CIAN model provides improved\nresults based on a newly published MNLI corpus.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 10:35:46 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Yang", "Han", ""], ["Costa-juss\u00e0", "Marta R.", ""], ["Fonollosa", "Jos\u00e9 A. R.", ""]]}, {"id": "1707.07530", "submitter": "Hamid Eghbal-Zadeh", "authors": "Hamid Eghbal-zadeh, Gerhard Widmer", "title": "Likelihood Estimation for Generative Adversarial Networks", "comments": "ICML 2017 Workshop on Implicit Models", "journal-ref": null, "doi": null, "report-no": "1707.07530", "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple method for assessing the quality of generated images in\nGenerative Adversarial Networks (GANs). The method can be applied in any kind\nof GAN without interfering with the learning procedure or affecting the\nlearning objective. The central idea is to define a likelihood function that\ncorrelates with the quality of the generated images. In particular, we derive a\nGaussian likelihood function from the distribution of the embeddings (hidden\nactivations) of the real images in the discriminator, and based on this, define\ntwo simple measures of how likely it is that the embeddings of generated images\nare from the distribution of the embeddings of the real images. This yields a\nsimple measure of fitness for generated images, for all varieties of GANs.\nEmpirical results on CIFAR-10 demonstrate a strong correlation between the\nproposed measures and the perceived quality of the generated images.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 12:58:46 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Eghbal-zadeh", "Hamid", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1707.07539", "submitter": "Qianqian Xu", "authors": "Qianqian Xu, Ming Yan, Chendi Huang, Jiechao Xiong, Qingming Huang,\n  Yuan Yao", "title": "Exploring Outliers in Crowdsourced Ranking for QoE", "comments": "accepted by ACM Multimedia 2017 (Oral presentation). arXiv admin\n  note: text overlap with arXiv:1407.7636", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection is a crucial part of robust evaluation for crowdsourceable\nassessment of Quality of Experience (QoE) and has attracted much attention in\nrecent years. In this paper, we propose some simple and fast algorithms for\noutlier detection and robust QoE evaluation based on the nonconvex optimization\nprinciple. Several iterative procedures are designed with or without knowing\nthe number of outliers in samples. Theoretical analysis is given to show that\nsuch procedures can reach statistically good estimates under mild conditions.\nFinally, experimental results with simulated and real-world crowdsourcing\ndatasets show that the proposed algorithms could produce similar performance to\nHuber-LASSO approach in robust ranking, yet with nearly 8 or 90 times speed-up,\nwithout or with a prior knowledge on the sparsity size of outliers,\nrespectively. Therefore the proposed methodology provides us a set of helpful\ntools for robust QoE evaluation with crowdsourcing data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 10:34:13 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Xu", "Qianqian", ""], ["Yan", "Ming", ""], ["Huang", "Chendi", ""], ["Xiong", "Jiechao", ""], ["Huang", "Qingming", ""], ["Yao", "Yuan", ""]]}, {"id": "1707.07565", "submitter": "Thomas Wollmann", "authors": "Thomas Wollmann, Karl Rohr", "title": "Automatic breast cancer grading in lymph nodes using a deep neural\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The progression of breast cancer can be quantified in lymph node whole-slide\nimages (WSIs). We describe a novel method for effectively performing\nclassification of whole-slide images and patient level breast cancer grading.\nOur method utilises a deep neural network. The method performs classification\non small patches and uses model averaging for boosting. In the first step,\nregion of interest patches are determined and cropped automatically by color\nthresholding and then classified by the deep neural network. The classification\nresults are used to determine a slide level class and for further aggregation\nto predict a patient level grade. Fast processing speed of our method enables\nhigh throughput image analysis.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 14:09:47 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Wollmann", "Thomas", ""], ["Rohr", "Karl", ""]]}, {"id": "1707.07576", "submitter": "Andreas Henelius", "authors": "Andreas Henelius, Kai Puolam\\\"aki, Antti Ukkonen", "title": "Interpreting Classifiers through Attribute Interactions in Datasets", "comments": "presented at 2017 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2017), Sydney, NSW, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present the novel ASTRID method for investigating which\nattribute interactions classifiers exploit when making predictions. Attribute\ninteractions in classification tasks mean that two or more attributes together\nprovide stronger evidence for a particular class label. Knowledge of such\ninteractions makes models more interpretable by revealing associations between\nattributes. This has applications, e.g., in pharmacovigilance to identify\ninteractions between drugs or in bioinformatics to investigate associations\nbetween single nucleotide polymorphisms. We also show how the found attribute\npartitioning is related to a factorisation of the data generating distribution\nand empirically demonstrate the utility of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 14:34:28 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Henelius", "Andreas", ""], ["Puolam\u00e4ki", "Kai", ""], ["Ukkonen", "Antti", ""]]}, {"id": "1707.07585", "submitter": "Weizheng Chen", "authors": "Zeya Zhang, Weizheng Chen and Hongfei Yan", "title": "Stock Prediction: a method based on extraction of news features and\n  recurrent neural networks", "comments": "in Chinese, The 22nd China Conference on Information Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed a method for stock prediction. In terms of feature\nextraction, we extract the features of stock-related news besides stock prices.\nWe first select some seed words based on experience which are the symbols of\ngood news and bad news. Then we propose an optimization method and calculate\nthe positive polar of all words. After that, we construct the features of news\nbased on the positive polar of their words. In consideration of sequential\nstock prices and continuous news effects, we propose a recurrent neural network\nmodel to help predict stock prices. Compared to SVM classifier with price\nfeatures, we find our proposed method has an over 5% improvement on stock\nprediction accuracy in experiments.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 13:40:06 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Zhang", "Zeya", ""], ["Chen", "Weizheng", ""], ["Yan", "Hongfei", ""]]}, {"id": "1707.07605", "submitter": "Mostafa Dehghani", "authors": "Mostafa Dehghani, Hosein Azarbonyad, Jaap Kamps, Maarten de Rijke", "title": "Share your Model instead of your Data: Privacy Preserving Mimic Learning\n  for Ranking", "comments": "SIGIR 2017 Workshop on Neural Information Retrieval\n  (Neu-IR'17)}{}{August 7--11, 2017, Shinjuku, Tokyo, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have become a primary tool for solving problems in many\nfields. They are also used for addressing information retrieval problems and\nshow strong performance in several tasks. Training these models requires large,\nrepresentative datasets and for most IR tasks, such data contains sensitive\ninformation from users. Privacy and confidentiality concerns prevent many data\nowners from sharing the data, thus today the research community can only\nbenefit from research on large-scale datasets in a limited manner. In this\npaper, we discuss privacy preserving mimic learning, i.e., using predictions\nfrom a privacy preserving trained model instead of labels from the original\nsensitive training data as a supervision signal. We present the results of\npreliminary experiments in which we apply the idea of mimic learning and\nprivacy preserving mimic learning for the task of document re-ranking as one of\nthe core IR tasks. This research is a step toward laying the ground for\nenabling researchers from data-rich environments to share knowledge learned\nfrom actual users' data, which should facilitate research collaborations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 15:23:41 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Dehghani", "Mostafa", ""], ["Azarbonyad", "Hosein", ""], ["Kamps", "Jaap", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1707.07609", "submitter": "Alexandre Thiery", "authors": "Sripad Krishna Devalla, Jean-Martial Mari, Tin A. Tun, Nicholas G.\n  Strouthidis, Tin Aung, Alexandre H. Thiery, Michael J. A. Girard", "title": "A Deep Learning Approach to Digitally Stain Optical Coherence Tomography\n  Images of the Optic Nerve Head", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To develop a deep learning approach to digitally-stain optical\ncoherence tomography (OCT) images of the optic nerve head (ONH).\n  Methods: A horizontal B-scan was acquired through the center of the ONH using\nOCT (Spectralis) for 1 eye of each of 100 subjects (40 normal & 60 glaucoma).\nAll images were enhanced using adaptive compensation. A custom deep learning\nnetwork was then designed and trained with the compensated images to digitally\nstain (i.e. highlight) 6 tissue layers of the ONH. The accuracy of our\nalgorithm was assessed (against manual segmentations) using the Dice\ncoefficient, sensitivity, and specificity. We further studied how compensation\nand the number of training images affected the performance of our algorithm.\n  Results: For images it had not yet assessed, our algorithm was able to\ndigitally stain the retinal nerve fiber layer + prelamina, the retinal pigment\nepithelium, all other retinal layers, the choroid, and the peripapillary sclera\nand lamina cribrosa. For all tissues, the mean dice coefficient was $0.84 \\pm\n0.03$, the mean sensitivity $0.92 \\pm 0.03$, and the mean specificity $0.99 \\pm\n0.00$. Our algorithm performed significantly better when compensated images\nwere used for training. Increasing the number of images (from 10 to 40) to\ntrain our algorithm did not significantly improve performance, except for the\nRPE.\n  Conclusion. Our deep learning algorithm can simultaneously stain neural and\nconnective tissues in ONH images. Our approach offers a framework to\nautomatically measure multiple key structural parameters of the ONH that may be\ncritical to improve glaucoma management.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 15:41:45 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Devalla", "Sripad Krishna", ""], ["Mari", "Jean-Martial", ""], ["Tun", "Tin A.", ""], ["Strouthidis", "Nicholas G.", ""], ["Aung", "Tin", ""], ["Thiery", "Alexandre H.", ""], ["Girard", "Michael J. A.", ""]]}, {"id": "1707.07620", "submitter": "Saptarshi Das", "authors": "Shre Kumar Chatterjee, Saptarshi Das, Koushik Maharatna, Elisa Masi,\n  Luisa Santopolo, Ilaria Colzi, Stefano Mancuso and Andrea Vitaletti", "title": "Comparison of Decision Tree Based Classification Strategies to Detect\n  External Chemical Stimuli from Raw and Filtered Plant Electrical Response", "comments": null, "journal-ref": "Sensors and Actuators B: Chemical, vol. 249, pp. 278-295, Oct.\n  2017", "doi": "10.1016/j.snb.2017.04.071", "report-no": null, "categories": "physics.bio-ph cs.LG physics.data-an stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plants monitor their surrounding environment and control their physiological\nfunctions by producing an electrical response. We recorded electrical signals\nfrom different plants by exposing them to Sodium Chloride (NaCl), Ozone (O3)\nand Sulfuric Acid (H2SO4) under laboratory conditions. After applying\npre-processing techniques such as filtering and drift removal, we extracted few\nstatistical features from the acquired plant electrical signals. Using these\nfeatures, combined with different classification algorithms, we used a decision\ntree based multi-class classification strategy to identify the three different\nexternal chemical stimuli. We here present our exploration to obtain the\noptimum set of ranked feature and classifier combination that can separate a\nparticular chemical stimulus from the incoming stream of plant electrical\nsignals. The paper also reports an exhaustive comparison of similar feature\nbased classification using the filtered and the raw plant signals, containing\nthe high frequency stochastic part and also the low frequency trends present in\nit, as two different cases for feature extraction. The work, presented in this\npaper opens up new possibilities for using plant electrical signals to monitor\nand detect other environmental stimuli apart from NaCl, O3 and H2SO4 in future.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 19:00:14 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Chatterjee", "Shre Kumar", ""], ["Das", "Saptarshi", ""], ["Maharatna", "Koushik", ""], ["Masi", "Elisa", ""], ["Santopolo", "Luisa", ""], ["Colzi", "Ilaria", ""], ["Mancuso", "Stefano", ""], ["Vitaletti", "Andrea", ""]]}, {"id": "1707.07657", "submitter": "Ehsan Sadrfaridpour", "authors": "E. Sadrfaridpour, T. Razzaghi, I. Safro", "title": "Engineering fast multilevel support vector machines", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational complexity of solving nonlinear support vector machine\n(SVM) is prohibitive on large-scale data. In particular, this issue becomes\nvery sensitive when the data represents additional difficulties such as highly\nimbalanced class sizes. Typically, nonlinear kernels produce significantly\nhigher classification quality to linear kernels but introduce extra kernel and\nmodel parameters which requires computationally expensive fitting. This\nincreases the quality but also reduces the performance dramatically. We\nintroduce a generalized fast multilevel framework for regular and weighted SVM\nand discuss several versions of its algorithmic components that lead to a good\ntrade-off between quality and time. Our framework is implemented using PETSc\nwhich allows an easy integration with scientific computing tasks. The\nexperimental results demonstrate significant speed up compared to the\nstate-of-the-art nonlinear SVM libraries.\n  Reproducibility: our source code, documentation and parameters are available\nat https:// github.com/esadr/mlsvm.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 17:32:37 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 00:41:30 GMT"}, {"version": "v3", "created": "Sat, 6 Apr 2019 02:37:58 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Sadrfaridpour", "E.", ""], ["Razzaghi", "T.", ""], ["Safro", "I.", ""]]}, {"id": "1707.07708", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang", "title": "Per-instance Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a refinement of differential privacy --- per instance\ndifferential privacy (pDP), which captures the privacy of a specific individual\nwith respect to a fixed data set. We show that this is a strict generalization\nof the standard DP and inherits all its desirable properties, e.g.,\ncomposition, invariance to side information and closedness to postprocessing,\nexcept that they all hold for every instance separately. When the data is drawn\nfrom a distribution, we show that per-instance DP implies generalization.\nMoreover, we provide explicit calculations of the per-instance DP for the\noutput perturbation on a class of smooth learning problems. The result reveals\nan interesting and intuitive fact that an individual has stronger privacy if\nhe/she has small \"leverage score\" with respect to the data set and if he/she\ncan be predicted more accurately using the leave-one-out data set. Our\nsimulation shows several orders-of-magnitude more favorable privacy and utility\ntrade-off when we consider the privacy of only the users in the data set. In a\ncase study on differentially private linear regression, provide a novel\nanalysis of the One-Posterior-Sample (OPS) estimator and show that when the\ndata set is well-conditioned it provides $(\\epsilon,\\delta)$-pDP for any target\nindividuals and matches the exact lower bound up to a\n$1+\\tilde{O}(n^{-1}\\epsilon^{-2})$ multiplicative factor. We also demonstrate\nhow we can use a \"pDP to DP conversion\" step to design AdaOPS which uses\nadaptive regularization to achieve the same results with\n$(\\epsilon,\\delta)$-DP.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 18:35:31 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 01:39:20 GMT"}, {"version": "v3", "created": "Sat, 7 Jul 2018 11:15:26 GMT"}, {"version": "v4", "created": "Wed, 14 Nov 2018 07:11:31 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Wang", "Yu-Xiang", ""]]}, {"id": "1707.07716", "submitter": "Jiasen Yang", "authors": "Jiasen Yang, Bruno Ribeiro, Jennifer Neville", "title": "Stochastic Gradient Descent for Relational Logistic Regression via\n  Partial Network Crawls", "comments": "7 pages, 3 figures, Proceedings of the Seventh International Workshop\n  on Statistical Relational AI (StarAI 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in statistical relational learning has produced a number of methods\nfor learning relational models from large-scale network data. While these\nmethods have been successfully applied in various domains, they have been\ndeveloped under the unrealistic assumption of full data access. In practice,\nhowever, the data are often collected by crawling the network, due to\nproprietary access, limited resources, and privacy concerns. Recently, we\nshowed that the parameter estimates for relational Bayes classifiers computed\nfrom network samples collected by existing network crawlers can be quite\ninaccurate, and developed a crawl-aware estimation method for such models\n(Yang, Ribeiro, and Neville, 2017). In this work, we extend the methodology to\nlearning relational logistic regression models via stochastic gradient descent\nfrom partial network crawls, and show that the proposed method yields accurate\nparameter estimates and confidence intervals.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 19:32:16 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 01:12:29 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Yang", "Jiasen", ""], ["Ribeiro", "Bruno", ""], ["Neville", "Jennifer", ""]]}, {"id": "1707.07767", "submitter": "Kun Li", "authors": "Kun Li, Yanan Sui, Joel W. Burdick", "title": "Bellman Gradient Iteration for Inverse Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops an inverse reinforcement learning algorithm aimed at\nrecovering a reward function from the observed actions of an agent. We\nintroduce a strategy to flexibly handle different types of actions with two\napproximations of the Bellman Optimality Equation, and a Bellman Gradient\nIteration method to compute the gradient of the Q-value with respect to the\nreward function. These methods allow us to build a differentiable relation\nbetween the Q-value and the reward function and learn an approximately optimal\nreward function with gradient methods. We test the proposed method in two\nsimulated environments by evaluating the accuracy of different approximations\nand comparing the proposed method with existing solutions. The results show\nthat even with a linear reward function, the proposed method has a comparable\naccuracy with the state-of-the-art method adopting a non-linear reward\nfunction, and the proposed method is more flexible because it is defined on\nobserved actions instead of trajectories.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 23:00:23 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Li", "Kun", ""], ["Sui", "Yanan", ""], ["Burdick", "Joel W.", ""]]}, {"id": "1707.07769", "submitter": "Gael Sent\\'is", "authors": "Gael Sent\\'is, John Calsamiglia, Ramon Munoz-Tapia", "title": "Exact Identification of a Quantum Change Point", "comments": "Published version. 5 pages, 2 figures", "journal-ref": "Phys. Rev. Lett. 119, 140506 (2017)", "doi": "10.1103/PhysRevLett.119.140506", "report-no": null, "categories": "quant-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of change points is a pivotal task in statistical analysis. In\nthe quantum realm, it is a new primitive where one aims at identifying the\npoint where a source that supposedly prepares a sequence of particles in\nidentical quantum states starts preparing a mutated one. We obtain the optimal\nprocedure to identify the change point with certainty---naturally at the price\nof having a certain probability of getting an inconclusive answer. We obtain\nthe analytical form of the optimal probability of successful identification for\nany length of the particle sequence. We show that the conditional success\nprobabilities of identifying each possible change point show an unexpected\noscillatory behaviour. We also discuss local (online) protocols and compare\nthem with the optimal procedure.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 23:05:44 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 18:08:16 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Sent\u00eds", "Gael", ""], ["Calsamiglia", "John", ""], ["Munoz-Tapia", "Ramon", ""]]}, {"id": "1707.07770", "submitter": "Thee Chanyaswad", "authors": "Artur Filipowicz, Thee Chanyaswad, S. Y. Kung", "title": "Desensitized RDCA Subspaces for Compressive Privacy in Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quest for better data analysis and artificial intelligence has lead to\nmore and more data being collected and stored. As a consequence, more data are\nexposed to malicious entities. This paper examines the problem of privacy in\nmachine learning for classification. We utilize the Ridge Discriminant\nComponent Analysis (RDCA) to desensitize data with respect to a privacy label.\nBased on five experiments, we show that desensitization by RDCA can effectively\nprotect privacy (i.e. low accuracy on the privacy label) with small loss in\nutility. On HAR and CMU Faces datasets, the use of desensitized data results in\nrandom guess level accuracies for privacy at a cost of 5.14% and 0.04%, on\naverage, drop in the utility accuracies. For Semeion Handwritten Digit dataset,\naccuracies of the privacy-sensitive digits are almost zero, while the\naccuracies for the utility-relevant digits drop by 7.53% on average. This\npresents a promising solution to the problem of privacy in machine learning for\nclassification.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 23:25:11 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Filipowicz", "Artur", ""], ["Chanyaswad", "Thee", ""], ["Kung", "S. Y.", ""]]}, {"id": "1707.07785", "submitter": "Seyed Mehran Kazemi", "authors": "Seyed Mehran Kazemi, Bahare Fatemi, Alexandra Kim, Zilun Peng, Moumita\n  Roy Tora, Xing Zeng, Matthew Dirks, David Poole", "title": "Comparing Aggregators for Relational Probabilistic Models", "comments": "8 pages, Accepted at Statistical Relational AI (StarAI) workshop 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Relational probabilistic models have the challenge of aggregation, where one\nvariable depends on a population of other variables. Consider the problem of\npredicting gender from movie ratings; this is challenging because the number of\nmovies per user and users per movie can vary greatly. Surprisingly, aggregation\nis not well understood. In this paper, we show that existing relational models\n(implicitly or explicitly) either use simple numerical aggregators that lose\ngreat amounts of information, or correspond to naive Bayes, logistic\nregression, or noisy-OR that suffer from overconfidence. We propose new simple\naggregators and simple modifications of existing models that empirically\noutperform the existing ones. The intuition we provide on different (existing\nor new) models and their shortcomings plus our empirical findings promise to\nform the foundation for future representations.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 01:32:20 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Kazemi", "Seyed Mehran", ""], ["Fatemi", "Bahare", ""], ["Kim", "Alexandra", ""], ["Peng", "Zilun", ""], ["Tora", "Moumita Roy", ""], ["Zeng", "Xing", ""], ["Dirks", "Matthew", ""], ["Poole", "David", ""]]}, {"id": "1707.07821", "submitter": "Heng Wang", "authors": "Shujian Yu, Zubin Abraham, Heng Wang, Mohak Shah, Yantao Wei and\n  Jos\\'e C. Pr\\'incipe", "title": "Concept Drift Detection and Adaptation with Hierarchical Hypothesis\n  Testing", "comments": "Manuscript accepted by the Journal of The Franklin Institute. A short\n  version of this manuscript, titled \"Concept Drift Detection with Hierarchical\n  Hypothesis Test\", was presented at the 2017 SIAM International Conference on\n  Data Mining (SDM) https://epubs.siam.org/doi/10.1137/1.9781611974973.86", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental issue for statistical classification models in a streaming\nenvironment is that the joint distribution between predictor and response\nvariables changes over time (a phenomenon also known as concept drifts), such\nthat their classification performance deteriorates dramatically. In this paper,\nwe first present a hierarchical hypothesis testing (HHT) framework that can\ndetect and also adapt to various concept drift types (e.g., recurrent or\nirregular, gradual or abrupt), even in the presence of imbalanced data labels.\nA novel concept drift detector, namely Hierarchical Linear Four Rates (HLFR),\nis implemented under the HHT framework thereafter. By substituting a\nwidely-acknowledged retraining scheme with an adaptive training strategy, we\nfurther demonstrate that the concept drift adaptation capability of HLFR can be\nsignificantly boosted. The theoretical analysis on the Type-I and Type-II\nerrors of HLFR is also performed. Experiments on both simulated and real-world\ndatasets illustrate that our methods outperform state-of-the-art methods in\nterms of detection precision, detection delay as well as the adaptability\nacross different concept drift types.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 06:05:27 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 01:11:13 GMT"}, {"version": "v3", "created": "Mon, 20 Nov 2017 01:11:08 GMT"}, {"version": "v4", "created": "Wed, 29 Aug 2018 22:28:04 GMT"}, {"version": "v5", "created": "Mon, 17 Sep 2018 07:20:37 GMT"}, {"version": "v6", "created": "Fri, 8 Feb 2019 18:54:26 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Yu", "Shujian", ""], ["Abraham", "Zubin", ""], ["Wang", "Heng", ""], ["Shah", "Mohak", ""], ["Wei", "Yantao", ""], ["Pr\u00edncipe", "Jos\u00e9 C.", ""]]}, {"id": "1707.07831", "submitter": "Zhun Sun", "authors": "Zhun Sun, Mete Ozay, Takayuki Okatani", "title": "Linear Discriminant Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel method for training of GANs for unsupervised and class\nconditional generation of images, called Linear Discriminant GAN (LD-GAN). The\ndiscriminator of an LD-GAN is trained to maximize the linear separability\nbetween distributions of hidden representations of generated and targeted\nsamples, while the generator is updated based on the decision hyper-planes\ncomputed by performing LDA over the hidden representations. LD-GAN provides a\nconcrete metric of separation capacity for the discriminator, and we\nexperimentally show that it is possible to stabilize the training of LD-GAN\nsimply by calibrating the update frequencies between generators and\ndiscriminators in the unsupervised case, without employment of normalization\nmethods and constraints on weights. In the class conditional generation tasks,\nthe proposed method shows improved training stability together with better\ngeneralization performance compared to WGAN that employs an auxiliary\nclassifier.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 06:33:49 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Sun", "Zhun", ""], ["Ozay", "Mete", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1707.07901", "submitter": "Mingsheng Long", "authors": "Zhangjie Cao, Mingsheng Long, Jianmin Wang, Michael I. Jordan", "title": "Partial Transfer Learning with Selective Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial learning has been successfully embedded into deep networks to\nlearn transferable features, which reduce distribution discrepancy between the\nsource and target domains. Existing domain adversarial networks assume fully\nshared label space across domains. In the presence of big data, there is strong\nmotivation of transferring both classification and representation models from\nexisting big domains to unknown small domains. This paper introduces partial\ntransfer learning, which relaxes the shared label space assumption to that the\ntarget label space is only a subspace of the source label space. Previous\nmethods typically match the whole source domain to the target domain, which are\nprone to negative transfer for the partial transfer problem. We present\nSelective Adversarial Network (SAN), which simultaneously circumvents negative\ntransfer by selecting out the outlier source classes and promotes positive\ntransfer by maximally matching the data distributions in the shared label\nspace. Experiments demonstrate that our models exceed state-of-the-art results\nfor partial transfer learning tasks on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 10:32:48 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Cao", "Zhangjie", ""], ["Long", "Mingsheng", ""], ["Wang", "Jianmin", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1707.07938", "submitter": "Fabien Lauer", "authors": "Fabien Lauer (ABC)", "title": "Error Bounds for Piecewise Smooth and Switching Regression", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice,after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with regression problems, in which the nonsmooth target is\nassumed to switch between different operating modes. Specifically, piecewise\nsmooth (PWS) regression considers target functions switching deterministically\nvia a partition of the input space, while switching regression considers\narbitrary switching laws. The paper derives generalization error bounds in\nthese two settings by following the approach based on Rademacher complexities.\nFor PWS regression, our derivation involves a chaining argument and a\ndecomposition of the covering numbers of PWS classes in terms of the ones of\ntheir component functions and the capacity of the classifier partitioning the\ninput space. This yields error bounds with a radical dependency on the number\nof modes. For switching regression, the decomposition can be performed directly\nat the level of the Rademacher complexities, which yields bounds with a linear\ndependency on the number of modes. By using once more chaining and a\ndecomposition at the level of covering numbers, we show how to recover a\nradical dependency. Examples of applications are given in particular for PWS\nand swichting regression with linear and kernel-based component functions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 12:06:22 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 14:09:26 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Lauer", "Fabien", "", "ABC"]]}, {"id": "1707.08005", "submitter": "Dacheng Tao", "authors": "Yunhe Wang, Chang Xu, Jiayan Qiu, Chao Xu, Dacheng Tao", "title": "Towards Evolutional Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressing convolutional neural networks (CNNs) is essential for\ntransferring the success of CNNs to a wide variety of applications to mobile\ndevices. In contrast to directly recognizing subtle weights or filters as\nredundant in a given CNN, this paper presents an evolutionary method to\nautomatically eliminate redundant convolution filters. We represent each\ncompressed network as a binary individual of specific fitness. Then, the\npopulation is upgraded at each evolutionary iteration using genetic operations.\nAs a result, an extremely compact CNN is generated using the fittest\nindividual. In this approach, either large or small convolution filters can be\nredundant, and filters in the compressed network are more distinct. In\naddition, since the number of filters in each convolutional layer is reduced,\nthe number of filter channels and the size of feature maps are also decreased,\nnaturally improving both the compression and speed-up ratios. Experiments on\nbenchmark deep CNN models suggest the superiority of the proposed algorithm\nover the state-of-the-art compression methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 14:02:03 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Wang", "Yunhe", ""], ["Xu", "Chang", ""], ["Qiu", "Jiayan", ""], ["Xu", "Chao", ""], ["Tao", "Dacheng", ""]]}, {"id": "1707.08008", "submitter": "Xi Li", "authors": "Te Pi, Xi Li, Zhongfei (Mark) Zhang", "title": "Boosted Zero-Shot Learning with Semantic Correlation Regularization", "comments": "7 pages; IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study zero-shot learning (ZSL) as a transfer learning problem, and focus\non the two key aspects of ZSL, model effectiveness and model adaptation. For\neffective modeling, we adopt the boosting strategy to learn a zero-shot\nclassifier from weak models to a strong model. For adaptable knowledge\ntransfer, we devise a Semantic Correlation Regularization (SCR) approach to\nregularize the boosted model to be consistent with the inter-class semantic\ncorrelations. With SCR embedded in the boosting objective, and with a\nself-controlled sample selection for learning robustness, we propose a unified\nframework, Boosted Zero-shot classification with Semantic Correlation\nRegularization (BZ-SCR). By balancing the SCR-regularized boosted model\nselection and the self-controlled sample selection, BZ-SCR is capable of\ncapturing both discriminative and adaptable feature-to-class semantic\nalignments, while ensuring the reliability and adaptability of the learned\nsamples. The experiments on two ZSL datasets show the superiority of BZ-SCR\nover the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 14:12:55 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Pi", "Te", "", "Mark"], ["Li", "Xi", "", "Mark"], ["Zhongfei", "", "", "Mark"], ["Zhang", "", ""]]}, {"id": "1707.08040", "submitter": "Vinay Verma Kumar", "authors": "Vinay Kumar Verma and Piyush Rai", "title": "A Simple Exponential Family Framework for Zero-Shot Learning", "comments": "Accepted in ECML-PKDD 2017, 16 Pages: Code and Data are available:\n  https://github.com/vkverma01/Zero-Shot/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a simple generative framework for learning to predict previously\nunseen classes, based on estimating class-attribute-gated class-conditional\ndistributions. We model each class-conditional distribution as an exponential\nfamily distribution and the parameters of the distribution of each seen/unseen\nclass are defined as functions of the respective observed class attributes.\nThese functions can be learned using only the seen class data and can be used\nto predict the parameters of the class-conditional distribution of each unseen\nclass. Unlike most existing methods for zero-shot learning that represent\nclasses as fixed embeddings in some vector space, our generative model\nnaturally represents each class as a probability distribution. It is simple to\nimplement and also allows leveraging additional unlabeled data from unseen\nclasses to improve the estimates of their class-conditional distributions using\ntransductive/semi-supervised learning. Moreover, it extends seamlessly to\nfew-shot learning by easily updating these distributions when provided with a\nsmall number of additional labelled examples from unseen classes. Through a\ncomprehensive set of experiments on several benchmark data sets, we demonstrate\nthe efficacy of our framework.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 15:28:22 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 06:50:51 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 05:37:04 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Verma", "Vinay Kumar", ""], ["Rai", "Piyush", ""]]}, {"id": "1707.08101", "submitter": "Andreas Eitel", "authors": "Andreas Eitel, Nico Hauff and Wolfram Burgard", "title": "Learning to Singulate Objects using a Push Proposal Network", "comments": "International Symposium on Robotics Research (ISRR) 2017, videos:\n  http://robotpush.cs.uni-freiburg.de", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to act in unstructured environments, such as cluttered piles of\nobjects, poses a substantial challenge for manipulation robots. We present a\nnovel neural network-based approach that separates unknown objects in clutter\nby selecting favourable push actions. Our network is trained from data\ncollected through autonomous interaction of a PR2 robot with randomly organized\ntabletop scenes. The model is designed to propose meaningful push actions based\non over-segmented RGB-D images. We evaluate our approach by singulating up to 8\nunknown objects in clutter. We demonstrate that our method enables the robot to\nperform the task with a high success rate and a low number of required push\nactions. Our results based on real-world experiments show that our network is\nable to generalize to novel objects of various sizes and shapes, as well as to\narbitrary object configurations. Videos of our experiments can be viewed at\nhttp://robotpush.cs.uni-freiburg.de\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 17:36:36 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 18:42:35 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Eitel", "Andreas", ""], ["Hauff", "Nico", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1707.08110", "submitter": "Amir Ghaderi", "authors": "Amir Ghaderi, Borhan M. Sanandaji, Faezeh Ghaderi", "title": "Deep Forecast: Deep Learning-based Spatio-Temporal Forecasting", "comments": "Accepted to the ICML 2017, Time Series Workshop. arXiv admin note:\n  text overlap with arXiv:1503.01210", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a spatio-temporal wind speed forecasting algorithm using\nDeep Learning (DL)and in particular, Recurrent Neural Networks(RNNs). Motivated\nby recent advances in renewable energy integration and smart grids, we apply\nour proposed algorithm for wind speed forecasting. Renewable energy resources\n(wind and solar)are random in nature and, thus, their integration is\nfacilitated with accurate short-term forecasts. In our proposed framework, we\nmodel the spatiotemporal information by a graph whose nodes are data generating\nentities and its edges basically model how these nodes are interacting with\neach other. One of the main contributions of our work is the fact that we\nobtain forecasts of all nodes of the graph at the same time based on one\nframework. Results of a case study on recorded time series data from a\ncollection of wind mills in the north-east of the U.S. show that the proposed\nDL-based forecasting algorithm significantly improves the short-term forecasts\ncompared to a set of widely-used benchmarks models.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 18:56:33 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Ghaderi", "Amir", ""], ["Sanandaji", "Borhan M.", ""], ["Ghaderi", "Faezeh", ""]]}, {"id": "1707.08114", "submitter": "Yu Zhang", "authors": "Yu Zhang and Qiang Yang", "title": "A Survey on Multi-Task Learning", "comments": "Accepted by IEEE Transactions on Knowledge and Data Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Task Learning (MTL) is a learning paradigm in machine learning and its\naim is to leverage useful information contained in multiple related tasks to\nhelp improve the generalization performance of all the tasks. In this paper, we\ngive a survey for MTL from the perspective of algorithmic modeling,\napplications and theoretical analyses. For algorithmic modeling, we give a\ndefinition of MTL and then classify different MTL algorithms into five\ncategories, including feature learning approach, low-rank approach, task\nclustering approach, task relation learning approach and decomposition approach\nas well as discussing the characteristics of each approach. In order to improve\nthe performance of learning tasks further, MTL can be combined with other\nlearning paradigms including semi-supervised learning, active learning,\nunsupervised learning, reinforcement learning, multi-view learning and\ngraphical models. When the number of tasks is large or the data dimensionality\nis high, we review online, parallel and distributed MTL models as well as\ndimensionality reduction and feature hashing to reveal their computational and\nstorage advantages. Many real-world applications use MTL to boost their\nperformance and we review representative works in this paper. Finally, we\npresent theoretical analyses and discuss several future directions for MTL.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 04:43:47 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 03:17:17 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 14:32:58 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Yu", ""], ["Yang", "Qiang", ""]]}, {"id": "1707.08120", "submitter": "Piotr Mardziel", "authors": "Anupam Datta, Matt Fredrikson, Gihyuk Ko, Piotr Mardziel, Shayak Sen", "title": "Proxy Non-Discrimination in Data-Driven Systems", "comments": "arXiv admin note: substantial text overlap with arXiv:1705.07807", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learnt systems inherit biases against protected classes, historically\ndisparaged groups, from training data. Usually, these biases are not explicit,\nthey rely on subtle correlations discovered by training algorithms, and are\ntherefore difficult to detect. We formalize proxy discrimination in data-driven\nsystems, a class of properties indicative of bias, as the presence of protected\nclass correlates that have causal influence on the system's output. We evaluate\nan implementation on a corpus of social datasets, demonstrating how to validate\nsystems against these properties and to repair violations where they occur.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 21:16:24 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Datta", "Anupam", ""], ["Fredrikson", "Matt", ""], ["Ko", "Gihyuk", ""], ["Mardziel", "Piotr", ""], ["Sen", "Shayak", ""]]}, {"id": "1707.08167", "submitter": "El Mahdi El Mhamdi", "authors": "El Mahdi El Mhamdi, Rachid Guerraoui, Sebastien Rouault", "title": "On The Robustness of a Neural Network", "comments": "36th IEEE International Symposium on Reliable Distributed Systems 26\n  - 29 September 2017. Hong Kong, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of neural networks based machine learning and their\nusage in mission critical applications, voices are rising against the\n\\textit{black box} aspect of neural networks as it becomes crucial to\nunderstand their limits and capabilities. With the rise of neuromorphic\nhardware, it is even more critical to understand how a neural network, as a\ndistributed system, tolerates the failures of its computing nodes, neurons, and\nits communication channels, synapses. Experimentally assessing the robustness\nof neural networks involves the quixotic venture of testing all the possible\nfailures, on all the possible inputs, which ultimately hits a combinatorial\nexplosion for the first, and the impossibility to gather all the possible\ninputs for the second.\n  In this paper, we prove an upper bound on the expected error of the output\nwhen a subset of neurons crashes. This bound involves dependencies on the\nnetwork parameters that can be seen as being too pessimistic in the average\ncase. It involves a polynomial dependency on the Lipschitz coefficient of the\nneurons activation function, and an exponential dependency on the depth of the\nlayer where a failure occurs. We back up our theoretical results with\nexperiments illustrating the extent to which our prediction matches the\ndependencies between the network parameters and robustness. Our results show\nthat the robustness of neural networks to the average crash can be estimated\nwithout the need to neither test the network on all failure configurations, nor\naccess the training set used to train the network, both of which are\npractically impossible requirements.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 19:22:55 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 16:18:24 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Mhamdi", "El Mahdi El", ""], ["Guerraoui", "Rachid", ""], ["Rouault", "Sebastien", ""]]}, {"id": "1707.08184", "submitter": "Vaneet Aggarwal", "authors": "Wenqi Wang and Vaneet Aggarwal and Shuchin Aeron", "title": "Efficient Low Rank Tensor Ring Completion", "comments": "in Proc. ICCV, Oct. 2017. arXiv admin note: text overlap with\n  arXiv:1609.05587", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the matrix product state (MPS) representation of the recently proposed\ntensor ring decompositions, in this paper we propose a tensor completion\nalgorithm, which is an alternating minimization algorithm that alternates over\nthe factors in the MPS representation. This development is motivated in part by\nthe success of matrix completion algorithms that alternate over the (low-rank)\nfactors. In this paper, we propose a spectral initialization for the tensor\nring completion algorithm and analyze the computational complexity of the\nproposed algorithm. We numerically compare it with existing methods that employ\na low rank tensor train approximation for data completion and show that our\nmethod outperforms the existing ones for a variety of real computer vision\nsettings, and thus demonstrate the improved expressive power of tensor ring as\ncompared to tensor train.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 06:46:08 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Wang", "Wenqi", ""], ["Aggarwal", "Vaneet", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1707.08214", "submitter": "Fr\\'ederic Godin", "authors": "Fr\\'ederic Godin, Jonas Degrave, Joni Dambre, Wesley De Neve", "title": "Dual Rectified Linear Units (DReLUs): A Replacement for Tanh Activation\n  Functions in Quasi-Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.patrec.2018.09.006", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel type of Rectified Linear Unit (ReLU),\ncalled a Dual Rectified Linear Unit (DReLU). A DReLU, which comes with an\nunbounded positive and negative image, can be used as a drop-in replacement for\na tanh activation function in the recurrent step of Quasi-Recurrent Neural\nNetworks (QRNNs) (Bradbury et al. (2017)). Similar to ReLUs, DReLUs are less\nprone to the vanishing gradient problem, they are noise robust, and they induce\nsparse activations.\n  We independently reproduce the QRNN experiments of Bradbury et al. (2017) and\ncompare our DReLU-based QRNNs with the original tanh-based QRNNs and Long\nShort-Term Memory networks (LSTMs) on sentiment classification and word-level\nlanguage modeling. Additionally, we evaluate on character-level language\nmodeling, showing that we are able to stack up to eight QRNN layers with\nDReLUs, thus making it possible to improve the current state-of-the-art in\ncharacter-level language modeling over shallow architectures based on LSTMs.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 20:52:32 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 15:50:57 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Godin", "Fr\u00e9deric", ""], ["Degrave", "Jonas", ""], ["Dambre", "Joni", ""], ["De Neve", "Wesley", ""]]}, {"id": "1707.08262", "submitter": "Siddharth Biswal", "authors": "Siddharth Biswal, Joshua Kulas, Haoqi Sun, Balaji Goparaju, M Brandon\n  Westover, Matt T Bianchi, Jimeng Sun", "title": "SLEEPNET: Automated Sleep Staging System via Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep disorders, such as sleep apnea, parasomnias, and hypersomnia, affect\n50-70 million adults in the United States (Hillman et al., 2006). Overnight\npolysomnography (PSG), including brain monitoring using electroencephalography\n(EEG), is a central component of the diagnostic evaluation for sleep disorders.\nWhile PSG is conventionally performed by trained technologists, the recent rise\nof powerful neural network learning algorithms combined with large\nphysiological datasets offers the possibility of automation, potentially making\nexpert-level sleep analysis more widely available. We propose SLEEPNET (Sleep\nEEG neural network), a deployed annotation tool for sleep staging. SLEEPNET\nuses a deep recurrent neural network trained on the largest sleep physiology\ndatabase assembled to date, consisting of PSGs from over 10,000 patients from\nthe Massachusetts General Hospital (MGH) Sleep Laboratory. SLEEPNET achieves\nhuman-level annotation performance on an independent test set of 1,000 EEGs,\nwith an average accuracy of 85.76% and algorithm-expert inter-rater agreement\n(IRA) of kappa = 79.46%, comparable to expert-expert IRA.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 00:39:59 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Biswal", "Siddharth", ""], ["Kulas", "Joshua", ""], ["Sun", "Haoqi", ""], ["Goparaju", "Balaji", ""], ["Westover", "M Brandon", ""], ["Bianchi", "Matt T", ""], ["Sun", "Jimeng", ""]]}, {"id": "1707.08265", "submitter": "Ting Pan", "authors": "Ting Pan", "title": "Dragon: A Computation Graph Virtual Machine Based Deep Learning\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.MS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has made a great progress for these years. However, it is still\ndifficult to master the implement of various models because different\nresearchers may release their code based on different frameworks or interfaces.\nIn this paper, we proposed a computation graph based framework which only aims\nto introduce well-known interfaces. It will help a lot when reproducing a newly\nmodel or transplanting models that were implemented by other frameworks.\nAdditionally, we implement numerous recent models covering both Computer Vision\nand Nature Language Processing. We demonstrate that our framework will not\nsuffer from model-starving because it is much easier to make full use of the\nworks that are already done.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 01:16:29 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Pan", "Ting", ""]]}, {"id": "1707.08273", "submitter": "Noseong Park", "authors": "Noseong Park, Ankesh Anand, Joel Ruben Antony Moniz, Kookjin Lee,\n  Tanmoy Chakraborty, Jaegul Choo, Hongkyu Park, Youngmin Kim", "title": "MMGAN: Manifold Matching Generative Adversarial Network", "comments": "the 24th International Conference on Pattern Recognition (ICPR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that GANs are difficult to train, and several different\ntechniques have been proposed in order to stabilize their training. In this\npaper, we propose a novel training method called manifold-matching, and a new\nGAN model called manifold-matching GAN (MMGAN). MMGAN finds two manifolds\nrepresenting the vector representations of real and fake images. If these two\nmanifolds match, it means that real and fake images are statistically\nidentical. To assist the manifold-matching task, we also use i) kernel tricks\nto find better manifold structures, ii) moving-averaged manifolds across\nmini-batches, and iii) a regularizer based on correlation matrix to suppress\nmode collapse.\n  We conduct in-depth experiments with three image datasets and compare with\nseveral state-of-the-art GAN models. 32.4% of images generated by the proposed\nMMGAN are recognized as fake images during our user study (16% enhancement\ncompared to other state-of-the-art model). MMGAN achieved an unsupervised\ninception score of 7.8 for CIFAR-10.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 02:09:34 GMT"}, {"version": "v2", "created": "Sun, 30 Jul 2017 06:29:16 GMT"}, {"version": "v3", "created": "Thu, 21 Sep 2017 18:31:19 GMT"}, {"version": "v4", "created": "Thu, 12 Apr 2018 06:46:15 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Park", "Noseong", ""], ["Anand", "Ankesh", ""], ["Moniz", "Joel Ruben Antony", ""], ["Lee", "Kookjin", ""], ["Chakraborty", "Tanmoy", ""], ["Choo", "Jaegul", ""], ["Park", "Hongkyu", ""], ["Kim", "Youngmin", ""]]}, {"id": "1707.08287", "submitter": "Kevin Xu", "authors": "Yuning Zhang, Maysam Haghdan, and Kevin S. Xu", "title": "Unsupervised Motion Artifact Detection in Wrist-Measured Electrodermal\n  Activity Data", "comments": "To appear at International Symposium on Wearable Computers (ISWC)\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main benefits of a wrist-worn computer is its ability to collect a\nvariety of physiological data in a minimally intrusive manner. Among these\ndata, electrodermal activity (EDA) is readily collected and provides a window\ninto a person's emotional and sympathetic responses. EDA data collected using a\nwearable wristband are easily influenced by motion artifacts (MAs) that may\nsignificantly distort the data and degrade the quality of analyses performed on\nthe data if not identified and removed. Prior work has demonstrated that MAs\ncan be successfully detected using supervised machine learning algorithms on a\nsmall data set collected in a lab setting. In this paper, we demonstrate that\nunsupervised learning algorithms perform competitively with supervised\nalgorithms for detecting MAs on EDA data collected in both a lab-based setting\nand a real-world setting comprising about 23 hours of data. We also find,\nsomewhat surprisingly, that incorporating accelerometer data as well as EDA\nimproves detection accuracy only slightly for supervised algorithms and\nsignificantly degrades the accuracy of unsupervised algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 05:02:45 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Zhang", "Yuning", ""], ["Haghdan", "Maysam", ""], ["Xu", "Kevin S.", ""]]}, {"id": "1707.08301", "submitter": "Renata Khasanova", "authors": "Renata Khasanova and Pascal Frossard", "title": "Graph-Based Classification of Omnidirectional Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omnidirectional cameras are widely used in such areas as robotics and virtual\nreality as they provide a wide field of view. Their images are often processed\nwith classical methods, which might unfortunately lead to non-optimal solutions\nas these methods are designed for planar images that have different geometrical\nproperties than omnidirectional ones. In this paper we study image\nclassification task by taking into account the specific geometry of\nomnidirectional cameras with graph-based representations. In particular, we\nextend deep learning architectures to data on graphs; we propose a principled\nway of graph construction such that convolutional filters respond similarly for\nthe same pattern on different positions of the image regardless of lens\ndistortions. Our experiments show that the proposed method outperforms current\ntechniques for the omnidirectional image classification problem.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 06:39:45 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Khasanova", "Renata", ""], ["Frossard", "Pascal", ""]]}, {"id": "1707.08308", "submitter": "Jean Kossaifi", "authors": "Jean Kossaifi, Zachary C. Lipton, Arinbjorn Kolbeinsson, Aran Khanna,\n  Tommaso Furlanello, Anima Anandkumar", "title": "Tensor Regression Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks typically consist of many convolutional layers\nfollowed by one or more fully connected layers. While convolutional layers map\nbetween high-order activation tensors, the fully connected layers operate on\nflattened activation vectors. Despite empirical success, this approach has\nnotable drawbacks. Flattening followed by fully connected layers discards\nmultilinear structure in the activations and requires many parameters. We\naddress these problems by incorporating tensor algebraic operations that\npreserve multilinear structure at every layer. First, we introduce Tensor\nContraction Layers (TCLs) that reduce the dimensionality of their input while\npreserving their multilinear structure using tensor contraction. Next, we\nintroduce Tensor Regression Layers (TRLs), which express outputs through a\nlow-rank multilinear mapping from a high-order activation tensor to an output\ntensor of arbitrary order. We learn the contraction and regression factors\nend-to-end, and produce accurate nets with fewer parameters. Additionally, our\nlayers regularize networks by imposing low-rank constraints on the activations\n(TCL) and regression weights (TRL). Experiments on ImageNet show that, applied\nto VGG and ResNet architectures, TCLs and TRLs reduce the number of parameters\ncompared to fully connected layers by more than 65% while maintaining or\nincreasing accuracy. In addition to the space savings, our approach's ability\nto leverage topological structure can be crucial for structured data such as\nMRI. In particular, we demonstrate significant performance improvements over\ncomparable architectures on three tasks associated with the UK Biobank dataset.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 07:37:57 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 16:40:06 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 17:17:27 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2020 22:11:36 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Kossaifi", "Jean", ""], ["Lipton", "Zachary C.", ""], ["Kolbeinsson", "Arinbjorn", ""], ["Khanna", "Aran", ""], ["Furlanello", "Tommaso", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1707.08316", "submitter": "Raksha Kumaraswamy", "authors": "Lei Le, Raksha Kumaraswamy, Martha White", "title": "Learning Sparse Representations in Reinforcement Learning with Sparse\n  Coding", "comments": "6(+1) pages, 2 figures, International Joint Conference on Artificial\n  Intelligence 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of representation learning approaches have been investigated for\nreinforcement learning; much less attention, however, has been given to\ninvestigating the utility of sparse coding. Outside of reinforcement learning,\nsparse coding representations have been widely used, with non-convex objectives\nthat result in discriminative representations. In this work, we develop a\nsupervised sparse coding objective for policy evaluation. Despite the\nnon-convexity of this objective, we prove that all local minima are global\nminima, making the approach amenable to simple optimization strategies. We\nempirically show that it is key to use a supervised objective, rather than the\nmore straightforward unsupervised sparse coding approach. We compare the\nlearned representations to a canonical fixed sparse representation, called\ntile-coding, demonstrating that the sparse coding representation outperforms a\nwide variety of tilecoding representations.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 08:23:04 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Le", "Lei", ""], ["Kumaraswamy", "Raksha", ""], ["White", "Martha", ""]]}, {"id": "1707.08325", "submitter": "Qing-Yuan Jiang", "authors": "Qing-Yuan Jiang, Wu-Jun Li", "title": "Asymmetric Deep Supervised Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has been widely used for large-scale approximate nearest neighbor\nsearch because of its storage and search efficiency. Recent work has found that\ndeep supervised hashing can significantly outperform non-deep supervised\nhashing in many applications. However, most existing deep supervised hashing\nmethods adopt a symmetric strategy to learn one deep hash function for both\nquery points and database (retrieval) points. The training of these symmetric\ndeep supervised hashing methods is typically time-consuming, which makes them\nhard to effectively utilize the supervised information for cases with\nlarge-scale database. In this paper, we propose a novel deep supervised hashing\nmethod, called asymmetric deep supervised hashing (ADSH), for large-scale\nnearest neighbor search. ADSH treats the query points and database points in an\nasymmetric way. More specifically, ADSH learns a deep hash function only for\nquery points, while the hash codes for database points are directly learned.\nThe training of ADSH is much more efficient than that of traditional symmetric\ndeep supervised hashing methods. Experiments show that ADSH can achieve\nstate-of-the-art performance in real applications.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 09:07:48 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Jiang", "Qing-Yuan", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1707.08352", "submitter": "Isabel Valera", "authors": "Isabel Valera, Melanie F. Pradier and Zoubin Ghahramani", "title": "General Latent Feature Modeling for Data Exploration Tasks", "comments": "presented at 2017 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2017), Sydney, NSW, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a general Bayesian non- parametric latent feature model\nsuitable to per- form automatic exploratory analysis of heterogeneous datasets,\nwhere the attributes describing each object can be either discrete, continuous\nor mixed variables. The proposed model presents several important properties.\nFirst, it accounts for heterogeneous data while can be inferred in linear time\nwith respect to the number of objects and attributes. Second, its Bayesian\nnonparametric nature allows us to automatically infer the model complexity from\nthe data, i.e., the number of features necessary to capture the latent\nstructure in the data. Third, the latent features in the model are\nbinary-valued variables, easing the interpretability of the obtained latent\nfeatures in data exploration tasks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 10:07:52 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Valera", "Isabel", ""], ["Pradier", "Melanie F.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1707.08369", "submitter": "Ratnik Gandhi", "authors": "Ratnik Gandhi and Amoli Rajgor", "title": "Updating Singular Value Decomposition for Rank One Matrix Perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient Singular Value Decomposition (SVD) algorithm is an important\ntool for distributed and streaming computation in big data problems. It is\nobserved that update of singular vectors of a rank-1 perturbed matrix is\nsimilar to a Cauchy matrix-vector product. With this observation, in this\npaper, we present an efficient method for updating Singular Value Decomposition\nof rank-1 perturbed matrix in $O(n^2 \\ \\text{log}(\\frac{1}{\\epsilon}))$ time.\nThe method uses Fast Multipole Method (FMM) for updating singular vectors in\n$O(n \\ \\text{log} (\\frac{1}{\\epsilon}))$ time, where $\\epsilon$ is the\nprecision of computation.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 10:51:22 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Gandhi", "Ratnik", ""], ["Rajgor", "Amoli", ""]]}, {"id": "1707.08381", "submitter": "Junqiu Wu", "authors": "Ke Liu (1), Xiangyan Sun (3), Jun Ma (3), Zhenyu Zhou (3), Qilin Dong\n  (4), Shengwen Peng (3), Junqiu Wu (3), Suocheng Tan (3), G\\\"unter Blobel (2),\n  and Jie Fan (1) ((1) Accutar Biotechnology, (2) Laboratory of Cell Biology,\n  Howard Hughes Medical Institute, The Rockefeller University (3) Accutar\n  Biotechnology (Shanghai), (4) Fudan University)", "title": "Prediction of amino acid side chain conformation using a deep neural\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep neural network based architecture was constructed to predict amino\nacid side chain conformation with unprecedented accuracy. Amino acid side chain\nconformation prediction is essential for protein homology modeling and protein\ndesign. Current widely-adopted methods use physics-based energy functions to\nevaluate side chain conformation. Here, using a deep neural network\narchitecture without physics-based assumptions, we have demonstrated that side\nchain conformation prediction accuracy can be improved by more than 25%,\nespecially for aromatic residues compared with current standard methods. More\nstrikingly, the prediction method presented here is robust enough to identify\nindividual conformational outliers from high resolution structures in a protein\ndata bank without providing its structural factors. We envisage that our amino\nacid side chain predictor could be used as a quality check step for future\nprotein structure model validation and many other potential applications such\nas side chain assignment in Cryo-electron microscopy, crystallography model\nauto-building, protein folding and small molecule ligand docking.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 11:22:57 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Liu", "Ke", ""], ["Sun", "Xiangyan", ""], ["Ma", "Jun", ""], ["Zhou", "Zhenyu", ""], ["Dong", "Qilin", ""], ["Peng", "Shengwen", ""], ["Wu", "Junqiu", ""], ["Tan", "Suocheng", ""], ["Blobel", "G\u00fcnter", ""], ["Fan", "Jie", ""]]}, {"id": "1707.08423", "submitter": "Yonatan Mintz", "authors": "Yonatan Mintz, Anil Aswani, Philip Kaminsky, Elena Flowers, Yoshimi\n  Fukuoka", "title": "Non-Stationary Bandits with Habituation and Recovery Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many settings involve sequential decision-making where a set of actions can\nbe chosen at each time step, each action provides a stochastic reward, and the\ndistribution for the reward of each action is initially unknown. However,\nfrequent selection of a specific action may reduce its expected reward, while\nabstaining from choosing an action may cause its expected reward to increase.\nSuch non-stationary phenomena are observed in many real world settings such as\npersonalized healthcare-adherence improving interventions and targeted online\nadvertising. Though finding an optimal policy for general models with\nnon-stationarity is PSPACE-complete, we propose and analyze a new class of\nmodels called ROGUE (Reducing or Gaining Unknown Efficacy) bandits, which we\nshow in this paper can capture these phenomena and are amenable to the design\nof effective policies. We first present a consistent maximum likelihood\nestimator for the parameters of these models. Next, we construct finite sample\nconcentration bounds that lead to an upper confidence bound policy called the\nROGUE Upper Confidence Bound (ROGUE-UCB) algorithm. We prove that under proper\nconditions the ROGUE-UCB algorithm achieves logarithmic in time regret, unlike\nexisting algorithms which result in linear regret. We conclude with a numerical\nexperiment using real data from a personalized healthcare-adherence improving\nintervention to increase physical activity. In this intervention, the goal is\nto optimize the selection of messages (e.g., confidence increasing vs.\nknowledge increasing) to send to each individual each day to increase adherence\nand physical activity. Our results show that ROGUE-UCB performs better in terms\nof regret and average reward as compared to state of the art algorithms, and\nthe use of ROGUE-UCB increases daily step counts by roughly 1,000 steps a day\n(about a half-mile more of walking) as compared to other algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 13:14:40 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 17:49:43 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2019 08:10:10 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Mintz", "Yonatan", ""], ["Aswani", "Anil", ""], ["Kaminsky", "Philip", ""], ["Flowers", "Elena", ""], ["Fukuoka", "Yoshimi", ""]]}, {"id": "1707.08475", "submitter": "Irina Higgins", "authors": "Irina Higgins, Arka Pal, Andrei A. Rusu, Loic Matthey, Christopher P\n  Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, Alexander\n  Lerchner", "title": "DARLA: Improving Zero-Shot Transfer in Reinforcement Learning", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is an important open problem in deep reinforcement learning\n(RL). In many scenarios of interest data is hard to obtain, so agents may learn\na source policy in a setting where data is readily available, with the hope\nthat it generalises well to the target domain. We propose a new multi-stage RL\nagent, DARLA (DisentAngled Representation Learning Agent), which learns to see\nbefore learning to act. DARLA's vision is based on learning a disentangled\nrepresentation of the observed environment. Once DARLA can see, it is able to\nacquire source policies that are robust to many domain shifts - even with no\naccess to the target domain. DARLA significantly outperforms conventional\nbaselines in zero-shot domain adaptation scenarios, an effect that holds across\na variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms\n(DQN, A3C and EC).\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 14:50:51 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 16:51:02 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Higgins", "Irina", ""], ["Pal", "Arka", ""], ["Rusu", "Andrei A.", ""], ["Matthey", "Loic", ""], ["Burgess", "Christopher P", ""], ["Pritzel", "Alexander", ""], ["Botvinick", "Matthew", ""], ["Blundell", "Charles", ""], ["Lerchner", "Alexander", ""]]}, {"id": "1707.08551", "submitter": "Hao Dong", "authors": "Hao Dong, Akara Supratak, Luo Mai, Fangde Liu, Axel Oehmichen, Simiao\n  Yu, Yike Guo", "title": "TensorLayer: A Versatile Library for Efficient Deep Learning Development", "comments": "ACM Multimedia 2017", "journal-ref": null, "doi": "10.1145/3123266.3129391", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has enabled major advances in the fields of computer vision,\nnatural language processing, and multimedia among many others. Developing a\ndeep learning system is arduous and complex, as it involves constructing neural\nnetwork architectures, managing training/trained models, tuning optimization\nprocess, preprocessing and organizing data, etc. TensorLayer is a versatile\nPython library that aims at helping researchers and engineers efficiently\ndevelop deep learning systems. It offers rich abstractions for neural networks,\nmodel and data management, and parallel workflow mechanism. While boosting\nefficiency, TensorLayer maintains both performance and scalability. TensorLayer\nwas released in September 2016 on GitHub, and has helped people from academia\nand industry develop real-world applications of deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 17:29:49 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 10:26:34 GMT"}, {"version": "v3", "created": "Thu, 3 Aug 2017 14:48:16 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Dong", "Hao", ""], ["Supratak", "Akara", ""], ["Mai", "Luo", ""], ["Liu", "Fangde", ""], ["Oehmichen", "Axel", ""], ["Yu", "Simiao", ""], ["Guo", "Yike", ""]]}, {"id": "1707.08552", "submitter": "Albert Berahas", "authors": "Albert S. Berahas and Martin Tak\\'a\\v{c}", "title": "A Robust Multi-Batch L-BFGS Method for Machine Learning", "comments": "50 pages, 33 figures. Extension of NIPS 2016 paper: arXiv:1605.06049", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an implementation of the L-BFGS method designed to deal\nwith two adversarial situations. The first occurs in distributed computing\nenvironments where some of the computational nodes devoted to the evaluation of\nthe function and gradient are unable to return results on time. A similar\nchallenge occurs in a multi-batch approach in which the data points used to\ncompute function and gradients are purposely changed at each iteration to\naccelerate the learning process. Difficulties arise because L-BFGS employs\ngradient differences to update the Hessian approximations, and when these\ngradients are computed using different data points the updating process can be\nunstable. This paper shows how to perform stable quasi-Newton updating in the\nmulti-batch setting, studies the convergence properties for both convex and\nnonconvex functions, and illustrates the behavior of the algorithm in a\ndistributed computing platform on binary classification logistic regression and\nneural network training problems that arise in machine learning.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 17:33:43 GMT"}, {"version": "v2", "created": "Sun, 31 Mar 2019 22:30:25 GMT"}, {"version": "v3", "created": "Tue, 27 Aug 2019 17:18:19 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Berahas", "Albert S.", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1707.08553", "submitter": "Frederik Ruelens", "authors": "Frederik Ruelens, Bert J. Claessens, Peter Vrancx, Fred Spiessens, and\n  Geert Deconinck", "title": "Direct Load Control of Thermostatically Controlled Loads Based on Sparse\n  Observations Using Deep Reinforcement Learning", "comments": "submitted and waiting review in IEEE transactions on smart grid 2017", "journal-ref": "CSEE Journal of Power and Energy Systems, Vol. 5, Iss. 4, Dec.\n  2019, pp. 423-432", "doi": "10.17775/CSEEJPES.2019.00590", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a demand response agent that must find a near-optimal\nsequence of decisions based on sparse observations of its environment.\nExtracting a relevant set of features from these observations is a challenging\ntask and may require substantial domain knowledge. One way to tackle this\nproblem is to store sequences of past observations and actions in the state\nvector, making it high dimensional, and apply techniques from deep learning.\nThis paper investigates the capabilities of different deep learning techniques,\nsuch as convolutional neural networks and recurrent neural networks, to extract\nrelevant features for finding near-optimal policies for a residential heating\nsystem and electric water heater that are hindered by sparse observations. Our\nsimulation results indicate that in this specific scenario, feeding sequences\nof time-series to an LSTM network, which is a specific type of recurrent neural\nnetwork, achieved a higher performance than stacking these time-series in the\ninput of a convolutional neural network or deep neural network.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 17:33:46 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Ruelens", "Frederik", ""], ["Claessens", "Bert J.", ""], ["Vrancx", "Peter", ""], ["Spiessens", "Fred", ""], ["Deconinck", "Geert", ""]]}, {"id": "1707.08559", "submitter": "Cheng-Yang Fu", "authors": "Cheng-Yang Fu, Joon Lee, Mohit Bansal, Alexander C. Berg", "title": "Video Highlight Prediction Using Audience Chat Reactions", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sports channel video portals offer an exciting domain for research on\nmultimodal, multilingual analysis. We present methods addressing the problem of\nautomatic video highlight prediction based on joint visual features and textual\nanalysis of the real-world audience discourse with complex slang, in both\nEnglish and traditional Chinese. We present a novel dataset based on League of\nLegends championships recorded from North American and Taiwanese Twitch.tv\nchannels (will be released for further research), and demonstrate strong\nresults on these using multimodal, character-level CNN-RNN model architectures.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 17:44:38 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Fu", "Cheng-Yang", ""], ["Lee", "Joon", ""], ["Bansal", "Mohit", ""], ["Berg", "Alexander C.", ""]]}, {"id": "1707.08561", "submitter": "Andrea Rocchetto", "authors": "Carlo Ciliberto, Mark Herbster, Alessandro Davide Ialongo,\n  Massimiliano Pontil, Andrea Rocchetto, Simone Severini, Leonard Wossnig", "title": "Quantum machine learning: a classical perspective", "comments": "v3 33 pages; typos corrected and references added", "journal-ref": "Proc. R. Soc. A, vol. 474, no. 2209, p. 20170551. The Royal\n  Society, 2018", "doi": "10.1098/rspa.2017.0551", "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, increased computational power and data availability, as well as\nalgorithmic advances, have led machine learning techniques to impressive\nresults in regression, classification, data-generation and reinforcement\nlearning tasks. Despite these successes, the proximity to the physical limits\nof chip fabrication alongside the increasing size of datasets are motivating a\ngrowing number of researchers to explore the possibility of harnessing the\npower of quantum computation to speed-up classical machine learning algorithms.\nHere we review the literature in quantum machine learning and discuss\nperspectives for a mixed readership of classical machine learning and quantum\ncomputation experts. Particular emphasis will be placed on clarifying the\nlimitations of quantum algorithms, how they compare with their best classical\ncounterparts and why quantum resources are expected to provide advantages for\nlearning problems. Learning in the presence of noise and certain\ncomputationally hard problems in machine learning are identified as promising\ndirections for the field. Practical questions, like how to upload classical\ndata into quantum form, will also be addressed.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 17:48:25 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 15:48:01 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 19:20:03 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Ciliberto", "Carlo", ""], ["Herbster", "Mark", ""], ["Ialongo", "Alessandro Davide", ""], ["Pontil", "Massimiliano", ""], ["Rocchetto", "Andrea", ""], ["Severini", "Simone", ""], ["Wossnig", "Leonard", ""]]}, {"id": "1707.08569", "submitter": "Ramviyas Parasuraman", "authors": "Mohamed Abudulaziz Ali Haseeb and Ramviyas Parasuraman", "title": "Wisture: RNN-based Learning of Wireless Signals for Gesture Recognition\n  in Unmodified Smartphones", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Wisture, a new online machine learning solution for\nrecognizing touch-less dynamic hand gestures on a smartphone. Wisture relies on\nthe standard Wi-Fi Received Signal Strength (RSS) using a Long Short-Term\nMemory (LSTM) Recurrent Neural Network (RNN), thresholding filters and traffic\ninduction. Unlike other Wi-Fi based gesture recognition methods, the proposed\nmethod does not require a modification of the smartphone hardware or the\noperating system, and performs the gesture recognition without interfering with\nthe normal operation of other smartphone applications.\n  We discuss the characteristics of Wisture, and conduct extensive experiments\nto compare its performance against state-of-the-art machine learning solutions\nin terms of both accuracy and time efficiency. The experiments include a set of\ndifferent scenarios in terms of both spatial setup and traffic between the\nsmartphone and Wi-Fi access points (AP). The results show that Wisture achieves\nan online recognition accuracy of up to 94% (average 78%) in detecting and\nclassifying three hand gestures.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 17:15:15 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 18:55:44 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Haseeb", "Mohamed Abudulaziz Ali", ""], ["Parasuraman", "Ramviyas", ""]]}, {"id": "1707.08588", "submitter": "Yikang Shen", "authors": "Yikang Shen, Shawn Tan, Chrisopher Pal and Aaron Courville", "title": "Self-organized Hierarchical Softmax", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new self-organizing hierarchical softmax formulation for\nneural-network-based language models over large vocabularies. Instead of using\na predefined hierarchical structure, our approach is capable of learning word\nclusters with clear syntactical and semantic meaning during the language model\ntraining process. We provide experiments on standard benchmarks for language\nmodeling and sentence compression tasks. We find that this approach is as fast\nas other efficient softmax approximations, while achieving comparable or even\nbetter performance relative to similar full softmax models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 18:01:32 GMT"}], "update_date": "2017-07-29", "authors_parsed": [["Shen", "Yikang", ""], ["Tan", "Shawn", ""], ["Pal", "Chrisopher", ""], ["Courville", "Aaron", ""]]}, {"id": "1707.08616", "submitter": "Mark Riedl", "authors": "Brent Harrison, Upol Ehsan, Mark O. Riedl", "title": "Guiding Reinforcement Learning Exploration Using Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a technique to use natural language to help\nreinforcement learning generalize to unseen environments. This technique uses\nneural machine translation, specifically the use of encoder-decoder networks,\nto learn associations between natural language behavior descriptions and\nstate-action information. We then use this learned model to guide agent\nexploration using a modified version of policy shaping to make it more\neffective at learning in unseen environments. We evaluate this technique using\nthe popular arcade game, Frogger, under ideal and non-ideal conditions. This\nevaluation shows that our modified policy shaping algorithm improves over a\nQ-learning agent as well as a baseline version of policy shaping.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 19:23:54 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 02:06:26 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Harrison", "Brent", ""], ["Ehsan", "Upol", ""], ["Riedl", "Mark O.", ""]]}, {"id": "1707.08689", "submitter": "Mohamed K. Helwa", "authors": "Mohamed K. Helwa, Angela P. Schoellig", "title": "Multi-Robot Transfer Learning: A Dynamical System Perspective", "comments": "7 pages, 6 figures, accepted at the 2017 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-robot transfer learning allows a robot to use data generated by a\nsecond, similar robot to improve its own behavior. The potential advantages are\nreducing the time of training and the unavoidable risks that exist during the\ntraining phase. Transfer learning algorithms aim to find an optimal transfer\nmap between different robots. In this paper, we investigate, through a\ntheoretical study of single-input single-output (SISO) systems, the properties\nof such optimal transfer maps. We first show that the optimal transfer learning\nmap is, in general, a dynamic system. The main contribution of the paper is to\nprovide an algorithm for determining the properties of this optimal dynamic map\nincluding its order and regressors (i.e., the variables it depends on). The\nproposed algorithm does not require detailed knowledge of the robots' dynamics,\nbut relies on basic system properties easily obtainable through simple\nexperimental tests. We validate the proposed algorithm experimentally through\nan example of transfer learning between two different quadrotor platforms.\nExperimental results show that an optimal dynamic map, with correct properties\nobtained from our proposed algorithm, achieves 60-70% reduction of transfer\nlearning error compared to the cases when the data is directly transferred or\ntransferred using an optimal static map.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 02:48:19 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Helwa", "Mohamed K.", ""], ["Schoellig", "Angela P.", ""]]}, {"id": "1707.08729", "submitter": "Zixing Zhang", "authors": "Zixing Zhang, Ding Liu, Jing Han, Kun Qian, Bj\\\"orn Schuller", "title": "Learning audio sequence representations for acoustic event\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustic Event Classification (AEC) has become a significant task for\nmachines to perceive the surrounding auditory scene. However, extracting\neffective representations that capture the underlying characteristics of the\nacoustic events is still challenging. Previous methods mainly focused on\ndesigning the audio features in a `hand-crafted' manner. Interestingly,\ndata-learnt features have been recently reported to show better performance. Up\nto now, these were only considered on the frame level. In this article, we\npropose an unsupervised learning framework to learn a vector representation of\nan audio sequence for AEC. This framework consists of a Recurrent Neural\nNetwork (RNN) encoder and an RNN decoder, which respectively transforms the\nvariable-length audio sequence into a fixed-length vector and reconstructs the\ninput sequence on the generated vector. After training the encoder-decoder, we\nfeed the audio sequences to the encoder and then take the learnt vectors as the\naudio sequence representations. Compared with previous methods, the proposed\nmethod can not only deal with the problem of arbitrary-lengths of audio\nstreams, but also learn the salient information of the sequence. Extensive\nevaluation on a large-size acoustic event database is performed, and the\nempirical results demonstrate that the learnt audio sequence representation\nyields a significant performance improvement by a large margin compared with\nother state-of-the-art hand-crafted sequence features for AEC.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 07:26:38 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 10:13:12 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Zhang", "Zixing", ""], ["Liu", "Ding", ""], ["Han", "Jing", ""], ["Qian", "Kun", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "1707.08819", "submitter": "Patryk Chrabaszcz", "authors": "Patryk Chrabaszcz, Ilya Loshchilov and Frank Hutter", "title": "A Downsampled Variant of ImageNet as an Alternative to the CIFAR\n  datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original ImageNet dataset is a popular large-scale benchmark for training\nDeep Neural Networks. Since the cost of performing experiments (e.g, algorithm\ndesign, architecture search, and hyperparameter tuning) on the original dataset\nmight be prohibitive, we propose to consider a downsampled version of ImageNet.\nIn contrast to the CIFAR datasets and earlier downsampled versions of ImageNet,\nour proposed ImageNet32$\\times$32 (and its variants ImageNet64$\\times$64 and\nImageNet16$\\times$16) contains exactly the same number of classes and images as\nImageNet, with the only difference that the images are downsampled to\n32$\\times$32 pixels per image (64$\\times$64 and 16$\\times$16 pixels for the\nvariants, respectively). Experiments on these downsampled variants are\ndramatically faster than on the original ImageNet and the characteristics of\nthe downsampled datasets with respect to optimal hyperparameters appear to\nremain similar. The proposed datasets and scripts to reproduce our results are\navailable at http://image-net.org/download-images and\nhttps://github.com/PatrykChrabaszcz/Imagenet32_Scripts\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 11:22:22 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 10:36:32 GMT"}, {"version": "v3", "created": "Wed, 23 Aug 2017 16:06:20 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Chrabaszcz", "Patryk", ""], ["Loshchilov", "Ilya", ""], ["Hutter", "Frank", ""]]}, {"id": "1707.08820", "submitter": "Mastane Achab", "authors": "Mastane Achab, Stephan Cl\\'emen\\c{c}on, Aur\\'elien Garivier, Anne\n  Sabourin, Claire Vernade", "title": "Max K-armed bandit: On the ExtremeHunter algorithm and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the study of the max K-armed bandit problem, which\nconsists in sequentially allocating resources in order to detect extreme\nvalues. Our contribution is twofold. We first significantly refine the analysis\nof the ExtremeHunter algorithm carried out in Carpentier and Valko (2014), and\nnext propose an alternative approach, showing that, remarkably, Extreme Bandits\ncan be reduced to a classical version of the bandit problem to a certain\nextent. Beyond the formal analysis, these two approaches are compared through\nnumerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 11:26:55 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Achab", "Mastane", ""], ["Cl\u00e9men\u00e7on", "Stephan", ""], ["Garivier", "Aur\u00e9lien", ""], ["Sabourin", "Anne", ""], ["Vernade", "Claire", ""]]}, {"id": "1707.08852", "submitter": "Varun Gangal", "authors": "Dongyeop Kang, Varun Gangal, Ang Lu, Zheng Chen, Eduard Hovy", "title": "Detecting and Explaining Causes From Text For a Time Series Event", "comments": "Accepted at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining underlying causes or effects about events is a challenging but\nvaluable task. We define a novel problem of generating explanations of a time\nseries event by (1) searching cause and effect relationships of the time series\nwith textual data and (2) constructing a connecting chain between them to\ngenerate an explanation. To detect causal features from text, we propose a\nnovel method based on the Granger causality of time series between features\nextracted from text such as N-grams, topics, sentiments, and their composition.\nThe generation of the sequence of causal entities requires a commonsense\ncausative knowledge base with efficient reasoning. To ensure good\ninterpretability and appropriate lexical usage we combine symbolic and neural\nrepresentations, using a neural reasoning algorithm trained on commonsense\ncausal tuples to predict the next cause step. Our quantitative and human\nanalysis show empirical evidence that our method successfully extracts\nmeaningful causality relationships between time series with textual features\nand generates appropriate explanation between them.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 13:14:57 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Kang", "Dongyeop", ""], ["Gangal", "Varun", ""], ["Lu", "Ang", ""], ["Chen", "Zheng", ""], ["Hovy", "Eduard", ""]]}, {"id": "1707.08872", "submitter": "Pauli Miettinen", "authors": "Sanjar Karaev and Pauli Miettinen", "title": "Algorithms for Approximate Subtropical Matrix Factorization", "comments": "40 pages, 9 figures. For the associated source code, see\n  http://people.mpi-inf.mpg.de/~pmiettin/tropical/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization methods are important tools in data mining and analysis.\nThey can be used for many tasks, ranging from dimensionality reduction to\nvisualization. In this paper we concentrate on the use of matrix factorizations\nfor finding patterns from the data. Rather than using the standard algebra --\nand the summation of the rank-1 components to build the approximation of the\noriginal matrix -- we use the subtropical algebra, which is an algebra over the\nnonnegative real values with the summation replaced by the maximum operator.\nSubtropical matrix factorizations allow \"winner-takes-it-all\" interpretations\nof the rank-1 components, revealing different structure than the normal\n(nonnegative) factorizations. We study the complexity and sparsity of the\nfactorizations, and present a framework for finding low-rank subtropical\nfactorizations. We present two specific algorithms, called Capricorn and\nCancer, that are part of our framework. They can be used with data that has\nbeen corrupted with different types of noise, and with different error metrics,\nincluding the sum-of-absolute differences, Frobenius norm, and Jensen--Shannon\ndivergence. Our experiments show that the algorithms perform well on data that\nhas subtropical structure, and that they can find factorizations that are both\nsparse and easy to interpret.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 15:01:08 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Karaev", "Sanjar", ""], ["Miettinen", "Pauli", ""]]}, {"id": "1707.08945", "submitter": "Kevin Eykholt", "authors": "Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati,\n  Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, Dawn Song", "title": "Robust Physical-World Attacks on Deep Learning Models", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies show that the state-of-the-art deep neural networks (DNNs) are\nvulnerable to adversarial examples, resulting from small-magnitude\nperturbations added to the input. Given that that emerging physical systems are\nusing DNNs in safety-critical situations, adversarial examples could mislead\nthese systems and cause dangerous situations.Therefore, understanding\nadversarial examples in the physical world is an important step towards\ndeveloping resilient learning algorithms. We propose a general attack\nalgorithm,Robust Physical Perturbations (RP2), to generate robust visual\nadversarial perturbations under different physical conditions. Using the\nreal-world case of road sign classification, we show that adversarial examples\ngenerated using RP2 achieve high targeted misclassification rates against\nstandard-architecture road sign classifiers in the physical world under various\nenvironmental conditions, including viewpoints. Due to the current lack of a\nstandardized testing method, we propose a two-stage evaluation methodology for\nrobust physical adversarial examples consisting of lab and field tests. Using\nthis methodology, we evaluate the efficacy of physical adversarial\nmanipulations on real objects. Witha perturbation in the form of only black and\nwhite stickers,we attack a real stop sign, causing targeted misclassification\nin 100% of the images obtained in lab settings, and in 84.8%of the captured\nvideo frames obtained on a moving vehicle(field test) for the target\nclassifier.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 17:37:22 GMT"}, {"version": "v2", "created": "Sun, 30 Jul 2017 15:58:21 GMT"}, {"version": "v3", "created": "Mon, 7 Aug 2017 23:52:10 GMT"}, {"version": "v4", "created": "Wed, 13 Sep 2017 03:59:56 GMT"}, {"version": "v5", "created": "Tue, 10 Apr 2018 16:22:47 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Eykholt", "Kevin", ""], ["Evtimov", "Ivan", ""], ["Fernandes", "Earlence", ""], ["Li", "Bo", ""], ["Rahmati", "Amir", ""], ["Xiao", "Chaowei", ""], ["Prakash", "Atul", ""], ["Kohno", "Tadayoshi", ""], ["Song", "Dawn", ""]]}, {"id": "1707.09060", "submitter": "Tianyi Chen", "authors": "Tianyi Chen and Georgios B. Giannakis", "title": "Bandit Convex Optimization for Scalable and Dynamic IoT Management", "comments": null, "journal-ref": "IEEE Internet of Things Journal, 22 May 2018", "doi": "10.1109/JIOT.2018.2839563", "report-no": null, "categories": "cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper deals with online convex optimization involving both\ntime-varying loss functions, and time-varying constraints. The loss functions\nare not fully accessible to the learner, and instead only the function values\n(a.k.a. bandit feedback) are revealed at queried points. The constraints are\nrevealed after making decisions, and can be instantaneously violated, yet they\nmust be satisfied in the long term. This setting fits nicely the emerging\nonline network tasks such as fog computing in the Internet-of-Things (IoT),\nwhere online decisions must flexibly adapt to the changing user preferences\n(loss functions), and the temporally unpredictable availability of resources\n(constraints). Tailored for such human-in-the-loop systems where the loss\nfunctions are hard to model, a family of bandit online saddle-point (BanSaP)\nschemes are developed, which adaptively adjust the online operations based on\n(possibly multiple) bandit feedback of the loss functions, and the changing\nenvironment. Performance here is assessed by: i) dynamic regret that\ngeneralizes the widely used static regret; and, ii) fit that captures the\naccumulated amount of constraint violations. Specifically, BanSaP is proved to\nsimultaneously yield sub-linear dynamic regret and fit, provided that the best\ndynamic solutions vary slowly over time. Numerical tests in fog computation\noffloading tasks corroborate that our proposed BanSaP approach offers\ncompetitive performance relative to existing approaches that are based on\ngradient feedback.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 22:17:33 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Chen", "Tianyi", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1707.09062", "submitter": "Rika Antonova", "authors": "Rika Antonova, Akshara Rai, Christopher G. Atkeson", "title": "Deep Kernels for Optimizing Locomotion Controllers", "comments": "(Rika Antonova and Akshara Rai contributed equally)", "journal-ref": "PMLR 78:47-56 (2017)", "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample efficiency is important when optimizing parameters of locomotion\ncontrollers, since hardware experiments are time consuming and expensive.\nBayesian Optimization, a sample-efficient optimization framework, has recently\nbeen widely applied to address this problem, but further improvements in sample\nefficiency are needed for practical applicability to real-world robots and\nhigh-dimensional controllers. To address this, prior work has proposed using\ndomain expertise for constructing custom distance metrics for locomotion. In\nthis work we show how to learn such a distance metric automatically. We use a\nneural network to learn an informed distance metric from data obtained in\nhigh-fidelity simulations. We conduct experiments on two different controllers\nand robot architectures. First, we demonstrate improvement in sample efficiency\nwhen optimizing a 5-dimensional controller on the ATRIAS robot hardware. We\nthen conduct simulation experiments to optimize a 16-dimensional controller for\na 7-link robot model and obtain significant improvements even when optimizing\nin perturbed environments. This demonstrates that our approach is able to\nenhance sample efficiency for two different controllers, hence is a fitting\ncandidate for further experiments on hardware in the future.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 22:30:35 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 17:43:07 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Antonova", "Rika", ""], ["Rai", "Akshara", ""], ["Atkeson", "Christopher G.", ""]]}, {"id": "1707.09094", "submitter": "Conrad Sanderson", "authors": "Conrad Sanderson, Ryan Curtin", "title": "An Open Source C++ Implementation of Multi-Threaded Gaussian Mixture\n  Models, k-Means and Expectation Maximisation", "comments": null, "journal-ref": "International Conference on Signal Processing and Communication\n  Systems, 2017", "doi": "10.1109/ICSPCS.2017.8270510", "report-no": null, "categories": "cs.MS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling of multivariate densities is a core component in many signal\nprocessing, pattern recognition and machine learning applications. The\nmodelling is often done via Gaussian mixture models (GMMs), which use\ncomputationally expensive and potentially unstable training algorithms. We\nprovide an overview of a fast and robust implementation of GMMs in the C++\nlanguage, employing multi-threaded versions of the Expectation Maximisation\n(EM) and k-means training algorithms. Multi-threading is achieved through\nreformulation of the EM and k-means algorithms into a MapReduce-like framework.\nFurthermore, the implementation uses several techniques to improve numerical\nstability and modelling accuracy. We demonstrate that the multi-threaded\nimplementation achieves a speedup of an order of magnitude on a recent 16 core\nmachine, and that it can achieve higher modelling accuracy than a previously\nwell-established publically accessible implementation. The multi-threaded\nimplementation is included as a user-friendly class in recent releases of the\nopen source Armadillo C++ linear algebra library. The library is provided under\nthe permissive Apache~2.0 license, allowing unencumbered use in commercial\nproducts.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 03:15:22 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Sanderson", "Conrad", ""], ["Curtin", "Ryan", ""]]}, {"id": "1707.09118", "submitter": "Carolin Lawrence", "authors": "Carolin Lawrence, Artem Sokolov, Stefan Riezler", "title": "Counterfactual Learning from Bandit Feedback under Deterministic\n  Logging: A Case Study in Statistical Machine Translation", "comments": "Conference on Empirical Methods in Natural Language Processing\n  (EMNLP), 2017, Copenhagen, Denmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of counterfactual learning for statistical machine translation (SMT)\nis to optimize a target SMT system from logged data that consist of user\nfeedback to translations that were predicted by another, historic SMT system. A\nchallenge arises by the fact that risk-averse commercial SMT systems\ndeterministically log the most probable translation. The lack of sufficient\nexploration of the SMT output space seemingly contradicts the theoretical\nrequirements for counterfactual learning. We show that counterfactual learning\nfrom deterministic bandit logs is possible nevertheless by smoothing out\ndeterministic components in learning. This can be achieved by additive and\nmultiplicative control variates that avoid degenerate behavior in empirical\nrisk minimization. Our simulation experiments show improvements of up to 2 BLEU\npoints by counterfactual learning from deterministic bandit feedback.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 06:32:47 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 13:22:23 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 13:44:31 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Lawrence", "Carolin", ""], ["Sokolov", "Artem", ""], ["Riezler", "Stefan", ""]]}, {"id": "1707.09157", "submitter": "Francis Bach", "authors": "Francis Bach (SIERRA)", "title": "Efficient Algorithms for Non-convex Isotonic Regression through\n  Submodular Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimization of submodular functions subject to ordering\nconstraints. We show that this optimization problem can be cast as a convex\noptimization problem on a space of uni-dimensional measures, with ordering\nconstraints corresponding to first-order stochastic dominance. We propose new\ndiscretization schemes that lead to simple and efficient algorithms based on\nzero-th, first, or higher order oracles; these algorithms also lead to\nimprovements without isotonic constraints. Finally, our experiments show that\nnon-convex loss functions can be much more robust to outliers for isotonic\nregression, while still leading to an efficient optimization problem.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 09:02:52 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Bach", "Francis", "", "SIERRA"]]}, {"id": "1707.09183", "submitter": "Pablo Hernandez-Leal", "authors": "Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag and Enrique Munoz\n  de Cote", "title": "A Survey of Learning in Multiagent Environments: Dealing with\n  Non-Stationarity", "comments": "64 pages, 7 figures. Under review since November 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key challenge in multiagent learning is learning a best response to the\nbehaviour of other agents, which may be non-stationary: if the other agents\nadapt their strategy as well, the learning target moves. Disparate streams of\nresearch have approached non-stationarity from several angles, which make a\nvariety of implicit assumptions that make it hard to keep an overview of the\nstate of the art and to validate the innovation and significance of new works.\nThis survey presents a coherent overview of work that addresses\nopponent-induced non-stationarity with tools from game theory, reinforcement\nlearning and multi-armed bandits. Further, we reflect on the principle\napproaches how algorithms model and cope with this non-stationarity, arriving\nat a new framework and five categories (in increasing order of sophistication):\nignore, forget, respond to target models, learn models, and theory of mind. A\nwide range of state-of-the-art algorithms is classified into a taxonomy, using\nthese categories and key characteristics of the environment (e.g.,\nobservability) and adaptation behaviour of the opponents (e.g., smooth,\nabrupt). To clarify even further we present illustrative variations of one\ndomain, contrasting the strengths and limitations of each category. Finally, we\ndiscuss in which environments the different approaches yield most merit, and\npoint to promising avenues of future research.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 10:49:41 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 20:17:29 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Hernandez-Leal", "Pablo", ""], ["Kaisers", "Michael", ""], ["Baarslag", "Tim", ""], ["de Cote", "Enrique Munoz", ""]]}, {"id": "1707.09198", "submitter": "Fengqi You", "authors": "Chao Ning and Fengqi You", "title": "Data-Driven Stochastic Robust Optimization: A General Computational\n  Framework and Algorithm for Optimization under Uncertainty in the Big Data\n  Era", "comments": null, "journal-ref": "Computers & Chemical Engineering, Volume 111, Pages 115-133, 4\n  March 2018,", "doi": "10.1016/j.compchemeng.2017.12.015", "report-no": null, "categories": "cs.LG cs.AI cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel data-driven stochastic robust optimization (DDSRO) framework is\nproposed for optimization under uncertainty leveraging labeled multi-class\nuncertainty data. Uncertainty data in large datasets are often collected from\nvarious conditions, which are encoded by class labels. Machine learning methods\nincluding Dirichlet process mixture model and maximum likelihood estimation are\nemployed for uncertainty modeling. A DDSRO framework is further proposed based\non the data-driven uncertainty model through a bi-level optimization structure.\nThe outer optimization problem follows a two-stage stochastic programming\napproach to optimize the expected objective across different data classes;\nadaptive robust optimization is nested as the inner problem to ensure the\nrobustness of the solution while maintaining computational tractability. A\ndecomposition-based algorithm is further developed to solve the resulting\nmulti-level optimization problem efficiently. Case studies on process network\ndesign and planning are presented to demonstrate the applicability of the\nproposed framework and algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 11:43:33 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 21:47:10 GMT"}, {"version": "v3", "created": "Thu, 12 Oct 2017 10:26:54 GMT"}, {"version": "v4", "created": "Fri, 29 Dec 2017 14:15:04 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Ning", "Chao", ""], ["You", "Fengqi", ""]]}, {"id": "1707.09219", "submitter": "Isabeau Pr\\'emont-Schwarz", "authors": "Isabeau Pr\\'emont-Schwarz, Alexander Ilin, Tele Hotloo Hao, Antti\n  Rasmus, Rinu Boney, Harri Valpola", "title": "Recurrent Ladder Networks", "comments": "9 pages, 9 figures, 7-page appendix, fixed fig 9 (c)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a recurrent extension of the Ladder networks whose structure is\nmotivated by the inference required in hierarchical latent variable models. We\ndemonstrate that the recurrent Ladder is able to handle a wide variety of\ncomplex learning tasks that benefit from iterative inference and temporal\nmodeling. The architecture shows close-to-optimal results on temporal modeling\nof video data, competitive results on music modeling, and improved perceptual\ngrouping based on higher order abstractions, such as stochastic textures and\nmotion cues. We present results for fully supervised, semi-supervised, and\nunsupervised tasks. The results suggest that the proposed architecture and\nprinciples are powerful tools for learning a hierarchy of abstractions,\nlearning iterative inference and handling temporal information.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 13:19:11 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 15:14:19 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 13:43:12 GMT"}, {"version": "v4", "created": "Mon, 18 Dec 2017 06:43:47 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Pr\u00e9mont-Schwarz", "Isabeau", ""], ["Ilin", "Alexander", ""], ["Hao", "Tele Hotloo", ""], ["Rasmus", "Antti", ""], ["Boney", "Rinu", ""], ["Valpola", "Harri", ""]]}, {"id": "1707.09240", "submitter": "Sam Toyer", "authors": "Sam Toyer, Anoop Cherian, Tengda Han, Stephen Gould", "title": "Human Pose Forecasting via Deep Markov Models", "comments": "Accepted to DICTA'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose forecasting is an important problem in computer vision with\napplications to human-robot interaction, visual surveillance, and autonomous\ndriving. Usually, forecasting algorithms use 3D skeleton sequences and are\ntrained to forecast for a few milliseconds into the future. Long-range\nforecasting is challenging due to the difficulty of estimating how long a\nperson continues an activity. To this end, our contributions are threefold: (i)\nwe propose a generative framework for poses using variational autoencoders\nbased on Deep Markov Models (DMMs); (ii) we evaluate our pose forecasts using a\npose-based action classifier, which we argue better reflects the subjective\nquality of pose forecasts than distance in coordinate space; (iii) last, for\nevaluation of the new model, we introduce a 480,000-frame video dataset called\nIkea Furniture Assembly (Ikea FA), which depicts humans repeatedly assembling\nand disassembling furniture. We demonstrate promising results for our approach\non both Ikea FA and the existing NTU RGB+D dataset.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 23:50:23 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 23:26:48 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Toyer", "Sam", ""], ["Cherian", "Anoop", ""], ["Han", "Tengda", ""], ["Gould", "Stephen", ""]]}, {"id": "1707.09241", "submitter": "Yannic Kilcher", "authors": "Yannic Kilcher, Aur\\'elien Lucchi, Thomas Hofmann", "title": "Generator Reversal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of training generative models with deep neural\nnetworks as generators, i.e. to map latent codes to data points. Whereas the\ndominant paradigm combines simple priors over codes with complex deterministic\nmodels, we propose instead to use more flexible code distributions. These\ndistributions are estimated non-parametrically by reversing the generator map\nduring training. The benefits include: more powerful generative models, better\nmodeling of latent structure and explicit control of the degree of\ngeneralization.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 14:07:56 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Kilcher", "Yannic", ""], ["Lucchi", "Aur\u00e9lien", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1707.09319", "submitter": "Hrushikesh Mhaskar", "authors": "Charles K. Chui, Hrushikesh N. Mhaskar", "title": "A Fourier-invariant method for locating point-masses and computing their\n  attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the interest of observing the growth of cancer cells among\nnormal living cells and exploring how galaxies and stars are truly formed, the\nobjective of this paper is to introduce a rigorous and effective method for\ncounting point-masses, determining their spatial locations, and computing their\nattributes. Based on computation of Hermite moments that are Fourier-invariant,\nour approach facilitates the processing of both spatial and Fourier data in any\ndimension.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 20:24:41 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Chui", "Charles K.", ""], ["Mhaskar", "Hrushikesh N.", ""]]}, {"id": "1707.09364", "submitter": "Weilin Cong", "authors": "Weilin Cong, Sanyuan Zhao, Hui Tian, Jianbing Shen", "title": "Improved Face Detection and Alignment using Cascade Deep Convolutional\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world face detection and alignment demand an advanced discriminative\nmodel to address challenges by pose, lighting and expression. Illuminated by\nthe deep learning algorithm, some convolutional neural networks based face\ndetection and alignment methods have been proposed. Recent studies have\nutilized the relation between face detection and alignment to make models\ncomputationally efficiency, however they ignore the connection between each\ncascade CNNs. In this paper, we propose an structure to propose higher quality\ntraining data for End-to-End cascade network training, which give computers\nmore space to automatic adjust weight parameter and accelerate convergence.\nExperiments demonstrate considerable improvement over existing detection and\nalignment models.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 06:07:38 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Cong", "Weilin", ""], ["Zhao", "Sanyuan", ""], ["Tian", "Hui", ""], ["Shen", "Jianbing", ""]]}, {"id": "1707.09376", "submitter": "Blaz Meden", "authors": "Bla\\v{z} Meden, Refik Can Mall{\\i}, Sebastjan Fabijan, Haz{\\i}m Kemal\n  Ekenel, Vitomir \\v{S}truc, Peter Peer", "title": "Face Deidentification with Generative Deep Neural Networks", "comments": "IET Signal Processing Special Issue on Deidentification 2017", "journal-ref": null, "doi": "10.1049/iet-spr.2017.0049", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face deidentification is an active topic amongst privacy and security\nresearchers. Early deidentification methods relying on image blurring or\npixelization were replaced in recent years with techniques based on formal\nanonymity models that provide privacy guaranties and at the same time aim at\nretaining certain characteristics of the data even after deidentification. The\nlatter aspect is particularly important, as it allows to exploit the\ndeidentified data in applications for which identity information is irrelevant.\nIn this work we present a novel face deidentification pipeline, which ensures\nanonymity by synthesizing artificial surrogate faces using generative neural\nnetworks (GNNs). The generated faces are used to deidentify subjects in images\nor video, while preserving non-identity-related aspects of the data and\nconsequently enabling data utilization. Since generative networks are very\nadaptive and can utilize a diverse set of parameters (pertaining to the\nappearance of the generated output in terms of facial expressions, gender,\nrace, etc.), they represent a natural choice for the problem of face\ndeidentification. To demonstrate the feasibility of our approach, we perform\nexperiments using automated recognition tools and human annotators. Our results\nshow that the recognition performance on deidentified images is close to\nchance, suggesting that the deidentification process based on GNNs is highly\neffective.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 18:39:09 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Meden", "Bla\u017e", ""], ["Mall\u0131", "Refik Can", ""], ["Fabijan", "Sebastjan", ""], ["Ekenel", "Haz\u0131m Kemal", ""], ["\u0160truc", "Vitomir", ""], ["Peer", "Peter", ""]]}, {"id": "1707.09378", "submitter": "EPTCS", "authors": "Konstantin Genin (Carnegie Mellon University), Kevin T. Kelly\n  (Carnegie Mellon University)", "title": "The Topology of Statistical Verifiability", "comments": "In Proceedings TARK 2017, arXiv:1707.08250", "journal-ref": "EPTCS 251, 2017, pp. 236-250", "doi": "10.4204/EPTCS.251.17", "report-no": null, "categories": "cs.LG cs.AI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological models of empirical and formal inquiry are increasingly\nprevalent. They have emerged in such diverse fields as domain theory [1, 16],\nformal learning theory [18], epistemology and philosophy of science [10, 15, 8,\n9, 2], statistics [6, 7] and modal logic [17, 4]. In those applications, open\nsets are typically interpreted as hypotheses deductively verifiable by true\npropositional information that rules out relevant possibilities. However, in\nstatistical data analysis, one routinely receives random samples logically\ncompatible with every statistical hypothesis. We bridge the gap between\npropositional and statistical data by solving for the unique topology on\nprobability measures in which the open sets are exactly the statistically\nverifiable hypotheses. Furthermore, we extend that result to a topological\ncharacterization of learnability in the limit from statistical data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 07:49:20 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Genin", "Konstantin", "", "Carnegie Mellon University"], ["Kelly", "Kevin T.", "", "Carnegie Mellon University"]]}, {"id": "1707.09394", "submitter": "Kun Li", "authors": "Kun Li, Joel W. Burdick", "title": "Inverse Reinforcement Learning in Large State Spaces via Function\n  Approximation", "comments": "Experiment updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new method for inverse reinforcement learning in\nlarge-scale and high-dimensional state spaces. To avoid solving the\ncomputationally expensive reinforcement learning problems in reward learning,\nwe propose a function approximation method to ensure that the Bellman\nOptimality Equation always holds, and then estimate a function to maximize the\nlikelihood of the observed motion. The time complexity of the proposed method\nis linearly proportional to the cardinality of the action set, thus it can\nhandle large state spaces efficiently. We test the proposed method in a\nsimulated environment, and show that it is more accurate than existing methods\nand significantly better in scalability. We also show that the proposed method\ncan extend many existing methods to high-dimensional state spaces. We then\napply the method to evaluating the effect of rehabilitative stimulations on\npatients with spinal cord injuries based on the observed patient motions.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 19:57:05 GMT"}, {"version": "v2", "created": "Sun, 6 Aug 2017 12:16:11 GMT"}, {"version": "v3", "created": "Sun, 13 Aug 2017 22:01:04 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Li", "Kun", ""], ["Burdick", "Joel W.", ""]]}, {"id": "1707.09405", "submitter": "Qifeng Chen", "authors": "Qifeng Chen and Vladlen Koltun", "title": "Photographic Image Synthesis with Cascaded Refinement Networks", "comments": "Published at the International Conference on Computer Vision (ICCV\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to synthesizing photographic images conditioned on\nsemantic layouts. Given a semantic label map, our approach produces an image\nwith photographic appearance that conforms to the input layout. The approach\nthus functions as a rendering engine that takes a two-dimensional semantic\nspecification of the scene and produces a corresponding photographic image.\nUnlike recent and contemporaneous work, our approach does not rely on\nadversarial training. We show that photographic images can be synthesized from\nsemantic layouts by a single feedforward network with appropriate structure,\ntrained end-to-end with a direct regression objective. The presented approach\nscales seamlessly to high resolutions; we demonstrate this by synthesizing\nphotographic images at 2-megapixel resolution, the full resolution of our\ntraining data. Extensive perceptual experiments on datasets of outdoor and\nindoor scenes demonstrate that images synthesized by the presented approach are\nconsiderably more realistic than alternative approaches. The results are shown\nin the supplementary video at https://youtu.be/0fhUJT21-bs\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 20:24:44 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Chen", "Qifeng", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1707.09416", "submitter": "Michael Hong Gang Li", "authors": "Michael H. Li, Tiago A. Mestre, Susan H. Fox, Babak Taati", "title": "Vision-Based Assessment of Parkinsonism and Levodopa-Induced Dyskinesia\n  with Deep Learning Pose Estimation", "comments": "8 pages, 1 figure. Under review", "journal-ref": "Journal of NeuroEngineering and Rehabilitation (2018) 15:97", "doi": "10.1186/s12984-018-0446-z", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To apply deep learning pose estimation algorithms for vision-based\nassessment of parkinsonism and levodopa-induced dyskinesia (LID). Methods: Nine\nparticipants with Parkinson's disease (PD) and LID completed a levodopa\ninfusion protocol, where symptoms were assessed at regular intervals using the\nUnified Dyskinesia Rating Scale (UDysRS) and Unified Parkinson's Disease Rating\nScale (UPDRS). A state-of-the-art deep learning pose estimation method was used\nto extract movement trajectories from videos of PD assessments. Features of the\nmovement trajectories were used to detect and estimate the severity of\nparkinsonism and LID using random forest. Communication and drinking tasks were\nused to assess LID, while leg agility and toe tapping tasks were used to assess\nparkinsonism. Feature sets from tasks were also combined to predict total\nUDysRS and UPDRS Part III scores. Results: For LID, the communication task\nyielded the best results for dyskinesia (severity estimation: r = 0.661,\ndetection: AUC = 0.930). For parkinsonism, leg agility had better results for\nseverity estimation (r = 0.618), while toe tapping was better for detection\n(AUC = 0.773). UDysRS and UPDRS Part III scores were predicted with r = 0.741\nand 0.530, respectively. Conclusion: This paper presents the first application\nof deep learning for vision-based assessment of parkinsonism and LID and\ndemonstrates promising performance for the future translation of deep learning\nto PD clinical practices. Significance: The proposed system provides insight\ninto the potential of computer vision and deep learning for clinical\napplication in PD.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 20:56:22 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 16:03:22 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Li", "Michael H.", ""], ["Mestre", "Tiago A.", ""], ["Fox", "Susan H.", ""], ["Taati", "Babak", ""]]}, {"id": "1707.09425", "submitter": "Leandro Minku", "authors": "Shuo Wang, Leandro L. Minku, Nitesh Chawla, Xin Yao", "title": "Proceedings of the IJCAI 2017 Workshop on Learning in the Presence of\n  Class Imbalance and Concept Drift (LPCICD'17)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the wide application of machine learning algorithms to the real world,\nclass imbalance and concept drift have become crucial learning issues. Class\nimbalance happens when the data categories are not equally represented, i.e.,\nat least one category is minority compared to other categories. It can cause\nlearning bias towards the majority class and poor generalization. Concept drift\nis a change in the underlying distribution of the problem, and is a significant\nissue specially when learning from data streams. It requires learners to be\nadaptive to dynamic changes.\n  Class imbalance and concept drift can significantly hinder predictive\nperformance, and the problem becomes particularly challenging when they occur\nsimultaneously. This challenge arises from the fact that one problem can affect\nthe treatment of the other. For example, drift detection algorithms based on\nthe traditional classification error may be sensitive to the imbalanced degree\nand become less effective; and class imbalance techniques need to be adaptive\nto changing imbalance rates, otherwise the class receiving the preferential\ntreatment may not be the correct minority class at the current moment.\nTherefore, the mutual effect of class imbalance and concept drift should be\nconsidered during algorithm design.\n  The aim of this workshop is to bring together researchers from the areas of\nclass imbalance learning and concept drift in order to encourage discussions\nand new collaborations on solving the combined issue of class imbalance and\nconcept drift. It provides a forum for international researchers and\npractitioners to share and discuss their original work on addressing new\nchallenges and research issues in class imbalance learning, concept drift, and\nthe combined issues of class imbalance and concept drift. The proceedings\ninclude 8 papers on these topics.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 21:53:20 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Wang", "Shuo", ""], ["Minku", "Leandro L.", ""], ["Chawla", "Nitesh", ""], ["Yao", "Xin", ""]]}, {"id": "1707.09428", "submitter": "Hrushikesh Mhaskar", "authors": "Charles K. Chui, Hrushikesh N. Mhaskar", "title": "A unified method for super-resolution recovery and real exponential-sum\n  separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, motivated by diffraction of traveling light waves, a simple\nmathematical model is proposed, both for the multivariate super-resolution\nproblem and the problem of blind-source separation of real-valued exponential\nsums. This model facilitates the development of a unified theory and a unified\nsolution of both problems in this paper. Our consideration of the\nsuper-resolution problem is aimed at applications to fluorescence microscopy\nand observational astronomy, and the motivation for our consideration of the\nsecond problem is the current need of extracting multivariate exponential\nfeatures in magnetic resonance spectroscopy (MRS) for the neurologist and\nradiologist as well as for providing a mathematical tool for isotope separation\nin Nuclear Chemistry. The unified method introduced in this paper can be easily\nrealized by processing only finitely many data, sampled at locations that are\nnot necessarily prescribed in advance, with computational scheme consisting\nonly of matrix - vector multiplication, peak finding, and clustering.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 20:28:27 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Chui", "Charles K.", ""], ["Mhaskar", "Hrushikesh N.", ""]]}, {"id": "1707.09430", "submitter": "Christian Albert Hammerschmidt", "authors": "Christian A. Hammerschmidt, Radu State, Sicco Verwer", "title": "Human in the Loop: Interactive Passive Automata Learning via\n  Evidence-Driven State-Merging Algorithms", "comments": "4 pages, presented at the Human in the Loop workshop at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an interactive version of an evidence-driven state-merging (EDSM)\nalgorithm for learning variants of finite state automata. Learning these\nautomata often amounts to recovering or reverse engineering the model\ngenerating the data despite noisy, incomplete, or imperfectly sampled data\nsources rather than optimizing a purely numeric target function. Domain\nexpertise and human knowledge about the target domain can guide this process,\nand typically is captured in parameter settings. Often, domain expertise is\nsubconscious and not expressed explicitly. Directly interacting with the\nlearning algorithm makes it easier to utilize this knowledge effectively.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 22:19:50 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Hammerschmidt", "Christian A.", ""], ["State", "Radu", ""], ["Verwer", "Sicco", ""]]}, {"id": "1707.09465", "submitter": "Yang Zhang", "authors": "Yang Zhang, Philip David, Boqing Gong", "title": "Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes", "comments": "This is the extended version of the ICCV 2017 paper \"Curriculum\n  Domain Adaptation for Semantic Segmentation of Urban Scenes\" with additional\n  GTA experiment", "journal-ref": null, "doi": "10.1109/ICCV.2017.223", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last half decade, convolutional neural networks (CNNs) have\ntriumphed over semantic segmentation, which is one of the core tasks in many\napplications such as autonomous driving. However, to train CNNs requires a\nconsiderable amount of data, which is difficult to collect and laborious to\nannotate. Recent advances in computer graphics make it possible to train CNNs\non photo-realistic synthetic imagery with computer-generated annotations.\nDespite this, the domain mismatch between the real images and the synthetic\ndata cripples the models' performance. Hence, we propose a curriculum-style\nlearning approach to minimize the domain gap in urban scenery semantic\nsegmentation. The curriculum domain adaptation solves easy tasks first to infer\nnecessary properties about the target domain; in particular, the first task is\nto learn global label distributions over images and local distributions over\nlandmark superpixels. These are easy to estimate because images of urban scenes\nhave strong idiosyncrasies (e.g., the size and spatial relations of buildings,\nstreets, cars, etc.). We then train a segmentation network while regularizing\nits predictions in the target domain to follow those inferred properties. In\nexperiments, our method outperforms the baselines on two datasets and two\nbackbone networks. We also report extensive ablation studies about our\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 05:47:43 GMT"}, {"version": "v2", "created": "Sat, 5 Aug 2017 05:01:47 GMT"}, {"version": "v3", "created": "Tue, 5 Jun 2018 20:21:52 GMT"}, {"version": "v4", "created": "Fri, 19 Oct 2018 21:55:45 GMT"}, {"version": "v5", "created": "Wed, 14 Nov 2018 01:03:47 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Zhang", "Yang", ""], ["David", "Philip", ""], ["Gong", "Boqing", ""]]}, {"id": "1707.09520", "submitter": "Kyle Helfrich", "authors": "Kyle Helfrich, Devin Willmott, Qiang Ye", "title": "Orthogonal Recurrent Neural Networks with Scaled Cayley Transform", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are designed to handle sequential data but\nsuffer from vanishing or exploding gradients. Recent work on Unitary Recurrent\nNeural Networks (uRNNs) have been used to address this issue and in some cases,\nexceed the capabilities of Long Short-Term Memory networks (LSTMs). We propose\na simpler and novel update scheme to maintain orthogonal recurrent weight\nmatrices without using complex valued matrices. This is done by parametrizing\nwith a skew-symmetric matrix using the Cayley transform. Such a parametrization\nis unable to represent matrices with negative one eigenvalues, but this\nlimitation is overcome by scaling the recurrent weight matrix by a diagonal\nmatrix consisting of ones and negative ones. The proposed training scheme\ninvolves a straightforward gradient calculation and update step. In several\nexperiments, the proposed scaled Cayley orthogonal recurrent neural network\n(scoRNN) achieves superior results with fewer trainable parameters than other\nunitary RNNs.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 14:37:48 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 21:30:51 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2018 14:51:55 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Helfrich", "Kyle", ""], ["Willmott", "Devin", ""], ["Ye", "Qiang", ""]]}, {"id": "1707.09562", "submitter": "Hantian Zhang", "authors": "Yu Liu, Hantian Zhang, Luyuan Zeng, Wentao Wu, Ce Zhang", "title": "MLBench: How Good Are Machine Learning Clouds for Binary Classification\n  Tasks on Structured Data?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct an empirical study of machine learning functionalities provided by\nmajor cloud service providers, which we call machine learning clouds. Machine\nlearning clouds hold the promise of hiding all the sophistication of running\nlarge-scale machine learning: Instead of specifying how to run a machine\nlearning task, users only specify what machine learning task to run and the\ncloud figures out the rest. Raising the level of abstraction, however, rarely\ncomes free - a performance penalty is possible. How good, then, are current\nmachine learning clouds on real-world machine learning workloads?\n  We study this question with a focus on binary classication problems. We\npresent mlbench, a novel benchmark constructed by harvesting datasets from\nKaggle competitions. We then compare the performance of the top winning code\navailable from Kaggle with that of running machine learning clouds from both\nAzure and Amazon on mlbench. Our comparative study reveals the strength and\nweakness of existing machine learning clouds and points out potential future\ndirections for improvement.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 21:59:18 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 16:36:55 GMT"}, {"version": "v3", "created": "Mon, 16 Oct 2017 11:13:32 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Liu", "Yu", ""], ["Zhang", "Hantian", ""], ["Zeng", "Luyuan", ""], ["Wu", "Wentao", ""], ["Zhang", "Ce", ""]]}, {"id": "1707.09564", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Srinadh Bhojanapalli, Nathan Srebro", "title": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for\n  Neural Networks", "comments": "Accepted to ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generalization bound for feedforward neural networks in terms of\nthe product of the spectral norm of the layers and the Frobenius norm of the\nweights. The generalization bound is derived using a PAC-Bayes analysis.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 22:36:35 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 22:30:45 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Bhojanapalli", "Srinadh", ""], ["Srebro", "Nathan", ""]]}, {"id": "1707.09571", "submitter": "Linfeng Zhang", "authors": "Linfeng Zhang, Jiequn Han, Han Wang, Roberto Car, and Weinan E", "title": "Deep Potential Molecular Dynamics: a scalable model with the accuracy of\n  quantum mechanics", "comments": null, "journal-ref": "Phys. Rev. Lett. 120, 143001 (2018)", "doi": "10.1103/PhysRevLett.120.143001", "report-no": null, "categories": "physics.comp-ph cs.LG physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a scheme for molecular simulations, the Deep Potential Molecular\nDynamics (DeePMD) method, based on a many-body potential and interatomic forces\ngenerated by a carefully crafted deep neural network trained with ab initio\ndata. The neural network model preserves all the natural symmetries in the\nproblem. It is \"first principle-based\" in the sense that there are no ad hoc\ncomponents aside from the network model. We show that the proposed scheme\nprovides an efficient and accurate protocol in a variety of systems, including\nbulk materials and molecules. In all these cases, DeePMD gives results that are\nessentially indistinguishable from the original data, at a cost that scales\nlinearly with system size.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 00:26:33 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 04:43:05 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Zhang", "Linfeng", ""], ["Han", "Jiequn", ""], ["Wang", "Han", ""], ["Car", "Roberto", ""], ["E", "Weinan", ""]]}, {"id": "1707.09641", "submitter": "Benjamin Lengerich", "authors": "Benjamin J. Lengerich, Sandeep Konam, Eric P. Xing, Stephanie\n  Rosenthal, Manuela Veloso", "title": "Towards Visual Explanations for Convolutional Neural Networks via Input\n  Resampling", "comments": "Presented at ICML 2017 Workshop on Visualization for Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predictive power of neural networks often costs model interpretability.\nSeveral techniques have been developed for explaining model outputs in terms of\ninput features; however, it is difficult to translate such interpretations into\nactionable insight. Here, we propose a framework to analyze predictions in\nterms of the model's internal features by inspecting information flow through\nthe network. Given a trained network and a test image, we select neurons by two\nmetrics, both measured over a set of images created by perturbations to the\ninput image: (1) magnitude of the correlation between the neuron activation and\nthe network output and (2) precision of the neuron activation. We show that the\nformer metric selects neurons that exert large influence over the network\noutput while the latter metric selects neurons that activate on generalizable\nfeatures. By comparing the sets of neurons selected by these two metrics, our\nframework suggests a way to investigate the internal attention mechanisms of\nconvolutional neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 17:12:20 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 14:02:23 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Lengerich", "Benjamin J.", ""], ["Konam", "Sandeep", ""], ["Xing", "Eric P.", ""], ["Rosenthal", "Stephanie", ""], ["Veloso", "Manuela", ""]]}, {"id": "1707.09676", "submitter": "Yize Chen", "authors": "Yize Chen and Yishen Wang and Daniel Kirschen and Baosen Zhang", "title": "Model-Free Renewable Scenario Generation Using Generative Adversarial\n  Networks", "comments": "Accepted to IEEE Transactions on Power Systems; code available at\n  https://github.com/chennnnnyize/Renewables_Scenario_Gen_GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scenario generation is an important step in the operation and planning of\npower systems with high renewable penetrations. In this work, we proposed a\ndata-driven approach for scenario generation using generative adversarial\nnetworks, which is based on two interconnected deep neural networks. Compared\nwith existing methods based on probabilistic models that are often hard to\nscale or sample from, our method is data-driven, and captures renewable energy\nproduction patterns in both temporal and spatial dimensions for a large number\nof correlated resources. For validation, we use wind and solar times-series\ndata from NREL integration data sets. We demonstrate that the proposed method\nis able to generate realistic wind and photovoltaic power profiles with full\ndiversity of behaviors. We also illustrate how to generate scenarios based on\ndifferent conditions of interest by using labeled data during training. For\nexample, scenarios can be conditioned on weather events~(e.g. high wind day) or\ntime of the year~(e,g. solar generation for a day in July). Because of the\nfeedforward nature of the neural networks, scenarios can be generated extremely\nefficiently without sophisticated sampling techniques.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 21:36:01 GMT"}, {"version": "v2", "created": "Sat, 3 Feb 2018 22:06:28 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Chen", "Yize", ""], ["Wang", "Yishen", ""], ["Kirschen", "Daniel", ""], ["Zhang", "Baosen", ""]]}, {"id": "1707.09678", "submitter": "Christos Dimitrakakis", "authors": "Philip Ekman, Sebastian Bellevik, Christos Dimitrakakis, Aristide\n  Tossou", "title": "Learning to Match", "comments": "5 pages. This version will be presented at the VAMS Recsys workshop\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outsourcing tasks to previously unknown parties is becoming more common. One\nspecific such problem involves matching a set of workers to a set of tasks.\nEven if the latter have precise requirements, the quality of individual workers\nis usually unknown. The problem is thus a version of matching under\nuncertainty. We believe that this type of problem is going to be increasingly\nimportant.\n  When the problem involves only a single skill or type of job, it is\nessentially a type of bandit problem, and can be solved with standard\nalgorithms. However, we develop an algorithm that can perform matching for\nworkers with multiple skills hired for multiple jobs with multiple\nrequirements. We perform an experimental evaluation in both single-task and\nmulti-task problems, comparing with the bounded $\\epsilon$-first algorithm, as\nwell as an oracle that knows the true skills of workers. One of the algorithms\nwe developed gives results approaching 85\\% of oracle's performance. We invite\nthe community to take a closer look at this problem and develop real-world\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 21:50:50 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Ekman", "Philip", ""], ["Bellevik", "Sebastian", ""], ["Dimitrakakis", "Christos", ""], ["Tossou", "Aristide", ""]]}, {"id": "1707.09727", "submitter": "Vishnu Raj", "authors": "Vishnu Raj and Sheetal Kalyani", "title": "Taming Non-stationary Bandits: A Bayesian Approach", "comments": "Submitted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the multi armed bandit problem in non-stationary environments.\nBased on the Bayesian method, we propose a variant of Thompson Sampling which\ncan be used in both rested and restless bandit scenarios. Applying discounting\nto the parameters of prior distribution, we describe a way to systematically\nreduce the effect of past observations. Further, we derive the exact expression\nfor the probability of picking sub-optimal arms. By increasing the exploitative\nvalue of Bayes' samples, we also provide an optimistic version of the\nalgorithm. Extensive empirical analysis is conducted under various scenarios to\nvalidate the utility of proposed algorithms. A comparison study with various\nstate-of-the-arm algorithms is also included.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 05:46:39 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Raj", "Vishnu", ""], ["Kalyani", "Sheetal", ""]]}, {"id": "1707.09792", "submitter": "Vishnu Raj", "authors": "Vishnu Raj, Irene Dias, Thulasi Tholeti and Sheetal Kalyani", "title": "Spectrum Access In Cognitive Radio Using A Two Stage Reinforcement\n  Learning Approach", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2018.2798920", "report-no": null, "categories": "cs.IT cs.LG cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of the 5th generation of wireless standards and an increasing\ndemand for higher throughput, methods to improve the spectral efficiency of\nwireless systems have become very important. In the context of cognitive radio,\na substantial increase in throughput is possible if the secondary user can make\nsmart decisions regarding which channel to sense and when or how often to\nsense. Here, we propose an algorithm to not only select a channel for data\ntransmission but also to predict how long the channel will remain unoccupied so\nthat the time spent on channel sensing can be minimized. Our algorithm learns\nin two stages - a reinforcement learning approach for channel selection and a\nBayesian approach to determine the optimal duration for which sensing can be\nskipped. Comparisons with other learning methods are provided through extensive\nsimulations. We show that the number of sensing is minimized with negligible\nincrease in primary interference; this implies that lesser energy is spent by\nthe secondary user in sensing and also higher throughput is achieved by saving\non sensing.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 10:18:19 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Raj", "Vishnu", ""], ["Dias", "Irene", ""], ["Tholeti", "Thulasi", ""], ["Kalyani", "Sheetal", ""]]}, {"id": "1707.09835", "submitter": "Zhenguo Li", "authors": "Zhenguo Li, Fengwei Zhou, Fei Chen, Hang Li", "title": "Meta-SGD: Learning to Learn Quickly for Few-Shot Learning", "comments": "reinforcement learning included, 20-way classification on\n  MiniImagenet included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning is challenging for learning algorithms that learn each task\nin isolation and from scratch. In contrast, meta-learning learns from many\nrelated tasks a meta-learner that can learn a new task more accurately and\nfaster with fewer examples, where the choice of meta-learners is crucial. In\nthis paper, we develop Meta-SGD, an SGD-like, easily trainable meta-learner\nthat can initialize and adapt any differentiable learner in just one step, on\nboth supervised learning and reinforcement learning. Compared to the popular\nmeta-learner LSTM, Meta-SGD is conceptually simpler, easier to implement, and\ncan be learned more efficiently. Compared to the latest meta-learner MAML,\nMeta-SGD has a much higher capacity by learning to learn not just the learner\ninitialization, but also the learner update direction and learning rate, all in\na single meta-learning process. Meta-SGD shows highly competitive performance\nfor few-shot learning on regression, classification, and reinforcement\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 13:08:11 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 15:59:41 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Li", "Zhenguo", ""], ["Zhou", "Fengwei", ""], ["Chen", "Fei", ""], ["Li", "Hang", ""]]}, {"id": "1707.09871", "submitter": "Yichen Pan", "authors": "Shitao Tang, Yichen Pan", "title": "Feature Extraction via Recurrent Random Deep Ensembles and its\n  Application in Gruop-level Happiness Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel ensemble framework to extract highly\ndiscriminative feature representation of image and its application for\ngroup-level happpiness intensity prediction in wild. In order to generate\nenough diversity of decisions, n convolutional neural networks are trained by\nbootstrapping the training set and extract n features for each image from them.\nA recurrent neural network (RNN) is then used to remember which network\nextracts better feature and generate the final feature representation for one\nindividual image. Several group emotion models (GEM) are used to aggregate face\nfea- tures in a group and use parameter-optimized support vector regressor\n(SVR) to get the final results. Through extensive experiments, the great\neffectiveness of the proposed recurrent random deep ensembles (RRDE) is\ndemonstrated in both structural and decisional ways. The best result yields a\n0.55 root-mean-square error (RMSE) on validation set of HAPPEI dataset,\nsignificantly better than the baseline of 0.78.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 08:16:43 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Tang", "Shitao", ""], ["Pan", "Yichen", ""]]}, {"id": "1707.09875", "submitter": "Fan Zhang", "authors": "Fan Zhang, Chen Hu, Qiang Yin, Wei Li, Hengchao Li and Wen Hong", "title": "SAR Target Recognition Using the Multi-aspect-aware Bidirectional LSTM\n  Recurrent Neural Networks", "comments": "11 pages, 10 figures", "journal-ref": "IEEE Access, vol.5, 2017", "doi": "10.1109/ACCESS.2017.2773363", "report-no": "26880-26891", "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outstanding pattern recognition performance of deep learning brings new\nvitality to the synthetic aperture radar (SAR) automatic target recognition\n(ATR). However, there is a limitation in current deep learning based ATR\nsolution that each learning process only handle one SAR image, namely learning\nthe static scattering information, while missing the space-varying information.\nIt is obvious that multi-aspect joint recognition introduced space-varying\nscattering information should improve the classification accuracy and\nrobustness. In this paper, a novel multi-aspect-aware method is proposed to\nachieve this idea through the bidirectional Long Short-Term Memory (LSTM)\nrecurrent neural networks based space-varying scattering information learning.\nSpecifically, we first select different aspect images to generate the\nmulti-aspect space-varying image sequences. Then, the Gabor filter and\nthree-patch local binary pattern (TPLBP) are progressively implemented to\nextract a comprehensive spatial features, followed by dimensionality reduction\nwith the Multi-layer Perceptron (MLP) network. Finally, we design a\nbidirectional LSTM recurrent neural network to learn the multi-aspect features\nwith further integrating the softmax classifier to achieve target recognition.\nExperimental results demonstrate that the proposed method can achieve 99.9%\naccuracy for 10-class recognition. Besides, its anti-noise and anti-confusion\nperformance are also better than the conventional deep learning based methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 04:01:25 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Zhang", "Fan", ""], ["Hu", "Chen", ""], ["Yin", "Qiang", ""], ["Li", "Wei", ""], ["Li", "Hengchao", ""], ["Hong", "Wen", ""]]}, {"id": "1707.09893", "submitter": "Shenggang Ying", "authors": "Shenggang Ying, Mingsheng Ying, Yuan Feng", "title": "Quantum Privacy-Preserving Perceptron", "comments": "30 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the extensive applications of machine learning, the issue of private or\nsensitive data in the training examples becomes more and more serious: during\nthe training process, personal information or habits may be disclosed to\nunexpected persons or organisations, which can cause serious privacy problems\nor even financial loss. In this paper, we present a quantum privacy-preserving\nalgorithm for machine learning with perceptron. There are mainly two steps to\nprotect original training examples. Firstly when checking the current\nclassifier, quantum tests are employed to detect data user's possible\ndishonesty. Secondly when updating the current classifier, private random noise\nis used to protect the original data. The advantages of our algorithm are: (1)\nit protects training examples better than the known classical methods; (2) it\nrequires no quantum database and thus is easy to implement.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 14:42:57 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Ying", "Shenggang", ""], ["Ying", "Mingsheng", ""], ["Feng", "Yuan", ""]]}, {"id": "1707.09917", "submitter": "Yafeng Niu", "authors": "Yafeng Niu, Dongsheng Zou, Yadong Niu, Zhongshi He, Hua Tan", "title": "A breakthrough in Speech emotion recognition using Deep Retinal\n  Convolution Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech emotion recognition (SER) is to study the formation and change of\nspeaker's emotional state from the speech signal perspective, so as to make the\ninteraction between human and computer more intelligent. SER is a challenging\ntask that has encountered the problem of less training data and low prediction\naccuracy. Here we propose a data augmentation algorithm based on the imaging\nprinciple of the retina and convex lens, to acquire the different sizes of\nspectrogram and increase the amount of training data by changing the distance\nbetween the spectrogram and the convex lens. Meanwhile, with the help of deep\nlearning to get the high-level features, we propose the Deep Retinal\nConvolution Neural Networks (DRCNNs) for SER and achieve the average accuracy\nover 99%. The experimental results indicate that DRCNNs outperforms the\nprevious studies in terms of both the number of emotions and the accuracy of\nrecognition. Predictably, our results will dramatically improve human-computer\ninteraction.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 02:00:15 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Niu", "Yafeng", ""], ["Zou", "Dongsheng", ""], ["Niu", "Yadong", ""], ["He", "Zhongshi", ""], ["Tan", "Hua", ""]]}, {"id": "1707.09933", "submitter": "Jayadeva", "authors": "Jayadeva, Himanshu Pant, Mayank Sharma, Abhimanyu Dubey, Sumit Soman,\n  Suraj Tripathi, Sai Guruju and Nihal Goalla", "title": "Learning Neural Network Classifiers with Low Model Complexity", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neural network architectures for large-scale learning tasks have\nsubstantially higher model complexities, which makes understanding, visualizing\nand training these architectures difficult. Recent contributions to deep\nlearning techniques have focused on architectural modifications to improve\nparameter efficiency and performance. In this paper, we derive a continuous and\ndifferentiable error functional for a neural network that minimizes its\nempirical error as well as a measure of the model complexity. The latter\nmeasure is obtained by deriving a differentiable upper bound on the\nVapnik-Chervonenkis (VC) dimension of the classifier layer of a class of deep\nnetworks. Using standard backpropagation, we realize a training rule that tries\nto minimize the error on training samples, while improving generalization by\nkeeping the model complexity low. We demonstrate the effectiveness of our\nformulation (the Low Complexity Neural Network - LCNN) across several deep\nlearning algorithms, and a variety of large benchmark datasets. We show that\nhidden layer neurons in the resultant networks learn features that are crisp,\nand in the case of image datasets, quantitatively sharper. Our proposed\napproach yields benefits across a wide range of architectures, in comparison to\nand in conjunction with methods such as Dropout and Batch Normalization, and\nour results strongly suggest that deep learning techniques can benefit from\nmodel complexity control methods such as the LCNN learning rule.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 16:03:50 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 16:16:18 GMT"}, {"version": "v3", "created": "Sat, 6 Mar 2021 04:40:29 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Jayadeva", "", ""], ["Pant", "Himanshu", ""], ["Sharma", "Mayank", ""], ["Dubey", "Abhimanyu", ""], ["Soman", "Sumit", ""], ["Tripathi", "Suraj", ""], ["Guruju", "Sai", ""], ["Goalla", "Nihal", ""]]}, {"id": "1707.09938", "submitter": "Jong Chul Ye", "authors": "Eunhee Kang, Jaejun Yoo, and Jong Chul Ye", "title": "Deep Convolutional Framelet Denosing for Low-Dose CT via Wavelet\n  Residual Network", "comments": "This will appear in IEEE Transaction on Medical Imaging, a special\n  issue of Machine Learning for Image Reconstruction", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model based iterative reconstruction (MBIR) algorithms for low-dose X-ray CT\nare computationally expensive. To address this problem, we recently proposed a\ndeep convolutional neural network (CNN) for low-dose X-ray CT and won the\nsecond place in 2016 AAPM Low-Dose CT Grand Challenge. However, some of the\ntexture were not fully recovered. To address this problem, here we propose a\nnovel framelet-based denoising algorithm using wavelet residual network which\nsynergistically combines the expressive power of deep learning and the\nperformance guarantee from the framelet-based denoising algorithms. The new\nalgorithms were inspired by the recent interpretation of the deep convolutional\nneural network (CNN) as a cascaded convolution framelet signal representation.\nExtensive experimental results confirm that the proposed networks have\nsignificantly improved performance and preserves the detail texture of the\noriginal images.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 16:17:31 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 16:10:04 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 07:46:15 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Kang", "Eunhee", ""], ["Yoo", "Jaejun", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1707.09971", "submitter": "Cong Ma", "authors": "Yuxin Chen, Jianqing Fan, Cong Ma, Kaizheng Wang", "title": "Spectral Method and Regularized MLE Are Both Optimal for Top-$K$ Ranking", "comments": "Add discussions on the setting of the general condition number", "journal-ref": "Annals of Statististics, Volume 47, Number 4 (2019), 2204-2235", "doi": "10.1214/18-AOS1745", "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the problem of top-$K$ ranking from pairwise\ncomparisons. Given a collection of $n$ items and a few pairwise comparisons\nacross them, one wishes to identify the set of $K$ items that receive the\nhighest ranks. To tackle this problem, we adopt the logistic parametric model\n--- the Bradley-Terry-Luce model, where each item is assigned a latent\npreference score, and where the outcome of each pairwise comparison depends\nsolely on the relative scores of the two items involved. Recent works have made\nsignificant progress towards characterizing the performance (e.g. the mean\nsquare error for estimating the scores) of several classical methods, including\nthe spectral method and the maximum likelihood estimator (MLE). However, where\nthey stand regarding top-$K$ ranking remains unsettled.\n  We demonstrate that under a natural random sampling model, the spectral\nmethod alone, or the regularized MLE alone, is minimax optimal in terms of the\nsample complexity --- the number of paired comparisons needed to ensure exact\ntop-$K$ identification, for the fixed dynamic range regime. This is\naccomplished via optimal control of the entrywise error of the score estimates.\nWe complement our theoretical studies by numerical experiments, confirming that\nboth methods yield low entrywise errors for estimating the underlying scores.\nOur theory is established via a novel leave-one-out trick, which proves\neffective for analyzing both iterative and non-iterative procedures. Along the\nway, we derive an elementary eigenvector perturbation bound for probability\ntransition matrices, which parallels the Davis-Kahan $\\sin\\Theta$ theorem for\nsymmetric matrices. This also allows us to close the gap between the $\\ell_2$\nerror upper bound for the spectral method and the minimax lower limit.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 17:33:15 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 03:20:10 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2018 16:35:37 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Chen", "Yuxin", ""], ["Fan", "Jianqing", ""], ["Ma", "Cong", ""], ["Wang", "Kaizheng", ""]]}]