[{"id": "1403.0057", "submitter": "Tian Lin", "authors": "Wei Chen, Tian Lin, Cheng Yang", "title": "Real-time Topic-aware Influence Maximization Using Preprocessing", "comments": "1 figure and 10 tables. Extended abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influence maximization is the task of finding a set of seed nodes in a social\nnetwork such that the influence spread of these seed nodes based on certain\ninfluence diffusion model is maximized. Topic-aware influence diffusion models\nhave been recently proposed to address the issue that influence between a pair\nof users are often topic-dependent and information, ideas, innovations etc.\nbeing propagated in networks (referred collectively as items in this paper) are\ntypically mixtures of topics. In this paper, we focus on the topic-aware\ninfluence maximization task. In particular, we study preprocessing methods for\nthese topics to avoid redoing influence maximization for each item from\nscratch. We explore two preprocessing algorithms with theoretical\njustifications. Our empirical results on data obtained in a couple of existing\nstudies demonstrate that one of our algorithms stands out as a strong candidate\nproviding microsecond online response time and competitive influence spread,\nwith reasonable preprocessing effort.\n", "versions": [{"version": "v1", "created": "Sat, 1 Mar 2014 07:18:42 GMT"}, {"version": "v2", "created": "Fri, 21 Nov 2014 08:23:40 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Chen", "Wei", ""], ["Lin", "Tian", ""], ["Yang", "Cheng", ""]]}, {"id": "1403.0156", "submitter": "Tara Babaie", "authors": "Tahereh Babaie, Sanjay Chawla, Romesh Abeysuriya", "title": "Sleep Analytics and Online Selective Anomaly Detection", "comments": "Submitted to 20th ACM SIGKDD Conference on Knowledge Discovery and\n  Data Mining 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new problem, the Online Selective Anomaly Detection (OSAD), to\nmodel a specific scenario emerging from research in sleep science. Scientists\nhave segmented sleep into several stages and stage two is characterized by two\npatterns (or anomalies) in the EEG time series recorded on sleep subjects.\nThese two patterns are sleep spindle (SS) and K-complex. The OSAD problem was\nintroduced to design a residual system, where all anomalies (known and unknown)\nare detected but the system only triggers an alarm when non-SS anomalies\nappear. The solution of the OSAD problem required us to combine techniques from\nboth machine learning and control theory. Experiments on data from real\nsubjects attest to the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 2 Mar 2014 04:14:23 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Babaie", "Tahereh", ""], ["Chawla", "Sanjay", ""], ["Abeysuriya", "Romesh", ""]]}, {"id": "1403.0157", "submitter": "Tara Babaie", "authors": "Tahereh Babaie, Sanjay Chawla, Sebastien Ardon", "title": "Network Traffic Decomposition for Anomaly Detection", "comments": "Submitted to The Journal of Data Mining and Knowledge Discovery\n  (DAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we focus on the detection of network anomalies like Denial of\nService (DoS) attacks and port scans in a unified manner. While there has been\nan extensive amount of research in network anomaly detection, current state of\nthe art methods are only able to detect one class of anomalies at the cost of\nothers. The key tool we will use is based on the spectral decomposition of a\ntrajectory/hankel matrix which is able to detect deviations from both between\nand within correlation present in the observed network traffic data. Detailed\nexperiments on synthetic and real network traces shows a significant\nimprovement in detection capability over competing approaches. In the process\nwe also address the issue of robustness of anomaly detection systems in a\nprincipled fashion.\n", "versions": [{"version": "v1", "created": "Sun, 2 Mar 2014 04:18:00 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Babaie", "Tahereh", ""], ["Chawla", "Sanjay", ""], ["Ardon", "Sebastien", ""]]}, {"id": "1403.0388", "submitter": "Mohammadzaman Zamani", "authors": "Mohammadzaman Zamani, Hamid Beigy, and Amirreza Shaban", "title": "Cascading Randomized Weighted Majority: A New Online Ensemble Learning\n  Algorithm", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing volume of data in the world, the best approach for\nlearning from this data is to exploit an online learning algorithm. Online\nensemble methods are online algorithms which take advantage of an ensemble of\nclassifiers to predict labels of data. Prediction with expert advice is a\nwell-studied problem in the online ensemble learning literature. The Weighted\nMajority algorithm and the randomized weighted majority (RWM) are the most\nwell-known solutions to this problem, aiming to converge to the best expert.\nSince among some expert, the best one does not necessarily have the minimum\nerror in all regions of data space, defining specific regions and converging to\nthe best expert in each of these regions will lead to a better result. In this\npaper, we aim to resolve this defect of RWM algorithms by proposing a novel\nonline ensemble algorithm to the problem of prediction with expert advice. We\npropose a cascading version of RWM to achieve not only better experimental\nresults but also a better error bound for sufficiently large datasets.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 11:05:10 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 17:57:03 GMT"}, {"version": "v3", "created": "Sun, 4 Jan 2015 03:01:38 GMT"}, {"version": "v4", "created": "Mon, 2 Feb 2015 17:18:43 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Zamani", "Mohammadzaman", ""], ["Beigy", "Hamid", ""], ["Shaban", "Amirreza", ""]]}, {"id": "1403.0481", "submitter": "Arindam Chaudhuri AC", "authors": "Arindam Chaudhuri", "title": "Support Vector Machine Model for Currency Crisis Discrimination", "comments": "Book Chapter Selected Works in Infrastructural Finance, Rudra P.\n  Pradhan, Indian Institute of Technology Kharagpur, Editor, Macmillan\n  Publishers, India, pp 249 - 256, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Machine (SVM) is powerful classification technique based on\nthe idea of structural risk minimization. Use of kernel function enables curse\nof dimensionality to be addressed. However, proper kernel function for certain\nproblem is dependent on specific dataset and as such there is no good method on\nchoice of kernel function. In this paper, SVM is used to build empirical models\nof currency crisis in Argentina. An estimation technique is developed by\ntraining model on real life data set which provides reasonably accurate model\noutputs and helps policy makers to identify situations in which currency crisis\nmay happen. The third and fourth order polynomial kernel is generally best\nchoice to achieve high generalization of classifier performance. SVM has high\nlevel of maturity with algorithms that are simple, easy to implement, tolerates\ncurse of dimensionality and good empirical performance. The satisfactory\nresults show that currency crisis situation is properly emulated using only\nsmall fraction of database and could be used as an evaluation tool as well as\nan early warning system. To the best of knowledge this is the first work on SVM\napproach for currency crisis evaluation of Argentina.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 16:34:38 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Chaudhuri", "Arindam", ""]]}, {"id": "1403.0598", "submitter": "Pinar Yanardag", "authors": "Pinar Yanardag, S.V.N. Vishwanathan", "title": "The Structurally Smoothed Graphlet Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A commonly used paradigm for representing graphs is to use a vector that\ncontains normalized frequencies of occurrence of certain motifs or sub-graphs.\nThis vector representation can be used in a variety of applications, such as,\nfor computing similarity between graphs. The graphlet kernel of Shervashidze et\nal. [32] uses induced sub-graphs of k nodes (christened as graphlets by Przulj\n[28]) as motifs in the vector representation, and computes the kernel via a dot\nproduct between these vectors. One can easily show that this is a valid kernel\nbetween graphs. However, such a vector representation suffers from a few\ndrawbacks. As k becomes larger we encounter the sparsity problem; most higher\norder graphlets will not occur in a given graph. This leads to diagonal\ndominance, that is, a given graph is similar to itself but not to any other\ngraph in the dataset. On the other hand, since lower order graphlets tend to be\nmore numerous, using lower values of k does not provide enough discrimination\nability. We propose a smoothing technique to tackle the above problems. Our\nmethod is based on a novel extension of Kneser-Ney and Pitman-Yor smoothing\ntechniques from natural language processing to graphs. We use the relationships\nbetween lower order and higher order graphlets in order to derive our method.\nConsequently, our smoothing algorithm not only respects the dependency between\nsub-graphs but also tackles the diagonal dominance problem by distributing the\nprobability mass across graphlets. In our experiments, the smoothed graphlet\nkernel outperforms graph kernels based on raw frequency counts.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 21:20:14 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Yanardag", "Pinar", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1403.0628", "submitter": "Hugh Brendan McMahan", "authors": "H. Brendan McMahan and Francesco Orabona", "title": "Unconstrained Online Linear Learning in Hilbert Spaces: Minimax\n  Algorithms and Normal Approximations", "comments": "Proceedings of the 27th Annual Conference on Learning Theory (COLT\n  2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study algorithms for online linear optimization in Hilbert spaces,\nfocusing on the case where the player is unconstrained. We develop a novel\ncharacterization of a large class of minimax algorithms, recovering, and even\nimproving, several previous results as immediate corollaries. Moreover, using\nour tools, we develop an algorithm that provides a regret bound of\n$\\mathcal{O}\\Big(U \\sqrt{T \\log(U \\sqrt{T} \\log^2 T +1)}\\Big)$, where $U$ is\nthe $L_2$ norm of an arbitrary comparator and both $T$ and $U$ are unknown to\nthe player. This bound is optimal up to $\\sqrt{\\log \\log T}$ terms. When $T$ is\nknown, we derive an algorithm with an optimal regret bound (up to constant\nfactors). For both the known and unknown $T$ case, a Normal approximation to\nthe conditional value of the game proves to be the key analysis tool.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 23:06:24 GMT"}, {"version": "v2", "created": "Wed, 21 May 2014 16:17:09 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["McMahan", "H. Brendan", ""], ["Orabona", "Francesco", ""]]}, {"id": "1403.0648", "submitter": "Jinli Hu Mr", "authors": "Jinli Hu and Amos Storkey", "title": "Multi-period Trading Prediction Markets with Connections to Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG q-fin.TR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new model for prediction markets, in which we use risk measures\nto model agents and introduce a market maker to describe the trading process.\nThis specific choice on modelling tools brings us mathematical convenience. The\nanalysis shows that the whole market effectively approaches a global objective,\ndespite that the market is designed such that each agent only cares about its\nown goal. Additionally, the market dynamics provides a sensible algorithm for\noptimising the global objective. An intimate connection between machine\nlearning and our markets is thus established, such that we could 1) analyse a\nmarket by applying machine learning methods to the global objective, and 2)\nsolve machine learning problems by setting up and running certain markets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 01:14:40 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Hu", "Jinli", ""], ["Storkey", "Amos", ""]]}, {"id": "1403.0667", "submitter": "James Voss", "authors": "James Voss, Mikhail Belkin, Luis Rademacher", "title": "The Hidden Convexity of Spectral Clustering", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, spectral clustering has become a standard method for data\nanalysis used in a broad range of applications. In this paper we propose a new\nclass of algorithms for multiway spectral clustering based on optimization of a\ncertain \"contrast function\" over the unit sphere. These algorithms, partly\ninspired by certain Independent Component Analysis techniques, are simple, easy\nto implement and efficient.\n  Geometrically, the proposed algorithms can be interpreted as hidden basis\nrecovery by means of function optimization. We give a complete characterization\nof the contrast functions admissible for provable basis recovery. We show how\nthese conditions can be interpreted as a \"hidden convexity\" of our optimization\nproblem on the sphere; interestingly, we use efficient convex maximization\nrather than the more common convex minimization. We also show encouraging\nexperimental results on real and simulated data.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 02:48:20 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 21:59:05 GMT"}, {"version": "v3", "created": "Wed, 4 May 2016 18:10:13 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Voss", "James", ""], ["Belkin", "Mikhail", ""], ["Rademacher", "Luis", ""]]}, {"id": "1403.0736", "submitter": "Marc Claesen", "authors": "Marc Claesen, Frank De Smet, Johan A.K. Suykens, Bart De Moor", "title": "Fast Prediction with SVM Models Containing RBF Kernels", "comments": "9 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approximation scheme for support vector machine models that use\nan RBF kernel. A second-order Maclaurin series approximation is used for\nexponentials of inner products between support vectors and test instances. The\napproximation is applicable to all kernel methods featuring sums of kernel\nevaluations and makes no assumptions regarding data normalization. The\nprediction speed of approximated models no longer relates to the amount of\nsupport vectors but is quadratic in terms of the number of input dimensions. If\nthe number of input dimensions is small compared to the amount of support\nvectors, the approximated model is significantly faster in prediction and has a\nsmaller memory footprint. An optimized C++ implementation was made to assess\nthe gain in prediction speed in a set of practical tests. We additionally\nprovide a method to verify the approximation accuracy, prior to training models\nor during run-time, to ensure the loss in accuracy remains acceptable and\nwithin known bounds.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 10:47:45 GMT"}, {"version": "v2", "created": "Sun, 13 Apr 2014 08:43:17 GMT"}, {"version": "v3", "created": "Fri, 3 Oct 2014 14:45:41 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Claesen", "Marc", ""], ["De Smet", "Frank", ""], ["Suykens", "Johan A. K.", ""], ["De Moor", "Bart", ""]]}, {"id": "1403.0745", "submitter": "Marc Claesen", "authors": "Marc Claesen, Frank De Smet, Johan Suykens, Bart De Moor", "title": "EnsembleSVM: A Library for Ensemble Learning Using Support Vector\n  Machines", "comments": "5 pages, 1 table", "journal-ref": "Journal of Machine Learning Research. 15 (2014) 141-145", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EnsembleSVM is a free software package containing efficient routines to\nperform ensemble learning with support vector machine (SVM) base models. It\ncurrently offers ensemble methods based on binary SVM models. Our\nimplementation avoids duplicate storage and evaluation of support vectors which\nare shared between constituent models. Experimental results show that using\nensemble approaches can drastically reduce training complexity while\nmaintaining high predictive accuracy. The EnsembleSVM software package is\nfreely available online at http://esat.kuleuven.be/stadius/ensemblesvm.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 11:28:59 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Claesen", "Marc", ""], ["De Smet", "Frank", ""], ["Suykens", "Johan", ""], ["De Moor", "Bart", ""]]}, {"id": "1403.0829", "submitter": "Weifeng Liu", "authors": "W. Liu, H. Liu, D. Tao, Y. Wang, Ke Lu", "title": "Multiview Hessian regularized logistic regression for action recognition", "comments": "13 pages,2 figures, submitted to signal processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of social media sharing, people often need to\nmanage the growing volume of multimedia data such as large scale video\nclassification and annotation, especially to organize those videos containing\nhuman activities. Recently, manifold regularized semi-supervised learning\n(SSL), which explores the intrinsic data probability distribution and then\nimproves the generalization ability with only a small number of labeled data,\nhas emerged as a promising paradigm for semiautomatic video classification. In\naddition, human action videos often have multi-modal content and different\nrepresentations. To tackle the above problems, in this paper we propose\nmultiview Hessian regularized logistic regression (mHLR) for human action\nrecognition. Compared with existing work, the advantages of mHLR lie in three\nfolds: (1) mHLR combines multiple Hessian regularization, each of which\nobtained from a particular representation of instance, to leverage the\nexploring of local geometry; (2) mHLR naturally handle multi-view instances\nwith multiple representations; (3) mHLR employs a smooth loss function and then\ncan be effectively optimized. We carefully conduct extensive experiments on the\nunstructured social activity attribute (USAA) dataset and the experimental\nresults demonstrate the effectiveness of the proposed multiview Hessian\nregularized logistic regression for human action recognition.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 01:11:40 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Liu", "W.", ""], ["Liu", "H.", ""], ["Tao", "D.", ""], ["Wang", "Y.", ""], ["Lu", "Ke", ""]]}, {"id": "1403.0873", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J Kir\\'aly and Louis Theran", "title": "Matroid Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DM cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algebraic combinatorial method for solving large sparse linear\nsystems of equations locally - that is, a method which can compute single\nevaluations of the signal without computing the whole signal. The method scales\nonly in the sparsity of the system and not in its size, and allows to provide\nerror estimates for any solution method. At the heart of our approach is the\nso-called regression matroid, a combinatorial object associated to sparsity\npatterns, which allows to replace inversion of the large matrix with the\ninversion of a kernel matrix that is constant size. We show that our method\nprovides the best linear unbiased estimator (BLUE) for this setting and the\nminimum variance unbiased estimator (MVUE) under Gaussian noise assumptions,\nand furthermore we show that the size of the kernel matrix which is to be\ninverted can be traded off with accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 17:54:37 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Kir\u00e1ly", "Franz J", ""], ["Theran", "Louis", ""]]}, {"id": "1403.0921", "submitter": "Kevin Xu", "authors": "Kevin S. Xu and Alfred O. Hero III", "title": "Dynamic stochastic blockmodels for time-evolving social networks", "comments": "To appear in Journal of Selected Topics in Signal Processing special\n  issue: Signal Processing for Social Networks", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing 8 (2014)\n  552-562", "doi": "10.1109/JSTSP.2014.2310294", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant efforts have gone into the development of statistical models for\nanalyzing data in the form of networks, such as social networks. Most existing\nwork has focused on modeling static networks, which represent either a single\ntime snapshot or an aggregate view over time. There has been recent interest in\nstatistical modeling of dynamic networks, which are observed at multiple points\nin time and offer a richer representation of many complex phenomena. In this\npaper, we present a state-space model for dynamic networks that extends the\nwell-known stochastic blockmodel for static networks to the dynamic setting. We\nfit the model in a near-optimal manner using an extended Kalman filter (EKF)\naugmented with a local search. We demonstrate that the EKF-based algorithm\nperforms competitively with a state-of-the-art algorithm based on Markov chain\nMonte Carlo sampling but is significantly less computationally demanding.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 19:54:07 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Xu", "Kevin S.", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1403.1024", "submitter": "Hyun Oh Song", "authors": "Hyun Oh Song, Ross Girshick, Stefanie Jegelka, Julien Mairal, Zaid\n  Harchaoui, Trevor Darrell", "title": "On learning to localize objects with minimal supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to localize objects with minimal supervision is an important problem\nin computer vision, since large fully annotated datasets are extremely costly\nto obtain. In this paper, we propose a new method that achieves this goal with\nonly image-level labels of whether the objects are present or not. Our approach\ncombines a discriminative submodular cover problem for automatically\ndiscovering a set of positive object windows with a smoothed latent SVM\nformulation. The latter allows us to leverage efficient quasi-Newton\noptimization techniques. Our experiments demonstrate that the proposed approach\nprovides a 50% relative improvement in mean average precision over the current\nstate-of-the-art on PASCAL VOC 2007 detection.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2014 07:21:20 GMT"}, {"version": "v2", "created": "Fri, 14 Mar 2014 00:50:26 GMT"}, {"version": "v3", "created": "Mon, 17 Mar 2014 21:04:49 GMT"}, {"version": "v4", "created": "Thu, 15 May 2014 22:08:59 GMT"}], "update_date": "2014-05-19", "authors_parsed": [["Song", "Hyun Oh", ""], ["Girshick", "Ross", ""], ["Jegelka", "Stefanie", ""], ["Mairal", "Julien", ""], ["Harchaoui", "Zaid", ""], ["Darrell", "Trevor", ""]]}, {"id": "1403.1124", "submitter": "Juha Karvanen", "authors": "Juha Karvanen", "title": "Estimating complex causal effects from incomplete observational data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the major advances taken in causal modeling, causality is still an\nunfamiliar topic for many statisticians. In this paper, it is demonstrated from\nthe beginning to the end how causal effects can be estimated from observational\ndata assuming that the causal structure is known. To make the problem more\nchallenging, the causal effects are highly nonlinear and the data are missing\nat random. The tools used in the estimation include causal models with design,\ncausal calculus, multiple imputation and generalized additive models. The main\nmessage is that a trained statistician can estimate causal effects by\njudiciously combining existing tools.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2014 13:40:29 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 08:12:09 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Karvanen", "Juha", ""]]}, {"id": "1403.1252", "submitter": "Rami Al-Rfou", "authors": "Bryan Perozzi, Rami Al-Rfou, Vivek Kulkarni, Steven Skiena", "title": "Inducing Language Networks from Continuous Space Word Representations", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in unsupervised feature learning have developed powerful\nlatent representations of words. However, it is still not clear what makes one\nrepresentation better than another and how we can learn the ideal\nrepresentation. Understanding the structure of latent spaces attained is key to\nany future advancement in unsupervised learning. In this work, we introduce a\nnew view of continuous space word representations as language networks. We\nexplore two techniques to create language networks from learned features by\ninducing them for two popular word representation methods and examining the\nproperties of their resulting networks. We find that the induced networks\ndiffer from other methods of creating language networks, and that they contain\nmeaningful community structure.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 01:36:53 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 17:36:43 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Perozzi", "Bryan", ""], ["Al-Rfou", "Rami", ""], ["Kulkarni", "Vivek", ""], ["Skiena", "Steven", ""]]}, {"id": "1403.1329", "submitter": "Lionel Ott", "authors": "Lionel Ott, Linsey Pang, Fabio Ramos, David Howe, Sanjay Chawla", "title": "Integer Programming Relaxations for Integrated Clustering and Outlier\n  Detection", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present methods for exemplar based clustering with outlier\nselection based on the facility location formulation. Given a distance function\nand the number of outliers to be found, the methods automatically determine the\nnumber of clusters and outliers. We formulate the problem as an integer program\nto which we present relaxations that allow for solutions that scale to large\ndata sets. The advantages of combining clustering and outlier selection\ninclude: (i) the resulting clusters tend to be compact and semantically\ncoherent (ii) the clusters are more robust against data perturbations and (iii)\nthe outliers are contextualised by the clusters and more interpretable, i.e. it\nis easier to distinguish between outliers which are the result of data errors\nfrom those that may be indicative of a new pattern emergent in the data. We\npresent and contrast three relaxations to the integer program formulation: (i)\na linear programming formulation (LP) (ii) an extension of affinity propagation\nto outlier detection (APOC) and (iii) a Lagrangian duality based formulation\n(LD). Evaluation on synthetic as well as real data shows the quality and\nscalability of these different methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 02:42:22 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Ott", "Lionel", ""], ["Pang", "Linsey", ""], ["Ramos", "Fabio", ""], ["Howe", "David", ""], ["Chawla", "Sanjay", ""]]}, {"id": "1403.1336", "submitter": "Kiran Sree Pokkuluri Prof", "authors": "Pokkuluri Kiran Sree, Inampudi Ramesh Babu", "title": "An Extensive Repot on the Efficiency of AIS-INMACA (A Novel Integrated\n  MACA based Clonal Classifier for Protein Coding and Promoter Region\n  Prediction)", "comments": "5 Pages, Review of Bioinformatics and Biometrics (RBB) Volume 3 Issue\n  1, March 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper exclusively reports the efficiency of AIS-INMACA. AIS-INMACA has\ncreated good impact on solving major problems in bioinformatics like protein\nregion identification and promoter region prediction with less time (Pokkuluri\nKiran Sree, 2014). This AIS-INMACA is now came with several variations\n(Pokkuluri Kiran Sree, 2014) towards projecting it as a tool in bioinformatics\nfor solving many problems in bioinformatics. So this paper will be very much\nuseful for so many researchers who are working in the domain of bioinformatics\nwith cellular automata.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 03:46:38 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Sree", "Pokkuluri Kiran", ""], ["Babu", "Inampudi Ramesh", ""]]}, {"id": "1403.1347", "submitter": "Jian Zhou Zhou", "authors": "Jian Zhou and Olga G. Troyanskaya", "title": "Deep Supervised and Convolutional Generative Stochastic Network for\n  Protein Secondary Structure Prediction", "comments": "Accepted by ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting protein secondary structure is a fundamental problem in protein\nstructure prediction. Here we present a new supervised generative stochastic\nnetwork (GSN) based method to predict local secondary structure with deep\nhierarchical representations. GSN is a recently proposed deep learning\ntechnique (Bengio & Thibodeau-Laufer, 2013) to globally train deep generative\nmodel. We present the supervised extension of GSN, which learns a Markov chain\nto sample from a conditional distribution, and applied it to protein structure\nprediction. To scale the model to full-sized, high-dimensional data, like\nprotein sequences with hundreds of amino acids, we introduce a convolutional\narchitecture, which allows efficient learning across multiple layers of\nhierarchical representations. Our architecture uniquely focuses on predicting\nstructured low-level labels informed with both low and high-level\nrepresentations learned by the model. In our application this corresponds to\nlabeling the secondary structure state of each amino-acid residue. We trained\nand tested the model on separate sets of non-homologous proteins sharing less\nthan 30% sequence identity. Our model achieves 66.4% Q8 accuracy on the CB513\ndataset, better than the previously reported best performance 64.9% (Wang et\nal., 2011) for this challenging secondary structure prediction problem.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 05:18:26 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Zhou", "Jian", ""], ["Troyanskaya", "Olga G.", ""]]}, {"id": "1403.1353", "submitter": "Yang Wu", "authors": "Yang Wu, Vansteenberge Jarich, Masayuki Mukunoki, and Michihiko Minoh", "title": "Collaborative Representation for Classification, Sparse or Non-sparse?", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation based classification (SRC) has been proved to be a\nsimple, effective and robust solution to face recognition. As it gets popular,\ndoubts on the necessity of enforcing sparsity starts coming up, and primary\nexperimental results showed that simply changing the $l_1$-norm based\nregularization to the computationally much more efficient $l_2$-norm based\nnon-sparse version would lead to a similar or even better performance. However,\nthat's not always the case. Given a new classification task, it's still unclear\nwhich regularization strategy (i.e., making the coefficients sparse or\nnon-sparse) is a better choice without trying both for comparison. In this\npaper, we present as far as we know the first study on solving this issue,\nbased on plenty of diverse classification experiments. We propose a scoring\nfunction for pre-selecting the regularization strategy using only the dataset\nsize, the feature dimensionality and a discrimination score derived from a\ngiven feature representation. Moreover, we show that when dictionary learning\nis taking into account, non-sparse representation has a more significant\nsuperiority to sparse representation. This work is expected to enrich our\nunderstanding of sparse/non-sparse collaborative representation for\nclassification and motivate further research activities.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 05:44:32 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Wu", "Yang", ""], ["Jarich", "Vansteenberge", ""], ["Mukunoki", "Masayuki", ""], ["Minoh", "Michihiko", ""]]}, {"id": "1403.1412", "submitter": "Katri Pulliyakode Saishankar", "authors": "K.P. Saishankar, Sheetal Kalyani, K. Narendran", "title": "Rate Prediction and Selection in LTE systems using Modified Source\n  Encoding Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In current wireless systems, the base-Station (eNodeB) tries to serve its\nuser-equipment (UE) at the highest possible rate that the UE can reliably\ndecode. The eNodeB obtains this rate information as a quantized feedback from\nthe UE at time n and uses this, for rate selection till the next feedback is\nreceived at time n + {\\delta}. The feedback received at n can become outdated\nbefore n + {\\delta}, because of a) Doppler fading, and b) Change in the set of\nactive interferers for a UE. Therefore rate prediction becomes essential.\nSince, the rates belong to a discrete set, we propose a discrete sequence\nprediction approach, wherein, frequency trees for the discrete sequences are\nbuilt using source encoding algorithms like Prediction by Partial Match (PPM).\nFinding the optimal depth of the frequency tree used for prediction is cast as\na model order selection problem. The rate sequence complexity is analysed to\nprovide an upper bound on model order. Information-theoretic criteria are then\nused to solve the model order problem. Finally, two prediction algorithms are\nproposed, using the PPM with optimal model order and system level simulations\ndemonstrate the improvement in packet loss and throughput due to these\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 11:32:00 GMT"}, {"version": "v2", "created": "Fri, 14 Mar 2014 06:25:30 GMT"}, {"version": "v3", "created": "Wed, 30 Apr 2014 15:55:38 GMT"}, {"version": "v4", "created": "Fri, 23 May 2014 11:16:58 GMT"}, {"version": "v5", "created": "Fri, 8 Aug 2014 11:10:18 GMT"}], "update_date": "2014-08-11", "authors_parsed": [["Saishankar", "K. P.", ""], ["Kalyani", "Sheetal", ""], ["Narendran", "K.", ""]]}, {"id": "1403.1430", "submitter": "Zhenfang Hu", "authors": "Zhenfang Hu, Gang Pan, Yueming Wang, and Zhaohui Wu", "title": "Sparse Principal Component Analysis via Rotation and Truncation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse principal component analysis (sparse PCA) aims at finding a sparse\nbasis to improve the interpretability over the dense basis of PCA, meanwhile\nthe sparse basis should cover the data subspace as much as possible. In\ncontrast to most of existing work which deal with the problem by adding some\nsparsity penalties on various objectives of PCA, in this paper, we propose a\nnew method SPCArt, whose motivation is to find a rotation matrix and a sparse\nbasis such that the sparse basis approximates the basis of PCA after the\nrotation. The algorithm of SPCArt consists of three alternating steps: rotate\nPCA basis, truncate small entries, and update the rotation matrix. Its\nperformance bounds are also given. SPCArt is efficient, with each iteration\nscaling linearly with the data dimension. It is easy to choose parameters in\nSPCArt, due to its explicit physical explanations. Besides, we give a unified\nview to several existing sparse PCA methods and discuss the connection with\nSPCArt. Some ideas in SPCArt are extended to GPower, a popular sparse PCA\nalgorithm, to overcome its drawback. Experimental results demonstrate that\nSPCArt achieves the state-of-the-art performance. It also achieves a good\ntradeoff among various criteria, including sparsity, explained variance,\northogonality, balance of sparsity among loadings, and computational speed.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 12:37:49 GMT"}, {"version": "v2", "created": "Thu, 1 May 2014 04:05:18 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Hu", "Zhenfang", ""], ["Pan", "Gang", ""], ["Wang", "Yueming", ""], ["Wu", "Zhaohui", ""]]}, {"id": "1403.1600", "submitter": "Kai Zhu", "authors": "Kai Zhu, Rui Wu, Lei Ying, R. Srikant", "title": "Collaborative Filtering with Information-Rich and Information-Sparse\n  Entities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a popular model for collaborative filtering in\nrecommender systems where some users of a website rate some items, such as\nmovies, and the goal is to recover the ratings of some or all of the unrated\nitems of each user. In particular, we consider both the clustering model, where\nonly users (or items) are clustered, and the co-clustering model, where both\nusers and items are clustered, and further, we assume that some users rate many\nitems (information-rich users) and some users rate only a few items\n(information-sparse users). When users (or items) are clustered, our algorithm\ncan recover the rating matrix with $\\omega(MK \\log M)$ noisy entries while $MK$\nentries are necessary, where $K$ is the number of clusters and $M$ is the\nnumber of items. In the case of co-clustering, we prove that $K^2$ entries are\nnecessary for recovering the rating matrix, and our algorithm achieves this\nlower bound within a logarithmic factor when $K$ is sufficiently large. We\ncompare our algorithms with a well-known algorithms called alternating\nminimization (AM), and a similarity score-based algorithm known as the\npopularity-among-friends (PAF) algorithm by applying all three to the MovieLens\nand Netflix data sets. Our co-clustering algorithm and AM have similar overall\nerror rates when recovering the rating matrix, both of which are lower than the\nerror rate under PAF. But more importantly, the error rate of our co-clustering\nalgorithm is significantly lower than AM and PAF in the scenarios of interest\nin recommender systems: when recommending a few items to each user or when\nrecommending items to users who only rated a few items (these users are the\nmajority of the total user population). The performance difference increases\neven more when noise is added to the datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 21:51:48 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Zhu", "Kai", ""], ["Wu", "Rui", ""], ["Ying", "Lei", ""], ["Srikant", "R.", ""]]}, {"id": "1403.1863", "submitter": "Hanie Sedghi", "authors": "Hanie Sedghi and Edmond Jonckheere", "title": "Statistical Structure Learning, Towards a Robust Smart Grid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust control and maintenance of the grid relies on accurate data. Both PMUs\nand state estimators are prone to false data injection attacks. Thus, it is\ncrucial to have a mechanism for fast and accurate detection of an agent\nmaliciously tampering with the data---for both preventing attacks that may lead\nto blackouts, and for routine monitoring and control tasks of current and\nfuture grids. We propose a decentralized false data injection detection scheme\nbased on Markov graph of the bus phase angles. We utilize the Conditional\nCovariance Test (CCT) to learn the structure of the grid. Using the DC power\nflow model, we show that under normal circumstances, and because of\nwalk-summability of the grid graph, the Markov graph of the voltage angles can\nbe determined by the power grid graph. Therefore, a discrepancy between\ncalculated Markov graph and learned structure should trigger the alarm. Local\ngrid topology is available online from the protection system and we exploit it\nto check for mismatch. Should a mismatch be detected, we use correlation\nanomaly score to detect the set of attacked nodes. Our method can detect the\nmost recent stealthy deception attack on the power grid that assumes knowledge\nof bus-branch model of the system and is capable of deceiving the state\nestimator, damaging power network observatory, control, monitoring, demand\nresponse and pricing schemes. Specifically, under the stealthy deception\nattack, the Markov graph of phase angles changes. In addition to detect a state\nof attack, our method can detect the set of attacked nodes. To the best of our\nknowledge, our remedy is the first to comprehensively detect this sophisticated\nattack and it does not need additional hardware. Moreover, our detection scheme\nis successful no matter the size of the attacked subset. Simulation of various\npower networks confirms our claims.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 20:26:09 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Sedghi", "Hanie", ""], ["Jonckheere", "Edmond", ""]]}, {"id": "1403.1891", "submitter": "Lihong Li", "authors": "Lihong Li and Shunbao Chen and Jim Kleban and Ankur Gupta", "title": "Counterfactual Estimation and Optimization of Click Metrics for Search\n  Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing an interactive system against a predefined online metric is\nparticularly challenging, when the metric is computed from user feedback such\nas clicks and payments. The key challenge is the counterfactual nature: in the\ncase of Web search, any change to a component of the search engine may result\nin a different search result page for the same query, but we normally cannot\ninfer reliably from search log how users would react to the new result page.\nConsequently, it appears impossible to accurately estimate online metrics that\ndepend on user feedback, unless the new engine is run to serve users and\ncompared with a baseline in an A/B test. This approach, while valid and\nsuccessful, is unfortunately expensive and time-consuming. In this paper, we\npropose to address this problem using causal inference techniques, under the\ncontextual-bandit framework. This approach effectively allows one to run\n(potentially infinitely) many A/B tests offline from search log, making it\npossible to estimate and optimize online metrics quickly and inexpensively.\nFocusing on an important component in a commercial search engine, we show how\nthese ideas can be instantiated and applied, and obtain very promising results\nthat suggest the wide applicability of these techniques.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 22:54:52 GMT"}, {"version": "v2", "created": "Wed, 12 Mar 2014 06:36:02 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Li", "Lihong", ""], ["Chen", "Shunbao", ""], ["Kleban", "Jim", ""], ["Gupta", "Ankur", ""]]}, {"id": "1403.1893", "submitter": "Michael Smith", "authors": "Michael R. Smith and Tony Martinez", "title": "Becoming More Robust to Label Noise with Classifier Diversity", "comments": "37 pages, 10 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely known in the machine learning community that class noise can be\n(and often is) detrimental to inducing a model of the data. Many current\napproaches use a single, often biased, measurement to determine if an instance\nis noisy. A biased measure may work well on certain data sets, but it can also\nbe less effective on a broader set of data sets. In this paper, we present\nnoise identification using classifier diversity (NICD) -- a method for deriving\na less biased noise measurement and integrating it into the learning process.\nTo lessen the bias of the noise measure, NICD selects a diverse set of\nclassifiers (based on their predictions of novel instances) to determine which\ninstances are noisy. We examine NICD as a technique for filtering, instance\nweighting, and selecting the base classifiers of a voting ensemble. We compare\nNICD with several other noise handling techniques that do not consider\nclassifier diversity on a set of 54 data sets and 5 learning algorithms. NICD\nsignificantly increases the classification accuracy over the other considered\napproaches and is effective across a broad set of data sets and learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 22:58:48 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Smith", "Michael R.", ""], ["Martinez", "Tony", ""]]}, {"id": "1403.1942", "submitter": "Chandrima Sarkar", "authors": "Chandrima Sarkar, Jaideep Srivastava", "title": "Predictive Overlapping Co-Clustering", "comments": "This paper has been withdrawn by the authors due to a crucial sign\n  error in objective function", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years co-clustering has emerged as an important data mining\ntool for two way data analysis. Co-clustering is more advantageous over\ntraditional one dimensional clustering in many ways such as, ability to find\nhighly correlated sub-groups of rows and columns. However, one of the\noverlooked benefits of co-clustering is that, it can be used to extract\nmeaningful knowledge for various other knowledge extraction purposes. For\nexample, building predictive models with high dimensional data and\nheterogeneous population is a non-trivial task. Co-clusters extracted from such\ndata, which shows similar pattern in both the dimension, can be used for a more\naccurate predictive model building. Several applications such as finding\npatient-disease cohorts in health care analysis, finding user-genre groups in\nrecommendation systems and community detection problems can benefit from\nco-clustering technique that utilizes the predictive power of the data to\ngenerate co-clusters for improved data analysis.\n  In this paper, we present the novel idea of Predictive Overlapping\nCo-Clustering (POCC) as an optimization problem for a more effective and\nimproved predictive analysis. Our algorithm generates optimal co-clusters by\nmaximizing predictive power of the co-clusters subject to the constraints on\nthe number of row and column clusters. In this paper precision, recall and\nf-measure have been used as evaluation measures of the resulting co-clusters.\nResults of our algorithm has been compared with two other well-known techniques\n- K-means and Spectral co-clustering, over four real data set namely, Leukemia,\nInternet-Ads, Ovarian cancer and MovieLens data set. The results demonstrate\nthe effectiveness and utility of our algorithm POCC in practice.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 07:07:12 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2014 20:04:30 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Sarkar", "Chandrima", ""], ["Srivastava", "Jaideep", ""]]}, {"id": "1403.1944", "submitter": "Ping Li PhD", "authors": "Ping Li and Hong Li and Min Wu", "title": "Multi-label ensemble based on variable pairwise constraint projection", "comments": "19 pages,5 tables, 2 figures; Published with Information Sciences\n  (INS)", "journal-ref": "Information Sciences, 222, 2013, pp.269-281.(Available online 7\n  August 2012)", "doi": "10.1016/j.ins.2012.07.066", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification has attracted an increasing amount of attention in\nrecent years. To this end, many algorithms have been developed to classify\nmulti-label data in an effective manner. However, they usually do not consider\nthe pairwise relations indicated by sample labels, which actually play\nimportant roles in multi-label classification. Inspired by this, we naturally\nextend the traditional pairwise constraints to the multi-label scenario via a\nflexible thresholding scheme. Moreover, to improve the generalization ability\nof the classifier, we adopt a boosting-like strategy to construct a multi-label\nensemble from a group of base classifiers. To achieve these goals, this paper\npresents a novel multi-label classification framework named Variable Pairwise\nConstraint projection for Multi-label Ensemble (VPCME). Specifically, we take\nadvantage of the variable pairwise constraint projection to learn a\nlower-dimensional data representation, which preserves the correlations between\nsamples and labels. Thereafter, the base classifiers are trained in the new\ndata space. For the boosting-like strategy, we employ both the variable\npairwise constraints and the bootstrap steps to diversify the base classifiers.\nEmpirical studies have shown the superiority of the proposed method in\ncomparison with other approaches.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 07:20:05 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Li", "Ping", ""], ["Li", "Hong", ""], ["Wu", "Min", ""]]}, {"id": "1403.1946", "submitter": "Mehdi Naseriparsa", "authors": "Mehdi Naseriparsa, Amir-masoud Bidgoli, Touraj Varaee", "title": "Improving Performance of a Group of Classification Algorithms Using\n  Resampling and Feature Selection", "comments": "7 pages", "journal-ref": "World of Computer Science and Information Technology Journal,Vol\n  3, No 4,pp 70-76,2013", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years the importance of finding a meaningful pattern from huge\ndatasets has become more challenging. Data miners try to adopt innovative\nmethods to face this problem by applying feature selection methods. In this\npaper we propose a new hybrid method in which we use a combination of\nresampling, filtering the sample domain and wrapper subset evaluation method\nwith genetic search to reduce dimensions of Lung-Cancer dataset that we\nreceived from UCI Repository of Machine Learning databases. Finally, we apply\nsome well- known classification algorithms (Na\\\"ive Bayes, Logistic, Multilayer\nPerceptron, Best First Decision Tree and JRIP) to the resulting dataset and\ncompare the results and prediction rates before and after the application of\nour feature selection method on that dataset. The results show a substantial\nprogress in the average performance of five classification algorithms\nsimultaneously and the classification error for these classifiers decreases\nconsiderably. The experiments also show that this method outperforms other\nfeature selection methods with a lower cost.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 07:47:44 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Naseriparsa", "Mehdi", ""], ["Bidgoli", "Amir-masoud", ""], ["Varaee", "Touraj", ""]]}, {"id": "1403.1949", "submitter": "Mehdi Naseriparsa", "authors": "Mehdi Naseriparsa, Mohammad Mansour Riahi Kashani", "title": "Combination of PCA with SMOTE Resampling to Boost the Prediction Rate in\n  Lung Cancer Dataset", "comments": "6 pages. arXiv admin note: text overlap with arXiv:1106.1813,\n  arXiv:1001.1446 by other authors", "journal-ref": "International Journal of Computer Applications,Vol 77,No 3,pp\n  33-38,2013", "doi": "10.5120/13376-0987", "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification algorithms are unable to make reliable models on the datasets\nwith huge sizes. These datasets contain many irrelevant and redundant features\nthat mislead the classifiers. Furthermore, many huge datasets have imbalanced\nclass distribution which leads to bias over majority class in the\nclassification process. In this paper combination of unsupervised\ndimensionality reduction methods with resampling is proposed and the results\nare tested on Lung-Cancer dataset. In the first step PCA is applied on\nLung-Cancer dataset to compact the dataset and eliminate irrelevant features\nand in the second step SMOTE resampling is carried out to balance the class\ndistribution and increase the variety of sample domain. Finally, Naive Bayes\nclassifier is applied on the resulting dataset and the results are compared and\nevaluation metrics are calculated. The experiments show the effectiveness of\nthe proposed method across four evaluation metrics: Overall accuracy, False\nPositive Rate, Precision, Recall.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 08:12:54 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Naseriparsa", "Mehdi", ""], ["Kashani", "Mohammad Mansour Riahi", ""]]}, {"id": "1403.2065", "submitter": "Jian Yu", "authors": "Jian Yu, Zongben Xu", "title": "Categorization Axioms for Clustering Results", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis has attracted more and more attention in the field of\nmachine learning and data mining. Numerous clustering algorithms have been\nproposed and are being developed due to diverse theories and various\nrequirements of emerging applications. Therefore, it is very worth establishing\nan unified axiomatic framework for data clustering. In the literature, it is an\nopen problem and has been proved very challenging. In this paper, clustering\nresults are axiomatized by assuming that an proper clustering result should\nsatisfy categorization axioms. The proposed axioms not only introduce\nclassification of clustering results and inequalities of clustering results,\nbut also are consistent with prototype theory and exemplar theory of\ncategorization models in cognitive science. Moreover, the proposed axioms lead\nto three principles of designing clustering algorithm and cluster validity\nindex, which follow many popular clustering algorithms and cluster validity\nindices.\n", "versions": [{"version": "v1", "created": "Sun, 9 Mar 2014 14:51:53 GMT"}, {"version": "v2", "created": "Sun, 30 Mar 2014 07:55:32 GMT"}, {"version": "v3", "created": "Mon, 21 Apr 2014 02:22:10 GMT"}, {"version": "v4", "created": "Sat, 26 Apr 2014 09:40:42 GMT"}, {"version": "v5", "created": "Mon, 5 May 2014 12:40:33 GMT"}, {"version": "v6", "created": "Mon, 7 Jul 2014 07:25:39 GMT"}, {"version": "v7", "created": "Wed, 28 Oct 2015 00:48:28 GMT"}, {"version": "v8", "created": "Thu, 14 Jan 2016 22:54:32 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Yu", "Jian", ""], ["Xu", "Zongben", ""]]}, {"id": "1403.2295", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain", "title": "Sublinear Models for Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution extends linear models for feature vectors to sublinear\nmodels for graphs and analyzes their properties. The results are (i) a\ngeometric interpretation of sublinear classifiers, (ii) a generic learning rule\nbased on the principle of empirical risk minimization, (iii) a convergence\ntheorem for the margin perceptron in the sublinearly separable case, and (iv)\nthe VC-dimension of sublinear functions. Empirical results on graph data show\nthat sublinear models on graphs have similar properties as linear models for\nfeature vectors.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 16:36:23 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Jain", "Brijnesh J.", ""]]}, {"id": "1403.2372", "submitter": "Mehdi Naseriparsa", "authors": "Mehdi Naseriparsa, Amir-Masoud Bidgoli, Touraj Varaee", "title": "A Hybrid Feature Selection Method to Improve Performance of a Group of\n  Classification Algorithms", "comments": "8 pages. arXiv admin note: substantial text overlap with\n  arXiv:1403.1946; and text overlap with arXiv:1106.1813 by other authors", "journal-ref": "International Journal of Computer Applications,Vol 69,No 17,pp\n  28-35,2013", "doi": "10.5120/12065-8172", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a hybrid feature selection method is proposed which takes\nadvantages of wrapper subset evaluation with a lower cost and improves the\nperformance of a group of classifiers. The method uses combination of sample\ndomain filtering and resampling to refine the sample domain and two feature\nsubset evaluation methods to select reliable features. This method utilizes\nboth feature space and sample domain in two phases. The first phase filters and\nresamples the sample domain and the second phase adopts a hybrid procedure by\ninformation gain, wrapper subset evaluation and genetic search to find the\noptimal feature space. Experiments carried out on different types of datasets\nfrom UCI Repository of Machine Learning databases and the results show a rise\nin the average performance of five classifiers (Naive Bayes, Logistic,\nMultilayer Perceptron, Best First Decision Tree and JRIP) simultaneously and\nthe classification error for these classifiers decreases considerably. The\nexperiments also show that this method outperforms other feature selection\nmethods with a lower cost.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 08:04:29 GMT"}], "update_date": "2014-03-12", "authors_parsed": [["Naseriparsa", "Mehdi", ""], ["Bidgoli", "Amir-Masoud", ""], ["Varaee", "Touraj", ""]]}, {"id": "1403.2433", "submitter": "Mark Reid", "authors": "Mark D. Reid and Rafael M. Frongillo and Robert C. Williamson", "title": "Generalised Mixability, Constant Regret, and Bayesian Updating", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixability of a loss is known to characterise when constant regret bounds are\nachievable in games of prediction with expert advice through the use of Vovk's\naggregating algorithm. We provide a new interpretation of mixability via convex\nanalysis that highlights the role of the Kullback-Leibler divergence in its\ndefinition. This naturally generalises to what we call $\\Phi$-mixability where\nthe Bregman divergence $D_\\Phi$ replaces the KL divergence. We prove that\nlosses that are $\\Phi$-mixable also enjoy constant regret bounds via a\ngeneralised aggregating algorithm that is similar to mirror descent.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 22:55:11 GMT"}], "update_date": "2014-03-12", "authors_parsed": [["Reid", "Mark D.", ""], ["Frongillo", "Rafael M.", ""], ["Williamson", "Robert C.", ""]]}, {"id": "1403.2484", "submitter": "Meng Fang", "authors": "Meng Fang, Jie Yin, Xingquan Zhu", "title": "Transfer Learning across Networks for Collective Classification", "comments": "Published in the proceedings of IEEE ICDM 2013", "journal-ref": null, "doi": "10.1109/ICDM.2013.116", "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of transferring useful knowledge from a\nsource network to predict node labels in a newly formed target network. While\nexisting transfer learning research has primarily focused on vector-based data,\nin which the instances are assumed to be independent and identically\ndistributed, how to effectively transfer knowledge across different information\nnetworks has not been well studied, mainly because networks may have their\ndistinct node features and link relationships between nodes. In this paper, we\npropose a new transfer learning algorithm that attempts to transfer common\nlatent structure features across the source and target networks. The proposed\nalgorithm discovers these latent features by constructing label propagation\nmatrices in the source and target networks, and mapping them into a shared\nlatent feature space. The latent features capture common structure patterns\nshared by two networks, and serve as domain-independent features to be\ntransferred between networks. Together with domain-dependent node features, we\nthereafter propose an iterative classification algorithm that leverages label\ncorrelations to predict node labels in the target network. Experiments on\nreal-world networks demonstrate that our proposed algorithm can successfully\nachieve knowledge transfer between networks to help improve the accuracy of\nclassifying nodes in the target network.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2014 06:49:56 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Fang", "Meng", ""], ["Yin", "Jie", ""], ["Zhu", "Xingquan", ""]]}, {"id": "1403.2485", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Richard Nock", "title": "Optimal interval clustering: Application to Bregman clustering and\n  statistical mixture learning", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generic dynamic programming method to compute the optimal\nclustering of $n$ scalar elements into $k$ pairwise disjoint intervals. This\ncase includes 1D Euclidean $k$-means, $k$-medoids, $k$-medians, $k$-centers,\netc. We extend the method to incorporate cluster size constraints and show how\nto choose the appropriate $k$ by model selection. Finally, we illustrate and\nrefine the method on two case studies: Bregman clustering and statistical\nmixture learning maximizing the complete likelihood.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2014 06:52:04 GMT"}, {"version": "v2", "created": "Mon, 26 May 2014 03:48:59 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Nielsen", "Frank", ""], ["Nock", "Richard", ""]]}, {"id": "1403.2654", "submitter": "Yanping Chen", "authors": "Yanping Chen, Adena Why, Gustavo Batista, Agenor Mafra-Neto, Eamonn\n  Keogh", "title": "Flying Insect Classification with Inexpensive Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The ability to use inexpensive, noninvasive sensors to accurately classify\nflying insects would have significant implications for entomological research,\nand allow for the development of many useful applications in vector control for\nboth medical and agricultural entomology. Given this, the last sixty years have\nseen many research efforts on this task. To date, however, none of this\nresearch has had a lasting impact. In this work, we explain this lack of\nprogress. We attribute the stagnation on this problem to several factors,\nincluding the use of acoustic sensing devices, the over-reliance on the single\nfeature of wingbeat frequency, and the attempts to learn complex models with\nrelatively little data. In contrast, we show that pseudo-acoustic optical\nsensors can produce vastly superior data, that we can exploit additional\nfeatures, both intrinsic and extrinsic to the insect's flight behavior, and\nthat a Bayesian classification approach allows us to efficiently learn\nclassification models that are very robust to over-fitting. We demonstrate our\nfindings with large scale experiments that dwarf all previous works combined,\nas measured by the number of insects and the number of species considered.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2014 18:36:39 GMT"}], "update_date": "2014-03-12", "authors_parsed": [["Chen", "Yanping", ""], ["Why", "Adena", ""], ["Batista", "Gustavo", ""], ["Mafra-Neto", "Agenor", ""], ["Keogh", "Eamonn", ""]]}, {"id": "1403.2660", "submitter": "Stanislav Minsker", "authors": "Stanislav Minsker, Sanvesh Srivastava, Lizhen Lin and David B. Dunson", "title": "Robust and Scalable Bayes via a Median of Subset Posterior Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DC cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to Bayesian analysis that is provably robust to\noutliers in the data and often has computational advantages over standard\nmethods. Our technique is based on splitting the data into non-overlapping\nsubgroups, evaluating the posterior distribution given each independent\nsubgroup, and then combining the resulting measures. The main novelty of our\napproach is the proposed aggregation step, which is based on the evaluation of\na median in the space of probability measures equipped with a suitable\ncollection of distances that can be quickly and efficiently evaluated in\npractice. We present both theoretical and numerical evidence illustrating the\nimprovements achieved by our method.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2014 17:37:18 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 21:56:34 GMT"}, {"version": "v3", "created": "Thu, 2 Jun 2016 00:59:28 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Minsker", "Stanislav", ""], ["Srivastava", "Sanvesh", ""], ["Lin", "Lizhen", ""], ["Dunson", "David B.", ""]]}, {"id": "1403.2802", "submitter": "Zhimin Cao", "authors": "Haoqiang Fan, Zhimin Cao, Yuning Jiang, Qi Yin, Chinchilla Doudou", "title": "Learning Deep Face Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face representation is a crucial step of face recognition systems. An optimal\nface representation should be discriminative, robust, compact, and very\neasy-to-implement. While numerous hand-crafted and learning-based\nrepresentations have been proposed, considerable room for improvement is still\npresent. In this paper, we present a very easy-to-implement deep learning\nframework for face representation. Our method bases on a new structure of deep\nnetwork (called Pyramid CNN). The proposed Pyramid CNN adopts a\ngreedy-filter-and-down-sample operation, which enables the training procedure\nto be very fast and computation-efficient. In addition, the structure of\nPyramid CNN can naturally incorporate feature sharing across multi-scale face\nrepresentations, increasing the discriminative ability of resulting\nrepresentation. Our basic network is capable of achieving high recognition\naccuracy ($85.8\\%$ on LFW benchmark) with only 8 dimension representation. When\nextended to feature-sharing Pyramid CNN, our system achieves the\nstate-of-the-art performance ($97.3\\%$) on LFW benchmark. We also introduce a\nnew benchmark of realistic face images on social network and validate our\nproposed representation has a good ability of generalization.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 03:47:18 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Fan", "Haoqiang", ""], ["Cao", "Zhimin", ""], ["Jiang", "Yuning", ""], ["Yin", "Qi", ""], ["Doudou", "Chinchilla", ""]]}, {"id": "1403.2877", "submitter": "Carlos Oscar Sorzano S.", "authors": "C.O.S. Sorzano, J. Vargas, A. Pascual Montano", "title": "A survey of dimensionality reduction techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental life sciences like biology or chemistry have seen in the recent\ndecades an explosion of the data available from experiments. Laboratory\ninstruments become more and more complex and report hundreds or thousands\nmeasurements for a single experiment and therefore the statistical methods face\nchallenging tasks when dealing with such high dimensional data. However, much\nof the data is highly redundant and can be efficiently brought down to a much\nsmaller number of variables without a significant loss of information. The\nmathematical procedures making possible this reduction are called\ndimensionality reduction techniques; they have widely been developed by fields\nlike Statistics or Machine Learning, and are currently a hot research topic. In\nthis review we categorize the plethora of dimension reduction techniques\navailable and give the mathematical insight behind them.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 10:35:15 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Sorzano", "C. O. S.", ""], ["Vargas", "J.", ""], ["Montano", "A. Pascual", ""]]}, {"id": "1403.2950", "submitter": "Saleema J S", "authors": "J S Saleema, N Bhagawathi, S Monica, P Deepa Shenoy, K R Venugopal and\n  L M Patnaik", "title": "Cancer Prognosis Prediction Using Balanced Stratified Sampling", "comments": null, "journal-ref": "International Journal on Soft Computing, Artificial Intelligence\n  and Applications (IJSCAI), Vol.3, No. 1, February 2014, pp 9-18", "doi": "10.5121/ijscai.2014.3102", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High accuracy in cancer prediction is important to improve the quality of the\ntreatment and to improve the rate of survivability of patients. As the data\nvolume is increasing rapidly in the healthcare research, the analytical\nchallenge exists in double. The use of effective sampling technique in\nclassification algorithms always yields good prediction accuracy. The SEER\npublic use cancer database provides various prominent class labels for\nprognosis prediction. The main objective of this paper is to find the effect of\nsampling techniques in classifying the prognosis variable and propose an ideal\nsampling method based on the outcome of the experimentation. In the first phase\nof this work the traditional random sampling and stratified sampling techniques\nhave been used. At the next level the balanced stratified sampling with\nvariations as per the choice of the prognosis class labels have been tested.\nMuch of the initial time has been focused on performing the pre_processing of\nthe SEER data set. The classification model for experimentation has been built\nusing the breast cancer, respiratory cancer and mixed cancer data sets with\nthree traditional classifiers namely Decision Tree, Naive Bayes and K-Nearest\nNeighbor. The three prognosis factors survival, stage and metastasis have been\nused as class labels for experimental comparisons. The results shows a steady\nincrease in the prediction accuracy of balanced stratified model as the sample\nsize increases, but the traditional approach fluctuates before the optimum\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 14:33:43 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Saleema", "J S", ""], ["Bhagawathi", "N", ""], ["Monica", "S", ""], ["Shenoy", "P Deepa", ""], ["Venugopal", "K R", ""], ["Patnaik", "L M", ""]]}, {"id": "1403.3080", "submitter": "Xi Chen", "authors": "Xi Chen, Qihang Lin, Dengyong Zhou", "title": "Statistical Decision Making for Optimal Budget Allocation in Crowd\n  Labeling", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In crowd labeling, a large amount of unlabeled data instances are outsourced\nto a crowd of workers. Workers will be paid for each label they provide, but\nthe labeling requester usually has only a limited amount of the budget. Since\ndata instances have different levels of labeling difficulty and workers have\ndifferent reliability, it is desirable to have an optimal policy to allocate\nthe budget among all instance-worker pairs such that the overall labeling\naccuracy is maximized. We consider categorical labeling tasks and formulate the\nbudget allocation problem as a Bayesian Markov decision process (MDP), which\nsimultaneously conducts learning and decision making. Using the dynamic\nprogramming (DP) recurrence, one can obtain the optimal allocation policy.\nHowever, DP quickly becomes computationally intractable when the size of the\nproblem increases. To solve this challenge, we propose a computationally\nefficient approximate policy, called optimistic knowledge gradient policy. Our\nMDP is a quite general framework, which applies to both pull crowdsourcing\nmarketplaces with homogeneous workers and push marketplaces with heterogeneous\nworkers. It can also incorporate the contextual information of instances when\nthey are available. The experiments on both simulated and real data show that\nthe proposed policy achieves a higher labeling accuracy than other existing\npolicies at the same budget level.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 19:55:00 GMT"}, {"version": "v2", "created": "Thu, 24 Apr 2014 08:52:28 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Chen", "Xi", ""], ["Lin", "Qihang", ""], ["Zhou", "Dengyong", ""]]}, {"id": "1403.3109", "submitter": "Cem Aksoylar", "authors": "Cem Aksoylar and Venkatesh Saligrama", "title": "Sparse Recovery with Linear and Nonlinear Observations: Dependent and\n  Noisy Data", "comments": "Extended version of the paper that was accepted to AISTATS 2014 as\n  \"Information-Theoretic Characterization of Sparse Recovery\". arXiv admin\n  note: text overlap with arXiv:1304.0682", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate sparse support recovery as a salient set identification problem\nand use information-theoretic analyses to characterize the recovery performance\nand sample complexity. We consider a very general model where we are not\nrestricted to linear models or specific distributions. We state non-asymptotic\nbounds on recovery probability and a tight mutual information formula for\nsample complexity. We evaluate our bounds for applications such as sparse\nlinear regression and explicitly characterize effects of correlation or noisy\nfeatures on recovery performance. We show improvements upon previous work and\nidentify gaps between the performance of recovery algorithms and fundamental\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 20:32:02 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Aksoylar", "Cem", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1403.3342", "submitter": "Michael Smith", "authors": "Michael R. Smith and Tony Martinez and Christophe Giraud-Carrier", "title": "The Potential Benefits of Filtering Versus Hyper-Parameter Optimization", "comments": "11 pages, 4 tables, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of an induced model by a learning algorithm is dependent on the\nquality of the training data and the hyper-parameters supplied to the learning\nalgorithm. Prior work has shown that improving the quality of the training data\n(i.e., by removing low quality instances) or tuning the learning algorithm\nhyper-parameters can significantly improve the quality of an induced model. A\ncomparison of the two methods is lacking though. In this paper, we estimate and\ncompare the potential benefits of filtering and hyper-parameter optimization.\nEstimating the potential benefit gives an overly optimistic estimate but also\nempirically demonstrates an approximation of the maximum potential benefit of\neach method. We find that, while both significantly improve the induced model,\nimproving the quality of the training set has a greater potential effect than\nhyper-parameter optimization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 17:48:19 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Smith", "Michael R.", ""], ["Martinez", "Tony", ""], ["Giraud-Carrier", "Christophe", ""]]}, {"id": "1403.3371", "submitter": "Hamed Firouzi", "authors": "Hamed Firouzi, Dennis Wei, Alfred O. Hero III", "title": "Spectral Correlation Hub Screening of Multivariate Time Series", "comments": "32 pages, To appear in Excursions in Harmonic Analysis: The February\n  Fourier Talks at the Norbert Wiener Center", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter discusses correlation analysis of stationary multivariate\nGaussian time series in the spectral or Fourier domain. The goal is to identify\nthe hub time series, i.e., those that are highly correlated with a specified\nnumber of other time series. We show that Fourier components of the time series\nat different frequencies are asymptotically statistically independent. This\nproperty permits independent correlation analysis at each frequency,\nalleviating the computational and statistical challenges of high-dimensional\ntime series. To detect correlation hubs at each frequency, an existing\ncorrelation screening method is extended to the complex numbers to accommodate\ncomplex-valued Fourier components. We characterize the number of hub\ndiscoveries at specified correlation and degree thresholds in the regime of\nincreasing dimension and fixed sample size. The theory specifies appropriate\nthresholds to apply to sample correlation matrices to detect hubs and also\nallows statistical significance to be attributed to hub discoveries. Numerical\nresults illustrate the accuracy of the theory and the usefulness of the\nproposed spectral framework.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 19:01:28 GMT"}, {"version": "v2", "created": "Wed, 9 Apr 2014 16:25:31 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Firouzi", "Hamed", ""], ["Wei", "Dennis", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1403.3378", "submitter": "Siong Thye Goh", "authors": "Siong Thye Goh, Cynthia Rudin", "title": "Box Drawings for Learning with Imbalanced Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of real world classification problems are imbalanced,\nmeaning there are far fewer data from the class of interest (the positive\nclass) than from other classes. We propose two machine learning algorithms to\nhandle highly imbalanced classification problems. The classifiers constructed\nby both methods are created as unions of parallel axis rectangles around the\npositive examples, and thus have the benefit of being interpretable. The first\nalgorithm uses mixed integer programming to optimize a weighted balance between\npositive and negative class accuracies. Regularization is introduced to improve\ngeneralization performance. The second method uses an approximation in order to\nassist with scalability. Specifically, it follows a \\textit{characterize then\ndiscriminate} approach, where the positive class is characterized first by\nboxes, and then each box boundary becomes a separate discriminative classifier.\nThis method has the computational advantages that it can be easily\nparallelized, and considers only the relevant regions of feature space.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 19:28:48 GMT"}, {"version": "v2", "created": "Sat, 7 Jun 2014 15:01:07 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Goh", "Siong Thye", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1403.3460", "submitter": "Chi Wang", "authors": "Chi Wang, Xueqing Liu, Yanglei Song, Jiawei Han", "title": "Scalable and Robust Construction of Topical Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated generation of high-quality topical hierarchies for a text\ncollection is a dream problem in knowledge engineering with many valuable\napplications. In this paper a scalable and robust algorithm is proposed for\nconstructing a hierarchy of topics from a text collection. We divide and\nconquer the problem using a top-down recursive framework, based on a tensor\northogonal decomposition technique. We solve a critical challenge to perform\nscalable inference for our newly designed hierarchical topic model. Experiments\nwith various real-world datasets illustrate its ability to generate robust,\nhigh-quality hierarchies efficiently. Our method reduces the time of\nconstruction by several orders of magnitude, and its robust feature renders it\npossible for users to interactively revise the hierarchy.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 23:22:21 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Wang", "Chi", ""], ["Liu", "Xueqing", ""], ["Song", "Yanglei", ""], ["Han", "Jiawei", ""]]}, {"id": "1403.3465", "submitter": "Hugh Brendan McMahan", "authors": "H. Brendan McMahan", "title": "A Survey of Algorithms and Analysis for Adaptive Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present tools for the analysis of Follow-The-Regularized-Leader (FTRL),\nDual Averaging, and Mirror Descent algorithms when the regularizer\n(equivalently, prox-function or learning rate schedule) is chosen adaptively\nbased on the data. Adaptivity can be used to prove regret bounds that hold on\nevery round, and also allows for data-dependent regret bounds as in\nAdaGrad-style algorithms (e.g., Online Gradient Descent with adaptive\nper-coordinate learning rates). We present results from a large number of prior\nworks in a unified manner, using a modular and tight analysis that isolates the\nkey arguments in easily re-usable lemmas. This approach strengthens pre-viously\nknown FTRL analysis techniques to produce bounds as tight as those achieved by\npotential functions or primal-dual analysis. Further, we prove a general and\nexact equivalence between an arbitrary adaptive Mirror Descent algorithm and a\ncorrespond- ing FTRL update, which allows us to analyze any Mirror Descent\nalgorithm in the same framework. The key to bridging the gap between Dual\nAveraging and Mirror Descent algorithms lies in an analysis of the\nFTRL-Proximal algorithm family. Our regret bounds are proved in the most\ngeneral form, holding for arbitrary norms and non-smooth regularizers with\ntime-varying weight.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2014 00:25:03 GMT"}, {"version": "v2", "created": "Mon, 13 Oct 2014 18:31:01 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2015 17:32:51 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["McMahan", "H. Brendan", ""]]}, {"id": "1403.3610", "submitter": "Naresh Manwani", "authors": "Aritra Ghosh, Naresh Manwani and P. S. Sastry", "title": "Making Risk Minimization Tolerant to Label Noise", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2014.09.081", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, the training data, from which one needs to learn a\nclassifier, is corrupted with label noise. Many standard algorithms such as SVM\nperform poorly in presence of label noise. In this paper we investigate the\nrobustness of risk minimization to label noise. We prove a sufficient condition\non a loss function for the risk minimization under that loss to be tolerant to\nuniform label noise. We show that the $0-1$ loss, sigmoid loss, ramp loss and\nprobit loss satisfy this condition though none of the standard convex loss\nfunctions satisfy it. We also prove that, by choosing a sufficiently large\nvalue of a parameter in the loss function, the sigmoid loss, ramp loss and\nprobit loss can be made tolerant to non-uniform label noise also if we can\nassume the classes to be separable under noise-free data distribution. Through\nextensive empirical studies, we show that risk minimization under the $0-1$\nloss, the sigmoid loss and the ramp loss has much better robustness to label\nnoise when compared to the SVM algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2014 15:30:23 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 06:33:57 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Ghosh", "Aritra", ""], ["Manwani", "Naresh", ""], ["Sastry", "P. S.", ""]]}, {"id": "1403.3628", "submitter": "Remi Flamary", "authors": "R\\'emi Flamary (LAGRANGE), Nisrine Jrad (GIPSA-lab), Ronald Phlypo\n  (GIPSA-lab), Marco Congedo (GIPSA-lab), Alain Rakotomamonjy (LITIS)", "title": "Mixed-norm Regularization for Brain Decoding", "comments": "Computational and Mathematical Methods in Medicine (2014)\n  http://www.hindawi.com/journals/cmmm/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the use of mixed-norm regularization for sensor\nselection in Event-Related Potential (ERP) based Brain-Computer Interfaces\n(BCI). The classification problem is cast as a discriminative optimization\nframework where sensor selection is induced through the use of mixed-norms.\nThis framework is extended to the multi-task learning situation where several\nsimilar classification tasks related to different subjects are learned\nsimultaneously. In this case, multi-task learning helps in leveraging data\nscarcity issue yielding to more robust classifiers. For this purpose, we have\nintroduced a regularizer that induces both sensor selection and classifier\nsimilarities. The different regularization approaches are compared on three ERP\ndatasets showing the interest of mixed-norm regularization in terms of sensor\nselection. The multi-task approaches are evaluated when a small number of\nlearning examples are available yielding to significant performance\nimprovements especially for subjects performing poorly.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2014 16:15:24 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Flamary", "R\u00e9mi", "", "LAGRANGE"], ["Jrad", "Nisrine", "", "GIPSA-lab"], ["Phlypo", "Ronald", "", "GIPSA-lab"], ["Congedo", "Marco", "", "GIPSA-lab"], ["Rakotomamonjy", "Alain", "", "LITIS"]]}, {"id": "1403.3707", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Christopher Cole, Jennifer Neville", "title": "Learning the Latent State Space of Time-Varying Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From social networks to Internet applications, a wide variety of electronic\ncommunication tools are producing streams of graph data; where the nodes\nrepresent users and the edges represent the contacts between them over time.\nThis has led to an increased interest in mechanisms to model the dynamic\nstructure of time-varying graphs. In this work, we develop a framework for\nlearning the latent state space of a time-varying email graph. We show how the\nframework can be used to find subsequences that correspond to global real-time\nevents in the Email graph (e.g. vacations, breaks, ...etc.). These events\nimpact the underlying graph process to make its characteristics non-stationary.\nWithin the framework, we compare two different representations of the temporal\nrelationships; discrete vs. probabilistic. We use the two representations as\ninputs to a mixture model to learn the latent state transitions that correspond\nto important changes in the Email graph structure over time.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2014 20:37:06 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Cole", "Christopher", ""], ["Neville", "Jennifer", ""]]}, {"id": "1403.3741", "submitter": "Ian Osband", "authors": "Ian Osband, Benjamin Van Roy", "title": "Near-optimal Reinforcement Learning in Factored MDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any reinforcement learning algorithm that applies to all Markov decision\nprocesses (MDPs) will suffer $\\Omega(\\sqrt{SAT})$ regret on some MDP, where $T$\nis the elapsed time and $S$ and $A$ are the cardinalities of the state and\naction spaces. This implies $T = \\Omega(SA)$ time to guarantee a near-optimal\npolicy. In many settings of practical interest, due to the curse of\ndimensionality, $S$ and $A$ can be so enormous that this learning time is\nunacceptable. We establish that, if the system is known to be a \\emph{factored}\nMDP, it is possible to achieve regret that scales polynomially in the number of\n\\emph{parameters} encoding the factored MDP, which may be exponentially smaller\nthan $S$ or $A$. We provide two algorithms that satisfy near-optimal regret\nbounds in this context: posterior sampling reinforcement learning (PSRL) and an\nupper confidence bound algorithm (UCRL-Factored).\n", "versions": [{"version": "v1", "created": "Sat, 15 Mar 2014 01:56:02 GMT"}, {"version": "v2", "created": "Fri, 6 Jun 2014 23:17:54 GMT"}, {"version": "v3", "created": "Fri, 31 Oct 2014 23:34:32 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1403.4017", "submitter": "Longqi Yang", "authors": "Longqi Yang, Yibing Wang, Zhisong Pan and Guyu Hu", "title": "Multi-task Feature Selection based Anomaly Detection", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network anomaly detection is still a vibrant research area. As the fast\ngrowth of network bandwidth and the tremendous traffic on the network, there\narises an extremely challengeable question: How to efficiently and accurately\ndetect the anomaly on multiple traffic? In multi-task learning, the traffic\nconsisting of flows at different time periods is considered as a task. Multiple\ntasks at different time periods performed simultaneously to detect anomalies.\nIn this paper, we apply the multi-task feature selection in network anomaly\ndetection area which provides a powerful method to gather information from\nmultiple traffic and detect anomalies on it simultaneously. In particular, the\nmulti-task feature selection includes the well-known l1-norm based feature\nselection as a special case given only one task. Moreover, we show that the\nmulti-task feature selection is more accurate by utilizing more information\nsimultaneously than the l1-norm based method. At the evaluation stage, we\npreprocess the raw data trace from trans-Pacific backbone link between Japan\nand the United States, label with anomaly communities, and generate a\n248-feature dataset. We show empirically that the multi-task feature selection\noutperforms independent l1-norm based feature selection on real traffic\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 08:04:41 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Yang", "Longqi", ""], ["Wang", "Yibing", ""], ["Pan", "Zhisong", ""], ["Hu", "Guyu", ""]]}, {"id": "1403.4224", "submitter": "Guillaume Rabusseau", "authors": "Guillaume Rabusseau and Fran\\c{c}ois Denis", "title": "Learning Negative Mixture Models by Tensor Decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers the problem of estimating the parameters of negative\nmixture models, i.e. mixture models that possibly involve negative weights. The\ncontributions of this paper are as follows. (i) We show that every rational\nprobability distributions on strings, a representation which occurs naturally\nin spectral learning, can be computed by a negative mixture of at most two\nprobabilistic automata (or HMMs). (ii) We propose a method to estimate the\nparameters of negative mixture models having a specific tensor structure in\ntheir low order observable moments. Building upon a recent paper on tensor\ndecompositions for learning latent variable models, we extend this work to the\nbroader setting of tensors having a symmetric decomposition with positive and\nnegative weights. We introduce a generalization of the tensor power method for\ncomplex valued tensors, and establish theoretical convergence guarantees. (iii)\nWe show how our approach applies to negative Gaussian mixture models, for which\nwe provide some experiments.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 19:35:06 GMT"}, {"version": "v2", "created": "Fri, 19 Sep 2014 05:32:30 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Rabusseau", "Guillaume", ""], ["Denis", "Fran\u00e7ois", ""]]}, {"id": "1403.4267", "submitter": "Cagdas Bilen", "authors": "Cagdas Bilen (INRIA - IRISA), Gilles Puy, R\\'emi Gribonval (INRIA -\n  IRISA), Laurent Daudet", "title": "Balancing Sparsity and Rank Constraints in Quadratic Basis Pursuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the methods that simultaneously enforce sparsity and low-rank\nstructure in a matrix as often employed for sparse phase retrieval problems or\nphase calibration problems in compressive sensing. We propose a new approach\nfor analyzing the trade off between the sparsity and low rank constraints in\nthese approaches which not only helps to provide guidelines to adjust the\nweights between the aforementioned constraints, but also enables new simulation\nstrategies for evaluating performance. We then provide simulation results for\nphase retrieval and phase calibration cases both to demonstrate the consistency\nof the proposed method with other approaches and to evaluate the change of\nperformance with different weights for the sparsity and low rank structure\nconstraints.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 20:34:18 GMT"}, {"version": "v2", "created": "Wed, 19 Mar 2014 14:07:22 GMT"}], "update_date": "2014-03-20", "authors_parsed": [["Bilen", "Cagdas", "", "INRIA - IRISA"], ["Puy", "Gilles", "", "INRIA -\n  IRISA"], ["Gribonval", "R\u00e9mi", "", "INRIA -\n  IRISA"], ["Daudet", "Laurent", ""]]}, {"id": "1403.4378", "submitter": "Debarghya Ghoshdastidar", "authors": "Debarghya Ghoshdastidar, Ambedkar Dukkipati, Ajay P. Adsul, Aparna S.\n  Vijayan", "title": "Spectral Clustering with Jensen-type kernels and their multi-point\n  extensions", "comments": "To appear in IEEE Computer Society Conference on Computer Vision and\n  Pattern Recognition", "journal-ref": null, "doi": "10.1109/CVPR.2014.191", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by multi-distribution divergences, which originate in information\ntheory, we propose a notion of `multi-point' kernels, and study their\napplications. We study a class of kernels based on Jensen type divergences and\nshow that these can be extended to measure similarity among multiple points. We\nstudy tensor flattening methods and develop a multi-point (kernel) spectral\nclustering (MSC) method. We further emphasize on a special case of the proposed\nkernels, which is a multi-point extension of the linear (dot-product) kernel\nand show the existence of cubic time tensor flattening algorithm in this case.\nFinally, we illustrate the usefulness of our contributions using standard data\nsets and image segmentation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2014 09:04:02 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Ghoshdastidar", "Debarghya", ""], ["Dukkipati", "Ambedkar", ""], ["Adsul", "Ajay P.", ""], ["Vijayan", "Aparna S.", ""]]}, {"id": "1403.4514", "submitter": "Prashanth L.A.", "authors": "Raphael Fonteneau and L.A. Prashanth", "title": "Simultaneous Perturbation Algorithms for Batch Off-Policy Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose novel policy search algorithms in the context of off-policy, batch\nmode reinforcement learning (RL) with continuous state and action spaces. Given\na batch collection of trajectories, we perform off-line policy evaluation using\nan algorithm similar to that by [Fonteneau et al., 2010]. Using this\nMonte-Carlo like policy evaluator, we perform policy search in a class of\nparameterized policies. We propose both first order policy gradient and second\norder policy Newton algorithms. All our algorithms incorporate simultaneous\nperturbation estimates for the gradient as well as the Hessian of the\ncost-to-go vector, since the latter is unknown and only biased estimates are\navailable. We demonstrate their practicality on a simple 1-dimensional\ncontinuous state space problem.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2014 15:57:48 GMT"}, {"version": "v2", "created": "Mon, 31 Mar 2014 23:02:53 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Fonteneau", "Raphael", ""], ["Prashanth", "L. A.", ""]]}, {"id": "1403.4540", "submitter": "Llu\\'is Belanche-Mu\\~noz", "authors": "Llu\\'is Belanche and Jer\\'onimo Hern\\'andez", "title": "Similarity networks for classification: a case study in the Horse Colic\n  problem", "comments": "16 pages, 1 figure Universitat Polit\\`ecnica de Catalunya preprint", "journal-ref": null, "doi": null, "report-no": "Technical Report LSI-14-4-R", "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a two-layer neural network in which the neuron model\ncomputes a user-defined similarity function between inputs and weights. The\nneuron transfer function is formed by composition of an adapted logistic\nfunction with the mean of the partial input-weight similarities. The resulting\nneuron model is capable of dealing directly with variables of potentially\ndifferent nature (continuous, fuzzy, ordinal, categorical). There is also\nprovision for missing values. The network is trained using a two-stage\nprocedure very similar to that used to train a radial basis function (RBF)\nneural network. The network is compared to two types of RBF networks in a\nnon-trivial dataset: the Horse Colic problem, taken as a case study and\nanalyzed in detail.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2014 17:15:21 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Belanche", "Llu\u00eds", ""], ["Hern\u00e1ndez", "Jer\u00f3nimo", ""]]}, {"id": "1403.4781", "submitter": "Subhadip Mukherjee", "authors": "Subhadip Mukherjee and Chandra Sekhar Seelamantula", "title": "A Split-and-Merge Dictionary Learning Algorithm for Sparse\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In big data image/video analytics, we encounter the problem of learning an\novercomplete dictionary for sparse representation from a large training\ndataset, which can not be processed at once because of storage and\ncomputational constraints. To tackle the problem of dictionary learning in such\nscenarios, we propose an algorithm for parallel dictionary learning. The\nfundamental idea behind the algorithm is to learn a sparse representation in\ntwo phases. In the first phase, the whole training dataset is partitioned into\nsmall non-overlapping subsets, and a dictionary is trained independently on\neach small database. In the second phase, the dictionaries are merged to form a\nglobal dictionary. We show that the proposed algorithm is efficient in its\nusage of memory and computational complexity, and performs on par with the\nstandard learning strategy operating on the entire data at a time. As an\napplication, we consider the problem of image denoising. We present a\ncomparative analysis of our algorithm with the standard learning techniques,\nthat use the entire database at a time, in terms of training and denoising\nperformance. We observe that the split-and-merge algorithm results in a\nremarkable reduction of training time, without significantly affecting the\ndenoising performance.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 12:16:17 GMT"}], "update_date": "2014-03-20", "authors_parsed": [["Mukherjee", "Subhadip", ""], ["Seelamantula", "Chandra Sekhar", ""]]}, {"id": "1403.5029", "submitter": "Wei Zhang", "authors": "Wei Zhang, Jae-Woong Chang, Lilong Lin, Kay Minn, Baolin Wu, Jeremy\n  Chien, Jeongsik Yong, Hui Zheng, Rui Kuang", "title": "Network-based Isoform Quantification with RNA-Seq Data for Cancer\n  Transcriptome Analysis", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1004465", "report-no": null, "categories": "cs.CE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  High-throughput mRNA sequencing (RNA-Seq) is widely used for transcript\nquantification of gene isoforms. Since RNA-Seq data alone is often not\nsufficient to accurately identify the read origins from the isoforms for\nquantification, we propose to explore protein domain-domain interactions as\nprior knowledge for integrative analysis with RNA-seq data. We introduce a\nNetwork-based method for RNA-Seq-based Transcript Quantification (Net-RSTQ) to\nintegrate protein domain-domain interaction network with short read alignments\nfor transcript abundance estimation. Based on our observation that the\nabundances of the neighboring isoforms by domain-domain interactions in the\nnetwork are positively correlated, Net-RSTQ models the expression of the\nneighboring transcripts as Dirichlet priors on the likelihood of the observed\nread alignments against the transcripts in one gene. The transcript abundances\nof all the genes are then jointly estimated with alternating optimization of\nmultiple EM problems. In simulation Net-RSTQ effectively improved isoform\ntranscript quantifications when isoform co-expressions correlate with their\ninteractions. qRT-PCR results on 25 multi-isoform genes in a stem cell line, an\novarian cancer cell line, and a breast cancer cell line also showed that\nNet-RSTQ estimated more consistent isoform proportions with RNA-Seq data. In\nthe experiments on the RNA-Seq data in The Cancer Genome Atlas (TCGA), the\ntranscript abundances estimated by Net-RSTQ are more informative for patient\nsample classification of ovarian cancer, breast cancer and lung cancer. All\nexperimental results collectively support that Net-RSTQ is a promising approach\nfor isoform quantification.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 02:35:15 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2015 16:06:01 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2015 15:48:46 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Zhang", "Wei", ""], ["Chang", "Jae-Woong", ""], ["Lin", "Lilong", ""], ["Minn", "Kay", ""], ["Wu", "Baolin", ""], ["Chien", "Jeremy", ""], ["Yong", "Jeongsik", ""], ["Zheng", "Hui", ""], ["Kuang", "Rui", ""]]}, {"id": "1403.5045", "submitter": "Branislav Kveton", "authors": "Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, Brian Eriksson", "title": "Matroid Bandits: Fast Combinatorial Optimization with Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A matroid is a notion of independence in combinatorial optimization which is\nclosely related to computational efficiency. In particular, it is well known\nthat the maximum of a constrained modular function can be found greedily if and\nonly if the constraints are associated with a matroid. In this paper, we bring\ntogether the ideas of bandits and matroids, and propose a new class of\ncombinatorial bandits, matroid bandits. The objective in these problems is to\nlearn how to maximize a modular function on a matroid. This function is\nstochastic and initially unknown. We propose a practical algorithm for solving\nour problem, Optimistic Matroid Maximization (OMM); and prove two upper bounds,\ngap-dependent and gap-free, on its regret. Both bounds are sublinear in time\nand at most linear in all other quantities of interest. The gap-dependent upper\nbound is tight and we prove a matching lower bound on a partition matroid\nbandit. Finally, we evaluate our method on three real-world problems and show\nthat it is practical.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 05:52:43 GMT"}, {"version": "v2", "created": "Fri, 11 Apr 2014 06:25:22 GMT"}, {"version": "v3", "created": "Mon, 16 Jun 2014 20:23:34 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Kveton", "Branislav", ""], ["Wen", "Zheng", ""], ["Ashkan", "Azin", ""], ["Eydgahi", "Hoda", ""], ["Eriksson", "Brian", ""]]}, {"id": "1403.5115", "submitter": "Ugo Louche", "authors": "Ugo Louche (LIF), Liva Ralaivola (LIF)", "title": "Unconfused Ultraconservative Multiclass Algorithms", "comments": "ACML, Australia (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of learning linear classifiers from noisy datasets in a\nmulticlass setting. The two-class version of this problem was studied a few\nyears ago by, e.g. Bylander (1994) and Blum et al. (1996): in these\ncontributions, the proposed approaches to fight the noise revolve around a\nPerceptron learning scheme fed with peculiar examples computed through a\nweighted average of points from the noisy training set. We propose to build\nupon these approaches and we introduce a new algorithm called UMA (for\nUnconfused Multiclass additive Algorithm) which may be seen as a generalization\nto the multiclass setting of the previous approaches. In order to characterize\nthe noise we use the confusion matrix as a multiclass extension of the\nclassification noise studied in the aforementioned literature. Theoretically\nwell-founded, UMA furthermore displays very good empirical noise robustness, as\nevidenced by numerical simulations conducted on both synthetic and real data.\nKeywords: Multiclass classification, Perceptron, Noisy labels, Confusion Matrix\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 12:46:33 GMT"}], "update_date": "2014-03-21", "authors_parsed": [["Louche", "Ugo", "", "LIF"], ["Ralaivola", "Liva", "", "LIF"]]}, {"id": "1403.5287", "submitter": "Paul Christiano", "authors": "Paul Christiano", "title": "Online Local Learning via Semidefinite Programming", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many online learning problems we are interested in predicting local\ninformation about some universe of items. For example, we may want to know\nwhether two items are in the same cluster rather than computing an assignment\nof items to clusters; we may want to know which of two teams will win a game\nrather than computing a ranking of teams. Although finding the optimal\nclustering or ranking is typically intractable, it may be possible to predict\nthe relationships between items as well as if you could solve the global\noptimization problem exactly.\n  Formally, we consider an online learning problem in which a learner\nrepeatedly guesses a pair of labels (l(x), l(y)) and receives an adversarial\npayoff depending on those labels. The learner's goal is to receive a payoff\nnearly as good as the best fixed labeling of the items. We show that a simple\nalgorithm based on semidefinite programming can obtain asymptotically optimal\nregret in the case where the number of possible labels is O(1), resolving an\nopen problem posed by Hazan, Kale, and Shalev-Schwartz. Our main technical\ncontribution is a novel use and analysis of the log determinant regularizer,\nexploiting the observation that log det(A + I) upper bounds the entropy of any\ndistribution with covariance matrix A.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 20:36:18 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Christiano", "Paul", ""]]}, {"id": "1403.5341", "submitter": "Daniel Russo", "authors": "Daniel Russo, Benjamin Van Roy", "title": "An Information-Theoretic Analysis of Thompson Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an information-theoretic analysis of Thompson sampling that\napplies across a broad range of online optimization problems in which a\ndecision-maker must learn from partial feedback. This analysis inherits the\nsimplicity and elegance of information theory and leads to regret bounds that\nscale with the entropy of the optimal-action distribution. This strengthens\npreexisting results and yields new insight into how information improves\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 01:42:53 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 19:05:44 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Russo", "Daniel", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1403.5370", "submitter": "Mathieu Dubois", "authors": "Mathieu Dubois (LIMSI), Frenoux Emmanuelle (LIMSI), Philippe Tarroux\n  (LIMSI)", "title": "Using n-grams models for visual semantic place recognition", "comments": "VISAPP (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to present a new method for visual place\nrecognition. Our system combines global image characterization and visual\nwords, which allows to use efficient Bayesian filtering methods to integrate\nseveral images. More precisely, we extend the classical HMM model with\ntechniques inspired by the field of Natural Language Processing. This paper\npresents our system and the Bayesian filtering algorithm. The performance of\nour system and the influence of the main parameters are evaluated on a standard\ndatabase. The discussion highlights the interest of using such models and\nproposes improvements.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 05:23:17 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Dubois", "Mathieu", "", "LIMSI"], ["Emmanuelle", "Frenoux", "", "LIMSI"], ["Tarroux", "Philippe", "", "LIMSI"]]}, {"id": "1403.5488", "submitter": "Tshilidzi Marwala", "authors": "Collins Leke, Bhekisipho Twala, and T. Marwala", "title": "Missing Data Prediction and Classification: The Use of Auto-Associative\n  Neural Networks and Optimization Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper presents methods which are aimed at finding approximations to\nmissing data in a dataset by using optimization algorithms to optimize the\nnetwork parameters after which prediction and classification tasks can be\nperformed. The optimization methods that are considered are genetic algorithm\n(GA), simulated annealing (SA), particle swarm optimization (PSO), random\nforest (RF) and negative selection (NS) and these methods are individually used\nin combination with auto-associative neural networks (AANN) for missing data\nestimation and the results obtained are compared. The methods suggested use the\noptimization algorithms to minimize an error function derived from training the\nauto-associative neural network during which the interrelationships between the\ninputs and the outputs are obtained and stored in the weights connecting the\ndifferent layers of the network. The error function is expressed as the square\nof the difference between the actual observations and predicted values from an\nauto-associative neural network. In the event of missing data, all the values\nof the actual observations are not known hence, the error function is\ndecomposed to depend on the known and unknown variable values. Multi-layer\nperceptron (MLP) neural network is employed to train the neural networks using\nthe scaled conjugate gradient (SCG) method. Prediction accuracy is determined\nby mean squared error (MSE), root mean squared error (RMSE), mean absolute\nerror (MAE), and correlation coefficient (r) computations. Accuracy in\nclassification is obtained by plotting ROC curves and calculating the areas\nunder these. Analysis of results depicts that the approach using RF with AANN\nproduces the most accurate predictions and classifications while on the other\nend of the scale is the approach which entails using NS with AANN.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 15:11:52 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Leke", "Collins", ""], ["Twala", "Bhekisipho", ""], ["Marwala", "T.", ""]]}, {"id": "1403.5556", "submitter": "Daniel Russo", "authors": "Daniel Russo and Benjamin Van Roy", "title": "Learning to Optimize via Information-Directed Sampling", "comments": "arXiv admin note: substantial text overlap with arXiv:1403.5341", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose information-directed sampling -- a new approach to online\noptimization problems in which a decision-maker must balance between\nexploration and exploitation while learning from partial feedback. Each action\nis sampled in a manner that minimizes the ratio between squared expected\nsingle-period regret and a measure of information gain: the mutual information\nbetween the optimal action and the next observation. We establish an expected\nregret bound for information-directed sampling that applies across a very\ngeneral class of models and scales with the entropy of the optimal action\ndistribution. We illustrate through simple analytic examples how\ninformation-directed sampling accounts for kinds of information that\nalternative approaches do not adequately address and that this can lead to\ndramatic performance gains. For the widely studied Bernoulli, Gaussian, and\nlinear bandit problems, we demonstrate state-of-the-art simulation performance.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 02:02:25 GMT"}, {"version": "v2", "created": "Sun, 8 Jun 2014 20:40:38 GMT"}, {"version": "v3", "created": "Thu, 3 Jul 2014 01:09:22 GMT"}, {"version": "v4", "created": "Tue, 22 Jul 2014 17:59:30 GMT"}, {"version": "v5", "created": "Fri, 12 Aug 2016 06:53:32 GMT"}, {"version": "v6", "created": "Wed, 24 May 2017 00:06:24 GMT"}, {"version": "v7", "created": "Fri, 7 Jul 2017 05:51:15 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Russo", "Daniel", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1403.5603", "submitter": "Jie Xu", "authors": "Jie Xu, Mihaela van der Schaar, Jiangchuan Liu and Haitao Li", "title": "Forecasting Popularity of Videos using Social Media", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2014.2370942", "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a systematic online prediction method (Social-Forecast)\nthat is capable to accurately forecast the popularity of videos promoted by\nsocial media. Social-Forecast explicitly considers the dynamically changing and\nevolving propagation patterns of videos in social media when making popularity\nforecasts, thereby being situation and context aware. Social-Forecast aims to\nmaximize the forecast reward, which is defined as a tradeoff between the\npopularity prediction accuracy and the timeliness with which a prediction is\nissued. The forecasting is performed online and requires no training phase or a\npriori knowledge. We analytically bound the prediction performance loss of\nSocial-Forecast as compared to that obtained by an omniscient oracle and prove\nthat the bound is sublinear in the number of video arrivals, thereby\nguaranteeing its short-term performance as well as its asymptotic convergence\nto the optimal performance. In addition, we conduct extensive experiments using\nreal-world data traces collected from the videos shared in RenRen, one of the\nlargest online social networks in China. These experiments show that our\nproposed method outperforms existing view-based approaches for popularity\nprediction (which are not context-aware) by more than 30% in terms of\nprediction rewards.\n", "versions": [{"version": "v1", "created": "Sat, 22 Mar 2014 02:15:39 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Xu", "Jie", ""], ["van der Schaar", "Mihaela", ""], ["Liu", "Jiangchuan", ""], ["Li", "Haitao", ""]]}, {"id": "1403.5607", "submitter": "Michael Gelbart", "authors": "Michael A. Gelbart, Jasper Snoek, Ryan P. Adams", "title": "Bayesian Optimization with Unknown Constraints", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on Bayesian optimization has shown its effectiveness in global\noptimization of difficult black-box objective functions. Many real-world\noptimization problems of interest also have constraints which are unknown a\npriori. In this paper, we study Bayesian optimization for constrained problems\nin the general case that noise may be present in the constraint functions, and\nthe objective and constraints may be evaluated independently. We provide\nmotivating practical examples, and present a general framework to solve such\nproblems. We demonstrate the effectiveness of our approach on optimizing the\nperformance of online latent Dirichlet allocation subject to topic sparsity\nconstraints, tuning a neural network given test-time memory constraints, and\noptimizing Hamiltonian Monte Carlo to achieve maximal effectiveness in a fixed\ntime, subject to passing standard convergence diagnostics.\n", "versions": [{"version": "v1", "created": "Sat, 22 Mar 2014 03:35:00 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Gelbart", "Michael A.", ""], ["Snoek", "Jasper", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1403.5647", "submitter": "Miao Xu", "authors": "Rong Jin, Shenghuo Zhu", "title": "CUR Algorithm with Incomplete Matrix Observation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CUR matrix decomposition is a randomized algorithm that can efficiently\ncompute the low rank approximation for a given rectangle matrix. One limitation\nwith the existing CUR algorithms is that they require an access to the full\nmatrix A for computing U. In this work, we aim to alleviate this limitation. In\nparticular, we assume that besides having an access to randomly sampled d rows\nand d columns from A, we only observe a subset of randomly sampled entries from\nA. Our goal is to develop a low rank approximation algorithm, similar to CUR,\nbased on (i) randomly sampled rows and columns from A, and (ii) randomly\nsampled entries from A. The proposed algorithm is able to perfectly recover the\ntarget matrix A with only O(rn log n) number of observed entries. In addition,\ninstead of having to solve an optimization problem involved trace norm\nregularization, the proposed algorithm only needs to solve a standard\nregression problem. Finally, unlike most matrix completion theories that hold\nonly when the target matrix is of low rank, we show a strong guarantee for the\nproposed algorithm even when the target matrix is not low rank.\n", "versions": [{"version": "v1", "created": "Sat, 22 Mar 2014 11:15:01 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Jin", "Rong", ""], ["Zhu", "Shenghuo", ""]]}, {"id": "1403.5693", "submitter": "Dougal Maclaurin", "authors": "Dougal Maclaurin and Ryan P. Adams", "title": "Firefly Monte Carlo: Exact MCMC with Subsets of Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is a popular and successful general-purpose\ntool for Bayesian inference. However, MCMC cannot be practically applied to\nlarge data sets because of the prohibitive cost of evaluating every likelihood\nterm at every iteration. Here we present Firefly Monte Carlo (FlyMC) an\nauxiliary variable MCMC algorithm that only queries the likelihoods of a\npotentially small subset of the data at each iteration yet simulates from the\nexact posterior distribution, in contrast to recent proposals that are\napproximate even in the asymptotic limit. FlyMC is compatible with a wide\nvariety of modern MCMC algorithms, and only requires a lower bound on the\nper-datum likelihood factors. In experiments, we find that FlyMC generates\nsamples from the posterior more than an order of magnitude faster than regular\nMCMC, opening up MCMC methods to larger datasets than were previously\nconsidered feasible.\n", "versions": [{"version": "v1", "created": "Sat, 22 Mar 2014 18:21:29 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Maclaurin", "Dougal", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1403.5877", "submitter": "Anastasios Kyrillidis", "authors": "Anastasios Kyrillidis and Anastasios Zouzias", "title": "Non-uniform Feature Sampling for Decision Tree Ensembles", "comments": "7 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effectiveness of non-uniform randomized feature selection in\ndecision tree classification. We experimentally evaluate two feature selection\nmethodologies, based on information extracted from the provided dataset: $(i)$\n\\emph{leverage scores-based} and $(ii)$ \\emph{norm-based} feature selection.\nExperimental evaluation of the proposed feature selection techniques indicate\nthat such approaches might be more effective compared to naive uniform feature\nselection and moreover having comparable performance to the random forest\nalgorithm [3]\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 08:26:19 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Kyrillidis", "Anastasios", ""], ["Zouzias", "Anastasios", ""]]}, {"id": "1403.5933", "submitter": "Kiran Sree Pokkuluri Prof", "authors": "Pokkuluri Kiran Sree, Inampudi Ramesh Babu", "title": "AIS-INMACA: A Novel Integrated MACA Based Clonal Classifier for Protein\n  Coding and Promoter Region Prediction", "comments": "7 Pages", "journal-ref": "Journal of Bioinformatics and Comparative Genomics,2014", "doi": null, "report-no": "Pokkuluri Kiran Sree, et al. (2014) AIS-INMACA: A Novel Integrated\n  MACA Based Clonal Classifier for Protein Coding and Promoter Region\n  Prediction. J Bioinfo Comp Genom 1: 1-7", "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the problems in bioinformatics are now the challenges in computing.\nThis paper aims at building a classifier based on Multiple Attractor Cellular\nAutomata (MACA) which uses fuzzy logic. It is strengthened with an artificial\nImmune System Technique (AIS), Clonal algorithm for identifying a protein\ncoding and promoter region in a given DNA sequence. The proposed classifier is\nnamed as AIS-INMACA introduces a novel concept to combine CA with artificial\nimmune system to produce a better classifier which can address major problems\nin bioinformatics. This will be the first integrated algorithm which can\npredict both promoter and protein coding regions. To obtain good fitness rules\nthe basic concept of Clonal selection algorithm was used. The proposed\nclassifier can handle DNA sequences of lengths 54,108,162,252,354. This\nclassifier gives the exact boundaries of both protein and promoter regions with\nan average accuracy of 89.6%. This classifier was tested with 97,000 data\ncomponents which were taken from Fickett & Toung, MPromDb, and other sequences\nfrom a renowned medical university. This proposed classifier can handle huge\ndata sets and can find protein and promoter regions even in mixed and\noverlapped DNA sequences. This work also aims at identifying the logicality\nbetween the major problems in bioinformatics and tries to obtaining a common\nframe work for addressing major problems in bioinformatics like protein\nstructure prediction, RNA structure prediction, predicting the splicing pattern\nof any primary transcript and analysis of information content in DNA, RNA,\nprotein sequences and structure. This work will attract more researchers\ntowards application of CA as a potential pattern classifier to many important\nproblems in bioinformatics\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 12:37:11 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Sree", "Pokkuluri Kiran", ""], ["Babu", "Inampudi Ramesh", ""]]}, {"id": "1403.5997", "submitter": "Niko Br\\\"ummer", "authors": "Niko Br\\\"ummer and Albert Swart", "title": "Bayesian calibration for forensic evidence reporting", "comments": "accepted for Interspeech 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian solution for the problem in forensic speaker\nrecognition, where there may be very little background material for estimating\nscore calibration parameters. We work within the Bayesian paradigm of evidence\nreporting and develop a principled probabilistic treatment of the problem,\nwhich results in a Bayesian likelihood-ratio as the vehicle for reporting\nweight of evidence. We show in contrast, that reporting a likelihood-ratio\ndistribution does not solve this problem. Our solution is experimentally\nexercised on a simulated forensic scenario, using NIST SRE'12 scores, which\ndemonstrates a clear advantage for the proposed method compared to the\ntraditional plugin calibration recipe.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 15:25:59 GMT"}, {"version": "v2", "created": "Tue, 25 Mar 2014 07:27:21 GMT"}, {"version": "v3", "created": "Tue, 10 Jun 2014 08:18:06 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Br\u00fcmmer", "Niko", ""], ["Swart", "Albert", ""]]}, {"id": "1403.6023", "submitter": "Luis Marujo", "authors": "Lu\\'is Marujo, Anatole Gershman, Jaime Carbonell, Jo\\~ao P. Neto,\n  David Martins de Matos", "title": "Ensemble Detection of Single & Multiple Events at Sentence-Level", "comments": "Preliminary version of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event classification at sentence level is an important Information Extraction\ntask with applications in several NLP, IR, and personalization systems.\nMulti-label binary relevance (BR) are the state-of-art methods. In this work,\nwe explored new multi-label methods known for capturing relations between event\ntypes. These new methods, such as the ensemble Chain of Classifiers, improve\nthe F1 on average across the 6 labels by 2.8% over the Binary Relevance. The\nlow occurrence of multi-label sentences motivated the reduction of the hard\nimbalanced multi-label classification problem with low number of occurrences of\nmultiple labels per instance to an more tractable imbalanced multiclass problem\nwith better results (+ 4.6%). We report the results of adding new features,\nsuch as sentiment strength, rhetorical signals, domain-id (source-id and date),\nand key-phrases in both single-label and multi-label event classification\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 16:21:04 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Marujo", "Lu\u00eds", ""], ["Gershman", "Anatole", ""], ["Carbonell", "Jaime", ""], ["Neto", "Jo\u00e3o P.", ""], ["de Matos", "David Martins", ""]]}, {"id": "1403.6248", "submitter": "Qifeng Qiao", "authors": "Qifeng Qiao and Peter A. Beling", "title": "Classroom Video Assessment and Retrieval via Multiple Instance Learning", "comments": null, "journal-ref": "The 14th International Conference on Artificial Intelligence in\n  Education 2011", "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multiple instance learning approach to content-based retrieval\nof classroom video for the purpose of supporting human assessing the learning\nenvironment. The key element of our approach is a mapping between the semantic\nconcepts of the assessment system and features of the video that can be\nmeasured using techniques from the fields of computer vision and speech\nanalysis. We report on a formative experiment in content-based video retrieval\ninvolving trained experts in the Classroom Assessment Scoring System, a widely\nused framework for assessment and improvement of learning environments. The\nresults of this experiment suggest that our approach has potential application\nto productivity enhancement in assessment and to broader retrieval tasks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 07:11:03 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Qiao", "Qifeng", ""], ["Beling", "Peter A.", ""]]}, {"id": "1403.6348", "submitter": "Bla\\v{z} Sovdat", "authors": "Blaz Sovdat", "title": "Updating Formulas and Algorithms for Computing Entropy and Gini Index\n  from Time-Changing Data Streams", "comments": "Added directions future work; more consistent notation; fixed the\n  errors in the updating algorithms for entropy; fixed an error in the\n  statement of theorem 5; added two references to related work; fixed a few\n  typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Despite growing interest in data stream mining the most successful\nincremental learners, such as VFDT, still use periodic recomputation to update\nattribute information gains and Gini indices. This note provides simple\nincremental formulas and algorithms for computing entropy and Gini index from\ntime-changing data streams.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 14:07:21 GMT"}, {"version": "v2", "created": "Sun, 6 Apr 2014 20:22:43 GMT"}, {"version": "v3", "created": "Mon, 28 Apr 2014 20:48:57 GMT"}, {"version": "v4", "created": "Thu, 4 Feb 2016 20:58:31 GMT"}, {"version": "v5", "created": "Sun, 7 Feb 2016 17:04:17 GMT"}, {"version": "v6", "created": "Sat, 30 Jul 2016 10:10:10 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Sovdat", "Blaz", ""]]}, {"id": "1403.6397", "submitter": "Michael R\\\"oder", "authors": "Frank Rosner, Alexander Hinneburg, Michael R\\\"oder, Martin Nettling,\n  Andreas Both", "title": "Evaluating topic coherence measures", "comments": "This work has been presented at the \"Topic Models: Computation,\n  Application and Evaluation\" workshop at the \"Neural Information Processing\n  Systems\" conference 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Topic models extract representative word sets - called topics - from word\ncounts in documents without requiring any semantic annotations. Topics are not\nguaranteed to be well interpretable, therefore, coherence measures have been\nproposed to distinguish between good and bad topics. Studies of topic coherence\nso far are limited to measures that score pairs of individual words. For the\nfirst time, we include coherence measures from scientific philosophy that score\npairs of more complex word subsets and apply them to topic scoring.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 15:44:14 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Rosner", "Frank", ""], ["Hinneburg", "Alexander", ""], ["R\u00f6der", "Michael", ""], ["Nettling", "Martin", ""], ["Both", "Andreas", ""]]}, {"id": "1403.6508", "submitter": "Xiaomin Lin", "authors": "Xiaomin Lin and Peter A. Beling and Randy Cogill", "title": "Multi-agent Inverse Reinforcement Learning for Two-person Zero-sum Games", "comments": null, "journal-ref": "IEEE Transactions on Games 10(1) (2018) 56-68", "doi": "10.1109/TCIAIG.2017.2679115", "report-no": null, "categories": "cs.GT cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this paper is a Bayesian framework for solving a class of\nproblems termed multi-agent inverse reinforcement learning (MIRL). Compared to\nthe well-known inverse reinforcement learning (IRL) problem, MIRL is formalized\nin the context of stochastic games, which generalize Markov decision processes\nto game theoretic scenarios. We establish a theoretical foundation for\ncompetitive two-agent zero-sum MIRL problems and propose a Bayesian solution\napproach in which the generative model is based on an assumption that the two\nagents follow a minimax bi-policy. Numerical results are presented comparing\nthe Bayesian MIRL method with two existing methods in the context of an\nabstract soccer game. Investigation centers on relationships between the extent\nof prior information and the quality of learned rewards. Results suggest that\ncovariance structure is more important than mean value in reward priors.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 21:03:57 GMT"}, {"version": "v2", "created": "Thu, 10 Apr 2014 22:51:35 GMT"}, {"version": "v3", "created": "Tue, 30 Jul 2019 01:28:36 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Lin", "Xiaomin", ""], ["Beling", "Peter A.", ""], ["Cogill", "Randy", ""]]}, {"id": "1403.6530", "submitter": "L.A. Prashanth", "authors": "Prashanth L.A. and Mohammad Ghavamzadeh", "title": "Variance-Constrained Actor-Critic Algorithms for Discounted and Average\n  Reward MDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many sequential decision-making problems we may want to manage risk by\nminimizing some measure of variability in rewards in addition to maximizing a\nstandard criterion. Variance related risk measures are among the most common\nrisk-sensitive criteria in finance and operations research. However, optimizing\nmany such criteria is known to be a hard problem. In this paper, we consider\nboth discounted and average reward Markov decision processes. For each\nformulation, we first define a measure of variability for a policy, which in\nturn gives us a set of risk-sensitive criteria to optimize. For each of these\ncriteria, we derive a formula for computing its gradient. We then devise\nactor-critic algorithms that operate on three timescales - a TD critic on the\nfastest timescale, a policy gradient (actor) on the intermediate timescale, and\na dual ascent for Lagrange multipliers on the slowest timescale. In the\ndiscounted setting, we point out the difficulty in estimating the gradient of\nthe variance of the return and incorporate simultaneous perturbation approaches\nto alleviate this. The average setting, on the other hand, allows for an actor\nupdate using compatible features to estimate the gradient of the variance. We\nestablish the convergence of our algorithms to locally risk-sensitive optimal\npolicies. Finally, we demonstrate the usefulness of our algorithms in a traffic\nsignal control application.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 23:00:50 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 15:42:31 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["A.", "Prashanth L.", ""], ["Ghavamzadeh", "Mohammad", ""]]}, {"id": "1403.6652", "submitter": "Rami Al-Rfou", "authors": "Bryan Perozzi, Rami Al-Rfou and Steven Skiena", "title": "DeepWalk: Online Learning of Social Representations", "comments": "10 pages, 5 figures, 4 tables", "journal-ref": null, "doi": "10.1145/2623330.2623732", "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepWalk, a novel approach for learning latent representations of\nvertices in a network. These latent representations encode social relations in\na continuous vector space, which is easily exploited by statistical models.\nDeepWalk generalizes recent advancements in language modeling and unsupervised\nfeature learning (or deep learning) from sequences of words to graphs. DeepWalk\nuses local information obtained from truncated random walks to learn latent\nrepresentations by treating walks as the equivalent of sentences. We\ndemonstrate DeepWalk's latent representations on several multi-label network\nclassification tasks for social networks such as BlogCatalog, Flickr, and\nYouTube. Our results show that DeepWalk outperforms challenging baselines which\nare allowed a global view of the network, especially in the presence of missing\ninformation. DeepWalk's representations can provide $F_1$ scores up to 10%\nhigher than competing methods when labeled data is sparse. In some experiments,\nDeepWalk's representations are able to outperform all baseline methods while\nusing 60% less training data. DeepWalk is also scalable. It is an online\nlearning algorithm which builds useful incremental results, and is trivially\nparallelizable. These qualities make it suitable for a broad class of real\nworld applications such as network classification, and anomaly detection.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 12:30:07 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 17:17:25 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Perozzi", "Bryan", ""], ["Al-Rfou", "Rami", ""], ["Skiena", "Steven", ""]]}, {"id": "1403.6706", "submitter": "Karthikeyan Natesan Ramamurthy", "authors": "Karthikeyan Natesan Ramamurthy, Aleksandr Y. Aravkin, Jayaraman J.\n  Thiagarajan", "title": "Beyond L2-Loss Functions for Learning Sparse Models", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating sparsity priors in learning tasks can give rise to simple, and\ninterpretable models for complex high dimensional data. Sparse models have\nfound widespread use in structure discovery, recovering data from corruptions,\nand a variety of large scale unsupervised and supervised learning problems.\nAssuming the availability of sufficient data, these methods infer dictionaries\nfor sparse representations by optimizing for high-fidelity reconstruction. In\nmost scenarios, the reconstruction quality is measured using the squared\nEuclidean distance, and efficient algorithms have been developed for both batch\nand online learning cases. However, new application domains motivate looking\nbeyond conventional loss functions. For example, robust loss functions such as\n$\\ell_1$ and Huber are useful in learning outlier-resilient models, and the\nquantile loss is beneficial in discovering structures that are the\nrepresentative of a particular quantile. These new applications motivate our\nwork in generalizing sparse learning to a broad class of convex loss functions.\nIn particular, we consider the class of piecewise linear quadratic (PLQ) cost\nfunctions that includes Huber, as well as $\\ell_1$, quantile, Vapnik, hinge\nloss, and smoothed variants of these penalties. We propose an algorithm to\nlearn dictionaries and obtain sparse codes when the data reconstruction\nfidelity is measured using any smooth PLQ cost function. We provide convergence\nguarantees for the proposed algorithm, and demonstrate the convergence behavior\nusing empirical experiments. Furthermore, we present three case studies that\nrequire the use of PLQ cost functions: (i) robust image modeling, (ii) tag\nrefinement for image annotation and retrieval and (iii) computing empirical\nconfidence limits for subspace clustering.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 15:16:56 GMT"}], "update_date": "2014-03-27", "authors_parsed": [["Ramamurthy", "Karthikeyan Natesan", ""], ["Aravkin", "Aleksandr Y.", ""], ["Thiagarajan", "Jayaraman J.", ""]]}, {"id": "1403.6822", "submitter": "Xiaomin Lin", "authors": "Xiaomin Lin and Peter A. Beling and Randy Cogill", "title": "Comparison of Multi-agent and Single-agent Inverse Learning on a\n  Simulated Soccer Example", "comments": "arXiv admin note: text overlap with arXiv:1403.6508", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare the performance of Inverse Reinforcement Learning (IRL) with the\nrelative new model of Multi-agent Inverse Reinforcement Learning (MIRL). Before\ncomparing the methods, we extend a published Bayesian IRL approach that is only\napplicable to the case where the reward is only state dependent to a general\none capable of tackling the case where the reward depends on both state and\naction. Comparison between IRL and MIRL is made in the context of an abstract\nsoccer game, using both a game model in which the reward depends only on state\nand one in which it depends on both state and action. Results suggest that the\nIRL approach performs much worse than the MIRL approach. We speculate that the\nunderperformance of IRL is because it fails to capture equilibrium information\nin the manner possible in MIRL.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 15:27:27 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Lin", "Xiaomin", ""], ["Beling", "Peter A.", ""], ["Cogill", "Randy", ""]]}, {"id": "1403.6863", "submitter": "Marcus Hutter", "authors": "Joel Veness and Marcus Hutter", "title": "Online Learning of k-CNF Boolean Functions", "comments": "20 LaTeX pages. 2 Algorithms. Some Theorems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits the problem of learning a k-CNF Boolean function from\nexamples in the context of online learning under the logarithmic loss. In doing\nso, we give a Bayesian interpretation to one of Valiant's celebrated PAC\nlearning algorithms, which we then build upon to derive two efficient, online,\nprobabilistic, supervised learning algorithms for predicting the output of an\nunknown k-CNF Boolean function. We analyze the loss of our methods, and show\nthat the cumulative log-loss can be upper bounded, ignoring logarithmic\nfactors, by a polynomial function of the size of each example.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 21:17:05 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Veness", "Joel", ""], ["Hutter", "Marcus", ""]]}, {"id": "1403.6901", "submitter": "Sunil Kumar Kopparapu Dr", "authors": "Sapna Soni and Ahmed Imran and Sunil Kumar Kopparapu", "title": "Automatic Segmentation of Broadcast News Audio using Self Similarity\n  Matrix", "comments": "4 pages, 5 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generally audio news broadcast on radio is com- posed of music, commercials,\nnews from correspondents and recorded statements in addition to the actual news\nread by the newsreader. When news transcripts are available, automatic\nsegmentation of audio news broadcast to time align the audio with the text\ntranscription to build frugal speech corpora is essential. We address the\nproblem of identifying segmentation in the audio news broadcast corresponding\nto the news read by the newsreader so that they can be mapped to the text\ntranscripts. The existing techniques produce sub-optimal solutions when used to\nextract newsreader read segments. In this paper, we propose a new technique\nwhich is able to identify the acoustic change points reliably using an acoustic\nSelf Similarity Matrix (SSM). We describe the two pass technique in detail and\nverify its performance on real audio news broadcast of All India Radio for\ndifferent languages.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 01:32:09 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Soni", "Sapna", ""], ["Imran", "Ahmed", ""], ["Kopparapu", "Sunil Kumar", ""]]}, {"id": "1403.7057", "submitter": "Alexander Kolesnikov", "authors": "Alexander Kolesnikov, Matthieu Guillaumin, Vittorio Ferrari and\n  Christoph H. Lampert", "title": "Closed-Form Training of Conditional Random Fields for Large Scale Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LS-CRF, a new method for very efficient large-scale training of\nConditional Random Fields (CRFs). It is inspired by existing closed-form\nexpressions for the maximum likelihood parameters of a generative graphical\nmodel with tree topology. LS-CRF training requires only solving a set of\nindependent regression problems, for which closed-form expression as well as\nefficient iterative solvers are available. This makes it orders of magnitude\nfaster than conventional maximum likelihood learning for CRFs that require\nrepeated runs of probabilistic inference. At the same time, the models learned\nby our method still allow for joint inference at test time. We apply LS-CRF to\nthe task of semantic image segmentation, showing that it is highly efficient,\neven for loopy models where probabilistic inference is problematic. It allows\nthe training of image segmentation models from significantly larger training\nsets than had been used previously. We demonstrate this on two new datasets\nthat form a second contribution of this paper. They consist of over 180,000\nimages with figure-ground segmentation annotations. Our large-scale experiments\nshow that the possibilities of CRF-based image segmentation are far from\nexhausted, indicating, for example, that semi-supervised learning and the use\nof non-linear predictors are promising directions for achieving higher\nsegmentation accuracy in the future.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 14:38:23 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Kolesnikov", "Alexander", ""], ["Guillaumin", "Matthieu", ""], ["Ferrari", "Vittorio", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1403.7087", "submitter": "Nick Williams", "authors": "Nick Williams", "title": "Conclusions from a NAIVE Bayes Operator Predicting the Medicare 2011\n  Transaction Data Set", "comments": "8 Pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction: The United States Federal Government operates one of the worlds\nlargest medical insurance programs, Medicare, to ensure payment for clinical\nservices for the elderly, illegal aliens and those without the ability to pay\nfor their care directly. This paper evaluates the Medicare 2011 Transaction\nData Set which details the transfer of funds from Medicare to private and\npublic clinical care facilities for specific clinical services for the\noperational year 2011. Methods: Data mining was conducted to establish the\nrelationships between reported and computed transaction values in the data set\nto better understand the drivers of Medicare transactions at a programmatic\nlevel. Results: The models averaged 88 for average model accuracy and 38 for\naverage Kappa during training. Some reported classes are highly independent\nfrom the available data as their predictability remains stable regardless of\nredaction of supporting and contradictory evidence. DRG or procedure type\nappears to be unpredictable from the available financial transaction values.\nConclusions: Overlay hypotheses such as charges being driven by the volume\nserved or DRG being related to charges or payments is readily false in this\nanalysis despite 28 million Americans being billed through Medicare in 2011 and\nthe program distributing over 70 billion in this transaction set alone. It may\nbe impossible to predict the dependencies and data structures the payer of last\nresort without data from payers of first and second resort. Political concerns\nabout Medicare would be better served focusing on these first and second order\npayer systems as what Medicare costs is not dependent on Medicare itself.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 03:12:51 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Williams", "Nick", ""]]}, {"id": "1403.7100", "submitter": "Baogang Hu", "authors": "Bao-Gang Hu and Wei-Ming Dong", "title": "A study on cost behaviors of binary classification measures in\n  class-imbalanced problems", "comments": "7 pages, 5 figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates into cost behaviors of binary classification measures\nin a background of class-imbalanced problems. Twelve performance measures are\nstudied, such as F measure, G-means in terms of accuracy rates, and of recall\nand precision, balance error rate (BER), Matthews correlation coefficient\n(MCC), Kappa coefficient, etc. A new perspective is presented for those\nmeasures by revealing their cost functions with respect to the class imbalance\nratio. Basically, they are described by four types of cost functions. The\nfunctions provides a theoretical understanding why some measures are suitable\nfor dealing with class-imbalanced problems. Based on their cost functions, we\nare able to conclude that G-means of accuracy rates and BER are suitable\nmeasures because they show \"proper\" cost behaviors in terms of \"a\nmisclassification from a small class will cause a greater cost than that from a\nlarge class\". On the contrary, F1 measure, G-means of recall and precision, MCC\nand Kappa coefficient measures do not produce such behaviors so that they are\nunsuitable to serve our goal in dealing with the problems properly.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 05:43:12 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Hu", "Bao-Gang", ""], ["Dong", "Wei-Ming", ""]]}, {"id": "1403.7308", "submitter": "Marko Robnik-\\v{S}ikonja", "authors": "Marko Robnik-\\v{S}ikonja", "title": "Data Generators for Learning Systems Based on RBF Networks", "comments": null, "journal-ref": "IEEE Transaction on Neural Networks and Learning Systems,\n  27(5):926-938, 2016", "doi": "10.1109/TNNLS.2015.2429711", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are plenty of problems where the data available is scarce and\nexpensive. We propose a generator of semi-artificial data with similar\nproperties to the original data which enables development and testing of\ndifferent data mining algorithms and optimization of their parameters. The\ngenerated data allow a large scale experimentation and simulations without\ndanger of overfitting. The proposed generator is based on RBF networks, which\nlearn sets of Gaussian kernels. These Gaussian kernels can be used in a\ngenerative mode to generate new data from the same distributions. To assess\nquality of the generated data we evaluated the statistical properties of the\ngenerated data, structural similarity and predictive similarity using\nsupervised and unsupervised learning techniques. To determine usability of the\nproposed generator we conducted a large scale evaluation using 51 UCI data\nsets. The results show a considerable similarity between the original and\ngenerated data and indicate that the method can be useful in several\ndevelopment and simulation scenarios. We analyze possible improvements in\nclassification performance by adding different amounts of generated data to the\ntraining set, performance on high dimensional data sets, and conditions when\nthe proposed approach is successful.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 08:55:21 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 21:49:33 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Robnik-\u0160ikonja", "Marko", ""]]}, {"id": "1403.7429", "submitter": "Aivar Sootla", "authors": "Wei Pan, Aivar Sootla and Guy-Bart Stan", "title": "Distributed Reconstruction of Nonlinear Networks: An ADMM Approach", "comments": "To appear in the Preprints of 19th IFAC World Congress 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a distributed algorithm for the reconstruction of\nlarge-scale nonlinear networks. In particular, we focus on the identification\nfrom time-series data of the nonlinear functional forms and associated\nparameters of large-scale nonlinear networks. Recently, a nonlinear network\nreconstruction problem was formulated as a nonconvex optimisation problem based\non the combination of a marginal likelihood maximisation procedure with\nsparsity inducing priors. Using a convex-concave procedure (CCCP), an iterative\nreweighted lasso algorithm was derived to solve the initial nonconvex\noptimisation problem. By exploiting the structure of the objective function of\nthis reweighted lasso algorithm, a distributed algorithm can be designed. To\nthis end, we apply the alternating direction method of multipliers (ADMM) to\ndecompose the original problem into several subproblems. To illustrate the\neffectiveness of the proposed methods, we use our approach to identify a\nnetwork of interconnected Kuramoto oscillators with different network sizes\n(500~100,000 nodes).\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 16:11:57 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Pan", "Wei", ""], ["Sootla", "Aivar", ""], ["Stan", "Guy-Bart", ""]]}, {"id": "1403.7471", "submitter": "Trevor Campbell", "authors": "Trevor Campbell and Jonathan P. How", "title": "Approximate Decentralized Bayesian Inference", "comments": "This paper was presented at UAI 2014. Please use the following BibTeX\n  citation: @inproceedings{Campbell14_UAI, Author = {Trevor Campbell and\n  Jonathan P. How}, Title = {Approximate Decentralized Bayesian Inference},\n  Booktitle = {Uncertainty in Artificial Intelligence (UAI)}, Year = {2014}}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approximate method for performing Bayesian inference\nin models with conditional independence over a decentralized network of\nlearning agents. The method first employs variational inference on each\nindividual learning agent to generate a local approximate posterior, the agents\ntransmit their local posteriors to other agents in the network, and finally\neach agent combines its set of received local posteriors. The key insight in\nthis work is that, for many Bayesian models, approximate inference schemes\ndestroy symmetry and dependencies in the model that are crucial to the correct\napplication of Bayes' rule when combining the local posteriors. The proposed\nmethod addresses this issue by including an additional optimization step in the\ncombination procedure that accounts for these broken dependencies. Experiments\non synthetic and real data demonstrate that the decentralized method provides\nadvantages in computational performance and predictive test likelihood over\nprevious batch and distributed methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 18:07:21 GMT"}, {"version": "v2", "created": "Wed, 4 Jun 2014 14:09:54 GMT"}, {"version": "v3", "created": "Thu, 12 Jun 2014 13:11:53 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Campbell", "Trevor", ""], ["How", "Jonathan P.", ""]]}, {"id": "1403.7550", "submitter": "Ce Zhang", "authors": "Ce Zhang and Christopher R\\'e", "title": "DimmWitted: A Study of Main-Memory Statistical Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform the first study of the tradeoff space of access methods and\nreplication to support statistical analytics using first-order methods executed\nin the main memory of a Non-Uniform Memory Access (NUMA) machine. Statistical\nanalytics systems differ from conventional SQL-analytics in the amount and\ntypes of memory incoherence they can tolerate. Our goal is to understand\ntradeoffs in accessing the data in row- or column-order and at what granularity\none should share the model and data for a statistical task. We study this new\ntradeoff space, and discover there are tradeoffs between hardware and\nstatistical efficiency. We argue that our tradeoff study may provide valuable\ninformation for designers of analytics engines: for each system we consider,\nour prototype engine can run at least one popular task at least 100x faster. We\nconduct our study across five architectures using popular models including\nSVMs, logistic regression, Gibbs sampling, and neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 21:48:00 GMT"}, {"version": "v2", "created": "Thu, 1 May 2014 06:14:29 GMT"}, {"version": "v3", "created": "Mon, 7 Jul 2014 17:20:20 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Zhang", "Ce", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1403.7726", "submitter": "Ayman I. Madbouly", "authors": "Ayman I. Madbouly, Amr M. Gody, Tamer M. Barakat", "title": "Relevant Feature Selection Model Using Data Mining for Intrusion\n  Detection System", "comments": "12 Pages, 3 figures, 5 tables, Published with \"International Journal\n  of Engineering Trends and Technology (IJETT)\". arXiv admin note: text overlap\n  with arXiv:1208.5997 by other authors without attribution", "journal-ref": "International Journal of Engineering Trends and Technology\n  (IJETT), V9(10),501-512 March 2014", "doi": "10.14445/22315381/IJETT-V9P296", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network intrusions have become a significant threat in recent years as a\nresult of the increased demand of computer networks for critical systems.\nIntrusion detection system (IDS) has been widely deployed as a defense measure\nfor computer networks. Features extracted from network traffic can be used as\nsign to detect anomalies. However with the huge amount of network traffic,\ncollected data contains irrelevant and redundant features that affect the\ndetection rate of the IDS, consumes high amount of system resources, and\nslowdown the training and testing process of the IDS. In this paper, a new\nfeature selection model is proposed; this model can effectively select the most\nrelevant features for intrusion detection. Our goal is to build a lightweight\nintrusion detection system by using a reduced features set. Deleting irrelevant\nand redundant features helps to build a faster training and testing process, to\nhave less resource consumption as well as to maintain high detection rates. The\neffectiveness and the feasibility of our feature selection model were verified\nby several experiments on KDD intrusion detection dataset. The experimental\nresults strongly showed that our model is not only able to yield high detection\nrates but also to speed up the detection process.\n", "versions": [{"version": "v1", "created": "Sun, 30 Mar 2014 09:41:17 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Madbouly", "Ayman I.", ""], ["Gody", "Amr M.", ""], ["Barakat", "Tamer M.", ""]]}, {"id": "1403.7735", "submitter": "Ahmed El Shafie", "authors": "Ahmed El Shafie and Tamer Khattab and Hussien Saad and Amr Mohamed", "title": "Optimal Cooperative Cognitive Relaying and Spectrum Access for an Energy\n  Harvesting Cognitive Radio: Reinforcement Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a cognitive setting under the context of\ncooperative communications, where the cognitive radio (CR) user is assumed to\nbe a self-organized relay for the network. The CR user and the PU are assumed\nto be energy harvesters. The CR user cooperatively relays some of the\nundelivered packets of the primary user (PU). Specifically, the CR user stores\na fraction of the undelivered primary packets in a relaying queue (buffer). It\nmanages the flow of the undelivered primary packets to its relaying queue using\nthe appropriate actions over time slots. Moreover, it has the decision of\nchoosing the used queue for channel accessing at idle time slots (slots where\nthe PU's queue is empty). It is assumed that one data packet transmission\ndissipates one energy packet. The optimal policy changes according to the\nprimary and CR users arrival rates to the data and energy queues as well as the\nchannels connectivity. The CR user saves energy for the PU by taking the\nresponsibility of relaying the undelivered primary packets. It optimally\norganizes its own energy packets to maximize its payoff as time progresses.\n", "versions": [{"version": "v1", "created": "Sun, 30 Mar 2014 10:59:58 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 21:40:13 GMT"}], "update_date": "2014-07-10", "authors_parsed": [["Shafie", "Ahmed El", ""], ["Khattab", "Tamer", ""], ["Saad", "Hussien", ""], ["Mohamed", "Amr", ""]]}, {"id": "1403.7737", "submitter": "Shusen Wang", "authors": "Shusen Wang", "title": "Sharpened Error Bounds for Random Sampling Based $\\ell_2$ Regression", "comments": "unpublished manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a data matrix $X \\in R^{n\\times d}$ and a response vector $y \\in\nR^{n}$, suppose $n>d$, it costs $O(n d^2)$ time and $O(n d)$ space to solve the\nleast squares regression (LSR) problem. When $n$ and $d$ are both large,\nexactly solving the LSR problem is very expensive. When $n \\gg d$, one feasible\napproach to speeding up LSR is to randomly embed $y$ and all columns of $X$\ninto a smaller subspace $R^c$; the induced LSR problem has the same number of\ncolumns but much fewer number of rows, and it can be solved in $O(c d^2)$ time\nand $O(c d)$ space.\n  We discuss in this paper two random sampling based methods for solving LSR\nmore efficiently. Previous work showed that the leverage scores based sampling\nbased LSR achieves $1+\\epsilon$ accuracy when $c \\geq O(d \\epsilon^{-2} \\log\nd)$. In this paper we sharpen this error bound, showing that $c = O(d \\log d +\nd \\epsilon^{-1})$ is enough for achieving $1+\\epsilon$ accuracy. We also show\nthat when $c \\geq O(\\mu d \\epsilon^{-2} \\log d)$, the uniform sampling based\nLSR attains a $2+\\epsilon$ bound with positive probability.\n", "versions": [{"version": "v1", "created": "Sun, 30 Mar 2014 11:21:39 GMT"}, {"version": "v2", "created": "Sat, 5 Apr 2014 05:56:04 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Wang", "Shusen", ""]]}, {"id": "1403.7746", "submitter": "Miron Kursa", "authors": "Miron B. Kursa, Alicja A. Wieczorkowska", "title": "Multi-label Ferns for Efficient Recognition of Musical Instruments in\n  Recordings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce multi-label ferns, and apply this technique for\nautomatic classification of musical instruments in audio recordings. We compare\nthe performance of our proposed method to a set of binary random ferns, using\njazz recordings as input data. Our main result is obtaining much faster\nclassification and higher F-score. We also achieve substantial reduction of the\nmodel size.\n", "versions": [{"version": "v1", "created": "Sun, 30 Mar 2014 12:22:36 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Kursa", "Miron B.", ""], ["Wieczorkowska", "Alicja A.", ""]]}, {"id": "1403.7752", "submitter": "Yann Ollivier", "authors": "Yann Ollivier", "title": "Auto-encoders: reconstruction versus compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the similarities and differences between training an auto-encoder\nto minimize the reconstruction error, and training the same auto-encoder to\ncompress the data via a generative model. Minimizing a codelength for the data\nusing an auto-encoder is equivalent to minimizing the reconstruction error plus\nsome correcting terms which have an interpretation as either a denoising or\ncontractive property of the decoding function. These terms are related but not\nidentical to those used in denoising or contractive auto-encoders [Vincent et\nal. 2010, Rifai et al. 2011]. In particular, the codelength viewpoint fully\ndetermines an optimal noise level for the denoising criterion.\n", "versions": [{"version": "v1", "created": "Sun, 30 Mar 2014 13:11:55 GMT"}, {"version": "v2", "created": "Fri, 23 Jan 2015 19:12:05 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Ollivier", "Yann", ""]]}, {"id": "1403.7890", "submitter": "Xiangyu Chang", "authors": "Xiangyu Chang, Yu Wang, Rongjian Li, Zongben Xu", "title": "Sparse K-Means with $\\ell_{\\infty}/\\ell_0$ Penalty for High-Dimensional\n  Data Clustering", "comments": "36 pages, 4 figures, Present the paper at ICSA 2013", "journal-ref": "Statistica Sinica 28 (2018)1265-1284", "doi": null, "report-no": "SS-2015-0261", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse clustering, which aims to find a proper partition of an extremely\nhigh-dimensional data set with redundant noise features, has been attracted\nmore and more interests in recent years. The existing studies commonly solve\nthe problem in a framework of maximizing the weighted feature contributions\nsubject to a $\\ell_2/\\ell_1$ penalty. Nevertheless, this framework has two\nserious drawbacks: One is that the solution of the framework unavoidably\ninvolves a considerable portion of redundant noise features in many situations,\nand the other is that the framework neither offers intuitive explanations on\nwhy this framework can select relevant features nor leads to any theoretical\nguarantee for feature selection consistency.\n  In this article, we attempt to overcome those drawbacks through developing a\nnew sparse clustering framework which uses a $\\ell_{\\infty}/\\ell_0$ penalty.\nFirst, we introduce new concepts on optimal partitions and noise features for\nthe high-dimensional data clustering problems, based on which the previously\nknown framework can be intuitively explained in principle. Then, we apply the\nsuggested $\\ell_{\\infty}/\\ell_0$ framework to formulate a new sparse k-means\nmodel with the $\\ell_{\\infty}/\\ell_0$ penalty ($\\ell_0$-k-means for short). We\npropose an efficient iterative algorithm for solving the $\\ell_0$-k-means. To\ndeeply understand the behavior of $\\ell_0$-k-means, we prove that the solution\nyielded by the $\\ell_0$-k-means algorithm has feature selection consistency\nwhenever the data matrix is generated from a high-dimensional Gaussian mixture\nmodel. Finally, we provide experiments with both synthetic data and the Allen\nDeveloping Mouse Brain Atlas data to support that the proposed $\\ell_0$-k-means\nexhibits better noise feature detection capacity over the previously known\nsparse k-means with the $\\ell_2/\\ell_1$ penalty ($\\ell_1$-k-means for short).\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 07:18:55 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Chang", "Xiangyu", ""], ["Wang", "Yu", ""], ["Li", "Rongjian", ""], ["Xu", "Zongben", ""]]}, {"id": "1403.8084", "submitter": "Smriti Bhagat", "authors": "Stratis Ioannidis, Andrea Montanari, Udi Weinsberg, Smriti Bhagat,\n  Nadia Fawaz, Nina Taft", "title": "Privacy Tradeoffs in Predictive Analytics", "comments": "Extended version of the paper appearing in SIGMETRICS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online services routinely mine user data to predict user preferences, make\nrecommendations, and place targeted ads. Recent research has demonstrated that\nseveral private user attributes (such as political affiliation, sexual\norientation, and gender) can be inferred from such data. Can a\nprivacy-conscious user benefit from personalization while simultaneously\nprotecting her private attributes? We study this question in the context of a\nrating prediction service based on matrix factorization. We construct a\nprotocol of interactions between the service and users that has remarkable\noptimality properties: it is privacy-preserving, in that no inference algorithm\ncan succeed in inferring a user's private attribute with a probability better\nthan random guessing; it has maximal accuracy, in that no other\nprivacy-preserving protocol improves rating prediction; and, finally, it\ninvolves a minimal disclosure, as the prediction accuracy strictly decreases\nwhen the service reveals less information. We extensively evaluate our protocol\nusing several rating datasets, demonstrating that it successfully blocks the\ninference of gender, age and political affiliation, while incurring less than\n5% decrease in the accuracy of rating prediction.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 16:53:04 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Ioannidis", "Stratis", ""], ["Montanari", "Andrea", ""], ["Weinsberg", "Udi", ""], ["Bhagat", "Smriti", ""], ["Fawaz", "Nadia", ""], ["Taft", "Nina", ""]]}, {"id": "1403.8144", "submitter": "Ping Li", "authors": "Ping Li, Michael Mitzenmacher, Anshumali Shrivastava", "title": "Coding for Random Projections and Approximate Near Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical note compares two coding (quantization) schemes for random\nprojections in the context of sub-linear time approximate near neighbor search.\nThe first scheme is based on uniform quantization while the second scheme\nutilizes a uniform quantization plus a uniformly random offset (which has been\npopular in practice). The prior work compared the two schemes in the context of\nsimilarity estimation and training linear classifiers, with the conclusion that\nthe step of random offset is not necessary and may hurt the performance\n(depending on the similarity level). The task of near neighbor search is\nrelated to similarity estimation with importance distinctions and requires own\nstudy. In this paper, we demonstrate that in the context of near neighbor\nsearch, the step of random offset is not needed either and may hurt the\nperformance (sometimes significantly so, depending on the similarity and other\nparameters).\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 19:43:53 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Li", "Ping", ""], ["Mitzenmacher", "Michael", ""], ["Shrivastava", "Anshumali", ""]]}]