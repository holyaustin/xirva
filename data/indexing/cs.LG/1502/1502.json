[{"id": "1502.00060", "submitter": "Xing He", "authors": "Xing He, Robert Caiming Qiu, Qian Ai, Yinshuang Cao, Jie Gu, Zhijian\n  Jin", "title": "A Random Matrix Theoretical Approach to Early Event Detection in Smart\n  Grid", "comments": "12 pages, 11 figures, submitted to IEEE Transactions on Smart Grid", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power systems are developing very fast nowadays, both in size and in\ncomplexity; this situation is a challenge for Early Event Detection (EED). This\npaper proposes a data- driven unsupervised learning method to handle this\nchallenge. Specifically, the random matrix theories (RMTs) are introduced as\nthe statistical foundations for random matrix models (RMMs); based on the RMMs,\nlinear eigenvalue statistics (LESs) are defined via the test functions as the\nsystem indicators. By comparing the values of the LES between the experimental\nand the theoretical ones, the anomaly detection is conducted. Furthermore, we\ndevelop 3D power-map to visualize the LES; it provides a robust auxiliary\ndecision-making mechanism to the operators. In this sense, the proposed method\nconducts EED with a pure statistical procedure, requiring no knowledge of\nsystem topologies, unit operation/control models, etc. The LES, as a key\ningredient during this procedure, is a high dimensional indictor derived\ndirectly from raw data. As an unsupervised learning indicator, the LES is much\nmore sensitive than the low dimensional indictors obtained from supervised\nlearning. With the statistical procedure, the proposed method is universal and\nfast; moreover, it is robust against traditional EED challenges (such as error\naccumulations, spurious correlations, and even bad data in core area). Case\nstudies, with both simulated data and real ones, validate the proposed method.\nTo manage large-scale distributed systems, data fusion is mentioned as another\ndata processing ingredient.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 03:07:40 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 05:40:40 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["He", "Xing", ""], ["Qiu", "Robert Caiming", ""], ["Ai", "Qian", ""], ["Cao", "Yinshuang", ""], ["Gu", "Jie", ""], ["Jin", "Zhijian", ""]]}, {"id": "1502.00062", "submitter": "Mallenahalli Naresh Kumar Prof. Dr.", "authors": "Vadrevu Sree Hari Rao and Mallenahalli Naresh Kumar", "title": "A New Intelligence Based Approach for Computer-Aided Diagnosis of Dengue\n  Fever", "comments": "7 pages, 5 figures. arXiv admin note: substantial text overlap with\n  arXiv:1501.07093", "journal-ref": "Information Technology in Biomedicine, IEEE Transactions on ,\n  vol.16, no.1, pp.112,118, Jan. 2012", "doi": "10.1109/TITB.2011.2171978", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of the influential clinical symptoms and laboratory features\nthat help in the diagnosis of dengue fever in early phase of the illness would\naid in designing effective public health management and virological\nsurveillance strategies. Keeping this as our main objective we develop in this\npaper, a new computational intelligence based methodology that predicts the\ndiagnosis in real time, minimizing the number of false positives and false\nnegatives. Our methodology consists of three major components (i) a novel\nmissing value imputation procedure that can be applied on any data set\nconsisting of categorical (nominal) and/or numeric (real or integer) (ii) a\nwrapper based features selection method with genetic search for extracting a\nsubset of most influential symptoms that can diagnose the illness and (iii) an\nalternating decision tree method that employs boosting for generating highly\naccurate decision rules. The predictive models developed using our methodology\nare found to be more accurate than the state-of-the-art methodologies used in\nthe diagnosis of the dengue fever.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 03:15:12 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Rao", "Vadrevu Sree Hari", ""], ["Kumar", "Mallenahalli Naresh", ""]]}, {"id": "1502.00064", "submitter": "Huan Wang", "authors": "Huan Wang, John Wright, Daniel Spielman", "title": "A Batchwise Monotone Algorithm for Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a batchwise monotone algorithm for dictionary learning. Unlike the\nstate-of-the-art dictionary learning algorithms which impose sparsity\nconstraints on a sample-by-sample basis, we instead treat the samples as a\nbatch, and impose the sparsity constraint on the whole. The benefit of\nbatchwise optimization is that the non-zeros can be better allocated across the\nsamples, leading to a better approximation of the whole. To accomplish this, we\npropose procedures to switch non-zeros in both rows and columns in the support\nof the coefficient matrix to reduce the reconstruction error. We prove in the\nproposed support switching procedure the objective of the algorithm, i.e., the\nreconstruction error, decreases monotonically and converges. Furthermore, we\nintroduce a block orthogonal matching pursuit algorithm that also operates on\nsample batches to provide a warm start. Experiments on both natural image\npatches and UCI data sets show that the proposed algorithm produces a better\napproximation with the same sparsity levels compared to the state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 03:40:17 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Wang", "Huan", ""], ["Wright", "John", ""], ["Spielman", "Daniel", ""]]}, {"id": "1502.00068", "submitter": "Ameet Talwalkar", "authors": "Evan R. Sparks, Ameet Talwalkar, Michael J. Franklin, Michael I.\n  Jordan, Tim Kraska", "title": "TuPAQ: An Efficient Planner for Large-scale Predictive Analytic Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of massive datasets combined with the development of\nsophisticated analytical techniques have enabled a wide variety of novel\napplications such as improved product recommendations, automatic image tagging,\nand improved speech-driven interfaces. These and many other applications can be\nsupported by Predictive Analytic Queries (PAQs). A major obstacle to supporting\nPAQs is the challenging and expensive process of identifying and training an\nappropriate predictive model. Recent efforts aiming to automate this process\nhave focused on single node implementations and have assumed that model\ntraining itself is a black box, thus limiting the effectiveness of such\napproaches on large-scale problems. In this work, we build upon these recent\nefforts and propose an integrated PAQ planning architecture that combines\nadvanced model search techniques, bandit resource allocation via runtime\nalgorithm introspection, and physical optimization via batching. The result is\nTuPAQ, a component of the MLbase system, which solves the PAQ planning problem\nwith comparable quality to exhaustive strategies but an order of magnitude more\nefficiently than the standard baseline approach, and can scale to models\ntrained on terabytes of data across hundreds of machines.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 04:51:58 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2015 22:02:24 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Sparks", "Evan R.", ""], ["Talwalkar", "Ameet", ""], ["Franklin", "Michael J.", ""], ["Jordan", "Michael I.", ""], ["Kraska", "Tim", ""]]}, {"id": "1502.00093", "submitter": "Sotetsu Koyamada", "authors": "Sotetsu Koyamada and Yumi Shikauchi and Ken Nakae and Masanori Koyama\n  and Shin Ishii", "title": "Deep learning of fMRI big data: a novel approach to subject-transfer\n  decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a technology to read brain states from measurable brain activities, brain\ndecoding are widely applied in industries and medical sciences. In spite of\nhigh demands in these applications for a universal decoder that can be applied\nto all individuals simultaneously, large variation in brain activities across\nindividuals has limited the scope of many studies to the development of\nindividual-specific decoders. In this study, we used deep neural network (DNN),\na nonlinear hierarchical model, to construct a subject-transfer decoder. Our\ndecoder is the first successful DNN-based subject-transfer decoder. When\napplied to a large-scale functional magnetic resonance imaging (fMRI) database,\nour DNN-based decoder achieved higher decoding accuracy than other baseline\nmethods, including support vector machine (SVM). In order to analyze the\nknowledge acquired by this decoder, we applied principal sensitivity analysis\n(PSA) to the decoder and visualized the discriminative features that are common\nto all subjects in the dataset. Our PSA successfully visualized the\nsubject-independent features contributing to the subject-transferability of the\ntrained decoder.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 11:58:26 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Koyamada", "Sotetsu", ""], ["Shikauchi", "Yumi", ""], ["Nakae", "Ken", ""], ["Koyama", "Masanori", ""], ["Ishii", "Shin", ""]]}, {"id": "1502.00094", "submitter": "Roman Dovgopol", "authors": "Roman Dovgopol, Matt Nohelty", "title": "Twitter Hash Tag Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The rise in popularity of microblogging services like Twitter has led to\nincreased use of content annotation strategies like the hashtag. Hashtags\nprovide users with a tagging mechanism to help organize, group, and create\nvisibility for their posts. This is a simple idea but can be challenging for\nthe user in practice which leads to infrequent usage. In this paper, we will\ninvestigate various methods of recommending hashtags as new posts are created\nto encourage more widespread adoption and usage. Hashtag recommendation comes\nwith numerous challenges including processing huge volumes of streaming data\nand content which is small and noisy. We will investigate preprocessing methods\nto reduce noise in the data and determine an effective method of hashtag\nrecommendation based on the popular classification algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 12:15:53 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Dovgopol", "Roman", ""], ["Nohelty", "Matt", ""]]}, {"id": "1502.00133", "submitter": "Sumeet Katariya", "authors": "Kevin Jamieson, Sumeet Katariya, Atul Deshpande and Robert Nowak", "title": "Sparse Dueling Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dueling bandit problem is a variation of the classical multi-armed bandit\nin which the allowable actions are noisy comparisons between pairs of arms.\nThis paper focuses on a new approach for finding the \"best\" arm according to\nthe Borda criterion using noisy comparisons. We prove that in the absence of\nstructural assumptions, the sample complexity of this problem is proportional\nto the sum of the inverse squared gaps between the Borda scores of each\nsuboptimal arm and the best arm. We explore this dependence further and\nconsider structural constraints on the pairwise comparison matrix (a particular\nform of sparsity natural to this problem) that can significantly reduce the\nsample complexity. This motivates a new algorithm called Successive Elimination\nwith Comparison Sparsity (SECS) that exploits sparsity to find the Borda winner\nusing fewer samples than standard algorithms. We also evaluate the new\nalgorithm experimentally with synthetic and real data. The results show that\nthe sparsity model and the new algorithm can provide significant improvements\nover standard approaches.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 16:18:14 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Jamieson", "Kevin", ""], ["Katariya", "Sumeet", ""], ["Deshpande", "Atul", ""], ["Nowak", "Robert", ""]]}, {"id": "1502.00163", "submitter": "Alaa Saade", "authors": "Alaa Saade, Florent Krzakala, Marc Lelarge and Lenka Zdeborov\\'a", "title": "Spectral Detection in the Censored Block Model", "comments": "ISIT 2015", "journal-ref": "IEEE International Symposium on Information Theory (ISIT),\n  pp.1184-1188 (2015)", "doi": "10.1109/ISIT.2015.7282642", "report-no": null, "categories": "cs.SI cond-mat.dis-nn cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of partially recovering hidden binary variables from\nthe observation of (few) censored edge weights, a problem with applications in\ncommunity detection, correlation clustering and synchronization. We describe\ntwo spectral algorithms for this task based on the non-backtracking and the\nBethe Hessian operators. These algorithms are shown to be asymptotically\noptimal for the partial recovery problem, in that they detect the hidden\nassignment as soon as it is information theoretically possible to do so.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 21:20:53 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 20:50:30 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Saade", "Alaa", ""], ["Krzakala", "Florent", ""], ["Lelarge", "Marc", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1502.00182", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani, George Atia", "title": "High Dimensional Low Rank plus Sparse Matrix Decomposition", "comments": "IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2017.2649482", "report-no": null, "categories": "cs.NA cs.DS cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the problem of low rank plus sparse matrix\ndecomposition for big data. Conventional algorithms for matrix decomposition\nuse the entire data to extract the low-rank and sparse components, and are\nbased on optimization problems with complexity that scales with the dimension\nof the data, which limits their scalability. Furthermore, existing randomized\napproaches mostly rely on uniform random sampling, which is quite inefficient\nfor many real world data matrices that exhibit additional structures (e.g.\nclustering). In this paper, a scalable subspace-pursuit approach that\ntransforms the decomposition problem to a subspace learning problem is\nproposed. The decomposition is carried out using a small data sketch formed\nfrom sampled columns/rows. Even when the data is sampled uniformly at random,\nit is shown that the sufficient number of sampled columns/rows is roughly\nO(r\\mu), where \\mu is the coherency parameter and r the rank of the low rank\ncomponent. In addition, adaptive sampling algorithms are proposed to address\nthe problem of column/row sampling from structured data. We provide an analysis\nof the proposed method with adaptive sampling and show that adaptive sampling\nmakes the required number of sampled columns/rows invariant to the distribution\nof the data. The proposed approach is amenable to online implementation and an\nonline scheme is proposed.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 00:57:57 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 03:56:48 GMT"}, {"version": "v3", "created": "Thu, 16 Mar 2017 06:41:34 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1502.00186", "submitter": "Haiping Huang", "authors": "Haiping Huang and Taro Toyoizumi", "title": "Advanced Mean Field Theory of Restricted Boltzmann Machine", "comments": "5 pages, 4 figures, accepted by Phys Rev E (Rapid Communication)", "journal-ref": "Phys. Rev. E 91, 050101 (2015)", "doi": "10.1103/PhysRevE.91.050101", "report-no": null, "categories": "cond-mat.stat-mech cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in restricted Boltzmann machine is typically hard due to the\ncomputation of gradients of log-likelihood function. To describe the network\nstate statistics of the restricted Boltzmann machine, we develop an advanced\nmean field theory based on the Bethe approximation. Our theory provides an\nefficient message passing based method that evaluates not only the partition\nfunction (free energy) but also its gradients without requiring statistical\nsampling. The results are compared with those obtained by the computationally\nexpensive sampling based method.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 02:23:12 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2015 04:57:37 GMT"}, {"version": "v3", "created": "Sat, 2 May 2015 03:54:30 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Huang", "Haiping", ""], ["Toyoizumi", "Taro", ""]]}, {"id": "1502.00231", "submitter": "Yishi Zhang", "authors": "Zhijun Chen, Chaozhong Wu, Yishi Zhang, Zhen Huang, Bin Ran, Ming\n  Zhong, Nengchao Lyu", "title": "Feature Selection with Redundancy-complementariness Dispersion", "comments": "28 pages, 13 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection has attracted significant attention in data mining and\nmachine learning in the past decades. Many existing feature selection methods\neliminate redundancy by measuring pairwise inter-correlation of features,\nwhereas the complementariness of features and higher inter-correlation among\nmore than two features are ignored. In this study, a modification item\nconcerning the complementariness of features is introduced in the evaluation\ncriterion of features. Additionally, in order to identify the interference\neffect of already-selected False Positives (FPs), the\nredundancy-complementariness dispersion is also taken into account to adjust\nthe measurement of pairwise inter-correlation of features. To illustrate the\neffectiveness of proposed method, classification experiments are applied with\nfour frequently used classifiers on ten datasets. Classification results verify\nthe superiority of proposed method compared with five representative feature\nselection methods.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 10:44:26 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Chen", "Zhijun", ""], ["Wu", "Chaozhong", ""], ["Zhang", "Yishi", ""], ["Huang", "Zhen", ""], ["Ran", "Bin", ""], ["Zhong", "Ming", ""], ["Lyu", "Nengchao", ""]]}, {"id": "1502.00245", "submitter": "Christian Samuel Perone", "authors": "Christian S. Perone", "title": "Injury risk prediction for traffic accidents in Porto Alegre/RS, Brazil", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This study describes the experimental application of Machine Learning\ntechniques to build prediction models that can assess the injury risk\nassociated with traffic accidents. This work uses an freely available data set\nof traffic accident records that took place in the city of Porto Alegre/RS\n(Brazil) during the year of 2013. This study also provides an analysis of the\nmost important attributes of a traffic accident that could produce an outcome\nof injury to the people involved in the accident.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 12:57:40 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Perone", "Christian S.", ""]]}, {"id": "1502.00363", "submitter": "Wangmeng Zuo", "authors": "Wangmeng Zuo, Faqiang Wang, David Zhang, Liang Lin, Yuchi Huang, Deyu\n  Meng, Lei Zhang", "title": "Iterated Support Vector Machines for Distance Metric Learning", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance metric learning aims to learn from the given training data a valid\ndistance metric, with which the similarity between data samples can be more\neffectively evaluated for classification. Metric learning is often formulated\nas a convex or nonconvex optimization problem, while many existing metric\nlearning algorithms become inefficient for large scale problems. In this paper,\nwe formulate metric learning as a kernel classification problem, and solve it\nby iterated training of support vector machines (SVM). The new formulation is\neasy to implement, efficient in training, and tractable for large-scale\nproblems. Two novel metric learning models, namely Positive-semidefinite\nConstrained Metric Learning (PCML) and Nonnegative-coefficient Constrained\nMetric Learning (NCML), are developed. Both PCML and NCML can guarantee the\nglobal optimality of their solutions. Experimental results on UCI dataset\nclassification, handwritten digit recognition, face verification and person\nre-identification demonstrate that the proposed metric learning methods achieve\nhigher classification accuracy than state-of-the-art methods and they are\nsignificantly more efficient in training.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 05:30:44 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Zuo", "Wangmeng", ""], ["Wang", "Faqiang", ""], ["Zhang", "David", ""], ["Lin", "Liang", ""], ["Huang", "Yuchi", ""], ["Meng", "Deyu", ""], ["Zhang", "Lei", ""]]}, {"id": "1502.00512", "submitter": "Will Williams", "authors": "Will Williams, Niranjani Prasad, David Mrva, Tom Ash, Tony Robinson", "title": "Scaling Recurrent Neural Network Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the scaling properties of Recurrent Neural Network\nLanguage Models (RNNLMs). We discuss how to train very large RNNs on GPUs and\naddress the questions of how RNNLMs scale with respect to model size,\ntraining-set size, computational costs and memory. Our analysis shows that\ndespite being more costly to train, RNNLMs obtain much lower perplexities on\nstandard benchmarks than n-gram models. We train the largest known RNNs and\npresent relative word error rates gains of 18% on an ASR task. We also present\nthe new lowest perplexities on the recently released billion word language\nmodelling benchmark, 1 BLEU point gain on machine translation and a 17%\nrelative hit rate gain in word prediction.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 15:27:37 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Williams", "Will", ""], ["Prasad", "Niranjani", ""], ["Mrva", "David", ""], ["Ash", "Tom", ""], ["Robinson", "Tony", ""]]}, {"id": "1502.00524", "submitter": "Ricard Marxer", "authors": "Ricard Marxer and Hendrik Purwins", "title": "Unsupervised Incremental Learning and Prediction of Music Signals", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TASLP.2016.2530409", "report-no": null, "categories": "cs.SD cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A system is presented that segments, clusters and predicts musical audio in\nan unsupervised manner, adjusting the number of (timbre) clusters\ninstantaneously to the audio input. A sequence learning algorithm adapts its\nstructure to a dynamically changing clustering tree. The flow of the system is\nas follows: 1) segmentation by onset detection, 2) timbre representation of\neach segment by Mel frequency cepstrum coefficients, 3) discretization by\nincremental clustering, yielding a tree of different sound classes (e.g.\ninstruments) that can grow or shrink on the fly driven by the instantaneous\nsound events, resulting in a discrete symbol sequence, 4) extraction of\nstatistical regularities of the symbol sequence, using hierarchical N-grams and\nthe newly introduced conceptual Boltzmann machine, and 5) prediction of the\nnext sound event in the sequence. The system's robustness is assessed with\nrespect to complexity and noisiness of the signal. Clustering in isolation\nyields an adjusted Rand index (ARI) of 82.7% / 85.7% for data sets of singing\nvoice and drums. Onset detection jointly with clustering achieve an ARI of\n81.3% / 76.3% and the prediction of the entire system yields an ARI of 27.2% /\n39.2%.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 15:45:38 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2015 14:37:45 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Marxer", "Ricard", ""], ["Purwins", "Hendrik", ""]]}, {"id": "1502.00598", "submitter": "Maurits Kaptein", "authors": "Maurits Kaptein and Davide Iannuzzi", "title": "Lock in Feedback in Sequential Experiments", "comments": "20 Pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We often encounter situations in which an experimenter wants to find, by\nsequential experimentation, $x_{max} = \\arg\\max_{x} f(x)$, where $f(x)$ is a\n(possibly unknown) function of a well controllable variable $x$. Taking\ninspiration from physics and engineering, we have designed a new method to\naddress this problem. In this paper, we first introduce the method in\ncontinuous time, and then present two algorithms for use in sequential\nexperiments. Through a series of simulation studies, we show that the method is\neffective for finding maxima of unknown functions by experimentation, even when\nthe maximum of the functions drifts or when the signal to noise ratio is low.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 20:00:13 GMT"}, {"version": "v2", "created": "Tue, 3 Feb 2015 08:11:33 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 08:20:21 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Kaptein", "Maurits", ""], ["Iannuzzi", "Davide", ""]]}, {"id": "1502.00702", "submitter": "Hui Jiang", "authors": "Shiliang Zhang and Hui Jiang", "title": "Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to\n  Probe and Learn Neural Networks", "comments": "31 pages, 5 Figures, technical report", "journal-ref": "Journal of Machine Learning Research (JMLR), 17(37):1-33, 2016.\n  (http://jmlr.org/papers/v17/15-335.html)", "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel model for high-dimensional data, called the\nHybrid Orthogonal Projection and Estimation (HOPE) model, which combines a\nlinear orthogonal projection and a finite mixture model under a unified\ngenerative modeling framework. The HOPE model itself can be learned\nunsupervised from unlabelled data based on the maximum likelihood estimation as\nwell as discriminatively from labelled data. More interestingly, we have shown\nthe proposed HOPE models are closely related to neural networks (NNs) in a\nsense that each hidden layer can be reformulated as a HOPE model. As a result,\nthe HOPE framework can be used as a novel tool to probe why and how NNs work,\nmore importantly, to learn NNs in either supervised or unsupervised ways. In\nthis work, we have investigated the HOPE framework to learn NNs for several\nstandard tasks, including image recognition on MNIST and speech recognition on\nTIMIT. Experimental results have shown that the HOPE framework yields\nsignificant performance gains over the current state-of-the-art methods in\nvarious types of NN learning problems, including unsupervised feature learning,\nsupervised or semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 01:38:19 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2015 01:57:42 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Zhang", "Shiliang", ""], ["Jiang", "Hui", ""]]}, {"id": "1502.00725", "submitter": "Hongwei Li", "authors": "Hongwei Li and Qiang Liu", "title": "Cheaper and Better: Selecting Good Workers for Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing provides a popular paradigm for data collection at scale. We\nstudy the problem of selecting subsets of workers from a given worker pool to\nmaximize the accuracy under a budget constraint. One natural question is\nwhether we should hire as many workers as the budget allows, or restrict on a\nsmall number of top-quality workers. By theoretically analyzing the error rate\nof a typical setting in crowdsourcing, we frame the worker selection problem\ninto a combinatorial optimization problem and propose an algorithm to solve it\nefficiently. Empirical results on both simulated and real-world datasets show\nthat our algorithm is able to select a small number of high-quality workers,\nand performs as good as, sometimes even better than, the much larger crowds as\nthe budget allows.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 03:45:48 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Li", "Hongwei", ""], ["Liu", "Qiang", ""]]}, {"id": "1502.00731", "submitter": "Jaeho Shin", "authors": "Jaeho Shin, Sen Wu, Feiran Wang, Christopher De Sa, Ce Zhang,\n  Christopher R\\'e", "title": "Incremental Knowledge Base Construction Using DeepDive", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Populating a database with unstructured information is a long-standing\nproblem in industry and research that encompasses problems of extraction,\ncleaning, and integration. Recent names used for this problem include dealing\nwith dark data and knowledge base construction (KBC). In this work, we describe\nDeepDive, a system that combines database and machine learning ideas to help\ndevelop KBC systems, and we present techniques to make the KBC process more\nefficient. We observe that the KBC process is iterative, and we develop\ntechniques to incrementally produce inference results for KBC systems. We\npropose two methods for incremental inference, based respectively on sampling\nand variational techniques. We also study the tradeoff space of these methods\nand develop a simple rule-based optimizer. DeepDive includes all of these\ncontributions, and we evaluate DeepDive on five KBC systems, showing that it\ncan speed up KBC inference tasks by up to two orders of magnitude with\nnegligible impact on quality.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 04:16:24 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 21:59:15 GMT"}, {"version": "v3", "created": "Sat, 18 Apr 2015 06:13:32 GMT"}, {"version": "v4", "created": "Mon, 15 Jun 2015 22:24:05 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Shin", "Jaeho", ""], ["Wu", "Sen", ""], ["Wang", "Feiran", ""], ["De Sa", "Christopher", ""], ["Zhang", "Ce", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1502.01057", "submitter": "Li Zhou", "authors": "Li Zhou", "title": "Personalized Web Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalization is important for search engines to improve user experience.\nMost of the existing work do pure feature engineering and extract a lot of\nsession-style features and then train a ranking model. Here we proposed a novel\nway to model both long term and short term user behavior using Multi-armed\nbandit algorithm. Our algorithm can generalize session information across users\nwell, and as an Explore-Exploit style algorithm, it can generalize to new urls\nand new users well. Experiments show that our algorithm can improve performance\nover the default ranking and outperforms several popular Multi-armed bandit\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 22:37:37 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Zhou", "Li", ""]]}, {"id": "1502.01094", "submitter": "Soheil Bahrampour", "authors": "Soheil Bahrampour, Nasser M. Nasrabadi, Asok Ray, W. Kenneth Jenkins", "title": "Multimodal Task-Driven Dictionary Learning for Image Classification", "comments": "To appear at IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2015.2496275", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning algorithms have been successfully used for both\nreconstructive and discriminative tasks, where an input signal is represented\nwith a sparse linear combination of dictionary atoms. While these methods are\nmostly developed for single-modality scenarios, recent studies have\ndemonstrated the advantages of feature-level fusion based on the joint sparse\nrepresentation of the multimodal inputs. In this paper, we propose a multimodal\ntask-driven dictionary learning algorithm under the joint sparsity constraint\n(prior) to enforce collaborations among multiple homogeneous/heterogeneous\nsources of information. In this task-driven formulation, the multimodal\ndictionaries are learned simultaneously with their corresponding classifiers.\nThe resulting multimodal dictionaries can generate discriminative latent\nfeatures (sparse codes) from the data that are optimized for a given task such\nas binary or multiclass classification. Moreover, we present an extension of\nthe proposed formulation using a mixed joint and independent sparsity prior\nwhich facilitates more flexible fusion of the modalities at feature level. The\nefficacy of the proposed algorithms for multimodal classification is\nillustrated on four different applications -- multimodal face recognition,\nmulti-view face recognition, multi-view action recognition, and multimodal\nbiometric recognition. It is also shown that, compared to the counterpart\nreconstructive-based dictionary learning algorithms, the task-driven\nformulations are more computationally efficient in the sense that they can be\nequipped with more compact dictionaries and still achieve superior performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 05:17:50 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 07:26:59 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Bahrampour", "Soheil", ""], ["Nasrabadi", "Nasser M.", ""], ["Ray", "Asok", ""], ["Jenkins", "W. Kenneth", ""]]}, {"id": "1502.01176", "submitter": "Ethan Fetaya", "authors": "Ethan Fetaya and Shimon Ullman", "title": "Learning Local Invariant Mahalanobis Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many tasks and data types, there are natural transformations to which the\ndata should be invariant or insensitive. For instance, in visual recognition,\nnatural images should be insensitive to rotation and translation. This\nrequirement and its implications have been important in many machine learning\napplications, and tolerance for image transformations was primarily achieved by\nusing robust feature vectors. In this paper we propose a novel and\ncomputationally efficient way to learn a local Mahalanobis metric per datum,\nand show how we can learn a local invariant metric to any transformation in\norder to improve performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 12:27:04 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Fetaya", "Ethan", ""], ["Ullman", "Shimon", ""]]}, {"id": "1502.01418", "submitter": "Cem Tekin", "authors": "Cem Tekin and Mihaela van der Schaar", "title": "RELEAF: An Algorithm for Learning and Exploiting Relevance", "comments": "to appear in IEEE Journal of Selected Topics in Signal Processing,\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems, medical diagnosis, network security, etc., require\non-going learning and decision-making in real time. These -- and many others --\nrepresent perfect examples of the opportunities and difficulties presented by\nBig Data: the available information often arrives from a variety of sources and\nhas diverse features so that learning from all the sources may be valuable but\nintegrating what is learned is subject to the curse of dimensionality. This\npaper develops and analyzes algorithms that allow efficient learning and\ndecision-making while avoiding the curse of dimensionality. We formalize the\ninformation available to the learner/decision-maker at a particular time as a\ncontext vector which the learner should consider when taking actions. In\ngeneral the context vector is very high dimensional, but in many settings, the\nmost relevant information is embedded into only a few relevant dimensions. If\nthese relevant dimensions were known in advance, the problem would be simple --\nbut they are not. Moreover, the relevant dimensions may be different for\ndifferent actions. Our algorithm learns the relevant dimensions for each\naction, and makes decisions based in what it has learned. Formally, we build on\nthe structure of a contextual multi-armed bandit by adding and exploiting a\nrelevance relation. We prove a general regret bound for our algorithm whose\ntime order depends only on the maximum number of relevant dimensions among all\nthe actions, which in the special case where the relevance relation is\nsingle-valued (a function), reduces to $\\tilde{O}(T^{2(\\sqrt{2}-1)})$; in the\nabsence of a relevance relation, the best known contextual bandit algorithms\nachieve regret $\\tilde{O}(T^{(D+1)/(D+2)})$, where $D$ is the full dimension of\nthe context vector.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 03:03:16 GMT"}, {"version": "v2", "created": "Sat, 7 Feb 2015 08:29:14 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Tekin", "Cem", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1502.01493", "submitter": "Pierre Dupont", "authors": "Samuel Branders, Roberto D'Ambrosio and Pierre Dupont", "title": "A mixture Cox-Logistic model for feature selection from survival and\n  classification data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an original approach for jointly fitting survival times\nand classifying samples into subgroups. The Coxlogit model is a generalized\nlinear model with a common set of selected features for both tasks. Survival\ntimes and class labels are here assumed to be conditioned by a common risk\nscore which depends on those features. Learning is then naturally expressed as\nmaximizing the joint probability of subgroup labels and the ordering of\nsurvival events, conditioned to a common weight vector. The model is estimated\nby minimizing a regularized log-likelihood through a coordinate descent\nalgorithm.\n  Validation on synthetic and breast cancer data shows that the proposed\napproach outperforms a standard Cox model or logistic regression when both\npredicting the survival times and classifying new samples into subgroups. It is\nalso better at selecting informative features for both tasks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 10:45:54 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Branders", "Samuel", ""], ["D'Ambrosio", "Roberto", ""], ["Dupont", "Pierre", ""]]}, {"id": "1502.01563", "submitter": "Emanuele Frandi", "authors": "Emanuele Frandi, Ricardo Nanculef, Johan A. K. Suykens", "title": "A PARTAN-Accelerated Frank-Wolfe Algorithm for Large-Scale SVM\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frank-Wolfe algorithms have recently regained the attention of the Machine\nLearning community. Their solid theoretical properties and sparsity guarantees\nmake them a suitable choice for a wide range of problems in this field. In\naddition, several variants of the basic procedure exist that improve its\ntheoretical properties and practical performance. In this paper, we investigate\nthe application of some of these techniques to Machine Learning, focusing in\nparticular on a Parallel Tangent (PARTAN) variant of the FW algorithm that has\nnot been previously suggested or studied for this type of problems. We provide\nexperiments both in a standard setting and using a stochastic speed-up\ntechnique, showing that the considered algorithms obtain promising results on\nseveral medium and large-scale benchmark datasets for SVM classification.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 14:17:55 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Frandi", "Emanuele", ""], ["Nanculef", "Ricardo", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1502.01632", "submitter": "Francesco Orabona", "authors": "Francesco Orabona", "title": "A Simple Expression for Mill's Ratio of the Student's $t$-Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I show a simple expression of the Mill's ratio of the Student's\nt-Distribution. I use it to prove Conjecture 1 in P. Auer, N. Cesa-Bianchi, and\nP. Fischer. Finite-time analysis of the multiarmed bandit problem. Mach.\nLearn., 47(2-3):235--256, May 2002.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 16:37:31 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Orabona", "Francesco", ""]]}, {"id": "1502.01664", "submitter": "Lewis Evans Mr", "authors": "Lewis P. G. Evans and Niall M. Adams and Christoforos Anagnostopoulos", "title": "Estimating Optimal Active Learning via Model Retraining Improvement", "comments": "arXiv admin note: substantial text overlap with arXiv:1407.8042", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central question for active learning (AL) is: \"what is the optimal\nselection?\" Defining optimality by classifier loss produces a new\ncharacterisation of optimal AL behaviour, by treating expected loss reduction\nas a statistical target for estimation. This target forms the basis of model\nretraining improvement (MRI), a novel approach providing a statistical\nestimation framework for AL. This framework is constructed to address the\ncentral question of AL optimality, and to motivate the design of estimation\nalgorithms. MRI allows the exploration of optimal AL behaviour, and the\nexamination of AL heuristics, showing precisely how they make sub-optimal\nselections. The abstract formulation of MRI is used to provide a new guarantee\nfor AL, that an unbiased MRI estimator should outperform random selection. This\nMRI framework reveals intricate estimation issues that in turn motivate the\nconstruction of new statistical AL algorithms. One new algorithm in particular\nperforms strongly in a large-scale experimental study, compared to standard AL\nmethods. This competitive performance suggests that practical efforts to\nminimise estimation bias may be important for AL applications.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 18:13:34 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Evans", "Lewis P. G.", ""], ["Adams", "Niall M.", ""], ["Anagnostopoulos", "Christoforos", ""]]}, {"id": "1502.01682", "submitter": "Michael Bloodgood", "authors": "Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr, Chris\n  Callison-Burch, Nathaniel W. Filardo, Christine Piatko, Lori Levin and Scott\n  Miller", "title": "Use of Modality and Negation in Semantically-Informed Syntactic MT", "comments": "28 pages, 13 figures, 2 tables; appeared in Computational\n  Linguistics, 38(2):411-438, 2012", "journal-ref": "Computational Linguistics, 38(2):411-438, 2012", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the resource- and system-building efforts of an\neight-week Johns Hopkins University Human Language Technology Center of\nExcellence Summer Camp for Applied Language Exploration (SCALE-2009) on\nSemantically-Informed Machine Translation (SIMT). We describe a new\nmodality/negation (MN) annotation scheme, the creation of a (publicly\navailable) MN lexicon, and two automated MN taggers that we built using the\nannotation scheme and lexicon. Our annotation scheme isolates three components\nof modality and negation: a trigger (a word that conveys modality or negation),\na target (an action associated with modality or negation) and a holder (an\nexperiencer of modality). We describe how our MN lexicon was semi-automatically\nproduced and we demonstrate that a structure-based MN tagger results in\nprecision around 86% (depending on genre) for tagging of a standard LDC data\nset.\n  We apply our MN annotation scheme to statistical machine translation using a\nsyntactic framework that supports the inclusion of semantic annotations.\nSyntactic tags enriched with semantic annotations are assigned to parse trees\nin the target-language training texts through a process of tree grafting. While\nthe focus of our work is modality and negation, the tree grafting procedure is\ngeneral and supports other types of semantic information. We exploit this\ncapability by including named entities, produced by a pre-existing tagger, in\naddition to the MN elements produced by the taggers described in this paper.\nThe resulting system significantly outperformed a linguistically naive baseline\nmodel (Hiero), and reached the highest scores yet reported on the NIST 2009\nUrdu-English test set. This finding supports the hypothesis that both syntactic\nand semantic information can improve translation quality.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 19:10:26 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Baker", "Kathryn", ""], ["Bloodgood", "Michael", ""], ["Dorr", "Bonnie J.", ""], ["Callison-Burch", "Chris", ""], ["Filardo", "Nathaniel W.", ""], ["Piatko", "Christine", ""], ["Levin", "Lori", ""], ["Miller", "Scott", ""]]}, {"id": "1502.01705", "submitter": "Xiaozhao Zhao", "authors": "Xiaozhao Zhao, Yuexian Hou, Dawei Song, Wenjie Li", "title": "A Confident Information First Principle for Parametric Reduction and\n  Model Selection of Boltzmann Machines", "comments": "16pages. arXiv admin note: substantial text overlap with\n  arXiv:1302.3931", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical dimensionality reduction (DR) methods are often data-oriented,\nfocusing on directly reducing the number of random variables (features) while\nretaining the maximal variations in the high-dimensional data. In unsupervised\nsituations, one of the main limitations of these methods lies in their\ndependency on the scale of data features. This paper aims to address the\nproblem from a new perspective and considers model-oriented dimensionality\nreduction in parameter spaces of binary multivariate distributions.\n  Specifically, we propose a general parameter reduction criterion, called\nConfident-Information-First (CIF) principle, to maximally preserve confident\nparameters and rule out less confident parameters. Formally, the confidence of\neach parameter can be assessed by its contribution to the expected Fisher\ninformation distance within the geometric manifold over the neighbourhood of\nthe underlying real distribution.\n  We then revisit Boltzmann machines (BM) from a model selection perspective\nand theoretically show that both the fully visible BM (VBM) and the BM with\nhidden units can be derived from the general binary multivariate distribution\nusing the CIF principle. This can help us uncover and formalize the essential\nparts of the target density that BM aims to capture and the non-essential parts\nthat BM should discard. Guided by the theoretical analysis, we develop a\nsample-specific CIF for model selection of BM that is adaptive to the observed\nsamples. The method is studied in a series of density estimation experiments\nand has been shown effective in terms of the estimate accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 20:28:01 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Zhao", "Xiaozhao", ""], ["Hou", "Yuexian", ""], ["Song", "Dawei", ""], ["Li", "Wenjie", ""]]}, {"id": "1502.01710", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Yann LeCun", "title": "Text Understanding from Scratch", "comments": "This technical report is superseded by a paper entitled\n  \"Character-level Convolutional Networks for Text Classification\",\n  arXiv:1509.01626. It has considerably more experimental results and a\n  rewritten introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article demontrates that we can apply deep learning to text\nunderstanding from character-level inputs all the way up to abstract text\nconcepts, using temporal convolutional networks (ConvNets). We apply ConvNets\nto various large-scale datasets, including ontology classification, sentiment\nanalysis, and text categorization. We show that temporal ConvNets can achieve\nastonishing performance without the knowledge of words, phrases, sentences and\nany other syntactic or semantic structures with regards to a human language.\nEvidence shows that our models can work for both English and Chinese.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 20:45:19 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2015 21:32:01 GMT"}, {"version": "v3", "created": "Sun, 7 Jun 2015 03:45:02 GMT"}, {"version": "v4", "created": "Tue, 8 Sep 2015 04:42:29 GMT"}, {"version": "v5", "created": "Mon, 4 Apr 2016 02:40:48 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Zhang", "Xiang", ""], ["LeCun", "Yann", ""]]}, {"id": "1502.01733", "submitter": "Othman Soufan", "authors": "Othman Soufan and Samer Arafat", "title": "Arrhythmia Detection using Mutual Information-Based Integration Method", "comments": "6 pages, 1 figure, 7 tables, WConSC 2011 conference\n  http://www.ece.ualberta.ca/~reform/WConSC/ (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to propose an application of mutual\ninformation-based ensemble methods to the analysis and classification of heart\nbeats associated with different types of Arrhythmia. Models of multilayer\nperceptrons, support vector machines, and radial basis function neural networks\nwere trained and tested using the MIT-BIH arrhythmia database. This research\nbrings a focus to an ensemble method that, to our knowledge, is a novel\napplication in the area of ECG Arrhythmia detection. The proposed classifier\nensemble method showed improved performance, relative to either majority voting\nclassifier integration or to individual classifier performance. The overall\nensemble accuracy was 98.25%.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 21:31:25 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Soufan", "Othman", ""], ["Arafat", "Samer", ""]]}, {"id": "1502.01753", "submitter": "Peter Wittek", "authors": "Peter Wittek, S\\'andor Dar\\'anyi, Efstratios Kontopoulos, Theodoros\n  Moysiadis, Ioannis Kompatsiaris", "title": "Monitoring Term Drift Based on Semantic Consistency in an Evolving\n  Vector Field", "comments": "8 pages, 1 figure. Code used to conduct the experiments is available\n  at https://github.com/peterwittek/concept_drifts", "journal-ref": "Proceedings of IJCNN-15, International Joint Conference on Neural\n  Networks, pages 1--8, 2015", "doi": "10.1109/IJCNN.2015.7280766", "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the Aristotelian concept of potentiality vs. actuality allowing for\nthe study of energy and dynamics in language, we propose a field approach to\nlexical analysis. Falling back on the distributional hypothesis to\nstatistically model word meaning, we used evolving fields as a metaphor to\nexpress time-dependent changes in a vector space model by a combination of\nrandom indexing and evolving self-organizing maps (ESOM). To monitor semantic\ndrifts within the observation period, an experiment was carried out on the term\nspace of a collection of 12.8 million Amazon book reviews. For evaluation, the\nsemantic consistency of ESOM term clusters was compared with their respective\nneighbourhoods in WordNet, and contrasted with distances among term vectors by\nrandom indexing. We found that at 0.05 level of significance, the terms in the\nclusters showed a high level of semantic consistency. Tracking the drift of\ndistributional patterns in the term space across time periods, we found that\nconsistency decreased, but not at a statistically significant level. Our method\nis highly scalable, with interpretations in philosophy.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 22:51:45 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Wittek", "Peter", ""], ["Dar\u00e1nyi", "S\u00e1ndor", ""], ["Kontopoulos", "Efstratios", ""], ["Moysiadis", "Theodoros", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "1502.01783", "submitter": "Venkatesh Saligrama", "authors": "Jing Qian, Jonathan Root, Venkatesh Saligrama", "title": "Learning Efficient Anomaly Detectors from $K$-NN Graphs", "comments": "arXiv admin note: text overlap with arXiv:1405.0530", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a non-parametric anomaly detection algorithm for high dimensional\ndata. We score each datapoint by its average $K$-NN distance, and rank them\naccordingly. We then train limited complexity models to imitate these scores\nbased on the max-margin learning-to-rank framework. A test-point is declared as\nan anomaly at $\\alpha$-false alarm level if the predicted score is in the\n$\\alpha$-percentile. The resulting anomaly detector is shown to be\nasymptotically optimal in that for any false alarm rate $\\alpha$, its decision\nregion converges to the $\\alpha$-percentile minimum volume level set of the\nunknown underlying density. In addition, we test both the statistical\nperformance and computational efficiency of our algorithm on a number of\nsynthetic and real-data experiments. Our results demonstrate the superiority of\nour algorithm over existing $K$-NN based anomaly detection algorithms, with\nsignificant computational savings.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 03:36:51 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Qian", "Jing", ""], ["Root", "Jonathan", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1502.01823", "submitter": "Anurag Kumar", "authors": "Anurag Kumar, Bhiksha Raj", "title": "Unsupervised Fusion Weight Learning in Multiple Classifier Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an unsupervised method to learn the weights with\nwhich the scores of multiple classifiers must be combined in classifier fusion\nsettings. We also introduce a novel metric for ranking instances based on an\nindex which depends upon the rank of weighted scores of test points among the\nweighted scores of training points. We show that the optimized index can be\nused for computing measures such as average precision. Unlike most classifier\nfusion methods where a single weight is learned to weigh all examples our\nmethod learns instance-specific weights. The problem is formulated as learning\nthe weight which maximizes a clarity index; subsequently the index itself and\nthe learned weights both are used separately to rank all the test points. Our\nmethod gives an unsupervised method of optimizing performance on actual test\ndata, unlike the well known stacking-based methods where optimization is done\nover a labeled training set. Moreover, we show that our method is tolerant to\nnoisy classifiers and can be used for selecting N-best classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 08:28:57 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Kumar", "Anurag", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1502.01827", "submitter": "Guang-Tong Zhou", "authors": "Guang-Tong Zhou, Sung Ju Hwang, Mark Schmidt, Leonid Sigal and Greg\n  Mori", "title": "Hierarchical Maximum-Margin Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hierarchical maximum-margin clustering method for unsupervised\ndata analysis. Our method extends beyond flat maximum-margin clustering, and\nperforms clustering recursively in a top-down manner. We propose an effective\ngreedy splitting criteria for selecting which cluster to split next, and employ\nregularizers that enforce feature sharing/competition for capturing data\nsemantics. Experimental results obtained on four standard datasets show that\nour method outperforms flat and hierarchical clustering baselines, while\nforming clean and semantically meaningful cluster hierarchies.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 08:37:55 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Zhou", "Guang-Tong", ""], ["Hwang", "Sung Ju", ""], ["Schmidt", "Mark", ""], ["Sigal", "Leonid", ""], ["Mori", "Greg", ""]]}, {"id": "1502.01852", "submitter": "Kaiming He", "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun", "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on\n  ImageNet Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rectified activation units (rectifiers) are essential for state-of-the-art\nneural networks. In this work, we study rectifier neural networks for image\nclassification from two aspects. First, we propose a Parametric Rectified\nLinear Unit (PReLU) that generalizes the traditional rectified unit. PReLU\nimproves model fitting with nearly zero extra computational cost and little\noverfitting risk. Second, we derive a robust initialization method that\nparticularly considers the rectifier nonlinearities. This method enables us to\ntrain extremely deep rectified models directly from scratch and to investigate\ndeeper or wider network architectures. Based on our PReLU networks\n(PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012\nclassification dataset. This is a 26% relative improvement over the ILSVRC 2014\nwinner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass\nhuman-level performance (5.1%, Russakovsky et al.) on this visual recognition\nchallenge.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 10:44:00 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["He", "Kaiming", ""], ["Zhang", "Xiangyu", ""], ["Ren", "Shaoqing", ""], ["Sun", "Jian", ""]]}, {"id": "1502.02072", "submitter": "Bharath Ramsundar", "authors": "Bharath Ramsundar, Steven Kearnes, Patrick Riley, Dale Webster, David\n  Konerding, Vijay Pande", "title": "Massively Multitask Networks for Drug Discovery", "comments": "Preliminary work. Under review by the International Conference on\n  Machine Learning (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massively multitask neural architectures provide a learning framework for\ndrug discovery that synthesizes information from many distinct biological\nsources. To train these architectures at scale, we gather large amounts of data\nfrom public sources to create a dataset of nearly 40 million measurements\nacross more than 200 biological targets. We investigate several aspects of the\nmultitask framework by performing a series of empirical studies and obtain some\ninteresting results: (1) massively multitask networks obtain predictive\naccuracies significantly better than single-task methods, (2) the predictive\npower of multitask networks improves as additional tasks and data are added,\n(3) the total amount of data and the total number of tasks both contribute\nsignificantly to multitask improvement, and (4) multitask networks afford\nlimited transferability to tasks not in the training set. Our results\nunderscore the need for greater data sharing and further algorithmic innovation\nto accelerate the drug discovery process.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 23:04:01 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Ramsundar", "Bharath", ""], ["Kearnes", "Steven", ""], ["Riley", "Patrick", ""], ["Webster", "Dale", ""], ["Konerding", "David", ""], ["Pande", "Vijay", ""]]}, {"id": "1502.02077", "submitter": "Matthew Hirn", "authors": "Matthew Hirn and Nicolas Poilvert and St\\'ephane Mallat", "title": "Quantum Energy Regression using Scattering Transforms", "comments": "9 pages, 2 figures, 1 table. v2: Correction to Section 4.3. v3:\n  Replaced by arXiv:1605.04654", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.chem-ph physics.comp-ph quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to the regression of quantum mechanical energies\nbased on a scattering transform of an intermediate electron density\nrepresentation. A scattering transform is a deep convolution network computed\nwith a cascade of multiscale wavelet transforms. It possesses appropriate\ninvariant and stability properties for quantum energy regression. This new\nframework removes fundamental limitations of Coulomb matrix based energy\nregressions, and numerical experiments give state-of-the-art accuracy over\nplanar molecules.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 23:55:13 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2015 19:21:29 GMT"}, {"version": "v3", "created": "Fri, 20 May 2016 14:02:49 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Hirn", "Matthew", ""], ["Poilvert", "Nicolas", ""], ["Mallat", "St\u00e9phane", ""]]}, {"id": "1502.02125", "submitter": "Cem Tekin", "authors": "Cem Tekin and Mihaela van der Schaar", "title": "Contextual Online Learning for Multimedia Content Aggregation", "comments": "To appear in IEEE Transactions on Multimedia, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has witnessed a tremendous growth in the volume as well as\nthe diversity of multimedia content generated by a multitude of sources (news\nagencies, social media, etc.). Faced with a variety of content choices,\nconsumers are exhibiting diverse preferences for content; their preferences\noften depend on the context in which they consume content as well as various\nexogenous events. To satisfy the consumers' demand for such diverse content,\nmultimedia content aggregators (CAs) have emerged which gather content from\nnumerous multimedia sources. A key challenge for such systems is to accurately\npredict what type of content each of its consumers prefers in a certain\ncontext, and adapt these predictions to the evolving consumers' preferences,\ncontexts and content characteristics. We propose a novel, distributed, online\nmultimedia content aggregation framework, which gathers content generated by\nmultiple heterogeneous producers to fulfill its consumers' demand for content.\nSince both the multimedia content characteristics and the consumers'\npreferences and contexts are unknown, the optimal content aggregation strategy\nis unknown a priori. Our proposed content aggregation algorithm is able to\nlearn online what content to gather and how to match content and users by\nexploiting similarities between consumer types. We prove bounds for our\nproposed learning algorithms that guarantee both the accuracy of the\npredictions as well as the learning speed. Importantly, our algorithms operate\nefficiently even when feedback from consumers is missing or content and\npreferences evolve over time. Illustrative results highlight the merits of the\nproposed content aggregation system in a variety of settings.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2015 11:14:10 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2015 12:04:41 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Tekin", "Cem", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1502.02127", "submitter": "Marc Claesen", "authors": "Marc Claesen and Bart De Moor", "title": "Hyperparameter Search in Machine Learning", "comments": "5 pages, accepted for MIC 2015: The XI Metaheuristics International\n  Conference in Agadir, Morocco", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the hyperparameter search problem in the field of machine\nlearning and discuss its main challenges from an optimization perspective.\nMachine learning methods attempt to build models that capture some element of\ninterest based on given data. Most common learning algorithms feature a set of\nhyperparameters that must be determined before training commences. The choice\nof hyperparameters can significantly affect the resulting model's performance,\nbut determining good values can be complex; hence a disciplined, theoretically\nsound search strategy is essential.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2015 11:46:22 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2015 15:44:52 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Claesen", "Marc", ""], ["De Moor", "Bart", ""]]}, {"id": "1502.02158", "submitter": "Roi Weiss", "authors": "Roi Weiss, Boaz Nadler", "title": "Learning Parametric-Output HMMs with Two Aliased States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various applications involving hidden Markov models (HMMs), some of the\nhidden states are aliased, having identical output distributions. The\nminimality, identifiability and learnability of such aliased HMMs have been\nlong standing problems, with only partial solutions provided thus far. In this\npaper we focus on parametric-output HMMs, whose output distributions come from\na parametric family, and that have exactly two aliased states. For this class,\nwe present a complete characterization of their minimality and identifiability.\nFurthermore, for a large family of parametric output distributions, we derive\ncomputationally efficient and statistically consistent algorithms to detect the\npresence of aliasing and learn the aliased HMM transition and emission\nparameters. We illustrate our theoretical analysis by several simulations.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2015 16:21:28 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Weiss", "Roi", ""], ["Nadler", "Boaz", ""]]}, {"id": "1502.02206", "submitter": "Kai-Wei Chang", "authors": "Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daum\\'e III,\n  John Langford", "title": "Learning to Search Better Than Your Teacher", "comments": "In ICML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for learning to search for structured prediction typically imitate a\nreference policy, with existing theoretical guarantees demonstrating low regret\ncompared to that reference. This is unsatisfactory in many applications where\nthe reference policy is suboptimal and the goal of learning is to improve upon\nit. Can learning to search work even when the reference is poor?\n  We provide a new learning to search algorithm, LOLS, which does well relative\nto the reference policy, but additionally guarantees low regret compared to\ndeviations from the learned policy: a local-optimality guarantee. Consequently,\nLOLS can improve upon the reference policy, unlike previous algorithms. This\nenables us to develop structured contextual bandits, a partial information\nstructured prediction setting with many potential applications.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 03:18:50 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 05:48:10 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Chang", "Kai-Wei", ""], ["Krishnamurthy", "Akshay", ""], ["Agarwal", "Alekh", ""], ["Daum\u00e9", "Hal", "III"], ["Langford", "John", ""]]}, {"id": "1502.02215", "submitter": "Jobin Wilson", "authors": "Jobin Wilson, Chitharanj Kachappilly, Rakesh Mohan, Prateek Kapadia,\n  Arun Soman, Santanu Chaudhury", "title": "Real World Applications of Machine Learning Techniques over Large Mobile\n  Subscriber Datasets", "comments": "SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop)\n  https://sites.google.com/site/software4ml/accepted-papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication Service Providers (CSPs) are in a unique position to utilize\ntheir vast transactional data assets generated from interactions of subscribers\nwith network elements as well as with other subscribers. CSPs could leverage\nits data assets for a gamut of applications such as service personalization,\npredictive offer management, loyalty management, revenue forecasting, network\ncapacity planning, product bundle optimization and churn management to gain\nsignificant competitive advantage. However, due to the sheer data volume,\nvariety, velocity and veracity of mobile subscriber datasets, sophisticated\ndata analytics techniques and frameworks are necessary to derive actionable\ninsights in a useable timeframe. In this paper, we describe our journey from a\nrelational database management system (RDBMS) based campaign management\nsolution which allowed data scientists and marketers to use hand-written rules\nfor service personalization and targeted promotions to a distributed Big Data\nAnalytics platform, capable of performing large scale machine learning and data\nmining to deliver real time service personalization, predictive modelling and\nproduct optimization. Our work involves a careful blend of technology,\nprocesses and best practices, which facilitate man-machine collaboration and\ncontinuous experimentation to derive measurable economic value from data. Our\nplatform has a reach of more than 500 million mobile subscribers worldwide,\ndelivering over 1 billion personalized recommendations annually, processing a\ntotal data volume of 64 Petabytes, corresponding to 8.5 trillion events.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 06:18:55 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Wilson", "Jobin", ""], ["Kachappilly", "Chitharanj", ""], ["Mohan", "Rakesh", ""], ["Kapadia", "Prateek", ""], ["Soman", "Arun", ""], ["Chaudhury", "Santanu", ""]]}, {"id": "1502.02251", "submitter": "Marc Deisenroth", "authors": "Niklas Wahlstr\\\"om and Thomas B. Sch\\\"on and Marc Peter Deisenroth", "title": "From Pixels to Torques: Policy Learning with Deep Dynamical Models", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-efficient learning in continuous state-action spaces using very\nhigh-dimensional observations remains a key challenge in developing fully\nautonomous systems. In this paper, we consider one instance of this challenge,\nthe pixels to torques problem, where an agent must learn a closed-loop control\npolicy from pixel information only. We introduce a data-efficient, model-based\nreinforcement learning algorithm that learns such a closed-loop policy directly\nfrom pixel information. The key ingredient is a deep dynamical model that uses\ndeep auto-encoders to learn a low-dimensional embedding of images jointly with\na predictive model in this low-dimensional feature space. Joint learning\nensures that not only static but also dynamic properties of the data are\naccounted for. This is crucial for long-term predictions, which lie at the core\nof the adaptive model predictive control strategy that we use for closed-loop\ncontrol. Compared to state-of-the-art reinforcement learning methods for\ncontinuous states and actions, our approach learns quickly, scales to\nhigh-dimensional state spaces and is an important step toward fully autonomous\nlearning from pixels to torques.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 13:57:59 GMT"}, {"version": "v2", "created": "Tue, 10 Feb 2015 11:27:12 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2015 16:59:43 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Wahlstr\u00f6m", "Niklas", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1502.02259", "submitter": "Assaf Hallak", "authors": "Assaf Hallak, Dotan Di Castro and Shie Mannor", "title": "Contextual Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a planning problem where the dynamics and rewards of the\nenvironment depend on a hidden static parameter referred to as the context. The\nobjective is to learn a strategy that maximizes the accumulated reward across\nall contexts. The new model, called Contextual Markov Decision Process (CMDP),\ncan model a customer's behavior when interacting with a website (the learner).\nThe customer's behavior depends on gender, age, location, device, etc. Based on\nthat behavior, the website objective is to determine customer characteristics,\nand to optimize the interaction between them. Our work focuses on one basic\nscenario--finite horizon with a small known number of possible contexts. We\nsuggest a family of algorithms with provable guarantees that learn the\nunderlying models and the latent contexts, and optimize the CMDPs. Bounds are\nobtained for specific naive implementations, and extensions of the framework\nare discussed, laying the ground for future research.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 14:58:50 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Hallak", "Assaf", ""], ["Di Castro", "Dotan", ""], ["Mannor", "Shie", ""]]}, {"id": "1502.02268", "submitter": "Martin Tak\\'a\\v{c}", "authors": "Zheng Qu and Peter Richt\\'arik and Martin Tak\\'a\\v{c} and Olivier\n  Fercoq", "title": "SDNA: Stochastic Dual Newton Ascent for Empirical Risk Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for minimizing regularized empirical loss:\nStochastic Dual Newton Ascent (SDNA). Our method is dual in nature: in each\niteration we update a random subset of the dual variables. However, unlike\nexisting methods such as stochastic dual coordinate ascent, SDNA is capable of\nutilizing all curvature information contained in the examples, which leads to\nstriking improvements in both theory and practice - sometimes by orders of\nmagnitude. In the special case when an L2-regularizer is used in the primal,\nthe dual problem is a concave quadratic maximization problem plus a separable\nterm. In this regime, SDNA in each step solves a proximal subproblem involving\na random principal submatrix of the Hessian of the quadratic function; whence\nthe name of the method. If, in addition, the loss functions are quadratic, our\nmethod can be interpreted as a novel variant of the recently introduced\nIterative Hessian Sketch.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 16:34:41 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Qu", "Zheng", ""], ["Richt\u00e1rik", "Peter", ""], ["Tak\u00e1\u010d", "Martin", ""], ["Fercoq", "Olivier", ""]]}, {"id": "1502.02322", "submitter": "Richard Nock", "authors": "Richard Nock and Giorgio Patrini and Arik Friedman", "title": "Rademacher Observations, Private Data, and Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimization of the logistic loss is a popular approach to batch\nsupervised learning. Our paper starts from the surprising observation that,\nwhen fitting linear (or kernelized) classifiers, the minimization of the\nlogistic loss is \\textit{equivalent} to the minimization of an exponential\n\\textit{rado}-loss computed (i) over transformed data that we call Rademacher\nobservations (rados), and (ii) over the \\textit{same} classifier as the one of\nthe logistic loss. Thus, a classifier learnt from rados can be\n\\textit{directly} used to classify \\textit{observations}. We provide a learning\nalgorithm over rados with boosting-compliant convergence rates on the\n\\textit{logistic loss} (computed over examples). Experiments on domains with up\nto millions of examples, backed up by theoretical arguments, display that\nlearning over a small set of random rados can challenge the state of the art\nthat learns over the \\textit{complete} set of examples. We show that rados\ncomply with various privacy requirements that make them good candidates for\nmachine learning in a privacy framework. We give several algebraic, geometric\nand computational hardness results on reconstructing examples from rados. We\nalso show how it is possible to craft, and efficiently learn from, rados in a\ndifferential privacy framework. Tests reveal that learning from differentially\nprivate rados can compete with learning from random rados, and hence with batch\nlearning from examples, achieving non-trivial privacy vs accuracy tradeoffs.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 01:12:11 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 03:55:51 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Nock", "Richard", ""], ["Patrini", "Giorgio", ""], ["Friedman", "Arik", ""]]}, {"id": "1502.02330", "submitter": "Dacheng Tao", "authors": "Yong Luo, Dacheng Tao, Yonggang Wen, Kotagiri Ramamohanarao, Chao Xu", "title": "Tensor Canonical Correlation Analysis for Multi-view Dimension Reduction", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) has proven an effective tool for\ntwo-view dimension reduction due to its profound theoretical foundation and\nsuccess in practical applications. In respect of multi-view learning, however,\nit is limited by its capability of only handling data represented by two-view\nfeatures, while in many real-world applications, the number of views is\nfrequently many more. Although the ad hoc way of simultaneously exploring all\npossible pairs of features can numerically deal with multi-view data, it\nignores the high order statistics (correlation information) which can only be\ndiscovered by simultaneously exploring all features.\n  Therefore, in this work, we develop tensor CCA (TCCA) which straightforwardly\nyet naturally generalizes CCA to handle the data of an arbitrary number of\nviews by analyzing the covariance tensor of the different views. TCCA aims to\ndirectly maximize the canonical correlation of multiple (more than two) views.\nCrucially, we prove that the multi-view canonical correlation maximization\nproblem is equivalent to finding the best rank-1 approximation of the data\ncovariance tensor, which can be solved efficiently using the well-known\nalternating least squares (ALS) algorithm. As a consequence, the high order\ncorrelation information contained in the different views is explored and thus a\nmore reliable common subspace shared by all features can be obtained. In\naddition, a non-linear extension of TCCA is presented. Experiments on various\nchallenge tasks, including large scale biometric structure prediction, internet\nadvertisement classification and web image annotation, demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 01:58:27 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Luo", "Yong", ""], ["Tao", "Dacheng", ""], ["Wen", "Yonggang", ""], ["Ramamohanarao", "Kotagiri", ""], ["Xu", "Chao", ""]]}, {"id": "1502.02362", "submitter": "Adith Swaminathan", "authors": "Adith Swaminathan and Thorsten Joachims", "title": "Counterfactual Risk Minimization: Learning from Logged Bandit Feedback", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a learning principle and an efficient algorithm for batch learning\nfrom logged bandit feedback. This learning setting is ubiquitous in online\nsystems (e.g., ad placement, web search, recommendation), where an algorithm\nmakes a prediction (e.g., ad ranking) for a given input (e.g., query) and\nobserves bandit feedback (e.g., user clicks on presented ads). We first address\nthe counterfactual nature of the learning problem through propensity scoring.\nNext, we prove generalization error bounds that account for the variance of the\npropensity-weighted empirical risk estimator. These constructive bounds give\nrise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM\ncan be used to derive a new learning method -- called Policy Optimizer for\nExponential Models (POEM) -- for learning stochastic linear rules for\nstructured output prediction. We present a decomposition of the POEM objective\nthat enables efficient stochastic gradient optimization. POEM is evaluated on\nseveral multi-label classification problems showing substantially improved\nrobustness and generalization performance compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 05:09:25 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 23:29:49 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Swaminathan", "Adith", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1502.02367", "submitter": "Junyoung Chung", "authors": "Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho and Yoshua Bengio", "title": "Gated Feedback Recurrent Neural Networks", "comments": "9 pages, removed appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel recurrent neural network (RNN) architecture.\nThe proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of\nstacking multiple recurrent layers by allowing and controlling signals flowing\nfrom upper recurrent layers to lower layers using a global gating unit for each\npair of layers. The recurrent signals exchanged between layers are gated\nadaptively based on the previous hidden states and the current input. We\nevaluated the proposed GF-RNN with different types of recurrent units, such as\ntanh, long short-term memory and gated recurrent units, on the tasks of\ncharacter-level language modeling and Python program evaluation. Our empirical\nevaluation of different RNN units, revealed that in both tasks, the GF-RNN\noutperforms the conventional approaches to build deep stacked RNNs. We suggest\nthat the improvement arises because the GF-RNN can adaptively assign different\nlayers to different timescales and layer-to-layer interactions (including the\ntop-down ones which are not usually present in a stacked RNN) by learning to\ngate these interactions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 05:25:54 GMT"}, {"version": "v2", "created": "Thu, 12 Feb 2015 19:18:07 GMT"}, {"version": "v3", "created": "Wed, 18 Feb 2015 11:34:38 GMT"}, {"version": "v4", "created": "Wed, 17 Jun 2015 06:26:21 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Chung", "Junyoung", ""], ["Gulcehre", "Caglar", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1502.02377", "submitter": "Jim Jing-Yan Wang", "authors": "Mohua Zhang, Jianhua Peng, Xuejie Liu, Jim Jing-Yan Wang", "title": "Sparse Coding with Earth Mover's Distance for Multi-Instance Histogram\n  Representation", "comments": null, "journal-ref": null, "doi": "10.1007/s00521-016-2269-9", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding (Sc) has been studied very well as a powerful data\nrepresentation method. It attempts to represent the feature vector of a data\nsample by reconstructing it as the sparse linear combination of some basic\nelements, and a $L_2$ norm distance function is usually used as the loss\nfunction for the reconstruction error. In this paper, we investigate using Sc\nas the representation method within multi-instance learning framework, where a\nsample is given as a bag of instances, and further represented as a histogram\nof the quantized instances. We argue that for the data type of histogram, using\n$L_2$ norm distance is not suitable, and propose to use the earth mover's\ndistance (EMD) instead of $L_2$ norm distance as a measure of the\nreconstruction error. By minimizing the EMD between the histogram of a sample\nand the its reconstruction from some basic histograms, a novel sparse coding\nmethod is developed, which is refereed as SC-EMD. We evaluate its performances\nas a histogram representation method in tow multi-instance learning problems\n--- abnormal image detection in wireless capsule endoscopy videos, and protein\nbinding site retrieval. The encouraging results demonstrate the advantages of\nthe new method over the traditional method using $L_2$ norm distance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 06:42:02 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 18:13:28 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Zhang", "Mohua", ""], ["Peng", "Jianhua", ""], ["Liu", "Xuejie", ""], ["Wang", "Jim Jing-Yan", ""]]}, {"id": "1502.02410", "submitter": "Elif Vural", "authors": "Elif Vural and Christine Guillemot", "title": "Out-of-sample generalizations for supervised manifold learning for\n  classification", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2520368", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised manifold learning methods for data classification map data samples\nresiding in a high-dimensional ambient space to a lower-dimensional domain in a\nstructure-preserving way, while enhancing the separation between different\nclasses in the learned embedding. Most nonlinear supervised manifold learning\nmethods compute the embedding of the manifolds only at the initially available\ntraining points, while the generalization of the embedding to novel points,\nknown as the out-of-sample extension problem in manifold learning, becomes\nespecially important in classification applications. In this work, we propose a\nsemi-supervised method for building an interpolation function that provides an\nout-of-sample extension for general supervised manifold learning algorithms\nstudied in the context of classification. The proposed algorithm computes a\nradial basis function (RBF) interpolator that minimizes an objective function\nconsisting of the total embedding error of unlabeled test samples, defined as\ntheir distance to the embeddings of the manifolds of their own class, as well\nas a regularization term that controls the smoothness of the interpolation\nfunction in a direction-dependent way. The class labels of test data and the\ninterpolation function parameters are estimated jointly with a progressive\nprocedure. Experimental results on face and object images demonstrate the\npotential of the proposed out-of-sample extension algorithm for the\nclassification of manifold-modeled data sets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 09:56:57 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Vural", "Elif", ""], ["Guillemot", "Christine", ""]]}, {"id": "1502.02445", "submitter": "Giovanni Montana", "authors": "Alexandre de Brebisson, Giovanni Montana", "title": "Deep Neural Networks for Anatomical Brain Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to automatically segment magnetic resonance (MR)\nimages of the human brain into anatomical regions. Our methodology is based on\na deep artificial neural network that assigns each voxel in an MR image of the\nbrain to its corresponding anatomical region. The inputs of the network capture\ninformation at different scales around the voxel of interest: 3D and orthogonal\n2D intensity patches capture the local spatial context while large, compressed\n2D orthogonal patches and distances to the regional centroids enforce global\nspatial consistency. Contrary to commonly used segmentation methods, our\ntechnique does not require any non-linear registration of the MR images. To\nbenchmark our model, we used the dataset provided for the MICCAI 2012 challenge\non multi-atlas labelling, which consists of 35 manually segmented MR images of\nthe brain. We obtained competitive results (mean dice coefficient 0.725, error\nrate 0.163) showing the potential of our approach. To our knowledge, our\ntechnique is the first to tackle the anatomical segmentation of the whole brain\nusing deep neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 11:48:42 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 16:19:44 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["de Brebisson", "Alexandre", ""], ["Montana", "Giovanni", ""]]}, {"id": "1502.02476", "submitter": "Marc-Alexandre C\\^ot\\'e", "authors": "Marc-Alexandre C\\^ot\\'e, Hugo Larochelle", "title": "An Infinite Restricted Boltzmann Machine", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a mathematical construction for the restricted Boltzmann machine\n(RBM) that doesn't require specifying the number of hidden units. In fact, the\nhidden layer size is adaptive and can grow during training. This is obtained by\nfirst extending the RBM to be sensitive to the ordering of its hidden units.\nThen, thanks to a carefully chosen definition of the energy function, we show\nthat the limit of infinitely many hidden units is well defined. As with RBM,\napproximate maximum likelihood training can be performed, resulting in an\nalgorithm that naturally and adaptively adds trained hidden units during\nlearning. We empirically study the behaviour of this infinite RBM, showing that\nits performance is competitive to that of the RBM, while not requiring the\ntuning of a hidden layer size.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 13:18:24 GMT"}, {"version": "v2", "created": "Tue, 10 Feb 2015 03:44:17 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2015 16:05:59 GMT"}, {"version": "v4", "created": "Fri, 18 Mar 2016 14:14:04 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["C\u00f4t\u00e9", "Marc-Alexandre", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1502.02506", "submitter": "Giovanni Montana", "authors": "Adrien Payan, Giovanni Montana", "title": "Predicting Alzheimer's disease: a neuroimaging study with 3D\n  convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern recognition methods using neuroimaging data for the diagnosis of\nAlzheimer's disease have been the subject of extensive research in recent\nyears. In this paper, we use deep learning methods, and in particular sparse\nautoencoders and 3D convolutional neural networks, to build an algorithm that\ncan predict the disease status of a patient, based on an MRI scan of the brain.\nWe report on experiments using the ADNI data set involving 2,265 historical\nscans. We demonstrate that 3D convolutional neural networks outperform several\nother classifiers reported in the literature and produce state-of-art results.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 14:46:40 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Payan", "Adrien", ""], ["Montana", "Giovanni", ""]]}, {"id": "1502.02512", "submitter": "Helio M. de Oliveira", "authors": "H.M. de Oliveira", "title": "The Adaptive Mean-Linkage Algorithm: A Bottom-Up Hierarchical Cluster\n  Technique", "comments": "4 pages, 2 figures, 2 tables. Congresso Brasileiro de Automatica CBA,\n  Natal, RN, Brazil, 2002", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a variant of the classical hierarchical cluster analysis is\nreported. This agglomerative (bottom-up) cluster technique is referred to as\nthe Adaptive Mean-Linkage Algorithm. It can be interpreted as a linkage\nalgorithm where the value of the threshold is conveniently up-dated at each\ninteraction. The superiority of the adaptive clustering with respect to the\naverage-linkage algorithm follows because it achieves a good compromise on\nthreshold values: Thresholds based on the cut-off distance are sufficiently\nsmall to assure the homogeneity and also large enough to guarantee at least a\npair of merging sets. This approach is applied to a set of possible\nsubstituents in a chemical series.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 14:57:58 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["de Oliveira", "H. M.", ""]]}, {"id": "1502.02551", "submitter": "Suyog Gupta", "authors": "Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, Pritish Narayanan", "title": "Deep Learning with Limited Numerical Precision", "comments": "10 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training of large-scale deep neural networks is often constrained by the\navailable computational resources. We study the effect of limited precision\ndata representation and computation on neural network training. Within the\ncontext of low-precision fixed-point computations, we observe the rounding\nscheme to play a crucial role in determining the network's behavior during\ntraining. Our results show that deep networks can be trained using only 16-bit\nwide fixed-point number representation when using stochastic rounding, and\nincur little to no degradation in the classification accuracy. We also\ndemonstrate an energy-efficient hardware accelerator that implements\nlow-precision fixed-point arithmetic with stochastic rounding.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 16:37:29 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Gupta", "Suyog", ""], ["Agrawal", "Ankur", ""], ["Gopalakrishnan", "Kailash", ""], ["Narayanan", "Pritish", ""]]}, {"id": "1502.02558", "submitter": "Mijung Park", "authors": "Mijung Park and Wittawat Jitkrittum and Dino Sejdinovic", "title": "K2-ABC: Approximate Bayesian Computation with Kernel Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complicated generative models often result in a situation where computing the\nlikelihood of observed data is intractable, while simulating from the\nconditional density given a parameter value is relatively easy. Approximate\nBayesian Computation (ABC) is a paradigm that enables simulation-based\nposterior inference in such cases by measuring the similarity between simulated\nand observed data in terms of a chosen set of summary statistics. However,\nthere is no general rule to construct sufficient summary statistics for complex\nmodels. Insufficient summary statistics will \"leak\" information, which leads to\nABC algorithms yielding samples from an incorrect (partial) posterior. In this\npaper, we propose a fully nonparametric ABC paradigm which circumvents the need\nfor manually selecting summary statistics. Our approach, K2-ABC, uses maximum\nmean discrepancy (MMD) as a dissimilarity measure between the distributions\nover observed and simulated data. MMD is easily estimated as the squared\ndifference between their empirical kernel embeddings. Experiments on a\nsimulated scenario and a real-world biological problem illustrate the\neffectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 16:49:31 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 13:21:39 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2015 13:04:48 GMT"}, {"version": "v4", "created": "Sat, 26 Dec 2015 16:57:55 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Park", "Mijung", ""], ["Jitkrittum", "Wittawat", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1502.02590", "submitter": "Alhussein Fawzi", "authors": "Alhussein Fawzi, Omar Fawzi, Pascal Frossard", "title": "Analysis of classifiers' robustness to adversarial perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to analyze an intriguing phenomenon recently\ndiscovered in deep networks, namely their instability to adversarial\nperturbations (Szegedy et. al., 2014). We provide a theoretical framework for\nanalyzing the robustness of classifiers to adversarial perturbations, and show\nfundamental upper bounds on the robustness of classifiers. Specifically, we\nestablish a general upper bound on the robustness of classifiers to adversarial\nperturbations, and then illustrate the obtained upper bound on the families of\nlinear and quadratic classifiers. In both cases, our upper bound depends on a\ndistinguishability measure that captures the notion of difficulty of the\nclassification task. Our results for both classes imply that in tasks involving\nsmall distinguishability, no classifier in the considered set will be robust to\nadversarial perturbations, even if a good accuracy is achieved. Our theoretical\nframework moreover suggests that the phenomenon of adversarial instability is\ndue to the low flexibility of classifiers, compared to the difficulty of the\nclassification task (captured by the distinguishability). Moreover, we show the\nexistence of a clear distinction between the robustness of a classifier to\nrandom noise and its robustness to adversarial perturbations. Specifically, the\nformer is shown to be larger than the latter by a factor that is proportional\nto \\sqrt{d} (with d being the signal dimension) for linear classifiers. This\nresult gives a theoretical explanation for the discrepancy between the two\nrobustness properties in high dimensional problems, which was empirically\nobserved in the context of neural networks. To the best of our knowledge, our\nresults provide the first theoretical work that addresses the phenomenon of\nadversarial instability recently observed for deep networks. Our analysis is\ncomplemented by experimental results on controlled and real-world data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 18:20:00 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 20:57:10 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 18:55:41 GMT"}, {"version": "v4", "created": "Mon, 28 Mar 2016 22:50:52 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Fawzi", "Omar", ""], ["Frossard", "Pascal", ""]]}, {"id": "1502.02599", "submitter": "Mohamed Elshrif", "authors": "Mohamed Elshrif, Ernest Fokoue", "title": "Adaptive Random SubSpace Learning (RSSL) Algorithm for Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel adaptive random subspace learning algorithm (RSSL) for\nprediction purpose. This new framework is flexible where it can be adapted with\nany learning technique. In this paper, we tested the algorithm for regression\nand classification problems. In addition, we provide a variety of weighting\nschemes to increase the robustness of the developed algorithm. These different\nwighting flavors were evaluated on simulated as well as on real-world data sets\nconsidering the cases where the ratio between features (attributes) and\ninstances (samples) is large and vice versa. The framework of the new algorithm\nconsists of many stages: first, calculate the weights of all features on the\ndata set using the correlation coefficient and F-statistic statistical\nmeasurements. Second, randomly draw n samples with replacement from the data\nset. Third, perform regular bootstrap sampling (bagging). Fourth, draw without\nreplacement the indices of the chosen variables. The decision was taken based\non the heuristic subspacing scheme. Fifth, call base learners and build the\nmodel. Sixth, use the model for prediction purpose on test set of the data. The\nresults show the advancement of the adaptive RSSL algorithm in most of the\ncases compared with the synonym (conventional) machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 18:49:29 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Elshrif", "Mohamed", ""], ["Fokoue", "Ernest", ""]]}, {"id": "1502.02606", "submitter": "Huy Nguyen", "authors": "Rafael da Ponte Barbosa and Alina Ene and Huy L. Nguyen and Justin\n  Ward", "title": "The Power of Randomization: Distributed Submodular Maximization on\n  Massive Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide variety of problems in machine learning, including exemplar\nclustering, document summarization, and sensor placement, can be cast as\nconstrained submodular maximization problems. Unfortunately, the resulting\nsubmodular optimization problems are often too large to be solved on a single\nmachine. We develop a simple distributed algorithm that is embarrassingly\nparallel and it achieves provable, constant factor, worst-case approximation\nguarantees. In our experiments, we demonstrate its efficiency in large problems\nwith different kinds of constraints with objective values always close to what\nis achievable in the centralized setting.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 19:04:43 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2015 17:49:22 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Barbosa", "Rafael da Ponte", ""], ["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""], ["Ward", "Justin", ""]]}, {"id": "1502.02609", "submitter": "Rushikesh Kamalapurkar", "authors": "Rushikesh Kamalapurkar, Joel A. Rosenfeld, Warren E. Dixon", "title": "Efficient model-based reinforcement learning for approximate online\n  optimal", "comments": null, "journal-ref": null, "doi": "10.1016/j.automatica.2016.08.004", "report-no": null, "categories": "cs.SY cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the infinite horizon optimal regulation problem is solved\nonline for a deterministic control-affine nonlinear dynamical system using the\nstate following (StaF) kernel method to approximate the value function. Unlike\ntraditional methods that aim to approximate a function over a large compact\nset, the StaF kernel method aims to approximate a function in a small\nneighborhood of a state that travels within a compact set. Simulation results\ndemonstrate that stability and approximate optimality of the control system can\nbe achieved with significantly fewer basis functions than may be required for\nglobal approximation methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 19:13:17 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Kamalapurkar", "Rushikesh", ""], ["Rosenfeld", "Joel A.", ""], ["Dixon", "Warren E.", ""]]}, {"id": "1502.02643", "submitter": "Huy Nguyen", "authors": "Alina Ene and Huy L. Nguyen", "title": "Random Coordinate Descent Methods for Minimizing Decomposable Submodular\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular function minimization is a fundamental optimization problem that\narises in several applications in machine learning and computer vision. The\nproblem is known to be solvable in polynomial time, but general purpose\nalgorithms have high running times and are unsuitable for large-scale problems.\nRecent work have used convex optimization techniques to obtain very practical\nalgorithms for minimizing functions that are sums of ``simple\" functions. In\nthis paper, we use random coordinate descent methods to obtain algorithms with\nfaster linear convergence rates and cheaper iteration costs. Compared to\nalternating projection methods, our algorithms do not rely on full-dimensional\nvector operations and they converge in significantly fewer iterations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 20:31:18 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""]]}, {"id": "1502.02651", "submitter": "Satyen Kale", "authors": "Alina Beygelzimer, Satyen Kale, and Haipeng Luo", "title": "Optimal and Adaptive Algorithms for Online Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online boosting, the task of converting any weak online learner into\na strong online learner. Based on a novel and natural definition of weak online\nlearnability, we develop two online boosting algorithms. The first algorithm is\nan online version of boost-by-majority. By proving a matching lower bound, we\nshow that this algorithm is essentially optimal in terms of the number of weak\nlearners and the sample complexity needed to achieve a specified accuracy. This\noptimal algorithm is not adaptive however. Using tools from online loss\nminimization, we derive an adaptive online boosting algorithm that is also\nparameter-free, but not optimal. Both algorithms work with base learners that\ncan handle example importance weights directly, as well as by rejection\nsampling examples with probability defined by the booster. Results are\ncomplemented with an extensive experimental study.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 20:58:38 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Beygelzimer", "Alina", ""], ["Kale", "Satyen", ""], ["Luo", "Haipeng", ""]]}, {"id": "1502.02704", "submitter": "Alina Beygelzimer", "authors": "Alina Beygelzimer, Hal Daum\\'e III, John Langford, Paul Mineiro", "title": "Learning Reductions that Really Work", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a summary of the mathematical and computational techniques that\nhave enabled learning reductions to effectively address a wide class of\nproblems, and show that this approach to solving machine learning problems can\nbe broadly useful.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 22:05:25 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Beygelzimer", "Alina", ""], ["Daum\u00e9", "Hal", "III"], ["Langford", "John", ""], ["Mineiro", "Paul", ""]]}, {"id": "1502.02710", "submitter": "Nikos Karampatziakis", "authors": "Nikos Karampatziakis, Paul Mineiro", "title": "Scalable Multilabel Prediction via Randomized Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the dependence between outputs is a fundamental challenge in\nmultilabel classification. In this work we show that a generic regularized\nnonlinearity mapping independent predictions to joint predictions is sufficient\nto achieve state-of-the-art performance on a variety of benchmark problems.\nCrucially, we compute the joint predictions without ever obtaining any\nindependent predictions, while incorporating low-rank and smoothness\nregularization. We achieve this by leveraging randomized algorithms for matrix\ndecomposition and kernel approximation. Furthermore, our techniques are\napplicable to the multiclass setting. We apply our method to a variety of\nmulticlass and multilabel data sets, obtaining state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 22:18:26 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2015 21:08:19 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Karampatziakis", "Nikos", ""], ["Mineiro", "Paul", ""]]}, {"id": "1502.02761", "submitter": "Yujia Li", "authors": "Yujia Li, Kevin Swersky and Richard Zemel", "title": "Generative Moment Matching Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning deep generative models from data. We\nformulate a method that generates an independent sample via a single\nfeedforward pass through a multilayer perceptron, as in the recently proposed\ngenerative adversarial networks (Goodfellow et al., 2014). Training a\ngenerative adversarial network, however, requires careful optimization of a\ndifficult minimax program. Instead, we utilize a technique from statistical\nhypothesis testing known as maximum mean discrepancy (MMD), which leads to a\nsimple objective that can be interpreted as matching all orders of statistics\nbetween a dataset and samples from the model, and can be trained by\nbackpropagation. We further boost the performance of this approach by combining\nour generative network with an auto-encoder network, using MMD to learn to\ngenerate codes that can then be decoded to produce samples. We show that the\ncombination of these techniques yields excellent generative models compared to\nbaseline approaches as measured on MNIST and the Toronto Face Database.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 02:54:58 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Li", "Yujia", ""], ["Swersky", "Kevin", ""], ["Zemel", "Richard", ""]]}, {"id": "1502.02763", "submitter": "Branislav Kveton", "authors": "Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan", "title": "Cascading Bandits: Learning to Rank in the Cascade Model", "comments": "Proceedings of the 32nd International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A search engine usually outputs a list of $K$ web pages. The user examines\nthis list, from the first web page to the last, and chooses the first\nattractive page. This model of user behavior is known as the cascade model. In\nthis paper, we propose cascading bandits, a learning variant of the cascade\nmodel where the objective is to identify $K$ most attractive items. We\nformulate our problem as a stochastic combinatorial partial monitoring problem.\nWe propose two algorithms for solving it, CascadeUCB1 and CascadeKL-UCB. We\nalso prove gap-dependent upper bounds on the regret of these algorithms and\nderive a lower bound on the regret in cascading bandits. The lower bound\nmatches the upper bound of CascadeKL-UCB up to a logarithmic factor. We\nexperiment with our algorithms on several problems. The algorithms perform\nsurprisingly well even when our modeling assumptions are violated.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 02:56:04 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 19:03:38 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Kveton", "Branislav", ""], ["Szepesvari", "Csaba", ""], ["Wen", "Zheng", ""], ["Ashkan", "Azin", ""]]}, {"id": "1502.02791", "submitter": "Mingsheng Long", "authors": "Mingsheng Long, Yue Cao, Jianmin Wang, Michael I. Jordan", "title": "Learning Transferable Features with Deep Adaptation Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies reveal that a deep neural network can learn transferable\nfeatures which generalize well to novel tasks for domain adaptation. However,\nas deep features eventually transition from general to specific along the\nnetwork, the feature transferability drops significantly in higher layers with\nincreasing domain discrepancy. Hence, it is important to formally reduce the\ndataset bias and enhance the transferability in task-specific layers. In this\npaper, we propose a new Deep Adaptation Network (DAN) architecture, which\ngeneralizes deep convolutional neural network to the domain adaptation\nscenario. In DAN, hidden representations of all task-specific layers are\nembedded in a reproducing kernel Hilbert space where the mean embeddings of\ndifferent domain distributions can be explicitly matched. The domain\ndiscrepancy is further reduced using an optimal multi-kernel selection method\nfor mean embedding matching. DAN can learn transferable features with\nstatistical guarantees, and can scale linearly by unbiased estimate of kernel\nembedding. Extensive empirical evidence shows that the proposed architecture\nyields state-of-the-art image classification error rates on standard domain\nadaptation benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 06:01:30 GMT"}, {"version": "v2", "created": "Wed, 27 May 2015 05:28:35 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Long", "Mingsheng", ""], ["Cao", "Yue", ""], ["Wang", "Jianmin", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1502.02846", "submitter": "Maren Mahsereci", "authors": "Maren Mahsereci and Philipp Hennig", "title": "Probabilistic Line Searches for Stochastic Optimization", "comments": "12 pages, including supplements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deterministic optimization, line searches are a standard tool ensuring\nstability and efficiency. Where only stochastic gradients are available, no\ndirect equivalent has so far been formulated, because uncertain gradients do\nnot allow for a strict sequence of decisions collapsing the search space. We\nconstruct a probabilistic line search by combining the structure of existing\ndeterministic methods with notions from Bayesian optimization. Our method\nretains a Gaussian process surrogate of the univariate optimization objective,\nand uses a probabilistic belief over the Wolfe conditions to monitor the\ndescent. The algorithm has very low computational cost, and no user-controlled\nparameters. Experiments show that it effectively removes the need to define a\nlearning rate for stochastic gradient descent.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 10:36:25 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 15:47:59 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2015 14:47:40 GMT"}, {"version": "v4", "created": "Mon, 18 Jan 2016 13:17:08 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Mahsereci", "Maren", ""], ["Hennig", "Philipp", ""]]}, {"id": "1502.02860", "submitter": "Marc Deisenroth", "authors": "Marc Peter Deisenroth, Dieter Fox and Carl Edward Rasmussen", "title": "Gaussian Processes for Data-Efficient Learning in Robotics and Control", "comments": "20 pages, 29 figures; fixed a typo in equation on page 8", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  vol. 37, issue no 2, pages 408-423, February 2015", "doi": "10.1109/TPAMI.2013.218", "report-no": null, "categories": "stat.ML cs.LG cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous learning has been a promising direction in control and robotics\nfor more than a decade since data-driven learning allows to reduce the amount\nof engineering knowledge, which is otherwise required. However, autonomous\nreinforcement learning (RL) approaches typically require many interactions with\nthe system to learn controllers, which is a practical limitation in real\nsystems, such as robots, where many interactions can be impractical and time\nconsuming. To address this problem, current learning approaches typically\nrequire task-specific knowledge in form of expert demonstrations, realistic\nsimulators, pre-shaped policies, or specific knowledge about the underlying\ndynamics. In this article, we follow a different approach and speed up learning\nby extracting more information from data. In particular, we learn a\nprobabilistic, non-parametric Gaussian process transition model of the system.\nBy explicitly incorporating model uncertainty into long-term planning and\ncontroller learning our approach reduces the effects of model errors, a key\nproblem in model-based learning. Compared to state-of-the art RL our\nmodel-based policy search method achieves an unprecedented speed of learning.\nWe demonstrate its applicability to autonomous learning in real robot and\ncontrol tasks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 11:09:38 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 18:25:45 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Deisenroth", "Marc Peter", ""], ["Fox", "Dieter", ""], ["Rasmussen", "Carl Edward", ""]]}, {"id": "1502.03044", "submitter": "Kelvin Xu", "authors": "Kelvin Xu and Jimmy Ba and Ryan Kiros and Kyunghyun Cho and Aaron\n  Courville and Ruslan Salakhutdinov and Richard Zemel and Yoshua Bengio", "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual\n  Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent work in machine translation and object detection, we\nintroduce an attention based model that automatically learns to describe the\ncontent of images. We describe how we can train this model in a deterministic\nmanner using standard backpropagation techniques and stochastically by\nmaximizing a variational lower bound. We also show through visualization how\nthe model is able to automatically learn to fix its gaze on salient objects\nwhile generating the corresponding words in the output sequence. We validate\nthe use of attention with state-of-the-art performance on three benchmark\ndatasets: Flickr8k, Flickr30k and MS COCO.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 19:18:29 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 02:58:54 GMT"}, {"version": "v3", "created": "Tue, 19 Apr 2016 16:43:09 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Xu", "Kelvin", ""], ["Ba", "Jimmy", ""], ["Kiros", "Ryan", ""], ["Cho", "Kyunghyun", ""], ["Courville", "Aaron", ""], ["Salakhutdinov", "Ruslan", ""], ["Zemel", "Richard", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1502.03126", "submitter": "Soheil Bahrampour", "authors": "Soheil Bahrampour and Nasser M. Nasrabadi and Asok Ray and Kenneth W.\n  Jenkins", "title": "Kernel Task-Driven Dictionary Learning for Hyperspectral Image\n  Classification", "comments": "5 pages, IEEE International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning algorithms have been successfully used in both\nreconstructive and discriminative tasks, where the input signal is represented\nby a linear combination of a few dictionary atoms. While these methods are\nusually developed under $\\ell_1$ sparsity constrain (prior) in the input\ndomain, recent studies have demonstrated the advantages of sparse\nrepresentation using structured sparsity priors in the kernel domain. In this\npaper, we propose a supervised dictionary learning algorithm in the kernel\ndomain for hyperspectral image classification. In the proposed formulation, the\ndictionary and classifier are obtained jointly for optimal classification\nperformance. The supervised formulation is task-driven and provides learned\nfeatures from the hyperspectral data that are well suited for the\nclassification task. Moreover, the proposed algorithm uses a joint\n($\\ell_{12}$) sparsity prior to enforce collaboration among the neighboring\npixels. The simulation results illustrate the efficiency of the proposed\ndictionary learning algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 21:27:27 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Bahrampour", "Soheil", ""], ["Nasrabadi", "Nasser M.", ""], ["Ray", "Asok", ""], ["Jenkins", "Kenneth W.", ""]]}, {"id": "1502.03163", "submitter": "Yuancheng Luo", "authors": "Yuancheng Luo, Dmitry N. Zotkin, Ramani Duraiswami", "title": "Gaussian Process Models for HRTF based Sound-Source Localization and\n  Active-Learning", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a machine learning perspective, the human ability localize sounds can be\nmodeled as a non-parametric and non-linear regression problem between binaural\nspectral features of sound received at the ears (input) and their sound-source\ndirections (output). The input features can be summarized in terms of the\nindividual's head-related transfer functions (HRTFs) which measure the spectral\nresponse between the listener's eardrum and an external point in $3$D. Based on\nthese viewpoints, two related problems are considered: how can one achieve an\noptimal sampling of measurements for training sound-source localization (SSL)\nmodels, and how can SSL models be used to infer the subject's HRTFs in\nlistening tests. First, we develop a class of binaural SSL models based on\nGaussian process regression and solve a \\emph{forward selection} problem that\nfinds a subset of input-output samples that best generalize to all SSL\ndirections. Second, we use an \\emph{active-learning} approach that updates an\nonline SSL model for inferring the subject's SSL errors via headphones and a\ngraphical user interface. Experiments show that only a small fraction of HRTFs\nare required for $5^{\\circ}$ localization accuracy and that the learned HRTFs\nare localized closer to their intended directions than non-individualized\nHRTFs.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 00:55:14 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Luo", "Yuancheng", ""], ["Zotkin", "Dmitry N.", ""], ["Duraiswami", "Ramani", ""]]}, {"id": "1502.03167", "submitter": "Sergey Ioffe", "authors": "Sergey Ioffe, Christian Szegedy", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing\n  Internal Covariate Shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training Deep Neural Networks is complicated by the fact that the\ndistribution of each layer's inputs changes during training, as the parameters\nof the previous layers change. This slows down the training by requiring lower\nlearning rates and careful parameter initialization, and makes it notoriously\nhard to train models with saturating nonlinearities. We refer to this\nphenomenon as internal covariate shift, and address the problem by normalizing\nlayer inputs. Our method draws its strength from making normalization a part of\nthe model architecture and performing the normalization for each training\nmini-batch. Batch Normalization allows us to use much higher learning rates and\nbe less careful about initialization. It also acts as a regularizer, in some\ncases eliminating the need for Dropout. Applied to a state-of-the-art image\nclassification model, Batch Normalization achieves the same accuracy with 14\ntimes fewer training steps, and beats the original model by a significant\nmargin. Using an ensemble of batch-normalized networks, we improve upon the\nbest published result on ImageNet classification: reaching 4.9% top-5\nvalidation error (and 4.8% test error), exceeding the accuracy of human raters.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 01:44:18 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2015 17:31:36 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2015 20:44:12 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Ioffe", "Sergey", ""], ["Szegedy", "Christian", ""]]}, {"id": "1502.03175", "submitter": "Brandon Willard", "authors": "Nicholas G. Polson, James G. Scott and Brandon T. Willard", "title": "Proximal Algorithms in Statistics and Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop proximal methods for statistical learning. Proximal\npoint algorithms are useful in statistics and machine learning for obtaining\noptimization solutions for composite functions. Our approach exploits\nclosed-form solutions of proximal operators and envelope representations based\non the Moreau, Forward-Backward, Douglas-Rachford and Half-Quadratic envelopes.\nEnvelope representations lead to novel proximal algorithms for statistical\noptimisation of composite objective functions which include both non-smooth and\nnon-convex objectives. We illustrate our methodology with regularized Logistic\nand Poisson regression and non-convex bridge penalties with a fused lasso norm.\nWe provide a discussion of convergence of non-descent algorithms with\nacceleration and for non-convex functions. Finally, we provide directions for\nfuture research.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 02:21:49 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 18:22:40 GMT"}, {"version": "v3", "created": "Sat, 30 May 2015 22:01:39 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""], ["Willard", "Brandon T.", ""]]}, {"id": "1502.03255", "submitter": "Fran\\c{c}ois Schnitzler", "authors": "Assaf Hallak and Fran\\c{c}ois Schnitzler and Timothy Mann and Shie\n  Mannor", "title": "Off-policy evaluation for MDPs with unknown structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-policy learning in dynamic decision problems is essential for providing\nstrong evidence that a new policy is better than the one in use. But how can we\nprove superiority without testing the new policy? To answer this question, we\nintroduce the G-SCOPE algorithm that evaluates a new policy based on data\ngenerated by the existing policy. Our algorithm is both computationally and\nsample efficient because it greedily learns to exploit factored structure in\nthe dynamics of the environment. We present a finite sample analysis of our\napproach and show through experiments that the algorithm scales well on\nhigh-dimensional problems with few samples.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 10:42:40 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Hallak", "Assaf", ""], ["Schnitzler", "Fran\u00e7ois", ""], ["Mann", "Timothy", ""], ["Mannor", "Shie", ""]]}, {"id": "1502.03296", "submitter": "Eduardo G. Altmann", "authors": "Eduardo G. Altmann and Martin Gerlach", "title": "Statistical laws in linguistics", "comments": "Proceedings of the Flow Machines Workshop: Creativity and\n  Universality in Language, Paris, June 18 to 20, 2014", "journal-ref": null, "doi": "10.1007/978-3-319-24403-7_2", "report-no": null, "categories": "physics.soc-ph cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zipf's law is just one out of many universal laws proposed to describe\nstatistical regularities in language. Here we review and critically discuss how\nthese laws can be statistically interpreted, fitted, and tested (falsified).\nThe modern availability of large databases of written text allows for tests\nwith an unprecedent statistical accuracy and also a characterization of the\nfluctuations around the typical behavior. We find that fluctuations are usually\nmuch larger than expected based on simplifying statistical assumptions (e.g.,\nindependence and lack of correlations between observations).These\nsimplifications appear also in usual statistical tests so that the large\nfluctuations can be erroneously interpreted as a falsification of the law.\nInstead, here we argue that linguistic laws are only meaningful (falsifiable)\nif accompanied by a model for which the fluctuations can be computed (e.g., a\ngenerative model of the text). The large fluctuations we report show that the\nconstraints imposed by linguistic laws on the creativity process of text\ngeneration are not as tight as one could expect.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 13:10:58 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Altmann", "Eduardo G.", ""], ["Gerlach", "Martin", ""]]}, {"id": "1502.03302", "submitter": "Abdulsalam Yassine Dr.", "authors": "Pallavi Kuhad, Abdulsalam Yassine, Shervin Shirmohammadi", "title": "Using Distance Estimation and Deep Learning to Simplify Calibration in\n  Food Calorie Measurement", "comments": "This paper has been withdrawn due to errors in equation 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High calorie intake in the human body on the one hand, has proved harmful in\nnumerous occasions leading to several diseases and on the other hand, a\nstandard amount of calorie intake has been deemed essential by dieticians to\nmaintain the right balance of calorie content in human body. As such,\nresearchers have proposed a variety of automatic tools and systems to assist\nusers measure their calorie in-take. In this paper, we consider the category of\nthose tools that use image processing to recognize the food, and we propose a\nmethod for fully automatic and user-friendly calibration of the dimension of\nthe food portion sizes, which is needed in order to measure food portion weight\nand its ensuing amount of calories. Experimental results show that our method,\nwhich uses deep learning, mobile cloud computing, distance estimation and size\ncalibration inside a mobile device, leads to an accuracy improvement to 95% on\naverage compared to previous work\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 13:27:01 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2015 11:45:11 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Kuhad", "Pallavi", ""], ["Yassine", "Abdulsalam", ""], ["Shirmohammadi", "Shervin", ""]]}, {"id": "1502.03409", "submitter": "Karl Ni", "authors": "Karl Ni, Roger Pearce, Kofi Boakye, Brian Van Essen, Damian Borth,\n  Barry Chen, Eric Wang", "title": "Large-Scale Deep Learning on the YFCC100M Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We present a work-in-progress snapshot of learning with a 15 billion\nparameter deep learning network on HPC architectures applied to the largest\npublicly available natural image and video dataset released to-date. Recent\nadvancements in unsupervised deep neural networks suggest that scaling up such\nnetworks in both model and training dataset size can yield significant\nimprovements in the learning of concepts at the highest layers. We train our\nthree-layer deep neural network on the Yahoo! Flickr Creative Commons 100M\ndataset. The dataset comprises approximately 99.2 million images and 800,000\nuser-created videos from Yahoo's Flickr image and video sharing platform.\nTraining of our network takes eight days on 98 GPU nodes at the High\nPerformance Computing Center at Lawrence Livermore National Laboratory.\nEncouraging preliminary results and future research directions are presented\nand discussed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 19:24:36 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Ni", "Karl", ""], ["Pearce", "Roger", ""], ["Boakye", "Kofi", ""], ["Van Essen", "Brian", ""], ["Borth", "Damian", ""], ["Chen", "Barry", ""], ["Wang", "Eric", ""]]}, {"id": "1502.03473", "submitter": "Shuai Li", "authors": "Shuai Li and Alexandros Karatzoglou and Claudio Gentile", "title": "Collaborative Filtering Bandits", "comments": "The 39th SIGIR (SIGIR 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical collaborative filtering, and content-based filtering methods try to\nlearn a static recommendation model given training data. These approaches are\nfar from ideal in highly dynamic recommendation domains such as news\nrecommendation and computational advertisement, where the set of items and\nusers is very fluid. In this work, we investigate an adaptive clustering\ntechnique for content recommendation based on exploration-exploitation\nstrategies in contextual multi-armed bandit settings. Our algorithm takes into\naccount the collaborative effects that arise due to the interaction of the\nusers with the items, by dynamically grouping users based on the items under\nconsideration and, at the same time, grouping items based on the similarity of\nthe clusterings induced over the users. The resulting algorithm thus takes\nadvantage of preference patterns in the data in a way akin to collaborative\nfiltering methods. We provide an empirical analysis on medium-size real-world\ndatasets, showing scalability and increased prediction performance (as measured\nby click-through rate) over state-of-the-art methods for clustering bandits. We\nalso provide a regret analysis within a standard linear stochastic noise\nsetting.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 22:28:14 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2015 17:51:41 GMT"}, {"version": "v3", "created": "Thu, 7 May 2015 17:03:39 GMT"}, {"version": "v4", "created": "Thu, 24 Dec 2015 17:24:07 GMT"}, {"version": "v5", "created": "Wed, 30 Mar 2016 10:29:12 GMT"}, {"version": "v6", "created": "Wed, 11 May 2016 15:17:30 GMT"}, {"version": "v7", "created": "Tue, 31 May 2016 18:47:03 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Li", "Shuai", ""], ["Karatzoglou", "Alexandros", ""], ["Gentile", "Claudio", ""]]}, {"id": "1502.03475", "submitter": "M. Sadegh Talebi", "authors": "Richard Combes and M. Sadegh Talebi and Alexandre Proutiere and Marc\n  Lelarge", "title": "Combinatorial Bandits Revisited", "comments": "30 pages, Advances in Neural Information Processing Systems 28 (NIPS\n  2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates stochastic and adversarial combinatorial multi-armed\nbandit problems. In the stochastic setting under semi-bandit feedback, we\nderive a problem-specific regret lower bound, and discuss its scaling with the\ndimension of the decision space. We propose ESCB, an algorithm that efficiently\nexploits the structure of the problem and provide a finite-time analysis of its\nregret. ESCB has better performance guarantees than existing algorithms, and\nsignificantly outperforms these algorithms in practice. In the adversarial\nsetting under bandit feedback, we propose \\textsc{CombEXP}, an algorithm with\nthe same regret scaling as state-of-the-art algorithms, but with lower\ncomputational complexity for some combinatorial problems.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 22:35:50 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2015 15:07:54 GMT"}, {"version": "v3", "created": "Fri, 6 Nov 2015 00:53:37 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Combes", "Richard", ""], ["Talebi", "M. Sadegh", ""], ["Proutiere", "Alexandre", ""], ["Lelarge", "Marc", ""]]}, {"id": "1502.03491", "submitter": "Allen Lavoie", "authors": "Mithun Chakraborty, Sanmay Das, Allen Lavoie", "title": "How to show a probabilistic model is better", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple theoretical framework, and corresponding practical\nprocedures, for comparing probabilistic models on real data in a traditional\nmachine learning setting. This framework is based on the theory of proper\nscoring rules, but requires only basic algebra and probability theory to\nunderstand and verify. The theoretical concepts presented are well-studied,\nprimarily in the statistics literature. The goal of this paper is to advocate\ntheir wider adoption for performance evaluation in empirical machine learning.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 23:44:02 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Chakraborty", "Mithun", ""], ["Das", "Sanmay", ""], ["Lavoie", "Allen", ""]]}, {"id": "1502.03492", "submitter": "David Duvenaud", "authors": "Dougal Maclaurin, David Duvenaud, Ryan P. Adams", "title": "Gradient-based Hyperparameter Optimization through Reversible Learning", "comments": "10 figures. Submitted to ICML", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tuning hyperparameters of learning algorithms is hard because gradients are\nusually unavailable. We compute exact gradients of cross-validation performance\nwith respect to all hyperparameters by chaining derivatives backwards through\nthe entire training procedure. These gradients allow us to optimize thousands\nof hyperparameters, including step-size and momentum schedules, weight\ninitialization distributions, richly parameterized regularization schemes, and\nneural network architectures. We compute hyperparameter gradients by exactly\nreversing the dynamics of stochastic gradient descent with momentum.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 23:52:36 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2015 19:26:39 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2015 17:40:44 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Maclaurin", "Dougal", ""], ["Duvenaud", "David", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1502.03496", "submitter": "Yu Cheng", "authors": "Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng, Shang-Hua Teng", "title": "Spectral Sparsification of Random-Walk Matrix Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a fundamental algorithmic question in spectral graph theory:\nCompute a spectral sparsifier of random-walk matrix-polynomial\n$$L_\\alpha(G)=D-\\sum_{r=1}^d\\alpha_rD(D^{-1}A)^r$$ where $A$ is the adjacency\nmatrix of a weighted, undirected graph, $D$ is the diagonal matrix of weighted\ndegrees, and $\\alpha=(\\alpha_1...\\alpha_d)$ are nonnegative coefficients with\n$\\sum_{r=1}^d\\alpha_r=1$. Recall that $D^{-1}A$ is the transition matrix of\nrandom walks on the graph. The sparsification of $L_\\alpha(G)$ appears to be\nalgorithmically challenging as the matrix power $(D^{-1}A)^r$ is defined by all\npaths of length $r$, whose precise calculation would be prohibitively\nexpensive.\n  In this paper, we develop the first nearly linear time algorithm for this\nsparsification problem: For any $G$ with $n$ vertices and $m$ edges, $d$\ncoefficients $\\alpha$, and $\\epsilon > 0$, our algorithm runs in time\n$O(d^2m\\log^2n/\\epsilon^{2})$ to construct a Laplacian matrix\n$\\tilde{L}=D-\\tilde{A}$ with $O(n\\log n/\\epsilon^{2})$ non-zeros such that\n$\\tilde{L}\\approx_{\\epsilon}L_\\alpha(G)$.\n  Matrix polynomials arise in mathematical analysis of matrix functions as well\nas numerical solutions of matrix equations. Our work is particularly motivated\nby the algorithmic problems for speeding up the classic Newton's method in\napplications such as computing the inverse square-root of the precision matrix\nof a Gaussian random field, as well as computing the $q$th-root transition (for\n$q\\geq1$) in a time-reversible Markov model. The key algorithmic step for both\napplications is the construction of a spectral sparsifier of a constant degree\nrandom-walk matrix-polynomials introduced by Newton's method. Our algorithm can\nalso be used to build efficient data structures for effective resistances for\nmulti-step time-reversible Markov models, and we anticipate that it could be\nuseful for other tasks in network analysis.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 00:25:32 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Cheng", "Dehua", ""], ["Cheng", "Yu", ""], ["Liu", "Yan", ""], ["Peng", "Richard", ""], ["Teng", "Shang-Hua", ""]]}, {"id": "1502.03505", "submitter": "Florian Yger", "authors": "Florian Yger and Masashi Sugiyama", "title": "Supervised LogEuclidean Metric Learning for Symmetric Positive Definite\n  Matrices", "comments": "19 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning has been shown to be highly effective to improve the\nperformance of nearest neighbor classification. In this paper, we address the\nproblem of metric learning for Symmetric Positive Definite (SPD) matrices such\nas covariance matrices, which arise in many real-world applications. Naively\nusing standard Mahalanobis metric learning methods under the Euclidean geometry\nfor SPD matrices is not appropriate, because the difference of SPD matrices can\nbe a non-SPD matrix and thus the obtained solution can be uninterpretable. To\ncope with this problem, we propose to use a properly parameterized LogEuclidean\ndistance and optimize the metric with respect to kernel-target alignment, which\nis a supervised criterion for kernel learning. Then the resulting non-trivial\noptimization problem is solved by utilizing the Riemannian geometry. Finally,\nwe experimentally demonstrate the usefulness of our LogEuclidean metric\nlearning algorithm on real-world classification tasks for EEG signals and\ntexture patches.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 01:38:36 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Yger", "Florian", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1502.03508", "submitter": "Martin Jaggi", "authors": "Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan, Peter\n  Richt\\'arik and Martin Tak\\'a\\v{c}", "title": "Adding vs. Averaging in Distributed Primal-Dual Optimization", "comments": "ICML 2015: JMLR W&CP volume37, Proceedings of The 32nd International\n  Conference on Machine Learning, pp. 1973-1982", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed optimization methods for large-scale machine learning suffer from\na communication bottleneck. It is difficult to reduce this bottleneck while\nstill efficiently and accurately aggregating partial work from different\nmachines. In this paper, we present a novel generalization of the recent\ncommunication-efficient primal-dual framework (CoCoA) for distributed\noptimization. Our framework, CoCoA+, allows for additive combination of local\nupdates to the global parameters at each iteration, whereas previous schemes\nwith convergence guarantees only allow conservative averaging. We give stronger\n(primal-dual) convergence rate guarantees for both CoCoA as well as our new\nvariants, and generalize the theory for both methods to cover non-smooth convex\nloss functions. We provide an extensive experimental comparison that shows the\nmarkedly improved performance of CoCoA+ on several real-world distributed\ndatasets, especially when scaling up the number of machines.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 01:51:08 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2015 19:35:13 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Ma", "Chenxin", ""], ["Smith", "Virginia", ""], ["Jaggi", "Martin", ""], ["Jordan", "Michael I.", ""], ["Richt\u00e1rik", "Peter", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1502.03509", "submitter": "Iain Murray", "authors": "Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle", "title": "MADE: Masked Autoencoder for Distribution Estimation", "comments": "9 pages and 1 page of supplementary material. Updated to match\n  published version", "journal-ref": "Proceedings of the 32nd International Conference on Machine\n  Learning, JMLR W&CP 37:881-889, 2015", "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a lot of recent interest in designing neural network models to\nestimate a distribution from a set of examples. We introduce a simple\nmodification for autoencoder neural networks that yields powerful generative\nmodels. Our method masks the autoencoder's parameters to respect autoregressive\nconstraints: each input is reconstructed only from previous inputs in a given\nordering. Constrained this way, the autoencoder outputs can be interpreted as a\nset of conditional probabilities, and their product, the full joint\nprobability. We can also train a single network that can decompose the joint\nprobability in multiple different orderings. Our simple framework can be\napplied to multiple architectures, including deep ones. Vectorized\nimplementations, such as on GPUs, are simple and fast. Experiments demonstrate\nthat this approach is competitive with state-of-the-art tractable distribution\nestimators. At test time, the method is significantly faster and scales better\nthan other autoregressive estimators.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 02:06:07 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 14:37:32 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Germain", "Mathieu", ""], ["Gregor", "Karol", ""], ["Murray", "Iain", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1502.03520", "submitter": "Yingyu Liang", "authors": "Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, Andrej Risteski", "title": "A Latent Variable Model Approach to PMI-based Word Embeddings", "comments": "Appear in Transactions of the Association for Computational\n  Linguistics (TACL), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic word embeddings represent the meaning of a word via a vector, and\nare created by diverse methods. Many use nonlinear operations on co-occurrence\nstatistics, and have hand-tuned hyperparameters and reweighting methods.\n  This paper proposes a new generative model, a dynamic version of the\nlog-linear topic model of~\\citet{mnih2007three}. The methodological novelty is\nto use the prior to compute closed form expressions for word statistics. This\nprovides a theoretical justification for nonlinear models like PMI, word2vec,\nand GloVe, as well as some hyperparameter choices. It also helps explain why\nlow-dimensional semantic embeddings contain linear algebraic structure that\nallows solution of word analogies, as shown by~\\citet{mikolov2013efficient} and\nmany subsequent papers.\n  Experimental support is provided for the generative model assumptions, the\nmost important of which is that latent word vectors are fairly uniformly\ndispersed in space.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 02:50:08 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 00:49:42 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2015 21:14:43 GMT"}, {"version": "v4", "created": "Wed, 22 Jul 2015 23:51:24 GMT"}, {"version": "v5", "created": "Wed, 14 Oct 2015 04:27:00 GMT"}, {"version": "v6", "created": "Wed, 24 Feb 2016 01:28:03 GMT"}, {"version": "v7", "created": "Fri, 22 Jul 2016 20:09:25 GMT"}, {"version": "v8", "created": "Wed, 19 Jun 2019 21:54:20 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Arora", "Sanjeev", ""], ["Li", "Yuanzhi", ""], ["Liang", "Yingyu", ""], ["Ma", "Tengyu", ""], ["Risteski", "Andrej", ""]]}, {"id": "1502.03529", "submitter": "Zhao Shen-Yi", "authors": "Shen-Yi Zhao, Wu-Jun Li, Zhi-Hua Zhou", "title": "Scalable Stochastic Alternating Direction Method of Multipliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic alternating direction method of multipliers (ADMM), which visits\nonly one sample or a mini-batch of samples each time, has recently been proved\nto achieve better performance than batch ADMM. However, most stochastic methods\ncan only achieve a convergence rate $O(1/\\sqrt T)$ on general convex\nproblems,where T is the number of iterations. Hence, these methods are not\nscalable with respect to convergence rate (computation cost). There exists only\none stochastic method, called SA-ADMM, which can achieve convergence rate\n$O(1/T)$ on general convex problems. However, an extra memory is needed for\nSA-ADMM to store the historic gradients on all samples, and thus it is not\nscalable with respect to storage cost. In this paper, we propose a novel\nmethod, called scalable stochastic ADMM(SCAS-ADMM), for large-scale\noptimization and learning problems. Without the need to store the historic\ngradients, SCAS-ADMM can achieve the same convergence rate $O(1/T)$ as the best\nstochastic method SA-ADMM and batch ADMM on general convex problems.\nExperiments on graph-guided fused lasso show that SCAS-ADMM can achieve\nstate-of-the-art performance in real applications\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 04:01:46 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2015 13:15:14 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2015 10:01:27 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Zhao", "Shen-Yi", ""], ["Li", "Wu-Jun", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1502.03537", "submitter": "Vamsi Ithapu", "authors": "Vamsi K Ithapu, Sathya Ravi, Vikas Singh", "title": "Convergence of gradient based pre-training in Denoising autoencoders", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep architectures is at least in part attributed to the\nlayer-by-layer unsupervised pre-training that initializes the network. Various\npapers have reported extensive empirical analysis focusing on the design and\nimplementation of good pre-training procedures. However, an understanding\npertaining to the consistency of parameter estimates, the convergence of\nlearning procedures and the sample size estimates is still unavailable in the\nliterature. In this work, we study pre-training in classical and distributed\ndenoising autoencoders with these goals in mind. We show that the gradient\nconverges at the rate of $\\frac{1}{\\sqrt{N}}$ and has a sub-linear dependence\non the size of the autoencoder network. In a distributed setting where disjoint\nsections of the whole network are pre-trained synchronously, we show that the\nconvergence improves by at least $\\tau^{3/4}$, where $\\tau$ corresponds to the\nsize of the sections. We provide a broad set of experiments to empirically\nevaluate the suggested behavior.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 04:31:36 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Ithapu", "Vamsi K", ""], ["Ravi", "Sathya", ""], ["Singh", "Vikas", ""]]}, {"id": "1502.03581", "submitter": "Ashish Chandra", "authors": "Ashish Chandra, Mohammad Suaib, and Dr. Rizwan Beg", "title": "Web spam classification using supervised artificial neural network\n  algorithms", "comments": "10 Pages in Advanced Computational Intelligence: An International\n  Journal (ACII), Vol.2, No.1, January 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the rapid growth in technology employed by the spammers, there is a\nneed of classifiers that are more efficient, generic and highly adaptive.\nNeural Network based technologies have high ability of adaption as well as\ngeneralization. As per our knowledge, very little work has been done in this\nfield using neural network. We present this paper to fill this gap. This paper\nevaluates performance of three supervised learning algorithms of artificial\nneural network by creating classifiers for the complex problem of latest web\nspam pattern classification. These algorithms are Conjugate Gradient algorithm,\nResilient Backpropagation learning, and Levenberg-Marquardt algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 09:58:23 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Chandra", "Ashish", ""], ["Suaib", "Mohammad", ""], ["Beg", "Dr. Rizwan", ""]]}, {"id": "1502.03601", "submitter": "Kalyan Nagaraj", "authors": "Kalyan Nagaraj, Amulyashree Sridhar", "title": "A Predictive System for detection of Bankruptcy using Machine Learning\n  techniques", "comments": "11 pages, 7 figures", "journal-ref": "Kalyan Nagaraj, Amulyashree Sridhar (2015). A Predictive System\n  for detection of Bankruptcy using Machine learning techniques. IJDKP. 5(1):\n  29-40", "doi": "10.5121/ijdkp.2015.5103", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bankruptcy is a legal procedure that claims a person or organization as a\ndebtor. It is essential to ascertain the risk of bankruptcy at initial stages\nto prevent financial losses. In this perspective, different soft computing\ntechniques can be employed to ascertain bankruptcy. This study proposes a\nbankruptcy prediction system to categorize the companies based on extent of\nrisk. The prediction system acts as a decision support tool for detection of\nbankruptcy\n  Keywords: Bankruptcy, soft computing, decision support tool\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 11:07:51 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Nagaraj", "Kalyan", ""], ["Sridhar", "Amulyashree", ""]]}, {"id": "1502.03630", "submitter": "Min Yang", "authors": "Min Yang, Tianyi Cui, Wenting Tu", "title": "Ordering-sensitive and Semantic-aware Topic Modeling", "comments": "To appear in proceedings of AAAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling of textual corpora is an important and challenging problem. In\nmost previous work, the \"bag-of-words\" assumption is usually made which ignores\nthe ordering of words. This assumption simplifies the computation, but it\nunrealistically loses the ordering information and the semantic of words in the\ncontext. In this paper, we present a Gaussian Mixture Neural Topic Model\n(GMNTM) which incorporates both the ordering of words and the semantic meaning\nof sentences into topic modeling. Specifically, we represent each topic as a\ncluster of multi-dimensional vectors and embed the corpus into a collection of\nvectors generated by the Gaussian mixture model. Each word is affected not only\nby its topic, but also by the embedding vector of its surrounding words and the\ncontext. The Gaussian mixture components and the topic of documents, sentences\nand words can be learnt jointly. Extensive experiments show that our model can\nlearn better topics and more accurate word distributions for each topic.\nQuantitatively, comparing to state-of-the-art topic modeling approaches, GMNTM\nobtains significantly better performance in terms of perplexity, retrieval\naccuracy and classification accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 12:32:39 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Yang", "Min", ""], ["Cui", "Tianyi", ""], ["Tu", "Wenting", ""]]}, {"id": "1502.03648", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Over-Sampling in a Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) are the state of the art on many engineering\nproblems such as computer vision and audition. A key factor in the success of\nthe DNN is scalability - bigger networks work better. However, the reason for\nthis scalability is not yet well understood. Here, we interpret the DNN as a\ndiscrete system, of linear filters followed by nonlinear activations, that is\nsubject to the laws of sampling theory. In this context, we demonstrate that\nover-sampled networks are more selective, learn faster and learn more robustly.\nOur findings may ultimately generalize to the human brain.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 13:29:03 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1502.03682", "submitter": "Matthias Samwald", "authors": "Jose Antonio Mi\\~narro-Gim\\'enez, Oscar Mar\\'in-Alonso, Matthias\n  Samwald", "title": "Applying deep learning techniques on medical corpora from the World Wide\n  Web: a prototypical system and evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  BACKGROUND: The amount of biomedical literature is rapidly growing and it is\nbecoming increasingly difficult to keep manually curated knowledge bases and\nontologies up-to-date. In this study we applied the word2vec deep learning\ntoolkit to medical corpora to test its potential for identifying relationships\nfrom unstructured text. We evaluated the efficiency of word2vec in identifying\nproperties of pharmaceuticals based on mid-sized, unstructured medical text\ncorpora available on the web. Properties included relationships to diseases\n('may treat') or physiological processes ('has physiological effect'). We\ncompared the relationships identified by word2vec with manually curated\ninformation from the National Drug File - Reference Terminology (NDF-RT)\nontology as a gold standard. RESULTS: Our results revealed a maximum accuracy\nof 49.28% which suggests a limited ability of word2vec to capture linguistic\nregularities on the collected medical corpora compared with other published\nresults. We were able to document the influence of different parameter settings\non result accuracy and found and unexpected trade-off between ranking quality\nand accuracy. Pre-processing corpora to reduce syntactic variability proved to\nbe a good strategy for increasing the utility of the trained vector models.\nCONCLUSIONS: Word2vec is a very efficient implementation for computing vector\nrepresentations and for its ability to identify relationships in textual data\nwithout any prior domain knowledge. We found that the ranking and retrieved\nresults generated by word2vec were not of sufficient quality for automatic\npopulation of knowledge bases and ontologies, but could serve as a starting\npoint for further manual curation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 14:44:15 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Mi\u00f1arro-Gim\u00e9nez", "Jose Antonio", ""], ["Mar\u00edn-Alonso", "Oscar", ""], ["Samwald", "Matthias", ""]]}, {"id": "1502.03879", "submitter": "Weiya Ren", "authors": "Weiya Ren", "title": "Semi-supervised Data Representation via Affinity Graph Learning", "comments": "10 pages,2 Tables. Written in Aug,2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the general problem of utilizing both labeled and unlabeled data\nto improve data representation performance. A new semi-supervised learning\nframework is proposed by combing manifold regularization and data\nrepresentation methods such as Non negative matrix factorization and sparse\ncoding. We adopt unsupervised data representation methods as the learning\nmachines because they do not depend on the labeled data, which can improve\nmachine's generation ability as much as possible. The proposed framework forms\nthe Laplacian regularizer through learning the affinity graph. We incorporate\nthe new Laplacian regularizer into the unsupervised data representation to\nsmooth the low dimensional representation of data and make use of label\ninformation. Experimental results on several real benchmark datasets indicate\nthat our semi-supervised learning framework achieves encouraging results\ncompared with state-of-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 03:35:15 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Ren", "Weiya", ""]]}, {"id": "1502.03919", "submitter": "Aviv Tamar", "authors": "Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, Shie Mannor", "title": "Policy Gradient for Coherent Risk Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several authors have recently developed risk-sensitive policy gradient\nmethods that augment the standard expected cost minimization problem with a\nmeasure of variability in cost. These studies have focused on specific\nrisk-measures, such as the variance or conditional value at risk (CVaR). In\nthis work, we extend the policy gradient method to the whole class of coherent\nrisk measures, which is widely accepted in finance and operations research,\namong other fields. We consider both static and time-consistent dynamic risk\nmeasures. For static risk measures, our approach is in the spirit of policy\ngradient algorithms and combines a standard sampling approach with convex\nprogramming. For dynamic risk measures, our approach is actor-critic style and\ninvolves explicit approximation of value function. Most importantly, our\ncontribution presents a unified approach to risk-sensitive reinforcement\nlearning that generalizes and extends previous results.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 09:16:24 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 06:31:42 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Tamar", "Aviv", ""], ["Chow", "Yinlam", ""], ["Ghavamzadeh", "Mohammad", ""], ["Mannor", "Shie", ""]]}, {"id": "1502.04033", "submitter": "Tobias Reitmaier", "authors": "Tobias Reitmaier and Bernhard Sick", "title": "The Responsibility Weighted Mahalanobis Kernel for Semi-Supervised\n  Training of Support Vector Machines for Classification", "comments": null, "journal-ref": null, "doi": "10.1016/j.ins.2015.06.027", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel functions in support vector machines (SVM) are needed to assess the\nsimilarity of input samples in order to classify these samples, for instance.\nBesides standard kernels such as Gaussian (i.e., radial basis function, RBF) or\npolynomial kernels, there are also specific kernels tailored to consider\nstructure in the data for similarity assessment. In this article, we will\ncapture structure in data by means of probabilistic mixture density models, for\nexample Gaussian mixtures in the case of real-valued input spaces. From the\ndistance measures that are inherently contained in these models, e.g.,\nMahalanobis distances in the case of Gaussian mixtures, we derive a new kernel,\nthe responsibility weighted Mahalanobis (RWM) kernel. Basically, this kernel\nemphasizes the influence of model components from which any two samples that\nare compared are assumed to originate (that is, the \"responsible\" model\ncomponents). We will see that this kernel outperforms the RBF kernel and other\nkernels capturing structure in data (such as the LAP kernel in Laplacian SVM)\nin many applications where partially labeled data are available, i.e., for\nsemi-supervised training of SVM. Other key advantages are that the RWM kernel\ncan easily be used with standard SVM implementations and training algorithms\nsuch as sequential minimal optimization, and heuristics known for the\nparametrization of RBF kernels in a C-SVM can easily be transferred to this new\nkernel. Properties of the RWM kernel are demonstrated with 20 benchmark data\nsets and an increasing percentage of labeled samples in the training data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 15:48:00 GMT"}, {"version": "v2", "created": "Mon, 16 Feb 2015 13:02:05 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Reitmaier", "Tobias", ""], ["Sick", "Bernhard", ""]]}, {"id": "1502.04042", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Abstract Learning via Demodulation in a Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the brain, deep neural networks (DNN) are thought to learn\nabstract representations through their hierarchical architecture. However, at\npresent, how this happens is not well understood. Here, we demonstrate that DNN\nlearn abstract representations by a process of demodulation. We introduce a\nbiased sigmoid activation function and use it to show that DNN learn and\nperform better when optimized for demodulation. Our findings constitute the\nfirst unambiguous evidence that DNN perform abstract learning in practical use.\nOur findings may also explain abstract learning in the human brain.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 16:09:41 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1502.04081", "submitter": "David Belanger", "authors": "David Belanger and Sham Kakade", "title": "A Linear Dynamical System Model for Text", "comments": "Accepted at International Conference of Machine Learning 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low dimensional representations of words allow accurate NLP models to be\ntrained on limited annotated data. While most representations ignore words'\nlocal context, a natural way to induce context-dependent representations is to\nperform inference in a probabilistic latent-variable sequence model. Given the\nrecent success of continuous vector space word representations, we provide such\nan inference procedure for continuous states, where words' representations are\ngiven by the posterior mean of a linear dynamical system. Here, efficient\ninference can be performed using Kalman filtering. Our learning algorithm is\nextremely scalable, operating on simple cooccurrence counts for both parameter\ninitialization using the method of moments and subsequent iterations of EM. In\nour experiments, we employ our inferred word embeddings as features in standard\ntagging tasks, obtaining significant accuracy improvements. Finally, the Kalman\nfilter updates can be seen as a linear recurrent neural network. We demonstrate\nthat using the parameters of our model to initialize a non-linear recurrent\nneural network language model reduces its training time by a day and yields\nlower perplexity.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 18:39:29 GMT"}, {"version": "v2", "created": "Sun, 31 May 2015 20:04:53 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Belanger", "David", ""], ["Kakade", "Sham", ""]]}, {"id": "1502.04137", "submitter": "Nader Bshouty", "authors": "Hasan Abasi and Nader H. Bshouty and Hanna Mazzawi", "title": "Non-Adaptive Learning a Hidden Hipergraph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new deterministic algorithm that non-adaptively learns a hidden\nhypergraph from edge-detecting queries. All previous non-adaptive algorithms\neither run in exponential time or have non-optimal query complexity. We give\nthe first polynomial time non-adaptive learning algorithm for learning\nhypergraph that asks almost optimal number of queries.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 21:32:12 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Abasi", "Hasan", ""], ["Bshouty", "Nader H.", ""], ["Mazzawi", "Hanna", ""]]}, {"id": "1502.04148", "submitter": "James Voss", "authors": "James Voss, Mikhail Belkin, and Luis Rademacher", "title": "A Pseudo-Euclidean Iteration for Optimal Recovery in Noisy ICA", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent Component Analysis (ICA) is a popular model for blind signal\nseparation. The ICA model assumes that a number of independent source signals\nare linearly mixed to form the observed signals. We propose a new algorithm,\nPEGI (for pseudo-Euclidean Gradient Iteration), for provable model recovery for\nICA with Gaussian noise. The main technical innovation of the algorithm is to\nuse a fixed point iteration in a pseudo-Euclidean (indefinite \"inner product\")\nspace. The use of this indefinite \"inner product\" resolves technical issues\ncommon to several existing algorithms for noisy ICA. This leads to an algorithm\nwhich is conceptually simple, efficient and accurate in testing.\n  Our second contribution is combining PEGI with the analysis of objectives for\noptimal recovery in the noisy ICA model. It has been observed that the direct\napproach of demixing with the inverse of the mixing matrix is suboptimal for\nsignal recovery in terms of the natural Signal to Interference plus Noise Ratio\n(SINR) criterion. There have been several partial solutions proposed in the ICA\nliterature. It turns out that any solution to the mixing matrix reconstruction\nproblem can be used to construct an SINR-optimal ICA demixing, despite the fact\nthat SINR itself cannot be computed from data. That allows us to obtain a\npractical and provably SINR-optimal recovery method for ICA with arbitrary\nGaussian noise.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 23:18:35 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 16:05:56 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Voss", "James", ""], ["Belkin", "Mikhail", ""], ["Rademacher", "Luis", ""]]}, {"id": "1502.04149", "submitter": "Po-Sen Huang", "authors": "Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson, Paris Smaragdis", "title": "Joint Optimization of Masks and Deep Recurrent Neural Networks for\n  Monaural Source Separation", "comments": null, "journal-ref": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n  vol.23, no.12, pp.2136-2147, Dec. 2015", "doi": "10.1109/TASLP.2015.2468583", "report-no": null, "categories": "cs.SD cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monaural source separation is important for many real world applications. It\nis challenging because, with only a single channel of information available,\nwithout any constraints, an infinite number of solutions are possible. In this\npaper, we explore joint optimization of masking functions and deep recurrent\nneural networks for monaural source separation tasks, including monaural speech\nseparation, monaural singing voice separation, and speech denoising. The joint\noptimization of the deep recurrent neural networks with an extra masking layer\nenforces a reconstruction constraint. Moreover, we explore a discriminative\ncriterion for training neural networks to further enhance the separation\nperformance. We evaluate the proposed system on the TSP, MIR-1K, and TIMIT\ndatasets for speech separation, singing voice separation, and speech denoising\ntasks, respectively. Our approaches achieve 2.30--4.98 dB SDR gain compared to\nNMF models in the speech separation task, 2.30--2.48 dB GNSDR gain and\n4.32--5.42 dB GSIR gain compared to existing models in the singing voice\nseparation task, and outperform NMF and DNN baselines in the speech denoising\ntask.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 23:22:16 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 04:22:20 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2015 04:20:33 GMT"}, {"version": "v4", "created": "Thu, 1 Oct 2015 02:58:01 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Huang", "Po-Sen", ""], ["Kim", "Minje", ""], ["Hasegawa-Johnson", "Mark", ""], ["Smaragdis", "Paris", ""]]}, {"id": "1502.04156", "submitter": "Yoshua Bengio", "authors": "Yoshua Bengio, Dong-Hyun Lee, Jorg Bornschein, Thomas Mesnard and\n  Zhouhan Lin", "title": "Towards Biologically Plausible Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroscientists have long criticised deep learning algorithms as incompatible\nwith current knowledge of neurobiology. We explore more biologically plausible\nversions of deep representation learning, focusing here mostly on unsupervised\nlearning but developing a learning mechanism that could account for supervised,\nunsupervised and reinforcement learning. The starting point is that the basic\nlearning rule believed to govern synaptic weight updates\n(Spike-Timing-Dependent Plasticity) arises out of a simple update rule that\nmakes a lot of sense from a machine learning point of view and can be\ninterpreted as gradient descent on some objective function so long as the\nneuronal dynamics push firing rates towards better values of the objective\nfunction (be it supervised, unsupervised, or reward-driven). The second main\nidea is that this corresponds to a form of the variational EM algorithm, i.e.,\nwith approximate rather than exact posteriors, implemented by neural dynamics.\nAnother contribution of this paper is that the gradients required for updating\nthe hidden states in the above variational interpretation can be estimated\nusing an approximation that only requires propagating activations forward and\nbackward, with pairs of layers learning to form a denoising auto-encoder.\nFinally, we extend the theory about the probabilistic interpretation of\nauto-encoders to justify improved sampling schemes based on the generative\ninterpretation of denoising auto-encoders, and we validate all these ideas on\ngenerative learning tasks.\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 01:11:25 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 04:13:44 GMT"}, {"version": "v3", "created": "Tue, 9 Aug 2016 01:57:09 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Bengio", "Yoshua", ""], ["Lee", "Dong-Hyun", ""], ["Bornschein", "Jorg", ""], ["Mesnard", "Thomas", ""], ["Lin", "Zhouhan", ""]]}, {"id": "1502.04168", "submitter": "Shaobo Lin", "authors": "Shaobo Lin", "title": "Nonparametric regression using needlet kernels for spherical data", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Needlets have been recognized as state-of-the-art tools to tackle spherical\ndata, due to their excellent localization properties in both spacial and\nfrequency domains.\n  This paper considers developing kernel methods associated with the needlet\nkernel for nonparametric regression problems whose predictor variables are\ndefined on a sphere. Due to the localization property in the frequency domain,\nwe prove that the regularization parameter of the kernel ridge regression\nassociated with the needlet kernel can decrease arbitrarily fast. A natural\nconsequence is that the regularization term for the kernel ridge regression is\nnot necessary in the sense of rate optimality. Based on the excellent\nlocalization property in the spacial domain further, we also prove that all the\n$l^{q}$ $(01\\leq q < \\infty)$ kernel regularization estimates associated with\nthe needlet kernel, including the kernel lasso estimate and the kernel bridge\nestimate, possess almost the same generalization capability for a large range\nof regularization parameters in the sense of rate optimality.\n  This finding tentatively reveals that, if the needlet kernel is utilized,\nthen the choice of $q$ might not have a strong impact in terms of the\ngeneralization capability in some modeling contexts. From this perspective, $q$\ncan be arbitrarily specified, or specified merely by other no generalization\ncriteria like smoothness, computational complexity, sparsity, etc..\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 05:37:32 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 12:34:58 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Lin", "Shaobo", ""]]}, {"id": "1502.04187", "submitter": "Soheil Keshmiri", "authors": "Soheil Keshmiri, Xin Zheng, Chee Meng Chew, Chee Khiang Pang", "title": "Application of Deep Neural Network in Estimation of the Weld Bead\n  Parameters", "comments": "Disapproval of funding organization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning approach to estimation of the bead parameters in\nwelding tasks. Our model is based on a four-hidden-layer neural network\narchitecture. More specifically, the first three hidden layers of this\narchitecture utilize Sigmoid function to produce their respective intermediate\noutputs. On the other hand, the last hidden layer uses a linear transformation\nto generate the final output of this architecture. This transforms our deep\nnetwork architecture from a classifier to a non-linear regression model. We\ncompare the performance of our deep network with a selected number of results\nin the literature to show a considerable improvement in reducing the errors in\nestimation of these values. Furthermore, we show its scalability on estimating\nthe weld bead parameters with same level of accuracy on combination of datasets\nthat pertain to different welding techniques. This is a nontrivial result that\nis counter-intuitive to the general belief in this field of research.\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 10:58:53 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2015 11:05:10 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Keshmiri", "Soheil", ""], ["Zheng", "Xin", ""], ["Chew", "Chee Meng", ""], ["Pang", "Chee Khiang", ""]]}, {"id": "1502.04248", "submitter": "Aly El Gamal", "authors": "Aamir Anis, Aly El Gamal, A. Salman Avestimehr, Antonio Ortega", "title": "Asymptotic Justification of Bandlimited Interpolation of Graph signals\n  for Semi-Supervised Learning", "comments": "To appear in ICASSP 2015, 5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based methods play an important role in unsupervised and\nsemi-supervised learning tasks by taking into account the underlying geometry\nof the data set. In this paper, we consider a statistical setting for\nsemi-supervised learning and provide a formal justification of the recently\nintroduced framework of bandlimited interpolation of graph signals. Our\nanalysis leads to the interpretation that, given enough labeled data, this\nmethod is very closely related to a constrained low density separation problem\nas the number of data points tends to infinity. We demonstrate the practical\nutility of our results through simple experiments.\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 21:23:14 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Anis", "Aamir", ""], ["Gamal", "Aly El", ""], ["Avestimehr", "A. Salman", ""], ["Ortega", "Antonio", ""]]}, {"id": "1502.04269", "submitter": "Berk Ustun", "authors": "Berk Ustun and Cynthia Rudin", "title": "Supersparse Linear Integer Models for Optimized Medical Scoring Systems", "comments": "This version reflects our findings on SLIM as of January 2016\n  (arXiv:1306.5860 and arXiv:1405.4047 are out-of-date). The final published\n  version of this articled is available at http://www.springerlink.com", "journal-ref": null, "doi": "10.1007/s10994-015-5528-6", "report-no": null, "categories": "stat.ML cs.DM cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scoring systems are linear classification models that only require users to\nadd, subtract and multiply a few small numbers in order to make a prediction.\nThese models are in widespread use by the medical community, but are difficult\nto learn from data because they need to be accurate and sparse, have coprime\ninteger coefficients, and satisfy multiple operational constraints. We present\na new method for creating data-driven scoring systems called a Supersparse\nLinear Integer Model (SLIM). SLIM scoring systems are built by solving an\ninteger program that directly encodes measures of accuracy (the 0-1 loss) and\nsparsity (the $\\ell_0$-seminorm) while restricting coefficients to coprime\nintegers. SLIM can seamlessly incorporate a wide range of operational\nconstraints related to accuracy and sparsity, and can produce highly tailored\nmodels without parameter tuning. We provide bounds on the testing and training\naccuracy of SLIM scoring systems, and present a new data reduction technique\nthat can improve scalability by eliminating a portion of the training data\nbeforehand. Our paper includes results from a collaboration with the\nMassachusetts General Hospital Sleep Laboratory, where SLIM was used to create\na highly tailored scoring system for sleep apnea screening\n", "versions": [{"version": "v1", "created": "Sun, 15 Feb 2015 01:26:41 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 14:46:40 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2016 17:34:21 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Ustun", "Berk", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1502.04390", "submitter": "Yann Dauphin", "authors": "Yann N. Dauphin, Harm de Vries, Yoshua Bengio", "title": "Equilibrated adaptive learning rates for non-convex optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter-specific adaptive learning rate methods are computationally\nefficient ways to reduce the ill-conditioning problems encountered when\ntraining large deep networks. Following recent work that strongly suggests that\nmost of the critical points encountered when training such networks are saddle\npoints, we find how considering the presence of negative eigenvalues of the\nHessian could help us design better suited adaptive learning rate schemes. We\nshow that the popular Jacobi preconditioner has undesirable behavior in the\npresence of both positive and negative curvature, and present theoretical and\nempirical evidence that the so-called equilibration preconditioner is\ncomparatively better suited to non-convex problems. We introduce a novel\nadaptive learning rate scheme, called ESGD, based on the equilibration\npreconditioner. Our experiments show that ESGD performs as well or better than\nRMSProp in terms of convergence speed, always clearly improving over plain\nstochastic gradient descent.\n", "versions": [{"version": "v1", "created": "Sun, 15 Feb 2015 23:41:33 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2015 23:04:39 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Dauphin", "Yann N.", ""], ["de Vries", "Harm", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1502.04434", "submitter": "Sergey Demyanov", "authors": "Sergey Demyanov, James Bailey, Ramamohanarao Kotagiri, Christopher\n  Leckie", "title": "Invariant backpropagation: how to train a transformation-invariant\n  neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many classification problems a classifier should be robust to small\nvariations in the input vector. This is a desired property not only for\nparticular transformations, such as translation and rotation in image\nclassification problems, but also for all others for which the change is small\nenough to retain the object perceptually indistinguishable. We propose two\nextensions of the backpropagation algorithm that train a neural network to be\nrobust to variations in the feature vector. While the first of them enforces\nrobustness of the loss function to all variations, the second method trains the\npredictions to be robust to a particular variation which changes the loss\nfunction the most. The second methods demonstrates better results, but is\nslightly slower. We analytically compare the proposed algorithm with two the\nmost similar approaches (Tangent BP and Adversarial Training), and propose\ntheir fast versions. In the experimental part we perform comparison of all\nalgorithms in terms of classification accuracy and robustness to noise on MNIST\nand CIFAR-10 datasets. Additionally we analyze how the performance of the\nproposed algorithm depends on the dataset size and data augmentation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 06:28:35 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2015 11:44:59 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2016 04:49:00 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Demyanov", "Sergey", ""], ["Bailey", "James", ""], ["Kotagiri", "Ramamohanarao", ""], ["Leckie", "Christopher", ""]]}, {"id": "1502.04469", "submitter": "Peng Yang", "authors": "Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang and Xiao-Li Li", "title": "Classification and its applications for drug-target interaction\n  identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.MN q-bio.QM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Classification is one of the most popular and widely used supervised learning\ntasks, which categorizes objects into predefined classes based on known\nknowledge. Classification has been an important research topic in machine\nlearning and data mining. Different classification methods have been proposed\nand applied to deal with various real-world problems. Unlike unsupervised\nlearning such as clustering, a classifier is typically trained with labeled\ndata before being used to make prediction, and usually achieves higher accuracy\nthan unsupervised one.\n  In this paper, we first define classification and then review several\nrepresentative methods. After that, we study in details the application of\nclassification to a critical problem in drug discovery, i.e., drug-target\nprediction, due to the challenges in predicting possible interactions between\ndrugs and targets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 09:17:40 GMT"}, {"version": "v2", "created": "Mon, 23 Feb 2015 02:11:57 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2015 15:57:38 GMT"}, {"version": "v4", "created": "Thu, 12 Mar 2015 02:05:46 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Mei", "Jian-Ping", ""], ["Kwoh", "Chee-Keong", ""], ["Yang", "Peng", ""], ["Li", "Xiao-Li", ""]]}, {"id": "1502.04492", "submitter": "Francesco  Palmieri A. N.", "authors": "Amedeo Buonanno and Francesco A.N. Palmieri", "title": "Towards Building Deep Networks with Bayesian Factor Graphs", "comments": "Submitted for journal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Multi-Layer Network based on the Bayesian framework of the\nFactor Graphs in Reduced Normal Form (FGrn) applied to a two-dimensional\nlattice. The Latent Variable Model (LVM) is the basic building block of a\nquadtree hierarchy built on top of a bottom layer of random variables that\nrepresent pixels of an image, a feature map, or more generally a collection of\nspatially distributed discrete variables. The multi-layer architecture\nimplements a hierarchical data representation that, via belief propagation, can\nbe used for learning and inference. Typical uses are pattern completion,\ncorrection and classification. The FGrn paradigm provides great flexibility and\nmodularity and appears as a promising candidate for building deep networks: the\nsystem can be easily extended by introducing new and different (in cardinality\nand in type) variables. Prior knowledge, or supervised information, can be\nintroduced at different scales. The FGrn paradigm provides a handy way for\nbuilding all kinds of architectures by interconnecting only three types of\nunits: Single Input Single Output (SISO) blocks, Sources and Replicators. The\nnetwork is designed like a circuit diagram and the belief messages flow\nbidirectionally in the whole system. The learning algorithms operate only\nlocally within each block. The framework is demonstrated in this paper in a\nthree-layer structure applied to images extracted from a standard data set.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 11:01:25 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Buonanno", "Amedeo", ""], ["Palmieri", "Francesco A. N.", ""]]}, {"id": "1502.04502", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "Clustering by Descending to the Nearest Neighbor in the Delaunay Graph\n  Space", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In our previous works, we proposed a physically-inspired rule to organize the\ndata points into an in-tree (IT) structure, in which some undesired edges are\nallowed to occur. By removing those undesired or redundant edges, this IT\nstructure is divided into several separate parts, each representing one\ncluster. In this work, we seek to prevent the undesired edges from arising at\nthe source. Before using the physically-inspired rule, data points are at first\norganized into a proximity graph which restricts each point to select the\noptimal directed neighbor just among its neighbors. Consequently, separated\nin-trees or clusters automatically arise, without redundant edges requiring to\nbe removed.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 11:50:42 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1502.04585", "submitter": "Moritz Hardt", "authors": "Avrim Blum and Moritz Hardt", "title": "The Ladder: A Reliable Leaderboard for Machine Learning Competitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The organizer of a machine learning competition faces the problem of\nmaintaining an accurate leaderboard that faithfully represents the quality of\nthe best submission of each competing team. What makes this estimation problem\nparticularly challenging is its sequential and adaptive nature. As participants\nare allowed to repeatedly evaluate their submissions on the leaderboard, they\nmay begin to overfit to the holdout data that supports the leaderboard. Few\ntheoretical results give actionable advice on how to design a reliable\nleaderboard. Existing approaches therefore often resort to poorly understood\nheuristics such as limiting the bit precision of answers and the rate of\nre-submission.\n  In this work, we introduce a notion of \"leaderboard accuracy\" tailored to the\nformat of a competition. We introduce a natural algorithm called \"the Ladder\"\nand demonstrate that it simultaneously supports strong theoretical guarantees\nin a fully adaptive model of estimation, withstands practical adversarial\nattacks, and achieves high utility on real submission files from an actual\ncompetition hosted by Kaggle.\n  Notably, we are able to sidestep a powerful recent hardness result for\nadaptive risk estimation that rules out algorithms such as ours under a\nseemingly very similar notion of accuracy. On a practical note, we provide a\ncompletely parameter-free variant of our algorithm that can be deployed in a\nreal competition with no tuning required whatsoever.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 15:53:03 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Blum", "Avrim", ""], ["Hardt", "Moritz", ""]]}, {"id": "1502.04617", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Deep Transform: Error Correction via Probabilistic Re-Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Errors in data are usually unwelcome and so some means to correct them is\nuseful. However, it is difficult to define, detect or correct errors in an\nunsupervised way. Here, we train a deep neural network to re-synthesize its\ninputs at its output layer for a given class of data. We then exploit the fact\nthat this abstract transformation, which we call a deep transform (DT),\ninherently rejects information (errors) existing outside of the abstract\nfeature space. Using the DT to perform probabilistic re-synthesis, we\ndemonstrate the recovery of data that has been subject to extreme degradation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 16:41:26 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1502.04622", "submitter": "Balaji Lakshminarayanan", "authors": "Balaji Lakshminarayanan, Daniel M. Roy and Yee Whye Teh", "title": "Particle Gibbs for Bayesian Additive Regression Trees", "comments": null, "journal-ref": "Proceedings of the 18th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2015, San Diego, CA, USA. JMLR: W&CP\n  volume 38", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive regression trees are flexible non-parametric models and popular\noff-the-shelf tools for real-world non-linear regression. In application\ndomains, such as bioinformatics, where there is also demand for probabilistic\npredictions with measures of uncertainty, the Bayesian additive regression\ntrees (BART) model, introduced by Chipman et al. (2010), is increasingly\npopular. As data sets have grown in size, however, the standard\nMetropolis-Hastings algorithms used to perform inference in BART are proving\ninadequate. In particular, these Markov chains make local changes to the trees\nand suffer from slow mixing when the data are high-dimensional or the best\nfitting trees are more than a few layers deep. We present a novel sampler for\nBART based on the Particle Gibbs (PG) algorithm (Andrieu et al., 2010) and a\ntop-down particle filtering algorithm for Bayesian decision trees\n(Lakshminarayanan et al., 2013). Rather than making local changes to individual\ntrees, the PG sampler proposes a complete tree to fit the residual. Experiments\nshow that the PG sampler outperforms existing samplers in many settings.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 16:48:30 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Lakshminarayanan", "Balaji", ""], ["Roy", "Daniel M.", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1502.04623", "submitter": "Ivo Danihelka", "authors": "Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan\n  Wierstra", "title": "DRAW: A Recurrent Neural Network For Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural\nnetwork architecture for image generation. DRAW networks combine a novel\nspatial attention mechanism that mimics the foveation of the human eye, with a\nsequential variational auto-encoding framework that allows for the iterative\nconstruction of complex images. The system substantially improves on the state\nof the art for generative models on MNIST, and, when trained on the Street View\nHouse Numbers dataset, it generates images that cannot be distinguished from\nreal data with the naked eye.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 16:48:56 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 15:29:42 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Gregor", "Karol", ""], ["Danihelka", "Ivo", ""], ["Graves", "Alex", ""], ["Rezende", "Danilo Jimenez", ""], ["Wierstra", "Daan", ""]]}, {"id": "1502.04635", "submitter": "Paul Reverdy", "authors": "Paul Reverdy and Naomi E. Leonard", "title": "Parameter estimation in softmax decision-making models with linear\n  objective functions", "comments": "In press", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an eye towards human-centered automation, we contribute to the\ndevelopment of a systematic means to infer features of human decision-making\nfrom behavioral data. Motivated by the common use of softmax selection in\nmodels of human decision-making, we study the maximum likelihood parameter\nestimation problem for softmax decision-making models with linear objective\nfunctions. We present conditions under which the likelihood function is convex.\nThese allow us to provide sufficient conditions for convergence of the\nresulting maximum likelihood estimator and to construct its asymptotic\ndistribution. In the case of models with nonlinear objective functions, we show\nhow the estimator can be applied by linearizing about a nominal parameter\nvalue. We apply the estimator to fit the stochastic UCL (Upper Credible Limit)\nmodel of human decision-making to human subject data. We show statistically\nsignificant differences in behavior across related, but distinct, tasks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 17:17:24 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2015 19:50:20 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Reverdy", "Paul", ""], ["Leonard", "Naomi E.", ""]]}, {"id": "1502.04681", "submitter": "Nitish Srivastava", "authors": "Nitish Srivastava, Elman Mansimov and Ruslan Salakhutdinov", "title": "Unsupervised Learning of Video Representations using LSTMs", "comments": "Added link to code on github", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use multilayer Long Short Term Memory (LSTM) networks to learn\nrepresentations of video sequences. Our model uses an encoder LSTM to map an\ninput sequence into a fixed length representation. This representation is\ndecoded using single or multiple decoder LSTMs to perform different tasks, such\nas reconstructing the input sequence, or predicting the future sequence. We\nexperiment with two kinds of input sequences - patches of image pixels and\nhigh-level representations (\"percepts\") of video frames extracted using a\npretrained convolutional net. We explore different design choices such as\nwhether the decoder LSTMs should condition on the generated output. We analyze\nthe outputs of the model qualitatively to see how well the model can\nextrapolate the learned video representation into the future and into the past.\nWe try to visualize and interpret the learned features. We stress test the\nmodel by running it on longer time scales and on out-of-domain data. We further\nevaluate the representations by finetuning them for a supervised learning\nproblem - human action recognition on the UCF-101 and HMDB-51 datasets. We show\nthat the representations help improve classification accuracy, especially when\nthere are only a few training examples. Even models pretrained on unrelated\ndatasets (300 hours of YouTube videos) can help action recognition performance.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 20:00:07 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 23:45:59 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2016 00:42:07 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Srivastava", "Nitish", ""], ["Mansimov", "Elman", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1502.04689", "submitter": "Zemin Zhang", "authors": "Zemin Zhang, Shuchin Aeron", "title": "Exact tensor completion using t-SVD", "comments": "16 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we focus on the problem of completion of multidimensional\narrays (also referred to as tensors) from limited sampling. Our approach is\nbased on a recently proposed tensor-Singular Value Decomposition (t-SVD) [1].\nUsing this factorization one can derive notion of tensor rank, referred to as\nthe tensor tubal rank, which has optimality properties similar to that of\nmatrix rank derived from SVD. As shown in [2] some multidimensional data, such\nas panning video sequences exhibit low tensor tubal rank and we look at the\nproblem of completing such data under random sampling of the data cube. We show\nthat by solving a convex optimization problem, which minimizes the tensor\nnuclear norm obtained as the convex relaxation of tensor tubal rank, one can\nguarantee recovery with overwhelming probability as long as samples in\nproportion to the degrees of freedom in t-SVD are observed. In this sense our\nresults are order-wise optimal. The conditions under which this result holds\nare very similar to the incoherency conditions for the matrix completion,\nalbeit we define incoherency under the algebraic set-up of t-SVD. We show the\nperformance of the algorithm on some real data sets and compare it with other\nexisting approaches based on tensor flattening and Tucker decomposition.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 20:37:35 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 19:31:25 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Zhang", "Zemin", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1502.04837", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "Nonparametric Nearest Neighbor Descent Clustering based on Delaunay\n  Triangulation", "comments": "7 pages; 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In our physically inspired in-tree (IT) based clustering algorithm and the\nseries after it, there is only one free parameter involved in computing the\npotential value of each point. In this work, based on the Delaunay\nTriangulation or its dual Voronoi tessellation, we propose a nonparametric\nprocess to compute potential values by the local information. This computation,\nthough nonparametric, is relatively very rough, and consequently, many local\nextreme points will be generated. However, unlike those gradient-based methods,\nour IT-based methods are generally insensitive to those local extremes. This\npositively demonstrates the superiority of these parametric (previous) and\nnonparametric (in this work) IT-based methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 09:27:03 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 12:17:46 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1502.04843", "submitter": "Brijnesh Jain", "authors": "Brijnesh Jain", "title": "Generalized Gradient Learning on Time Series under Elastic\n  Transformations", "comments": "accepted for publication in Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of machine learning algorithms assumes that objects are\nrepresented as vectors. But often the objects we want to learn on are more\nnaturally represented by other data structures such as sequences and time\nseries. For these representations many standard learning algorithms are\nunavailable. We generalize gradient-based learning algorithms to time series\nunder dynamic time warping. To this end, we introduce elastic functions, which\nextend functions on time series to matrix spaces. Necessary conditions are\npresented under which generalized gradient learning on time series is\nconsistent. We indicate how results carry over to arbitrary elastic distance\nfunctions and to sequences consisting of symbolic elements. Specifically, four\nlinear classifiers are extended to time series under dynamic time warping and\napplied to benchmark datasets. Results indicate that generalized gradient\nlearning via elastic functions have the potential to complement the\nstate-of-the-art in statistical pattern recognition on time series.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 10:08:48 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 10:50:41 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Jain", "Brijnesh", ""]]}, {"id": "1502.04868", "submitter": "Rafael Boloix-Tortosa", "authors": "Rafael Boloix-Tortosa, F. Javier Pay\\'an-Somet, Eva Arias-de-Reyna and\n  Juan Jos\\'e Murillo-Fuentes", "title": "Proper Complex Gaussian Processes for Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex-valued signals are used in the modeling of many systems in\nengineering and science, hence being of fundamental interest. Often, random\ncomplex-valued signals are considered to be proper. A proper complex random\nvariable or process is uncorrelated with its complex conjugate. This assumption\nis a good model of the underlying physics in many problems, and simplifies the\ncomputations. While linear processing and neural networks have been widely\nstudied for these signals, the development of complex-valued nonlinear kernel\napproaches remains an open problem. In this paper we propose Gaussian processes\nfor regression as a framework to develop 1) a solution for proper\ncomplex-valued kernel regression and 2) the design of the reproducing kernel\nfor complex-valued inputs, using the convolutional approach for\ncross-covariances. In this design we pay attention to preserve, in the complex\ndomain, the measure of similarity between near inputs. The hyperparameters of\nthe kernel are learned maximizing the marginal likelihood using Wirtinger\nderivatives. Besides, the approach is connected to the multiple output learning\nscenario. In the experiments included, we first solve a proper complex Gaussian\nprocess where the cross-covariance does not cancel, a challenging scenario when\ndealing with proper complex signals. Then we successfully use these novel\nresults to solve some problems previously proposed in the literature as\nbenchmarks, reporting a remarkable improvement in the estimation error.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 11:59:44 GMT"}, {"version": "v2", "created": "Wed, 18 Feb 2015 09:33:34 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Boloix-Tortosa", "Rafael", ""], ["Pay\u00e1n-Somet", "F. Javier", ""], ["Arias-de-Reyna", "Eva", ""], ["Murillo-Fuentes", "Juan Jos\u00e9", ""]]}, {"id": "1502.04956", "submitter": "Wolfgang Gatterbauer", "authors": "Wolfgang Gatterbauer", "title": "The Linearization of Belief Propagation on Pairwise Markov Networks", "comments": "Full version of AAAI 2017 paper with same title (23 pages, 9 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief Propagation (BP) is a widely used approximation for exact\nprobabilistic inference in graphical models, such as Markov Random Fields\n(MRFs). In graphs with cycles, however, no exact convergence guarantees for BP\nare known, in general. For the case when all edges in the MRF carry the same\nsymmetric, doubly stochastic potential, recent works have proposed to\napproximate BP by linearizing the update equations around default values, which\nwas shown to work well for the problem of node classification. The present\npaper generalizes all prior work and derives an approach that approximates\nloopy BP on any pairwise MRF with the problem of solving a linear equation\nsystem. This approach combines exact convergence guarantees and a fast matrix\nimplementation with the ability to model heterogenous networks. Experiments on\nsynthetic graphs with planted edge potentials show that the linearization has\ncomparable labeling accuracy as BP for graphs with weak potentials, while\nspeeding-up inference by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 16:49:23 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2016 15:01:40 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Gatterbauer", "Wolfgang", ""]]}, {"id": "1502.05023", "submitter": "Srinadh Bhojanapalli", "authors": "Srinadh Bhojanapalli, Sujay Sanghavi", "title": "A New Sampling Technique for Tensors", "comments": "29 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose new techniques to sample arbitrary third-order\ntensors, with an objective of speeding up tensor algorithms that have recently\ngained popularity in machine learning. Our main contribution is a new way to\nselect, in a biased random way, only $O(n^{1.5}/\\epsilon^2)$ of the possible\n$n^3$ elements while still achieving each of the three goals: \\\\ {\\em (a)\ntensor sparsification}: for a tensor that has to be formed from arbitrary\nsamples, compute very few elements to get a good spectral approximation, and\nfor arbitrary orthogonal tensors {\\em (b) tensor completion:} recover an\nexactly low-rank tensor from a small number of samples via alternating least\nsquares, or {\\em (c) tensor factorization:} approximating factors of a low-rank\ntensor corrupted by noise. \\\\ Our sampling can be used along with existing\ntensor-based algorithms to speed them up, removing the computational bottleneck\nin these methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 20:23:13 GMT"}, {"version": "v2", "created": "Thu, 19 Feb 2015 21:05:53 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Bhojanapalli", "Srinadh", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1502.05056", "submitter": "Reshef Meir", "authors": "Reshef Meir and David Parkes", "title": "On Sex, Evolution, and the Multiplicative Weights Update Algorithm", "comments": "full version of a paper accepted to AAMAS-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a recent innovative theory by Chastain et al. on the role of sex\nin evolution [PNAS'14]. In short, the theory suggests that the evolutionary\nprocess of gene recombination implements the celebrated multiplicative weights\nupdates algorithm (MWUA). They prove that the population dynamics induced by\nsexual reproduction can be precisely modeled by genes that use MWUA as their\nlearning strategy in a particular coordination game. The result holds in the\nenvironments of \\emph{weak selection}, under the assumption that the population\nfrequencies remain a product distribution.\n  We revisit the theory, eliminating both the requirement of weak selection and\nany assumption on the distribution of the population. Removing the assumption\nof product distributions is crucial, since as we show, this assumption is\ninconsistent with the population dynamics. We show that the marginal allele\ndistributions induced by the population dynamics precisely match the marginals\ninduced by a multiplicative weights update algorithm in this general setting,\nthereby affirming and substantially generalizing these earlier results.\n  We further revise the implications for convergence and utility or fitness\nguarantees in coordination games. In contrast to the claim of Chastain et\nal.[PNAS'14], we conclude that the sexual evolutionary dynamics does not entail\nany property of the population distribution, beyond those already implied by\nconvergence.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 21:02:37 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Meir", "Reshef", ""], ["Parkes", "David", ""]]}, {"id": "1502.05090", "submitter": "Aldo Pacchiano", "authors": "Aldo Pacchiano, Oliver Williams", "title": "Real time clustering of time series using triangular potentials", "comments": "AIFU15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of computing investment portfolio weightings we\ninvestigate various methods of clustering as alternatives to traditional\nmean-variance approaches. Such methods can have significant benefits from a\npractical point of view since they remove the need to invert a sample\ncovariance matrix, which can suffer from estimation error and will almost\ncertainly be non-stationary. The general idea is to find groups of assets which\nshare similar return characteristics over time and treat each group as a single\ncomposite asset. We then apply inverse volatility weightings to these new\ncomposite assets. In the course of our investigation we devise a method of\nclustering based on triangular potentials and we present associated theoretical\nresults as well as various examples based on synthetic data.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 00:27:39 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Pacchiano", "Aldo", ""], ["Williams", "Oliver", ""]]}, {"id": "1502.05111", "submitter": "Fangfang Li", "authors": "Fangfang Li, Guandong Xu, Longbing Cao", "title": "CSAL: Self-adaptive Labeling based Clustering Integrating Supervised\n  Learning on Unlabeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised classification approaches can predict labels for unknown data\nbecause of the supervised training process. The success of classification is\nheavily dependent on the labeled training data. Differently, clustering is\neffective in revealing the aggregation property of unlabeled data, but the\nperformance of most clustering methods is limited by the absence of labeled\ndata. In real applications, however, it is time-consuming and sometimes\nimpossible to obtain labeled data. The combination of clustering and\nclassification is a promising and active approach which can largely improve the\nperformance. In this paper, we propose an innovative and effective clustering\nframework based on self-adaptive labeling (CSAL) which integrates clustering\nand classification on unlabeled data. Clustering is first employed to partition\ndata and a certain proportion of clustered data are selected by our proposed\nlabeling approach for training classifiers. In order to refine the trained\nclassifiers, an iterative process of Expectation-Maximization algorithm is\ndevised into the proposed clustering framework CSAL. Experiments are conducted\non publicly data sets to test different combinations of clustering algorithms\nand classification models as well as various training data labeling methods.\nThe experimental results show that our approach along with the self-adaptive\nmethod outperforms other methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 04:04:45 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Li", "Fangfang", ""], ["Xu", "Guandong", ""], ["Cao", "Longbing", ""]]}, {"id": "1502.05113", "submitter": "Jiajun Liu", "authors": "Jiajun Liu, Kun Zhao, Brano Kusy, Ji-rong Wen, Raja Jurdak", "title": "Temporal Embedding in Convolutional Neural Networks for Robust Learning\n  of Abstract Snippets", "comments": "a submission to kdd 15'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of periodical time-series remains challenging due to various\ntypes of data distortions and misalignments. Here, we propose a novel model\ncalled Temporal embedding-enhanced convolutional neural Network (TeNet) to\nlearn repeatedly-occurring-yet-hidden structural elements in periodical\ntime-series, called abstract snippets, for predicting future changes. Our model\nuses convolutional neural networks and embeds a time-series with its potential\nneighbors in the temporal domain for aligning it to the dominant patterns in\nthe dataset. The model is robust to distortions and misalignments in the\ntemporal domain and demonstrates strong prediction power for periodical\ntime-series.\n  We conduct extensive experiments and discover that the proposed model shows\nsignificant and consistent advantages over existing methods on a variety of\ndata modalities ranging from human mobility to household power consumption\nrecords. Empirical results indicate that the model is robust to various factors\nsuch as number of samples, variance of data, numerical ranges of data etc. The\nexperiments also verify that the intuition behind the model can be generalized\nto multiple data types and applications and promises significant improvement in\nprediction performances across the datasets studied.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 04:25:23 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Liu", "Jiajun", ""], ["Zhao", "Kun", ""], ["Kusy", "Brano", ""], ["Wen", "Ji-rong", ""], ["Jurdak", "Raja", ""]]}, {"id": "1502.05134", "submitter": "Jingbin Wang", "authors": "Jingbin Wang, Yihua Zhou, Kanghong Duan, Jim Jing-Yan Wang, Halima\n  Bensmail", "title": "Supervised cross-modal factor analysis for multiple modal data\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of learning from multiple modal data for\npurpose of document classification. In this problem, each document is composed\ntwo different modals of data, i.e., an image and a text. Cross-modal factor\nanalysis (CFA) has been proposed to project the two different modals of data to\na shared data space, so that the classification of a image or a text can be\nperformed directly in this space. A disadvantage of CFA is that it has ignored\nthe supervision information. In this paper, we improve CFA by incorporating the\nsupervision information to represent and classify both image and text modals of\ndocuments. We project both image and text data to a shared data space by factor\nanalysis, and then train a class label predictor in the shared space to use the\nclass label information. The factor analysis parameter and the predictor\nparameter are learned jointly by solving one single objective function. With\nthis objective function, we minimize the distance between the projections of\nimage and text of the same document, and the classification error of the\nprojection measured by hinge loss function. The objective function is optimized\nby an alternate optimization strategy in an iterative algorithm. Experiments in\ntwo different multiple modal document data sets show the advantage of the\nproposed algorithm over other CFA methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 06:55:07 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 05:20:59 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Wang", "Jingbin", ""], ["Zhou", "Yihua", ""], ["Duan", "Kanghong", ""], ["Wang", "Jim Jing-Yan", ""], ["Bensmail", "Halima", ""]]}, {"id": "1502.05167", "submitter": "Mansaf Alam Dr", "authors": "Kashish Ara Shakil, Shadma Anis and Mansaf Alam", "title": "Dengue disease prediction using weka data mining tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dengue is a life threatening disease prevalent in several developed as well\nas developing countries like India.In this paper we discuss various algorithm\napproaches of data mining that have been utilized for dengue disease\nprediction. Data mining is a well known technique used by health organizations\nfor classification of diseases such as dengue, diabetes and cancer in\nbioinformatics research. In the proposed approach we have used WEKA with 10\ncross validation to evaluate data and compare results. Weka has an extensive\ncollection of different machine learning and data mining algorithms. In this\npaper we have firstly classified the dengue data set and then compared the\ndifferent data mining techniques in weka through Explorer, knowledge flow and\nExperimenter interfaces. Furthermore in order to validate our approach we have\nused a dengue dataset with 108 instances but weka used 99 rows and 18\nattributes to determine the prediction of disease and their accuracy using\nclassifications of different algorithms to find out the best performance. The\nmain objective of this paper is to classify data and assist the users in\nextracting useful information from data and easily identify a suitable\nalgorithm for accurate predictive model from it. From the findings of this\npaper it can be concluded that Na\\\"ive Bayes and J48 are the best performance\nalgorithms for classified accuracy because they achieved maximum accuracy= 100%\nwith 99 correctly classified instances, maximum ROC = 1, had least mean\nabsolute error and it took minimum time for building this model through\nExplorer and Knowledge flow results\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 09:52:55 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Shakil", "Kashish Ara", ""], ["Anis", "Shadma", ""], ["Alam", "Mansaf", ""]]}, {"id": "1502.05213", "submitter": "Sankar Mukherjee", "authors": "Sankar Mukherjee, Shyamal Kumar Das Mandal", "title": "F0 Modeling In Hmm-Based Speech Synthesis System Using Deep Belief\n  Network", "comments": "OCOCOSDA 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In recent years multilayer perceptrons (MLPs) with many hid- den layers Deep\nNeural Network (DNN) has performed sur- prisingly well in many speech tasks,\ni.e. speech recognition, speaker verification, speech synthesis etc. Although\nin the context of F0 modeling these techniques has not been ex- ploited\nproperly. In this paper, Deep Belief Network (DBN), a class of DNN family has\nbeen employed and applied to model the F0 contour of synthesized speech which\nwas generated by HMM-based speech synthesis system. The experiment was done on\nBengali language. Several DBN-DNN architectures ranging from four to seven\nhidden layers and up to 200 hid- den units per hidden layer was presented and\nevaluated. The results were compared against clustering tree techniques pop-\nularly found in statistical parametric speech synthesis. We show that from\ntextual inputs DBN-DNN learns a high level structure which in turn improves F0\ncontour in terms of ob- jective and subjective tests.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 13:15:13 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Mukherjee", "Sankar", ""], ["Mandal", "Shyamal Kumar Das", ""]]}, {"id": "1502.05375", "submitter": "Arnab Bhattacharyya", "authors": "Arnab Bhattacharyya, Ameet Gadekar, Ninad Rajgopal", "title": "On learning k-parities with and without noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first consider the problem of learning $k$-parities in the on-line\nmistake-bound model: given a hidden vector $x \\in \\{0,1\\}^n$ with $|x|=k$ and a\nsequence of \"questions\" $a_1, a_2, ...\\in \\{0,1\\}^n$, where the algorithm must\nreply to each question with $< a_i, x> \\pmod 2$, what is the best tradeoff\nbetween the number of mistakes made by the algorithm and its time complexity?\nWe improve the previous best result of Buhrman et al. by an $\\exp(k)$ factor in\nthe time complexity.\n  Second, we consider the problem of learning $k$-parities in the presence of\nclassification noise of rate $\\eta \\in (0,1/2)$. A polynomial time algorithm\nfor this problem (when $\\eta > 0$ and $k = \\omega(1)$) is a longstanding\nchallenge in learning theory. Grigorescu et al. showed an algorithm running in\ntime ${n \\choose k/2}^{1 + 4\\eta^2 +o(1)}$. Note that this algorithm inherently\nrequires time ${n \\choose k/2}$ even when the noise rate $\\eta$ is polynomially\nsmall. We observe that for sufficiently small noise rate, it is possible to\nbreak the $n \\choose k/2$ barrier. In particular, if for some function $f(n) =\n\\omega(1)$ and $\\alpha \\in [1/2, 1)$, $k = n/f(n)$ and $\\eta = o(f(n)^{-\n\\alpha}/\\log n)$, then there is an algorithm for the problem with running time\n$poly(n)\\cdot {n \\choose k}^{1-\\alpha} \\cdot e^{-k/4.01}$.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 20:36:19 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Gadekar", "Ameet", ""], ["Rajgopal", "Ninad", ""]]}, {"id": "1502.05472", "submitter": "Fabrizio Sebastiani", "authors": "Diego Marcheggiani and Fabrizio Sebastiani", "title": "On the Effects of Low-Quality Training Data on Information Extraction\n  from Clinical Reports", "comments": "Submitted for publication", "journal-ref": null, "doi": "10.1145/3106235", "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In the last five years there has been a flurry of work on information\nextraction from clinical documents, i.e., on algorithms capable of extracting,\nfrom the informal and unstructured texts that are generated during everyday\nclinical practice, mentions of concepts relevant to such practice. Most of this\nliterature is about methods based on supervised learning, i.e., methods for\ntraining an information extraction system from manually annotated examples.\nWhile a lot of work has been devoted to devising learning methods that generate\nmore and more accurate information extractors, no work has been devoted to\ninvestigating the effect of the quality of training data on the learning\nprocess. Low quality in training data often derives from the fact that the\nperson who has annotated the data is different from the one against whose\njudgment the automatically annotated data must be evaluated. In this paper we\ntest the impact of such data quality issues on the accuracy of information\nextraction systems as applied to the clinical domain. We do this by comparing\nthe accuracy deriving from training data annotated by the authoritative coder\n(i.e., the one who has also annotated the test data, and by whose judgment we\nmust abide), with the accuracy deriving from training data annotated by a\ndifferent coder. The results indicate that, although the disagreement between\nthe two coders (as measured on the training set) is substantial, the difference\nis (surprisingly enough) not always statistically significant.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 06:04:40 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 08:08:49 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Marcheggiani", "Diego", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "1502.05477", "submitter": "John Schulman", "authors": "John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan,\n  Pieter Abbeel", "title": "Trust Region Policy Optimization", "comments": "16 pages, ICML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an iterative procedure for optimizing policies, with guaranteed\nmonotonic improvement. By making several approximations to the\ntheoretically-justified procedure, we develop a practical algorithm, called\nTrust Region Policy Optimization (TRPO). This algorithm is similar to natural\npolicy gradient methods and is effective for optimizing large nonlinear\npolicies such as neural networks. Our experiments demonstrate its robust\nperformance on a wide variety of tasks: learning simulated robotic swimming,\nhopping, and walking gaits; and playing Atari games using images of the screen\nas input. Despite its approximations that deviate from the theory, TRPO tends\nto give monotonic improvement, with little tuning of hyperparameters.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 06:44:25 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 14:56:50 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2015 10:47:03 GMT"}, {"version": "v4", "created": "Mon, 6 Jun 2016 01:00:57 GMT"}, {"version": "v5", "created": "Thu, 20 Apr 2017 18:04:12 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Schulman", "John", ""], ["Levine", "Sergey", ""], ["Moritz", "Philipp", ""], ["Jordan", "Michael I.", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1502.05491", "submitter": "Fabrizio Sebastiani", "authors": "Andrea Esuli and Fabrizio Sebastiani", "title": "Optimizing Text Quantifiers for Multivariate Loss Functions", "comments": "In press in ACM Transactions on Knowledge Discovery from Data, 2015", "journal-ref": null, "doi": "10.1145/2700406", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We address the problem of \\emph{quantification}, a supervised learning task\nwhose goal is, given a class, to estimate the relative frequency (or\n\\emph{prevalence}) of the class in a dataset of unlabelled items.\nQuantification has several applications in data and text mining, such as\nestimating the prevalence of positive reviews in a set of reviews of a given\nproduct, or estimating the prevalence of a given support issue in a dataset of\ntranscripts of phone calls to tech support. So far, quantification has been\naddressed by learning a general-purpose classifier, counting the unlabelled\nitems which have been assigned the class, and tuning the obtained counts\naccording to some heuristics. In this paper we depart from the tradition of\nusing general-purpose classifiers, and use instead a supervised learning model\nfor \\emph{structured prediction}, capable of generating classifiers directly\noptimized for the (multivariate and non-linear) function used for evaluating\nquantification accuracy. The experiments that we have run on 5500 binary\nhigh-dimensional datasets (averaging more than 14,000 documents each) show that\nthis method is more accurate, more stable, and more efficient than existing,\nstate-of-the-art quantification methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 08:06:54 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 14:45:59 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Esuli", "Andrea", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "1502.05534", "submitter": "Kalyan Nagaraj", "authors": "Kalyan Nagaraj and Amulyashree Sridhar", "title": "NeuroSVM: A Graphical User Interface for Identification of Liver\n  Patients", "comments": "9 pages, 6 figures", "journal-ref": "IJCSIT. 5(6): 8280-8284 (2014)", "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnosis of liver infection at preliminary stage is important for better\ntreatment. In todays scenario devices like sensors are used for detection of\ninfections. Accurate classification techniques are required for automatic\nidentification of disease samples. In this context, this study utilizes data\nmining approaches for classification of liver patients from healthy\nindividuals. Four algorithms (Naive Bayes, Bagging, Random forest and SVM) were\nimplemented for classification using R platform. Further to improve the\naccuracy of classification a hybrid NeuroSVM model was developed using SVM and\nfeed-forward artificial neural network (ANN). The hybrid model was tested for\nits performance using statistical parameters like root mean square error (RMSE)\nand mean absolute percentage error (MAPE). The model resulted in a prediction\naccuracy of 98.83%. The results suggested that development of hybrid model\nimproved the accuracy of prediction. To serve the medicinal community for\nprediction of liver disease among patients, a graphical user interface (GUI)\nhas been developed using R. The GUI is deployed as a package in local\nrepository of R platform for users to perform prediction.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 11:45:37 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Nagaraj", "Kalyan", ""], ["Sridhar", "Amulyashree", ""]]}, {"id": "1502.05556", "submitter": "Lucas Maystre", "authors": "Lucas Maystre, Matthias Grossglauser", "title": "Just Sort It! A Simple and Effective Approach to Active Preference\n  Learning", "comments": "Accepted at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of learning a ranking by using adaptively chosen\npairwise comparisons. Our goal is to recover the ranking accurately but to\nsample the comparisons sparingly. If all comparison outcomes are consistent\nwith the ranking, the optimal solution is to use an efficient sorting\nalgorithm, such as Quicksort. But how do sorting algorithms behave if some\ncomparison outcomes are inconsistent with the ranking? We give favorable\nguarantees for Quicksort for the popular Bradley-Terry model, under natural\nassumptions on the parameters. Furthermore, we empirically demonstrate that\nsorting algorithms lead to a very simple and effective active learning\nstrategy: repeatedly sort the items. This strategy performs as well as\nstate-of-the-art methods (and much better than random sampling) at a minuscule\nfraction of the computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 12:50:13 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 15:19:38 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Maystre", "Lucas", ""], ["Grossglauser", "Matthias", ""]]}, {"id": "1502.05577", "submitter": "L.A. Prashanth", "authors": "Prashanth L.A., Shalabh Bhatnagar, Michael Fu and Steve Marcus", "title": "Adaptive system optimization using random directions stochastic\n  approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present novel algorithms for simulation optimization using random\ndirections stochastic approximation (RDSA). These include first-order\n(gradient) as well as second-order (Newton) schemes. We incorporate both\ncontinuous-valued as well as discrete-valued perturbations into both our\nalgorithms. The former are chosen to be independent and identically distributed\n(i.i.d.) symmetric, uniformly distributed random variables (r.v.), while the\nlatter are i.i.d., asymmetric, Bernoulli r.v.s. Our Newton algorithm, with a\nnovel Hessian estimation scheme, requires N-dimensional perturbations and three\nloss measurements per iteration, whereas the simultaneous perturbation Newton\nsearch algorithm of [1] requires 2N-dimensional perturbations and four loss\nmeasurements per iteration. We prove the unbiasedness of both gradient and\nHessian estimates and asymptotic (strong) convergence for both first-order and\nsecond-order schemes. We also provide asymptotic normality results, which in\nparticular establish that the asymmetric Bernoulli variant of Newton RDSA\nmethod is better than 2SPSA of [1]. Numerical experiments are used to validate\nthe theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 14:11:21 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2015 21:04:32 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["A.", "Prashanth L.", ""], ["Bhatnagar", "Shalabh", ""], ["Fu", "Michael", ""], ["Marcus", "Steve", ""]]}, {"id": "1502.05675", "submitter": "Malik Magdon-Ismail", "authors": "Malik Magdon-Ismail", "title": "NP-Hardness and Inapproximability of Sparse PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a reduction from {\\sc clique} to establish that sparse PCA is\nNP-hard. The reduction has a gap which we use to exclude an FPTAS for sparse\nPCA (unless P=NP). Under weaker complexity assumptions, we also exclude\npolynomial constant-factor approximation algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 19:30:46 GMT"}, {"version": "v2", "created": "Fri, 20 Feb 2015 13:00:17 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Magdon-Ismail", "Malik", ""]]}, {"id": "1502.05696", "submitter": "Nihar Shah", "authors": "Nihar B. Shah, Dengyong Zhou, Yuval Peres", "title": "Approval Voting and Incentives in Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing need for labeled training data has made crowdsourcing an\nimportant part of machine learning. The quality of crowdsourced labels is,\nhowever, adversely affected by three factors: (1) the workers are not experts;\n(2) the incentives of the workers are not aligned with those of the requesters;\nand (3) the interface does not allow workers to convey their knowledge\naccurately, by forcing them to make a single choice among a set of options. In\nthis paper, we address these issues by introducing approval voting to utilize\nthe expertise of workers who have partial knowledge of the true answer, and\ncoupling it with a (\"strictly proper\") incentive-compatible compensation\nmechanism. We show rigorous theoretical guarantees of optimality of our\nmechanism together with a simple axiomatic characterization. We also conduct\npreliminary empirical studies on Amazon Mechanical Turk which validate our\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 20:42:55 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 09:12:50 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2015 05:21:06 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Shah", "Nihar B.", ""], ["Zhou", "Dengyong", ""], ["Peres", "Yuval", ""]]}, {"id": "1502.05744", "submitter": "Francesco Orabona", "authors": "Francesco Orabona and David Pal", "title": "Scale-Free Algorithms for Online Linear Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design algorithms for online linear optimization that have optimal regret\nand at the same time do not need to know any upper or lower bounds on the norm\nof the loss vectors. We achieve adaptiveness to norms of loss vectors by scale\ninvariance, i.e., our algorithms make exactly the same decisions if the\nsequence of loss vectors is multiplied by any positive constant. Our algorithms\nwork for any decision set, bounded or unbounded. For unbounded decisions sets,\nthese are the first truly adaptive algorithms for online linear optimization.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 23:05:04 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2015 20:56:34 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Orabona", "Francesco", ""], ["Pal", "David", ""]]}, {"id": "1502.05752", "submitter": "Zhiwu Lu", "authors": "Zhenyong Fu and Zhiwu Lu", "title": "Pairwise Constraint Propagation: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the most important types of (weaker) supervised information in\nmachine learning and pattern recognition, pairwise constraint, which specifies\nwhether a pair of data points occur together, has recently received significant\nattention, especially the problem of pairwise constraint propagation. At least\ntwo reasons account for this trend: the first is that compared to the data\nlabel, pairwise constraints are more general and easily to collect, and the\nsecond is that since the available pairwise constraints are usually limited,\nthe constraint propagation problem is thus important.\n  This paper provides an up-to-date critical survey of pairwise constraint\npropagation research. There are two underlying motivations for us to write this\nsurvey paper: the first is to provide an up-to-date review of the existing\nliterature, and the second is to offer some insights into the studies of\npairwise constraint propagation. To provide a comprehensive survey, we not only\ncategorize existing propagation techniques but also present detailed\ndescriptions of representative methods within each category.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 23:59:48 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Fu", "Zhenyong", ""], ["Lu", "Zhiwu", ""]]}, {"id": "1502.05767", "submitter": "Atilim Gunes Baydin", "authors": "Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul,\n  Jeffrey Mark Siskind", "title": "Automatic differentiation in machine learning: a survey", "comments": "43 pages, 5 figures", "journal-ref": "Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich\n  Radul, Jeffrey Mark Siskind. Automatic differentiation in machine learning: a\n  survey. The Journal of Machine Learning Research, 18(153):1--43, 2018", "doi": null, "report-no": null, "categories": "cs.SC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in\nmachine learning. Automatic differentiation (AD), also called algorithmic\ndifferentiation or simply \"autodiff\", is a family of techniques similar to but\nmore general than backpropagation for efficiently and accurately evaluating\nderivatives of numeric functions expressed as computer programs. AD is a small\nbut established field with applications in areas including computational fluid\ndynamics, atmospheric sciences, and engineering design optimization. Until very\nrecently, the fields of machine learning and AD have largely been unaware of\neach other and, in some cases, have independently discovered each other's\nresults. Despite its relevance, general-purpose AD has been missing from the\nmachine learning toolbox, a situation slowly changing with its ongoing adoption\nunder the names \"dynamic computational graphs\" and \"differentiable\nprogramming\". We survey the intersection of AD and machine learning, cover\napplications where AD has direct relevance, and address the main implementation\ntechniques. By precisely defining the main differentiation techniques and their\ninterrelationships, we aim to bring clarity to the usage of the terms\n\"autodiff\", \"automatic differentiation\", and \"symbolic differentiation\" as\nthese are encountered more and more in machine learning settings.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 04:20:47 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2015 16:49:13 GMT"}, {"version": "v3", "created": "Thu, 17 Aug 2017 16:45:07 GMT"}, {"version": "v4", "created": "Mon, 5 Feb 2018 15:57:57 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Baydin", "Atilim Gunes", ""], ["Pearlmutter", "Barak A.", ""], ["Radul", "Alexey Andreyevich", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1502.05774", "submitter": "Bo Waggoner", "authors": "Jacob Abernethy, Yiling Chen, Chien-Ju Ho, Bo Waggoner", "title": "Low-Cost Learning via Active Data Procurement", "comments": "Full version of EC 2015 paper. Color recommended for figures but\n  nonessential. 36 pages, of which 12 appendix", "journal-ref": null, "doi": "10.1145/2764468.2764519", "report-no": null, "categories": "cs.GT cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design mechanisms for online procurement of data held by strategic agents\nfor machine learning tasks. The challenge is to use past data to actively price\nfuture data and give learning guarantees even when an agent's cost for\nrevealing her data may depend arbitrarily on the data itself. We achieve this\ngoal by showing how to convert a large class of no-regret algorithms into\nonline posted-price and learning mechanisms. Our results in a sense parallel\nclassic sample complexity guarantees, but with the key resource being money\nrather than quantity of data: With a budget constraint $B$, we give robust risk\n(predictive error) bounds on the order of $1/\\sqrt{B}$. Because we use an\nactive approach, we can often guarantee to do significantly better by\nleveraging correlations between costs and data.\n  Our algorithms and analysis go through a model of no-regret learning with $T$\narriving pairs (cost, data) and a budget constraint of $B$. Our regret bounds\nfor this model are on the order of $T/\\sqrt{B}$ and we give lower bounds on the\nsame order.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 05:11:44 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2015 03:24:36 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Abernethy", "Jacob", ""], ["Chen", "Yiling", ""], ["Ho", "Chien-Ju", ""], ["Waggoner", "Bo", ""]]}, {"id": "1502.05777", "submitter": "James Henderson", "authors": "James A. Henderson, TingTing A. Gibson, Janet Wiles", "title": "Spike Event Based Learning in Neural Networks", "comments": "Figure 4 can be viewed as a movie in a separate file", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A scheme is derived for learning connectivity in spiking neural networks. The\nscheme learns instantaneous firing rates that are conditional on the activity\nin other parts of the network. The scheme is independent of the choice of\nneuron dynamics or activation function, and network architecture. It involves\ntwo simple, online, local learning rules that are applied only in response to\noccurrences of spike events. This scheme provides a direct method for\ntransferring ideas between the fields of deep learning and computational\nneuroscience. This learning scheme is demonstrated using a layered feedforward\nspiking neural network trained self-supervised on a prediction and\nclassification task for moving MNIST images collected using a Dynamic Vision\nSensor.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 05:26:09 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Henderson", "James A.", ""], ["Gibson", "TingTing A.", ""], ["Wiles", "Janet", ""]]}, {"id": "1502.05832", "submitter": "Pierre Baque", "authors": "Pierre Baqu\\'e, Jean-Hubert Hours, Fran\\c{c}ois Fleuret, Pascal Fua", "title": "A provably convergent alternating minimization method for mean field\n  inference", "comments": "Submitted to Colt 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean-Field is an efficient way to approximate a posterior distribution in\ncomplex graphical models and constitutes the most popular class of Bayesian\nvariational approximation methods. In most applications, the mean field\ndistribution parameters are computed using an alternate coordinate\nminimization. However, the convergence properties of this algorithm remain\nunclear. In this paper, we show how, by adding an appropriate penalization\nterm, we can guarantee convergence to a critical point, while keeping a closed\nform update at each step. A convergence rate estimate can also be derived based\non recent results in non-convex optimization.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 11:23:58 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Baqu\u00e9", "Pierre", ""], ["Hours", "Jean-Hubert", ""], ["Fleuret", "Fran\u00e7ois", ""], ["Fua", "Pascal", ""]]}, {"id": "1502.05886", "submitter": "Emilio Ferrara", "authors": "Lei Le, Emilio Ferrara, Alessandro Flammini", "title": "On predictability of rare events leveraging social media: a machine\n  learning perspective", "comments": "10 pages, 10 tables, 8 figures", "journal-ref": "Proceedings of the 2015 ACM on Conference on Online Social\n  Networks (pp. 3-13). ACM. 2015", "doi": "10.1145/2817946.2817949", "report-no": null, "categories": "cs.SI cs.LG physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information extracted from social media streams has been leveraged to\nforecast the outcome of a large number of real-world events, from political\nelections to stock market fluctuations. An increasing amount of studies\ndemonstrates how the analysis of social media conversations provides cheap\naccess to the wisdom of the crowd. However, extents and contexts in which such\nforecasting power can be effectively leveraged are still unverified at least in\na systematic way. It is also unclear how social-media-based predictions compare\nto those based on alternative information sources. To address these issues,\nhere we develop a machine learning framework that leverages social media\nstreams to automatically identify and predict the outcomes of soccer matches.\nWe focus in particular on matches in which at least one of the possible\noutcomes is deemed as highly unlikely by professional bookmakers. We argue that\nsport events offer a systematic approach for testing the predictive power of\nsocial media, and allow to compare such power against the rigorous baselines\nset by external sources. Despite such strict baselines, our framework yields\nabove 8% marginal profit when used to inform simple betting strategies. The\nsystem is based on real-time sentiment analysis and exploits data collected\nimmediately before the games, allowing for informed bets. We discuss the\nrationale behind our approach, describe the learning framework, its prediction\nperformance and the return it provides as compared to a set of betting\nstrategies. To test our framework we use both historical Twitter data from the\n2014 FIFA World Cup games, and real-time Twitter data collected by monitoring\nthe conversations about all soccer matches of four major European tournaments\n(FA Premier League, Serie A, La Liga, and Bundesliga), and the 2014 UEFA\nChampions League, during the period between Oct. 25th 2014 and Nov. 26th 2014.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 14:42:26 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Le", "Lei", ""], ["Ferrara", "Emilio", ""], ["Flammini", "Alessandro", ""]]}, {"id": "1502.05890", "submitter": "Akshay Krishnamurthy", "authors": "Akshay Krishnamurthy, Alekh Agarwal, Miroslav Dudik", "title": "Contextual Semibandits via Supervised Learning Oracles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an online decision making problem where on each round a learner\nchooses a list of items based on some side information, receives a scalar\nfeedback value for each individual item, and a reward that is linearly related\nto this feedback. These problems, known as contextual semibandits, arise in\ncrowdsourcing, recommendation, and many other domains. This paper reduces\ncontextual semibandits to supervised learning, allowing us to leverage powerful\nsupervised learning methods in this partial-feedback setting. Our first\nreduction applies when the mapping from feedback to reward is known and leads\nto a computationally efficient algorithm with near-optimal regret. We show that\nthis algorithm outperforms state-of-the-art approaches on real-world\nlearning-to-rank datasets, demonstrating the advantage of oracle-based\nalgorithms. Our second reduction applies to the previously unstudied setting\nwhen the linear mapping from feedback to reward is unknown. Our regret\nguarantees are superior to prior techniques that ignore the feedback.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 14:55:41 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 01:38:23 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2016 00:43:13 GMT"}, {"version": "v4", "created": "Fri, 4 Nov 2016 19:28:07 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Krishnamurthy", "Akshay", ""], ["Agarwal", "Alekh", ""], ["Dudik", "Miroslav", ""]]}, {"id": "1502.05911", "submitter": "Uwe Aickelin", "authors": "Alexandros Ladas, Eamonn Ferguson, Uwe Aickelin and Jon Garibaldi", "title": "A Data Mining framework to model Consumer Indebtedness with\n  Psychological Factors", "comments": "IEEE International Conference of Data Mining: The Seventh\n  International Workshop on Domain Driven Data Mining 2014 (DDDM 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling Consumer Indebtedness has proven to be a problem of complex nature.\nIn this work we utilise Data Mining techniques and methods to explore the\nmultifaceted aspect of Consumer Indebtedness by examining the contribution of\nPsychological Factors, like Impulsivity to the analysis of Consumer Debt. Our\nresults confirm the beneficial impact of Psychological Factors in modelling\nConsumer Indebtedness and suggest a new approach in analysing Consumer Debt,\nthat would take into consideration more Psychological characteristics of\nconsumers and adopt techniques and practices from Data Mining.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 15:47:49 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Ladas", "Alexandros", ""], ["Ferguson", "Eamonn", ""], ["Aickelin", "Uwe", ""], ["Garibaldi", "Jon", ""]]}, {"id": "1502.05925", "submitter": "Feng Nan", "authors": "Feng Nan, Joseph Wang, Venkatesh Saligrama", "title": "Feature-Budgeted Random Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek decision rules for prediction-time cost reduction, where complete\ndata is available for training, but during prediction-time, each feature can\nonly be acquired for an additional cost. We propose a novel random forest\nalgorithm to minimize prediction error for a user-specified {\\it average}\nfeature acquisition budget. While random forests yield strong generalization\nperformance, they do not explicitly account for feature costs and furthermore\nrequire low correlation among trees, which amplifies costs. Our random forest\ngrows trees with low acquisition cost and high strength based on greedy minimax\ncost-weighted-impurity splits. Theoretically, we establish near-optimal\nacquisition cost guarantees for our algorithm. Empirically, on a number of\nbenchmark datasets we demonstrate superior accuracy-cost curves against\nstate-of-the-art prediction-time algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 16:42:40 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Nan", "Feng", ""], ["Wang", "Joseph", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1502.05934", "submitter": "Haipeng Luo", "authors": "Haipeng Luo and Robert E. Schapire", "title": "Achieving All with No Parameters: Adaptive NormalHedge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classic online learning problem of predicting with expert\nadvice, and propose a truly parameter-free and adaptive algorithm that achieves\nseveral objectives simultaneously without using any prior information. The main\ncomponent of this work is an improved version of the NormalHedge.DT algorithm\n(Luo and Schapire, 2014), called AdaNormalHedge. On one hand, this new\nalgorithm ensures small regret when the competitor has small loss and almost\nconstant regret when the losses are stochastic. On the other hand, the\nalgorithm is able to compete with any convex combination of the experts\nsimultaneously, with a regret in terms of the relative entropy of the prior and\nthe competitor. This resolves an open problem proposed by Chaudhuri et al.\n(2009) and Chernov and Vovk (2010). Moreover, we extend the results to the\nsleeping expert setting and provide two applications to illustrate the power of\nAdaNormalHedge: 1) competing with time-varying unknown competitors and 2)\npredicting almost as well as the best pruning tree. Our results on these\napplications significantly improve previous work from different aspects, and a\nspecial case of the first application resolves another open problem proposed by\nWarmuth and Koolen (2014) on whether one can simultaneously achieve optimal\nshifting regret for both adversarial and stochastic losses.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 16:58:36 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Luo", "Haipeng", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1502.05943", "submitter": "Uwe Aickelin", "authors": "Jenna M. Reps, Uwe Aickelin, Jiangang Ma, Yanchun Zhang", "title": "Refining Adverse Drug Reactions using Association Rule Mining for\n  Electronic Healthcare Data", "comments": "IEEE International Conference of Data Mining: Data Mining in\n  Biomedical Informatics and Healthcare (DMBIH) Workshop 2014, 2014", "journal-ref": null, "doi": "10.1109/ICDMW.2014.53", "report-no": null, "categories": "cs.DB cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Side effects of prescribed medications are a common occurrence. Electronic\nhealthcare databases present the opportunity to identify new side effects\nefficiently but currently the methods are limited due to confounding (i.e. when\nan association between two variables is identified due to them both being\nassociated to a third variable).\n  In this paper we propose a proof of concept method that learns common\nassociations and uses this knowledge to automatically refine side effect\nsignals (i.e. exposure-outcome associations) by removing instances of the\nexposure-outcome associations that are caused by confounding. This leaves the\nsignal instances that are most likely to correspond to true side effect\noccurrences. We then calculate a novel measure termed the confounding-adjusted\nrisk value, a more accurate absolute risk value of a patient experiencing the\noutcome within 60 days of the exposure.\n  Tentative results suggest that the method works. For the four signals (i.e.\nexposure-outcome associations) investigated we are able to correctly filter the\nmajority of exposure-outcome instances that were unlikely to correspond to true\nside effects. The method is likely to improve when tuning the association rule\nmining parameters for specific health outcomes.\n  This paper shows that it may be possible to filter signals at a patient level\nbased on association rules learned from considering patients' medical\nhistories. However, additional work is required to develop a way to automate\nthe tuning of the method's parameters.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 17:14:17 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Reps", "Jenna M.", ""], ["Aickelin", "Uwe", ""], ["Ma", "Jiangang", ""], ["Zhang", "Yanchun", ""]]}, {"id": "1502.05988", "submitter": "Jesse Read", "authors": "Jesse Read, Fernando Perez-Cruz", "title": "Deep Learning for Multi-label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-label classification, the main focus has been to develop ways of\nlearning the underlying dependencies between labels, and to take advantage of\nthis at classification time. Developing better feature-space representations\nhas been predominantly employed to reduce complexity, e.g., by eliminating\nnon-helpful feature attributes from the input space prior to (or during)\ntraining. This is an important task, since many multi-label methods typically\ncreate many different copies or views of the same input data as they transform\nit, and considerable memory can be saved by taking advantage of redundancy. In\nthis paper, we show that a proper development of the feature space can make\nlabels less interdependent and easier to model and predict at inference time.\nFor this task we use a deep learning approach with restricted Boltzmann\nmachines. We present a deep network that, in an empirical evaluation,\noutperforms a number of competitive methods from the literature\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 12:06:47 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Read", "Jesse", ""], ["Perez-Cruz", "Fernando", ""]]}, {"id": "1502.06064", "submitter": "Ken Miura", "authors": "Ken Miura, Tetsuaki Mano, Atsushi Kanehira, Yuichiro Tsuchiya and\n  Tatsuya Harada", "title": "MILJS : Brand New JavaScript Libraries for Matrix Calculation and\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MILJS is a collection of state-of-the-art, platform-independent, scalable,\nfast JavaScript libraries for matrix calculation and machine learning. Our core\nlibrary offering a matrix calculation is called Sushi, which exhibits far\nbetter performance than any other leading machine learning libraries written in\nJavaScript. Especially, our matrix multiplication is 177 times faster than the\nfastest JavaScript benchmark. Based on Sushi, a machine learning library called\nTempura is provided, which supports various algorithms widely used in machine\nlearning research. We also provide Soba as a visualization library. The\nimplementations of our libraries are clearly written, properly documented and\nthus can are easy to get started with, as long as there is a web browser. These\nlibraries are available from http://mil-tokyo.github.io/ under the MIT license.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 04:29:41 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Miura", "Ken", ""], ["Mano", "Tetsuaki", ""], ["Kanehira", "Atsushi", ""], ["Tsuchiya", "Yuichiro", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1502.06105", "submitter": "Taehoon Lee", "authors": "Taehoon Lee, Taesup Moon, Seung Jean Kim, Sungroh Yoon", "title": "Regularization and Kernelization of the Maximin Correlation Approach", "comments": "Submitted to IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2016.2551727", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust classification becomes challenging when each class consists of\nmultiple subclasses. Examples include multi-font optical character recognition\nand automated protein function prediction. In correlation-based\nnearest-neighbor classification, the maximin correlation approach (MCA)\nprovides the worst-case optimal solution by minimizing the maximum\nmisclassification risk through an iterative procedure. Despite the optimality,\nthe original MCA has drawbacks that have limited its wide applicability in\npractice. That is, the MCA tends to be sensitive to outliers, cannot\neffectively handle nonlinearities in datasets, and suffers from having high\ncomputational complexity. To address these limitations, we propose an improved\nsolution, named regularized maximin correlation approach (R-MCA). We first\nreformulate MCA as a quadratically constrained linear programming (QCLP)\nproblem, incorporate regularization by introducing slack variables in the\nprimal problem of the QCLP, and derive the corresponding Lagrangian dual. The\ndual formulation enables us to apply the kernel trick to R-MCA so that it can\nbetter handle nonlinearities. Our experimental results demonstrate that the\nregularization and kernelization make the proposed R-MCA more robust and\naccurate for various classification tasks than the original MCA. Furthermore,\nwhen the data size or dimensionality grows, R-MCA runs substantially faster by\nsolving either the primal or dual (whichever has a smaller variable dimension)\nof the QCLP.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 14:37:44 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 04:42:12 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Lee", "Taehoon", ""], ["Moon", "Taesup", ""], ["Kim", "Seung Jean", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1502.06132", "submitter": "Dan Guralnik", "authors": "Dan P. Guralnik and Daniel E. Koditschek", "title": "Universal Memory Architectures for Autonomous Machines", "comments": "Technical report, 31 pages, 1 table, 14 figures, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-organizing memory architecture for perceptual experience,\ncapable of supporting autonomous learning and goal-directed problem solving in\nthe absence of any prior information about the agent's environment. The\narchitecture is simple enough to ensure (1) a quadratic bound (in the number of\navailable sensors) on space requirements, and (2) a quadratic bound on the\ntime-complexity of the update-execute cycle. At the same time, it is\nsufficiently complex to provide the agent with an internal representation which\nis (3) minimal among all representations of its class which account for every\nsensory equivalence class subject to the agent's belief state; (4) capable, in\nprinciple, of recovering the homotopy type of the system's state space; (5)\nlearnable with arbitrary precision through a random application of the\navailable actions. The provable properties of an effectively trained memory\nstructure exploit a duality between weak poc sets -- a symbolic (discrete)\nrepresentation of subset nesting relations -- and non-positively curved cubical\ncomplexes, whose rich convexity theory underlies the planning cycle of the\nproposed architecture.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 19:11:23 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Guralnik", "Dan P.", ""], ["Koditschek", "Daniel E.", ""]]}, {"id": "1502.06134", "submitter": "Tengyuan Liang", "authors": "Tengyuan Liang, Alexander Rakhlin, Karthik Sridharan", "title": "Learning with Square Loss: Localization through Offset Rademacher\n  Complexity", "comments": "21 pages, 1 figure", "journal-ref": "Proceedings of the 28th Conference on Learning Theory 40 (2015)\n  1260-1285", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider regression with square loss and general classes of functions\nwithout the boundedness assumption. We introduce a notion of offset Rademacher\ncomplexity that provides a transparent way to study localization both in\nexpectation and in high probability. For any (possibly non-convex) class, the\nexcess loss of a two-step estimator is shown to be upper bounded by this offset\ncomplexity through a novel geometric inequality. In the convex case, the\nestimator reduces to an empirical risk minimizer. The method recovers the\nresults of \\citep{RakSriTsy15} for the bounded case while also providing\nguarantees without the boundedness assumption.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 19:20:44 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 16:10:05 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2015 15:20:08 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Liang", "Tengyuan", ""], ["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1502.06144", "submitter": "Quentin Berthet", "authors": "Quentin Berthet and Jordan S. Ellenberg", "title": "Detection of Planted Solutions for Flat Satisfiability Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the detection problem of finding planted solutions in random\ninstances of flat satisfiability problems, a generalization of boolean\nsatisfiability formulas. We describe the properties of random instances of flat\nsatisfiability, as well of the optimal rates of detection of the associated\nhypothesis testing problem. We also study the performance of an algorithmically\nefficient testing procedure. We introduce a modification of our model, the\nlight planting of solutions, and show that it is as hard as the problem of\nlearning parity with noise. This hints strongly at the difficulty of detecting\nplanted flat satisfiability for a wide class of tests.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 22:14:04 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 15:07:54 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Berthet", "Quentin", ""], ["Ellenberg", "Jordan S.", ""]]}, {"id": "1502.06161", "submitter": "Thiago Marzag\\~ao", "authors": "Thiago Marzag\\~ao", "title": "Using NLP to measure democracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses natural language processing to create the first machine-coded\ndemocracy index, which I call Automated Democracy Scores (ADS). The ADS are\nbased on 42 million news articles from 6,043 different sources and cover all\nindependent countries in the 1993-2012 period. Unlike the democracy indices we\nhave today the ADS are replicable and have standard errors small enough to\nactually distinguish between cases.\n  The ADS are produced with supervised learning. Three approaches are tried: a)\na combination of Latent Semantic Analysis and tree-based regression methods; b)\na combination of Latent Dirichlet Allocation and tree-based regression methods;\nand c) the Wordscores algorithm. The Wordscores algorithm outperforms the\nalternatives, so it is the one on which the ADS are based.\n  There is a web application where anyone can change the training set and see\nhow the results change: democracy-scores.org\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 01:30:32 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Marzag\u00e3o", "Thiago", ""]]}, {"id": "1502.06177", "submitter": "Shai Shalev-Shwartz", "authors": "Shai Shalev-Shwartz", "title": "SDCA without Duality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Dual Coordinate Ascent is a popular method for solving regularized\nloss minimization for the case of convex losses. In this paper we show how a\nvariant of SDCA can be applied for non-convex losses. We prove linear\nconvergence rate even if individual loss functions are non-convex as long as\nthe expected loss is convex.\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 04:42:01 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Shalev-Shwartz", "Shai", ""]]}, {"id": "1502.06187", "submitter": "Shay Moran", "authors": "Shay Moran, Amir Shpilka, Avi Wigderson, and Amir Yehudayoff", "title": "Teaching and compressing for low VC-dimension", "comments": "The final version is due to be published in the collection of papers\n  \"A Journey through Discrete Mathematics. A Tribute to Jiri Matousek\" edited\n  by Martin Loebl, Jaroslav Nesetril and Robin Thomas, due to be published by\n  Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the quantitative relation between VC-dimension and two\nother basic parameters related to learning and teaching. Namely, the quality of\nsample compression schemes and of teaching sets for classes of low\nVC-dimension. Let $C$ be a binary concept class of size $m$ and VC-dimension\n$d$. Prior to this work, the best known upper bounds for both parameters were\n$\\log(m)$, while the best lower bounds are linear in $d$. We present\nsignificantly better upper bounds on both as follows. Set $k = O(d 2^d \\log\n\\log |C|)$.\n  We show that there always exists a concept $c$ in $C$ with a teaching set\n(i.e. a list of $c$-labeled examples uniquely identifying $c$ in $C$) of size\n$k$. This problem was studied by Kuhlmann (1999). Our construction implies that\nthe recursive teaching (RT) dimension of $C$ is at most $k$ as well. The\nRT-dimension was suggested by Zilles et al. and Doliwa et al. (2010). The same\nnotion (under the name partial-ID width) was independently studied by Wigderson\nand Yehudayoff (2013). An upper bound on this parameter that depends only on\n$d$ is known just for the very simple case $d=1$, and is open even for $d=2$.\nWe also make small progress towards this seemingly modest goal.\n  We further construct sample compression schemes of size $k$ for $C$, with\nadditional information of $k \\log(k)$ bits. Roughly speaking, given any list of\n$C$-labelled examples of arbitrary length, we can retain only $k$ labeled\nexamples in a way that allows to recover the labels of all others examples in\nthe list, using additional $k\\log (k)$ information bits. This problem was first\nsuggested by Littlestone and Warmuth (1986).\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 06:21:28 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 01:46:11 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Moran", "Shay", ""], ["Shpilka", "Amir", ""], ["Wigderson", "Avi", ""], ["Yehudayoff", "Amir", ""]]}, {"id": "1502.06189", "submitter": "Hamed Firouzi", "authors": "Hamed Firouzi, Alfred Hero, Bala Rajaratnam", "title": "Two-stage Sampling, Prediction and Adaptive Regression via Correlation\n  Screening (SPARCS)", "comments": "To appear in IEEE Transactions on Information Theory. 40 Pages. arXiv\n  admin note: text overlap with arXiv:1303.2378", "journal-ref": null, "doi": "10.1109/TIT.2016.2621111", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a general adaptive procedure for budget-limited predictor\ndesign in high dimensions called two-stage Sampling, Prediction and Adaptive\nRegression via Correlation Screening (SPARCS). SPARCS can be applied to high\ndimensional prediction problems in experimental science, medicine, finance, and\nengineering, as illustrated by the following. Suppose one wishes to run a\nsequence of experiments to learn a sparse multivariate predictor of a dependent\nvariable $Y$ (disease prognosis for instance) based on a $p$ dimensional set of\nindependent variables $\\mathbf X=[X_1,\\ldots, X_p]^T$ (assayed biomarkers).\nAssume that the cost of acquiring the full set of variables $\\mathbf X$\nincreases linearly in its dimension. SPARCS breaks the data collection into two\nstages in order to achieve an optimal tradeoff between sampling cost and\npredictor performance. In the first stage we collect a few ($n$) expensive\nsamples $\\{y_i,\\mathbf x_i\\}_{i=1}^n$, at the full dimension $p\\gg n$ of\n$\\mathbf X$, winnowing the number of variables down to a smaller dimension $l <\np$ using a type of cross-correlation or regression coefficient screening. In\nthe second stage we collect a larger number $(t-n)$ of cheaper samples of the\n$l$ variables that passed the screening of the first stage. At the second\nstage, a low dimensional predictor is constructed by solving the standard\nregression problem using all $t$ samples of the selected variables. SPARCS is\nan adaptive online algorithm that implements false positive control on the\nselected variables, is well suited to small sample sizes, and is scalable to\nhigh dimensions. We establish asymptotic bounds for the Familywise Error Rate\n(FWER), specify high dimensional convergence rates for support recovery, and\nestablish optimal sample allocation rules to the first and second stages.\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 06:44:18 GMT"}, {"version": "v2", "created": "Sun, 2 Oct 2016 01:09:34 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Firouzi", "Hamed", ""], ["Hero", "Alfred", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1502.06197", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Andrea Montanari", "title": "On Online Control of False Discovery Rate", "comments": "31 pages, 6 figures (minor edits)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple hypotheses testing is a core problem in statistical inference and\narises in almost every scientific field. Given a sequence of null hypotheses\n$\\mathcal{H}(n) = (H_1,..., H_n)$, Benjamini and Hochberg\n\\cite{benjamini1995controlling} introduced the false discovery rate (FDR)\ncriterion, which is the expected proportion of false positives among rejected\nnull hypotheses, and proposed a testing procedure that controls FDR below a\npre-assigned significance level. They also proposed a different criterion,\ncalled mFDR, which does not control a property of the realized set of tests;\nrather it controls the ratio of expected number of false discoveries to the\nexpected number of discoveries.\n  In this paper, we propose two procedures for multiple hypotheses testing that\nwe will call \"LOND\" and \"LORD\". These procedures control FDR and mFDR in an\n\\emph{online manner}. Concretely, we consider an ordered --possibly infinite--\nsequence of null hypotheses $\\mathcal{H} = (H_1,H_2,H_3,...)$ where, at each\nstep $i$, the statistician must decide whether to reject hypothesis $H_i$\nhaving access only to the previous decisions. To the best of our knowledge, our\nwork is the first that controls FDR in this setting. This model was introduced\nby Foster and Stine \\cite{alpha-investing} whose alpha-investing rule only\ncontrols mFDR in online manner.\n  In order to compare different procedures, we develop lower bounds on the\ntotal discovery rate under the mixture model and prove that both LOND and LORD\nhave nearly linear number of discoveries. We further propose adjustment to LOND\nto address arbitrary correlation among the $p$-values. Finally, we evaluate the\nperformance of our procedures on both synthetic and real data comparing them\nwith alpha-investing rule, Benjamin-Hochberg method and a Bonferroni procedure.\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 09:07:07 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 00:39:16 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Javanmard", "Adel", ""], ["Montanari", "Andrea", ""]]}, {"id": "1502.06208", "submitter": "Aryeh Kontorovich", "authors": "Lee-Ad Gottlieb and Aryeh Kontorovich", "title": "Nearly optimal classification for semimetrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the rigorous study of classification in semimetric spaces, which\nare point sets with a distance function that is non-negative and symmetric, but\nneed not satisfy the triangle inequality. For metric spaces, the doubling\ndimension essentially characterizes both the runtime and sample complexity of\nclassification algorithms --- yet we show that this is not the case for\nsemimetrics. Instead, we define the {\\em density dimension} and discover that\nit plays a central role in the statistical and algorithmic feasibility of\nlearning in semimetric spaces. We present nearly optimal sample compression\nalgorithms and use these to obtain generalization guarantees, including fast\nrates. The latter hold for general sample compression schemes and may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 10:42:52 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Gottlieb", "Lee-Ad", ""], ["Kontorovich", "Aryeh", ""]]}, {"id": "1502.06254", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk", "title": "The fundamental nature of the log loss function", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard loss functions used in the literature on probabilistic\nprediction are the log loss function, the Brier loss function, and the\nspherical loss function; however, any computable proper loss function can be\nused for comparison of prediction algorithms. This note shows that the log loss\nfunction is most selective in that any prediction algorithm that is optimal for\na given data sequence (in the sense of the algorithmic theory of randomness)\nunder the log loss function will be optimal under any computable proper mixable\nloss function; on the other hand, there is a data sequence and a prediction\nalgorithm that is optimal for that sequence under either of the two other\nstandard loss functions but not under the log loss function.\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 17:58:05 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2015 15:00:20 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Vovk", "Vladimir", ""]]}, {"id": "1502.06256", "submitter": "Gregory Kucherov", "authors": "Karel Brinda and Maciej Sykulski and Gregory Kucherov", "title": "Spaced seeds improve k-mer-based metagenomic classification", "comments": "23 pages", "journal-ref": "Bioinformatics (2015) 31 (22): 3584-3592", "doi": "10.1093/bioinformatics/btv419", "report-no": null, "categories": "q-bio.GN cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metagenomics is a powerful approach to study genetic content of environmental\nsamples that has been strongly promoted by NGS technologies. To cope with\nmassive data involved in modern metagenomic projects, recent tools [4, 39] rely\non the analysis of k-mers shared between the read to be classified and sampled\nreference genomes. Within this general framework, we show in this work that\nspaced seeds provide a significant improvement of classification accuracy as\nopposed to traditional contiguous k-mers. We support this thesis through a\nseries a different computational experiments, including simulations of\nlarge-scale metagenomic projects. Scripts and programs used in this study, as\nwell as supplementary material, are available from\nhttp://github.com/gregorykucherov/spaced-seeds-for-metagenomics.\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 18:30:58 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 18:25:54 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2015 09:47:00 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Brinda", "Karel", ""], ["Sykulski", "Maciej", ""], ["Kucherov", "Gregory", ""]]}, {"id": "1502.06309", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang, Jing Lei, Stephen E. Fienberg", "title": "Learning with Differential Privacy: Stability, Learnability and the\n  Sufficiency and Necessity of ERM Principle", "comments": "to appear, Journal of Machine Learning Research, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While machine learning has proven to be a powerful data-driven solution to\nmany real-life problems, its use in sensitive domains has been limited due to\nprivacy concerns. A popular approach known as **differential privacy** offers\nprovable privacy guarantees, but it is often observed in practice that it could\nsubstantially hamper learning accuracy. In this paper we study the learnability\n(whether a problem can be learned by any algorithm) under Vapnik's general\nlearning setting with differential privacy constraint, and reveal some\nintricate relationships between privacy, stability and learnability.\n  In particular, we show that a problem is privately learnable **if an only\nif** there is a private algorithm that asymptotically minimizes the empirical\nrisk (AERM). In contrast, for non-private learning AERM alone is not sufficient\nfor learnability. This result suggests that when searching for private learning\nalgorithms, we can restrict the search to algorithms that are AERM. In light of\nthis, we propose a conceptual procedure that always finds a universally\nconsistent algorithm whenever the problem is learnable under privacy\nconstraint. We also propose a generic and practical algorithm and show that\nunder very general conditions it privately learns a wide class of learning\nproblems. Lastly, we extend some of the results to the more practical\n$(\\epsilon,\\delta)$-differential privacy and establish the existence of a\nphase-transition on the class of problems that are approximately privately\nlearnable with respect to how small $\\delta$ needs to be.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 03:52:08 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2015 13:02:20 GMT"}, {"version": "v3", "created": "Wed, 27 Apr 2016 19:55:21 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Wang", "Yu-Xiang", ""], ["Lei", "Jing", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1502.06354", "submitter": "Gergely Neu", "authors": "Gergely Neu", "title": "First-order regret bounds for combinatorial semi-bandits", "comments": "To appear at COLT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online combinatorial optimization under\nsemi-bandit feedback, where a learner has to repeatedly pick actions from a\ncombinatorial decision set in order to minimize the total losses associated\nwith its decisions. After making each decision, the learner observes the losses\nassociated with its action, but not other losses. For this problem, there are\nseveral learning algorithms that guarantee that the learner's expected regret\ngrows as $\\widetilde{O}(\\sqrt{T})$ with the number of rounds $T$. In this\npaper, we propose an algorithm that improves this scaling to\n$\\widetilde{O}(\\sqrt{{L_T^*}})$, where $L_T^*$ is the total loss of the best\naction. Our algorithm is among the first to achieve such guarantees in a\npartial-feedback scheme, and the first one to do so in a combinatorial setting.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 09:12:26 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 11:51:51 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Neu", "Gergely", ""]]}, {"id": "1502.06362", "submitter": "Masrour Zoghi", "authors": "Miroslav Dud\\'ik and Katja Hofmann and Robert E. Schapire and\n  Aleksandrs Slivkins and Masrour Zoghi", "title": "Contextual Dueling Bandits", "comments": "25 pages, 4 figures, Published at COLT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning to choose actions using contextual\ninformation when provided with limited feedback in the form of relative\npairwise comparisons. We study this problem in the dueling-bandits framework of\nYue et al. (2009), which we extend to incorporate context. Roughly, the\nlearner's goal is to find the best policy, or way of behaving, in some space of\npolicies, although \"best\" is not always so clearly defined. Here, we propose a\nnew and natural solution concept, rooted in game theory, called a von Neumann\nwinner, a randomized policy that beats or ties every other policy. We show that\nthis notion overcomes important limitations of existing solutions, particularly\nthe Condorcet winner which has typically been used in the past, but which\nrequires strong and often unrealistic assumptions. We then present three\nefficient algorithms for online learning in our setting, and for approximating\na von Neumann winner from batch-like data. The first of these algorithms\nachieves particularly low regret, even when data is adversarial, although its\ntime and space requirements are linear in the size of the policy space. The\nother two algorithms require time and space only logarithmic in the size of the\npolicy space when provided access to an oracle for solving classification\nproblems on the space.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 09:47:54 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2015 21:56:25 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Dud\u00edk", "Miroslav", ""], ["Hofmann", "Katja", ""], ["Schapire", "Robert E.", ""], ["Slivkins", "Aleksandrs", ""], ["Zoghi", "Masrour", ""]]}, {"id": "1502.06398", "submitter": "Tomer Koren", "authors": "S\\'ebastien Bubeck, Ofer Dekel, Tomer Koren, Yuval Peres", "title": "Bandit Convex Optimization: sqrt{T} Regret in One Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the minimax regret of the adversarial bandit convex optimization\nproblem. Focusing on the one-dimensional case, we prove that the minimax regret\nis $\\widetilde\\Theta(\\sqrt{T})$ and partially resolve a decade-old open\nproblem. Our analysis is non-constructive, as we do not present a concrete\nalgorithm that attains this regret rate. Instead, we use minimax duality to\nreduce the problem to a Bayesian setting, where the convex loss functions are\ndrawn from a worst-case distribution, and then we solve the Bayesian version of\nthe problem with a variant of Thompson Sampling. Our analysis features a novel\nuse of convexity, formalized as a \"local-to-global\" property of convex\nfunctions, that may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 11:54:30 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Dekel", "Ofer", ""], ["Koren", "Tomer", ""], ["Peres", "Yuval", ""]]}, {"id": "1502.06434", "submitter": "Barack Wanjawa Mr.", "authors": "B. W. Wanjawa and L. Muchemi", "title": "ANN Model to Predict Stock Prices at Stock Exchange Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CE cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stock exchanges are considered major players in financial sectors of many\ncountries. Most Stockbrokers, who execute stock trade, use technical,\nfundamental or time series analysis in trying to predict stock prices, so as to\nadvise clients. However, these strategies do not usually guarantee good returns\nbecause they guide on trends and not the most likely price. It is therefore\nnecessary to explore improved methods of prediction.\n  The research proposes the use of Artificial Neural Network that is\nfeedforward multi-layer perceptron with error backpropagation and develops a\nmodel of configuration 5:21:21:1 with 80% training data in 130,000 cycles. The\nresearch develops a prototype and tests it on 2008-2012 data from stock markets\ne.g. Nairobi Securities Exchange and New York Stock Exchange, where prediction\nresults show MAPE of between 0.71% and 2.77%. Validation done with Encog and\nNeuroph realized comparable results. The model is thus capable of prediction on\ntypical stock markets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 06:59:18 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Wanjawa", "B. W.", ""], ["Muchemi", "L.", ""]]}, {"id": "1502.06464", "submitter": "Djork-Arn\\'e Clevert", "authors": "Djork-Arn\\'e Clevert, Andreas Mayr, Thomas Unterthiner, Sepp\n  Hochreiter", "title": "Rectified Factor Networks", "comments": "9 pages + 49 pages supplement", "journal-ref": "Advances in Neural Information Processing Systems 28 (NIPS 2015)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose rectified factor networks (RFNs) to efficiently construct very\nsparse, non-linear, high-dimensional representations of the input. RFN models\nidentify rare and small events in the input, have a low interference between\ncode units, have a small reconstruction error, and explain the data covariance\nstructure. RFN learning is a generalized alternating minimization algorithm\nderived from the posterior regularization method which enforces non-negative\nand normalized posterior means. We proof convergence and correctness of the RFN\nlearning algorithm. On benchmarks, RFNs are compared to other unsupervised\nmethods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to\nprevious sparse coding methods, RFNs yield sparser codes, capture the data's\ncovariance structure more precisely, and have a significantly smaller\nreconstruction error. We test RFNs as pretraining technique for deep networks\non different vision datasets, where RFNs were superior to RBMs and\nautoencoders. On gene expression data from two pharmaceutical drug discovery\nstudies, RFNs detected small and rare gene modules that revealed highly\nrelevant new biological insights which were so far missed by other unsupervised\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 15:44:37 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2015 21:27:53 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Clevert", "Djork-Arn\u00e9", ""], ["Mayr", "Andreas", ""], ["Unterthiner", "Thomas", ""], ["Hochreiter", "Sepp", ""]]}, {"id": "1502.06531", "submitter": "Josip Djolonga", "authors": "Josip Djolonga and Andreas Krause", "title": "Scalable Variational Inference in Log-supermodular Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximate Bayesian inference in log-supermodular\nmodels. These models encompass regular pairwise MRFs with binary variables, but\nallow to capture high-order interactions, which are intractable for existing\napproximate inference techniques such as belief propagation, mean field, and\nvariants. We show that a recently proposed variational approach to inference in\nlog-supermodular models -L-FIELD- reduces to the widely-studied minimum norm\nproblem for submodular minimization. This insight allows to leverage powerful\nexisting tools, and hence to solve the variational problem orders of magnitude\nmore efficiently than previously possible. We then provide another natural\ninterpretation of L-FIELD, demonstrating that it exactly minimizes a specific\ntype of R\\'enyi divergence measure. This insight sheds light on the nature of\nthe variational approximations produced by L-FIELD. Furthermore, we show how to\nperform parallel inference as message passing in a suitable factor graph at a\nlinear convergence rate, without having to sum up over all the configurations\nof the factor. Finally, we apply our approach to a challenging image\nsegmentation task. Our experiments confirm scalability of our approach, high\nquality of the marginals, and the benefit of incorporating higher-order\npotentials.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 18:08:07 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 16:43:05 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Djolonga", "Josip", ""], ["Krause", "Andreas", ""]]}, {"id": "1502.06626", "submitter": "Malik Magdon-Ismail", "authors": "Malik Magdon-Ismail, Christos Boutsidis", "title": "Optimal Sparse Linear Auto-Encoders and Sparse PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal components analysis (PCA) is the optimal linear auto-encoder of\ndata, and it is often used to construct features. Enforcing sparsity on the\nprincipal components can promote better generalization, while improving the\ninterpretability of the features. We study the problem of constructing optimal\nsparse linear auto-encoders. Two natural questions in such a setting are: i)\nGiven a level of sparsity, what is the best approximation to PCA that can be\nachieved? ii) Are there low-order polynomial-time algorithms which can\nasymptotically achieve this optimal tradeoff between the sparsity and the\napproximation quality?\n  In this work, we answer both questions by giving efficient low-order\npolynomial-time algorithms for constructing asymptotically \\emph{optimal}\nlinear auto-encoders (in particular, sparse features with near-PCA\nreconstruction error) and demonstrate the performance of our algorithms on real\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 21:06:39 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Magdon-Ismail", "Malik", ""], ["Boutsidis", "Christos", ""]]}, {"id": "1502.06644", "submitter": "Robert Vandermeulen", "authors": "Robert A. Vandermeulen and Clayton D. Scott", "title": "On The Identifiability of Mixture Models from Grouped Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixture models are statistical models which appear in many problems in\nstatistics and machine learning. In such models it is assumed that data are\ndrawn from random probability measures, called mixture components, which are\nthemselves drawn from a probability measure P over probability measures. When\nestimating mixture models, it is common to make assumptions on the mixture\ncomponents, such as parametric assumptions. In this paper, we make no\nassumption on the mixture components, and instead assume that observations from\nthe mixture model are grouped, such that observations in the same group are\nknown to be drawn from the same component. We show that any mixture of m\nprobability measures can be uniquely identified provided there are 2m-1\nobservations per group. Moreover we show that, for any m, there exists a\nmixture of m probability measures that cannot be uniquely identified when\ngroups have 2m-2 observations. Our results hold for any sample space with more\nthan one element.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 22:24:26 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Vandermeulen", "Robert A.", ""], ["Scott", "Clayton D.", ""]]}, {"id": "1502.06665", "submitter": "Jacob Steinhardt", "authors": "Jacob Steinhardt and Percy Liang", "title": "Reified Context Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classic tension exists between exact inference in a simple model and\napproximate inference in a complex model. The latter offers expressivity and\nthus accuracy, but the former provides coverage of the space, an important\nproperty for confidence estimation and learning with indirect supervision. In\nthis work, we introduce a new approach, reified context models, to reconcile\nthis tension. Specifically, we let the amount of context (the arity of the\nfactors in a graphical model) be chosen \"at run-time\" by reifying it---that is,\nletting this choice itself be a random variable inside the model. Empirically,\nwe show that our approach obtains expressivity and coverage on three natural\nlanguage tasks.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 01:26:43 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Steinhardt", "Jacob", ""], ["Liang", "Percy", ""]]}, {"id": "1502.06668", "submitter": "Jacob Steinhardt", "authors": "Jacob Steinhardt and Percy Liang", "title": "Learning Fast-Mixing Models for Structured Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) algorithms are often used for approximate\ninference inside learning, but their slow mixing can be difficult to diagnose\nand the approximations can seriously degrade learning. To alleviate these\nissues, we define a new model family using strong Doeblin Markov chains, whose\nmixing times can be precisely controlled by a parameter. We also develop an\nalgorithm to learn such models, which involves maximizing the data likelihood\nunder the induced stationary distribution of these chains. We show empirical\nimprovements on two challenging inference tasks.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 01:42:09 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Steinhardt", "Jacob", ""], ["Liang", "Percy", ""]]}, {"id": "1502.06800", "submitter": "Francis Bach", "authors": "Francis Bach (LIENS, SIERRA)", "title": "On the Equivalence between Kernel Quadrature Rules and Random Feature\n  Expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that kernel-based quadrature rules for computing integrals can be\nseen as a special case of random feature expansions for positive definite\nkernels, for a particular decomposition that always exists for such kernels. We\nprovide a theoretical analysis of the number of required samples for a given\napproximation error, leading to both upper and lower bounds that are based\nsolely on the eigenvalues of the associated integral operator and match up to\nlogarithmic terms. In particular, we show that the upper bound may be obtained\nfrom independent and identically distributed samples from a specific\nnon-uniform distribution, while the lower bound if valid for any set of points.\nApplying our results to kernel-based quadrature, while our results are fairly\ngeneral, we recover known upper and lower bounds for the special cases of\nSobolev spaces. Moreover, our results extend to the more general problem of\nfull function approximations (beyond simply computing an integral), with\nresults in L2- and L$\\infty$-norm that match known results for special cases.\nApplying our results to random features, we show an improvement of the number\nof random features needed to preserve the generalization guarantees for\nlearning with Lipschitz-continuous losses.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 13:12:51 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 14:29:04 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Bach", "Francis", "", "LIENS, SIERRA"]]}, {"id": "1502.06895", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang, Chenlei Leng, David B. Dunson", "title": "On the consistency theory of high dimensional variable screening", "comments": "adding comments on REC", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable screening is a fast dimension reduction technique for assisting high\ndimensional feature selection. As a preselection method, it selects a moderate\nsize subset of candidate variables for further refining via feature selection\nto produce the final model. The performance of variable screening depends on\nboth computational efficiency and the ability to dramatically reduce the number\nof variables without discarding the important ones. When the data dimension $p$\nis substantially larger than the sample size $n$, variable screening becomes\ncrucial as 1) Faster feature selection algorithms are needed; 2) Conditions\nguaranteeing selection consistency might fail to hold. This article studies a\nclass of linear screening methods and establishes consistency theory for this\nspecial class. In particular, we prove the restricted diagonally dominant (RDD)\ncondition is a necessary and sufficient condition for strong screening\nconsistency. As concrete examples, we show two screening methods $SIS$ and\n$HOLP$ are both strong screening consistent (subject to additional constraints)\nwith large probability if $n > O((\\rho s + \\sigma/\\tau)^2\\log p)$ under random\ndesigns. In addition, we relate the RDD condition to the irrepresentable\ncondition, and highlight limitations of $SIS$.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 17:52:20 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 18:38:42 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2015 07:33:02 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Wang", "Xiangyu", ""], ["Leng", "Chenlei", ""], ["Dunson", "David B.", ""]]}, {"id": "1502.06922", "submitter": "Hamid Palangi", "authors": "Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He,\n  Jianshu Chen, Xinying Song, Rabab Ward", "title": "Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis\n  and Application to Information Retrieval", "comments": "To appear in IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing", "journal-ref": null, "doi": "10.1109/TASLP.2016.2520371", "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a model that addresses sentence embedding, a hot topic in\ncurrent natural language processing research, using recurrent neural networks\nwith Long Short-Term Memory (LSTM) cells. Due to its ability to capture long\nterm memory, the LSTM-RNN accumulates increasingly richer information as it\ngoes through the sentence, and when it reaches the last word, the hidden layer\nof the network provides a semantic representation of the whole sentence. In\nthis paper, the LSTM-RNN is trained in a weakly supervised manner on user\nclick-through data logged by a commercial web search engine. Visualization and\nanalysis are performed to understand how the embedding process works. The model\nis found to automatically attenuate the unimportant words and detects the\nsalient keywords in the sentence. Furthermore, these detected keywords are\nfound to automatically activate different cells of the LSTM-RNN, where words\nbelonging to a similar topic activate the same cell. As a semantic\nrepresentation of the sentence, the embedding vector can be used in many\ndifferent applications. These automatic keyword detection and topic allocation\nabilities enabled by the LSTM-RNN allow the network to perform document\nretrieval, a difficult language processing task, where the similarity between\nthe query and documents can be measured by the distance between their\ncorresponding sentence embedding vectors computed by the LSTM-RNN. On a web\nsearch task, the LSTM-RNN embedding is shown to significantly outperform\nseveral existing state of the art methods. We emphasize that the proposed model\ngenerates sentence embedding vectors that are specially useful for web document\nretrieval tasks. A comparison with a well known general sentence embedding\nmethod, the Paragraph Vector, is performed. The results show that the proposed\nmethod in this paper significantly outperforms it for web document retrieval\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 19:39:27 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2015 06:11:19 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2016 06:35:23 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Palangi", "Hamid", ""], ["Deng", "Li", ""], ["Shen", "Yelong", ""], ["Gao", "Jianfeng", ""], ["He", "Xiaodong", ""], ["Chen", "Jianshu", ""], ["Song", "Xinying", ""], ["Ward", "Rabab", ""]]}, {"id": "1502.07058", "submitter": "Adam Harley", "authors": "Adam W. Harley, Alex Ufkes, and Konstantinos G. Derpanis", "title": "Evaluation of Deep Convolutional Nets for Document Image Classification\n  and Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new state-of-the-art for document image classification\nand retrieval, using features learned by deep convolutional neural networks\n(CNNs). In object and scene analysis, deep neural nets are capable of learning\na hierarchical chain of abstraction from pixel inputs to concise and\ndescriptive representations. The current work explores this capacity in the\nrealm of document analysis, and confirms that this representation strategy is\nsuperior to a variety of popular hand-crafted alternatives. Experiments also\nshow that (i) features extracted from CNNs are robust to compression, (ii) CNNs\ntrained on non-document images transfer well to document analysis tasks, and\n(iii) enforcing region-specific feature-learning is unnecessary given\nsufficient training data. This work also makes available a new labelled subset\nof the IIT-CDIP collection, containing 400,000 document images across 16\ncategories, useful for training new CNNs for document analysis.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 05:58:43 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Harley", "Adam W.", ""], ["Ufkes", "Alex", ""], ["Derpanis", "Konstantinos G.", ""]]}, {"id": "1502.07073", "submitter": "Alon Gonen", "authors": "Amit Daniely, Alon Gonen, Shai Shalev-Shwartz", "title": "Strongly Adaptive Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strongly adaptive algorithms are algorithms whose performance on every time\ninterval is close to optimal. We present a reduction that can transform\nstandard low-regret algorithms to strongly adaptive. As a consequence, we\nderive simple, yet efficient, strongly adaptive algorithms for a handful of\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 07:24:40 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 15:10:11 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2015 07:31:45 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Daniely", "Amit", ""], ["Gonen", "Alon", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1502.07143", "submitter": "Mark Herbster", "authors": "Mark Herbster, Paul Rubenstein, James Townsend", "title": "The VC-Dimension of Similarity Hypotheses Spaces", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $X$ and a function $h:X\\longrightarrow\\{0,1\\}$ which labels each\nelement of $X$ with either $0$ or $1$, we may define a function $h^{(s)}$ to\nmeasure the similarity of pairs of points in $X$ according to $h$.\nSpecifically, for $h\\in \\{0,1\\}^X$ we define $h^{(s)}\\in \\{0,1\\}^{X\\times X}$\nby $h^{(s)}(w,x):= \\mathbb{1}[h(w) = h(x)]$. This idea can be extended to a set\nof functions, or hypothesis space $\\mathcal{H} \\subseteq \\{0,1\\}^X$ by defining\na similarity hypothesis space $\\mathcal{H}^{(s)}:=\\{h^{(s)}:h\\in\\mathcal{H}\\}$.\nWe show that ${{vc-dimension}}(\\mathcal{H}^{(s)}) \\in\n\\Theta({{vc-dimension}}(\\mathcal{H}))$.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 12:14:04 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Herbster", "Mark", ""], ["Rubenstein", "Paul", ""], ["Townsend", "James", ""]]}, {"id": "1502.07190", "submitter": "Linda S. L. Tan", "authors": "Linda S. L. Tan, Aik Hui Chan and Tian Zheng", "title": "Topic-adjusted visibility metric for scientific articles", "comments": null, "journal-ref": "Annals of Applied Statistics, Volume 10, Number 1 (2016), 1-31", "doi": "10.1214/15-AOAS887", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the impact of scientific articles is important for evaluating the\nresearch output of individual scientists, academic institutions and journals.\nWhile citations are raw data for constructing impact measures, there exist\nbiases and potential issues if factors affecting citation patterns are not\nproperly accounted for. In this work, we address the problem of field variation\nand introduce an article level metric useful for evaluating individual\narticles' visibility. This measure derives from joint probabilistic modeling of\nthe content in the articles and the citations amongst them using latent\nDirichlet allocation (LDA) and the mixed membership stochastic blockmodel\n(MMSB). Our proposed model provides a visibility metric for individual articles\nadjusted for field variation in citation rates, a structural understanding of\ncitation behavior in different fields, and article recommendations which take\ninto account article visibility and citation patterns. We develop an efficient\nalgorithm for model fitting using variational methods. To scale up to large\nnetworks, we develop an online variant using stochastic gradient methods and\ncase-control likelihood approximation. We apply our methods to the benchmark\nKDD Cup 2003 dataset with approximately 30,000 high energy physics papers.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 14:57:55 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2015 15:08:27 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2015 13:50:03 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Tan", "Linda S. L.", ""], ["Chan", "Aik Hui", ""], ["Zheng", "Tian", ""]]}, {"id": "1502.07229", "submitter": "Yiming Ying", "authors": "Yiming Ying and Ding-Xuan Zhou", "title": "Online Pairwise Learning Algorithms with Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise learning usually refers to a learning task which involves a loss\nfunction depending on pairs of examples, among which most notable ones include\nranking, metric learning and AUC maximization. In this paper, we study an\nonline algorithm for pairwise learning with a least-square loss function in an\nunconstrained setting of a reproducing kernel Hilbert space (RKHS), which we\nrefer to as the Online Pairwise lEaRning Algorithm (OPERA). In contrast to\nexisting works \\cite{Kar,Wang} which require that the iterates are restricted\nto a bounded domain or the loss function is strongly-convex, OPERA is\nassociated with a non-strongly convex objective function and learns the target\nfunction in an unconstrained RKHS. Specifically, we establish a general theorem\nwhich guarantees the almost surely convergence for the last iterate of OPERA\nwithout any assumptions on the underlying distribution. Explicit convergence\nrates are derived under the condition of polynomially decaying step sizes. We\nalso establish an interesting property for a family of widely-used kernels in\nthe setting of pairwise learning and illustrate the above convergence results\nusing such kernels. Our methodology mainly depends on the characterization of\nRKHSs using its associated integral operators and probability inequalities for\nrandom variables with values in a Hilbert space.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 16:26:33 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Ying", "Yiming", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1502.07617", "submitter": "Tomer Koren", "authors": "Noga Alon, Nicol\\`o Cesa-Bianchi, Ofer Dekel, Tomer Koren", "title": "Online Learning with Feedback Graphs: Beyond Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a general class of online learning problems where the feedback is\nspecified by a graph. This class includes online prediction with expert advice\nand the multi-armed bandit problem, but also several learning problems where\nthe online player does not necessarily observe his own loss. We analyze how the\nstructure of the feedback graph controls the inherent difficulty of the induced\n$T$-round learning problem. Specifically, we show that any feedback graph\nbelongs to one of three classes: strongly observable graphs, weakly observable\ngraphs, and unobservable graphs. We prove that the first class induces learning\nproblems with $\\widetilde\\Theta(\\alpha^{1/2} T^{1/2})$ minimax regret, where\n$\\alpha$ is the independence number of the underlying graph; the second class\ninduces problems with $\\widetilde\\Theta(\\delta^{1/3}T^{2/3})$ minimax regret,\nwhere $\\delta$ is the domination number of a certain portion of the graph; and\nthe third class induces problems with linear minimax regret. Our results\nsubsume much of the previous work on learning with feedback graphs and reveal\nnew connections to partial monitoring games. We also show how the regret is\naffected if the graphs are allowed to vary with time.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 16:18:53 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Alon", "Noga", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""], ["Dekel", "Ofer", ""], ["Koren", "Tomer", ""]]}, {"id": "1502.07641", "submitter": "Rina Foygel Barber", "authors": "Rina Foygel Barber and Mladen Kolar", "title": "ROCKET: Robust Confidence Intervals via Kendall's Tau for\n  Transelliptical Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undirected graphical models are used extensively in the biological and social\nsciences to encode a pattern of conditional independences between variables,\nwhere the absence of an edge between two nodes $a$ and $b$ indicates that the\ncorresponding two variables $X_a$ and $X_b$ are believed to be conditionally\nindependent, after controlling for all other measured variables. In the\nGaussian case, conditional independence corresponds to a zero entry in the\nprecision matrix $\\Omega$ (the inverse of the covariance matrix $\\Sigma$). Real\ndata often exhibits heavy tail dependence between variables, which cannot be\ncaptured by the commonly-used Gaussian or nonparanormal (Gaussian copula)\ngraphical models. In this paper, we study the transelliptical model, an\nelliptical copula model that generalizes Gaussian and nonparanormal models to a\nbroader family of distributions. We propose the ROCKET method, which constructs\nan estimator of $\\Omega_{ab}$ that we prove to be asymptotically normal under\nmild assumptions. Empirically, ROCKET outperforms the nonparanormal and\nGaussian models in terms of achieving accurate inference on simulated data. We\nalso compare the three methods on real data (daily stock returns), and find\nthat the ROCKET estimator is the only method whose behavior across subsamples\nagrees with the distribution predicted by the theory.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 17:25:03 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2015 19:23:59 GMT"}, {"version": "v3", "created": "Fri, 1 Sep 2017 18:59:57 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Barber", "Rina Foygel", ""], ["Kolar", "Mladen", ""]]}, {"id": "1502.07645", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang, Stephen E. Fienberg, Alex Smola", "title": "Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of Bayesian learning on sensitive datasets and\npresent two simple but somewhat surprising results that connect Bayesian\nlearning to \"differential privacy:, a cryptographic approach to protect\nindividual-level privacy while permiting database-level utility. Specifically,\nwe show that that under standard assumptions, getting one single sample from a\nposterior distribution is differentially private \"for free\". We will see that\nestimator is statistically consistent, near optimal and computationally\ntractable whenever the Bayesian model of interest is consistent, optimal and\ntractable. Similarly but separately, we show that a recent line of works that\nuse stochastic gradient for Hybrid Monte Carlo (HMC) sampling also preserve\ndifferentially privacy with minor or no modifications of the algorithmic\nprocedure at all, these observations lead to an \"anytime\" algorithm for\nBayesian learning under privacy constraint. We demonstrate that it performs\nmuch better than the state-of-the-art differential private methods on synthetic\nand real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 17:38:47 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2015 02:53:05 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Wang", "Yu-Xiang", ""], ["Fienberg", "Stephen E.", ""], ["Smola", "Alex", ""]]}, {"id": "1502.07697", "submitter": "Sebastien Gerchinovitz", "authors": "Pierre Gaillard (GREGHEC, EDF R\\&D), S\\'ebastien Gerchinovitz (IMT,\n  UPS)", "title": "A Chaining Algorithm for Online Nonparametric Regression", "comments": "Published in the proceedings of COLT 2015:\n  http://jmlr.org/proceedings/papers/v40/Gaillard15.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online nonparametric regression with arbitrary\ndeterministic sequences. Using ideas from the chaining technique, we design an\nalgorithm that achieves a Dudley-type regret bound similar to the one obtained\nin a non-constructive fashion by Rakhlin and Sridharan (2014). Our regret bound\nis expressed in terms of the metric entropy in the sup norm, which yields\noptimal guarantees when the metric and sequential entropies are of the same\norder of magnitude. In particular our algorithm is the first one that achieves\noptimal rates for online regression over H{\\\"o}lder balls. In addition we show\nfor this example how to adapt our chaining algorithm to get a reasonable\ncomputational efficiency with similar regret guarantees (up to a log factor).\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 19:47:41 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2015 18:37:24 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Gaillard", "Pierre", "", "GREGHEC, EDF R\\&D"], ["Gerchinovitz", "S\u00e9bastien", "", "IMT,\n  UPS"]]}, {"id": "1502.07776", "submitter": "Slimane Bellaouar", "authors": "Slimane Bellaouar, Hadda Cherroun, and Djelloul Ziadi", "title": "Efficient Geometric-based Computation of the String Subsequence Kernel", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods are powerful tools in machine learning. They have to be\ncomputationally efficient. In this paper, we present a novel Geometric-based\napproach to compute efficiently the string subsequence kernel (SSK). Our main\nidea is that the SSK computation reduces to range query problem. We started by\nthe construction of a match list $L(s,t)=\\{(i,j):s_{i}=t_{j}\\}$ where $s$ and\n$t$ are the strings to be compared; such match list contains only the required\ndata that contribute to the result. To compute efficiently the SSK, we extended\nthe layered range tree data structure to a layered range sum tree, a\nrange-aggregation data structure. The whole process takes $ O(p|L|\\log|L|)$\ntime and $O(|L|\\log|L|)$ space, where $|L|$ is the size of the match list and\n$p$ is the length of the SSK. We present empiric evaluations of our approach\nagainst the dynamic and the sparse programming approaches both on synthetically\ngenerated data and on newswire article data. Such experiments show the\nefficiency of our approach for large alphabet size except for very short\nstrings. Moreover, compared to the sparse dynamic approach, the proposed\napproach outperforms absolutely for long strings.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 22:12:22 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Bellaouar", "Slimane", ""], ["Cherroun", "Hadda", ""], ["Ziadi", "Djelloul", ""]]}, {"id": "1502.07813", "submitter": "Parthan Kasarapu Mr", "authors": "Parthan Kasarapu and Lloyd Allison", "title": "Minimum message length estimation of mixtures of multivariate Gaussian\n  and von Mises-Fisher distributions", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture modelling involves explaining some observed evidence using a\ncombination of probability distributions. The crux of the problem is the\ninference of an optimal number of mixture components and their corresponding\nparameters. This paper discusses unsupervised learning of mixture models using\nthe Bayesian Minimum Message Length (MML) criterion. To demonstrate the\neffectiveness of search and inference of mixture parameters using the proposed\napproach, we select two key probability distributions, each handling\nfundamentally different types of data: the multivariate Gaussian distribution\nto address mixture modelling of data distributed in Euclidean space, and the\nmultivariate von Mises-Fisher (vMF) distribution to address mixture modelling\nof directional data distributed on a unit hypersphere. The key contributions of\nthis paper, in addition to the general search and inference methodology,\ninclude the derivation of MML expressions for encoding the data using\nmultivariate Gaussian and von Mises-Fisher distributions, and the analytical\nderivation of the MML estimates of the parameters of the two distributions. Our\napproach is tested on simulated and real world data sets. For instance, we\ninfer vMF mixtures that concisely explain experimentally determined\nthree-dimensional protein conformations, providing an effective null model\ndescription of protein structures that is central to many inference problems in\nstructural bioinformatics. The experimental results demonstrate that the\nperformance of our proposed search and inference method along with the encoding\nschemes improve on the state of the art mixture modelling techniques.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 03:32:49 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Kasarapu", "Parthan", ""], ["Allison", "Lloyd", ""]]}, {"id": "1502.07943", "submitter": "Ameet Talwalkar", "authors": "Kevin Jamieson, Ameet Talwalkar", "title": "Non-stochastic Best Arm Identification and Hyperparameter Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the task of hyperparameter optimization, we introduce the\nnon-stochastic best-arm identification problem. Within the multi-armed bandit\nliterature, the cumulative regret objective enjoys algorithms and analyses for\nboth the non-stochastic and stochastic settings while to the best of our\nknowledge, the best-arm identification framework has only been considered in\nthe stochastic setting. We introduce the non-stochastic setting under this\nframework, identify a known algorithm that is well-suited for this setting, and\nanalyze its behavior. Next, by leveraging the iterative nature of standard\nmachine learning algorithms, we cast hyperparameter optimization as an instance\nof non-stochastic best-arm identification, and empirically evaluate our\nproposed algorithm on this task. Our empirical results show that, by allocating\nmore resources to promising hyperparameter settings, we typically achieve\ncomparable test accuracies an order of magnitude faster than baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 15:58:45 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Jamieson", "Kevin", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "1502.07976", "submitter": "Miguel Angel Bautista Martin", "authors": "Miguel Angel Bautista, Oriol Pujol, Fernando de la Torre and Sergio\n  Escalera", "title": "Error-Correcting Factorization", "comments": "Under review at TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error Correcting Output Codes (ECOC) is a successful technique in multi-class\nclassification, which is a core problem in Pattern Recognition and Machine\nLearning. A major advantage of ECOC over other methods is that the multi- class\nproblem is decoupled into a set of binary problems that are solved\nindependently. However, literature defines a general error-correcting\ncapability for ECOCs without analyzing how it distributes among classes,\nhindering a deeper analysis of pair-wise error-correction. To address these\nlimitations this paper proposes an Error-Correcting Factorization (ECF) method,\nour contribution is three fold: (I) We propose a novel representation of the\nerror-correction capability, called the design matrix, that enables us to build\nan ECOC on the basis of allocating correction to pairs of classes. (II) We\nderive the optimal code length of an ECOC using rank properties of the design\nmatrix. (III) ECF is formulated as a discrete optimization problem, and a\nrelaxed solution is found using an efficient constrained block coordinate\ndescent approach. (IV) Enabled by the flexibility introduced with the design\nmatrix we propose to allocate the error-correction on classes that are prone to\nconfusion. Experimental results in several databases show that when allocating\nthe error-correction to confusable classes ECF outperforms state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 17:22:53 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 17:49:16 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Bautista", "Miguel Angel", ""], ["Pujol", "Oriol", ""], ["de la Torre", "Fernando", ""], ["Escalera", "Sergio", ""]]}, {"id": "1502.08009", "submitter": "Wouter Koolen", "authors": "Wouter M. Koolen and Tim van Erven", "title": "Second-order Quantile Methods for Experts and Combinatorial Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to design strategies for sequential decision making that adjust to the\ndifficulty of the learning problem. We study this question both in the setting\nof prediction with expert advice, and for more general combinatorial decision\ntasks. We are not satisfied with just guaranteeing minimax regret rates, but we\nwant our algorithms to perform significantly better on easy data. Two popular\nways to formalize such adaptivity are second-order regret bounds and quantile\nbounds. The underlying notions of 'easy data', which may be paraphrased as \"the\nlearning problem has small variance\" and \"multiple decisions are useful\", are\nsynergetic. But even though there are sophisticated algorithms that exploit one\nof the two, no existing algorithm is able to adapt to both.\n  In this paper we outline a new method for obtaining such adaptive algorithms,\nbased on a potential function that aggregates a range of learning rates (which\nare essential tuning parameters). By choosing the right prior we construct\nefficient algorithms and show that they reap both benefits by proving the first\nbounds that are both second-order and incorporate quantiles.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 18:56:45 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Koolen", "Wouter M.", ""], ["van Erven", "Tim", ""]]}, {"id": "1502.08029", "submitter": "Li Yao", "authors": "Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal,\n  Hugo Larochelle, Aaron Courville", "title": "Describing Videos by Exploiting Temporal Structure", "comments": "Accepted to ICCV15. This version comes with code release and\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in using recurrent neural networks (RNNs) for image\ndescription has motivated the exploration of their application for video\ndescription. However, while images are static, working with videos requires\nmodeling their dynamic temporal structure and then properly integrating that\ninformation into a natural language description. In this context, we propose an\napproach that successfully takes into account both the local and global\ntemporal structure of videos to produce descriptions. First, our approach\nincorporates a spatial temporal 3-D convolutional neural network (3-D CNN)\nrepresentation of the short temporal dynamics. The 3-D CNN representation is\ntrained on video action recognition tasks, so as to produce a representation\nthat is tuned to human motion and behavior. Second we propose a temporal\nattention mechanism that allows to go beyond local temporal modeling and learns\nto automatically select the most relevant temporal segments given the\ntext-generating RNN. Our approach exceeds the current state-of-art for both\nBLEU and METEOR metrics on the Youtube2Text dataset. We also present results on\na new, larger and more challenging dataset of paired video and natural language\ndescriptions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 19:30:40 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 17:24:47 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2015 15:27:08 GMT"}, {"version": "v4", "created": "Sat, 25 Apr 2015 20:32:27 GMT"}, {"version": "v5", "created": "Thu, 1 Oct 2015 00:12:46 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Yao", "Li", ""], ["Torabi", "Atousa", ""], ["Cho", "Kyunghyun", ""], ["Ballas", "Nicolas", ""], ["Pal", "Christopher", ""], ["Larochelle", "Hugo", ""], ["Courville", "Aaron", ""]]}, {"id": "1502.08030", "submitter": "Hung Nghiep Tran", "authors": "Hung Nghiep Tran, Tin Huynh, Tien Do", "title": "Author Name Disambiguation by Using Deep Neural Network", "comments": null, "journal-ref": "Asian Conference on Intelligent Information and Database Systems\n  (ACIIDS 2014)", "doi": "10.1007/978-3-319-05476-6_13", "report-no": null, "categories": "cs.DL cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Author name ambiguity decreases the quality and reliability of information\nretrieved from digital libraries. Existing methods have tried to solve this\nproblem by predefining a feature set based on expert's knowledge for a specific\ndataset. In this paper, we propose a new approach which uses deep neural\nnetwork to learn features automatically from data. Additionally, we propose the\ngeneral system architecture for author name disambiguation on any dataset. In\nthis research, we evaluate the proposed method on a dataset containing\nVietnamese author names. The results show that this method significantly\noutperforms other methods that use predefined feature set. The proposed method\nachieves 99.31% in terms of accuracy. Prediction error rate decreases from\n1.83% to 0.69%, i.e., it decreases by 1.14%, or 62.3% relatively compared with\nother methods that use predefined feature set (Table 3).\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 19:34:42 GMT"}, {"version": "v2", "created": "Sat, 29 Jul 2017 01:32:31 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Tran", "Hung Nghiep", ""], ["Huynh", "Tin", ""], ["Do", "Tien", ""]]}, {"id": "1502.08039", "submitter": "Jihun Hamm", "authors": "Jihun Hamm, Mikhail Belkin", "title": "Probabilistic Zero-shot Classification with Semantic Rankings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a non-metric ranking-based representation of\nsemantic similarity that allows natural aggregation of semantic information\nfrom multiple heterogeneous sources. We apply the ranking-based representation\nto zero-shot learning problems, and present deterministic and probabilistic\nzero-shot classifiers which can be built from pre-trained classifiers without\nretraining. We demonstrate their the advantages on two large real-world image\ndatasets. In particular, we show that aggregating different sources of semantic\ninformation, including crowd-sourcing, leads to more accurate classification.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 20:00:53 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Hamm", "Jihun", ""], ["Belkin", "Mikhail", ""]]}, {"id": "1502.08053", "submitter": "Dominik Csiba", "authors": "Dominik Csiba, Zheng Qu, Peter Richt\\'arik", "title": "Stochastic Dual Coordinate Ascent with Adaptive Probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces AdaSDCA: an adaptive variant of stochastic dual\ncoordinate ascent (SDCA) for solving the regularized empirical risk\nminimization problems. Our modification consists in allowing the method\nadaptively change the probability distribution over the dual variables\nthroughout the iterative process. AdaSDCA achieves provably better complexity\nbound than SDCA with the best fixed probability distribution, known as\nimportance sampling. However, it is of a theoretical character as it is\nexpensive to implement. We also propose AdaSDCA+: a practical variant which in\nour experiments outperforms existing non-adaptive methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 20:54:03 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Csiba", "Dominik", ""], ["Qu", "Zheng", ""], ["Richt\u00e1rik", "Peter", ""]]}]