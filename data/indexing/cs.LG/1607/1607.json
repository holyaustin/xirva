[{"id": "1607.00024", "submitter": "Tal Hadad", "authors": "Tal Hadad", "title": "Review Based Rating Prediction", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems are an important units in today's e-commerce\napplications, such as targeted advertising, personalized marketing and\ninformation retrieval. In recent years, the importance of contextual\ninformation has motivated generation of personalized recommendations according\nto the available contextual information of users.\n  Compared to the traditional systems which mainly utilize users' rating\nhistory, review-based recommendation hopefully provide more relevant results to\nusers. We introduce a review-based recommendation approach that obtains\ncontextual information by mining user reviews. The proposed approach relate to\nfeatures obtained by analyzing textual reviews using methods developed in\nNatural Language Processing (NLP) and information retrieval discipline to\ncompute a utility function over a given item. An item utility is a measure that\nshows how much it is preferred according to user's current context.\n  In our system, the context inference is modeled as similarity between the\nusers reviews history and the item reviews history. As an example application,\nwe used our method to mine contextual data from customers' reviews of movies\nand use it to produce review-based rating prediction. The predicted ratings can\ngenerate recommendations that are item-based and should appear at the\nrecommended items list in the product page. Our evaluations suggest that our\nsystem can help produce better prediction rating scores in comparison to the\nstandard prediction methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 20:16:58 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 18:50:22 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2016 11:06:09 GMT"}, {"version": "v4", "created": "Thu, 28 Jul 2016 07:48:46 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Hadad", "Tal", ""]]}, {"id": "1607.00034", "submitter": "Tom Hope", "authors": "Tom Hope and Dafna Shahaf", "title": "Ballpark Learning: Estimating Labels from Rough Group Comparisons", "comments": "To appear in the European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery (ECML-PKDD) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in estimating individual labels given only coarse,\naggregated signal over the data points. In our setting, we receive sets\n(\"bags\") of unlabeled instances with constraints on label proportions. We relax\nthe unrealistic assumption of known label proportions, made in previous work;\ninstead, we assume only to have upper and lower bounds, and constraints on bag\ndifferences. We motivate the problem, propose an intuitive formulation and\nalgorithm, and apply our methods to real-world scenarios. Across several\ndomains, we show how using only proportion constraints and no labeled examples,\nwe can achieve surprisingly high accuracy. In particular, we demonstrate how to\npredict income level using rough stereotypes and how to perform sentiment\nanalysis using very little information. We also apply our method to guide\nexploratory analysis, recovering geographical differences in twitter dialect.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 20:40:24 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Hope", "Tom", ""], ["Shahaf", "Dafna", ""]]}, {"id": "1607.00036", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "Caglar Gulcehre, Sarath Chandar, Kyunghyun Cho, Yoshua Bengio", "title": "Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend neural Turing machine (NTM) model into a dynamic neural Turing\nmachine (D-NTM) by introducing a trainable memory addressing scheme. This\naddressing scheme maintains for each memory cell two separate vectors, content\nand address vectors. This allows the D-NTM to learn a wide variety of\nlocation-based addressing strategies including both linear and nonlinear ones.\nWe implement the D-NTM with both continuous, differentiable and discrete,\nnon-differentiable read/write mechanisms. We investigate the mechanisms and\neffects of learning to read and write into a memory through experiments on\nFacebook bAbI tasks using both a feedforward and GRUcontroller. The D-NTM is\nevaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM\nbaselines. We have done extensive analysis of our model and different\nvariations of NTM on bAbI task. We also provide further experimental results on\nsequential pMNIST, Stanford Natural Language Inference, associative recall and\ncopy tasks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 20:45:12 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 05:56:48 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Chandar", "Sarath", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1607.00067", "submitter": "Fariba Yousefi", "authors": "Fariba Yousefi, Zhenwen Dai, Carl Henrik Ek, Neil Lawrence", "title": "Unsupervised Learning with Imbalanced Data via Structure Consolidation\n  Latent Variable Model", "comments": "ICLR 2016 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning on imbalanced data is challenging because, when given\nimbalanced data, current model is often dominated by the major category and\nignores the categories with small amount of data. We develop a latent variable\nmodel that can cope with imbalanced data by dividing the latent space into a\nshared space and a private space. Based on Gaussian Process Latent Variable\nModels, we propose a new kernel formulation that enables the separation of\nlatent space and derives an efficient variational inference method. The\nperformance of our model is demonstrated with an imbalanced medical image\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 22:25:20 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Yousefi", "Fariba", ""], ["Dai", "Zhenwen", ""], ["Ek", "Carl Henrik", ""], ["Lawrence", "Neil", ""]]}, {"id": "1607.00076", "submitter": "Daria Reshetova", "authors": "Daria Reshetova", "title": "Multi-class classification: mirror descent approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of multi-class classification and a stochastic opti-\nmization approach to it. We derive risk bounds for stochastic mirror descent\nalgorithm and provide examples of set geometries that make the use of the\nalgorithm efficient in terms of error in k.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 23:12:20 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 08:15:53 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Reshetova", "Daria", ""]]}, {"id": "1607.00087", "submitter": "Miao Cheng", "authors": "Miao Cheng and Ah Chung Tsoi", "title": "Fractal Dimension Pattern Based Multiresolution Analysis for Rough\n  Estimator of Person-Dependent Audio Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a general means of expression, audio analysis and recognition has\nattracted much attentions for its wide applications in real-life world. Audio\nemotion recognition (AER) attempts to understand emotional states of human with\nthe given utterance signals, and has been studied abroad for its further\ndevelopment on friendly human-machine interfaces. Distinguish from other\nexisting works, the person-dependent patterns of audio emotions are conducted,\nand fractal dimension features are calculated for acoustic feature extraction.\nFurthermore, it is able to efficiently learn intrinsic characteristics of\nauditory emotions, while the utterance features are learned from fractal\ndimensions of each sub-bands. Experimental results show the proposed method is\nable to provide comparative performance for audio emotion recognition.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 00:54:10 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 13:12:35 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Cheng", "Miao", ""], ["Tsoi", "Ah Chung", ""]]}, {"id": "1607.00101", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu", "title": "Randomized block proximal damped Newton method for composite\n  self-concordant minimization", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the composite self-concordant (CSC) minimization\nproblem, which minimizes the sum of a self-concordant function $f$ and a\n(possibly nonsmooth) proper closed convex function $g$. The CSC minimization is\nthe cornerstone of the path-following interior point methods for solving a\nbroad class of convex optimization problems. It has also found numerous\napplications in machine learning. The proximal damped Newton (PDN) methods have\nbeen well studied in the literature for solving this problem that enjoy a nice\niteration complexity. Given that at each iteration these methods typically\nrequire evaluating or accessing the Hessian of $f$ and also need to solve a\nproximal Newton subproblem, the cost per iteration can be prohibitively high\nwhen applied to large-scale problems. Inspired by the recent success of block\ncoordinate descent methods, we propose a randomized block proximal damped\nNewton (RBPDN) method for solving the CSC minimization. Compared to the PDN\nmethods, the computational cost per iteration of RBPDN is usually significantly\nlower. The computational experiment on a class of regularized logistic\nregression problems demonstrate that RBPDN is indeed promising in solving\nlarge-scale CSC minimization problems. The convergence of RBPDN is also\nanalyzed in the paper. In particular, we show that RBPDN is globally convergent\nwhen $g$ is Lipschitz continuous. It is also shown that RBPDN enjoys a local\nlinear convergence. Moreover, we show that for a class of $g$ including the\ncase where $g$ is Lipschitz differentiable, RBPDN enjoys a global linear\nconvergence. As a striking consequence, it shows that the classical damped\nNewton methods [22,40] and the PDN [31] for such $g$ are globally linearly\nconvergent, which was previously unknown in the literature. Moreover, this\nresult can be used to sharpen the existing iteration complexity of these\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 03:16:57 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Lu", "Zhaosong", ""]]}, {"id": "1607.00110", "submitter": "Iman Alodah", "authors": "Iman Alodah and Jennifer Neville", "title": "Combining Gradient Boosting Machines with Collective Inference to\n  Predict Continuous Values", "comments": "7 pages, 3 Figures, Sixth International Workshop on Statistical\n  Relational AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient boosting of regression trees is a competitive procedure for learning\npredictive models of continuous data that fits the data with an additive\nnon-parametric model. The classic version of gradient boosting assumes that the\ndata is independent and identically distributed. However, relational data with\ninterdependent, linked instances is now common and the dependencies in such\ndata can be exploited to improve predictive performance. Collective inference\nis one approach to exploit relational correlation patterns and significantly\nreduce classification error. However, much of the work on collective learning\nand inference has focused on discrete prediction tasks rather than continuous.\n%target values has not got that attention in terms of collective inference. In\nthis work, we investigate how to combine these two paradigms together to\nimprove regression in relational domains. Specifically, we propose a boosting\nalgorithm for learning a collective inference model that predicts a continuous\ntarget variable. In the algorithm, we learn a basic relational model,\ncollectively infer the target values, and then iteratively learn relational\nmodels to predict the residuals. We evaluate our proposed algorithm on a real\nnetwork dataset and show that it outperforms alternative boosting methods.\nHowever, our investigation also revealed that the relational features interact\ntogether to produce better predictions.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 05:21:15 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Alodah", "Iman", ""], ["Neville", "Jennifer", ""]]}, {"id": "1607.00122", "submitter": "Heechul Jung", "authors": "Heechul Jung, Jeongwoo Ju, Minju Jung, Junmo Kim", "title": "Less-forgetting Learning in Deep Neural Networks", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A catastrophic forgetting problem makes deep neural networks forget the\npreviously learned information, when learning data collected in new\nenvironments, such as by different sensors or in different light conditions.\nThis paper presents a new method for alleviating the catastrophic forgetting\nproblem. Unlike previous research, our method does not use any information from\nthe source domain. Surprisingly, our method is very effective to forget less of\nthe information in the source domain, and we show the effectiveness of our\nmethod using several experiments. Furthermore, we observed that the forgetting\nproblem occurs between mini-batches when performing general training processes\nusing stochastic gradient descent methods, and this problem is one of the\nfactors that degrades generalization performance of the network. We also try to\nsolve this problem using the proposed method. Finally, we show our\nless-forgetting learning method is also helpful to improve the performance of\ndeep neural networks in terms of recognition rates.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 06:50:17 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Jung", "Heechul", ""], ["Ju", "Jeongwoo", ""], ["Jung", "Minju", ""], ["Kim", "Junmo", ""]]}, {"id": "1607.00133", "submitter": "Ilya Mironov", "authors": "Mart\\'in Abadi and Andy Chu and Ian Goodfellow and H. Brendan McMahan\n  and Ilya Mironov and Kunal Talwar and Li Zhang", "title": "Deep Learning with Differential Privacy", "comments": null, "journal-ref": "Proceedings of the 2016 ACM SIGSAC Conference on Computer and\n  Communications Security (ACM CCS), pp. 308-318, 2016", "doi": "10.1145/2976749.2978318", "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques based on neural networks are achieving remarkable\nresults in a wide variety of domains. Often, the training of models requires\nlarge, representative datasets, which may be crowdsourced and contain sensitive\ninformation. The models should not expose private information in these\ndatasets. Addressing this goal, we develop new algorithmic techniques for\nlearning and a refined analysis of privacy costs within the framework of\ndifferential privacy. Our implementation and experiments demonstrate that we\ncan train deep neural networks with non-convex objectives, under a modest\nprivacy budget, and at a manageable cost in software complexity, training\nefficiency, and model quality.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 07:29:10 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 11:59:40 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Abadi", "Mart\u00edn", ""], ["Chu", "Andy", ""], ["Goodfellow", "Ian", ""], ["McMahan", "H. Brendan", ""], ["Mironov", "Ilya", ""], ["Talwar", "Kunal", ""], ["Zhang", "Li", ""]]}, {"id": "1607.00136", "submitter": "Collins Leke", "authors": "Collins Leke and Tshilidzi Marwala", "title": "Missing Data Estimation in High-Dimensional Datasets: A Swarm\n  Intelligence-Deep Neural Network Approach", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine the problem of missing data in high-dimensional\ndatasets by taking into consideration the Missing Completely at Random and\nMissing at Random mechanisms, as well as theArbitrary missing pattern.\nAdditionally, this paper employs a methodology based on Deep Learning and Swarm\nIntelligence algorithms in order to provide reliable estimates for missing\ndata. The deep learning technique is used to extract features from the input\ndata via an unsupervised learning approach by modeling the data distribution\nbased on the input. This deep learning technique is then used as part of the\nobjective function for the swarm intelligence technique in order to estimate\nthe missing data after a supervised fine-tuning phase by minimizing an error\nfunction based on the interrelationship and correlation between features in the\ndataset. The investigated methodology in this paper therefore has longer\nrunning times, however, the promising potential outcomes justify the trade-off.\nAlso, basic knowledge of statistics is presumed.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 07:34:50 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Leke", "Collins", ""], ["Marwala", "Tshilidzi", ""]]}, {"id": "1607.00146", "submitter": "Kush Bhatia", "authors": "Kush Bhatia, Prateek Jain, Parameswaran Kamalaruban, Purushottam Kar", "title": "Efficient and Consistent Robust Time Series Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of robust time series analysis under the standard\nauto-regressive (AR) time series model in the presence of arbitrary outliers.\nWe devise an efficient hard thresholding based algorithm which can obtain a\nconsistent estimate of the optimal AR model despite a large fraction of the\ntime series points being corrupted. Our algorithm alternately estimates the\ncorrupted set of points and the model parameters, and is inspired by recent\nadvances in robust regression and hard-thresholding methods. However, a direct\napplication of existing techniques is hindered by a critical difference in the\ntime-series domain: each point is correlated with all previous points rendering\nexisting tools inapplicable directly. We show how to overcome this hurdle using\nnovel proof techniques. Using our techniques, we are also able to provide the\nfirst efficient and provably consistent estimator for the robust regression\nproblem where a standard linear observation model with white additive noise is\ncorrupted arbitrarily. We illustrate our methods on synthetic datasets and show\nthat our methods indeed are able to consistently recover the optimal parameters\ndespite a large fraction of points being corrupted.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 08:17:27 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Bhatia", "Kush", ""], ["Jain", "Prateek", ""], ["Kamalaruban", "Parameswaran", ""], ["Kar", "Purushottam", ""]]}, {"id": "1607.00148", "submitter": "Pankaj Malhotra Mr.", "authors": "Pankaj Malhotra, Anusha Ramakrishnan, Gaurangi Anand, Lovekesh Vig,\n  Puneet Agarwal, Gautam Shroff", "title": "LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection", "comments": "Accepted at ICML 2016 Anomaly Detection Workshop, New York, NY, USA,\n  2016. Reference update in this version (v2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mechanical devices such as engines, vehicles, aircrafts, etc., are typically\ninstrumented with numerous sensors to capture the behavior and health of the\nmachine. However, there are often external factors or variables which are not\ncaptured by sensors leading to time-series which are inherently unpredictable.\nFor instance, manual controls and/or unmonitored environmental conditions or\nload may lead to inherently unpredictable time-series. Detecting anomalies in\nsuch scenarios becomes challenging using standard approaches based on\nmathematical models that rely on stationarity, or prediction models that\nutilize prediction errors to detect anomalies. We propose a Long Short Term\nMemory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD)\nthat learns to reconstruct 'normal' time-series behavior, and thereafter uses\nreconstruction error to detect anomalies. We experiment with three publicly\navailable quasi predictable time-series datasets: power demand, space shuttle,\nand ECG, and two real-world engine datasets with both predictive and\nunpredictable behavior. We show that EncDec-AD is robust and can detect\nanomalies from predictable, unpredictable, periodic, aperiodic, and\nquasi-periodic time-series. Further, we show that EncDec-AD is able to detect\nanomalies from short time-series (length as small as 30) as well as long\ntime-series (length as large as 500).\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 08:25:48 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 09:33:48 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Malhotra", "Pankaj", ""], ["Ramakrishnan", "Anusha", ""], ["Anand", "Gaurangi", ""], ["Vig", "Lovekesh", ""], ["Agarwal", "Puneet", ""], ["Shroff", "Gautam", ""]]}, {"id": "1607.00215", "submitter": "Ian Osband", "authors": "Ian Osband, Benjamin Van Roy", "title": "Why is Posterior Sampling Better than Optimism for Reinforcement\n  Learning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational results demonstrate that posterior sampling for reinforcement\nlearning (PSRL) dramatically outperforms algorithms driven by optimism, such as\nUCRL2. We provide insight into the extent of this performance boost and the\nphenomenon that drives it. We leverage this insight to establish an\n$\\tilde{O}(H\\sqrt{SAT})$ Bayesian expected regret bound for PSRL in\nfinite-horizon episodic Markov decision processes, where $H$ is the horizon,\n$S$ is the number of states, $A$ is the number of actions and $T$ is the time\nelapsed. This improves upon the best previous bound of $\\tilde{O}(H S\n\\sqrt{AT})$ for any reinforcement learning algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 11:58:28 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 22:43:10 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 15:54:51 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1607.00325", "submitter": "Morten Kolb{\\ae}k", "authors": "Dong Yu, Morten Kolb{\\ae}k, Zheng-Hua Tan, and Jesper Jensen", "title": "Permutation Invariant Training of Deep Models for Speaker-Independent\n  Multi-talker Speech Separation", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep learning model, which supports permutation invariant\ntraining (PIT), for speaker independent multi-talker speech separation,\ncommonly known as the cocktail-party problem. Different from most of the prior\narts that treat speech separation as a multi-class regression problem and the\ndeep clustering technique that considers it a segmentation (or clustering)\nproblem, our model optimizes for the separation regression error, ignoring the\norder of mixing sources. This strategy cleverly solves the long-lasting label\npermutation problem that has prevented progress on deep learning based\ntechniques for speech separation. Experiments on the equal-energy mixing setup\nof a Danish corpus confirms the effectiveness of PIT. We believe improvements\nbuilt upon PIT can eventually solve the cocktail-party problem and enable\nreal-world adoption of, e.g., automatic meeting transcription and multi-party\nhuman-computer interaction, where overlapping speech is common.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 17:34:16 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 19:57:37 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Yu", "Dong", ""], ["Kolb\u00e6k", "Morten", ""], ["Tan", "Zheng-Hua", ""], ["Jensen", "Jesper", ""]]}, {"id": "1607.00345", "submitter": "Simon Lacoste-Julien", "authors": "Simon Lacoste-Julien", "title": "Convergence Rate of Frank-Wolfe for Non-Convex Objectives", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple proof that the Frank-Wolfe algorithm obtains a stationary\npoint at a rate of $O(1/\\sqrt{t})$ on non-convex objectives with a Lipschitz\ncontinuous gradient. Our analysis is affine invariant and is the first, to the\nbest of our knowledge, giving a similar rate to what was already proven for\nprojected gradient methods (though on slightly different measures of\nstationarity).\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 18:37:33 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Lacoste-Julien", "Simon", ""]]}, {"id": "1607.00360", "submitter": "Aditya Menon", "authors": "Richard Nock and Aditya Krishna Menon and Cheng Soon Ong", "title": "A scaled Bregman theorem with applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bregman divergences play a central role in the design and analysis of a range\nof machine learning algorithms. This paper explores the use of Bregman\ndivergences to establish reductions between such algorithms and their analyses.\nWe present a new scaled isodistortion theorem involving Bregman divergences\n(scaled Bregman theorem for short) which shows that certain \"Bregman\ndistortions'\" (employing a potentially non-convex generator) may be exactly\nre-written as a scaled Bregman divergence computed over transformed data.\nAdmissible distortions include geodesic distances on curved manifolds and\nprojections or gauge-normalisation, while admissible data include scalars,\nvectors and matrices.\n  Our theorem allows one to leverage to the wealth and convenience of Bregman\ndivergences when analysing algorithms relying on the aforementioned Bregman\ndistortions. We illustrate this with three novel applications of our theorem: a\nreduction from multi-class density ratio to class-probability estimation, a new\nadaptive projection free yet norm-enforcing dual norm mirror descent algorithm,\nand a reduction from clustering on flat manifolds to clustering on curved\nmanifolds. Experiments on each of these domains validate the analyses and\nsuggest that the scaled Bregman theorem might be a worthy addition to the\npopular handful of Bregman divergence properties that have been pervasive in\nmachine learning.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 19:27:28 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Nock", "Richard", ""], ["Menon", "Aditya Krishna", ""], ["Ong", "Cheng Soon", ""]]}, {"id": "1607.00410", "submitter": "Yusuke Watanabe Dr.", "authors": "Yusuke Watanabe, Kazuma Hashimoto, Yoshimasa Tsuruoka", "title": "Domain Adaptation for Neural Networks by Parameter Augmentation", "comments": "9 page. To appear in the first ACL Workshop on Representation\n  Learning for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple domain adaptation method for neural networks in a\nsupervised setting. Supervised domain adaptation is a way of improving the\ngeneralization performance on the target domain by using the source domain\ndataset, assuming that both of the datasets are labeled. Recently, recurrent\nneural networks have been shown to be successful on a variety of NLP tasks such\nas caption generation; however, the existing domain adaptation techniques are\nlimited to (1) tune the model parameters by the target dataset after the\ntraining by the source dataset, or (2) design the network to have dual output,\none for the source domain and the other for the target domain. Reformulating\nthe idea of the domain adaptation technique proposed by Daume (2007), we\npropose a simple domain adaptation method, which can be applied to neural\nnetworks trained with a cross-entropy loss. On captioning datasets, we show\nperformance improvements over other domain adaptation methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 21:24:21 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Watanabe", "Yusuke", ""], ["Hashimoto", "Kazuma", ""], ["Tsuruoka", "Yoshimasa", ""]]}, {"id": "1607.00424", "submitter": "Ameet Soni", "authors": "Dileep Viswanathan and Ameet Soni and Jude Shavlik and Sriraam\n  Natarajan", "title": "Learning Relational Dependency Networks for Relation Extraction", "comments": "In Proceedings of Sixth International Workshop on Statistical\n  Relational AI at the 25th International Joint Conference on Artificial\n  Intelligence (IJCAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of KBP slot filling -- extracting relation information\nfrom newswire documents for knowledge base construction. We present our\npipeline, which employs Relational Dependency Networks (RDNs) to learn\nlinguistic patterns for relation extraction. Additionally, we demonstrate how\nseveral components such as weak supervision, word2vec features, joint learning\nand the use of human advice, can be incorporated in this relational framework.\nWe evaluate the different components in the benchmark KBP 2015 task and show\nthat RDNs effectively model a diverse set of features and perform competitively\nwith current state-of-the-art relation extraction.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 22:11:38 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Viswanathan", "Dileep", ""], ["Soni", "Ameet", ""], ["Shavlik", "Jude", ""], ["Natarajan", "Sriraam", ""]]}, {"id": "1607.00435", "submitter": "Jianwen Xie", "authors": "Jianwen Xie, Pamela K. Douglas, Ying Nian Wu, Arthur L. Brody, Ariana\n  E. Anderson", "title": "Decoding the Encoding of Functional Brain Networks: an fMRI\n  Classification Comparison of Non-negative Matrix Factorization (NMF),\n  Independent Component Analysis (ICA), and Sparse Coding Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain networks in fMRI are typically identified using spatial independent\ncomponent analysis (ICA), yet mathematical constraints such as sparse coding\nand positivity both provide alternate biologically-plausible frameworks for\ngenerating brain networks. Non-negative Matrix Factorization (NMF) would\nsuppress negative BOLD signal by enforcing positivity. Spatial sparse coding\nalgorithms ($L1$ Regularized Learning and K-SVD) would impose local\nspecialization and a discouragement of multitasking, where the total observed\nactivity in a single voxel originates from a restricted number of possible\nbrain networks.\n  The assumptions of independence, positivity, and sparsity to encode\ntask-related brain networks are compared; the resulting brain networks for\ndifferent constraints are used as basis functions to encode the observed\nfunctional activity at a given time point. These encodings are decoded using\nmachine learning to compare both the algorithms and their assumptions, using\nthe time series weights to predict whether a subject is viewing a video,\nlistening to an audio cue, or at rest, in 304 fMRI scans from 51 subjects.\n  For classifying cognitive activity, the sparse coding algorithm of $L1$\nRegularized Learning consistently outperformed 4 variations of ICA across\ndifferent numbers of networks and noise levels (p$<$0.001). The NMF algorithms,\nwhich suppressed negative BOLD signal, had the poorest accuracy. Within each\nalgorithm, encodings using sparser spatial networks (containing more\nzero-valued voxels) had higher classification accuracy (p$<$0.001). The success\nof sparse coding algorithms may suggest that algorithms which enforce sparse\ncoding, discourage multitasking, and promote local specialization may capture\nbetter the underlying source processes than those which allow inexhaustible\nlocal processes such as ICA.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 23:48:35 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Xie", "Jianwen", ""], ["Douglas", "Pamela K.", ""], ["Wu", "Ying Nian", ""], ["Brody", "Arthur L.", ""], ["Anderson", "Ariana E.", ""]]}, {"id": "1607.00446", "submitter": "Adam White", "authors": "Martha White and Adam White", "title": "A Greedy Approach to Adapting the Trace Parameter for Temporal\n  Difference Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main obstacles to broad application of reinforcement learning\nmethods is the parameter sensitivity of our core learning algorithms. In many\nlarge-scale applications, online computation and function approximation\nrepresent key strategies in scaling up reinforcement learning algorithms. In\nthis setting, we have effective and reasonably well understood algorithms for\nadapting the learning-rate parameter, online during learning. Such\nmeta-learning approaches can improve robustness of learning and enable\nspecialization to current task, improving learning speed. For\ntemporal-difference learning algorithms which we study here, there is yet\nanother parameter, $\\lambda$, that similarly impacts learning speed and\nstability in practice. Unfortunately, unlike the learning-rate parameter,\n$\\lambda$ parametrizes the objective function that temporal-difference methods\noptimize. Different choices of $\\lambda$ produce different fixed-point\nsolutions, and thus adapting $\\lambda$ online and characterizing the\noptimization is substantially more complex than adapting the learning-rate\nparameter. There are no meta-learning method for $\\lambda$ that can achieve (1)\nincremental updating, (2) compatibility with function approximation, and (3)\nmaintain stability of learning under both on and off-policy sampling. In this\npaper we contribute a novel objective function for optimizing $\\lambda$ as a\nfunction of state rather than time. We derive a new incremental, linear\ncomplexity $\\lambda$-adaption algorithm that does not require offline batch\nupdating or access to a model of the world, and present a suite of experiments\nillustrating the practicality of our new algorithm in three different settings.\nTaken together, our contributions represent a concrete step towards black-box\napplication of temporal-difference learning methods in real world problems.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 01:33:00 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 19:25:51 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["White", "Martha", ""], ["White", "Adam", ""]]}, {"id": "1607.00455", "submitter": "Ehsan Hosseini-Asl", "authors": "Ehsan Hosseini-Asl, Robert Keynto, Ayman El-Baz", "title": "Alzheimer's Disease Diagnostics by Adaptation of 3D Convolutional\n  Network", "comments": "This paper is accepted for publication at IEEE ICIP 2016 conference", "journal-ref": null, "doi": "10.1109/ICIP.2016.7532332", "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis, playing an important role in preventing progress and\ntreating the Alzheimer\\{'}s disease (AD), is based on classification of\nfeatures extracted from brain images. The features have to accurately capture\nmain AD-related variations of anatomical brain structures, such as, e.g.,\nventricles size, hippocampus shape, cortical thickness, and brain volume. This\npaper proposed to predict the AD with a deep 3D convolutional neural network\n(3D-CNN), which can learn generic features capturing AD biomarkers and adapt to\ndifferent domain datasets. The 3D-CNN is built upon a 3D convolutional\nautoencoder, which is pre-trained to capture anatomical shape variations in\nstructural brain MRI scans. Fully connected upper layers of the 3D-CNN are then\nfine-tuned for each task-specific AD classification. Experiments on the\nCADDementia MRI dataset with no skull-stripping preprocessing have shown our\n3D-CNN outperforms several conventional classifiers by accuracy. Abilities of\nthe 3D-CNN to generalize the features learnt and adapt to other domains have\nbeen validated on the ADNI dataset.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 02:55:16 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Hosseini-Asl", "Ehsan", ""], ["Keynto", "Robert", ""], ["El-Baz", "Ayman", ""]]}, {"id": "1607.00466", "submitter": "Parsa Bagherzadeh", "authors": "Parsa Bagherzadeh and Hadi Sadoghi Yazdi", "title": "Outlier absorbing based on a Bayesian approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of outliers is prevalent in machine learning applications and\nmay produce misleading results. In this paper a new method for dealing with\noutliers and anomal samples is proposed. To overcome the outlier issue, the\nproposed method combines the global and local views of the samples. By\ncombination of these views, our algorithm performs in a robust manner. The\nexperimental results show the capabilities of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 04:48:59 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Bagherzadeh", "Parsa", ""], ["Yazdi", "Hadi Sadoghi", ""]]}, {"id": "1607.00474", "submitter": "Shobeir Fakhraei", "authors": "Shobeir Fakhraei, Dhanya Sridhar, Jay Pujara, Lise Getoor", "title": "Adaptive Neighborhood Graph Construction for Inference in\n  Multi-Relational Networks", "comments": "Presented at SIGKDD 12th International Workshop on Mining and\n  Learning with Graphs (MLG'16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A neighborhood graph, which represents the instances as vertices and their\nrelations as weighted edges, is the basis of many semi-supervised and\nrelational models for node labeling and link prediction. Most methods employ a\nsequential process to construct the neighborhood graph. This process often\nconsists of generating a candidate graph, pruning the candidate graph to make a\nneighborhood graph, and then performing inference on the variables (i.e.,\nnodes) in the neighborhood graph. In this paper, we propose a framework that\ncan dynamically adapt the neighborhood graph based on the states of variables\nfrom intermediate inference results, as well as structural properties of the\nrelations connecting them. A key strength of our framework is its ability to\nhandle multi-relational data and employ varying amounts of relations for each\ninstance based on the intermediate inference results. We formulate the link\nprediction task as inference on neighborhood graphs, and include preliminary\nresults illustrating the effects of different strategies in our proposed\nframework.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 07:41:45 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Fakhraei", "Shobeir", ""], ["Sridhar", "Dhanya", ""], ["Pujara", "Jay", ""], ["Getoor", "Lise", ""]]}, {"id": "1607.00485", "submitter": "Simone Scardapane", "authors": "Simone Scardapane, Danilo Comminiello, Amir Hussain, Aurelio Uncini", "title": "Group Sparse Regularization for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2017.02.029", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the joint task of simultaneously optimizing (i)\nthe weights of a deep neural network, (ii) the number of neurons for each\nhidden layer, and (iii) the subset of active input features (i.e., feature\nselection). While these problems are generally dealt with separately, we\npresent a simple regularized formulation allowing to solve all three of them in\nparallel, using standard optimization routines. Specifically, we extend the\ngroup Lasso penalty (originated in the linear regression literature) in order\nto impose group-level sparsity on the network's connections, where each group\nis defined as the set of outgoing weights from a unit. Depending on the\nspecific case, the weights can be related to an input variable, to a hidden\nneuron, or to a bias unit, thus performing simultaneously all the\naforementioned tasks in order to obtain a compact network. We perform an\nextensive experimental evaluation, by comparing with classical weight decay and\nLasso penalties. We show that a sparse version of the group Lasso penalty is\nable to achieve competitive performances, while at the same time resulting in\nextremely compact networks with a smaller number of input features. We evaluate\nboth on a toy dataset for handwritten digit recognition, and on multiple\nrealistic large-scale classification problems.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 09:55:26 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Scardapane", "Simone", ""], ["Comminiello", "Danilo", ""], ["Hussain", "Amir", ""], ["Uncini", "Aurelio", ""]]}, {"id": "1607.00509", "submitter": "Evangelos Psomakelis Mr", "authors": "Evangelos Psomakelis, Fotis Aisopos, Antonios Litke, Konstantinos\n  Tserpes, Magdalini Kardara, Pablo Mart\\'inez Campo", "title": "Big IoT and social networking data for smart cities: Algorithmic\n  improvements on Big Data Analysis in the context of RADICAL city applications", "comments": "Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a SOA (Service Oriented Architecture)-based\nplatform, enabling the retrieval and analysis of big datasets stemming from\nsocial networking (SN) sites and Internet of Things (IoT) devices, collected by\nsmart city applications and socially-aware data aggregation services. A large\nset of city applications in the areas of Participating Urbanism, Augmented\nReality and Sound-Mapping throughout participating cities is being applied,\nresulting into produced sets of millions of user-generated events and online SN\nreports fed into the RADICAL platform. Moreover, we study the application of\ndata analytics such as sentiment analysis to the combined IoT and SN data saved\ninto an SQL database, further investigating algorithmic and configurations to\nminimize delays in dataset processing and results retrieval.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 13:35:02 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Psomakelis", "Evangelos", ""], ["Aisopos", "Fotis", ""], ["Litke", "Antonios", ""], ["Tserpes", "Konstantinos", ""], ["Kardara", "Magdalini", ""], ["Campo", "Pablo Mart\u00ednez", ""]]}, {"id": "1607.00514", "submitter": "Nicolo Colombo", "authors": "Nicolo Colombo and Nikos Vlassis", "title": "Approximate Joint Matrix Triangularization", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximate joint triangularization of a set of\nnoisy jointly diagonalizable real matrices. Approximate joint triangularizers\nare commonly used in the estimation of the joint eigenstructure of a set of\nmatrices, with applications in signal processing, linear algebra, and tensor\ndecomposition. By assuming the input matrices to be perturbations of\nnoise-free, simultaneously diagonalizable ground-truth matrices, the\napproximate joint triangularizers are expected to be perturbations of the exact\njoint triangularizers of the ground-truth matrices. We provide a priori and a\nposteriori perturbation bounds on the `distance' between an approximate joint\ntriangularizer and its exact counterpart. The a priori bounds are theoretical\ninequalities that involve functions of the ground-truth matrices and noise\nmatrices, whereas the a posteriori bounds are given in terms of observable\nquantities that can be computed from the input matrices. From a practical\nperspective, the problem of finding the best approximate joint triangularizer\nof a set of noisy matrices amounts to solving a nonconvex optimization problem.\nWe show that, under a condition on the noise level of the input matrices, it is\npossible to find a good initial triangularizer such that the solution obtained\nby any local descent-type algorithm has certain global guarantees. Finally, we\ndiscuss the application of approximate joint matrix triangularization to\ncanonical tensor decomposition and we derive novel estimation error bounds.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 14:25:58 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Colombo", "Nicolo", ""], ["Vlassis", "Nikos", ""]]}, {"id": "1607.00556", "submitter": "Ehsan Hosseini-Asl", "authors": "Ehsan Hosseini-Asl, Georgy Gimel'farb, Ayman El-Baz", "title": "Alzheimer's Disease Diagnostics by a Deeply Supervised Adaptable 3D\n  Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis, playing an important role in preventing progress and\ntreating the Alzheimer's disease (AD), is based on classification of features\nextracted from brain images. The features have to accurately capture main\nAD-related variations of anatomical brain structures, such as, e.g., ventricles\nsize, hippocampus shape, cortical thickness, and brain volume. This paper\nproposes to predict the AD with a deep 3D convolutional neural network\n(3D-CNN), which can learn generic features capturing AD biomarkers and adapt to\ndifferent domain datasets. The 3D-CNN is built upon a 3D convolutional\nautoencoder, which is pre-trained to capture anatomical shape variations in\nstructural brain MRI scans. Fully connected upper layers of the 3D-CNN are then\nfine-tuned for each task-specific AD classification. Experiments on the\n\\emph{ADNI} MRI dataset with no skull-stripping preprocessing have shown our\n3D-CNN outperforms several conventional classifiers by accuracy and robustness.\nAbilities of the 3D-CNN to generalize the features learnt and adapt to other\ndomains have been validated on the \\emph{CADDementia} dataset.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 19:55:56 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Hosseini-Asl", "Ehsan", ""], ["Gimel'farb", "Georgy", ""], ["El-Baz", "Ayman", ""]]}, {"id": "1607.00567", "submitter": "Yury Maximov", "authors": "Yury Maximov, Massih-Reza Amini, Zaid Harchaoui", "title": "Rademacher Complexity Bounds for a Penalized Multiclass Semi-Supervised\n  Algorithm", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Rademacher complexity bounds for multiclass classifiers trained\nwith a two-step semi-supervised model. In the first step, the algorithm\npartitions the partially labeled data and then identifies dense clusters\ncontaining $\\kappa$ predominant classes using the labeled training examples\nsuch that the proportion of their non-predominant classes is below a fixed\nthreshold. In the second step, a classifier is trained by minimizing a margin\nempirical loss over the labeled training set and a penalization term measuring\nthe disability of the learner to predict the $\\kappa$ predominant classes of\nthe identified clusters. The resulting data-dependent generalization error\nbound involves the margin distribution of the classifier, the stability of the\nclustering technique used in the first step and Rademacher complexity terms\ncorresponding to partially labeled training data. Our theoretical result\nexhibit convergence rates extending those proposed in the literature for the\nbinary case, and experimental results on different multiclass classification\nproblems show empirical evidence that supports the theory.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 22:20:59 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2016 10:13:05 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 08:37:30 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Maximov", "Yury", ""], ["Amini", "Massih-Reza", ""], ["Harchaoui", "Zaid", ""]]}, {"id": "1607.00653", "submitter": "Aditya Grover", "authors": "Aditya Grover, Jure Leskovec", "title": "node2vec: Scalable Feature Learning for Networks", "comments": "In Proceedings of the 22nd ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction tasks over nodes and edges in networks require careful effort in\nengineering features used by learning algorithms. Recent research in the\nbroader field of representation learning has led to significant progress in\nautomating prediction by learning the features themselves. However, present\nfeature learning approaches are not expressive enough to capture the diversity\nof connectivity patterns observed in networks. Here we propose node2vec, an\nalgorithmic framework for learning continuous feature representations for nodes\nin networks. In node2vec, we learn a mapping of nodes to a low-dimensional\nspace of features that maximizes the likelihood of preserving network\nneighborhoods of nodes. We define a flexible notion of a node's network\nneighborhood and design a biased random walk procedure, which efficiently\nexplores diverse neighborhoods. Our algorithm generalizes prior work which is\nbased on rigid notions of network neighborhoods, and we argue that the added\nflexibility in exploring neighborhoods is the key to learning richer\nrepresentations. We demonstrate the efficacy of node2vec over existing\nstate-of-the-art techniques on multi-label classification and link prediction\nin several real-world networks from diverse domains. Taken together, our work\nrepresents a new way for efficiently learning state-of-the-art task-independent\nrepresentations in complex networks.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 16:09:30 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Grover", "Aditya", ""], ["Leskovec", "Jure", ""]]}, {"id": "1607.00662", "submitter": "Danilo Jimenez Rezende", "authors": "Danilo Jimenez Rezende and S. M. Ali Eslami and Shakir Mohamed and\n  Peter Battaglia and Max Jaderberg and Nicolas Heess", "title": "Unsupervised Learning of 3D Structure from Images", "comments": "Appears in Advances in Neural Information Processing Systems 29 (NIPS\n  2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key goal of computer vision is to recover the underlying 3D structure from\n2D observations of the world. In this paper we learn strong deep generative\nmodels of 3D structures, and recover these structures from 3D and 2D images via\nprobabilistic inference. We demonstrate high-quality samples and report\nlog-likelihoods on several datasets, including ShapeNet [2], and establish the\nfirst benchmarks in the literature. We also show how these models and their\ninference networks can be trained end-to-end from 2D images. This demonstrates\nfor the first time the feasibility of learning to infer 3D representations of\nthe world in a purely unsupervised manner.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 17:53:11 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 17:26:53 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Rezende", "Danilo Jimenez", ""], ["Eslami", "S. M. Ali", ""], ["Mohamed", "Shakir", ""], ["Battaglia", "Peter", ""], ["Jaderberg", "Max", ""], ["Heess", "Nicolas", ""]]}, {"id": "1607.00669", "submitter": "Charbel Sakr", "authors": "Charbel Sakr, Ameya Patil, Sai Zhang, Yongjune Kim, Naresh Shanbhag", "title": "Understanding the Energy and Precision Requirements for Online Learning", "comments": "14 pages, 5 figures 4 of which have 2 subfigures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that the precision of data, hyperparameters, and internal\nrepresentations employed in learning systems directly impacts its energy,\nthroughput, and latency. The precision requirements for the training algorithm\nare also important for systems that learn on-the-fly. Prior work has shown that\nthe data and hyperparameters can be quantized heavily without incurring much\npenalty in classification accuracy when compared to floating point\nimplementations. These works suffer from two key limitations. First, they\nassume uniform precision for the classifier and for the training algorithm and\nthus miss out on the opportunity to further reduce precision. Second, prior\nworks are empirical studies. In this article, we overcome both these\nlimitations by deriving analytical lower bounds on the precision requirements\nof the commonly employed stochastic gradient descent (SGD) on-line learning\nalgorithm in the specific context of a support vector machine (SVM). Lower\nbounds on the data precision are derived in terms of the the desired\nclassification accuracy and precision of the hyperparameters used in the\nclassifier. Additionally, lower bounds on the hyperparameter precision in the\nSGD training algorithm are obtained. These bounds are validated using both\nsynthetic and the UCI breast cancer dataset. Additionally, the impact of these\nprecisions on the energy consumption of a fixed-point SVM with on-line training\nis studied.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 18:54:25 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 23:01:12 GMT"}, {"version": "v3", "created": "Fri, 26 Aug 2016 21:56:03 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Sakr", "Charbel", ""], ["Patil", "Ameya", ""], ["Zhang", "Sai", ""], ["Kim", "Yongjune", ""], ["Shanbhag", "Naresh", ""]]}, {"id": "1607.00847", "submitter": "Majdi Khalid", "authors": "Majdi Khalid, Indrakshi Ray, and Hamidreza Chitsaz", "title": "Confidence-Weighted Bipartite Ranking", "comments": "15 pages, 6 tables, and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite ranking is a fundamental machine learning and data mining problem.\nIt commonly concerns the maximization of the AUC metric. Recently, a number of\nstudies have proposed online bipartite ranking algorithms to learn from massive\nstreams of class-imbalanced data. These methods suggest both linear and\nkernel-based bipartite ranking algorithms based on first and second-order\nonline learning. Unlike kernelized ranker, linear ranker is more scalable\nlearning algorithm. The existing linear online bipartite ranking algorithms\nlack either handling non-separable data or constructing adaptive large margin.\nThese limitations yield unreliable bipartite ranking performance. In this work,\nwe propose a linear online confidence-weighted bipartite ranking algorithm\n(CBR) that adopts soft confidence-weighted learning. The proposed algorithm\nleverages the same properties of soft confidence-weighted learning in a\nframework for bipartite ranking. We also develop a diagonal variation of the\nproposed confidence-weighted bipartite ranking algorithm to deal with\nhigh-dimensional data by maintaining only the diagonal elements of the\ncovariance matrix. We empirically evaluate the effectiveness of the proposed\nalgorithms on several benchmark and high-dimensional datasets. The experimental\nresults validate the reliability of the proposed algorithms. The results also\nshow that our algorithms outperform or are at least comparable to the competing\nonline AUC maximization methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 12:21:04 GMT"}, {"version": "v2", "created": "Sat, 9 Jul 2016 06:20:07 GMT"}, {"version": "v3", "created": "Mon, 18 Jul 2016 19:07:44 GMT"}, {"version": "v4", "created": "Sun, 24 Jul 2016 22:04:09 GMT"}, {"version": "v5", "created": "Mon, 26 Sep 2016 19:15:22 GMT"}, {"version": "v6", "created": "Tue, 27 Sep 2016 01:55:44 GMT"}, {"version": "v7", "created": "Wed, 28 Sep 2016 18:21:00 GMT"}, {"version": "v8", "created": "Thu, 24 Nov 2016 20:04:28 GMT"}, {"version": "v9", "created": "Sun, 10 Mar 2019 08:17:05 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Khalid", "Majdi", ""], ["Ray", "Indrakshi", ""], ["Chitsaz", "Hamidreza", ""]]}, {"id": "1607.00872", "submitter": "Patrick O. Glauner", "authors": "Patrick Glauner, Jorge Meira, Lautaro Dolberg, Radu State, Franck\n  Bettinger, Yves Rangoni, Diogo Duarte", "title": "Neighborhood Features Help Detecting Non-Technical Losses in Big Data\n  Sets", "comments": "Proceedings of the 3rd IEEE/ACM International Conference on Big Data\n  Computing Applications and Technologies (BDCAT 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electricity theft is a major problem around the world in both developed and\ndeveloping countries and may range up to 40% of the total electricity\ndistributed. More generally, electricity theft belongs to non-technical losses\n(NTL), which are losses that occur during the distribution of electricity in\npower grids. In this paper, we build features from the neighborhood of\ncustomers. We first split the area in which the customers are located into\ngrids of different sizes. For each grid cell we then compute the proportion of\ninspected customers and the proportion of NTL found among the inspected\ncustomers. We then analyze the distributions of features generated and show why\nthey are useful to predict NTL. In addition, we compute features from the\nconsumption time series of customers. We also use master data features of\ncustomers, such as their customer class and voltage of their connection. We\ncompute these features for a Big Data base of 31M meter readings, 700K\ncustomers and 400K inspection results. We then use these features to train four\nmachine learning algorithms that are particularly suitable for Big Data sets\nbecause of their parallelizable structure: logistic regression, k-nearest\nneighbors, linear support vector machine and random forest. Using the\nneighborhood features instead of only analyzing the time series has resulted in\nappreciable results for Big Data sets for varying NTL proportions of 1%-90%.\nThis work can therefore be deployed to a wide range of different regions around\nthe world.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 13:08:19 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 04:41:44 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Glauner", "Patrick", ""], ["Meira", "Jorge", ""], ["Dolberg", "Lautaro", ""], ["State", "Radu", ""], ["Bettinger", "Franck", ""], ["Rangoni", "Yves", ""], ["Duarte", "Diogo", ""]]}, {"id": "1607.00932", "submitter": "Srinivasan Arunachalam", "authors": "Srinivasan Arunachalam (CWI) and Ronald de Wolf (CWI and U of\n  Amsterdam)", "title": "Optimal Quantum Sample Complexity of Learning Algorithms", "comments": "31 pages LaTeX. Arxiv abstract shortened to fit in their\n  1920-character limit. Version 3: many small changes, no change in results", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $ \\newcommand{\\eps}{\\varepsilon} $In learning theory, the VC dimension of a\nconcept class $C$ is the most common way to measure its \"richness.\" In the PAC\nmodel $$ \\Theta\\Big(\\frac{d}{\\eps} + \\frac{\\log(1/\\delta)}{\\eps}\\Big) $$\nexamples are necessary and sufficient for a learner to output, with probability\n$1-\\delta$, a hypothesis $h$ that is $\\eps$-close to the target concept $c$. In\nthe related agnostic model, where the samples need not come from a $c\\in C$, we\nknow that $$ \\Theta\\Big(\\frac{d}{\\eps^2} + \\frac{\\log(1/\\delta)}{\\eps^2}\\Big)\n$$ examples are necessary and sufficient to output an hypothesis $h\\in C$ whose\nerror is at most $\\eps$ worse than the best concept in $C$.\n  Here we analyze quantum sample complexity, where each example is a coherent\nquantum state. This model was introduced by Bshouty and Jackson, who showed\nthat quantum examples are more powerful than classical examples in some\nfixed-distribution settings. However, Atici and Servedio, improved by Zhang,\nshowed that in the PAC setting, quantum examples cannot be much more powerful:\nthe required number of quantum examples is $$\n\\Omega\\Big(\\frac{d^{1-\\eta}}{\\eps} + d + \\frac{\\log(1/\\delta)}{\\eps}\\Big)\\mbox{\nfor all }\\eta> 0. $$ Our main result is that quantum and classical sample\ncomplexity are in fact equal up to constant factors in both the PAC and\nagnostic models. We give two approaches. The first is a fairly simple\ninformation-theoretic argument that yields the above two classical bounds and\nyields the same bounds for quantum sample complexity up to a $\\log(d/\\eps)$\nfactor. We then give a second approach that avoids the log-factor loss, based\non analyzing the behavior of the \"Pretty Good Measurement\" on the quantum state\nidentification problems that correspond to learning. This shows classical and\nquantum sample complexity are equal up to constant factors.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 15:31:32 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 16:57:38 GMT"}, {"version": "v3", "created": "Tue, 6 Jun 2017 21:14:58 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Arunachalam", "Srinivasan", "", "CWI"], ["de Wolf", "Ronald", "", "CWI and U of\n  Amsterdam"]]}, {"id": "1607.00970", "submitter": "Lili Mou", "authors": "Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, Zhi Jin", "title": "Sequence to Backward and Forward Sequences: A Content-Introducing\n  Approach to Generative Short-Text Conversation", "comments": "Accepted by COLING", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using neural networks to generate replies in human-computer dialogue systems\nis attracting increasing attention over the past few years. However, the\nperformance is not satisfactory: the neural network tends to generate safe,\nuniversally relevant replies which carry little meaning. In this paper, we\npropose a content-introducing approach to neural network-based generative\ndialogue systems. We first use pointwise mutual information (PMI) to predict a\nnoun as a keyword, reflecting the main gist of the reply. We then propose\nseq2BF, a \"sequence to backward and forward sequences\" model, which generates a\nreply containing the given keyword. Experimental results show that our approach\nsignificantly outperforms traditional sequence-to-sequence models in terms of\nhuman evaluation and the entropy measure, and that the predicted keyword can\nappear at an appropriate position in the reply.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 17:42:52 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 07:40:37 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Mou", "Lili", ""], ["Song", "Yiping", ""], ["Yan", "Rui", ""], ["Li", "Ge", ""], ["Zhang", "Lu", ""], ["Jin", "Zhi", ""]]}, {"id": "1607.01027", "submitter": "Tianbao Yang", "authors": "Yi Xu, Qihang Lin, Tianbao Yang", "title": "Accelerate Stochastic Subgradient Method by Leveraging Local Growth\n  Condition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new theory is developed for first-order stochastic convex\noptimization, showing that the global convergence rate is sufficiently\nquantified by a local growth rate of the objective function in a neighborhood\nof the optimal solutions. In particular, if the objective function $F(\\mathbf\nw)$ in the $\\epsilon$-sublevel set grows as fast as $\\|\\mathbf w - \\mathbf\nw_*\\|_2^{1/\\theta}$, where $\\mathbf w_*$ represents the closest optimal\nsolution to $\\mathbf w$ and $\\theta\\in(0,1]$ quantifies the local growth rate,\nthe iteration complexity of first-order stochastic optimization for achieving\nan $\\epsilon$-optimal solution can be $\\widetilde O(1/\\epsilon^{2(1-\\theta)})$,\nwhich is optimal at most up to a logarithmic factor. To achieve the faster\nglobal convergence, we develop two different accelerated stochastic subgradient\nmethods by iteratively solving the original problem approximately in a local\nregion around a historical solution with the size of the local region gradually\ndecreasing as the solution approaches the optimal set. Besides the theoretical\nimprovements, this work also includes new contributions towards making the\nproposed algorithms practical: (i) we present practical variants of accelerated\nstochastic subgradient methods that can run without the knowledge of\nmultiplicative growth constant and even the growth rate $\\theta$; (ii) we\nconsider a broad family of problems in machine learning to demonstrate that the\nproposed algorithms enjoy faster convergence than traditional stochastic\nsubgradient method. We also characterize the complexity of the proposed\nalgorithms for ensuring the gradient is small without the smoothness\nassumption.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 20:01:17 GMT"}, {"version": "v2", "created": "Sun, 4 Dec 2016 03:03:55 GMT"}, {"version": "v3", "created": "Tue, 17 Jan 2017 15:24:58 GMT"}, {"version": "v4", "created": "Sun, 21 Jul 2019 20:44:28 GMT"}, {"version": "v5", "created": "Wed, 6 May 2020 03:21:49 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Xu", "Yi", ""], ["Lin", "Qihang", ""], ["Yang", "Tianbao", ""]]}, {"id": "1607.01036", "submitter": "Jun Han Mr", "authors": "Jun Han, Qiang Liu", "title": "Bootstrap Model Aggregation for Distributed Statistical Learning", "comments": "This paper is about variance reduction on Monte Carol estimation of\n  KL divergence, NIPS, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In distributed, or privacy-preserving learning, we are often given a set of\nprobabilistic models estimated from different local repositories, and asked to\ncombine them into a single model that gives efficient statistical estimation. A\nsimple method is to linearly average the parameters of the local models, which,\nhowever, tends to be degenerate or not applicable on non-convex models, or\nmodels with different parameter dimensions. One more practical strategy is to\ngenerate bootstrap samples from the local models, and then learn a joint model\nbased on the combined bootstrap set. Unfortunately, the bootstrap procedure\nintroduces additional noise and can significantly deteriorate the performance.\nIn this work, we propose two variance reduction methods to correct the\nbootstrap noise, including a weighted M-estimator that is both statistically\nefficient and practically powerful. Both theoretical and empirical analysis is\nprovided to demonstrate our methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 20:12:41 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 20:06:40 GMT"}, {"version": "v3", "created": "Sun, 5 Feb 2017 03:07:51 GMT"}, {"version": "v4", "created": "Mon, 27 Feb 2017 21:01:20 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Han", "Jun", ""], ["Liu", "Qiang", ""]]}, {"id": "1607.01050", "submitter": "Sriraam Natarajan", "authors": "Shuo Yang, Mohammed Korayem, Khalifeh AlJadda, Trey Grainger, Sriraam\n  Natarajan", "title": "Application of Statistical Relational Learning to Hybrid Recommendation\n  Systems", "comments": "Statistical Relational AI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems usually involve exploiting the relations among known\nfeatures and content that describe items (content-based filtering) or the\noverlap of similar users who interacted with or rated the target item\n(collaborative filtering). To combine these two filtering approaches, current\nmodel-based hybrid recommendation systems typically require extensive feature\nengineering to construct a user profile. Statistical Relational Learning (SRL)\nprovides a straightforward way to combine the two approaches. However, due to\nthe large scale of the data used in real world recommendation systems, little\nresearch exists on applying SRL models to hybrid recommendation systems, and\nessentially none of that research has been applied on real big-data-scale\nsystems. In this paper, we proposed a way to adapt the state-of-the-art in SRL\nlearning approaches to construct a real hybrid recommendation system.\nFurthermore, in order to satisfy a common requirement in recommendation systems\n(i.e. that false positives are more undesirable and therefore penalized more\nharshly than false negatives), our approach can also allow tuning the trade-off\nbetween the precision and recall of the system in a principled way. Our\nexperimental results demonstrate the efficiency of our proposed approach as\nwell as its improved performance on recommendation precision.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 21:21:59 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Yang", "Shuo", ""], ["Korayem", "Mohammed", ""], ["AlJadda", "Khalifeh", ""], ["Grainger", "Trey", ""], ["Natarajan", "Sriraam", ""]]}, {"id": "1607.01097", "submitter": "Scott Yang", "authors": "Corinna Cortes, Xavi Gonzalvo, Vitaly Kuznetsov, Mehryar Mohri and\n  Scott Yang", "title": "AdaNet: Adaptive Structural Learning of Artificial Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new algorithms for adaptively learning artificial neural networks.\nOur algorithms (AdaNet) adaptively learn both the structure of the network and\nits weights. They are based on a solid theoretical analysis, including\ndata-dependent generalization guarantees that we prove and discuss in detail.\nWe report the results of large-scale experiments with one of our algorithms on\nseveral binary classification tasks extracted from the CIFAR-10 dataset. The\nresults demonstrate that our algorithm can automatically learn network\nstructures with very competitive performance accuracies when compared with\nthose achieved for neural networks found by standard approaches.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 02:51:33 GMT"}, {"version": "v2", "created": "Sat, 19 Nov 2016 00:46:26 GMT"}, {"version": "v3", "created": "Tue, 28 Feb 2017 02:58:11 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Cortes", "Corinna", ""], ["Gonzalvo", "Xavi", ""], ["Kuznetsov", "Vitaly", ""], ["Mohri", "Mehryar", ""], ["Yang", "Scott", ""]]}, {"id": "1607.01136", "submitter": "Soheil Keshmiri", "authors": "Soheil Keshmiri", "title": "Minimalist Regression Network with Reinforced Gradients and Weighted\n  Estimates: a Case Study on Parameters Estimation in Automated Welding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a minimalist neural regression network as an aggregate of\nindependent identical regression blocks that are trained simultaneously.\nMoreover, it introduces a new multiplicative parameter, shared by all the\nneural units of a given layer, to maintain the quality of its gradients.\nFurthermore, it increases its estimation accuracy via learning a weight factor\nwhose quantity captures the redundancy between the estimated and actual values\nat each training iteration. We choose the estimation of the direct weld\nparameters of different welding techniques to show a significant improvement in\ncalculation of these parameters by our model in contrast to state-of-the-arts\ntechniques in the literature. Furthermore, we demonstrate the ability of our\nmodel to retain its performance when presented with combined data of different\nwelding techniques. This is a nontrivial result in attaining an scalable model\nwhose quality of estimation is independent of adopted welding techniques.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 07:47:37 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Keshmiri", "Soheil", ""]]}, {"id": "1607.01152", "submitter": "Nicolas Goix", "authors": "Nicolas Goix (LTCI)", "title": "How to Evaluate the Quality of Unsupervised Anomaly Detection\n  Algorithms?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When sufficient labeled data are available, classical criteria based on\nReceiver Operating Characteristic (ROC) or Precision-Recall (PR) curves can be\nused to compare the performance of un-supervised anomaly detection algorithms.\nHowever , in many situations, few or no data are labeled. This calls for\nalternative criteria one can compute on non-labeled data. In this paper, two\ncriteria that do not require labels are empirically shown to discriminate\naccurately (w.r.t. ROC or PR based criteria) between algorithms. These criteria\nare based on existing Excess-Mass (EM) and Mass-Volume (MV) curves, which\ngenerally cannot be well estimated in large dimension. A methodology based on\nfeature sub-sampling and aggregating is also described and tested, extending\nthe use of these criteria to high-dimensional datasets and solving major\ndrawbacks inherent to standard EM and MV curves.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 08:58:44 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Goix", "Nicolas", "", "LTCI"]]}, {"id": "1607.01231", "submitter": "Shiqian Ma", "authors": "Xiao Wang, Shiqian Ma, Donald Goldfarb, Wei Liu", "title": "Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization", "comments": "published in SIAM Journal on Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study stochastic quasi-Newton methods for nonconvex\nstochastic optimization, where we assume that noisy information about the\ngradients of the objective function is available via a stochastic first-order\noracle (SFO). We propose a general framework for such methods, for which we\nprove almost sure convergence to stationary points and analyze its worst-case\niteration complexity. When a randomly chosen iterate is returned as the output\nof such an algorithm, we prove that in the worst-case, the SFO-calls complexity\nis $O(\\epsilon^{-2})$ to ensure that the expectation of the squared norm of the\ngradient is smaller than the given accuracy tolerance $\\epsilon$. We also\npropose a specific algorithm, namely a stochastic damped L-BFGS (SdLBFGS)\nmethod, that falls under the proposed framework. {Moreover, we incorporate the\nSVRG variance reduction technique into the proposed SdLBFGS method, and analyze\nits SFO-calls complexity. Numerical results on a nonconvex binary\nclassification problem using SVM, and a multiclass classification problem using\nneural networks are reported.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 12:51:33 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 08:45:20 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2016 07:18:03 GMT"}, {"version": "v4", "created": "Sun, 21 May 2017 06:23:50 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Wang", "Xiao", ""], ["Ma", "Shiqian", ""], ["Goldfarb", "Donald", ""], ["Liu", "Wei", ""]]}, {"id": "1607.01274", "submitter": "Baiyang Wang", "authors": "Baiyang Wang, Diego Klabjan", "title": "Temporal Topic Analysis with Endogenous and Exogenous Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of modeling temporal textual data taking endogenous\nand exogenous processes into account. Such text documents arise in real world\napplications, including job advertisements and economic news articles, which\nare influenced by the fluctuations of the general economy. We propose a\nhierarchical Bayesian topic model which imposes a \"group-correlated\"\nhierarchical structure on the evolution of topics over time incorporating both\nprocesses, and show that this model can be estimated from Markov chain Monte\nCarlo sampling methods. We further demonstrate that this model captures the\nintrinsic relationships between the topic distribution and the time-dependent\nfactors, and compare its performance with latent Dirichlet allocation (LDA) and\ntwo other related models. The model is applied to two collections of documents\nto illustrate its empirical performance: online job advertisements from\nDirectEmployers Association and journalists' postings on BusinessInsider.com.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 01:16:55 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Wang", "Baiyang", ""], ["Klabjan", "Diego", ""]]}, {"id": "1607.01346", "submitter": "Shahid Shah", "authors": "Shahid Mehraj Shah, Krishna Chaitanya A and Vinod Sharma", "title": "Resource Allocation in a MAC with and without security via Game\n  Theoretic Learning", "comments": "27 pages, 12 figures. Part of the paper was presented in 2016 IEEE\n  Information theory and applicaitons (ITA) Workshop, San Diego, USA in Feb.\n  2016. Submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a $K$-user fading multiple access channel with and without\nsecurity constraints is studied. First we consider a F-MAC without the security\nconstraints. Under the assumption of individual CSI of users, we propose the\nproblem of power allocation as a stochastic game when the receiver sends an ACK\nor a NACK depending on whether it was able to decode the message or not. We\nhave used Multiplicative weight no-regret algorithm to obtain a Coarse\nCorrelated Equilibrium (CCE). Then we consider the case when the users can\ndecode ACK/NACK of each other. In this scenario we provide an algorithm to\nmaximize the weighted sum-utility of all the users and obtain a Pareto optimal\npoint. PP is socially optimal but may be unfair to individual users. Next we\nconsider the case where the users can cooperate with each other so as to\ndisagree with the policy which will be unfair to individual user. We then\nobtain a Nash bargaining solution, which in addition to being Pareto optimal,\nis also fair to each user.\n  Next we study a $K$-user fading multiple access wiretap Channel with CSI of\nEve available to the users. We use the previous algorithms to obtain a CCE, PP\nand a NBS.\n  Next we consider the case where each user does not know the CSI of Eve but\nonly its distribution. In that case we use secrecy outage as the criterion for\nthe receiver to send an ACK or a NACK. Here also we use the previous algorithms\nto obtain a CCE, PP or a NBS. Finally we show that our algorithms can be\nextended to the case where a user can transmit at different rates. At the end\nwe provide a few examples to compute different solutions and compare them under\ndifferent CSI scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 17:43:52 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Shah", "Shahid Mehraj", ""], ["A", "Krishna Chaitanya", ""], ["Sharma", "Vinod", ""]]}, {"id": "1607.01354", "submitter": "Kumar Eswaran Dr.", "authors": "Vishwajeet Singh, Killamsetti Ravi Kumar and K Eswaran", "title": "Learning Discriminative Features using Encoder-Decoder type Deep Neural\n  Nets", "comments": "12 pages, 8 figures and 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning is applied to an increasing variety of complex problems,\nwhich are defined by high dimensional and complex data sets, the necessity for\ntask oriented feature learning grows in importance. With the advancement of\nDeep Learning algorithms, various successful feature learning techniques have\nevolved. In this paper, we present a novel way of learning discriminative\nfeatures by training Deep Neural Nets which have Encoder or Decoder type\narchitecture similar to an Autoencoder. We demonstrate that our approach can\nlearn discriminative features which can perform better at pattern\nclassification tasks when the number of training samples is relatively small in\nsize.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 18:46:13 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Singh", "Vishwajeet", ""], ["Kumar", "Killamsetti Ravi", ""], ["Eswaran", "K", ""]]}, {"id": "1607.01400", "submitter": "Young Woong Park", "authors": "Young Woong Park and Diego Klabjan", "title": "An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality\n  in Machine Learning", "comments": null, "journal-ref": "Machine Learning 105 (2016) 199 - 232", "doi": "10.1007/s10994-016-5562-z", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a clustering-based iterative algorithm to solve certain\noptimization problems in machine learning, where we start the algorithm by\naggregating the original data, solving the problem on aggregated data, and then\nin subsequent steps gradually disaggregate the aggregated data. We apply the\nalgorithm to common machine learning problems such as the least absolute\ndeviation regression problem, support vector machines, and semi-supervised\nsupport vector machines. We derive model-specific data aggregation and\ndisaggregation procedures. We also show optimality, convergence, and the\noptimality gap of the approximated solution in each iteration. A computational\nstudy is provided.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 20:04:57 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Park", "Young Woong", ""], ["Klabjan", "Diego", ""]]}, {"id": "1607.01417", "submitter": "Young Woong Park", "authors": "Young Woong Park, Yan Jiang, Diego Klabjan, Loren Williams", "title": "Algorithms for Generalized Cluster-wise Linear Regression", "comments": null, "journal-ref": "INFORMS Journal on Computing 29-2(2017): 301 - 317", "doi": "10.1287/ijoc.2016.0729", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster-wise linear regression (CLR), a clustering problem intertwined with\nregression, is to find clusters of entities such that the overall sum of\nsquared errors from regressions performed over these clusters is minimized,\nwhere each cluster may have different variances. We generalize the CLR problem\nby allowing each entity to have more than one observation, and refer to it as\ngeneralized CLR. We propose an exact mathematical programming based approach\nrelying on column generation, a column generation based heuristic algorithm\nthat clusters predefined groups of entities, a metaheuristic genetic algorithm\nwith adapted Lloyd's algorithm for K-means clustering, a two-stage approach,\nand a modified algorithm of Sp{\\\"a}th \\cite{Spath1979} for solving generalized\nCLR. We examine the performance of our algorithms on a stock keeping unit (SKU)\nclustering problem employed in forecasting halo and cannibalization effects in\npromotions using real-world retail data from a large supermarket chain. In the\nSKU clustering problem, the retailer needs to cluster SKUs based on their\nseasonal effects in response to promotions. The seasonal effects are the\nresults of regressions with predictors being promotion mechanisms and seasonal\ndummies performed over clusters generated. We compare the performance of all\nproposed algorithms for the SKU problem with real-world and synthetic data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 21:04:08 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 21:38:18 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Park", "Young Woong", ""], ["Jiang", "Yan", ""], ["Klabjan", "Diego", ""], ["Williams", "Loren", ""]]}, {"id": "1607.01462", "submitter": "Yingfei Wang", "authors": "Yingfei Wang and Warren Powell", "title": "An optimal learning method for developing personalized treatment regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A treatment regime is a function that maps individual patient information to\na recommended treatment, hence explicitly incorporating the heterogeneity in\nneed for treatment across individuals. Patient responses are dichotomous and\ncan be predicted through an unknown relationship that depends on the patient\ninformation and the selected treatment. The goal is to find the treatments that\nlead to the best patient responses on average. Each experiment is expensive,\nforcing us to learn the most from each experiment. We adopt a Bayesian approach\nboth to incorporate possible prior information and to update our treatment\nregime continuously as information accrues, with the potential to allow smaller\nyet more informative trials and for patients to receive better treatment. By\nformulating the problem as contextual bandits, we introduce a knowledge\ngradient policy to guide the treatment assignment by maximizing the expected\nvalue of information, for which an approximation method is used to overcome\ncomputational challenges. We provide a detailed study on how to make sequential\nmedical decisions under uncertainty to reduce health care costs on a real world\nknee replacement dataset. We use clustering and LASSO to deal with the\nintrinsic sparsity in health datasets. We show experimentally that even though\nthe problem is sparse, through careful selection of physicians (versus picking\nthem at random), we can significantly improve the success rates.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 02:34:21 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Wang", "Yingfei", ""], ["Powell", "Warren", ""]]}, {"id": "1607.01551", "submitter": "Tarun Kathuria", "authors": "Tarun Kathuria, Amit Deshpande", "title": "On Sampling and Greedy MAP Inference of Constrained Determinantal Point\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subset selection problems ask for a small, diverse yet representative subset\nof the given data. When pairwise similarities are captured by a kernel, the\ndeterminants of submatrices provide a measure of diversity or independence of\nitems within a subset. Matroid theory gives another notion of independence,\nthus giving rise to optimization and sampling questions about Determinantal\nPoint Processes (DPPs) under matroid constraints. Partition constraints, as a\nspecial case, arise naturally when incorporating additional labeling or\nclustering information, besides the kernel, in DPPs. Finding the maximum\ndeterminant submatrix under matroid constraints on its row/column indices has\nbeen previously studied. However, the corresponding question of sampling from\nDPPs under matroid constraints has been unresolved, beyond the simple\ncardinality constrained k-DPPs. We give the first polynomial time algorithm to\nsample exactly from DPPs under partition constraints, for any constant number\nof partitions. We complement this by a complexity theoretic barrier that rules\nout such a result under general matroid constraints. Our experiments indicate\nthat partition-constrained DPPs offer more flexibility and more diversity than\nk-DPPs and their naive extensions, while being reasonably efficient in running\ntime. We also show that a simple greedy initialization followed by local search\ngives improved approximation guarantees for the problem of MAP inference from\nk- DPPs on well-conditioned kernels. Our experiments show that this improvement\nis significant for larger values of k, supporting our theoretical result.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 10:40:23 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Kathuria", "Tarun", ""], ["Deshpande", "Amit", ""]]}, {"id": "1607.01582", "submitter": "Gerasimos Spanakis", "authors": "Gerasimos Spanakis and Gerhard Weiss and Anne Roefs", "title": "Bagged Boosted Trees for Classification of Ecological Momentary\n  Assessment Data", "comments": "to be presented at ECAI2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ecological Momentary Assessment (EMA) data is organized in multiple levels\n(per-subject, per-day, etc.) and this particular structure should be taken into\naccount in machine learning algorithms used in EMA like decision trees and its\nvariants. We propose a new algorithm called BBT (standing for Bagged Boosted\nTrees) that is enhanced by a over/under sampling method and can provide better\nestimates for the conditional class probability function. Experimental results\non a real-world dataset show that BBT can benefit EMA data classification and\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 12:10:29 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Spanakis", "Gerasimos", ""], ["Weiss", "Gerhard", ""], ["Roefs", "Anne", ""]]}, {"id": "1607.01668", "submitter": "Xiao Fu", "authors": "Nicholas D. Sidiropoulos, Lieven De Lathauwer, Xiao Fu, Kejun Huang,\n  Evangelos E. Papalexakis, Christos Faloutsos", "title": "Tensor Decomposition for Signal Processing and Machine Learning", "comments": "revised version, overview article", "journal-ref": null, "doi": "10.1109/TSP.2017.2690524", "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensors or {\\em multi-way arrays} are functions of three or more indices\n$(i,j,k,\\cdots)$ -- similar to matrices (two-way arrays), which are functions\nof two indices $(r,c)$ for (row,column). Tensors have a rich history,\nstretching over almost a century, and touching upon numerous disciplines; but\nthey have only recently become ubiquitous in signal and data analytics at the\nconfluence of signal processing, statistics, data mining and machine learning.\nThis overview article aims to provide a good starting point for researchers and\npractitioners interested in learning about and working with tensors. As such,\nit focuses on fundamentals and motivation (using various application examples),\naiming to strike an appropriate balance of breadth {\\em and depth} that will\nenable someone having taken first graduate courses in matrix algebra and\nprobability to get started doing research and/or developing tensor algorithms\nand software. Some background in applied optimization is useful but not\nstrictly required. The material covered includes tensor rank and rank\ndecomposition; basic tensor factorization models and their relationships and\nproperties (including fairly good coverage of identifiability); broad coverage\nof algorithms ranging from alternating optimization to stochastic gradient;\nstatistical performance analysis; and applications ranging from source\nseparation to collaborative filtering, mixture and topic modeling,\nclassification, and multilinear subspace learning.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 15:22:31 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 15:16:53 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Sidiropoulos", "Nicholas D.", ""], ["De Lathauwer", "Lieven", ""], ["Fu", "Xiao", ""], ["Huang", "Kejun", ""], ["Papalexakis", "Evangelos E.", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1607.01690", "submitter": "Cen Wan", "authors": "Cen Wan and Alex A. Freitas", "title": "A New Hierarchical Redundancy Eliminated Tree Augmented Naive Bayes\n  Classifier for Coping with Gene Ontology-based Features", "comments": "International Conference on Machine Learning (ICML 2016)\n  Computational Biology Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tree Augmented Naive Bayes classifier is a type of probabilistic\ngraphical model that can represent some feature dependencies. In this work, we\npropose a Hierarchical Redundancy Eliminated Tree Augmented Naive Bayes\n(HRE-TAN) algorithm, which considers removing the hierarchical redundancy\nduring the classifier learning process, when coping with data containing\nhierarchically structured features. The experiments showed that HRE-TAN obtains\nsignificantly better predictive performance than the conventional Tree\nAugmented Naive Bayes classifier, and enhanced the robustness against\nimbalanced class distributions, in aging-related gene datasets with Gene\nOntology terms used as features.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 16:00:43 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Wan", "Cen", ""], ["Freitas", "Alex A.", ""]]}, {"id": "1607.01719", "submitter": "Baochen Sun", "authors": "Baochen Sun, Kate Saenko", "title": "Deep CORAL: Correlation Alignment for Deep Domain Adaptation", "comments": "Extended Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are able to learn powerful representations from large\nquantities of labeled input data, however they cannot always generalize well\nacross changes in input distributions. Domain adaptation algorithms have been\nproposed to compensate for the degradation in performance due to domain shift.\nIn this paper, we address the case when the target domain is unlabeled,\nrequiring unsupervised adaptation. CORAL is a \"frustratingly easy\" unsupervised\ndomain adaptation method that aligns the second-order statistics of the source\nand target distributions with a linear transformation. Here, we extend CORAL to\nlearn a nonlinear transformation that aligns correlations of layer activations\nin deep neural networks (Deep CORAL). Experiments on standard benchmark\ndatasets show state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 17:35:55 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Sun", "Baochen", ""], ["Saenko", "Kate", ""]]}, {"id": "1607.01842", "submitter": "Barak Shani", "authors": "Steven D. Galbraith, Joel Laity and Barak Shani", "title": "Finding Significant Fourier Coefficients: Clarifications,\n  Simplifications, Applications and Limitations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ideas from Fourier analysis have been used in cryptography for the last three\ndecades. Akavia, Goldwasser and Safra unified some of these ideas to give a\ncomplete algorithm that finds significant Fourier coefficients of functions on\nany finite abelian group. Their algorithm stimulated a lot of interest in the\ncryptography community, especially in the context of `bit security'. This\nmanuscript attempts to be a friendly and comprehensive guide to the tools and\nresults in this field. The intended readership is cryptographers who have heard\nabout these tools and seek an understanding of their mechanics and their\nusefulness and limitations. A compact overview of the algorithm is presented\nwith emphasis on the ideas behind it. We show how these ideas can be extended\nto a `modulus-switching' variant of the algorithm. We survey some applications\nof this algorithm, and explain that several results should be taken in the\nright context. In particular, we point out that some of the most important bit\nsecurity problems are still open. Our original contributions include: a\ndiscussion of the limitations on the usefulness of these tools; an answer to an\nopen question about the modular inversion hidden number problem.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 23:54:40 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 02:15:41 GMT"}, {"version": "v3", "created": "Tue, 31 Jan 2017 13:21:49 GMT"}, {"version": "v4", "created": "Thu, 13 Dec 2018 15:26:07 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Galbraith", "Steven D.", ""], ["Laity", "Joel", ""], ["Shani", "Barak", ""]]}, {"id": "1607.01958", "submitter": "Kalyani Joshi Ms", "authors": "Joshi Kalyani, Prof. H. N. Bharathi, Prof. Rao Jyothi", "title": "Stock trend prediction using news sentiment analysis", "comments": "11 PAGES, 4 FIGURES", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient Market Hypothesis is the popular theory about stock prediction.\nWith its failure much research has been carried in the area of prediction of\nstocks. This project is about taking non quantifiable data such as financial\nnews articles about a company and predicting its future stock trend with news\nsentiment classification. Assuming that news articles have impact on stock\nmarket, this is an attempt to study relationship between news and stock trend.\nTo show this, we created three different classification models which depict\npolarity of news articles being positive or negative. Observations show that RF\nand SVM perform well in all types of testing. Na\\\"ive Bayes gives good result\nbut not compared to the other two. Experiments are conducted to evaluate\nvarious aspects of the proposed model and encouraging results are obtained in\nall of the experiments. The accuracy of the prediction model is more than 80%\nand in comparison with news random labeling with 50% of accuracy; the model has\nincreased the accuracy by 30%.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 10:48:34 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Kalyani", "Joshi", ""], ["Bharathi", "Prof. H. N.", ""], ["Jyothi", "Prof. Rao", ""]]}, {"id": "1607.01963", "submitter": "Liang Lu", "authors": "Liang Lu", "title": "Sequence Training and Adaptation of Highway Deep Neural Networks", "comments": "6 pages, 3 figures, published at IEEE SLT 2016. arXiv admin note:\n  text overlap with arXiv:1610.05812", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highway deep neural network (HDNN) is a type of depth-gated feedforward\nneural network, which has shown to be easier to train with more hidden layers\nand also generalise better compared to conventional plain deep neural networks\n(DNNs). Previously, we investigated a structured HDNN architecture for speech\nrecognition, in which the two gate functions were tied across all the hidden\nlayers, and we were able to train a much smaller model without sacrificing the\nrecognition accuracy. In this paper, we carry on the study of this architecture\nwith sequence-discriminative training criterion and speaker adaptation\ntechniques on the AMI meeting speech recognition corpus. We show that these two\ntechniques improve speech recognition accuracy on top of the model trained with\nthe cross entropy criterion. Furthermore, we demonstrate that the two gate\nfunctions that are tied across all the hidden layers are able to control the\ninformation flow over the whole network, and we can achieve considerable\nimprovements by only updating these gate functions in both sequence training\nand adaptation experiments.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 11:24:51 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 15:19:55 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2016 10:10:30 GMT"}, {"version": "v4", "created": "Thu, 28 Jul 2016 10:23:26 GMT"}, {"version": "v5", "created": "Wed, 22 Mar 2017 15:59:30 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Lu", "Liang", ""]]}, {"id": "1607.01981", "submitter": "David Barber", "authors": "Aleksandar Botev, Guy Lever, David Barber", "title": "Nesterov's Accelerated Gradient and Momentum as approximations to\n  Regularised Update Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unifying framework for adapting the update direction in\ngradient-based iterative optimization methods. As natural special cases we\nre-derive classical momentum and Nesterov's accelerated gradient method,\nlending a new intuitive interpretation to the latter algorithm. We show that a\nnew algorithm, which we term Regularised Gradient Descent, can converge more\nquickly than either Nesterov's algorithm or the classical momentum algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 12:12:11 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 08:05:18 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Botev", "Aleksandar", ""], ["Lever", "Guy", ""], ["Barber", "David", ""]]}, {"id": "1607.02024", "submitter": "Maurizio Filippone", "authors": "Yufei Han, Maurizio Filippone", "title": "Mini-Batch Spectral Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The cost of computing the spectrum of Laplacian matrices hinders the\napplication of spectral clustering to large data sets. While approximations\nrecover computational tractability, they can potentially affect clustering\nperformance. This paper proposes a practical approach to learn spectral\nclustering based on adaptive stochastic gradient optimization. Crucially, the\nproposed approach recovers the exact spectrum of Laplacian matrices in the\nlimit of the iterations, and the cost of each iteration is linear in the number\nof samples. Extensive experimental validation on data sets with up to half a\nmillion samples demonstrate its scalability and its ability to outperform\nstate-of-the-art approximate methods to learn spectral clustering for a given\ncomputational budget.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 14:06:06 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2016 12:52:39 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Han", "Yufei", ""], ["Filippone", "Maurizio", ""]]}, {"id": "1607.02028", "submitter": "Nadir Murru", "authors": "Giuseppe Air\\`o Farulla, Tiziana Armano, Anna Capietto, Nadir Murru,\n  Rosaria Rossini", "title": "Artificial neural networks and fuzzy logic for recognizing alphabet\n  characters and mathematical symbols", "comments": null, "journal-ref": "Lecture Notes in Computer Science, Volume 9759 2016, Computers\n  Helping People with Special Needs, p. 7-14", "doi": "10.1007/978-3-319-41264-1_1", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Character Recognition software (OCR) are important tools for\nobtaining accessible texts. We propose the use of artificial neural networks\n(ANN) in order to develop pattern recognition algorithms capable of recognizing\nboth normal texts and formulae. We present an original improvement of the\nbackpropagation algorithm. Moreover, we describe a novel image segmentation\nalgorithm that exploits fuzzy logic for separating touching characters.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 12:23:47 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Farulla", "Giuseppe Air\u00f2", ""], ["Armano", "Tiziana", ""], ["Capietto", "Anna", ""], ["Murru", "Nadir", ""], ["Rossini", "Rosaria", ""]]}, {"id": "1607.02078", "submitter": "Ritambhara Singh", "authors": "Ritambhara Singh, Jack Lanchantin, Gabriel Robins, and Yanjun Qi", "title": "DeepChrome: Deep-learning for predicting gene expression from histone\n  modifications", "comments": "This work will be originally published in Bioinformatics Journal\n  (ECCB 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Histone modifications are among the most important factors that\ncontrol gene regulation. Computational methods that predict gene expression\nfrom histone modification signals are highly desirable for understanding their\ncombinatorial effects in gene regulation. This knowledge can help in developing\n'epigenetic drugs' for diseases like cancer. Previous studies for quantifying\nthe relationship between histone modifications and gene expression levels\neither failed to capture combinatorial effects or relied on multiple methods\nthat separate predictions and combinatorial analysis. This paper develops a\nunified discriminative framework using a deep convolutional neural network to\nclassify gene expression using histone modification data as input. Our system,\ncalled DeepChrome, allows automatic extraction of complex interactions among\nimportant features. To simultaneously visualize the combinatorial interactions\namong histone modifications, we propose a novel optimization-based technique\nthat generates feature pattern maps from the learnt deep model. This provides\nan intuitive description of underlying epigenetic mechanisms that regulate\ngenes. Results: We show that DeepChrome outperforms state-of-the-art models\nlike Support Vector Machines and Random Forests for gene expression\nclassification task on 56 different cell-types from REMC database. The output\nof our visualization technique not only validates the previous observations but\nalso allows novel insights about combinatorial interactions among histone\nmodification marks, some of which have recently been observed by experimental\nstudies.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 16:50:57 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Singh", "Ritambhara", ""], ["Lanchantin", "Jack", ""], ["Robins", "Gabriel", ""], ["Qi", "Yanjun", ""]]}, {"id": "1607.02173", "submitter": "John Hershey", "authors": "Yusuf Isik, Jonathan Le Roux, Zhuo Chen, Shinji Watanabe, John R.\n  Hershey", "title": "Single-Channel Multi-Speaker Separation using Deep Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep clustering is a recently introduced deep learning architecture that uses\ndiscriminatively trained embeddings as the basis for clustering. It was\nrecently applied to spectrogram segmentation, resulting in impressive results\non speaker-independent multi-speaker separation. In this paper we extend the\nbaseline system with an end-to-end signal approximation objective that greatly\nimproves performance on a challenging speech separation. We first significantly\nimprove upon the baseline system performance by incorporating better\nregularization, larger temporal context, and a deeper architecture, culminating\nin an overall improvement in signal to distortion ratio (SDR) of 10.3 dB\ncompared to the baseline of 6.0 dB for two-speaker separation, as well as a 7.1\ndB SDR improvement for three-speaker separation. We then extend the model to\nincorporate an enhancement layer to refine the signal estimates, and perform\nend-to-end training through both the clustering and enhancement stages to\nmaximize signal fidelity. We evaluate the results using automatic speech\nrecognition. The new signal approximation objective, combined with end-to-end\ntraining, produces unprecedented performance, reducing the word error rate\n(WER) from 89.1% down to 30.8%. This represents a major advancement towards\nsolving the cocktail party problem.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 21:06:48 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Isik", "Yusuf", ""], ["Roux", "Jonathan Le", ""], ["Chen", "Zhuo", ""], ["Watanabe", "Shinji", ""], ["Hershey", "John R.", ""]]}, {"id": "1607.02177", "submitter": "Afshin Oroojlooy Jadid", "authors": "Afshin Oroojlooyjadid and Lawrence Snyder and Martin Tak\\'a\\v{c}", "title": "Applying Deep Learning to the Newsvendor Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The newsvendor problem is one of the most basic and widely applied inventory\nmodels. There are numerous extensions of this problem. If the probability\ndistribution of the demand is known, the problem can be solved analytically.\n  However, approximating the probability distribution is not easy and is prone\nto error; therefore, the resulting solution to the newsvendor problem may be\nnot optimal. To address this issue, we propose an algorithm based on deep\nlearning that optimizes the order quantities for all products based on features\nof the demand data. Our algorithm integrates the forecasting and\ninventory-optimization steps, rather than solving them separately, as is\ntypically done, and does not require knowledge of the probability distributions\nof the demand. Numerical experiments on real-world data suggest that our\nalgorithm outperforms other approaches, including data-driven and machine\nlearning approaches, especially for demands with high volatility. Finally, in\norder to show how this approach can be used for other inventory optimization\nproblems, we provide an extension for (r,Q) policies.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 21:44:53 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 14:02:31 GMT"}, {"version": "v3", "created": "Thu, 31 Aug 2017 13:36:39 GMT"}, {"version": "v4", "created": "Tue, 6 Mar 2018 18:39:00 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Oroojlooyjadid", "Afshin", ""], ["Snyder", "Lawrence", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1607.02241", "submitter": "Darryl Lin", "authors": "Darryl D. Lin and Sachin S. Talathi", "title": "Overcoming Challenges in Fixed Point Training of Deep Convolutional\n  Networks", "comments": "ICML2016 - Workshop on On-Device Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that training deep neural networks, in particular, deep\nconvolutional networks, with aggressively reduced numerical precision is\nchallenging. The stochastic gradient descent algorithm becomes unstable in the\npresence of noisy gradient updates resulting from arithmetic with limited\nnumeric precision. One of the well-accepted solutions facilitating the training\nof low precision fixed point networks is stochastic rounding. However, to the\nbest of our knowledge, the source of the instability in training neural\nnetworks with noisy gradient updates has not been well investigated. This work\nis an attempt to draw a theoretical connection between low numerical precision\nand training algorithm stability. In doing so, we will also propose and verify\nthrough experiments methods that are able to improve the training performance\nof deep convolutional networks in fixed point.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 06:07:03 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Lin", "Darryl D.", ""], ["Talathi", "Sachin S.", ""]]}, {"id": "1607.02303", "submitter": "Huy Phan", "authors": "Huy Phan, Lars Hertel, Marco Maass, Philipp Koch, Alfred Mertins", "title": "CNN-LTE: a Class of 1-X Pooling Convolutional Neural Networks on Label\n  Tree Embeddings for Audio Scene Recognition", "comments": "Task1 technical report for the DCASE2016 challenge. arXiv admin note:\n  text overlap with arXiv:1606.07908", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe in this report our audio scene recognition system submitted to\nthe DCASE 2016 challenge. Firstly, given the label set of the scenes, a label\ntree is automatically constructed. This category taxonomy is then used in the\nfeature extraction step in which an audio scene instance is represented by a\nlabel tree embedding image. Different convolutional neural networks, which are\ntailored for the task at hand, are finally learned on top of the image features\nfor scene recognition. Our system reaches an overall recognition accuracy of\n81.2% and 83.3% and outperforms the DCASE 2016 baseline with absolute\nimprovements of 8.7% and 6.1% on the development and test data, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 10:39:05 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 18:05:00 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Phan", "Huy", ""], ["Hertel", "Lars", ""], ["Maass", "Marco", ""], ["Koch", "Philipp", ""], ["Mertins", "Alfred", ""]]}, {"id": "1607.02306", "submitter": "Huy Phan", "authors": "Huy Phan, Lars Hertel, Marco Maass, Philipp Koch, Alfred Mertins", "title": "CaR-FOREST: Joint Classification-Regression Decision Forests for\n  Overlapping Audio Event Detection", "comments": "Task2 and Task3 technical report for the DCASE2016 challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes our submissions to Task2 and Task3 of the DCASE 2016\nchallenge. The systems aim at dealing with the detection of overlapping audio\nevents in continuous streams, where the detectors are based on random decision\nforests. The proposed forests are jointly trained for classification and\nregression simultaneously. Initially, the training is classification-oriented\nto encourage the trees to select discriminative features from overlapping\nmixtures to separate positive audio segments from the negative ones. The\nregression phase is then carried out to let the positive audio segments vote\nfor the event onsets and offsets, and therefore model the temporal structure of\naudio events. One random decision forest is specifically trained for each event\ncategory of interest. Experimental results on the development data show that\nour systems significantly outperform the baseline on the Task2 evaluation while\nthey are inferior to the baseline in the Task3 evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 10:42:43 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 18:02:09 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Phan", "Huy", ""], ["Hertel", "Lars", ""], ["Maass", "Marco", ""], ["Koch", "Philipp", ""], ["Mertins", "Alfred", ""]]}, {"id": "1607.02310", "submitter": "Tamara Polajnar", "authors": "Tamara Polajnar", "title": "Collaborative Training of Tensors for Compositional Distributional\n  Semantics", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Type-based compositional distributional semantic models present an\ninteresting line of research into functional representations of linguistic\nmeaning. One of the drawbacks of such models, however, is the lack of training\ndata required to train each word-type combination. In this paper we address\nthis by introducing training methods that share parameters between similar\nwords. We show that these methods enable zero-shot learning for words that have\nno training data at all, as well as enabling construction of high-quality\ntensors from very few training examples per word.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 11:01:56 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 14:09:05 GMT"}, {"version": "v3", "created": "Fri, 5 May 2017 11:17:57 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Polajnar", "Tamara", ""]]}, {"id": "1607.02329", "submitter": "Markus Wulfmeier", "authors": "Markus Wulfmeier, Dominic Zeng Wang and Ingmar Posner", "title": "Watch This: Scalable Cost-Function Learning for Path Planning in Urban\n  Environments", "comments": "Accepted for publication in the Proceedings of the 2016 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an approach to learn cost maps for driving in\ncomplex urban environments from a very large number of demonstrations of\ndriving behaviour by human experts. The learned cost maps are constructed\ndirectly from raw sensor measurements, bypassing the effort of manually\ndesigning cost maps as well as features. When deploying the learned cost maps,\nthe trajectories generated not only replicate human-like driving behaviour but\nare also demonstrably robust against systematic errors in putative robot\nconfiguration. To achieve this we deploy a Maximum Entropy based, non-linear\nIRL framework which uses Fully Convolutional Neural Networks (FCNs) to\nrepresent the cost model underlying expert driving behaviour. Using a deep,\nparametric approach enables us to scale efficiently to large datasets and\ncomplex behaviours by being run-time independent of dataset extent during\ndeployment. We demonstrate the scalability and the performance of the proposed\napproach on an ambitious dataset collected over the course of one year\nincluding more than 25k demonstration trajectories extracted from over 120km of\ndriving around pedestrianised areas in the city of Milton Keynes, UK. We\nevaluate the resulting cost representations by showing the advantages over a\ncarefully manually designed cost map and, in addition, demonstrate its\nrobustness to systematic errors by learning precise cost-maps even in the\npresence of system calibration perturbations.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 11:59:51 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Wulfmeier", "Markus", ""], ["Wang", "Dominic Zeng", ""], ["Posner", "Ingmar", ""]]}, {"id": "1607.02413", "submitter": "Jonathan Scarlett", "authors": "Jonathan Scarlett and Volkan Cevher", "title": "Lower Bounds on Active Learning for Graphical Model Selection", "comments": "Accepted to AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.SI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the underlying graph associated with a\nMarkov random field, with the added twist that the decoding algorithm can\niteratively choose which subsets of nodes to sample based on the previous\nsamples, resulting in an active learning setting. Considering both Ising and\nGaussian models, we provide algorithm-independent lower bounds for\nhigh-probability recovery within the class of degree-bounded graphs. Our main\nresults are minimax lower bounds for the active setting that match the best\nknown lower bounds for the passive setting, which in turn are known to be tight\nin several cases of interest. Our analysis is based on Fano's inequality, along\nwith novel mutual information bounds for the active learning setting, and the\napplication of restricted graph ensembles. While we consider ensembles that are\nsimilar or identical to those used in the passive setting, we require different\nanalysis techniques, with a key challenge being bounding a mutual information\nquantity associated with observed subsets of nodes, as opposed to full\nobservations.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 15:25:46 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 16:42:59 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Scarlett", "Jonathan", ""], ["Cevher", "Volkan", ""]]}, {"id": "1607.02444", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi, George Fazekas, Mark Sandler", "title": "Explaining Deep Convolutional Neural Networks on Music Classification", "comments": "7 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep convolutional neural networks (CNNs) have been actively adopted in the\nfield of music information retrieval, e.g. genre classification, mood\ndetection, and chord recognition. However, the process of learning and\nprediction is little understood, particularly when it is applied to\nspectrograms. We introduce auralisation of a CNN to understand its underlying\nmechanism, which is based on a deconvolution procedure introduced in [2].\nAuralisation of a CNN is converting the learned convolutional features that are\nobtained from deconvolution into audio signals. In the experiments and\ndiscussions, we explain trained features of a 5-layer CNN based on the\ndeconvolved spectrograms and auralised signals. The pairwise correlations per\nlayers with varying different musical attributes are also investigated to\nunderstand the evolution of the learnt features. It is shown that in the deep\nlayers, the features are learnt to capture textures, the patterns of continuous\ndistributions, rather than shapes of lines.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 16:40:30 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "George", ""], ["Sandler", "Mark", ""]]}, {"id": "1607.02450", "submitter": "Kush Varshney", "authors": "Kush R. Varshney", "title": "Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning\nin Social Good Applications, which was held on June 24, 2016 in New York.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 16:55:31 GMT"}, {"version": "v2", "created": "Sun, 28 Aug 2016 15:23:47 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Varshney", "Kush R.", ""]]}, {"id": "1607.02467", "submitter": "Marc Dymetman", "authors": "Marc Dymetman, Chunyang Xiao", "title": "Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior\n  Knowledge", "comments": "Updated version of arXiv:1607.02467. Presented at the NIPS-2016 RNN\n  Symposium, Barcelona, December 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce LL-RNNs (Log-Linear RNNs), an extension of Recurrent Neural\nNetworks that replaces the softmax output layer by a log-linear output layer,\nof which the softmax is a special case. This conceptually simple move has two\nmain advantages. First, it allows the learner to combat training data sparsity\nby allowing it to model words (or more generally, output symbols) as complex\ncombinations of attributes without requiring that each combination is directly\nobserved in the training data (as the softmax does). Second, it permits the\ninclusion of flexible prior knowledge in the form of a priori specified modular\nfeatures, where the neural network component learns to dynamically control the\nweights of a log-linear distribution exploiting these features.\n  We conduct experiments in the domain of language modelling of French, that\nexploit morphological prior knowledge and show an important decrease in\nperplexity relative to a baseline RNN.\n  We provide other motivating iillustrations, and finally argue that the\nlog-linear and the neural-network components contribute complementary strengths\nto the LL-RNN: the LL aspect allows the model to incorporate rich prior\nknowledge, while the NN aspect, according to the \"representation learning\"\nparadigm, allows the model to discover novel combination of characteristics.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 17:35:51 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 10:56:22 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Dymetman", "Marc", ""], ["Xiao", "Chunyang", ""]]}, {"id": "1607.02488", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks and Kevin Gimpel", "title": "Adjusting for Dropout Variance in Batch Normalization and Weight\n  Initialization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to adjust for the variance introduced by dropout with corrections\nto weight initialization and Batch Normalization, yielding higher accuracy.\nThough dropout can preserve the expected input to a neuron between train and\ntest, the variance of the input differs. We thus propose a new weight\ninitialization by correcting for the influence of dropout rates and an\narbitrary nonlinearity's influence on variance through simple corrective\nscalars. Since Batch Normalization trained with dropout estimates the variance\nof a layer's incoming distribution with some inputs dropped, the variance also\ndiffers between train and test. After training a network with Batch\nNormalization and dropout, we simply update Batch Normalization's variance\nmoving averages with dropout off and obtain state of the art on CIFAR-10 and\nCIFAR-100 without data augmentation.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 18:39:47 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 18:36:50 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Hendrycks", "Dan", ""], ["Gimpel", "Kevin", ""]]}, {"id": "1607.02531", "submitter": "Kush Varshney", "authors": "Been Kim, Dmitry M. Malioutov, Kush R. Varshney", "title": "Proceedings of the 2016 ICML Workshop on Human Interpretability in\n  Machine Learning (WHI 2016)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the Proceedings of the 2016 ICML Workshop on Human Interpretability\nin Machine Learning (WHI 2016), which was held in New York, NY, June 23, 2016.\n  Invited speakers were Susan Athey, Rich Caruana, Jacob Feldman, Percy Liang,\nand Hanna Wallach.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 21:07:54 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 19:00:49 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Kim", "Been", ""], ["Malioutov", "Dmitry M.", ""], ["Varshney", "Kush R.", ""]]}, {"id": "1607.02533", "submitter": "Alexey Kurakin", "authors": "Alexey Kurakin, Ian Goodfellow and Samy Bengio", "title": "Adversarial examples in the physical world", "comments": "14 pages, 6 figures. Demo available at https://youtu.be/zQ_uMenoBCk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing machine learning classifiers are highly vulnerable to\nadversarial examples. An adversarial example is a sample of input data which\nhas been modified very slightly in a way that is intended to cause a machine\nlearning classifier to misclassify it. In many cases, these modifications can\nbe so subtle that a human observer does not even notice the modification at\nall, yet the classifier still makes a mistake. Adversarial examples pose\nsecurity concerns because they could be used to perform an attack on machine\nlearning systems, even if the adversary has no access to the underlying model.\nUp to now, all previous work have assumed a threat model in which the adversary\ncan feed data directly into the machine learning classifier. This is not always\nthe case for systems operating in the physical world, for example those which\nare using signals from cameras and other sensors as an input. This paper shows\nthat even in such physical world scenarios, machine learning systems are\nvulnerable to adversarial examples. We demonstrate this by feeding adversarial\nimages obtained from cell-phone camera to an ImageNet Inception classifier and\nmeasuring the classification accuracy of the system. We find that a large\nfraction of adversarial examples are classified incorrectly even when perceived\nthrough the camera.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 21:12:11 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 22:57:31 GMT"}, {"version": "v3", "created": "Fri, 4 Nov 2016 20:34:37 GMT"}, {"version": "v4", "created": "Sat, 11 Feb 2017 00:39:39 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Kurakin", "Alexey", ""], ["Goodfellow", "Ian", ""], ["Bengio", "Samy", ""]]}, {"id": "1607.02535", "submitter": "Rose Yu", "authors": "Rose Yu, Yan Liu", "title": "Learning from Multiway Data: Simple and Efficient Tensor Regression", "comments": "10 pages, Proceedings of the 33rd International Conference on Machine\n  Learning (ICML-16), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor regression has shown to be advantageous in learning tasks with\nmulti-directional relatedness. Given massive multiway data, traditional methods\nare often too slow to operate on or suffer from memory bottleneck. In this\npaper, we introduce subsampled tensor projected gradient to solve the problem.\nOur algorithm is impressively simple and efficient. It is built upon projected\ngradient method with fast tensor power iterations, leveraging randomized\nsketching for further acceleration. Theoretical analysis shows that our\nalgorithm converges to the correct solution in fixed number of iterations. The\nmemory requirement grows linearly with the size of the problem. We demonstrate\nsuperior empirical performance on both multi-linear multi-task learning and\nspatio-temporal applications.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 21:40:44 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Yu", "Rose", ""], ["Liu", "Yan", ""]]}, {"id": "1607.02552", "submitter": "Pranav Sakulkar", "authors": "Pranav Sakulkar and Bhaskar Krishnamachari", "title": "Online Learning Schemes for Power Allocation in Energy Harvesting\n  Communications", "comments": "This paper is under submission in the IEEE Transaction on Information\n  Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of power allocation over a time-varying channel with\nunknown distribution in energy harvesting communication systems. In this\nproblem, the transmitter has to choose the transmit power based on the amount\nof stored energy in its battery with the goal of maximizing the average rate\nobtained over time. We model this problem as a Markov decision process (MDP)\nwith the transmitter as the agent, the battery status as the state, the\ntransmit power as the action and the rate obtained as the reward. The average\nreward maximization problem over the MDP can be solved by a linear program (LP)\nthat uses the transition probabilities for the state-action pairs and their\nreward values to choose a power allocation policy. Since the rewards associated\nthe state-action pairs are unknown, we propose two online learning algorithms:\nUCLP and Epoch-UCLP that learn these rewards and adapt their policies along the\nway. The UCLP algorithm solves the LP at each step to decide its current policy\nusing the upper confidence bounds on the rewards, while the Epoch-UCLP\nalgorithm divides the time into epochs, solves the LP only at the beginning of\nthe epochs and follows the obtained policy in that epoch. We prove that the\nreward losses or regrets incurred by both these algorithms are upper bounded by\nconstants. Epoch-UCLP incurs a higher regret compared to UCLP, but reduces the\ncomputational requirements substantially. We also show that the presented\nalgorithms work for online learning in cost minimization problems like the\npacket scheduling with power-delay tradeoff with minor changes.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 23:42:22 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 01:06:24 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Sakulkar", "Pranav", ""], ["Krishnamachari", "Bhaskar", ""]]}, {"id": "1607.02559", "submitter": "Xiaojun Chang", "authors": "Sen Wang and Feiping Nie and Xiaojun Chang and Xue Li and Quan Z.\n  Sheng and Lina Yao", "title": "Uncovering Locally Discriminative Structure for Feature Analysis", "comments": "Accepted by ECML/PKDD2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold structure learning is often used to exploit geometric information\namong data in semi-supervised feature learning algorithms. In this paper, we\nfind that local discriminative information is also of importance for\nsemi-supervised feature learning. We propose a method that utilizes both the\nmanifold structure of data and local discriminant information. Specifically, we\ndefine a local clique for each data point. The k-Nearest Neighbors (kNN) is\nused to determine the structural information within each clique. We then employ\na variant of Fisher criterion model to each clique for local discriminant\nevaluation and sum all cliques as global integration into the framework. In\nthis way, local discriminant information is embedded. Labels are also utilized\nto minimize distances between data from the same class. In addition, we use the\nkernel method to extend our proposed model and facilitate feature learning in a\nhigh-dimensional space after feature mapping. Experimental results show that\nour method is superior to all other compared methods over a number of datasets.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 02:29:53 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Wang", "Sen", ""], ["Nie", "Feiping", ""], ["Chang", "Xiaojun", ""], ["Li", "Xue", ""], ["Sheng", "Quan Z.", ""], ["Yao", "Lina", ""]]}, {"id": "1607.02586", "submitter": "Jiajun Wu", "authors": "Tianfan Xue, Jiajun Wu, Katherine L. Bouman, William T. Freeman", "title": "Visual Dynamics: Probabilistic Future Frame Synthesis via Cross\n  Convolutional Networks", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of synthesizing a number of likely future frames from a\nsingle input image. In contrast to traditional methods, which have tackled this\nproblem in a deterministic or non-parametric way, we propose a novel approach\nthat models future frames in a probabilistic manner. Our probabilistic model\nmakes it possible for us to sample and synthesize many possible future frames\nfrom a single input image. Future frame synthesis is challenging, as it\ninvolves low- and high-level image and motion understanding. We propose a novel\nnetwork structure, namely a Cross Convolutional Network to aid in synthesizing\nfuture frames; this network structure encodes image and motion information as\nfeature maps and convolutional kernels, respectively. In experiments, our model\nperforms well on synthetic data, such as 2D shapes and animated game sprites,\nas well as on real-wold videos. We also show that our model can be applied to\ntasks such as visual analogy-making, and present an analysis of the learned\nnetwork representations.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 08:41:40 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Xue", "Tianfan", ""], ["Wu", "Jiajun", ""], ["Bouman", "Katherine L.", ""], ["Freeman", "William T.", ""]]}, {"id": "1607.02665", "submitter": "Anurag Kumar", "authors": "Anurag Kumar, Bhiksha Raj", "title": "Classifier Risk Estimation under Limited Labeling Resources", "comments": "PAKDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose strategies for estimating performance of a\nclassifier when labels cannot be obtained for the whole test set. The number of\ntest instances which can be labeled is very small compared to the whole test\ndata size. The goal then is to obtain a precise estimate of classifier\nperformance using as little labeling resource as possible. Specifically, we try\nto answer, how to select a subset of the large test set for labeling such that\nthe performance of a classifier estimated on this subset is as close as\npossible to the one on the whole test set. We propose strategies based on\nstratified sampling for selecting this subset. We show that these strategies\ncan reduce the variance in estimation of classifier accuracy by a significant\namount compared to simple random sampling (over 65% in several cases). Hence,\nour proposed methods are much more precise compared to random sampling for\naccuracy estimation under restricted labeling resources. The reduction in\nnumber of samples required (compared to random sampling) to estimate the\nclassifier accuracy with only 1% error is high as 60% in some cases.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 21:18:23 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 20:18:35 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Kumar", "Anurag", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1607.02705", "submitter": "Charmgil Hong", "authors": "Charmgil Hong, Rumi Ghosh, Soundar Srinivasan", "title": "Dealing with Class Imbalance using Thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose thresholding as an approach to deal with class imbalance. We\ndefine the concept of thresholding as a process of determining a decision\nboundary in the presence of a tunable parameter. The threshold is the maximum\nvalue of this tunable parameter where the conditions of a certain decision are\nsatisfied. We show that thresholding is applicable not only for linear\nclassifiers but also for non-linear classifiers. We show that this is the\nimplicit assumption for many approaches to deal with class imbalance in linear\nclassifiers. We then extend this paradigm beyond linear classification and show\nhow non-linear classification can be dealt with under this umbrella framework\nof thresholding. The proposed method can be used for outlier detection in many\nreal-life scenarios like in manufacturing. In advanced manufacturing units,\nwhere the manufacturing process has matured over time, the number of instances\n(or parts) of the product that need to be rejected (based on a strict regime of\nquality tests) becomes relatively rare and are defined as outliers. How to\ndetect these rare parts or outliers beforehand? How to detect combination of\nconditions leading to these outliers? These are the questions motivating our\nresearch. This paper focuses on prediction of outliers and conditions leading\nto outliers using classification. We address the problem of outlier detection\nusing classification. The classes are good parts (those passing the quality\ntests) and bad parts (those failing the quality tests and can be considered as\noutliers). The rarity of outliers transforms this problem into a\nclass-imbalanced classification problem.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 07:34:27 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Hong", "Charmgil", ""], ["Ghosh", "Rumi", ""], ["Srinivasan", "Soundar", ""]]}, {"id": "1607.02763", "submitter": "Oran Richman", "authors": "Oran Richman, Shie Mannor", "title": "How to Allocate Resources For Features Acquisition?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study classification problems where features are corrupted by noise and\nwhere the magnitude of the noise in each feature is influenced by the resources\nallocated to its acquisition. This is the case, for example, when multiple\nsensors share a common resource (power, bandwidth, attention, etc.). We develop\na method for computing the optimal resource allocation for a variety of\nscenarios and derive theoretical bounds concerning the benefit that may arise\nby non-uniform allocation. We further demonstrate the effectiveness of the\ndeveloped method in simulations.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 16:19:00 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Richman", "Oran", ""], ["Mannor", "Shie", ""]]}, {"id": "1607.02793", "submitter": "Xingguo Li", "authors": "Xingguo Li, Tuo Zhao, Raman Arora, Han Liu, Mingyi Hong", "title": "On Faster Convergence of Cyclic Block Coordinate Descent-type Methods\n  for Strongly Convex Minimization", "comments": "Accepted by JLMR", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cyclic block coordinate descent-type (CBCD-type) methods, which performs\niterative updates for a few coordinates (a block) simultaneously throughout the\nprocedure, have shown remarkable computational performance for solving strongly\nconvex minimization problems. Typical applications include many popular\nstatistical machine learning methods such as elastic-net regression, ridge\npenalized logistic regression, and sparse additive regression. Existing\noptimization literature has shown that for strongly convex minimization, the\nCBCD-type methods attain iteration complexity of\n$\\mathcal{O}(p\\log(1/\\epsilon))$, where $\\epsilon$ is a pre-specified accuracy\nof the objective value, and $p$ is the number of blocks. However, such\niteration complexity explicitly depends on $p$, and therefore is at least $p$\ntimes worse than the complexity $\\mathcal{O}(\\log(1/\\epsilon))$ of gradient\ndescent (GD) methods. To bridge this theoretical gap, we propose an improved\nconvergence analysis for the CBCD-type methods. In particular, we first show\nthat for a family of quadratic minimization problems, the iteration complexity\n$\\mathcal{O}(\\log^2(p)\\cdot\\log(1/\\epsilon))$ of the CBCD-type methods matches\nthat of the GD methods in term of dependency on $p$, up to a $\\log^2 p$ factor.\nThus our complexity bounds are sharper than the existing bounds by at least a\nfactor of $p/\\log^2(p)$. We also provide a lower bound to confirm that our\nimproved complexity bounds are tight (up to a $\\log^2 (p)$ factor), under the\nassumption that the largest and smallest eigenvalues of the Hessian matrix do\nnot scale with $p$. Finally, we generalize our analysis to other strongly\nconvex minimization problems beyond quadratic ones.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 23:15:18 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 15:04:11 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 15:40:31 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Li", "Xingguo", ""], ["Zhao", "Tuo", ""], ["Arora", "Raman", ""], ["Liu", "Han", ""], ["Hong", "Mingyi", ""]]}, {"id": "1607.02834", "submitter": "Balasubramanian Sivan", "authors": "Nick Gravin, Yuval Peres, Balasubramanian Sivan", "title": "Tight Lower Bounds for Multiplicative Weights Algorithmic Families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problem of prediction with expert advice and develop\nregret lower bounds for a large family of algorithms for this problem. We\ndevelop simple adversarial primitives, that lend themselves to various\ncombinations leading to sharp lower bounds for many algorithmic families. We\nuse these primitives to show that the classic Multiplicative Weights Algorithm\n(MWA) has a regret of $\\sqrt{\\frac{T \\ln k}{2}}$, there by completely closing\nthe gap between upper and lower bounds. We further show a regret lower bound of\n$\\frac{2}{3}\\sqrt{\\frac{T\\ln k}{2}}$ for a much more general family of\nalgorithms than MWA, where the learning rate can be arbitrarily varied over\ntime, or even picked from arbitrary distributions over time. We also use our\nprimitives to construct adversaries in the geometric horizon setting for MWA to\nprecisely characterize the regret at $\\frac{0.391}{\\sqrt{\\delta}}$ for the case\nof $2$ experts and a lower bound of $\\frac{1}{2}\\sqrt{\\frac{\\ln k}{2\\delta}}$\nfor the case of arbitrary number of experts $k$.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 06:45:46 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 03:04:51 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Gravin", "Nick", ""], ["Peres", "Yuval", ""], ["Sivan", "Balasubramanian", ""]]}, {"id": "1607.02857", "submitter": "Lars Hertel", "authors": "Lars Hertel, Huy Phan, Alfred Mertins", "title": "Classifying Variable-Length Audio Files with All-Convolutional Networks\n  and Masked Global Pooling", "comments": "Technical report for the DCASE-2016 challenge (task 1 and task 4)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We trained a deep all-convolutional neural network with masked global pooling\nto perform single-label classification for acoustic scene classification and\nmulti-label classification for domestic audio tagging in the DCASE-2016\ncontest. Our network achieved an average accuracy of 84.5% on the four-fold\ncross-validation for acoustic scene recognition, compared to the provided\nbaseline of 72.5%, and an average equal error rate of 0.17 for domestic audio\ntagging, compared to the baseline of 0.21. The network therefore improves the\nbaselines by a relative amount of 17% and 19%, respectively. The network only\nconsists of convolutional layers to extract features from the short-time\nFourier transform and one global pooling layer to combine those features. It\nparticularly possesses neither fully-connected layers, besides the\nfully-connected output layer, nor dropout layers.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 08:33:48 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Hertel", "Lars", ""], ["Phan", "Huy", ""], ["Mertins", "Alfred", ""]]}, {"id": "1607.02858", "submitter": "Takuya Kitazawa", "authors": "Takuya Kitazawa", "title": "Incremental Factorization Machines for Persistently Cold-starting Online\n  Item Recommendation", "comments": "4 pages, 6 figures, The 1st Workshop on Profiling User Preferences\n  for Dynamic Online and Real-Time Recommendations, RecSys 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world item recommenders commonly suffer from a persistent cold-start\nproblem which is caused by dynamically changing users and items. In order to\novercome the problem, several context-aware recommendation techniques have been\nrecently proposed. In terms of both feasibility and performance, factorization\nmachine (FM) is one of the most promising methods as generalization of the\nconventional matrix factorization techniques. However, since online algorithms\nare suitable for dynamic data, the static FMs are still inadequate. Thus, this\npaper proposes incremental FMs (iFMs), a general online factorization\nframework, and specially extends iFMs into an online item recommender. The\nproposed framework can be a promising baseline for further development of the\nproduction recommender systems. Evaluation is done empirically both on\nsynthetic and real-world unstable datasets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 08:37:42 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Kitazawa", "Takuya", ""]]}, {"id": "1607.02959", "submitter": "Asish Ghoshal", "authors": "Asish Ghoshal and Jean Honorio", "title": "From Behavior to Sparse Graphical Games: Efficient Recovery of\n  Equilibria", "comments": "Accepted at 54th Annual Allerton Conference on Communication,\n  Control, and Computing (2016)", "journal-ref": "Allerton Conference on Communication, Control, and Computing\n  (2016)", "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of exact recovery of the pure-strategy\nNash equilibria (PSNE) set of a graphical game from noisy observations of joint\nactions of the players alone. We consider sparse linear influence games --- a\nparametric class of graphical games with linear payoffs, and represented by\ndirected graphs of n nodes (players) and in-degree of at most k. We present an\n$\\ell_1$-regularized logistic regression based algorithm for recovering the\nPSNE set exactly, that is both computationally efficient --- i.e. runs in\npolynomial time --- and statistically efficient --- i.e. has logarithmic sample\ncomplexity. Specifically, we show that the sufficient number of samples\nrequired for exact PSNE recovery scales as $\\mathcal{O}(\\mathrm{poly}(k) \\log\nn)$. We also validate our theoretical results using synthetic experiments.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 14:05:16 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 19:56:19 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Ghoshal", "Asish", ""], ["Honorio", "Jean", ""]]}, {"id": "1607.03050", "submitter": "Daniel Jiwoong  Im", "authors": "Daniel Jiwoong Im, Graham W. Taylor", "title": "Learning a metric for class-conditional KNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naive Bayes Nearest Neighbour (NBNN) is a simple and effective framework\nwhich addresses many of the pitfalls of K-Nearest Neighbour (KNN)\nclassification. It has yielded competitive results on several computer vision\nbenchmarks. Its central tenet is that during NN search, a query is not compared\nto every example in a database, ignoring class information. Instead, NN\nsearches are performed within each class, generating a score per class. A key\nproblem with NN techniques, including NBNN, is that they fail when the data\nrepresentation does not capture perceptual (e.g.~class-based) similarity. NBNN\ncircumvents this by using independent engineered descriptors (e.g.~SIFT). To\nextend its applicability outside of image-based domains, we propose to learn a\nmetric which captures perceptual similarity. Similar to how Neighbourhood\nComponents Analysis optimizes a differentiable form of KNN classification, we\npropose \"Class Conditional\" metric learning (CCML), which optimizes a soft form\nof the NBNN selection rule. Typical metric learning algorithms learn either a\nglobal or local metric. However, our proposed method can be adjusted to a\nparticular level of locality by tuning a single parameter. An empirical\nevaluation on classification and retrieval tasks demonstrates that our proposed\nmethod clearly outperforms existing learned distance metrics across a variety\nof image and non-image datasets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 17:29:19 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Im", "Daniel Jiwoong", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1607.03081", "submitter": "Hiva Ghanbari", "authors": "Hiva Ghanbari, Katya Scheinberg", "title": "Proximal Quasi-Newton Methods for Regularized Convex Optimization with\n  Linear and Accelerated Sublinear Convergence Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In [19], a general, inexact, efficient proximal quasi-Newton algorithm for\ncomposite optimization problems has been proposed and a sublinear global\nconvergence rate has been established. In this paper, we analyze the\nconvergence properties of this method, both in the exact and inexact setting,\nin the case when the objective function is strongly convex. We also investigate\na practical variant of this method by establishing a simple stopping criterion\nfor the subproblem optimization. Furthermore, we consider an accelerated\nvariant, based on FISTA [1], to the proximal quasi-Newton algorithm. A similar\naccelerated method has been considered in [7], where the convergence rate\nanalysis relies on very strong impractical assumptions. We present a modified\nanalysis while relaxing these assumptions and perform a practical comparison of\nthe accelerated proximal quasi- Newton algorithm and the regular one. Our\nanalysis and computational results show that acceleration may not bring any\nbenefit in the quasi-Newton setting.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 19:20:06 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 01:08:29 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Ghanbari", "Hiva", ""], ["Scheinberg", "Katya", ""]]}, {"id": "1607.03084", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck and Ronen Eldan and Yin Tat Lee", "title": "Kernel-based methods for bandit convex optimization", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the adversarial convex bandit problem and we build the first\n$\\mathrm{poly}(T)$-time algorithm with $\\mathrm{poly}(n) \\sqrt{T}$-regret for\nthis problem. To do so we introduce three new ideas in the derivative-free\noptimization literature: (i) kernel methods, (ii) a generalization of Bernoulli\nconvolutions, and (iii) a new annealing schedule for exponential weights (with\nincreasing learning rate). The basic version of our algorithm achieves\n$\\tilde{O}(n^{9.5} \\sqrt{T})$-regret, and we show that a simple variant of this\nalgorithm can be run in $\\mathrm{poly}(n \\log(T))$-time per step at the cost of\nan additional $\\mathrm{poly}(n) T^{o(1)}$ factor in the regret. These results\nimprove upon the $\\tilde{O}(n^{11} \\sqrt{T})$-regret and\n$\\exp(\\mathrm{poly}(T))$-time result of the first two authors, and the\n$\\log(T)^{\\mathrm{poly}(n)} \\sqrt{T}$-regret and\n$\\log(T)^{\\mathrm{poly}(n)}$-time result of Hazan and Li. Furthermore we\nconjecture that another variant of the algorithm could achieve\n$\\tilde{O}(n^{1.5} \\sqrt{T})$-regret, and moreover that this regret is\nunimprovable (the current best lower bound being $\\Omega(n \\sqrt{T})$ and it is\nachieved with linear functions). For the simpler situation of zeroth order\nstochastic convex optimization this corresponds to the conjecture that the\noptimal query complexity is of order $n^3 / \\epsilon^2$.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 19:25:07 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Eldan", "Ronen", ""], ["Lee", "Yin Tat", ""]]}, {"id": "1607.03085", "submitter": "Kamil Rocki", "authors": "Kamil Rocki", "title": "Recurrent Memory Array Structures", "comments": "Minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following report introduces ideas augmenting standard Long Short Term\nMemory (LSTM) architecture with multiple memory cells per hidden unit in order\nto improve its generalization capabilities. It considers both deterministic and\nstochastic variants of memory operation. It is shown that the nondeterministic\nArray-LSTM approach improves state-of-the-art performance on character level\ntext prediction achieving 1.402 BPC on enwik8 dataset. Furthermore, this report\nestabilishes baseline neural-based results of 1.12 BPC and 1.19 BPC for enwik9\nand enwik10 datasets respectively.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 19:29:44 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 16:46:33 GMT"}, {"version": "v3", "created": "Sun, 23 Oct 2016 02:01:55 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Rocki", "Kamil", ""]]}, {"id": "1607.03182", "submitter": "Linqi Song", "authors": "Linqi Song", "title": "Stream-based Online Active Learning in a Contextual Multi-Armed Bandit\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stream-based online active learning in a contextual multi-armed\nbandit framework. In this framework, the reward depends on both the arm and the\ncontext. In a stream-based active learning setting, obtaining the ground truth\nof the reward is costly, and the conventional contextual multi-armed bandit\nalgorithm fails to achieve a sublinear regret due to this cost. Hence, the\nalgorithm needs to determine whether or not to request the ground truth of the\nreward at current time slot. In our framework, we consider a stream-based\nactive learning setting in which a query request for the ground truth is sent\nto the annotator, together with some prior information of the ground truth.\nDepending on the accuracy of the prior information, the query cost varies. Our\nalgorithm mainly carries out two operations: the refinement of the context and\narm spaces and the selection of actions. In our algorithm, the partitions of\nthe context space and the arm space are maintained for a certain time slots,\nand then become finer as more information about the rewards accumulates. We use\na strategic way to select the arms and to request the ground truth of the\nreward, aiming to maximize the total reward. We analytically show that the\nregret is sublinear and in the same order with that of the conventional\ncontextual multi-armed bandit algorithms, where no query cost\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 22:08:58 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Song", "Linqi", ""]]}, {"id": "1607.03183", "submitter": "Andrej Risteski", "authors": "Andrej Risteski", "title": "How to calculate partition functions using convex programming\n  hierarchies: provable bounds for variational methods", "comments": "This paper was accepted for presentation at Conference on Learning\n  Theory (COLT) 2016", "journal-ref": "29th Annual Conference on Learning Theory (pp. 1402-1416), 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating partition functions for Ising\nmodels. We make use of recent tools in combinatorial optimization: the\nSherali-Adams and Lasserre convex programming hierarchies, in combination with\nvariational methods to get algorithms for calculating partition functions in\nthese families. These techniques give new, non-trivial approximation guarantees\nfor the partition function beyond the regime of correlation decay. They also\ngeneralize some classical results from statistical physics about the\nCurie-Weiss ferromagnetic Ising model, as well as provide a partition function\ncounterpart of classical results about max-cut on dense graphs\n\\cite{arora1995polynomial}. With this, we connect techniques from two\napparently disparate research areas -- optimization and counting/partition\nfunction approximations. (i.e. \\#-P type of problems).\n  Furthermore, we design to the best of our knowledge the first provable,\nconvex variational methods. Though in the literature there are a host of convex\nversions of variational methods \\cite{wainwright2003tree, wainwright2005new,\nheskes2006convexity, meshi2009convexifying}, they come with no guarantees\n(apart from some extremely special cases, like e.g. the graph has a single\ncycle \\cite{weiss2000correctness}). We consider dense and low threshold rank\ngraphs, and interestingly, the reason our approach works on these types of\ngraphs is because local correlations propagate to global correlations --\ncompletely the opposite of algorithms based on correlation decay. In the\nprocess we design novel entropy approximations based on the low-order moments\nof a distribution.\n  Our proof techniques are very simple and generic, and likely to be applicable\nto many other settings other than Ising models.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 22:10:04 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Risteski", "Andrej", ""]]}, {"id": "1607.03191", "submitter": "Vaneet Aggarwal", "authors": "Wenqi Wang and Shuchin Aeron and Vaneet Aggarwal", "title": "On Deterministic Conditions for Subspace Clustering under Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present deterministic conditions for success of sparse\nsubspace clustering (SSC) under missing data, when data is assumed to come from\na Union of Subspaces (UoS) model. We consider two algorithms, which are\nvariants of SSC with entry-wise zero-filling that differ in terms of the\noptimization problems used to find affinity matrix for spectral clustering. For\nboth the algorithms, we provide deterministic conditions for any pattern of\nmissing data such that perfect clustering can be achieved. We provide extensive\nsets of simulation results for clustering as well as completion of data at\nmissing entries, under the UoS model. Our experimental results indicate that in\ncontrast to the full data case, accurate clustering does not imply accurate\nsubspace identification and completion, indicating the natural order of\nrelative hardness of these problems.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 22:40:31 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Wang", "Wenqi", ""], ["Aeron", "Shuchin", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1607.03195", "submitter": "J Massey Cashore", "authors": "J. Massey Cashore, Lemuel Kumarga, Peter I. Frazier", "title": "Multi-Step Bayesian Optimization for One-Dimensional Feasibility\n  Determination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization methods allocate limited sampling budgets to maximize\nexpensive-to-evaluate functions. One-step-lookahead policies are often used,\nbut computing optimal multi-step-lookahead policies remains a challenge. We\nconsider a specialized Bayesian optimization problem: finding the superlevel\nset of an expensive one-dimensional function, with a Markov process prior. We\ncompute the Bayes-optimal sampling policy efficiently, and characterize the\nsuboptimality of one-step lookahead. Our numerical experiments demonstrate that\nthe one-step lookahead policy is close to optimal in this problem, performing\nwithin 98% of optimal in the experimental settings considered.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 23:09:52 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Cashore", "J. Massey", ""], ["Kumarga", "Lemuel", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1607.03204", "submitter": "Rajiv Khanna", "authors": "Rajiv Khanna, Joydeep Ghosh, Russell Poldrack, Oluwasanmi Koyejo", "title": "Information Projection and Approximate Inference for Structured Sparse\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate inference via information projection has been recently introduced\nas a general-purpose approach for efficient probabilistic inference given\nsparse variables. This manuscript goes beyond classical sparsity by proposing\nefficient algorithms for approximate inference via information projection that\nare applicable to any structure on the set of variables that admits enumeration\nusing a \\emph{matroid}. We show that the resulting information projection can\nbe reduced to combinatorial submodular optimization subject to matroid\nconstraints. Further, leveraging recent advances in submodular optimization, we\nprovide an efficient greedy algorithm with strong optimization-theoretic\nguarantees. The class of probabilistic models that can be expressed in this way\nis quite broad and, as we show, includes group sparse regression, group sparse\nprincipal components analysis and sparse canonical correlation analysis, among\nothers. Moreover, empirical results on simulated data and high dimensional\nneuroimaging data highlight the superior performance of the information\nprojection approach as compared to established baselines for a range of\nprobabilistic models.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 00:11:59 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Khanna", "Rajiv", ""], ["Ghosh", "Joydeep", ""], ["Poldrack", "Russell", ""], ["Koyejo", "Oluwasanmi", ""]]}, {"id": "1607.03250", "submitter": "Hengyuan Hu", "authors": "Hengyuan Hu, Rui Peng, Yu-Wing Tai, Chi-Keung Tang", "title": "Network Trimming: A Data-Driven Neuron Pruning Approach towards\n  Efficient Deep Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art neural networks are getting deeper and wider. While their\nperformance increases with the increasing number of layers and neurons, it is\ncrucial to design an efficient deep architecture in order to reduce\ncomputational and memory costs. Designing an efficient neural network, however,\nis labor intensive requiring many experiments, and fine-tunings. In this paper,\nwe introduce network trimming which iteratively optimizes the network by\npruning unimportant neurons based on analysis of their outputs on a large\ndataset. Our algorithm is inspired by an observation that the outputs of a\nsignificant portion of neurons in a large network are mostly zero, regardless\nof what inputs the network received. These zero activation neurons are\nredundant, and can be removed without affecting the overall accuracy of the\nnetwork. After pruning the zero activation neurons, we retrain the network\nusing the weights before pruning as initialization. We alternate the pruning\nand retraining to further reduce zero activations in a network. Our experiments\non the LeNet and VGG-16 show that we can achieve high compression ratio of\nparameters without losing or even achieving higher accuracy than the original\nnetwork.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 07:43:01 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Hu", "Hengyuan", ""], ["Peng", "Rui", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1607.03313", "submitter": "Andreas Loukas", "authors": "Andreas Loukas and Nathanael Perraudin", "title": "Predicting the evolution of stationary graph signals", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emerging way of tackling the dimensionality issues arising in the modeling\nof a multivariate process is to assume that the inherent data structure can be\ncaptured by a graph. Nevertheless, though state-of-the-art graph-based methods\nhave been successful for many learning tasks, they do not consider\ntime-evolving signals and thus are not suitable for prediction. Based on the\nrecently introduced joint stationarity framework for time-vertex processes,\nthis letter considers multivariate models that exploit the graph topology so as\nto facilitate the prediction. The resulting method yields similar accuracy to\nthe joint (time-graph) mean-squared error estimator but at lower complexity,\nand outperforms purely time-based methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 11:30:30 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Loukas", "Andreas", ""], ["Perraudin", "Nathanael", ""]]}, {"id": "1607.03343", "submitter": "Michael Iliadis", "authors": "Michael Iliadis, Leonidas Spinoulas, Aggelos K. Katsaggelos", "title": "DeepBinaryMask: Learning a Binary Mask for Video Compressive Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel encoder-decoder neural network model\nreferred to as DeepBinaryMask for video compressive sensing. In video\ncompressive sensing one frame is acquired using a set of coded masks (sensing\nmatrix) from which a number of video frames is reconstructed, equal to the\nnumber of coded masks. The proposed framework is an end-to-end model where the\nsensing matrix is trained along with the video reconstruction. The encoder\nlearns the binary elements of the sensing matrix and the decoder is trained to\nrecover the unknown video sequence. The reconstruction performance is found to\nimprove when using the trained sensing mask from the network as compared to\nother mask designs such as random, across a wide variety of compressive sensing\nreconstruction algorithms. Finally, our analysis and discussion offers insights\ninto understanding the characteristics of the trained mask designs that lead to\nthe improved reconstruction quality.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 13:14:02 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 17:21:36 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Iliadis", "Michael", ""], ["Spinoulas", "Leonidas", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "1607.03360", "submitter": "Andrej Risteski", "authors": "Yuanzhi Li, Andrej Risteski", "title": "Approximate maximum entropy principles via Goemans-Williamson with\n  applications to provable variational methods", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well known maximum-entropy principle due to Jaynes, which states that\ngiven mean parameters, the maximum entropy distribution matching them is in an\nexponential family, has been very popular in machine learning due to its\n\"Occam's razor\" interpretation. Unfortunately, calculating the potentials in\nthe maximum-entropy distribution is intractable \\cite{bresler2014hardness}. We\nprovide computationally efficient versions of this principle when the mean\nparameters are pairwise moments: we design distributions that approximately\nmatch given pairwise moments, while having entropy which is comparable to the\nmaximum entropy distribution matching those moments.\n  We additionally provide surprising applications of the approximate maximum\nentropy principle to designing provable variational methods for partition\nfunction calculations for Ising models without any assumptions on the\npotentials of the model. More precisely, we show that in every temperature, we\ncan get approximation guarantees for the log-partition function comparable to\nthose in the low-temperature limit, which is the setting of optimization of\nquadratic forms over the hypercube. \\cite{alon2006approximating}\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 14:09:03 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Li", "Yuanzhi", ""], ["Risteski", "Andrej", ""]]}, {"id": "1607.03401", "submitter": "Qianqian Xu", "authors": "Qianqian Xu, Jiechao Xiong, Xiaochun Cao, and Yuan Yao", "title": "Parsimonious Mixed-Effects HodgeRank for Crowdsourced Preference\n  Aggregation", "comments": "10 pages, ACM Multimedia (full paper) accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In crowdsourced preference aggregation, it is often assumed that all the\nannotators are subject to a common preference or utility function which\ngenerates their comparison behaviors in experiments. However, in reality\nannotators are subject to variations due to multi-criteria, abnormal, or a\nmixture of such behaviors. In this paper, we propose a parsimonious\nmixed-effects model based on HodgeRank, which takes into account both the fixed\neffect that the majority of annotators follows a common linear utility model,\nand the random effect that a small subset of annotators might deviate from the\ncommon significantly and exhibits strongly personalized preferences. HodgeRank\nhas been successfully applied to subjective quality evaluation of multimedia\nand resolves pairwise crowdsourced ranking data into a global consensus ranking\nand cyclic conflicts of interests. As an extension, our proposed methodology\nfurther explores the conflicts of interests through the random effect in\nannotator specific variations. The key algorithm in this paper establishes a\ndynamic path from the common utility to individual variations, with different\nlevels of parsimony or sparsity on personalization, based on newly developed\nLinearized Bregman Algorithms with Inverse Scale Space method. Finally the\nvalidity of the methodology are supported by experiments with both simulated\nexamples and three real-world crowdsourcing datasets, which shows that our\nproposed method exhibits better performance (i.e. smaller test error) compared\nwith HodgeRank due to its parsimonious property.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 15:30:10 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Xu", "Qianqian", ""], ["Xiong", "Jiechao", ""], ["Cao", "Xiaochun", ""], ["Yao", "Yuan", ""]]}, {"id": "1607.03428", "submitter": "Pantita Palittapongarnpim", "authors": "Pantita Palittapongarnpim, Peter Wittek, Ehsan Zahedinejad, Shakib\n  Vedaie, Barry C. Sanders", "title": "Learning in Quantum Control: High-Dimensional Global Optimization for\n  Noisy Quantum Dynamics", "comments": "32 pages, 4 figures, extension of proceedings in ESANN 2016\n  conference submitted to Neurocomputing", "journal-ref": "Neurocomputing 268 (2017) 116-126", "doi": "10.1016/j.neucom.2016.12.087", "report-no": null, "categories": "cs.LG cs.SY quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum control is valuable for various quantum technologies such as\nhigh-fidelity gates for universal quantum computing, adaptive quantum-enhanced\nmetrology, and ultra-cold atom manipulation. Although supervised machine\nlearning and reinforcement learning are widely used for optimizing control\nparameters in classical systems, quantum control for parameter optimization is\nmainly pursued via gradient-based greedy algorithms. Although the quantum\nfitness landscape is often compatible with greedy algorithms, sometimes greedy\nalgorithms yield poor results, especially for large-dimensional quantum\nsystems. We employ differential evolution algorithms to circumvent the\nstagnation problem of non-convex optimization. We improve quantum control\nfidelity for noisy system by averaging over the objective function. To reduce\ncomputational cost, we introduce heuristics for early termination of runs and\nfor adaptive selection of search subspaces. Our implementation is massively\nparallel and vectorized to reduce run time even further. We demonstrate our\nmethods with two examples, namely quantum phase estimation and quantum gate\ndesign, for which we achieve superior fidelity and scalability than obtained\nusing greedy algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 16:17:38 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 21:47:53 GMT"}, {"version": "v3", "created": "Fri, 25 Nov 2016 23:24:10 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Palittapongarnpim", "Pantita", ""], ["Wittek", "Peter", ""], ["Zahedinejad", "Ehsan", ""], ["Vedaie", "Shakib", ""], ["Sanders", "Barry C.", ""]]}, {"id": "1607.03456", "submitter": "Moshe Salhov", "authors": "Amit Bermanis, Aviv Rotbart, Moshe Salhov and Amir Averbuch", "title": "Incomplete Pivoted QR-based Dimensionality Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional big data appears in many research fields such as image\nrecognition, biology and collaborative filtering. Often, the exploration of\nsuch data by classic algorithms is encountered with difficulties due to `curse\nof dimensionality' phenomenon. Therefore, dimensionality reduction methods are\napplied to the data prior to its analysis. Many of these methods are based on\nprincipal components analysis, which is statistically driven, namely they map\nthe data into a low-dimension subspace that preserves significant statistical\nproperties of the high-dimensional data. As a consequence, such methods do not\ndirectly address the geometry of the data, reflected by the mutual distances\nbetween multidimensional data point. Thus, operations such as classification,\nanomaly detection or other machine learning tasks may be affected.\n  This work provides a dictionary-based framework for geometrically driven data\nanalysis that includes dimensionality reduction, out-of-sample extension and\nanomaly detection. It embeds high-dimensional data in a low-dimensional\nsubspace. This embedding preserves the original high-dimensional geometry of\nthe data up to a user-defined distortion rate. In addition, it identifies a\nsubset of landmark data points that constitute a dictionary for the analyzed\ndataset. The dictionary enables to have a natural extension of the\nlow-dimensional embedding to out-of-sample data points, which gives rise to a\ndistortion-based criterion for anomaly detection. The suggested method is\ndemonstrated on synthetic and real-world datasets and achieves good results for\nclassification, anomaly detection and out-of-sample tasks.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 18:20:23 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Bermanis", "Amit", ""], ["Rotbart", "Aviv", ""], ["Salhov", "Moshe", ""], ["Averbuch", "Amir", ""]]}, {"id": "1607.03463", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yuanzhi Li", "title": "LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain", "comments": "first circulated on May 20, 2016; this newer version improves writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study $k$-SVD that is to obtain the first $k$ singular vectors of a matrix\n$A$. Recently, a few breakthroughs have been discovered on $k$-SVD: Musco and\nMusco [1] proved the first gap-free convergence result using the block Krylov\nmethod, Shamir [2] discovered the first variance-reduction stochastic method,\nand Bhojanapalli et al. [3] provided the fastest $O(\\mathsf{nnz}(A) +\n\\mathsf{poly}(1/\\varepsilon))$-time algorithm using alternating minimization.\n  In this paper, we put forward a new and simple LazySVD framework to improve\nthe above breakthroughs. This framework leads to a faster gap-free method\noutperforming [1], and the first accelerated and stochastic method\noutperforming [2]. In the $O(\\mathsf{nnz}(A) + \\mathsf{poly}(1/\\varepsilon))$\nrunning-time regime, LazySVD outperforms [3] in certain parameter regimes\nwithout even using alternating minimization.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 18:41:52 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 18:55:31 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1607.03474", "submitter": "Julian Georg Zilly", "authors": "Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutn\\'ik and\n  J\\\"urgen Schmidhuber", "title": "Recurrent Highway Networks", "comments": "12 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many sequential processing tasks require complex nonlinear transition\nfunctions from one step to the next. However, recurrent neural networks with\n'deep' transition functions remain difficult to train, even when using Long\nShort-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of\nrecurrent networks based on Gersgorin's circle theorem that illuminates several\nmodeling and optimization issues and improves our understanding of the LSTM\ncell. Based on this analysis we propose Recurrent Highway Networks, which\nextend the LSTM architecture to allow step-to-step transition depths larger\nthan one. Several language modeling experiments demonstrate that the proposed\narchitecture results in powerful and efficient models. On the Penn Treebank\ncorpus, solely increasing the transition depth from 1 to 10 improves word-level\nperplexity from 90.6 to 65.4 using the same number of parameters. On the larger\nWikipedia datasets for character prediction (text8 and enwik8), RHNs outperform\nall previous results and achieve an entropy of 1.27 bits per character.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 19:36:50 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 17:07:42 GMT"}, {"version": "v3", "created": "Thu, 27 Oct 2016 19:39:22 GMT"}, {"version": "v4", "created": "Fri, 3 Mar 2017 21:10:42 GMT"}, {"version": "v5", "created": "Tue, 4 Jul 2017 19:29:23 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Zilly", "Julian Georg", ""], ["Srivastava", "Rupesh Kumar", ""], ["Koutn\u00edk", "Jan", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1607.03475", "submitter": "Ping Li", "authors": "Ping Li", "title": "Nystrom Method for Approximating the GMM Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The GMM (generalized min-max) kernel was recently proposed (Li, 2016) as a\nmeasure of data similarity and was demonstrated effective in machine learning\ntasks. In order to use the GMM kernel for large-scale datasets, the prior work\nresorted to the (generalized) consistent weighted sampling (GCWS) to convert\nthe GMM kernel to linear kernel. We call this approach as ``GMM-GCWS''.\n  In the machine learning literature, there is a popular algorithm which we\ncall ``RBF-RFF''. That is, one can use the ``random Fourier features'' (RFF) to\nconvert the ``radial basis function'' (RBF) kernel to linear kernel. It was\nempirically shown in (Li, 2016) that RBF-RFF typically requires substantially\nmore samples than GMM-GCWS in order to achieve comparable accuracies.\n  The Nystrom method is a general tool for computing nonlinear kernels, which\nagain converts nonlinear kernels into linear kernels. We apply the Nystrom\nmethod for approximating the GMM kernel, a strategy which we name as\n``GMM-NYS''. In this study, our extensive experiments on a set of fairly large\ndatasets confirm that GMM-NYS is also a strong competitor of RBF-RFF.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 19:42:40 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1607.03516", "submitter": "Muhammad Ghifary", "authors": "Muhammad Ghifary and W. Bastiaan Kleijn and Mengjie Zhang and David\n  Balduzzi and Wen Li", "title": "Deep Reconstruction-Classification Networks for Unsupervised Domain\n  Adaptation", "comments": "to appear in European Conference on Computer Vision (ECCV) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel unsupervised domain adaptation algorithm\nbased on deep learning for visual object recognition. Specifically, we design a\nnew model called Deep Reconstruction-Classification Network (DRCN), which\njointly learns a shared encoding representation for two tasks: i) supervised\nclassification of labeled source data, and ii) unsupervised reconstruction of\nunlabeled target data.In this way, the learnt representation not only preserves\ndiscriminability, but also encodes useful information from the target domain.\nOur new DRCN model can be optimized by using backpropagation similarly as the\nstandard neural networks.\n  We evaluate the performance of DRCN on a series of cross-domain object\nrecognition tasks, where DRCN provides a considerable improvement (up to ~8% in\naccuracy) over the prior state-of-the-art algorithms. Interestingly, we also\nobserve that the reconstruction pipeline of DRCN transforms images from the\nsource domain into images whose appearance resembles the target dataset. This\nsuggests that DRCN's performance is due to constructing a single composite\nrepresentation that encodes information about both the structure of target\nimages and the classification of source images. Finally, we provide a formal\nanalysis to justify the algorithm's objective in domain adaptation context.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 20:48:58 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 09:58:13 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Ghifary", "Muhammad", ""], ["Kleijn", "W. Bastiaan", ""], ["Zhang", "Mengjie", ""], ["Balduzzi", "David", ""], ["Li", "Wen", ""]]}, {"id": "1607.03547", "submitter": "Ron Appel", "authors": "Ron Appel, Xavier Burgos-Artizzu, Pietro Perona", "title": "Improved Multi-Class Cost-Sensitive Boosting via Estimation of the\n  Minimum-Risk Class", "comments": "Project website: http://www.vision.caltech.edu/~appel/projects/REBEL/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple unified framework for multi-class cost-sensitive\nboosting. The minimum-risk class is estimated directly, rather than via an\napproximation of the posterior distribution. Our method jointly optimizes\nbinary weak learners and their corresponding output vectors, requiring classes\nto share features at each iteration. By training in a cost-sensitive manner,\nweak learners are invested in separating classes whose discrimination is\nimportant, at the expense of less relevant classification boundaries.\nAdditional contributions are a family of loss functions along with proof that\nour algorithm is Boostable in the theoretical sense, as well as an efficient\nprocedure for growing decision trees for use as weak learners. We evaluate our\nmethod on a variety of datasets: a collection of synthetic planar data, common\nUCI datasets, MNIST digits, SUN scenes, and CUB-200 birds. Results show\nstate-of-the-art performance across all datasets against several strong\nbaselines, including non-boosting multi-class approaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 23:56:33 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 19:29:30 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Appel", "Ron", ""], ["Burgos-Artizzu", "Xavier", ""], ["Perona", "Pietro", ""]]}, {"id": "1607.03559", "submitter": "Stefanie Jegelka", "authors": "Chengtao Li, Stefanie Jegelka, Suvrit Sra", "title": "Fast Sampling for Strongly Rayleigh Measures with Application to\n  Determinantal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we consider sampling from (non-homogeneous) strongly Rayleigh\nprobability measures. As an important corollary, we obtain a fast mixing Markov\nChain sampler for Determinantal Point Processes.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 01:22:04 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Li", "Chengtao", ""], ["Jegelka", "Stefanie", ""], ["Sra", "Suvrit", ""]]}, {"id": "1607.03594", "submitter": "Volodymyr Kuleshov", "authors": "Volodymyr Kuleshov and Stefano Ermon", "title": "Estimating Uncertainty Online Against an Adversary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing uncertainty is an important step towards ensuring the safety and\nreliability of machine learning systems. Existing uncertainty estimation\ntechniques may fail when their modeling assumptions are not met, e.g. when the\ndata distribution differs from the one seen at training time. Here, we propose\ntechniques that assess a classification algorithm's uncertainty via calibrated\nprobabilities (i.e. probabilities that match empirical outcome frequencies in\nthe long run) and which are guaranteed to be reliable (i.e. accurate and\ncalibrated) on out-of-distribution input, including input generated by an\nadversary. This represents an extension of classical online learning that\nhandles uncertainty in addition to guaranteeing accuracy under adversarial\nassumptions. We establish formal guarantees for our methods, and we validate\nthem on two real-world problems: question answering and medical diagnosis from\ngenomic data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 05:07:33 GMT"}, {"version": "v2", "created": "Sun, 22 Jan 2017 04:25:30 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Kuleshov", "Volodymyr", ""], ["Ermon", "Stefano", ""]]}, {"id": "1607.03611", "submitter": "Weishan Dong", "authors": "Weishan Dong, Jian Li, Renjie Yao, Changsheng Li, Ting Yuan, Lanjun\n  Wang", "title": "Characterizing Driving Styles with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing driving styles of human drivers using vehicle sensor data,\ne.g., GPS, is an interesting research problem and an important real-world\nrequirement from automotive industries. A good representation of driving\nfeatures can be highly valuable for autonomous driving, auto insurance, and\nmany other application scenarios. However, traditional methods mainly rely on\nhandcrafted features, which limit machine learning algorithms to achieve a\nbetter performance. In this paper, we propose a novel deep learning solution to\nthis problem, which could be the first attempt of extending deep learning to\ndriving behavior analysis based on GPS data. The proposed approach can\neffectively extract high level and interpretable features describing complex\ndriving patterns. It also requires significantly less human experience and\nwork. The power of the learned driving style representations are validated\nthrough the driver identification problem using a large real dataset.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 07:15:30 GMT"}, {"version": "v2", "created": "Sat, 8 Oct 2016 05:21:00 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Dong", "Weishan", ""], ["Li", "Jian", ""], ["Yao", "Renjie", ""], ["Li", "Changsheng", ""], ["Yuan", "Ting", ""], ["Wang", "Lanjun", ""]]}, {"id": "1607.03626", "submitter": "Yehya Abouelnaga", "authors": "Yehya Abouelnaga", "title": "San Francisco Crime Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  San Francisco Crime Classification is an online competition administered by\nKaggle Inc. The competition aims at predicting the future crimes based on a\ngiven set of geographical and time-based features. In this paper, I achieved a\nan accuracy that ranks at top %18, as of May 19th, 2016. I will explore the\ndata, and explain in details the tools I used to achieve that result.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 08:03:35 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Abouelnaga", "Yehya", ""]]}, {"id": "1607.03681", "submitter": "Yong Xu", "authors": "Yong Xu, Qiang Huang, Wenwu Wang, Peter Foster, Siddharth Sigtia,\n  Philip J. B. Jackson, and Mark D. Plumbley", "title": "Unsupervised Feature Learning Based on Deep Models for Environmental\n  Audio Tagging", "comments": "10 pages, dcase 2016 challenge", "journal-ref": "IEEE/ACM Transactions on Audio, Speech and Language Processing\n  25(6):1230-1241, Jun 2017", "doi": "10.1109/TASLP.2017.2690563", "report-no": null, "categories": "cs.SD cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental audio tagging aims to predict only the presence or absence of\ncertain acoustic events in the interested acoustic scene. In this paper we make\ncontributions to audio tagging in two parts, respectively, acoustic modeling\nand feature learning. We propose to use a shrinking deep neural network (DNN)\nframework incorporating unsupervised feature learning to handle the multi-label\nclassification task. For the acoustic modeling, a large set of contextual\nframes of the chunk are fed into the DNN to perform a multi-label\nclassification for the expected tags, considering that only chunk (or\nutterance) level rather than frame-level labels are available. Dropout and\nbackground noise aware training are also adopted to improve the generalization\ncapability of the DNNs. For the unsupervised feature learning, we propose to\nuse a symmetric or asymmetric deep de-noising auto-encoder (sDAE or aDAE) to\ngenerate new data-driven features from the Mel-Filter Banks (MFBs) features.\nThe new features, which are smoothed against background noise and more compact\nwith contextual information, can further improve the performance of the DNN\nbaseline. Compared with the standard Gaussian Mixture Model (GMM) baseline of\nthe DCASE 2016 audio tagging challenge, our proposed method obtains a\nsignificant equal error rate (EER) reduction from 0.21 to 0.13 on the\ndevelopment set. The proposed aDAE system can get a relative 6.7% EER reduction\ncompared with the strong DNN baseline on the development set. Finally, the\nresults also show that our approach obtains the state-of-the-art performance\nwith 0.15 EER on the evaluation set of the DCASE 2016 audio tagging task while\nEER of the first prize of this challenge is 0.17.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 11:31:14 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 15:56:36 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Xu", "Yong", ""], ["Huang", "Qiang", ""], ["Wang", "Wenwu", ""], ["Foster", "Peter", ""], ["Sigtia", "Siddharth", ""], ["Jackson", "Philip J. B.", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1607.03682", "submitter": "Yong Xu", "authors": "Yong Xu, Qiang Huang, Wenwu Wang, Mark D. Plumbley", "title": "Hierarchical learning for DNN-based acoustic scene classification", "comments": "5 pages, DCASE 2016 challenge workshop paper, poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a deep neural network (DNN)-based acoustic scene\nclassification framework. Two hierarchical learning methods are proposed to\nimprove the DNN baseline performance by incorporating the hierarchical taxonomy\ninformation of environmental sounds. Firstly, the parameters of the DNN are\ninitialized by the proposed hierarchical pre-training. Multi-level objective\nfunction is then adopted to add more constraint on the cross-entropy based loss\nfunction. A series of experiments were conducted on the Task1 of the Detection\nand Classification of Acoustic Scenes and Events (DCASE) 2016 challenge. The\nfinal DNN-based system achieved a 22.9% relative improvement on average scene\nclassification error as compared with the Gaussian Mixture Model (GMM)-based\nbenchmark system across four standard folds.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 11:31:25 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 15:32:42 GMT"}, {"version": "v3", "created": "Sat, 13 Aug 2016 10:37:53 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Xu", "Yong", ""], ["Huang", "Qiang", ""], ["Wang", "Wenwu", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1607.03691", "submitter": "Gabriella Contardo", "authors": "Gabriella Contardo, Ludovic Denoyer, Thierry Arti\\`eres", "title": "Sequential Cost-Sensitive Feature Acquisition", "comments": "12 pages, conference : accepted at IDA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a reinforcement learning based approach to tackle the\ncost-sensitive learning problem where each input feature has a specific cost.\nThe acquisition process is handled through a stochastic policy which allows\nfeatures to be acquired in an adaptive way. The general architecture of our\napproach relies on representation learning to enable performing prediction on\nany partially observed sample, whatever the set of its observed features are.\nThe resulting model is an original mix of representation learning and of\nreinforcement learning ideas. It is learned with policy gradient techniques to\nminimize a budgeted inference cost. We demonstrate the effectiveness of our\nproposed method with several experiments on a variety of datasets for the\nsparse prediction problem where all features have the same cost, but also for\nsome cost-sensitive settings.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 12:10:08 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Contardo", "Gabriella", ""], ["Denoyer", "Ludovic", ""], ["Arti\u00e8res", "Thierry", ""]]}, {"id": "1607.03705", "submitter": "Philippe Leray", "authors": "Maroua Haddad (LINA, LARODEC), Philippe Leray (LINA), Nahla Ben Amor\n  (LARODEC)", "title": "Possibilistic Networks: Parameters Learning from Imprecise Data and\n  Evaluation strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an ever-increasing interest in multidisciplinary research on\nrepresenting and reasoning with imperfect data. Possibilistic networks present\none of the powerful frameworks of interest for representing uncertain and\nimprecise information. This paper covers the problem of their parameters\nlearning from imprecise datasets, i.e., containing multi-valued data. We\npropose in the rst part of this paper a possibilistic networks sampling\nprocess. In the second part, we propose a likelihood function which explores\nthe link between random sets theory and possibility theory. This function is\nthen deployed to parametrize possibilistic networks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 12:45:53 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Haddad", "Maroua", "", "LINA, LARODEC"], ["Leray", "Philippe", "", "LINA"], ["Amor", "Nahla Ben", "", "LARODEC"]]}, {"id": "1607.03707", "submitter": "Hwiyeol Jo", "authors": "Hwiyeol Jo, Yohan Moon, Jong In Kim, and Jeong Ryu", "title": "Re-presenting a Story by Emotional Factors using Sentimental Analysis\n  Method", "comments": "Paper version of CogSci2016; We should correct poor English", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remembering an event is affected by personal emotional status. We examined\nthe psychological status and personal factors; depression (Center for\nEpidemiological Studies - Depression, Radloff, 1977), present affective\n(Positive Affective and Negative Affective Schedule, Watson et al., 1988), life\norient (Life Orient Test, Scheier & Carver, 1985), self-awareness (Core Self\nEvaluation Scale, Judge et al., 2003), and social factor (Social Support,\nSarason et al., 1983) of undergraduate students (N=64) and got summaries of a\nstory, Chronicle of a Death Foretold (Gabriel Garcia Marquez, 1981) from them.\nWe implement a sentimental analysis model based on convolutional neural network\n(LeCun & Bengio, 1995) to evaluate each summary. From the same vein used for\ntransfer learning (Pan & Yang, 2010), we collected 38,265 movie review data to\ntrain the model and then use them to score summaries of each student. The\nresults of CES-D and PANAS show the relationship between emotion and memory\nretrieval as follows: depressed people have shown a tendency of representing a\nstory more negatively, and they seemed less expressive. People with full of\nemotion - high in PANAS - have retrieved their memory more expressively than\nothers, using more negative words then others. The contributions of this study\ncan be summarized as follows: First, lightening the relationship between\nemotion and its effect during times of storing or retrieving a memory. Second,\nsuggesting objective methods to evaluate the intensity of emotion in natural\nlanguage format, using a sentimental analysis model.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 12:48:33 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Jo", "Hwiyeol", ""], ["Moon", "Yohan", ""], ["Kim", "Jong In", ""], ["Ryu", "Jeong", ""]]}, {"id": "1607.03730", "submitter": "Hamid Dadkhahi", "authors": "Hamid Dadkhahi, Nazir Saleheen, Santosh Kumar, Benjamin Marlin", "title": "Learning Shallow Detection Cascades for Wearable Sensor-Based Mobile\n  Health Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of mobile health aims to leverage recent advances in wearable\non-body sensing technology and smart phone computing capabilities to develop\nsystems that can monitor health states and deliver just-in-time adaptive\ninterventions. However, existing work has largely focused on analyzing\ncollected data in the off-line setting. In this paper, we propose a novel\napproach to learning shallow detection cascades developed explicitly for use in\na real-time wearable-phone or wearable-phone-cloud systems. We apply our\napproach to the problem of cigarette smoking detection from a combination of\nwrist-worn actigraphy data and respiration chest band data using two and three\nstage cascades.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 13:47:49 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Dadkhahi", "Hamid", ""], ["Saleheen", "Nazir", ""], ["Kumar", "Santosh", ""], ["Marlin", "Benjamin", ""]]}, {"id": "1607.03780", "submitter": "James Henderson", "authors": "James Henderson and Diana Nicoleta Popa", "title": "A Vector Space for Distributional Semantics for Entailment", "comments": "To appear in Proc. 54th Annual Meeting of the Association\n  Computational Linguistics (ACL 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributional semantics creates vector-space representations that capture\nmany forms of semantic similarity, but their relation to semantic entailment\nhas been less clear. We propose a vector-space model which provides a formal\nfoundation for a distributional semantics of entailment. Using a mean-field\napproximation, we develop approximate inference procedures and entailment\noperators over vectors of probabilities of features being known (versus\nunknown). We use this framework to reinterpret an existing\ndistributional-semantic model (Word2Vec) as approximating an entailment-based\nmodel of the distributions of words in contexts, thereby predicting lexical\nentailment relations. In both unsupervised and semi-supervised experiments on\nhyponymy detection, we get substantial improvements over previous results.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 15:08:26 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Henderson", "James", ""], ["Popa", "Diana Nicoleta", ""]]}, {"id": "1607.03822", "submitter": "Tony Basil", "authors": "Choudur Lakshminarayan and Tony Basil", "title": "Feature Extraction and Automated Classification of Heartbeats by Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present algorithms for the detection of a class of heart arrhythmias with\nthe goal of eventual adoption by practicing cardiologists. In clinical\npractice, detection is based on a small number of meaningful features extracted\nfrom the heartbeat cycle. However, techniques proposed in the literature use\nhigh dimensional vectors consisting of morphological, and time based features\nfor detection. Using electrocardiogram (ECG) signals, we found smaller subsets\nof features sufficient to detect arrhythmias with high accuracy. The features\nwere found by an iterative step-wise feature selection method. We depart from\ncommon literature in the following aspects: 1. As opposed to a high dimensional\nfeature vectors, we use a small set of features with meaningful clinical\ninterpretation, 2. we eliminate the necessity of short-duration\npatient-specific ECG data to append to the global training data for\nclassification 3. We apply semi-parametric classification procedures (in an\nensemble framework) for arrhythmia detection, and 4. our approach is based on a\nreduced sampling rate of ~ 115 Hz as opposed to 360 Hz in standard literature.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 16:46:55 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Lakshminarayan", "Choudur", ""], ["Basil", "Tony", ""]]}, {"id": "1607.03827", "submitter": "Matthias Plappert", "authors": "Matthias Plappert, Christian Mandery, Tamim Asfour", "title": "The KIT Motion-Language Dataset", "comments": "5 figures, 4 tables, submitted to Big Data journal, Special Issue on\n  Robotics", "journal-ref": null, "doi": "10.1089/big.2016.0028", "report-no": null, "categories": "cs.RO cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linking human motion and natural language is of great interest for the\ngeneration of semantic representations of human activities as well as for the\ngeneration of robot activities based on natural language input. However, while\nthere have been years of research in this area, no standardized and openly\navailable dataset exists to support the development and evaluation of such\nsystems. We therefore propose the KIT Motion-Language Dataset, which is large,\nopen, and extensible. We aggregate data from multiple motion capture databases\nand include them in our dataset using a unified representation that is\nindependent of the capture system or marker set, making it easy to work with\nthe data regardless of its origin. To obtain motion annotations in natural\nlanguage, we apply a crowd-sourcing approach and a web-based tool that was\nspecifically build for this purpose, the Motion Annotation Tool. We thoroughly\ndocument the annotation process itself and discuss gamification methods that we\nused to keep annotators motivated. We further propose a novel method,\nperplexity-based selection, which systematically selects motions for further\nannotation that are either under-represented in our dataset or that have\nerroneous annotations. We show that our method mitigates the two aforementioned\nproblems and ensures a systematic annotation process. We provide an in-depth\nanalysis of the structure and contents of our resulting dataset, which, as of\nOctober 10, 2016, contains 3911 motions with a total duration of 11.23 hours\nand 6278 annotations in natural language that contain 52,903 words. We believe\nthis makes our dataset an excellent choice that enables more transparent and\ncomparable research in this important area.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 17:08:01 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 14:24:47 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Plappert", "Matthias", ""], ["Mandery", "Christian", ""], ["Asfour", "Tamim", ""]]}, {"id": "1607.03849", "submitter": "Piotr Beben", "authors": "Piotr Beben", "title": "Fitting a Simplicial Complex using a Variation of k-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple and effective two stage algorithm for approximating a point\ncloud $\\mathcal{S}\\subset\\mathbb{R}^m$ by a simplicial complex $K$. The first\nstage is an iterative fitting procedure that generalizes k-means clustering,\nwhile the second stage involves deleting redundant simplices. A form of\ndimension reduction of $\\mathcal{S}$ is obtained as a consequence.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 18:15:52 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 15:34:40 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Beben", "Piotr", ""]]}, {"id": "1607.03967", "submitter": "Johann Bengua", "authors": "Johann A. Bengua, Hoang D. Tuan, Ho N. Phien, Minh N. Do", "title": "Concatenated image completion via tensor augmentation and completion", "comments": "7 pages, 6 figures, submitted to ICSPCS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel framework called concatenated image completion\nvia tensor augmentation and completion (ICTAC), which recovers missing entries\nof color images with high accuracy. Typical images are second- or third-order\ntensors (2D/3D) depending if they are grayscale or color, hence tensor\ncompletion algorithms are ideal for their recovery. The proposed framework\nperforms image completion by concatenating copies of a single image that has\nmissing entries into a third-order tensor, applying a dimensionality\naugmentation technique to the tensor, utilizing a tensor completion algorithm\nfor recovering its missing entries, and finally extracting the recovered image\nfrom the tensor. The solution relies on two key components that have been\nrecently proposed to take advantage of the tensor train (TT) rank: A tensor\naugmentation tool called ket augmentation (KA) that represents a low-order\ntensor by a higher-order tensor, and the algorithm tensor completion by\nparallel matrix factorization via tensor train (TMac-TT), which has been\ndemonstrated to outperform state-of-the-art tensor completion algorithms.\nSimulation results for color image recovery show the clear advantage of our\nframework against current state-of-the-art tensor completion algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 00:24:33 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Bengua", "Johann A.", ""], ["Tuan", "Hoang D.", ""], ["Phien", "Ho N.", ""], ["Do", "Minh N.", ""]]}, {"id": "1607.03990", "submitter": "Jerry Li", "authors": "Jayadev Acharya, Ilias Diakonikolas, Jerry Li, Ludwig Schmidt", "title": "Fast Algorithms for Segmented Regression", "comments": "27 pages, appeared in ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fixed design segmented regression problem: Given noisy samples\nfrom a piecewise linear function $f$, we want to recover $f$ up to a desired\naccuracy in mean-squared error.\n  Previous rigorous approaches for this problem rely on dynamic programming\n(DP) and, while sample efficient, have running time quadratic in the sample\nsize. As our main contribution, we provide new sample near-linear time\nalgorithms for the problem that -- while not being minimax optimal -- achieve a\nsignificantly better sample-time tradeoff on large datasets compared to the DP\napproach. Our experimental evaluation shows that, compared with the DP\napproach, our algorithms provide a convergence rate that is only off by a\nfactor of $2$ to $4$, while achieving speedups of three orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 04:52:53 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Acharya", "Jayadev", ""], ["Diakonikolas", "Ilias", ""], ["Li", "Jerry", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "1607.04228", "submitter": "Evgeny Frolov", "authors": "Evgeny Frolov, Ivan Oseledets", "title": "Fifty Shades of Ratings: How to Benefit from a Negative Feedback in\n  Top-N Recommendations Tasks", "comments": "Accepted as a long paper at ACM RecSys 2016 conference, 8 pages, 6\n  figures, 2 tables", "journal-ref": null, "doi": "10.1145/2959100.2959170", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional collaborative filtering techniques treat a top-n recommendations\nproblem as a task of generating a list of the most relevant items. This\nformulation, however, disregards an opposite - avoiding recommendations with\ncompletely irrelevant items. Due to that bias, standard algorithms, as well as\ncommonly used evaluation metrics, become insensitive to negative feedback. In\norder to resolve this problem we propose to treat user feedback as a\ncategorical variable and model it with users and items in a ternary way. We\nemploy a third-order tensor factorization technique and implement a higher\norder folding-in method to support online recommendations. The method is\nequally sensitive to entire spectrum of user ratings and is able to accurately\npredict relevant items even from a negative only feedback. Our method may\npartially eliminate the need for complicated rating elicitation process as it\nprovides means for personalized recommendations from the very beginning of an\ninteraction with a recommender system. We also propose a modification of\nstandard metrics which helps to reveal unwanted biases and account for\nsensitivity to a negative feedback. Our model achieves state-of-the-art quality\nin standard recommendation tasks while significantly outperforming other\nmethods in the cold-start \"no-positive-feedback\" scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 17:55:33 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Frolov", "Evgeny", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1607.04315", "submitter": "Tsendsuren Munkhdalai", "authors": "Tsendsuren Munkhdalai and Hong Yu", "title": "Neural Semantic Encoders", "comments": "Accepted in EACL 2017, added: comparison with NTM, qualitative\n  analysis and memory visualization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a memory augmented neural network for natural language\nunderstanding: Neural Semantic Encoders. NSE is equipped with a novel memory\nupdate rule and has a variable sized encoding memory that evolves over time and\nmaintains the understanding of input sequences through read}, compose and write\noperations. NSE can also access multiple and shared memories. In this paper, we\ndemonstrated the effectiveness and the flexibility of NSE on five different\nnatural language tasks: natural language inference, question answering,\nsentence classification, document sentiment analysis and machine translation\nwhere NSE achieved state-of-the-art performance when evaluated on publically\navailable benchmarks. For example, our shared-memory model showed an\nencouraging result on neural machine translation, improving an attention-based\nbaseline by approximately 1.0 BLEU.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 20:58:26 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 14:01:11 GMT"}, {"version": "v3", "created": "Thu, 5 Jan 2017 15:41:13 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Munkhdalai", "Tsendsuren", ""], ["Yu", "Hong", ""]]}, {"id": "1607.04331", "submitter": "Subhaneil Lahiri", "authors": "Subhaneil Lahiri, Peiran Gao, Surya Ganguli", "title": "Random projections of random manifolds", "comments": "45 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interesting data often concentrate on low dimensional smooth manifolds inside\na high dimensional ambient space. Random projections are a simple, powerful\ntool for dimensionality reduction of such data. Previous works have studied\nbounds on how many projections are needed to accurately preserve the geometry\nof these manifolds, given their intrinsic dimensionality, volume and curvature.\nHowever, such works employ definitions of volume and curvature that are\ninherently difficult to compute. Therefore such theory cannot be easily tested\nagainst numerical simulations to understand the tightness of the proven bounds.\nWe instead study typical distortions arising in random projections of an\nensemble of smooth Gaussian random manifolds. We find explicitly computable,\napproximate theoretical bounds on the number of projections required to\naccurately preserve the geometry of these manifolds. Our bounds, while\napproximate, can only be violated with a probability that is exponentially\nsmall in the ambient dimension, and therefore they hold with high probability\nin cases of practical interest. Moreover, unlike previous work, we test our\ntheoretical bounds against numerical experiments on the actual geometric\ndistortions that typically occur for random projections of random smooth\nmanifolds. We find our bounds are tighter than previous results by several\norders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 21:43:39 GMT"}, {"version": "v2", "created": "Sat, 10 Sep 2016 02:37:47 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Lahiri", "Subhaneil", ""], ["Gao", "Peiran", ""], ["Ganguli", "Surya", ""]]}, {"id": "1607.04379", "submitter": "Renzhi Cao", "authors": "Renzhi Cao, Debswapna Bhattacharya, Jie Hou, and Jianlin Cheng", "title": "DeepQA: Improving the estimation of single protein model quality with\n  deep belief networks", "comments": "19 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Protein quality assessment (QA) by ranking and selecting protein models has\nlong been viewed as one of the major challenges for protein tertiary structure\nprediction. Especially, estimating the quality of a single protein model, which\nis important for selecting a few good models out of a large model pool\nconsisting of mostly low-quality models, is still a largely unsolved problem.\nWe introduce a novel single-model quality assessment method DeepQA based on\ndeep belief network that utilizes a number of selected features describing the\nquality of a model from different perspectives, such as energy, physio-chemical\ncharacteristics, and structural information. The deep belief network is trained\non several large datasets consisting of models from the Critical Assessment of\nProtein Structure Prediction (CASP) experiments, several publicly available\ndatasets, and models generated by our in-house ab initio method. Our experiment\ndemonstrate that deep belief network has better performance compared to Support\nVector Machines and Neural Networks on the protein model quality assessment\nproblem, and our method DeepQA achieves the state-of-the-art performance on\nCASP11 dataset. It also outperformed two well-established methods in selecting\ngood outlier models from a large set of models of mostly low quality generated\nby ab initio modeling methods. DeepQA is a useful tool for protein single model\nquality assessment and protein structure prediction. The source code,\nexecutable, document and training/test datasets of DeepQA for Linux is freely\navailable to non-commercial users at http://cactus.rnet.missouri.edu/DeepQA/.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 04:28:55 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Cao", "Renzhi", ""], ["Bhattacharya", "Debswapna", ""], ["Hou", "Jie", ""], ["Cheng", "Jianlin", ""]]}, {"id": "1607.04427", "submitter": "Joe Suzuki", "authors": "Joe Suzuki", "title": "A Theoretical Analysis of the BDeu Scores in Bayesian Network Structure\n  Learning", "comments": "published in Springer Behaviormetrika 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian network structure learning (BNSL), we need the prior probability\nover structures and parameters. If the former is the uniform distribution, the\nlatter determines the correctness of BNSL. In this paper, we compare BDeu\n(Bayesian Dirichlet equivalent uniform) and Jeffreys' prior w.r.t. their\nconsistency. When we seek a parent set $U$ of a variable $X$, we require\nregularity that if $H(X|U)\\leq H(X|U')$ and $U\\subsetneq U'$, then $U$ should\nbe chosen rather than $U'$. We prove that the BDeu scores violate the property\nand cause fatal situations in BNSL. This is because for the BDeu scores, for\nany sample size $n$,there exists a probability in the form\n$P(X,Y,Z)={P(XZ)P(YZ)}/{P(Z)}$ such that the probability of deciding that $X$\nand $Y$ are not conditionally independent given $Z$ is more than a half. For\nJeffreys' prior, the false-positive probability uniformly converges to zero\nwithout depending on any parameter values, and no such an inconvenience occurs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 09:22:55 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 09:35:54 GMT"}, {"version": "v3", "created": "Fri, 2 Dec 2016 20:25:40 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Suzuki", "Joe", ""]]}, {"id": "1607.04450", "submitter": "Senthilmurugan Sengottuvelan", "authors": "S. Senthilmurugan, Junaid Ansari, Petri M\\\"ah\\\"onen, T.G. Venkatesh,\n  and Marina Petrova", "title": "Channel Selection Algorithm for Cognitive Radio Networks with\n  Heavy-Tailed Idle Times", "comments": "14 pages, 14 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multichannel Cognitive Radio Network (CRN), where secondary\nusers sequentially sense channels for opportunistic spectrum access. In this\nscenario, the Channel Selection Algorithm (CSA) allows secondary users to find\na vacant channel with the minimal number of channel switches. Most of the\nexisting CSA literature assumes exponential ON-OFF time distribution for\nprimary users (PU) channel occupancy pattern. This exponential assumption might\nbe helpful to get performance bounds; but not useful to evaluate the\nperformance of CSA under realistic conditions. An in-depth analysis of\nindependent spectrum measurement traces reveals that wireless channels have\ntypically heavy-tailed PU OFF times. In this paper, we propose an extension to\nthe Predictive CSA framework and its generalization for heavy tailed PU OFF\ntime distribution, which represents realistic scenarios. In particular, we\ncalculate the probability of channel being idle for hyper-exponential OFF times\nto use in CSA. We implement our proposed CSA framework in a wireless test-bed\nand comprehensively evaluate its performance by recreating the realistic PU\nchannel occupancy patterns. The proposed CSA shows significant reduction in\nchannel switches and energy consumption as compared to Predictive CSA which\nalways assumes exponential PU ON-OFF times.Through our work, we show the impact\nof the PU channel occupancy pattern on the performance of CSA in multichannel\nCRN.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 10:46:43 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Senthilmurugan", "S.", ""], ["Ansari", "Junaid", ""], ["M\u00e4h\u00f6nen", "Petri", ""], ["Venkatesh", "T. G.", ""], ["Petrova", "Marina", ""]]}, {"id": "1607.04492", "submitter": "Tsendsuren Munkhdalai", "authors": "Tsendsuren Munkhdalai and Hong Yu", "title": "Neural Tree Indexers for Text Understanding", "comments": "Accepted at EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) process input text sequentially and model\nthe conditional transition between word tokens. In contrast, the advantages of\nrecursive networks include that they explicitly model the compositionality and\nthe recursive structure of natural language. However, the current recursive\narchitecture is limited by its dependence on syntactic tree. In this paper, we\nintroduce a robust syntactic parsing-independent tree structured model, Neural\nTree Indexers (NTI) that provides a middle ground between the sequential RNNs\nand the syntactic treebased recursive models. NTI constructs a full n-ary tree\nby processing the input text with its node function in a bottom-up fashion.\nAttention mechanism can then be applied to both structure and node function. We\nimplemented and evaluated a binarytree model of NTI, showing the model achieved\nthe state-of-the-art performance on three different NLP tasks: natural language\ninference, answer sentence selection, and sentence classification,\noutperforming state-of-the-art recurrent and recursive neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 12:59:01 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 17:10:33 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Munkhdalai", "Tsendsuren", ""], ["Yu", "Hong", ""]]}, {"id": "1607.04579", "submitter": "Bo Dai", "authors": "Bo Dai, Niao He, Yunpeng Pan, Byron Boots, Le Song", "title": "Learning from Conditional Distributions via Dual Embeddings", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning tasks, such as learning with invariance and policy\nevaluation in reinforcement learning, can be characterized as problems of\nlearning from conditional distributions. In such problems, each sample $x$\nitself is associated with a conditional distribution $p(z|x)$ represented by\nsamples $\\{z_i\\}_{i=1}^M$, and the goal is to learn a function $f$ that links\nthese conditional distributions to target values $y$. These learning problems\nbecome very challenging when we only have limited samples or in the extreme\ncase only one sample from each conditional distribution. Commonly used\napproaches either assume that $z$ is independent of $x$, or require an\noverwhelmingly large samples from each conditional distribution.\n  To address these challenges, we propose a novel approach which employs a new\nmin-max reformulation of the learning from conditional distribution problem.\nWith such new reformulation, we only need to deal with the joint distribution\n$p(z,x)$. We also design an efficient learning algorithm, Embedding-SGD, and\nestablish theoretical sample complexity for such problems. Finally, our\nnumerical experiments on both synthetic and real-world datasets show that the\nproposed approach can significantly improve over the existing algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 16:56:22 GMT"}, {"version": "v2", "created": "Sat, 31 Dec 2016 06:54:37 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Dai", "Bo", ""], ["He", "Niao", ""], ["Pan", "Yunpeng", ""], ["Boots", "Byron", ""], ["Song", "Le", ""]]}, {"id": "1607.04589", "submitter": "Siddharth Sigtia", "authors": "Siddharth Sigtia, Adam M. Stark, Sacha Krstulovic and Mark D. Plumbley", "title": "Automatic Environmental Sound Recognition: Performance versus\n  Computational Cost", "comments": null, "journal-ref": "IEEE/ACM Transactions on Audio, Speech and Language Processing\n  24(11): 2096-2107, Nov 2016", "doi": "10.1109/TASLP.2016.2592698", "report-no": null, "categories": "cs.SD cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of the Internet of Things (IoT), sound sensing applications\nare required to run on embedded platforms where notions of product pricing and\nform factor impose hard constraints on the available computing power. Whereas\nAutomatic Environmental Sound Recognition (AESR) algorithms are most often\ndeveloped with limited consideration for computational cost, this article seeks\nwhich AESR algorithm can make the most of a limited amount of computing power\nby comparing the sound classification performance em as a function of its\ncomputational cost. Results suggest that Deep Neural Networks yield the best\nratio of sound classification accuracy across a range of computational costs,\nwhile Gaussian Mixture Models offer a reasonable accuracy at a consistently\nsmall cost, and Support Vector Machines stand between both in terms of\ncompromise between accuracy and computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 17:29:26 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Sigtia", "Siddharth", ""], ["Stark", "Adam M.", ""], ["Krstulovic", "Sacha", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1607.04606", "submitter": "Edouard Grave", "authors": "Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov", "title": "Enriching Word Vectors with Subword Information", "comments": "Accepted to TACL. The two first authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous word representations, trained on large unlabeled corpora are\nuseful for many natural language processing tasks. Popular models that learn\nsuch representations ignore the morphology of words, by assigning a distinct\nvector to each word. This is a limitation, especially for languages with large\nvocabularies and many rare words. In this paper, we propose a new approach\nbased on the skipgram model, where each word is represented as a bag of\ncharacter $n$-grams. A vector representation is associated to each character\n$n$-gram; words being represented as the sum of these representations. Our\nmethod is fast, allowing to train models on large corpora quickly and allows us\nto compute word representations for words that did not appear in the training\ndata. We evaluate our word representations on nine different languages, both on\nword similarity and analogy tasks. By comparing to recently proposed\nmorphological word representations, we show that our vectors achieve\nstate-of-the-art performance on these tasks.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 18:27:55 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 17:41:07 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Bojanowski", "Piotr", ""], ["Grave", "Edouard", ""], ["Joulin", "Armand", ""], ["Mikolov", "Tomas", ""]]}, {"id": "1607.04614", "submitter": "William Montgomery Iv", "authors": "William Montgomery, Sergey Levine", "title": "Guided Policy Search as Approximate Mirror Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guided policy search algorithms can be used to optimize complex nonlinear\npolicies, such as deep neural networks, without directly computing policy\ngradients in the high-dimensional parameter space. Instead, these methods use\nsupervised learning to train the policy to mimic a \"teacher\" algorithm, such as\na trajectory optimizer or a trajectory-centric reinforcement learning method.\nGuided policy search methods provide asymptotic local convergence guarantees by\nconstruction, but it is not clear how much the policy improves within a small,\nfinite number of iterations. We show that guided policy search algorithms can\nbe interpreted as an approximate variant of mirror descent, where the\nprojection onto the constraint manifold is not exact. We derive a new guided\npolicy search algorithm that is simpler and provides appealing improvement and\nconvergence guarantees in simplified convex and linear settings, and show that\nin the more general nonlinear setting, the error in the projection step can be\nbounded. We provide empirical results on several simulated robotic navigation\nand manipulation tasks that show that our method is stable and achieves similar\nor better performance when compared to prior guided policy search methods, with\na simpler formulation and fewer hyperparameters.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 18:54:15 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Montgomery", "William", ""], ["Levine", "Sergey", ""]]}, {"id": "1607.04683", "submitter": "Raziel Alvarez", "authors": "Raziel Alvarez, Rohit Prabhavalkar, Anton Bakhtin", "title": "On the efficient representation and execution of deep acoustic models", "comments": "Accepted conference paper: \"The Annual Conference of the\n  International Speech Communication Association (Interspeech), 2016\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a simple and computationally efficient quantization\nscheme that enables us to reduce the resolution of the parameters of a neural\nnetwork from 32-bit floating point values to 8-bit integer values. The proposed\nquantization scheme leads to significant memory savings and enables the use of\noptimized hardware instructions for integer arithmetic, thus significantly\nreducing the cost of inference. Finally, we propose a \"quantization aware\"\ntraining process that applies the proposed scheme during network training and\nfind that it allows us to recover most of the loss in accuracy introduced by\nquantization. We validate the proposed techniques by applying them to a long\nshort-term memory-based acoustic model on an open-ended large vocabulary speech\nrecognition task.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 23:31:45 GMT"}, {"version": "v2", "created": "Sat, 17 Dec 2016 01:31:31 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Alvarez", "Raziel", ""], ["Prabhavalkar", "Rohit", ""], ["Bakhtin", "Anton", ""]]}, {"id": "1607.04747", "submitter": "Chao Lan", "authors": "Chao Lan, Yuhao Yang, Xiaoli Li, Bo Luo, Jun Huan", "title": "Learning Social Circles in Ego Networks based on Multi-View Social\n  Graphs", "comments": "This paper has been withdrawn by the author due to its current\n  unsatisfactory quality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In social network analysis, automatic social circle detection in ego-networks\nis becoming a fundamental and important task, with many potential applications\nsuch as user privacy protection or interest group recommendation. So far, most\nstudies have focused on addressing two questions, namely, how to detect\noverlapping circles and how to detect circles using a combination of network\nstructure and network node attributes. This paper asks an orthogonal research\nquestion, that is, how to detect circles based on network structures that are\n(usually) described by multiple views? Our investigation begins with crawling\nego-networks from Twitter and employing classic techniques to model their\nstructures by six views, including user relationships, user interactions and\nuser content. We then apply both standard and our modified multi-view spectral\nclustering techniques to detect social circles in these ego-networks. Based on\nextensive automatic and manual experimental evaluations, we deliver two major\nfindings: first, multi-view clustering techniques perform better than common\nsingle-view clustering techniques, which only use one view or naively integrate\nall views for detection, second, the standard multi-view clustering technique\nis less robust than our modified technique, which selectively transfers\ninformation across views based on an assumption that sparse network structures\nare (potentially) incomplete. In particular, the second finding makes us\nbelieve a direct application of standard clustering on potentially incomplete\nnetworks may yield biased results. We lightly examine this issue in theory,\nwhere we derive an upper bound for such bias by integrating theories of\nspectral clustering and matrix perturbation, and discuss how it may be affected\nby several network characteristics.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 15:00:40 GMT"}, {"version": "v2", "created": "Sat, 24 Dec 2016 02:27:15 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Lan", "Chao", ""], ["Yang", "Yuhao", ""], ["Li", "Xiaoli", ""], ["Luo", "Bo", ""], ["Huan", "Jun", ""]]}, {"id": "1607.04770", "submitter": "Ary Setijadi Prihatmanto", "authors": "Andrien Ivander Wijaya, Ary Setijadi Prihatmanto, Rifki Wijaya", "title": "Shesop Healthcare: Stress and influenza classification using support\n  vector machine kernel", "comments": "Keywords: Heart Rate, RRI, Stress, Influenza, SVM, Classification", "journal-ref": null, "doi": "10.13140/RG.2.1.2449.0486", "report-no": null, "categories": "cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Shesop is an integrated system to make human lives more easily and to help\npeople in terms of healthcare. Stress and influenza classification is a part of\nShesop's application for a healthcare devices such as smartwatch, polar and\nfitbit. The main objective of this paper is to classify a new data and inform\nwhether you are stress, depressed, caught by influenza or not. We will use the\nheart rate data taken for months in Bandung, analyze the data and find the\nHeart rate variance that constantly related with the stress and flu level.\nAfter we found the variable, we will use the variable as an input to the\nsupport vector machine learning. We will use the lagrangian and kernel\ntechnique to transform 2D data into 3D data so we can use the linear\nclassification in 3D space. In the end, we could use the machine learning's\nresult to classify new data and get the final result immediately: stress or\nnot, influenza or not.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 17:22:00 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Wijaya", "Andrien Ivander", ""], ["Prihatmanto", "Ary Setijadi", ""], ["Wijaya", "Rifki", ""]]}, {"id": "1607.04780", "submitter": "Junwei Liang", "authors": "Junwei Liang, Lu Jiang, Deyu Meng, Alexander Hauptmann", "title": "Exploiting Multi-modal Curriculum in Noisy Web Data for Large-scale\n  Concept Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning video concept detectors automatically from the big but noisy web\ndata with no additional manual annotations is a novel but challenging area in\nthe multimedia and the machine learning community. A considerable amount of\nvideos on the web are associated with rich but noisy contextual information,\nsuch as the title, which provides weak annotations or labels about the video\ncontent. To leverage the big noisy web labels, this paper proposes a novel\nmethod called WEbly-Labeled Learning (WELL), which is established on the\nstate-of-the-art machine learning algorithm inspired by the learning process of\nhuman. WELL introduces a number of novel multi-modal approaches to incorporate\nmeaningful prior knowledge called curriculum from the noisy web videos. To\ninvestigate this problem, we empirically study the curriculum constructed from\nthe multi-modal features of the videos collected from YouTube and Flickr. The\nefficacy and the scalability of WELL have been extensively demonstrated on two\npublic benchmarks, including the largest multimedia dataset and the largest\nmanually-labeled video set. The comprehensive experimental results demonstrate\nthat WELL outperforms state-of-the-art studies by a statically significant\nmargin on learning concepts from noisy web video data. In addition, the results\nalso verify that WELL is robust to the level of noisiness in the video data.\nNotably, WELL trained on sufficient noisy web labels is able to achieve a\ncomparable accuracy to supervised learning methods trained on the clean\nmanually-labeled data.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 18:14:51 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Liang", "Junwei", ""], ["Jiang", "Lu", ""], ["Meng", "Deyu", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1607.04793", "submitter": "Eliya Nachmani", "authors": "Eliya Nachmani, Yair Beery and David Burshtein", "title": "Learning to Decode Linear Codes Using Deep Learning", "comments": "Presented at the Allerton Conference 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.NE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel deep learning method for improving the belief propagation algorithm\nis proposed. The method generalizes the standard belief propagation algorithm\nby assigning weights to the edges of the Tanner graph. These edges are then\ntrained using deep learning techniques. A well-known property of the belief\npropagation algorithm is the independence of the performance on the transmitted\ncodeword. A crucial property of our new method is that our decoder preserved\nthis property. Furthermore, this property allows us to learn only a single\ncodeword instead of exponential number of code-words. Improvements over the\nbelief propagation algorithm are demonstrated for various high density parity\ncheck codes.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 19:09:26 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 14:43:52 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Nachmani", "Eliya", ""], ["Beery", "Yair", ""], ["Burshtein", "David", ""]]}, {"id": "1607.04805", "submitter": "Paris Perdikaris", "authors": "Maziar Raissi, Paris Perdikaris, George Em. Karniadakis", "title": "Inferring solutions of differential equations using noisy multi-fidelity\n  data", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": "10.1016/j.jcp.2017.01.060", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For more than two centuries, solutions of differential equations have been\nobtained either analytically or numerically based on typically well-behaved\nforcing and boundary conditions for well-posed problems. We are changing this\nparadigm in a fundamental way by establishing an interface between\nprobabilistic machine learning and differential equations. We develop\ndata-driven algorithms for general linear equations using Gaussian process\npriors tailored to the corresponding integro-differential operators. The only\nobservables are scarce noisy multi-fidelity data for the forcing and solution\nthat are not required to reside on the domain boundary. The resulting\npredictive posterior distributions quantify uncertainty and naturally lead to\nadaptive solution refinement via active learning. This general framework\ncircumvents the tyranny of numerical discretization as well as the consistency\nand stability issues of time-integration, and is scalable to high-dimensions.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 22:12:26 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Raissi", "Maziar", ""], ["Perdikaris", "Paris", ""], ["Karniadakis", "George Em.", ""]]}, {"id": "1607.04867", "submitter": "Aarti Sathyanarayana", "authors": "Aarti Sathyanarayana, Ferda Ofli, Luis Fernandes-Luque, Jaideep\n  Srivastava, Ahmed Elmagarmid, Teresa Arora, Shahrad Taheri", "title": "Robust Automated Human Activity Recognition and its Application to Sleep\n  Research", "comments": null, "journal-ref": null, "doi": "10.1109/ICDMW.2016.0077", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Activity Recognition (HAR) is a powerful tool for understanding human\nbehaviour. Applying HAR to wearable sensors can provide new insights by\nenriching the feature set in health studies, and enhance the personalisation\nand effectiveness of health, wellness, and fitness applications. Wearable\ndevices provide an unobtrusive platform for user monitoring, and due to their\nincreasing market penetration, feel intrinsic to the wearer. The integration of\nthese devices in daily life provide a unique opportunity for understanding\nhuman health and wellbeing. This is referred to as the \"quantified self\"\nmovement. The analyses of complex health behaviours such as sleep,\ntraditionally require a time-consuming manual interpretation by experts. This\nmanual work is necessary due to the erratic periodicity and persistent\nnoisiness of human behaviour. In this paper, we present a robust automated\nhuman activity recognition algorithm, which we call RAHAR. We test our\nalgorithm in the application area of sleep research by providing a novel\nframework for evaluating sleep quality and examining the correlation between\nthe aforementioned and an individual's physical activity. Our results improve\nthe state-of-the-art procedure in sleep research by 15 percent for area under\nROC and by 30 percent for F1 score on average. However, application of RAHAR is\nnot limited to sleep analysis and can be used for understanding other health\nproblems such as obesity, diabetes, and cardiac diseases.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 13:14:56 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 07:01:07 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Sathyanarayana", "Aarti", ""], ["Ofli", "Ferda", ""], ["Fernandes-Luque", "Luis", ""], ["Srivastava", "Jaideep", ""], ["Elmagarmid", "Ahmed", ""], ["Arora", "Teresa", ""], ["Taheri", "Shahrad", ""]]}, {"id": "1607.04903", "submitter": "Stephanie L. Hyland", "authors": "Stephanie L. Hyland, Gunnar R\\\"atsch", "title": "Learning Unitary Operators with Help From u(n)", "comments": "9 pages, 3 figures, 5 figures inc. subfigures, to appear at AAAI-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in the training of recurrent neural networks is the\nso-called vanishing or exploding gradient problem. The use of a norm-preserving\ntransition operator can address this issue, but parametrization is challenging.\nIn this work we focus on unitary operators and describe a parametrization using\nthe Lie algebra $\\mathfrak{u}(n)$ associated with the Lie group $U(n)$ of $n\n\\times n$ unitary matrices. The exponential map provides a correspondence\nbetween these spaces, and allows us to define a unitary matrix using $n^2$ real\ncoefficients relative to a basis of the Lie algebra. The parametrization is\nclosed under additive updates of these coefficients, and thus provides a simple\nspace in which to do gradient descent. We demonstrate the effectiveness of this\nparametrization on the problem of learning arbitrary unitary operators,\ncomparing to several baselines and outperforming a recently-proposed\nlower-dimensional parametrization. We additionally use our parametrization to\ngeneralize a recently-proposed unitary recurrent neural network to arbitrary\nunitary matrices, using it to solve standard long-memory tasks.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 18:58:12 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 16:02:25 GMT"}, {"version": "v3", "created": "Tue, 10 Jan 2017 11:13:35 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Hyland", "Stephanie L.", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1607.04917", "submitter": "Blaine Rister", "authors": "Blaine Rister, Daniel L Rubin", "title": "Piecewise convexity of artificial neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although artificial neural networks have shown great promise in applications\nincluding computer vision and speech recognition, there remains considerable\npractical and theoretical difficulty in optimizing their parameters. The\nseemingly unreasonable success of gradient descent methods in minimizing these\nnon-convex functions remains poorly understood. In this work we offer some\ntheoretical guarantees for networks with piecewise affine activation functions,\nwhich have in recent years become the norm. We prove three main results.\nFirstly, that the network is piecewise convex as a function of the input data.\nSecondly, that the network, considered as a function of the parameters in a\nsingle layer, all others held constant, is again piecewise convex. Finally,\nthat the network as a function of all its parameters is piecewise multi-convex,\na generalization of biconvexity. From here we characterize the local minima and\nstationary points of the training objective, showing that they minimize certain\nsubsets of the parameter space. We then analyze the performance of two\noptimization algorithms on multi-convex problems: gradient descent, and a\nmethod which repeatedly solves a number of convex sub-problems. We prove\nnecessary convergence conditions for the first algorithm and both necessary and\nsufficient conditions for the second, after introducing regularization to the\nobjective. Finally, we remark on the remaining difficulty of the global\noptimization problem. Under the squared error objective, we show that by\nvarying the training data, a single rectifier neuron admits local minima\narbitrarily far apart, both in objective value and parameter space.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 21:49:00 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 06:39:01 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Rister", "Blaine", ""], ["Rubin", "Daniel L", ""]]}, {"id": "1607.04984", "submitter": "He Sun", "authors": "He Sun, Luca Zanetti", "title": "Distributed Graph Clustering by Load Balancing", "comments": "There is a gap in the proof of the paper, which makes the paper's\n  main statement invalid. We presented an improved algorithm with a completely\n  different proof in the paper with arXiv:1711.01262", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph clustering is a fundamental computational problem with a number of\napplications in algorithm design, machine learning, data mining, and analysis\nof social networks. Over the past decades, researchers have proposed a number\nof algorithmic design methods for graph clustering. However, most of these\nmethods are based on complicated spectral techniques or convex optimisation,\nand cannot be applied directly for clustering many networks that occur in\npractice, whose information is often collected on different sites. Designing a\nsimple and distributed clustering algorithm is of great interest, and has wide\napplications for processing big datasets. In this paper we present a simple and\ndistributed algorithm for graph clustering: for a wide class of graphs that are\ncharacterised by a strong cluster-structure, our algorithm finishes in a\npoly-logarithmic number of rounds, and recovers a partition of the graph close\nto an optimal partition. The main component of our algorithm is an application\nof the random matching model of load balancing, which is a fundamental protocol\nin distributed computing and has been extensively studied in the past 20 years.\nHence, our result highlights an intrinsic and interesting connection between\ngraph clustering and load balancing. At a technical level, we present a purely\nalgebraic result characterising the early behaviours of load balancing\nprocesses for graphs exhibiting a cluster-structure. We believe that this\nresult can be further applied to analyse other gossip processes, such as rumour\nspreading and averaging processes.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 09:30:49 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 20:36:55 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2019 13:47:40 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Sun", "He", ""], ["Zanetti", "Luca", ""]]}, {"id": "1607.05002", "submitter": "Pourya Habib Zadeh", "authors": "Pourya Habib Zadeh, Reshad Hosseini and Suvrit Sra", "title": "Geometric Mean Metric Learning", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the task of learning a Euclidean metric from data. We approach\nthis problem from first principles and formulate it as a surprisingly simple\noptimization problem. Indeed, our formulation even admits a closed form\nsolution. This solution possesses several very attractive properties: (i) an\ninnate geometric appeal through the Riemannian geometry of positive definite\nmatrices; (ii) ease of interpretability; and (iii) computational speed several\norders of magnitude faster than the widely used LMNN and ITML methods.\nFurthermore, on standard benchmark datasets, our closed-form solution\nconsistently attains higher classification accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 10:14:46 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Zadeh", "Pourya Habib", ""], ["Hosseini", "Reshad", ""], ["Sra", "Suvrit", ""]]}, {"id": "1607.05047", "submitter": "Susan Murphy A", "authors": "S.A. Murphy, Y. Deng, E.B. Laber, H.R. Maei, R.S. Sutton, K.\n  Witkiewitz", "title": "A Batch, Off-Policy, Actor-Critic Algorithm for Optimizing the Average\n  Reward", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an off-policy actor-critic algorithm for learning an optimal\npolicy from a training set composed of data from multiple individuals. This\nalgorithm is developed with a view towards its use in mobile health.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 12:43:40 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Murphy", "S. A.", ""], ["Deng", "Y.", ""], ["Laber", "E. B.", ""], ["Maei", "H. R.", ""], ["Sutton", "R. S.", ""], ["Witkiewitz", "K.", ""]]}, {"id": "1607.05241", "submitter": "Khanh Nguyen", "authors": "Khanh Nguyen", "title": "Imitation Learning with Recurrent Neural Networks", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel view that unifies two frameworks that aim to solve\nsequential prediction problems: learning to search (L2S) and recurrent neural\nnetworks (RNN). We point out equivalences between elements of the two\nframeworks. By complementing what is missing from one framework comparing to\nthe other, we introduce a more advanced imitation learning framework that, on\none hand, augments L2S s notion of search space and, on the other hand,\nenhances RNNs training procedure to be more robust to compounding errors\narising from training on highly correlated examples.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 19:01:00 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Nguyen", "Khanh", ""]]}, {"id": "1607.05271", "submitter": "Niels Landwehr", "authors": "Ahmed Abdelwahab, Reinhold Kliegl and Niels Landwehr", "title": "A Semiparametric Model for Bayesian Reader Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of identifying individuals based on their characteristic\ngaze patterns during reading of arbitrary text. The motivation for this problem\nis an unobtrusive biometric setting in which a user is observed during access\nto a document, but no specific challenge protocol requiring the user's time and\nattention is carried out. Existing models of individual differences in gaze\ncontrol during reading are either based on simple aggregate features of eye\nmovements, or rely on parametric density models to describe, for instance,\nsaccade amplitudes or word fixation durations. We develop flexible\nsemiparametric models of eye movements during reading in which densities are\ninferred under a Gaussian process prior centered at a parametric distribution\nfamily that is expected to approximate the true distribution well. An empirical\nstudy on reading data from 251 individuals shows significant improvements over\nthe state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 14:46:05 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Abdelwahab", "Ahmed", ""], ["Kliegl", "Reinhold", ""], ["Landwehr", "Niels", ""]]}, {"id": "1607.05387", "submitter": "Hanock Kwak", "authors": "Hanock Kwak, Byoung-Tak Zhang", "title": "Generating Images Part by Part with Composite Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image generation remains a fundamental problem in artificial intelligence in\ngeneral and deep learning in specific. The generative adversarial network (GAN)\nwas successful in generating high quality samples of natural images. We propose\na model called composite generative adversarial network, that reveals the\ncomplex structure of images with multiple generators in which each generator\ngenerates some part of the image. Those parts are combined by alpha blending\nprocess to create a new single image. It can generate, for example, background\nand face sequentially with two generators, after training on face dataset.\nTraining was done in an unsupervised way without any labels about what each\ngenerator should generate. We found possibilities of learning the structure by\nusing this generative model empirically.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 03:09:31 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 07:32:35 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Kwak", "Hanock", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1607.05397", "submitter": "Zhiwei Steven Wu", "authors": "Aaron Roth, Aleksandrs Slivkins, Jonathan Ullman, Zhiwei Steven Wu", "title": "Multidimensional Dynamic Pricing for Welfare Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of a seller dynamically pricing $d$ distinct types of\nindivisible goods, when faced with the online arrival of unit-demand buyers\ndrawn independently from an unknown distribution. The goods are not in limited\nsupply, but can only be produced at a limited rate and are costly to produce.\nThe seller observes only the bundle of goods purchased at each day, but nothing\nelse about the buyer's valuation function. Our main result is a dynamic pricing\nalgorithm for optimizing welfare (including the seller's cost of production)\nthat runs in time and a number of rounds that are polynomial in $d$ and the\napproximation parameter. We are able to do this despite the fact that (i) the\nprice-response function is not continuous, and even its fractional relaxation\nis a non-concave function of the prices, and (ii) the welfare is not observable\nto the seller.\n  We derive this result as an application of a general technique for optimizing\nwelfare over \\emph{divisible} goods, which is of independent interest. When\nbuyers have strongly concave, H\\\"older continuous valuation functions over $d$\ndivisible goods, we give a general polynomial time dynamic pricing technique.\nWe are able to apply this technique to the setting of unit demand buyers\ndespite the fact that in that setting the goods are not divisible, and the\nnatural fractional relaxation of a unit demand valuation is not strongly\nconcave. In order to apply our general technique, we introduce a novel price\nrandomization procedure which has the effect of implicitly inducing buyers to\n\"regularize\" their valuations with a strongly concave function. Finally, we\nalso extend our results to a limited-supply setting in which the number of\ncopies of each good cannot be replenished.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 04:22:00 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 01:06:58 GMT"}, {"version": "v3", "created": "Sat, 10 Jun 2017 20:10:09 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Roth", "Aaron", ""], ["Slivkins", "Aleksandrs", ""], ["Ullman", "Jonathan", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1607.05691", "submitter": "Francois Chollet", "authors": "Fran\\c{c}ois Chollet", "title": "Information-theoretical label embeddings for large-scale image\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for training multi-label, massively multi-class image\nclassification models, that is faster and more accurate than supervision via a\nsigmoid cross-entropy loss (logistic regression). Our method consists in\nembedding high-dimensional sparse labels onto a lower-dimensional dense sphere\nof unit-normed vectors, and treating the classification problem as a cosine\nproximity regression problem on this sphere. We test our method on a dataset of\n300 million high-resolution images with 17,000 labels, where it yields\nconsiderably faster convergence, as well as a 7% higher mean average precision\ncompared to logistic regression.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 18:40:01 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Chollet", "Fran\u00e7ois", ""]]}, {"id": "1607.05749", "submitter": "Md Mansurul Bhuiyan", "authors": "Mansurul Bhuiyan and Mohammad Al Hasan", "title": "PRIIME: A Generic Framework for Interactive Personalized Interesting\n  Pattern Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional frequent pattern mining algorithms generate an exponentially\nlarge number of patterns of which a substantial proportion are not much\nsignificant for many data analysis endeavors. Discovery of a small number of\npersonalized interesting patterns from the large output set according to a\nparticular user's interest is an important as well as challenging task.\nExisting works on pattern summarization do not solve this problem from the\npersonalization viewpoint. In this work, we propose an interactive pattern\ndiscovery framework named PRIIME which identifies a set of interesting patterns\nfor a specific user without requiring any prior input on the interestingness\nmeasure of patterns from the user. The proposed framework is generic to support\ndiscovery of the interesting set, sequence and graph type patterns. We develop\na softmax classification based iterative learning algorithm that uses a limited\nnumber of interactive feedback from the user to learn her interestingness\nprofile, and use this profile for pattern recommendation. To handle sequence\nand graph type patterns PRIIME adopts a neural net (NN) based unsupervised\nfeature construction approach. We also develop a strategy that combines\nexploration and exploitation to select patterns for feedback. We show\nexperimental results on several real-life datasets to validate the performance\nof the proposed method. We also compare with the existing methods of\ninteractive pattern discovery to show that our method is substantially superior\nin performance. To portray the applicability of the framework, we present a\ncase study from the real-estate domain.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 20:21:43 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Bhuiyan", "Mansurul", ""], ["Hasan", "Mohammad Al", ""]]}, {"id": "1607.05952", "submitter": "Luca Pappalardo", "authors": "Luca Pappalardo and Filippo Simini", "title": "Data-driven generation of spatio-temporal routines in human mobility", "comments": "Data Mining and Knowledge Discovery, 2018", "journal-ref": null, "doi": "10.1007/s10618-017-0548-4", "report-no": null, "categories": "cs.SI cs.LG physics.data-an physics.soc-ph stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generation of realistic spatio-temporal trajectories of human mobility is\nof fundamental importance in a wide range of applications, such as the\ndeveloping of protocols for mobile ad-hoc networks or what-if analysis in urban\necosystems. Current generative algorithms fail in accurately reproducing the\nindividuals' recurrent schedules and at the same time in accounting for the\npossibility that individuals may break the routine during periods of variable\nduration. In this article we present DITRAS (DIary-based TRAjectory Simulator),\na framework to simulate the spatio-temporal patterns of human mobility. DITRAS\noperates in two steps: the generation of a mobility diary and the translation\nof the mobility diary into a mobility trajectory. We propose a data-driven\nalgorithm which constructs a diary generator from real data, capturing the\ntendency of individuals to follow or break their routine. We also propose a\ntrajectory generator based on the concept of preferential exploration and\npreferential return. We instantiate DITRAS with the proposed diary and\ntrajectory generators and compare the resulting algorithm with real data and\nsynthetic data produced by other generative algorithms, built by instantiating\nDITRAS with several combinations of diary and trajectory generators. We show\nthat the proposed algorithm reproduces the statistical properties of real\ntrajectories in the most accurate way, making a step forward the understanding\nof the origin of the spatio-temporal patterns of human mobility.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 11:54:27 GMT"}, {"version": "v2", "created": "Sat, 29 Apr 2017 18:51:08 GMT"}, {"version": "v3", "created": "Sat, 9 Dec 2017 10:51:19 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Pappalardo", "Luca", ""], ["Simini", "Filippo", ""]]}, {"id": "1607.05962", "submitter": "Chaoyang Jiang", "authors": "Chaoyang Jiang, Mustafa K. Masood, Yeng Chai Soh, and Hua Li", "title": "Indoor occupancy estimation from carbon dioxide concentration", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an indoor occupancy estimator with which we can estimate\nthe number of real-time indoor occupants based on the carbon dioxide (CO2)\nmeasurement. The estimator is actually a dynamic model of the occupancy level.\nTo identify the dynamic model, we propose the Feature Scaled Extreme Learning\nMachine (FS-ELM) algorithm, which is a variation of the standard Extreme\nLearning Machine (ELM) but is shown to perform better for the occupancy\nestimation problem. The measured CO2 concentration suffers from serious spikes.\nWe find that pre-smoothing the CO2 data can greatly improve the estimation\naccuracy. In real applications, however, we cannot obtain the real-time\nglobally smoothed CO2 data. We provide a way to use the locally smoothed CO2\ndata instead, which is real-time available. We introduce a new criterion, i.e.\n$x$-tolerance accuracy, to assess the occupancy estimator. The proposed\noccupancy estimator was tested in an office room with 24 cubicles and 11 open\nseats. The accuracy is up to 94 percent with a tolerance of four occupants.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 14:00:53 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Jiang", "Chaoyang", ""], ["Masood", "Mustafa K.", ""], ["Soh", "Yeng Chai", ""], ["Li", "Hua", ""]]}, {"id": "1607.05966", "submitter": "Philip Schniter", "authors": "Mark Borgerding and Philip Schniter", "title": "Onsager-corrected deep learning for sparse linear inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has gained great popularity due to its widespread success on\nmany inference problems. We consider the application of deep learning to the\nsparse linear inverse problem encountered in compressive sensing, where one\nseeks to recover a sparse signal from a small number of noisy linear\nmeasurements. In this paper, we propose a novel neural-network architecture\nthat decouples prediction errors across layers in the same way that the\napproximate message passing (AMP) algorithm decouples them across iterations:\nthrough Onsager correction. Numerical experiments suggest that our \"learned\nAMP\" network significantly improves upon Gregor and LeCun's \"learned ISTA\"\nnetwork in both accuracy and complexity.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 14:14:49 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Borgerding", "Mark", ""], ["Schniter", "Philip", ""]]}, {"id": "1607.05970", "submitter": "James Edwards", "authors": "James Edwards, Paul Fearnhead, Kevin Glazebrook", "title": "On the Identification and Mitigation of Weaknesses in the Knowledge\n  Gradient Policy for Multi-Armed Bandits", "comments": "Minor typos corrected", "journal-ref": "Probability in the Engineering and Informational Sciences, 31(2)\n  239-263 (2017)", "doi": "10.1017/S0269964816000279", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Knowledge Gradient (KG) policy was originally proposed for online ranking\nand selection problems but has recently been adapted for use in online decision\nmaking in general and multi-armed bandit problems (MABs) in particular. We\nstudy its use in a class of exponential family MABs and identify weaknesses,\nincluding a propensity to take actions which are dominated with respect to both\nexploitation and exploration. We propose variants of KG which avoid such\nerrors. These new policies include an index heuristic which deploys a KG\napproach to develop an approximation to the Gittins index. A numerical study\nshows this policy to perform well over a range of MABs including those for\nwhich index policies are not optimal. While KG does not make dominated actions\nwhen bandits are Gaussian, it fails to be index consistent and appears not to\nenjoy a performance advantage over competitor policies when arms are correlated\nto compensate for its greater computational demands.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 14:21:42 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 15:36:30 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Edwards", "James", ""], ["Fearnhead", "Paul", ""], ["Glazebrook", "Kevin", ""]]}, {"id": "1607.06011", "submitter": "Julius Julius", "authors": "Julius, Gopinath Mahale, Sumana T., C. S. Adityakrishna", "title": "On the Modeling of Error Functions as High Dimensional Landscapes for\n  Weight Initialization in Learning Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next generation deep neural networks for classification hosted on embedded\nplatforms will rely on fast, efficient, and accurate learning algorithms.\nInitialization of weights in learning networks has a great impact on the\nclassification accuracy. In this paper we focus on deriving good initial\nweights by modeling the error function of a deep neural network as a\nhigh-dimensional landscape. We observe that due to the inherent complexity in\nits algebraic structure, such an error function may conform to general results\nof the statistics of large systems. To this end we apply some results from\nRandom Matrix Theory to analyse these functions. We model the error function in\nterms of a Hamiltonian in N-dimensions and derive some theoretical results\nabout its general behavior. These results are further used to make better\ninitial guesses of weights for the learning algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 16:25:27 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Julius", "", ""], ["Mahale", "Gopinath", ""], ["T.", "Sumana", ""], ["Adityakrishna", "C. S.", ""]]}, {"id": "1607.06017", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yuanzhi Li", "title": "Doubly Accelerated Methods for Faster CCA and Generalized\n  Eigendecomposition", "comments": "We have now stated more clearly why this paper has outperformed\n  relevant previous results, and included discussions for doubly-stochastic\n  methods. arXiv admin note: text overlap with arXiv:1607.03463", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study $k$-GenEV, the problem of finding the top $k$ generalized\neigenvectors, and $k$-CCA, the problem of finding the top $k$ vectors in\ncanonical-correlation analysis. We propose algorithms $\\mathtt{LazyEV}$ and\n$\\mathtt{LazyCCA}$ to solve the two problems with running times linearly\ndependent on the input size and on $k$.\n  Furthermore, our algorithms are DOUBLY-ACCELERATED: our running times depend\nonly on the square root of the matrix condition number, and on the square root\nof the eigengap. This is the first such result for both $k$-GenEV or $k$-CCA.\nWe also provide the first gap-free results, which provide running times that\ndepend on $1/\\sqrt{\\varepsilon}$ rather than the eigengap.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 16:43:18 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 03:18:24 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1607.06123", "submitter": "Gaurav Singh", "authors": "Sandra Mitrovi\\'c and Gaurav Singh", "title": "Predicting Branch Visits and Credit Card Up-selling using Temporal\n  Banking Data", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an abundance of temporal and non-temporal data in banking (and other\nindustries), but such temporal activity data can not be used directly with\nclassical machine learning models. In this work, we perform extensive feature\nextraction from the temporal user activity data in an attempt to predict user\nvisits to different branches and credit card up-selling utilizing user\ninformation and the corresponding activity data, as part of \\emph{ECML/PKDD\nDiscovery Challenge 2016 on Bank Card Usage Analysis}. Our solution ranked\n\\nth{4} for \\emph{Task 1} and achieved an AUC of \\textbf{$0.7056$} for\n\\emph{Task 2} on public leaderboard.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 20:55:14 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2016 21:15:06 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Mitrovi\u0107", "Sandra", ""], ["Singh", "Gaurav", ""]]}, {"id": "1607.06125", "submitter": "Ahmed Hassanien", "authors": "Ahmed Mamdouh A. Hassanien", "title": "Sequence to sequence learning for unconstrained scene text recognition", "comments": "It is my master thesis. The thesis was done at Sony Technology Center\n  Stuttgart and presented to Nile University. The thesis supervisors are Mark\n  Blaxall, Fabien Cardinaux, and Motaz Abdelwahab", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a state-of-the-art approach for unconstrained natural\nscene text recognition. We propose a cascade approach that incorporates a\nconvolutional neural network (CNN) architecture followed by a long short term\nmemory model (LSTM). The CNN learns visual features for the characters and uses\nthem with a softmax layer to detect sequence of characters. While the CNN gives\nvery good recognition results, it does not model relation between characters,\nhence gives rise to false positive and false negative cases (confusing\ncharacters due to visual similarities like \"g\" and \"9\", or confusing background\npatches with characters; either removing existing characters or adding\nnon-existing ones) To alleviate these problems we leverage recent developments\nin LSTM architectures to encode contextual information. We show that the LSTM\ncan dramatically reduce such errors and achieve state-of-the-art accuracy in\nthe task of unconstrained natural scene text recognition. Moreover we manually\nremove all occurrences of the words that exist in the test set from our\ntraining set to test whether our approach will generalize to unseen data. We\nuse the ICDAR 13 test set for evaluation and compare the results with the state\nof the art approaches [11, 18]. We finally present an application of the work\nin the domain of for traffic monitoring.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 21:02:16 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Hassanien", "Ahmed Mamdouh A.", ""]]}, {"id": "1607.06146", "submitter": "Leonardo Banchi", "authors": "Leonardo Banchi, Nicola Pancotti, Sougato Bose", "title": "Supervised quantum gate \"teaching\" for quantum hardware design", "comments": "6 pages, 1 figure, based on arXiv:1509.04298", "journal-ref": "Proceedings of the European Symposium on Artificial Neural\n  Networks 2016", "doi": null, "report-no": null, "categories": "cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to train a quantum network of pairwise interacting qubits such\nthat its evolution implements a target quantum algorithm into a given network\nsubset. Our strategy is inspired by supervised learning and is designed to help\nthe physical construction of a quantum computer which operates with minimal\nexternal classical control.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 22:46:32 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Banchi", "Leonardo", ""], ["Pancotti", "Nicola", ""], ["Bose", "Sougato", ""]]}, {"id": "1607.06182", "submitter": "Shiyu Chang", "authors": "Shiyu Chang, Yang Zhang, Jiliang Tang, Dawei Yin, Yi Chang, Mark A.\n  Hasegawa-Johnson, Thomas S. Huang", "title": "Streaming Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of real-world recommender systems produces data\ncontinuously and rapidly, and it becomes more realistic to study recommender\nsystems under streaming scenarios. Data streams present distinct properties\nsuch as temporally ordered, continuous and high-velocity, which poses\ntremendous challenges to traditional recommender systems. In this paper, we\ninvestigate the problem of recommendation with stream inputs. In particular, we\nprovide a principled framework termed sRec, which provides explicit\ncontinuous-time random process models of the creation of users and topics, and\nof the evolution of their interests. A variational Bayesian approach called\nrecursive meanfield approximation is proposed, which permits computationally\nefficient instantaneous on-line inference. Experimental results on several\nreal-world datasets demonstrate the advantages of our sRec over other\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 04:10:38 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Chang", "Shiyu", ""], ["Zhang", "Yang", ""], ["Tang", "Jiliang", ""], ["Yin", "Dawei", ""], ["Chang", "Yi", ""], ["Hasegawa-Johnson", "Mark A.", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1607.06190", "submitter": "Uwe Aickelin", "authors": "Christopher Roadknight, Durga Suryanarayanan, Uwe Aickelin, John\n  Scholefield, Lindy Durrant", "title": "An ensemble of machine learning and anti-learning methods for predicting\n  tumour patient survival rates", "comments": "IEEE International Conference on Data Science and Advanced Analytics\n  (IEEE DSAA'2015), pp. 1-8, 2015. arXiv admin note: text overlap with\n  arXiv:1307.1599, arXiv:1409.0788", "journal-ref": null, "doi": "10.1109/DSAA.2015.7344863", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper primarily addresses a dataset relating to cellular, chemical and\nphysical conditions of patients gathered at the time they are operated upon to\nremove colorectal tumours. This data provides a unique insight into the\nbiochemical and immunological status of patients at the point of tumour removal\nalong with information about tumour classification and post-operative survival.\nThe relationship between severity of tumour, based on TNM staging, and survival\nis still unclear for patients with TNM stage 2 and 3 tumours. We ask whether it\nis possible to predict survival rate more accurately using a selection of\nmachine learning techniques applied to subsets of data to gain a deeper\nunderstanding of the relationships between a patient's biochemical markers and\nsurvival. We use a range of feature selection and single classification\ntechniques to predict the 5 year survival rate of TNM stage 2 and 3 patients\nwhich initially produces less than ideal results. The performance of each model\nindividually is then compared with subsets of the data where agreement is\nreached for multiple models. This novel method of selective ensembling\ndemonstrates that significant improvements in model accuracy on an unseen test\nset can be achieved for patients where agreement between models is achieved.\nFinally we point at a possible method to identify whether a patients prognosis\ncan be accurately predicted or not.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 04:57:16 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Roadknight", "Christopher", ""], ["Suryanarayanan", "Durga", ""], ["Aickelin", "Uwe", ""], ["Scholefield", "John", ""], ["Durrant", "Lindy", ""]]}, {"id": "1607.06203", "submitter": "Matus Telgarsky", "authors": "Daniel Hsu and Matus Telgarsky", "title": "Greedy bi-criteria approximations for $k$-medians and $k$-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the following natural greedy procedure for clustering\nin the bi-criterion setting: iteratively grow a set of centers, in each round\nadding the center from a candidate set that maximally decreases clustering\ncost. In the case of $k$-medians and $k$-means, the key results are as follows.\n  $\\bullet$ When the method considers all data points as candidate centers,\nthen selecting $\\mathcal{O}(k\\log(1/\\varepsilon))$ centers achieves cost at\nmost $2+\\varepsilon$ times the optimal cost with $k$ centers.\n  $\\bullet$ Alternatively, the same guarantees hold if each round samples\n$\\mathcal{O}(k/\\varepsilon^5)$ candidate centers proportionally to their\ncluster cost (as with $\\texttt{kmeans++}$, but holding centers fixed).\n  $\\bullet$ In the case of $k$-means, considering an augmented set of\n$n^{\\lceil1/\\varepsilon\\rceil}$ candidate centers gives $1+\\varepsilon$\napproximation with $\\mathcal{O}(k\\log(1/\\varepsilon))$ centers, the entire\nalgorithm taking\n$\\mathcal{O}(dk\\log(1/\\varepsilon)n^{1+\\lceil1/\\varepsilon\\rceil})$ time, where\n$n$ is the number of data points in $\\mathbb{R}^d$.\n  $\\bullet$ In the case of Euclidean $k$-medians, generating a candidate set\nvia $n^{\\mathcal{O}(1/\\varepsilon^2)}$ executions of stochastic gradient\ndescent with adaptively determined constraint sets will once again give\napproximation $1+\\varepsilon$ with $\\mathcal{O}(k\\log(1/\\varepsilon))$ centers\nin $dk\\log(1/\\varepsilon)n^{\\mathcal{O}(1/\\varepsilon^2)}$ time.\n  Ancillary results include: guarantees for cluster costs based on powers of\nmetrics; a brief, favorable empirical evaluation against $\\texttt{kmeans++}$;\ndata-dependent bounds allowing $1+\\varepsilon$ in the first two bullets above,\nfor example with $k$-medians over finite metric spaces.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 06:04:36 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Hsu", "Daniel", ""], ["Telgarsky", "Matus", ""]]}, {"id": "1607.06280", "submitter": "Julie Moeyersoms", "authors": "Julie Moeyersoms, Brian d'Alessandro, Foster Provost, David Martens", "title": "Explaining Classification Models Built on High-Dimensional Sparse Data", "comments": "5 pages, 1 figure, 2 Tables; ICML conference, Workshop on Human\n  Interpretability In Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive modeling applications increasingly use data representing people's\nbehavior, opinions, and interactions. Fine-grained behavior data often has\ndifferent structure from traditional data, being very high-dimensional and\nsparse. Models built from these data are quite difficult to interpret, since\nthey contain many thousands or even many millions of features. Listing features\nwith large model coefficients is not sufficient, because the model coefficients\ndo not incorporate information on feature presence, which is key when analysing\nsparse data. In this paper we introduce two alternatives for explaining\npredictive models by listing important features. We evaluate these alternatives\nin terms of explanation \"bang for the buck,\", i.e., how many examples'\ninferences are explained for a given number of features listed. The bottom\nline: (i) The proposed alternatives have double the bang-for-the-buck as\ncompared to just listing the high-coefficient features, and (ii) interestingly,\nalthough they come from different sources and motivations, the two new\nalternatives provide strikingly similar rankings of important features.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 11:50:41 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 23:01:11 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Moeyersoms", "Julie", ""], ["d'Alessandro", "Brian", ""], ["Provost", "Foster", ""], ["Martens", "David", ""]]}, {"id": "1607.06294", "submitter": "Santiago Segarra", "authors": "Gunnar Carlsson, Facundo M\\'emoli, Alejandro Ribeiro, Santiago Segarra", "title": "Hierarchical Clustering of Asymmetric Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1301.7724", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers networks where relationships between nodes are\nrepresented by directed dissimilarities. The goal is to study methods that,\nbased on the dissimilarity structure, output hierarchical clusters, i.e., a\nfamily of nested partitions indexed by a connectivity parameter. Our\nconstruction of hierarchical clustering methods is built around the concept of\nadmissible methods, which are those that abide by the axioms of value - nodes\nin a network with two nodes are clustered together at the maximum of the two\ndissimilarities between them - and transformation - when dissimilarities are\nreduced, the network may become more clustered but not less. Two particular\nmethods, termed reciprocal and nonreciprocal clustering, are shown to provide\nupper and lower bounds in the space of admissible methods. Furthermore,\nalternative clustering methodologies and axioms are considered. In particular,\nmodifying the axiom of value such that clustering in two-node networks occurs\nat the minimum of the two dissimilarities entails the existence of a unique\nadmissible clustering method.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 12:32:47 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Carlsson", "Gunnar", ""], ["M\u00e9moli", "Facundo", ""], ["Ribeiro", "Alejandro", ""], ["Segarra", "Santiago", ""]]}, {"id": "1607.06333", "submitter": "Massil Achab", "authors": "Massil Achab, Emmanuel Bacry, St\\'ephane Ga\\\"iffas, Iacopo\n  Mastromatteo, Jean-Francois Muzy", "title": "Uncovering Causality from Multivariate Hawkes Integrated Cumulants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a new nonparametric method that allows one to estimate the matrix\nof integrated kernels of a multivariate Hawkes process. This matrix not only\nencodes the mutual influences of each nodes of the process, but also\ndisentangles the causality relationships between them. Our approach is the\nfirst that leads to an estimation of this matrix without any parametric\nmodeling and estimation of the kernels themselves. A consequence is that it can\ngive an estimation of causality relationships between nodes (or users), based\non their activity timestamps (on a social network for instance), without\nknowing or estimating the shape of the activities lifetime. For that purpose,\nwe introduce a moment matching method that fits the third-order integrated\ncumulants of the process. We show on numerical experiments that our approach is\nindeed very robust to the shape of the kernels, and gives appealing results on\nthe MemeTracker database.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 14:19:23 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 22:46:27 GMT"}, {"version": "v3", "created": "Tue, 30 May 2017 00:06:14 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Achab", "Massil", ""], ["Bacry", "Emmanuel", ""], ["Ga\u00efffas", "St\u00e9phane", ""], ["Mastromatteo", "Iacopo", ""], ["Muzy", "Jean-Francois", ""]]}, {"id": "1607.06335", "submitter": "Santiago Segarra", "authors": "Gunnar Carlsson, Facundo M\\'emoli, Alejandro Ribeiro, Santiago Segarra", "title": "Admissible Hierarchical Clustering Methods and Algorithms for Asymmetric\n  Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1301.7724", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper characterizes hierarchical clustering methods that abide by two\npreviously introduced axioms -- thus, denominated admissible methods -- and\nproposes tractable algorithms for their implementation. We leverage the fact\nthat, for asymmetric networks, every admissible method must be contained\nbetween reciprocal and nonreciprocal clustering, and describe three families of\nintermediate methods. Grafting methods exchange branches between dendrograms\ngenerated by different admissible methods. The convex combination family\ncombines admissible methods through a convex operation in the space of\ndendrograms, and thirdly, the semi-reciprocal family clusters nodes that are\nrelated by strong cyclic influences in the network. Algorithms for the\ncomputation of hierarchical clusters generated by reciprocal and nonreciprocal\nclustering as well as the grafting, convex combination, and semi-reciprocal\nfamilies are derived using matrix operations in a dioid algebra. Finally, the\nintroduced clustering methods and algorithms are exemplified through their\napplication to a network describing the interrelation between sectors of the\nUnited States (U.S.) economy.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 14:22:12 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Carlsson", "Gunnar", ""], ["M\u00e9moli", "Facundo", ""], ["Ribeiro", "Alejandro", ""], ["Segarra", "Santiago", ""]]}, {"id": "1607.06339", "submitter": "Santiago Segarra", "authors": "Gunnar Carlsson, Facundo M\\'emoli, Alejandro Ribeiro, Santiago Segarra", "title": "Excisive Hierarchical Clustering Methods for Network Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two practical properties of hierarchical clustering methods for\n(possibly asymmetric) network data: excisiveness and linear scale preservation.\nThe latter enforces imperviousness to change in units of measure whereas the\nformer ensures local consistency of the clustering outcome. Algorithmically,\nexcisiveness implies that we can reduce computational complexity by only\nclustering a data subset of interest while theoretically guaranteeing that the\nsame hierarchical outcome would be observed when clustering the whole dataset.\nMoreover, we introduce the concept of representability, i.e. a generative model\nfor describing clustering methods through the specification of their action on\na collection of networks. We further show that, within a rich set of admissible\nmethods, requiring representability is equivalent to requiring both\nexcisiveness and linear scale preservation. Leveraging this equivalence, we\nshow that all excisive and linear scale preserving methods can be factored into\ntwo steps: a transformation of the weights in the input network followed by the\napplication of a canonical clustering method. Furthermore, their factorization\ncan be used to show stability of excisive and linear scale preserving methods\nin the sense that a bounded perturbation in the input network entails a bounded\nperturbation in the clustering output.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 14:28:51 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Carlsson", "Gunnar", ""], ["M\u00e9moli", "Facundo", ""], ["Ribeiro", "Alejandro", ""], ["Segarra", "Santiago", ""]]}, {"id": "1607.06364", "submitter": "Simone Scardapane", "authors": "Simone Scardapane", "title": "Distributed Supervised Learning using Neural Networks", "comments": "Author's Ph.D. thesis (DIET Dept., Sapienza University of Rome, May\n  2016). Supervisor: Prof. Aurelio Uncini", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed learning is the problem of inferring a function in the case where\ntraining data is distributed among multiple geographically separated sources.\nParticularly, the focus is on designing learning strategies with low\ncomputational requirements, in which communication is restricted only to\nneighboring agents, with no reliance on a centralized authority. In this\nthesis, we analyze multiple distributed protocols for a large number of neural\nnetwork architectures. The first part of the thesis is devoted to a definition\nof the problem, followed by an extensive overview of the state-of-the-art.\nNext, we introduce different strategies for a relatively simple class of single\nlayer neural networks, where a linear output layer is preceded by a nonlinear\nlayer, whose weights are stochastically assigned in the beginning of the\nlearning process. We consider both batch and sequential learning, with\nhorizontally and vertically partitioned data. In the third part, we consider\ninstead the more complex problem of semi-supervised distributed learning, where\neach agent is provided with an additional set of unlabeled training samples. We\npropose two different algorithms based on diffusion processes for linear\nsupport vector machines and kernel ridge regression. Subsequently, the fourth\npart extends the discussion to learning with time-varying data (e.g.\ntime-series) using recurrent neural networks. We consider two different\nfamilies of networks, namely echo state networks (extending the algorithms\nintroduced in the second part), and spline adaptive filters. Overall, the\nalgorithms presented throughout the thesis cover a wide range of possible\npractical applications, and lead the way to numerous future extensions, which\nare briefly summarized in the conclusive chapter.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 15:32:47 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Scardapane", "Simone", ""]]}, {"id": "1607.06450", "submitter": "Jimmy Ba", "authors": "Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton", "title": "Layer Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training state-of-the-art, deep neural networks is computationally expensive.\nOne way to reduce the training time is to normalize the activities of the\nneurons. A recently introduced technique called batch normalization uses the\ndistribution of the summed input to a neuron over a mini-batch of training\ncases to compute a mean and variance which are then used to normalize the\nsummed input to that neuron on each training case. This significantly reduces\nthe training time in feed-forward neural networks. However, the effect of batch\nnormalization is dependent on the mini-batch size and it is not obvious how to\napply it to recurrent neural networks. In this paper, we transpose batch\nnormalization into layer normalization by computing the mean and variance used\nfor normalization from all of the summed inputs to the neurons in a layer on a\nsingle training case. Like batch normalization, we also give each neuron its\nown adaptive bias and gain which are applied after the normalization but before\nthe non-linearity. Unlike batch normalization, layer normalization performs\nexactly the same computation at training and test times. It is also\nstraightforward to apply to recurrent neural networks by computing the\nnormalization statistics separately at each time step. Layer normalization is\nvery effective at stabilizing the hidden state dynamics in recurrent networks.\nEmpirically, we show that layer normalization can substantially reduce the\ntraining time compared with previously published techniques.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 19:57:52 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Ba", "Jimmy Lei", ""], ["Kiros", "Jamie Ryan", ""], ["Hinton", "Geoffrey E.", ""]]}, {"id": "1607.06520", "submitter": "Tolga Bolukbasi", "authors": "Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam\n  Kalai", "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The blind application of machine learning runs the risk of amplifying biases\npresent in data. Such a danger is facing us with word embedding, a popular\nframework to represent text data as vectors which has been used in many machine\nlearning and natural language processing tasks. We show that even word\nembeddings trained on Google News articles exhibit female/male gender\nstereotypes to a disturbing extent. This raises concerns because their\nwidespread use, as we describe, often tends to amplify these biases.\nGeometrically, gender bias is first shown to be captured by a direction in the\nword embedding. Second, gender neutral words are shown to be linearly separable\nfrom gender definition words in the word embedding. Using these properties, we\nprovide a methodology for modifying an embedding to remove gender stereotypes,\nsuch as the association between between the words receptionist and female,\nwhile maintaining desired associations such as between the words queen and\nfemale. We define metrics to quantify both direct and indirect gender biases in\nembeddings, and develop algorithms to \"debias\" the embedding. Using\ncrowd-worker evaluation as well as standard benchmarks, we empirically\ndemonstrate that our algorithms significantly reduce gender bias in embeddings\nwhile preserving the its useful properties such as the ability to cluster\nrelated concepts and to solve analogy tasks. The resulting embeddings can be\nused in applications without amplifying gender bias.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 22:26:20 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Bolukbasi", "Tolga", ""], ["Chang", "Kai-Wei", ""], ["Zou", "James", ""], ["Saligrama", "Venkatesh", ""], ["Kalai", "Adam", ""]]}, {"id": "1607.06525", "submitter": "Xi Zhang", "authors": "Xi Zhang and Di Ma and Lin Gan and Shanshan Jiang and Gady Agam", "title": "CGMOS: Certainty Guided Minority OverSampling", "comments": "Accepted by The 25th ACM International Conference on Information and\n  Knowledge Management (CIKM 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handling imbalanced datasets is a challenging problem that if not treated\ncorrectly results in reduced classification performance. Imbalanced datasets\nare commonly handled using minority oversampling, whereas the SMOTE algorithm\nis a successful oversampling algorithm with numerous extensions. SMOTE\nextensions do not have a theoretical guarantee during training to work better\nthan SMOTE and in many instances their performance is data dependent. In this\npaper we propose a novel extension to the SMOTE algorithm with a theoretical\nguarantee for improved classification performance. The proposed approach\nconsiders the classification performance of both the majority and minority\nclasses. In the proposed approach CGMOS (Certainty Guided Minority\nOverSampling) new data points are added by considering certainty changes in the\ndataset. The paper provides a proof that the proposed algorithm is guaranteed\nto work better than SMOTE for training data. Further experimental results on 30\nreal-world datasets show that CGMOS works better than existing algorithms when\nusing 6 different classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 23:09:46 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Zhang", "Xi", ""], ["Ma", "Di", ""], ["Gan", "Lin", ""], ["Jiang", "Shanshan", ""], ["Agam", "Gady", ""]]}, {"id": "1607.06657", "submitter": "Ge Ou", "authors": "Yan Wang, Ge Ou, Wei Pang, Lan Huang, George Macleod Coghill", "title": "e-Distance Weighted Support Vector Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel support vector regression approach called e-Distance\nWeighted Support Vector Regression (e-DWSVR).e-DWSVR specifically addresses two\nchallenging issues in support vector regression: first, the process of noisy\ndata; second, how to deal with the situation when the distribution of boundary\ndata is different from that of the overall data. The proposed e-DWSVR optimizes\nthe minimum margin and the mean of functional margin simultaneously to tackle\nthese two issues. In addition, we use both dual coordinate descent (CD) and\naveraged stochastic gradient descent (ASGD) strategies to make e-DWSVR scalable\nto large scale problems. We report promising results obtained by e-DWSVR in\ncomparison with existing methods on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 02:35:57 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 05:03:31 GMT"}, {"version": "v3", "created": "Wed, 31 Aug 2016 08:28:10 GMT"}, {"version": "v4", "created": "Thu, 27 Oct 2016 10:47:49 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Wang", "Yan", ""], ["Ou", "Ge", ""], ["Pang", "Wei", ""], ["Huang", "Lan", ""], ["Coghill", "George Macleod", ""]]}, {"id": "1607.06781", "submitter": "Fabio Massimo Zennaro", "authors": "Fabio Massimo Zennaro, Ke Chen", "title": "On the Use of Sparse Filtering for Covariate Shift Adaptation", "comments": "40 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we formally analyse the use of sparse filtering algorithms to\nperform covariate shift adaptation. We provide a theoretical analysis of sparse\nfiltering by evaluating the conditions required to perform covariate shift\nadaptation. We prove that sparse filtering can perform adaptation only if the\nconditional distribution of the labels has a structure explained by a cosine\nmetric. To overcome this limitation, we propose a new algorithm, named periodic\nsparse filtering, and carry out the same theoretical analysis regarding\ncovariate shift adaptation. We show that periodic sparse filtering can perform\nadaptation under the looser and more realistic requirement that the conditional\ndistribution of the labels has a periodic structure, which may be satisfied,\nfor instance, by user-dependent data sets. We experimentally validate our\ntheoretical results on synthetic data. Moreover, we apply periodic sparse\nfiltering to real-world data sets to demonstrate that this simple and\ncomputationally efficient algorithm is able to achieve competitive\nperformances.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 18:17:10 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 14:45:16 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Zennaro", "Fabio Massimo", ""], ["Chen", "Ke", ""]]}, {"id": "1607.06988", "submitter": "Shankar Vembu", "authors": "Shankar Vembu, Sandra Zilles", "title": "Interactive Learning from Multiple Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive learning is a process in which a machine learning algorithm is\nprovided with meaningful, well-chosen examples as opposed to randomly chosen\nexamples typical in standard supervised learning. In this paper, we propose a\nnew method for interactive learning from multiple noisy labels where we exploit\nthe disagreement among annotators to quantify the easiness (or meaningfulness)\nof an example. We demonstrate the usefulness of this method in estimating the\nparameters of a latent variable classification model, and conduct experimental\nanalyses on a range of synthetic and benchmark datasets. Furthermore, we\ntheoretically analyze the performance of perceptron in this interactive\nlearning framework.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 01:14:19 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Vembu", "Shankar", ""], ["Zilles", "Sandra", ""]]}, {"id": "1607.06996", "submitter": "Weizhong Zhang", "authors": "Weizhong Zhang and Bin Hong and Wei Liu and Jieping Ye and Deng Cai\n  and Xiaofei He and Jie Wang", "title": "Scaling Up Sparse Support Vector Machines by Simultaneous Feature and\n  Sample Reduction", "comments": "accepted by JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse support vector machine (SVM) is a popular classification technique\nthat can simultaneously learn a small set of the most interpretable features\nand identify the support vectors. It has achieved great successes in many\nreal-world applications. However, for large-scale problems involving a huge\nnumber of samples and ultra-high dimensional features, solving sparse SVMs\nremains challenging. By noting that sparse SVMs induce sparsities in both\nfeature and sample spaces, we propose a novel approach, which is based on\naccurate estimations of the primal and dual optima of sparse SVMs, to\nsimultaneously identify the inactive features and samples that are guaranteed\nto be irrelevant to the outputs. Thus, we can remove the identified inactive\nsamples and features from the training phase, leading to substantial savings in\nthe computational cost without sacrificing the accuracy. Moreover, we show that\nour method can be extended to multi-class sparse support vector machines. To\nthe best of our knowledge, the proposed method is the \\emph{first}\n\\emph{static} feature and sample reduction method for sparse SVMs and\nmulti-class sparse SVMs. Experiments on both synthetic and real data sets\ndemonstrate that our approach significantly outperforms state-of-the-art\nmethods and the speedup gained by our approach can be orders of magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 04:00:30 GMT"}, {"version": "v2", "created": "Sun, 14 Aug 2016 18:45:52 GMT"}, {"version": "v3", "created": "Sat, 25 Feb 2017 14:25:20 GMT"}, {"version": "v4", "created": "Wed, 8 Mar 2017 02:07:36 GMT"}, {"version": "v5", "created": "Fri, 23 Jun 2017 08:33:14 GMT"}, {"version": "v6", "created": "Thu, 18 Jul 2019 10:21:41 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Zhang", "Weizhong", ""], ["Hong", "Bin", ""], ["Liu", "Wei", ""], ["Ye", "Jieping", ""], ["Cai", "Deng", ""], ["He", "Xiaofei", ""], ["Wang", "Jie", ""]]}, {"id": "1607.07034", "submitter": "Aarti Sathyanarayana", "authors": "Aarti Sathyanarayana, Shafiq Joty, Luis Fernandez-Luque, Ferda Ofli,\n  Jaideep Srivastava, Ahmed Elmagarmid, Shahrad Taheri, Teresa Arora", "title": "Impact of Physical Activity on Sleep:A Deep Learning Based Exploration", "comments": null, "journal-ref": "JMIR Mhealth Uhealth 2016;4(4):e125", "doi": "10.2196/mhealth.6562", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of sleep is paramount for maintaining physical, emotional and\nmental wellbeing. Though the relationship between sleep and physical activity\nis known to be important, it is not yet fully understood. The explosion in\npopularity of actigraphy and wearable devices, provides a unique opportunity to\nunderstand this relationship. Leveraging this information source requires new\ntools to be developed to facilitate data-driven research for sleep and activity\npatient-recommendations.\n  In this paper we explore the use of deep learning to build sleep quality\nprediction models based on actigraphy data. We first use deep learning as a\npure model building device by performing human activity recognition (HAR) on\nraw sensor data, and using deep learning to build sleep prediction models. We\ncompare the deep learning models with those build using classical approaches,\ni.e. logistic regression, support vector machines, random forest and adaboost.\nSecondly, we employ the advantage of deep learning with its ability to handle\nhigh dimensional datasets. We explore several deep learning models on the raw\nwearable sensor output without performing HAR or any other feature extraction.\n  Our results show that using a convolutional neural network on the raw\nwearables output improves the predictive value of sleep quality from physical\nactivity, by an additional 8% compared to state-of-the-art non-deep learning\napproaches, which itself shows a 15% improvement over current practice.\nMoreover, utilizing deep learning on raw data eliminates the need for data\npre-processing and simplifies the overall workflow to analyze actigraphy data\nfor sleep and physical activity research.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 12:12:03 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Sathyanarayana", "Aarti", ""], ["Joty", "Shafiq", ""], ["Fernandez-Luque", "Luis", ""], ["Ofli", "Ferda", ""], ["Srivastava", "Jaideep", ""], ["Elmagarmid", "Ahmed", ""], ["Taheri", "Shahrad", ""], ["Arora", "Teresa", ""]]}, {"id": "1607.07043", "submitter": "Amir Shahroudy", "authors": "Jun Liu, Amir Shahroudy, Dong Xu, and Gang Wang", "title": "Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D action recognition - analysis of human actions based on 3D skeleton data -\nbecomes popular recently due to its succinctness, robustness, and\nview-invariant representation. Recent attempts on this problem suggested to\ndevelop RNN-based learning methods to model the contextual dependency in the\ntemporal domain. In this paper, we extend this idea to spatio-temporal domains\nto analyze the hidden sources of action-related information within the input\ndata over both domains concurrently. Inspired by the graphical structure of the\nhuman skeleton, we further propose a more powerful tree-structure based\ntraversal method. To handle the noise and occlusion in 3D skeleton data, we\nintroduce new gating mechanism within LSTM to learn the reliability of the\nsequential input data and accordingly adjust its effect on updating the\nlong-term context information stored in the memory cell. Our method achieves\nstate-of-the-art performance on 4 challenging benchmark datasets for 3D human\naction analysis.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 13:39:11 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Liu", "Jun", ""], ["Shahroudy", "Amir", ""], ["Xu", "Dong", ""], ["Wang", "Gang", ""]]}, {"id": "1607.07086", "submitter": "Dzmitry Bahdanau", "authors": "Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan\n  Lowe, Joelle Pineau, Aaron Courville, Yoshua Bengio", "title": "An Actor-Critic Algorithm for Sequence Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to training neural networks to generate sequences\nusing actor-critic methods from reinforcement learning (RL). Current\nlog-likelihood training methods are limited by the discrepancy between their\ntraining and testing modes, as models must generate tokens conditioned on their\nprevious guesses rather than the ground-truth tokens. We address this problem\nby introducing a \\textit{critic} network that is trained to predict the value\nof an output token, given the policy of an \\textit{actor} network. This results\nin a training procedure that is much closer to the test phase, and allows us to\ndirectly optimize for a task-specific score such as BLEU. Crucially, since we\nleverage these techniques in the supervised learning setting rather than the\ntraditional RL setting, we condition the critic network on the ground-truth\noutput. We show that our method leads to improved performance on both a\nsynthetic task, and for German-English machine translation. Our analysis paves\nthe way for such methods to be applied in natural language generation tasks,\nsuch as machine translation, caption generation, and dialogue modelling.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 20:05:07 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 16:08:30 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 15:43:52 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Bahdanau", "Dzmitry", ""], ["Brakel", "Philemon", ""], ["Xu", "Kelvin", ""], ["Goyal", "Anirudh", ""], ["Lowe", "Ryan", ""], ["Pineau", "Joelle", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1607.07110", "submitter": "Hrushikesh Mhaskar", "authors": "Charles K. Chui, H. N. Mhaskar", "title": "Deep nets for local manifold learning", "comments": "Submitted on Sept. 17, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of extending a function $f$ defined on a training data\n$\\mathcal{C}$ on an unknown manifold $\\mathbb{X}$ to the entire manifold and a\ntubular neighborhood of this manifold is considered in this paper. For\n$\\mathbb{X}$ embedded in a high dimensional ambient Euclidean space\n$\\mathbb{R}^D$, a deep learning algorithm is developed for finding a local\ncoordinate system for the manifold {\\bf without eigen--decomposition}, which\nreduces the problem to the classical problem of function approximation on a low\ndimensional cube. Deep nets (or multilayered neural networks) are proposed to\naccomplish this approximation scheme by using the training data. Our methods do\nnot involve such optimization techniques as back--propagation, while assuring\noptimal (a priori) error bounds on the output in terms of the number of\nderivatives of the target function. In addition, these methods are universal,\nin that they do not require a prior knowledge of the smoothness of the target\nfunction, but adjust the accuracy of approximation locally and automatically,\ndepending only upon the local smoothness of the target function. Our ideas are\neasily extended to solve both the pre--image problem and the out--of--sample\nextension problem, with a priori bounds on the growth of the function thus\nextended.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 23:23:32 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Chui", "Charles K.", ""], ["Mhaskar", "H. N.", ""]]}, {"id": "1607.07186", "submitter": "Alessandro Rozza", "authors": "Pietro Cassara and Alessandro Rozza and Mirco Nanni", "title": "A Cross-Entropy-based Method to Perform Information-based Feature\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a machine learning point of view, identifying a subset of relevant\nfeatures from a real data set can be useful to improve the results achieved by\nclassification methods and to reduce their time and space complexity. To\nachieve this goal, feature selection methods are usually employed. These\napproaches assume that the data contains redundant or irrelevant attributes\nthat can be eliminated. In this work, we propose a novel algorithm to manage\nthe optimization problem that is at the foundation of the Mutual Information\nfeature selection methods. Furthermore, our novel approach is able to estimate\nautomatically the number of dimensions to retain. The quality of our method is\nconfirmed by the promising results achieved on standard real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 09:25:25 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 08:57:04 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Cassara", "Pietro", ""], ["Rozza", "Alessandro", ""], ["Nanni", "Mirco", ""]]}, {"id": "1607.07195", "submitter": "Mathieu Blondel", "authors": "Mathieu Blondel, Akinori Fujino, Naonori Ueda and Masakazu Ishihata", "title": "Higher-Order Factorization Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorization machines (FMs) are a supervised learning approach that can use\nsecond-order feature combinations even when the data is very high-dimensional.\nUnfortunately, despite increasing interest in FMs, there exists to date no\nefficient training algorithm for higher-order FMs (HOFMs). In this paper, we\npresent the first generic yet efficient algorithms for training arbitrary-order\nHOFMs. We also present new variants of HOFMs with shared parameters, which\ngreatly reduce model size and prediction times while maintaining similar\naccuracy. We demonstrate the proposed approaches on four different link\nprediction tasks.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 10:19:27 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 06:32:13 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Blondel", "Mathieu", ""], ["Fujino", "Akinori", ""], ["Ueda", "Naonori", ""], ["Ishihata", "Masakazu", ""]]}, {"id": "1607.07270", "submitter": "Francesco Solera", "authors": "Francesco Solera and Andrea Palazzi", "title": "A Statistical Test for Joint Distributions Equivalence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a distribution-free test that can be used to determine whether any\ntwo joint distributions $p$ and $q$ are statistically different by inspection\nof a large enough set of samples. Following recent efforts from Long et al.\n[1], we rely on joint kernel distribution embedding to extend the kernel\ntwo-sample test of Gretton et al. [2] to the case of joint probability\ndistributions. Our main result can be directly applied to verify if a\ndataset-shift has occurred between training and test distributions in a\nlearning framework, without further assuming the shift has occurred only in the\ninput, in the target or in the conditional distribution.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 13:48:20 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Solera", "Francesco", ""], ["Palazzi", "Andrea", ""]]}, {"id": "1607.07330", "submitter": "Kevin Xu", "authors": "Ruthwik R. Junuthula, Kevin S. Xu, and Vijay K. Devabhaktuni", "title": "Evaluating Link Prediction Accuracy on Dynamic Networks with Added and\n  Removed Edges", "comments": "To appear in Proceedings of SocialCom 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of predicting future relationships in a social network, known as\nlink prediction, has been studied extensively in the literature. Many link\nprediction methods have been proposed, ranging from common neighbors to\nprobabilistic models. Recent work by Yang et al. has highlighted several\nchallenges in evaluating link prediction accuracy. In dynamic networks where\nedges are both added and removed over time, the link prediction problem is more\ncomplex and involves predicting both newly added and newly removed edges. This\nresults in new challenges in the evaluation of dynamic link prediction methods,\nand the recommendations provided by Yang et al. are no longer applicable,\nbecause they do not address edge removal. In this paper, we investigate several\nmetrics currently used for evaluating accuracies of dynamic link prediction\nmethods and demonstrate why they can be misleading in many cases. We provide\nseveral recommendations on evaluating dynamic link prediction accuracy,\nincluding separation into two categories of evaluation. Finally we propose a\nunified metric to characterize link prediction accuracy effectively using a\nsingle number.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 16:00:32 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Junuthula", "Ruthwik R.", ""], ["Xu", "Kevin S.", ""], ["Devabhaktuni", "Vijay K.", ""]]}, {"id": "1607.07395", "submitter": "Chuanren Liu", "authors": "Kai Zhang, Chuanren Liu, Jie Zhang, Hui Xiong, Eric Xing, Jieping Ye", "title": "Seeing the Forest from the Trees in Two Looks: Matrix Sketching by\n  Cascaded Bilateral Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix sketching is aimed at finding close approximations of a matrix by\nfactors of much smaller dimensions, which has important applications in\noptimization and machine learning. Given a matrix A of size m by n,\nstate-of-the-art randomized algorithms take O(m * n) time and space to obtain\nits low-rank decomposition. Although quite useful, the need to store or\nmanipulate the entire matrix makes it a computational bottleneck for truly\nlarge and dense inputs. Can we sketch an m-by-n matrix in O(m + n) cost by\naccessing only a small fraction of its rows and columns, without knowing\nanything about the remaining data? In this paper, we propose the cascaded\nbilateral sampling (CABS) framework to solve this problem. We start from\ndemonstrating how the approximation quality of bilateral matrix sketching\ndepends on the encoding powers of sampling. In particular, the sampled rows and\ncolumns should correspond to the code-vectors in the ground truth\ndecompositions. Motivated by this analysis, we propose to first generate a\npilot-sketch using simple random sampling, and then pursue more advanced,\n\"follow-up\" sampling on the pilot-sketch factors seeking maximal encoding\npowers. In this cascading process, the rise of approximation quality is shown\nto be lower-bounded by the improvement of encoding powers in the follow-up\nsampling step, thus theoretically guarantees the algorithmic boosting property.\nComputationally, our framework only takes linear time and space, and at the\nsame time its performance rivals the quality of state-of-the-art algorithms\nconsuming a quadratic amount of resources. Empirical evaluations on benchmark\ndata fully demonstrate the potential of our methods in large scale matrix\nsketching and related areas.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 18:28:18 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 14:00:28 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2016 03:52:43 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Zhang", "Kai", ""], ["Liu", "Chuanren", ""], ["Zhang", "Jie", ""], ["Xiong", "Hui", ""], ["Xing", "Eric", ""], ["Ye", "Jieping", ""]]}, {"id": "1607.07405", "submitter": "Ankur Handa", "authors": "Ankur Handa, Michael Bloesch, Viorica Patraucean, Simon Stent, John\n  McCormac, Andrew Davison", "title": "gvnn: Neural Network Library for Geometric Computer Vision", "comments": "Submitted to ECCV Workshop on Deep Geometry", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce gvnn, a neural network library in Torch aimed towards bridging\nthe gap between classic geometric computer vision and deep learning. Inspired\nby the recent success of Spatial Transformer Networks, we propose several new\nlayers which are often used as parametric transformations on the data in\ngeometric computer vision. These layers can be inserted within a neural network\nmuch in the spirit of the original spatial transformers and allow\nbackpropagation to enable end-to-end learning of a network involving any domain\nknowledge in geometric computer vision. This opens up applications in learning\ninvariance to 3D geometric transformation for place recognition, end-to-end\nvisual odometry, depth estimation and unsupervised learning through warping\nwith a parametric transformation for image reconstruction error.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 18:57:17 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2016 22:49:32 GMT"}, {"version": "v3", "created": "Fri, 12 Aug 2016 17:28:24 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Handa", "Ankur", ""], ["Bloesch", "Michael", ""], ["Patraucean", "Viorica", ""], ["Stent", "Simon", ""], ["McCormac", "John", ""], ["Davison", "Andrew", ""]]}, {"id": "1607.07423", "submitter": "Arin Chaudhuri", "authors": "Deovrat Kakde, Sergriy Peredriy, Arin Chaudhuri, Anya Mcguirk", "title": "A Non-Parametric Control Chart For High Frequency Multivariate Data", "comments": null, "journal-ref": null, "doi": "10.1109/RAM.2017.7889786", "report-no": null, "categories": "cs.LG stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Data Description (SVDD) is a machine learning technique used\nfor single class classification and outlier detection. SVDD based K-chart was\nfirst introduced by Sun and Tsung for monitoring multivariate processes when\nunderlying distribution of process parameters or quality characteristics depart\nfrom Normality. The method first trains a SVDD model on data obtained from\nstable or in-control operations of the process to obtain a threshold $R^2$ and\nkernel center a. For each new observation, its Kernel distance from the Kernel\ncenter a is calculated. The kernel distance is compared against the threshold\n$R^2$ to determine if the observation is within the control limits. The\nnon-parametric K-chart provides an attractive alternative to the traditional\ncontrol charts such as the Hotelling's $T^2$ charts when distribution of the\nunderlying multivariate data is either non-normal or is unknown. But there are\nchallenges when K-chart is deployed in practice. The K-chart requires\ncalculating kernel distance of each new observation but there are no guidelines\non how to interpret the kernel distance plot and infer about shifts in process\nmean or changes in process variation. This limits the application of K-charts\nin big-data applications such as equipment health monitoring, where\nobservations are generated at a very high frequency. In this scenario, the\nanalyst using the K-chart is inundated with kernel distance results at a very\nhigh frequency, generally without any recourse for detecting presence of any\nassignable causes of variation. We propose a new SVDD based control chart,\ncalled as $K_T$ chart, which addresses challenges encountered when using\nK-chart for big-data applications. The $K_T$ charts can be used to\nsimultaneously track process variation and central tendency. We illustrate the\nsuccessful use of $K_T$ chart using the Tennessee Eastman process data.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 19:40:55 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 22:23:47 GMT"}, {"version": "v3", "created": "Fri, 29 Jul 2016 20:31:54 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Kakde", "Deovrat", ""], ["Peredriy", "Sergriy", ""], ["Chaudhuri", "Arin", ""], ["Mcguirk", "Anya", ""]]}, {"id": "1607.07519", "submitter": "Phuoc Nguyen", "authors": "Phuoc Nguyen, Truyen Tran, Nilmini Wickramasinghe, Svetha Venkatesh", "title": "Deepr: A Convolutional Net for Medical Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature engineering remains a major bottleneck when creating predictive\nsystems from electronic medical records. At present, an important missing\nelement is detecting predictive regular clinical motifs from irregular episodic\nrecords. We present Deepr (short for Deep record), a new end-to-end deep\nlearning system that learns to extract features from medical records and\npredicts future risk automatically. Deepr transforms a record into a sequence\nof discrete elements separated by coded time gaps and hospital transfers. On\ntop of the sequence is a convolutional neural net that detects and combines\npredictive local clinical motifs to stratify the risk. Deepr permits\ntransparent inspection and visualization of its inner working. We validate\nDeepr on hospital data to predict unplanned readmission after discharge. Deepr\nachieves superior accuracy compared to traditional techniques, detects\nmeaningful clinical motifs, and uncovers the underlying structure of the\ndisease and intervention space.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 02:06:33 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Nguyen", "Phuoc", ""], ["Tran", "Truyen", ""], ["Wickramasinghe", "Nilmini", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1607.07526", "submitter": "Zhi-Hua Zhou", "authors": "Wei Gao and Bin-Bin Yang and Zhi-Hua Zhou", "title": "On the Resistance of Nearest Neighbor to Random Noisy Labels", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor has always been one of the most appealing non-parametric\napproaches in machine learning, pattern recognition, computer vision, etc.\nPrevious empirical studies partly shows that nearest neighbor is resistant to\nnoise, yet there is a lack of deep analysis. This work presents the\nfinite-sample and distribution-dependent bounds on the consistency of nearest\nneighbor in the random noise setting. The theoretical results show that, for\nasymmetric noises, k-nearest neighbor is robust enough to classify most data\ncorrectly, except for a handful of examples, whose labels are totally misled by\nrandom noises. For symmetric noises, however, k-nearest neighbor achieves the\nsame consistent rate as that of noise-free setting, which verifies the\nresistance of k-nearest neighbor to random noisy labels. Motivated by the\ntheoretical analysis, we propose the Robust k-Nearest Neighbor (RkNN) approach\nto deal with noisy labels. The basic idea is to make unilateral corrections to\nexamples, whose labels are totally misled by random noises, and classify the\nothers directly by utilizing the robustness of k-nearest neighbor. We verify\nthe effectiveness of the proposed algorithm both theoretically and empirically.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 02:58:16 GMT"}, {"version": "v2", "created": "Sun, 18 Sep 2016 07:06:26 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 06:47:10 GMT"}, {"version": "v4", "created": "Thu, 15 Feb 2018 00:55:24 GMT"}, {"version": "v5", "created": "Thu, 13 Sep 2018 14:45:07 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Gao", "Wei", ""], ["Yang", "Bin-Bin", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1607.07590", "submitter": "Satoru Tokuda", "authors": "Satoru Tokuda, Kenji Nagata, and Masato Okada", "title": "Simultaneous Estimation of Noise Variance and Number of Peaks in\n  Bayesian Spectral Deconvolution", "comments": null, "journal-ref": null, "doi": "10.7566/JPSJ.86.024001", "report-no": null, "categories": "physics.data-an cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The heuristic identification of peaks from noisy complex spectra often leads\nto misunderstanding of the physical and chemical properties of matter. In this\npaper, we propose a framework based on Bayesian inference, which enables us to\nseparate multipeak spectra into single peaks statistically and consists of two\nsteps. The first step is estimating both the noise variance and the number of\npeaks as hyperparameters based on Bayes free energy, which generally is not\nanalytically tractable. The second step is fitting the parameters of each peak\nfunction to the given spectrum by calculating the posterior density, which has\na problem of local minima and saddles since multipeak models are nonlinear and\nhierarchical. Our framework enables the escape from local minima or saddles by\nusing the exchange Monte Carlo method and calculates Bayes free energy via the\nmultiple histogram method. We discuss a simulation demonstrating how efficient\nour framework is and show that estimating both the noise variance and the\nnumber of peaks prevents overfitting, overpenalizing, and misunderstanding the\nprecision of parameter estimation.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 08:36:41 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 11:43:21 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Tokuda", "Satoru", ""], ["Nagata", "Kenji", ""], ["Okada", "Masato", ""]]}, {"id": "1607.07607", "submitter": "Gianna Maria Del Corso", "authors": "Gianna M. Del Corso and Francesco Romani", "title": "Adaptive Nonnegative Matrix Factorization and Measure Comparisons for\n  Recommender Systems", "comments": null, "journal-ref": "Applied Mathematics and Computation 354, pp. 164-179, 2019", "doi": "10.1016/j.amc.2019.01.047", "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nonnegative Matrix Factorization (NMF) of the rating matrix has shown to\nbe an effective method to tackle the recommendation problem. In this paper we\npropose new methods based on the NMF of the rating matrix and we compare them\nwith some classical algorithms such as the SVD and the regularized and\nunregularized non-negative matrix factorization approach. In particular a new\nalgorithm is obtained changing adaptively the function to be minimized at each\nstep, realizing a sort of dynamic prior strategy. Another algorithm is obtained\nmodifying the function to be minimized in the NMF formulation by enforcing the\nreconstruction of the unknown ratings toward a prior term. We then combine\ndifferent methods obtaining two mixed strategies which turn out to be very\neffective in the reconstruction of missing observations. We perform a\nthoughtful comparison of different methods on the basis of several evaluation\nmeasures. We consider in particular rating, classification and ranking measures\nshowing that the algorithm obtaining the best score for a given measure is in\ngeneral the best also when different measures are considered, lowering the\ninterest in designing specific evaluation measures. The algorithms have been\ntested on different datasets, in particular the 1M, and 10M MovieLens datasets\ncontaining ratings on movies, the Jester dataset with ranting on jokes and\nAmazon Fine Foods dataset with ratings on foods. The comparison of the\ndifferent algorithms, shows the good performance of methods employing both an\nexplicit and an implicit regularization scheme. Moreover we can get a boost by\nmixed strategies combining a fast method with a more accurate one.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 09:26:20 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 10:06:23 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 09:11:05 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Del Corso", "Gianna M.", ""], ["Romani", "Francesco", ""]]}, {"id": "1607.07611", "submitter": "Matthew Howard", "authors": "Hsiu-Chin Lin and Matthew Howard", "title": "Learning Null Space Projections in Operational Space Formulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a number of tools have become available that recover the\nunderlying control policy from constrained movements. However, few have\nexplicitly considered learning the constraints of the motion and ways to cope\nwith unknown environment. In this paper, we consider learning the null space\nprojection matrix of a kinematically constrained system in the absence of any\nprior knowledge either on the underlying policy, the geometry, or\ndimensionality of the constraints. Our evaluations have demonstrated the\neffectiveness of the proposed approach on problems of differing dimensionality,\nand with different degrees of non-linearity.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 09:40:23 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Lin", "Hsiu-Chin", ""], ["Howard", "Matthew", ""]]}, {"id": "1607.07684", "submitter": "Vasilis Syrgkanis", "authors": "Tim Roughgarden, Vasilis Syrgkanis, Eva Tardos", "title": "The Price of Anarchy in Auctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey outlines a general and modular theory for proving approximation\nguarantees for equilibria of auctions in complex settings. This theory\ncomplements traditional economic techniques, which generally focus on exact and\noptimal solutions and are accordingly limited to relatively stylized settings.\n  We highlight three user-friendly analytical tools: smoothness-type\ninequalities, which immediately yield approximation guarantees for many auction\nformats of interest in the special case of complete information and\ndeterministic strategies; extension theorems, which extend such guarantees to\nrandomized strategies, no-regret learning outcomes, and incomplete-information\nsettings; and composition theorems, which extend such guarantees from simpler\nto more complex auctions. Combining these tools yields tight worst-case\napproximation guarantees for the equilibria of many widely-used auction\nformats.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 13:23:20 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Roughgarden", "Tim", ""], ["Syrgkanis", "Vasilis", ""], ["Tardos", "Eva", ""]]}, {"id": "1607.07695", "submitter": "Itir Onal Ertugrul", "authors": "Itir Onal Ertugrul, Mete Ozay, Fatos Tunay Yarman Vural", "title": "Hierarchical Multi-resolution Mesh Networks for Brain Decoding", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework, called Hierarchical Multi-resolution Mesh\nNetworks (HMMNs), which establishes a set of brain networks at multiple time\nresolutions of fMRI signal to represent the underlying cognitive process. The\nsuggested framework, first, decomposes the fMRI signal into various frequency\nsubbands using wavelet transforms. Then, a brain network, called mesh network,\nis formed at each subband by ensembling a set of local meshes. The locality\naround each anatomic region is defined with respect to a neighborhood system\nbased on functional connectivity. The arc weights of a mesh are estimated by\nridge regression formed among the average region time series. In the final\nstep, the adjacency matrices of mesh networks obtained at different subbands\nare ensembled for brain decoding under a hierarchical learning architecture,\ncalled, fuzzy stacked generalization (FSG). Our results on Human Connectome\nProject task-fMRI dataset reflect that the suggested HMMN model can\nsuccessfully discriminate tasks by extracting complementary information\nobtained from mesh arc weights of multiple subbands. We study the topological\nproperties of the mesh networks at different resolutions using the network\nmeasures, namely, node degree, node strength, betweenness centrality and global\nefficiency; and investigate the connectivity of anatomic regions, during a\ncognitive task. We observe significant variations among the network topologies\nobtained for different subbands. We, also, analyze the diversity properties of\nclassifier ensemble, trained by the mesh networks in multiple subbands and\nobserve that the classifiers in the ensemble collaborate with each other to\nfuse the complementary information freed at each subband. We conclude that the\nfMRI data, recorded during a cognitive task, embed diverse information across\nthe anatomic regions at each resolution.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 17:26:31 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 20:42:47 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Ertugrul", "Itir Onal", ""], ["Ozay", "Mete", ""], ["Vural", "Fatos Tunay Yarman", ""]]}, {"id": "1607.07751", "submitter": "Franz J. Kir\\'aly", "authors": "Bilal A. Mateen and Matthias Bussas and Catherine Doogan and Denise\n  Waller and Alessia Saverino and Franz J Kir\\'aly and E Diane Playford", "title": "Machine Learning in Falls Prediction; A cognition-based predictor of\n  falls for the acute neurological in-patient population", "comments": null, "journal-ref": null, "doi": "10.1177/0269215518771127", "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background Information: Falls are associated with high direct and indirect\ncosts, and significant morbidity and mortality for patients. Pathological falls\nare usually a result of a compromised motor system, and/or cognition. Very\nlittle research has been conducted on predicting falls based on this premise.\n  Aims: To demonstrate that cognitive and motor tests can be used to create a\nrobust predictive tool for falls.\n  Methods: Three tests of attention and executive function (Stroop, Trail\nMaking, and Semantic Fluency), a measure of physical function (Walk-12), a\nseries of questions (concerning recent falls, surgery and physical function)\nand demographic information were collected from a cohort of 323 patients at a\ntertiary neurological center. The principal outcome was a fall during the\nin-patient stay (n = 54). Data-driven, predictive modelling was employed to\nidentify the statistical modelling strategies which are most accurate in\npredicting falls, and which yield the most parsimonious models of clinical\nrelevance.\n  Results: The Trail test was identified as the best predictor of falls.\nMoreover, addition of any others variables, to the results of the Trail test\ndid not improve the prediction (Wilcoxon signed-rank p < .001). The best\nstatistical strategy for predicting falls was the random forest (Wilcoxon\nsigned-rank p < .001), based solely on results of the Trail test. Tuning of the\nmodel results in the following optimized values: 68% (+- 7.7) sensitivity, 90%\n(+- 2.3) specificity, with a positive predictive value of 60%, when the\nrelevant data is available.\n  Conclusion: Predictive modelling has identified a simple yet powerful machine\nlearning prediction strategy based on a single clinical test, the Trail test.\nPredictive evaluation shows this strategy to be robust, suggesting predictive\nmodelling and machine learning as the standard for future predictive tools.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 17:10:40 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Mateen", "Bilal A.", ""], ["Bussas", "Matthias", ""], ["Doogan", "Catherine", ""], ["Waller", "Denise", ""], ["Saverino", "Alessia", ""], ["Kir\u00e1ly", "Franz J", ""], ["Playford", "E Diane", ""]]}, {"id": "1607.07762", "submitter": "Zi Wang", "authors": "Zi Wang, Stefanie Jegelka, Leslie Pack Kaelbling, Tom\\'as\n  Lozano-P\\'erez", "title": "Focused Model-Learning and Planning for Non-Gaussian Continuous\n  State-Action Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for model learning and planning in stochastic\ndomains with continuous state and action spaces and non-Gaussian transition\nmodels. It is efficient because (1) local models are estimated only when the\nplanner requires them; (2) the planner focuses on the most relevant states to\nthe current planning problem; and (3) the planner focuses on the most\ninformative and/or high-value actions. Our theoretical analysis shows the\nvalidity and asymptotic optimality of the proposed approach. Empirically, we\ndemonstrate the effectiveness of our algorithm on a simulated multi-modal\npushing problem.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 15:48:03 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 18:08:50 GMT"}, {"version": "v3", "created": "Sun, 2 Oct 2016 05:21:17 GMT"}, {"version": "v4", "created": "Sun, 23 Oct 2016 04:05:34 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Wang", "Zi", ""], ["Jegelka", "Stefanie", ""], ["Kaelbling", "Leslie Pack", ""], ["Lozano-P\u00e9rez", "Tom\u00e1s", ""]]}, {"id": "1607.07804", "submitter": "Sai Zhang", "authors": "Sai Zhang, Naresh Shanbhag", "title": "Error-Resilient Machine Learning in Near Threshold Voltage via\n  Classifier Ensemble", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the design of error-resilient machine learning\narchitectures by employing a distributed machine learning framework referred to\nas classifier ensemble (CE). CE combines several simple classifiers to obtain a\nstrong one. In contrast, centralized machine learning employs a single complex\nblock. We compare the random forest (RF) and the support vector machine (SVM),\nwhich are representative techniques from the CE and centralized frameworks,\nrespectively. Employing the dataset from UCI machine learning repository and\narchitectural-level error models in a commercial 45 nm CMOS process, it is\ndemonstrated that RF-based architectures are significantly more robust than SVM\narchitectures in presence of timing errors due to process variations in\nnear-threshold voltage (NTV) regions (0.3 V - 0.7 V). In particular, the RF\narchitecture exhibits a detection accuracy (P_{det}) that varies by 3.2% while\nmaintaining a median P_{det} > 0.9 at a gate level delay variation of 28.9% .\nIn comparison, SVM exhibits a P_{det} that varies by 16.8%. Additionally, we\npropose an error weighted voting technique that incorporates the timing error\nstatistics of the NTV circuit fabric to further enhance robustness. Simulation\nresults confirm that the error weighted voting achieves a P_{det} that varies\nby only 1.4%, which is 12X lower compared to SVM.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 16:34:24 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Zhang", "Sai", ""], ["Shanbhag", "Naresh", ""]]}, {"id": "1607.07817", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Ieva Vasiljeva and Ognjen Arandjelovic", "title": "Prediction of future hospital admissions - what is the tradeoff between\n  specificity and accuracy?", "comments": "In Proc. International Conference on Bioinformatics and Computational\n  Biology, April 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large amounts of electronic medical records collected by hospitals across the\ndeveloped world offer unprecedented possibilities for knowledge discovery using\ncomputer based data mining and machine learning. Notwithstanding significant\nresearch efforts, the use of this data in the prediction of disease development\nhas largely been disappointing. In this paper we examine in detail a recently\nproposed method which has in preliminary experiments demonstrated highly\npromising results on real-world data. We scrutinize the authors' claims that\nthe proposed model is scalable and investigate whether the tradeoff between\nprediction specificity (i.e. the ability of the model to predict a wide number\nof different ailments) and accuracy (i.e. the ability of the model to make the\ncorrect prediction) is practically viable. Our experiments conducted on a data\ncorpus of nearly 3,000,000 admissions support the authors' expectations and\ndemonstrate that the high prediction accuracy is maintained well even when the\nnumber of admission types explicitly included in the model is increased to\naccount for 98% of all admissions in the corpus. Thus several promising\ndirections for future work are highlighted.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2016 00:09:21 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Vasiljeva", "Ieva", ""], ["Arandjelovic", "Ognjen", ""]]}, {"id": "1607.07837", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yuanzhi Li", "title": "First Efficient Convergence for Streaming k-PCA: a Global, Gap-Free, and\n  Near-Optimal Rate", "comments": "REMARK: v4 adds discussions and polishes writing; v3 contains a\n  stronger Theorem 2, a new lower bound Theorem 6, as well as new Oja++ results\n  Theorem 4 and Theorem 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study streaming principal component analysis (PCA), that is to find, in\n$O(dk)$ space, the top $k$ eigenvectors of a $d\\times d$ hidden matrix $\\bf\n\\Sigma$ with online vectors drawn from covariance matrix $\\bf \\Sigma$.\n  We provide $\\textit{global}$ convergence for Oja's algorithm which is\npopularly used in practice but lacks theoretical understanding for $k>1$. We\nalso provide a modified variant $\\mathsf{Oja}^{++}$ that runs $\\textit{even\nfaster}$ than Oja's. Our results match the information theoretic lower bound in\nterms of dependency on error, on eigengap, on rank $k$, and on dimension $d$,\nup to poly-log factors. In addition, our convergence rate can be made gap-free,\nthat is proportional to the approximation error and independent of the\neigengap.\n  In contrast, for general rank $k$, before our work (1) it was open to design\nany algorithm with efficient global convergence rate; and (2) it was open to\ndesign any algorithm with (even local) gap-free convergence rate in $O(dk)$\nspace.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 18:46:21 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 02:00:20 GMT"}, {"version": "v3", "created": "Fri, 4 Nov 2016 17:09:52 GMT"}, {"version": "v4", "created": "Mon, 17 Apr 2017 02:40:11 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1607.07903", "submitter": "Ericsson Marin", "authors": "Ericsson Marin, Ahmad Diab and Paulo Shakarian", "title": "Product Offerings in Malicious Hacker Markets", "comments": "3 pages, 1 figure, 3 tables. Accepted for publication in IEEE\n  Intelligence and Security Informatics (ISI2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marketplaces specializing in malicious hacking products - including malware\nand exploits - have recently become more prominent on the darkweb and deepweb.\nWe scrape 17 such sites and collect information about such products in a\nunified database schema. Using a combination of manual labeling and\nunsupervised clustering, we examine a corpus of products in order to understand\ntheir various categories and how they become specialized with respect to vendor\nand marketplace. This initial study presents how we effectively employed\nunsupervised techniques to this data as well as the types of insights we gained\non various categories of malicious hacking products.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 21:32:11 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Marin", "Ericsson", ""], ["Diab", "Ahmad", ""], ["Shakarian", "Paulo", ""]]}, {"id": "1607.07939", "submitter": "Ali Ghadirzadeh", "authors": "Ali Ghadirzadeh, Judith B\\\"utepage, Atsuto Maki, Danica Kragic and\n  M{\\aa}rten Bj\\\"orkman", "title": "A Sensorimotor Reinforcement Learning Framework for Physical Human-Robot\n  Interaction", "comments": "The paper is accepted for publication at the 2016 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling of physical human-robot collaborations is generally a challenging\nproblem due to the unpredictive nature of human behavior. To address this\nissue, we present a data-efficient reinforcement learning framework which\nenables a robot to learn how to collaborate with a human partner. The robot\nlearns the task from its own sensorimotor experiences in an unsupervised\nmanner. The uncertainty of the human actions is modeled using Gaussian\nprocesses (GP) to implement action-value functions. Optimal action selection\ngiven the uncertain GP model is ensured by Bayesian optimization. We apply the\nframework to a scenario in which a human and a PR2 robot jointly control the\nball position on a plank based on vision and force/torque data. Our\nexperimental results show the suitability of the proposed method in terms of\nfast and data-efficient model learning, optimal action selection under\nuncertainties and equal role sharing between the partners.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 02:29:52 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Ghadirzadeh", "Ali", ""], ["B\u00fctepage", "Judith", ""], ["Maki", "Atsuto", ""], ["Kragic", "Danica", ""], ["Bj\u00f6rkman", "M\u00e5rten", ""]]}, {"id": "1607.07959", "submitter": "Ansaf Salleb-Aouissi", "authors": "Ilia Vovsha, Ansaf Salleb-Aouissi, Anita Raja, Thomas Koch, Alex\n  Rybchuk, Axinia Radeva, Ashwath Rajan, Yiwen Huang, Hatim Diab, Ashish Tomar,\n  and Ronald Wapner", "title": "Using Kernel Methods and Model Selection for Prediction of Preterm Birth", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA. In this revision, we updated page 4 by adding the\n  reference Vovsha et al. (2013) (incorrectly referenced as XXX in the previous\n  version due to double blind reviewing). The bibtex entry is now added to the\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an application of machine learning to the problem of predicting\npreterm birth. We conduct a secondary analysis on a clinical trial dataset\ncollected by the National In- stitute of Child Health and Human Development\n(NICHD) while focusing our attention on predicting different classes of preterm\nbirth. We compare three approaches for deriving predictive models: a support\nvector machine (SVM) approach with linear and non-linear kernels, logistic\nregression with different model selection along with a model based on decision\nrules prescribed by physician experts for prediction of preterm birth. Our\napproach highlights the pre-processing methods applied to handle the inherent\ndynamics, noise and gaps in the data and describe techniques used to handle\nskewed class distributions. Empirical experiments demonstrate significant\nimprovement in predicting preterm birth compared to past work.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 04:56:57 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 12:25:00 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Vovsha", "Ilia", ""], ["Salleb-Aouissi", "Ansaf", ""], ["Raja", "Anita", ""], ["Koch", "Thomas", ""], ["Rybchuk", "Alex", ""], ["Radeva", "Axinia", ""], ["Rajan", "Ashwath", ""], ["Huang", "Yiwen", ""], ["Diab", "Hatim", ""], ["Tomar", "Ashish", ""], ["Wapner", "Ronald", ""]]}, {"id": "1607.08012", "submitter": "Quanming Yao", "authors": "Quanming Yao and James T. Kwok", "title": "Learning of Generalized Low-Rank Models: A Greedy Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning of low-rank matrices is fundamental to many machine learning\napplications. A state-of-the-art algorithm is the rank-one matrix pursuit\n(R1MP). However, it can only be used in matrix completion problems with the\nsquare loss. In this paper, we develop a more flexible greedy algorithm for\ngeneralized low-rank models whose optimization objective can be smooth or\nnonsmooth, general convex or strongly convex. The proposed algorithm has low\nper-iteration time complexity and fast convergence rate. Experimental results\nshow that it is much faster than the state-of-the-art, with comparable or even\nbetter prediction performance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 09:18:25 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Yao", "Quanming", ""], ["Kwok", "James T.", ""]]}, {"id": "1607.08064", "submitter": "Christian Bailer", "authors": "Christian Bailer and Kiran Varanasi and Didier Stricker", "title": "CNN-based Patch Matching for Optical Flow with Thresholded Hinge\n  Embedding Loss", "comments": "Fixed bracket error in equation 3 (it has no major influence in the\n  approach, but on the optimal t value)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning based approaches have not yet achieved their full potential in\noptical flow estimation, where their performance still trails heuristic\napproaches. In this paper, we present a CNN based patch matching approach for\noptical flow estimation. An important contribution of our approach is a novel\nthresholded loss for Siamese networks. We demonstrate that our loss performs\nclearly better than existing losses. It also allows to speed up training by a\nfactor of 2 in our tests. Furthermore, we present a novel way for calculating\nCNN based features for different image scales, which performs better than\nexisting methods. We also discuss new ways of evaluating the robustness of\ntrained features for the application of patch matching for optical flow. An\ninteresting discovery in our paper is that low-pass filtering of feature maps\ncan increase the robustness of features created by CNNs. We proved the\ncompetitive performance of our approach by submitting it to the KITTI 2012,\nKITTI 2015 and MPI-Sintel evaluation portals where we obtained state-of-the-art\nresults on all three datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 12:41:00 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 16:29:19 GMT"}, {"version": "v3", "created": "Thu, 18 May 2017 18:57:55 GMT"}, {"version": "v4", "created": "Mon, 25 May 2020 06:28:24 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Bailer", "Christian", ""], ["Varanasi", "Kiran", ""], ["Stricker", "Didier", ""]]}, {"id": "1607.08085", "submitter": "Maxime Bucher", "authors": "Maxime Bucher (Palaiseau), St\\'ephane Herbin (Palaiseau), Fr\\'ed\\'eric\n  Jurie", "title": "Improving Semantic Embedding Consistency by Metric Learning for\n  Zero-Shot Classification", "comments": "in ECCV 2016, Oct 2016, amsterdam, Netherlands. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of zero-shot image classification. The key\ncontribution of the proposed approach is to control the semantic embedding of\nimages -- one of the main ingredients of zero-shot learning -- by formulating\nit as a metric learning problem. The optimized empirical criterion associates\ntwo types of sub-task constraints: metric discriminating capacity and accurate\nattribute prediction. This results in a novel expression of zero-shot learning\nnot requiring the notion of class in the training phase: only pairs of\nimage/attributes, augmented with a consistency indicator, are given as ground\ntruth. At test time, the learned model can predict the consistency of a test\nimage with a given set of attributes , allowing flexible ways to produce\nrecognition inferences. Despite its simplicity, the proposed approach gives\nstate-of-the-art results on four challenging datasets used for zero-shot\nrecognition evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 13:35:16 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Bucher", "Maxime", "", "Palaiseau"], ["Herbin", "St\u00e9phane", "", "Palaiseau"], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1607.08161", "submitter": "Chlo\\'e-Agathe Azencott", "authors": "Chlo\\'e-Agathe Azencott", "title": "Network-Guided Biomarker Discovery", "comments": "18 pages, 1 figure, published in LNCS 9605 (Machine Learning for\n  Health Informatics)", "journal-ref": null, "doi": "10.1007/978-3-319-50478-0_16", "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying measurable genetic indicators (or biomarkers) of a specific\ncondition of a biological system is a key element of precision medicine. Indeed\nit allows to tailor diagnostic, prognostic and treatment choice to individual\ncharacteristics of a patient. In machine learning terms, biomarker discovery\ncan be framed as a feature selection problem on whole-genome data sets.\nHowever, classical feature selection methods are usually underpowered to\nprocess these data sets, which contain orders of magnitude more features than\nsamples. This can be addressed by making the assumption that genetic features\nthat are linked on a biological network are more likely to work jointly towards\nexplaining the phenotype of interest. We review here three families of methods\nfor feature selection that integrate prior knowledge in the form of networks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 15:53:02 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 13:09:49 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Azencott", "Chlo\u00e9-Agathe", ""]]}, {"id": "1607.08194", "submitter": "Vardan Papyan", "authors": "Vardan Papyan, Yaniv Romano and Michael Elad", "title": "Convolutional Neural Networks Analyzed via Convolutional Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have led to many state-of-the-art results\nspanning through various fields. However, a clear and profound theoretical\nunderstanding of the forward pass, the core algorithm of CNN, is still lacking.\nIn parallel, within the wide field of sparse approximation, Convolutional\nSparse Coding (CSC) has gained increasing attention in recent years. A\ntheoretical study of this model was recently conducted, establishing it as a\nreliable and stable alternative to the commonly practiced patch-based\nprocessing. Herein, we propose a novel multi-layer model, ML-CSC, in which\nsignals are assumed to emerge from a cascade of CSC layers. This is shown to be\ntightly connected to CNN, so much so that the forward pass of the CNN is in\nfact the thresholding pursuit serving the ML-CSC model. This connection brings\na fresh view to CNN, as we are able to attribute to this architecture\ntheoretical claims such as uniqueness of the representations throughout the\nnetwork, and their stable estimation, all guaranteed under simple local\nsparsity conditions. Lastly, identifying the weaknesses in the above pursuit\nscheme, we propose an alternative to the forward pass, which is connected to\ndeconvolutional, recurrent and residual networks, and has better theoretical\nguarantees.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 17:44:05 GMT"}, {"version": "v2", "created": "Sat, 30 Jul 2016 18:58:02 GMT"}, {"version": "v3", "created": "Thu, 6 Oct 2016 21:14:01 GMT"}, {"version": "v4", "created": "Mon, 10 Oct 2016 22:37:55 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Papyan", "Vardan", ""], ["Romano", "Yaniv", ""], ["Elad", "Michael", ""]]}, {"id": "1607.08206", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Hedvig Kjellstrom, Carl Henrik Ek, Bo C. Bertilson", "title": "Diagnostic Prediction Using Discomfort Drawings with IBTM", "comments": "Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the possibility to apply machine learning to make\ndiagnostic predictions using discomfort drawings. A discomfort drawing is an\nintuitive way for patients to express discomfort and pain related symptoms.\nThese drawings have proven to be an effective method to collect patient data\nand make diagnostic decisions in real-life practice. A dataset from real-world\npatient cases is collected for which medical experts provide diagnostic labels.\nNext, we use a factorized multimodal topic model, Inter-Battery Topic Model\n(IBTM), to train a system that can make diagnostic predictions given an unseen\ndiscomfort drawing. The number of output diagnostic labels is determined by\nusing mean-shift clustering on the discomfort drawing. Experimental results\nshow reasonable predictions of diagnostic labels given an unseen discomfort\ndrawing. Additionally, we generate synthetic discomfort drawings with IBTM\ngiven a diagnostic label, which results in typical cases of symptoms. The\npositive result indicates a significant potential of machine learning to be\nused for parts of the pain diagnostic process and to be a decision support\nsystem for physicians and other health care personnel.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 18:20:01 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 16:26:41 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Zhang", "Cheng", ""], ["Kjellstrom", "Hedvig", ""], ["Ek", "Carl Henrik", ""], ["Bertilson", "Bo C.", ""]]}, {"id": "1607.08254", "submitter": "Sashank J. Reddi", "authors": "Sashank J. Reddi, Suvrit Sra, Barnabas Poczos, Alex Smola", "title": "Stochastic Frank-Wolfe Methods for Nonconvex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Frank-Wolfe methods for nonconvex stochastic and finite-sum\noptimization problems. Frank-Wolfe methods (in the convex case) have gained\ntremendous recent interest in machine learning and optimization communities due\nto their projection-free property and their ability to exploit structured\nconstraints. However, our understanding of these algorithms in the nonconvex\nsetting is fairly limited. In this paper, we propose nonconvex stochastic\nFrank-Wolfe methods and analyze their convergence properties. For objective\nfunctions that decompose into a finite-sum, we leverage ideas from variance\nreduction techniques for convex optimization to obtain new variance reduced\nnonconvex Frank-Wolfe methods that have provably faster convergence than the\nclassical Frank-Wolfe method. Finally, we show that the faster convergence\nrates of our variance reduced methods also translate into improved convergence\nrates for the stochastic setting.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 20:03:47 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2016 05:01:34 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Reddi", "Sashank J.", ""], ["Sra", "Suvrit", ""], ["Poczos", "Barnabas", ""], ["Smola", "Alex", ""]]}, {"id": "1607.08289", "submitter": "Gopal P. Sarma", "authors": "Gopal P. Sarma and Nick J. Hay", "title": "Mammalian Value Systems", "comments": "12 pages", "journal-ref": "Informatica Vol. 41 No. 3 (2017)", "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing human values is a topic deeply interwoven with the sciences,\nhumanities, art, and many other human endeavors. In recent years, a number of\nthinkers have argued that accelerating trends in computer science, cognitive\nscience, and related disciplines foreshadow the creation of intelligent\nmachines which meet and ultimately surpass the cognitive abilities of human\nbeings, thereby entangling an understanding of human values with future\ntechnological development. Contemporary research accomplishments suggest\nsophisticated AI systems becoming widespread and responsible for managing many\naspects of the modern world, from preemptively planning users' travel schedules\nand logistics, to fully autonomous vehicles, to domestic robots assisting in\ndaily living. The extrapolation of these trends has been most forcefully\ndescribed in the context of a hypothetical \"intelligence explosion,\" in which\nthe capabilities of an intelligent software agent would rapidly increase due to\nthe presence of feedback loops unavailable to biological organisms. The\npossibility of superintelligent agents, or simply the widespread deployment of\nsophisticated, autonomous AI systems, highlights an important theoretical\nproblem: the need to separate the cognitive and rational capacities of an agent\nfrom the fundamental goal structure, or value system, which constrains and\nguides the agent's actions. The \"value alignment problem\" is to specify a goal\nstructure for autonomous agents compatible with human values. In this brief\narticle, we suggest that recent ideas from affective neuroscience and related\ndisciplines aimed at characterizing neurological and behavioral universals in\nthe mammalian class provide important conceptual foundations relevant to\ndescribing human values. We argue that the notion of \"mammalian value systems\"\npoints to a potential avenue for fundamental research in AI safety and AI\nethics.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 01:22:26 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 18:17:46 GMT"}, {"version": "v3", "created": "Sun, 31 Dec 2017 18:47:10 GMT"}, {"version": "v4", "created": "Mon, 21 Jan 2019 19:29:30 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Sarma", "Gopal P.", ""], ["Hay", "Nick J.", ""]]}, {"id": "1607.08316", "submitter": "Ilija Ilievski", "authors": "Ilija Ilievski and Taimoor Akhtar and Jiashi Feng and Christine\n  Annette Shoemaker", "title": "Efficient Hyperparameter Optimization of Deep Learning Algorithms Using\n  Deterministic RBF Surrogates", "comments": "AAAI-17 Camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically searching for optimal hyperparameter configurations is of\ncrucial importance for applying deep learning algorithms in practice. Recently,\nBayesian optimization has been proposed for optimizing hyperparameters of\nvarious machine learning algorithms. Those methods adopt probabilistic\nsurrogate models like Gaussian processes to approximate and minimize the\nvalidation error function of hyperparameter values. However, probabilistic\nsurrogates require accurate estimates of sufficient statistics (e.g.,\ncovariance) of the error distribution and thus need many function evaluations\nwith a sizeable number of hyperparameters. This makes them inefficient for\noptimizing hyperparameters of deep learning algorithms, which are highly\nexpensive to evaluate. In this work, we propose a new deterministic and\nefficient hyperparameter optimization method that employs radial basis\nfunctions as error surrogates. The proposed mixed integer algorithm, called\nHORD, searches the surrogate for the most promising hyperparameter values\nthrough dynamic coordinate search and requires many fewer function evaluations.\nHORD does well in low dimensions but it is exceptionally better in higher\ndimensions. Extensive evaluations on MNIST and CIFAR-10 for four deep neural\nnetworks demonstrate HORD significantly outperforms the well-established\nBayesian optimization methods such as GP, SMAC, and TPE. For instance, on\naverage, HORD is more than 6 times faster than GP-EI in obtaining the best\nconfiguration of 19 hyperparameters.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 05:03:32 GMT"}, {"version": "v2", "created": "Sat, 21 Jan 2017 03:26:06 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Ilievski", "Ilija", ""], ["Akhtar", "Taimoor", ""], ["Feng", "Jiashi", ""], ["Shoemaker", "Christine Annette", ""]]}, {"id": "1607.08400", "submitter": "Aida Brankovic", "authors": "Aida Brankovic, Alessandro Falsone, Maria Prandini, Luigi Piroddi", "title": "Randomised Algorithm for Feature Selection and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We here introduce a novel classification approach adopted from the nonlinear\nmodel identification framework, which jointly addresses the feature selection\nand classifier design tasks. The classifier is constructed as a polynomial\nexpansion of the original attributes and a model structure selection process is\napplied to find the relevant terms of the model. The selection method\nprogressively refines a probability distribution defined on the model structure\nspace, by extracting sample models from the current distribution and using the\naggregate information obtained from the evaluation of the population of models\nto reinforce the probability of extracting the most important terms. To reduce\nthe initial search space, distance correlation filtering can be applied as a\npreprocessing technique. The proposed method is evaluated and compared to other\nwell-known feature selection and classification methods on standard benchmark\nclassification problems. The results show the effectiveness of the proposed\nmethod with respect to competitor methods both in terms of classification\naccuracy and model complexity. The obtained models have a simple structure,\neasily amenable to interpretation and analysis.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 11:07:31 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Brankovic", "Aida", ""], ["Falsone", "Alessandro", ""], ["Prandini", "Maria", ""], ["Piroddi", "Luigi", ""]]}, {"id": "1607.08456", "submitter": "Matth\\\"aus Kleindessner", "authors": "Matth\\\"aus Kleindessner and Ulrike von Luxburg", "title": "Kernel functions based on triplet comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given only information in the form of similarity triplets \"Object A is more\nsimilar to object B than to object C\" about a data set, we propose two ways of\ndefining a kernel function on the data set. While previous approaches construct\na low-dimensional Euclidean embedding of the data set that reflects the given\nsimilarity triplets, we aim at defining kernel functions that correspond to\nhigh-dimensional embeddings. These kernel functions can subsequently be used to\napply any kernel method to the data set.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 13:46:06 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 21:33:41 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Kleindessner", "Matth\u00e4us", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1607.08634", "submitter": "Jorge Luis Rivero Jlrivero", "authors": "Jorge Luis Rivero P\\'erez and Bernardete Ribeiro", "title": "Attribute Learning for Network Intrusion Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network intrusion detection is one of the most visible uses for Big Data\nanalytics. One of the main problems in this application is the constant rise of\nnew attacks. This scenario, characterized by the fact that not enough labeled\nexamples are available for the new classes of attacks is hardly addressed by\ntraditional machine learning approaches. New findings on the capabilities of\nZero-Shot learning (ZSL) approach makes it an interesting solution for this\nproblem because it has the ability to classify instances of unseen classes. ZSL\nhas inherently two stages: the attribute learning and the inference stage. In\nthis paper we propose a new algorithm for the attribute learning stage of ZSL.\nThe idea is to learn new values for the attributes based on decision trees\n(DT). Our results show that based on the rules extracted from the DT a better\ndistribution for the attribute values can be found. We also propose an\nexperimental setup for the evaluation of ZSL on network intrusion detection\n(NID).\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 20:36:37 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["P\u00e9rez", "Jorge Luis Rivero", ""], ["Ribeiro", "Bernardete", ""]]}, {"id": "1607.08691", "submitter": "Hamidreza Alvari", "authors": "Hamidreza Alvari, Paulo Shakarian, J.E. Kelly Snyder", "title": "A Non-Parametric Learning Approach to Identify Online Human Trafficking", "comments": "Accepted in IEEE Intelligence and Security Informatics 2016\n  Conference (ISI 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human trafficking is among the most challenging law enforcement problems\nwhich demands persistent fight against from all over the globe. In this study,\nwe leverage readily available data from the website \"Backpage\"-- used for\nclassified advertisement-- to discern potential patterns of human trafficking\nactivities which manifest online and identify most likely trafficking related\nadvertisements. Due to the lack of ground truth, we rely on two human analysts\n--one human trafficking victim survivor and one from law enforcement, for\nhand-labeling the small portion of the crawled data. We then present a\nsemi-supervised learning approach that is trained on the available labeled and\nunlabeled data and evaluated on unseen data with further verification of\nexperts.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 06:05:08 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 00:48:29 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Alvari", "Hamidreza", ""], ["Shakarian", "Paulo", ""], ["Snyder", "J. E. Kelly", ""]]}, {"id": "1607.08720", "submitter": "Benjamin Rubinstein", "authors": "Jiazhen He, Benjamin I. P. Rubinstein, James Bailey, Rui Zhang, Sandra\n  Milligan", "title": "TopicResponse: A Marriage of Topic Modelling and Rasch Modelling for\n  Automatic Measurement in MOOCs", "comments": "In preparation for journal submission; Revisions to improve clarity\n  with additional examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the suitability of using automatically discovered topics\nfrom MOOC discussion forums for modelling students' academic abilities. The\nRasch model from psychometrics is a popular generative probabilistic model that\nrelates latent student skill, latent item difficulty, and observed student-item\nresponses within a principled, unified framework. According to scholarly\neducational theory, discovered topics can be regarded as appropriate\nmeasurement items if (1) students' participation across the discovered topics\nis well fit by the Rasch model, and if (2) the topics are interpretable to\nsubject-matter experts as being educationally meaningful. Such Rasch-scaled\ntopics, with associated difficulty levels, could be of potential benefit to\ncurriculum refinement, student assessment and personalised feedback. The\ntechnical challenge that remains, is to discover meaningful topics that\nsimultaneously achieve good statistical fit with the Rasch model. To address\nthis challenge, we combine the Rasch model with non-negative matrix\nfactorisation based topic modelling, jointly fitting both models. We\ndemonstrate the suitability of our approach with quantitative experiments on\ndata from three Coursera MOOCs, and with qualitative survey results on topic\ninterpretability on a Discrete Optimisation MOOC.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 08:17:45 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 04:30:38 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["He", "Jiazhen", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Bailey", "James", ""], ["Zhang", "Rui", ""], ["Milligan", "Sandra", ""]]}, {"id": "1607.08723", "submitter": "Emmanuel Dupoux", "authors": "Emmanuel Dupoux", "title": "Cognitive Science in the era of Artificial Intelligence: A roadmap for\n  reverse-engineering the infant language-learner", "comments": "27 pages, 5 figures, 3 tables, supplementary materials", "journal-ref": "Dupoux, E. (2018). Cognitive science in the era of artificial\n  intelligence: A roadmap for reverse-engineering the infant language learner.\n  Cognition, 173, 43-59", "doi": "10.1016/j.cognition.2017.11.008", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During their first years of life, infants learn the language(s) of their\nenvironment at an amazing speed despite large cross cultural variations in\namount and complexity of the available language input. Understanding this\nsimple fact still escapes current cognitive and linguistic theories. Recently,\nspectacular progress in the engineering science, notably, machine learning and\nwearable technology, offer the promise of revolutionizing the study of\ncognitive development. Machine learning offers powerful learning algorithms\nthat can achieve human-like performance on many linguistic tasks. Wearable\nsensors can capture vast amounts of data, which enable the reconstruction of\nthe sensory experience of infants in their natural environment. The project of\n'reverse engineering' language development, i.e., of building an effective\nsystem that mimics infant's achievements appears therefore to be within reach.\nHere, we analyze the conditions under which such a project can contribute to\nour scientific understanding of early language development. We argue that\ninstead of defining a sub-problem or simplifying the data, computational models\nshould address the full complexity of the learning situation, and take as input\nthe raw sensory signals available to infants. This implies that (1) accessible\nbut privacy-preserving repositories of home data be setup and widely shared,\nand (2) models be evaluated at different linguistic levels through a benchmark\nof psycholinguist tests that can be passed by machines and humans alike, (3)\nlinguistically and psychologically plausible learning architectures be scaled\nup to real data using probabilistic/optimization principles from machine\nlearning. We discuss the feasibility of this approach and present preliminary\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 08:33:10 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 08:07:08 GMT"}, {"version": "v3", "created": "Fri, 28 Oct 2016 12:59:59 GMT"}, {"version": "v4", "created": "Wed, 14 Feb 2018 15:56:51 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Dupoux", "Emmanuel", ""]]}, {"id": "1607.08810", "submitter": "Mathieu Blondel", "authors": "Mathieu Blondel, Masakazu Ishihata, Akinori Fujino, Naonori Ueda", "title": "Polynomial Networks and Factorization Machines: New Insights and\n  Efficient Training Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial networks and factorization machines are two recently-proposed\nmodels that can efficiently use feature interactions in classification and\nregression tasks. In this paper, we revisit both models from a unified\nperspective. Based on this new view, we study the properties of both models and\npropose new efficient training algorithms. Key to our approach is to cast\nparameter learning as a low-rank symmetric tensor estimation problem, which we\nsolve by multi-convex optimization. We demonstrate our approach on regression\nand recommender system tasks.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 13:54:51 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Blondel", "Mathieu", ""], ["Ishihata", "Masakazu", ""], ["Fujino", "Akinori", ""], ["Ueda", "Naonori", ""]]}, {"id": "1607.08863", "submitter": "Panayotis Mertikopoulos", "authors": "Johanne Cohen and Am\\'elie H\\'eliou and Panayotis Mertikopoulos", "title": "Exponentially fast convergence to (strict) equilibrium via hedging", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications to data networks where fast convergence is\nessential, we analyze the problem of learning in generic N-person games that\nadmit a Nash equilibrium in pure strategies. Specifically, we consider a\nscenario where players interact repeatedly and try to learn from past\nexperience by small adjustments based on local - and possibly imperfect -\npayoff information. For concreteness, we focus on the so-called \"hedge\" variant\nof the exponential weights algorithm where players select an action with\nprobability proportional to the exponential of the action's cumulative payoff\nover time. When players have perfect information on their mixed payoffs, the\nalgorithm converges locally to a strict equilibrium and the rate of convergence\nis exponentially fast - of the order of\n$\\mathcal{O}(\\exp(-a\\sum_{j=1}^{t}\\gamma_{j}))$ where $a>0$ is a constant and\n$\\gamma_{j}$ is the algorithm's step-size. In the presence of uncertainty,\nconvergence requires a more conservative step-size policy, but with high\nprobability, the algorithm remains locally convergent and achieves an\nexponential convergence rate.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 16:16:49 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Cohen", "Johanne", ""], ["H\u00e9liou", "Am\u00e9lie", ""], ["Mertikopoulos", "Panayotis", ""]]}, {"id": "1607.08878", "submitter": "Randal Olson", "authors": "Randal S. Olson and Jason H. Moore", "title": "Identifying and Harnessing the Building Blocks of Machine Learning\n  Pipelines for Sensible Initialization of a Data Science Automation Tool", "comments": "13 pages, 5 figures, preprint of chapter to appear in GPTP 2016 book", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data science continues to grow in popularity, there will be an increasing\nneed to make data science tools more scalable, flexible, and accessible. In\nparticular, automated machine learning (AutoML) systems seek to automate the\nprocess of designing and optimizing machine learning pipelines. In this\nchapter, we present a genetic programming-based AutoML system called TPOT that\noptimizes a series of feature preprocessors and machine learning models with\nthe goal of maximizing classification accuracy on a supervised classification\nproblem. Further, we analyze a large database of pipelines that were previously\nused to solve various supervised classification problems and identify 100 short\nseries of machine learning operations that appear the most frequently, which we\ncall the building blocks of machine learning pipelines. We harness these\nbuilding blocks to initialize TPOT with promising solutions, and find that this\nsensible initialization method significantly improves TPOT's performance on one\nbenchmark at no cost of significantly degrading performance on the others.\nThus, sensible initialization with machine learning pipeline building blocks\nshows promise for GP-based AutoML systems, and should be further refined in\nfuture work.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 18:06:39 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Olson", "Randal S.", ""], ["Moore", "Jason H.", ""]]}]