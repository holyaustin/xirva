[{"id": "1409.0107", "submitter": "Marco Congedo", "authors": "Alexandre Barachant and Marco Congedo", "title": "A Plug&Play P300 BCI Using Information Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new classification methods for Event Related Potentials\n(ERP) based on an Information geometry framework. Through a new estimation of\ncovariance matrices, this work extend the use of Riemannian geometry, which was\npreviously limited to SMR-based BCI, to the problem of classification of ERPs.\nAs compared to the state-of-the-art, this new method increases performance,\nreduces the number of data needed for the calibration and features good\ngeneralisation across sessions and subjects. This method is illustrated on data\nrecorded with the P300-based game brain invaders. Finally, an online and\nadaptive implementation is described, where the BCI is initialized with generic\nparameters derived from a database and continuously adapt to the individual,\nallowing the user to play the game without any calibration while keeping a high\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 30 Aug 2014 12:17:15 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Barachant", "Alexandre", ""], ["Congedo", "Marco", ""]]}, {"id": "1409.0203", "submitter": "Afsaneh Asaei", "authors": "Mohammad J. Taghizadeh, Reza Parhizkar, Philip N. Garner, Herve\n  Bourlard, Afsaneh Asaei", "title": "Ad Hoc Microphone Array Calibration: Euclidean Distance Matrix\n  Completion Algorithm and Theoretical Guarantees", "comments": "In Press, available online, August 1, 2014.\n  http://www.sciencedirect.com/science/article/pii/S0165168414003508, Signal\n  Processing, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of ad hoc microphone array calibration where\nonly partial information about the distances between microphones is available.\nWe construct a matrix consisting of the pairwise distances and propose to\nestimate the missing entries based on a novel Euclidean distance matrix\ncompletion algorithm by alternative low-rank matrix completion and projection\nonto the Euclidean distance space. This approach confines the recovered matrix\nto the EDM cone at each iteration of the matrix completion algorithm. The\ntheoretical guarantees of the calibration performance are obtained considering\nthe random and locally structured missing entries as well as the measurement\nnoise on the known distances. This study elucidates the links between the\ncalibration error and the number of microphones along with the noise level and\nthe ratio of missing distances. Thorough experiments on real data recordings\nand simulated setups are conducted to demonstrate these theoretical insights. A\nsignificant improvement is achieved by the proposed Euclidean distance matrix\ncompletion algorithm over the state-of-the-art techniques for ad hoc microphone\narray calibration.\n", "versions": [{"version": "v1", "created": "Sun, 31 Aug 2014 10:33:45 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Taghizadeh", "Mohammad J.", ""], ["Parhizkar", "Reza", ""], ["Garner", "Philip N.", ""], ["Bourlard", "Herve", ""], ["Asaei", "Afsaneh", ""]]}, {"id": "1409.0272", "submitter": "Andre Goncalves", "authors": "Andre R. Goncalves, Puja Das, Soumyadeep Chatterjee, Vidyashankar\n  Sivakumar, Fernando J. Von Zuben, Arindam Banerjee", "title": "Multi-task Sparse Structure Learning", "comments": "23rd ACM International Conference on Information and Knowledge\n  Management - CIKM 2014", "journal-ref": null, "doi": "10.1145/2661829.2662091", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) aims to improve generalization performance by\nlearning multiple related tasks simultaneously. While sometimes the underlying\ntask relationship structure is known, often the structure needs to be estimated\nfrom data at hand. In this paper, we present a novel family of models for MTL,\napplicable to regression and classification problems, capable of learning the\nstructure of task relationships. In particular, we consider a joint estimation\nproblem of the task relationship structure and the individual task parameters,\nwhich is solved using alternating minimization. The task relationship structure\nlearning component builds on recent advances in structure learning of Gaussian\ngraphical models based on sparse estimators of the precision (inverse\ncovariance) matrix. We illustrate the effectiveness of the proposed model on a\nvariety of synthetic and benchmark datasets for regression and classification.\nWe also consider the problem of combining climate model outputs for better\nprojections of future climate, with focus on temperature in South America, and\nshow that the proposed model outperforms several existing methods for the\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 00:33:38 GMT"}, {"version": "v2", "created": "Tue, 2 Sep 2014 00:33:35 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Goncalves", "Andre R.", ""], ["Das", "Puja", ""], ["Chatterjee", "Soumyadeep", ""], ["Sivakumar", "Vidyashankar", ""], ["Von Zuben", "Fernando J.", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1409.0473", "submitter": "Dzmitry Bahdanau", "authors": "Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio", "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "comments": "Accepted at ICLR 2015 as oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation is a recently proposed approach to machine\ntranslation. Unlike the traditional statistical machine translation, the neural\nmachine translation aims at building a single neural network that can be\njointly tuned to maximize the translation performance. The models proposed\nrecently for neural machine translation often belong to a family of\nencoder-decoders and consists of an encoder that encodes a source sentence into\na fixed-length vector from which a decoder generates a translation. In this\npaper, we conjecture that the use of a fixed-length vector is a bottleneck in\nimproving the performance of this basic encoder-decoder architecture, and\npropose to extend this by allowing a model to automatically (soft-)search for\nparts of a source sentence that are relevant to predicting a target word,\nwithout having to form these parts as a hard segment explicitly. With this new\napproach, we achieve a translation performance comparable to the existing\nstate-of-the-art phrase-based system on the task of English-to-French\ntranslation. Furthermore, qualitative analysis reveals that the\n(soft-)alignments found by the model agree well with our intuition.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 16:33:02 GMT"}, {"version": "v2", "created": "Thu, 4 Sep 2014 18:32:00 GMT"}, {"version": "v3", "created": "Tue, 7 Oct 2014 18:10:39 GMT"}, {"version": "v4", "created": "Fri, 19 Dec 2014 21:39:11 GMT"}, {"version": "v5", "created": "Sun, 22 Mar 2015 17:08:39 GMT"}, {"version": "v6", "created": "Fri, 24 Apr 2015 13:25:33 GMT"}, {"version": "v7", "created": "Thu, 19 May 2016 21:53:22 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Bahdanau", "Dzmitry", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1409.0553", "submitter": "Alessandro Abate", "authors": "Sofie Haesaert and Robert Babuska and Alessandro Abate", "title": "Sampling-based Approximations with Quantitative Performance for the\n  Probabilistic Reach-Avoid Problem over General Markov Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with stochastic processes endowed with the Markov\n(memoryless) property and evolving over general (uncountable) state spaces. The\nmodels further depend on a non-deterministic quantity in the form of a control\ninput, which can be selected to affect the probabilistic dynamics. We address\nthe computation of maximal reach-avoid specifications, together with the\nsynthesis of the corresponding optimal controllers. The reach-avoid\nspecification deals with assessing the likelihood that any finite-horizon\ntrajectory of the model enters a given goal set, while avoiding a given set of\nundesired states. This article newly provides an approximate computational\nscheme for the reach-avoid specification based on the Fitted Value Iteration\nalgorithm, which hinges on random sample extractions, and gives a-priori\ncomputable formal probabilistic bounds on the error made by the approximation\nalgorithm: as such, the output of the numerical scheme is quantitatively\nassessed and thus meaningful for safety-critical applications. Furthermore, we\nprovide tighter probabilistic error bounds that are sample-based. The overall\ncomputational scheme is put in relationship with alternative approximation\nalgorithms in the literature, and finally its performance is practically\nassessed over a benchmark case study.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 20:03:48 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2015 20:20:13 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Haesaert", "Sofie", ""], ["Babuska", "Robert", ""], ["Abate", "Alessandro", ""]]}, {"id": "1409.0585", "submitter": "KyungHyun Cho", "authors": "Li Yao and Sherjil Ozair and Kyunghyun Cho and Yoshua Bengio", "title": "On the Equivalence Between Deep NADE and Generative Stochastic Networks", "comments": "ECML/PKDD 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Autoregressive Distribution Estimators (NADEs) have recently been\nshown as successful alternatives for modeling high dimensional multimodal\ndistributions. One issue associated with NADEs is that they rely on a\nparticular order of factorization for $P(\\mathbf{x})$. This issue has been\nrecently addressed by a variant of NADE called Orderless NADEs and its deeper\nversion, Deep Orderless NADE. Orderless NADEs are trained based on a criterion\nthat stochastically maximizes $P(\\mathbf{x})$ with all possible orders of\nfactorizations. Unfortunately, ancestral sampling from deep NADE is very\nexpensive, corresponding to running through a neural net separately predicting\neach of the visible variables given some others. This work makes a connection\nbetween this criterion and the training criterion for Generative Stochastic\nNetworks (GSNs). It shows that training NADEs in this way also trains a GSN,\nwhich defines a Markov chain associated with the NADE model. Based on this\nconnection, we show an alternative way to sample from a trained Orderless NADE\nthat allows to trade-off computing time and quality of the samples: a 3 to\n10-fold speedup (taking into account the waste due to correlations between\nconsecutive samples of the chain) can be obtained without noticeably reducing\nthe quality of the samples. This is achieved using a novel sampling procedure\nfor GSNs called annealed GSN sampling, similar to tempering methods that\ncombines fast mixing (obtained thanks to steps at high noise levels) with\naccurate samples (obtained thanks to steps at low noise levels).\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 01:22:42 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Yao", "Li", ""], ["Ozair", "Sherjil", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1409.0745", "submitter": "Hongyang Zhang", "authors": "Hongyang Zhang and Ruben H. Zamar", "title": "Multi-rank Sparse Hierarchical Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a surge in the number of large and flat data sets - data sets\ncontaining a large number of features and a relatively small number of\nobservations - due to the growing ability to collect and store information in\nmedical research and other fields. Hierarchical clustering is a widely used\nclustering tool. In hierarchical clustering, large and flat data sets may allow\nfor a better coverage of clustering features (features that help explain the\ntrue underlying clusters) but, such data sets usually include a large fraction\nof noise features (non-clustering features) that may hide the underlying\nclusters. Witten and Tibshirani (2010) proposed a sparse hierarchical\nclustering framework to cluster the observations using an adaptively chosen\nsubset of the features, however, we show that this framework has some\nlimitations when the data sets contain clustering features with complex\nstructure. In this paper, we propose the Multi-rank sparse hierarchical\nclustering (MrSHC). We show that, using simulation studies and real data\nexamples, MrSHC produces superior feature selection and clustering performance\ncomparing to the classical (of-the-shelf) hierarchical clustering and the\nexisting sparse hierarchical clustering framework.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 15:10:33 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 19:06:40 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Zhang", "Hongyang", ""], ["Zamar", "Ruben H.", ""]]}, {"id": "1409.0748", "submitter": "Uwe Aickelin", "authors": "Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack\n  Gibson, Richard Hubbard", "title": "Comparison of algorithms that detect drug side effects using electronic\n  healthcare databases", "comments": "Soft Computing, 17(12) pp. 2381-2397, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electronic healthcare databases are starting to become more readily\navailable and are thought to have excellent potential for generating adverse\ndrug reaction signals. The Health Improvement Network (THIN) database is an\nelectronic healthcare database containing medical information on over 11\nmillion patients that has excellent potential for detecting ADRs. In this paper\nwe apply four existing electronic healthcare database signal detecting\nalgorithms (MUTARA, HUNT, Temporal Pattern Discovery and modified ROR) on the\nTHIN database for a selection of drugs from six chosen drug families. This is\nthe first comparison of ADR signalling algorithms that includes MUTARA and HUNT\nand enabled us to set a benchmark for the adverse drug reaction signalling\nability of the THIN database. The drugs were selectively chosen to enable a\ncomparison with previous work and for variety. It was found that no algorithm\nwas generally superior and the algorithms' natural thresholds act at variable\nstringencies. Furthermore, none of the algorithms perform well at detecting\nrare ADRs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 15:16:26 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Reps", "Jenna", ""], ["Garibaldi", "Jonathan M.", ""], ["Aickelin", "Uwe", ""], ["Soria", "Daniele", ""], ["Gibson", "Jack", ""], ["Hubbard", "Richard", ""]]}, {"id": "1409.0763", "submitter": "Uwe Aickelin", "authors": "Qi Chen, Amanda Whitbrook, Uwe Aickelin and Chris Roadknight", "title": "Data classification using the Dempster-Shafer method", "comments": "Journal of Experimental & Theoretical Artificial Intelligence,\n  ahead-of-print, 2014", "journal-ref": null, "doi": "10.1080/0952813X.2014.886301", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the Dempster-Shafer method is employed as the theoretical\nbasis for creating data classification systems. Testing is carried out using\nthree popular (multiple attribute) benchmark datasets that have two, three and\nfour classes. In each case, a subset of the available data is used for training\nto establish thresholds, limits or likelihoods of class membership for each\nattribute, and hence create mass functions that establish probability of class\nmembership for each attribute of the test data. Classification of each data\nitem is achieved by combination of these probabilities via Dempster's Rule of\nCombination. Results for the first two datasets show extremely high\nclassification accuracy that is competitive with other popular methods. The\nthird dataset is non-numerical and difficult to classify, but good results can\nbe achieved provided the system and mass functions are designed carefully and\nthe right attributes are chosen for combination. In all cases the\nDempster-Shafer method provides comparable performance to other more popular\nalgorithms, but the overhead of generating accurate mass functions increases\nthe complexity with the addition of new attributes. Overall, the results\nsuggest that the D-S approach provides a suitable framework for the design of\nclassification systems and that automating the mass function design and\ncalculation would increase the viability of the algorithm for complex\nclassification problems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 15:49:40 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Chen", "Qi", ""], ["Whitbrook", "Amanda", ""], ["Aickelin", "Uwe", ""], ["Roadknight", "Chris", ""]]}, {"id": "1409.0768", "submitter": "Uwe Aickelin", "authors": "Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack\n  E. Gibson, Richard B. Hubbard", "title": "A Novel Semi-Supervised Algorithm for Rare Prescription Side Effect\n  Discovery", "comments": null, "journal-ref": "IEEE Journal of Biomedical and Health Informatics, 18 (2), pp.\n  537-547, 2014", "doi": "10.2139/ssrn.2823251", "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drugs are frequently prescribed to patients with the aim of improving each\npatient's medical state, but an unfortunate consequence of most prescription\ndrugs is the occurrence of undesirable side effects. Side effects that occur in\nmore than one in a thousand patients are likely to be signalled efficiently by\ncurrent drug surveillance methods, however, these same methods may take decades\nbefore generating signals for rarer side effects, risking medical morbidity or\nmortality in patients prescribed the drug while the rare side effect is\nundiscovered. In this paper we propose a novel computational meta-analysis\nframework for signalling rare side effects that integrates existing methods,\nknowledge from the web, metric learning and semi-supervised clustering. The\nnovel framework was able to signal many known rare and serious side effects for\nthe selection of drugs investigated, such as tendon rupture when prescribed\nCiprofloxacin or Levofloxacin, renal failure with Naproxen and depression\nassociated with Rimonabant. Furthermore, for the majority of the drug\ninvestigated it generated signals for rare side effects at a more stringent\nsignalling threshold than existing methods and shows the potential to become a\nfundamental part of post marketing surveillance to detect rare side effects.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 16:00:23 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Reps", "Jenna", ""], ["Garibaldi", "Jonathan M.", ""], ["Aickelin", "Uwe", ""], ["Soria", "Daniele", ""], ["Gibson", "Jack E.", ""], ["Hubbard", "Richard B.", ""]]}, {"id": "1409.0772", "submitter": "Uwe Aickelin", "authors": "Jenna M. Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria,\n  Jack E. Gibson, Richard B. Hubbard", "title": "Signalling Paediatric Side Effects using an Ensemble of Simple Study\n  Designs", "comments": "Drug Safety, 37 (3), pp. 163-170, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Children are frequently prescribed medication off-label, meaning\nthere has not been sufficient testing of the medication to determine its safety\nor effectiveness. The main reason this safety knowledge is lacking is due to\nethical restrictions that prevent children from being included in the majority\nof clinical trials. Objective: The objective of this paper is to investigate\nwhether an ensemble of simple study designs can be implemented to signal\nacutely occurring side effects effectively within the paediatric population by\nusing historical longitudinal data. The majority of pharmacovigilance\ntechniques are unsupervised, but this research presents a supervised framework.\nMethods: Multiple measures of association are calculated for each drug and\nmedical event pair and these are used as features that are fed into a\nclassiffier to determine the likelihood of the drug and medical event pair\ncorresponding to an adverse drug reaction. The classiffier is trained using\nknown adverse drug reactions or known non-adverse drug reaction relationships.\nResults: The novel ensemble framework obtained a false positive rate of 0:149,\na sensitivity of 0:547 and a specificity of 0:851 when implemented on a\nreference set of drug and medical event pairs. The novel framework consistently\noutperformed each individual simple study design. Conclusion: This research\nshows that it is possible to exploit the mechanism of causality and presents a\nframework for signalling adverse drug reactions effectively.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 16:17:25 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Reps", "Jenna M.", ""], ["Garibaldi", "Jonathan M.", ""], ["Aickelin", "Uwe", ""], ["Soria", "Daniele", ""], ["Gibson", "Jack E.", ""], ["Hubbard", "Richard B.", ""]]}, {"id": "1409.0775", "submitter": "Uwe Aickelin", "authors": "Yihui Liu and Uwe Aickelin", "title": "Feature selection in detection of adverse drug reactions from the Health\n  Improvement Network (THIN) database", "comments": "International Journal of Information Technology and Computer Science\n  (IJITCS), in print, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adverse drug reaction (ADR) is widely concerned for public health issue. ADRs\nare one of most common causes to withdraw some drugs from market. Prescription\nevent monitoring (PEM) is an important approach to detect the adverse drug\nreactions. The main problem to deal with this method is how to automatically\nextract the medical events or side effects from high-throughput medical events,\nwhich are collected from day to day clinical practice. In this study we propose\na novel concept of feature matrix to detect the ADRs. Feature matrix, which is\nextracted from big medical data from The Health Improvement Network (THIN)\ndatabase, is created to characterize the medical events for the patients who\ntake drugs. Feature matrix builds the foundation for the irregular and big\nmedical data. Then feature selection methods are performed on feature matrix to\ndetect the significant features. Finally the ADRs can be located based on the\nsignificant features. The experiments are carried out on three drugs:\nAtorvastatin, Alendronate, and Metoclopramide. Major side effects for each drug\nare detected and better performance is achieved compared to other computerized\nmethods. The detected ADRs are based on computerized methods, further\ninvestigation is needed.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 16:25:58 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Liu", "Yihui", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1409.0788", "submitter": "Uwe Aickelin", "authors": "Chris Roadknight, Uwe Aickelin, John Scholefield, Lindy Durrant", "title": "Ensemble Learning of Colorectal Cancer Survival Rates", "comments": "IEEE International Conference on Computational Intelligence and\n  Virtual Environments for Measurement Systems and Applications (CIVEMSA) 2013,\n  pp. 82 - 86, 2013", "journal-ref": null, "doi": "10.1109/CIVEMSA.2013.6617400", "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a dataset relating to cellular and physical\nconditions of patients who are operated upon to remove colorectal tumours. This\ndata provides a unique insight into immunological status at the point of tumour\nremoval, tumour classification and post-operative survival. We build on\nexisting research on clustering and machine learning facets of this data to\ndemonstrate a role for an ensemble approach to highlighting patients with\nclearer prognosis parameters. Results for survival prediction using 3 different\napproaches are shown for a subset of the data which is most difficult to model.\nThe performance of each model individually is compared with subsets of the data\nwhere some agreement is reached for multiple models. Significant improvements\nin model accuracy on an unseen test set can be achieved for patients where\nagreement between models is achieved.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 16:52:16 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Roadknight", "Chris", ""], ["Aickelin", "Uwe", ""], ["Scholefield", "John", ""], ["Durrant", "Lindy", ""]]}, {"id": "1409.0791", "submitter": "Jian Yang", "authors": "Jian Yang, Liqiu Meng", "title": "Feature Selection in Conditional Random Fields for Map Matching of GPS\n  Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Map matching of the GPS trajectory serves the purpose of recovering the\noriginal route on a road network from a sequence of noisy GPS observations. It\nis a fundamental technique to many Location Based Services. However, map\nmatching of a low sampling rate on urban road network is still a challenging\ntask. In this paper, the characteristics of Conditional Random Fields with\nregard to inducing many contextual features and feature selection are explored\nfor the map matching of the GPS trajectories at a low sampling rate.\nExperiments on a taxi trajectory dataset show that our method may achieve\ncompetitive results along with the success of reducing model complexity for\ncomputation-limited applications.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 16:52:53 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Yang", "Jian", ""], ["Meng", "Liqiu", ""]]}, {"id": "1409.0797", "submitter": "Jian Yang", "authors": "Jian Yang and Liqiu Meng", "title": "Feature Engineering for Map Matching of Low-Sampling-Rate GPS\n  Trajectories in Road Network", "comments": "ECML/PKDD14 workshop on Machine Learning for Urban Sensor\n  Data(SenseML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Map matching of GPS trajectories from a sequence of noisy observations serves\nthe purpose of recovering the original routes in a road network. In this work\nin progress, we attempt to share our experience of feature construction in a\nspatial database by reporting our ongoing experiment of feature extrac-tion in\nConditional Random Fields (CRFs) for map matching. Our preliminary results are\nobtained from real-world taxi GPS trajectories.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 17:15:58 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Yang", "Jian", ""], ["Meng", "Liqiu", ""]]}, {"id": "1409.0919", "submitter": "Ahmad Hassanat", "authors": "Ahmad Basheer Hassanat, Mohammad Ali Abbadi, Ghada Awad Altarawneh,\n  Ahmad Ali Alhasanat", "title": "Solving the Problem of the K Parameter in the KNN Classifier Using an\n  Ensemble Learning Approach", "comments": null, "journal-ref": "International Journal of Computer Science and Information\n  Security, International Journal of Computer Science and Information Security,\n  Vol. 12, No. 8, August 2014", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new solution for choosing the K parameter in the\nk-nearest neighbor (KNN) algorithm, the solution depending on the idea of\nensemble learning, in which a weak KNN classifier is used each time with a\ndifferent K, starting from one to the square root of the size of the training\nset. The results of the weak classifiers are combined using the weighted sum\nrule. The proposed solution was tested and compared to other solutions using a\ngroup of experiments in real life problems. The experimental results show that\nthe proposed classifier outperforms the traditional KNN classifier that uses a\ndifferent number of neighbors, is competitive with other classifiers, and is a\npromising classifier with strong potential for a wide range of applications.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 23:28:22 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Hassanat", "Ahmad Basheer", ""], ["Abbadi", "Mohammad Ali", ""], ["Altarawneh", "Ghada Awad", ""], ["Alhasanat", "Ahmad Ali", ""]]}, {"id": "1409.0923", "submitter": "Ahmad Hassanat", "authors": "Ahmad Basheer Hassanat", "title": "Dimensionality Invariant Similarity Measure", "comments": "(ISSN: 1545-1003). http://www.jofamericanscience.org", "journal-ref": "J Am Sci 2014;10(8):221-226", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new similarity measure to be used for general tasks\nincluding supervised learning, which is represented by the K-nearest neighbor\nclassifier (KNN). The proposed similarity measure is invariant to large\ndifferences in some dimensions in the feature space. The proposed metric is\nproved mathematically to be a metric. To test its viability for different\napplications, the KNN used the proposed metric for classifying test examples\nchosen from a number of real datasets. Compared to some other well known\nmetrics, the experimental results show that the proposed metric is a promising\ndistance measure for the KNN classifier with strong potential for a wide range\nof applications.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 23:45:29 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Hassanat", "Ahmad Basheer", ""]]}, {"id": "1409.0934", "submitter": "Takafumi Kanamori Dr.", "authors": "Takafumi Kanamori, Shuhei Fujiwara, Akiko Takeda", "title": "Breakdown Point of Robust Support Vector Machine", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The support vector machine (SVM) is one of the most successful learning\nmethods for solving classification problems. Despite its popularity, SVM has a\nserious drawback, that is sensitivity to outliers in training samples. The\npenalty on misclassification is defined by a convex loss called the hinge loss,\nand the unboundedness of the convex loss causes the sensitivity to outliers. To\ndeal with outliers, robust variants of SVM have been proposed, such as the\nrobust outlier detection algorithm and an SVM with a bounded loss called the\nramp loss. In this paper, we propose a robust variant of SVM and investigate\nits robustness in terms of the breakdown point. The breakdown point is a\nrobustness measure that is the largest amount of contamination such that the\nestimated classifier still gives information about the non-contaminated data.\nThe main contribution of this paper is to show an exact evaluation of the\nbreakdown point for the robust SVM. For learning parameters such as the\nregularization parameter in our algorithm, we derive a simple formula that\nguarantees the robustness of the classifier. When the learning parameters are\ndetermined with a grid search using cross validation, our formula works to\nreduce the number of candidate search points. The robustness of the proposed\nmethod is confirmed in numerical experiments. We show that the statistical\nproperties of the robust SVM are well explained by a theoretical analysis of\nthe breakdown point.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 01:39:34 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Kanamori", "Takafumi", ""], ["Fujiwara", "Shuhei", ""], ["Takeda", "Akiko", ""]]}, {"id": "1409.0940", "submitter": "Haim Avron", "authors": "Vikas Sindhwani and Haim Avron", "title": "High-performance Kernel Machines with Implicit Distributed Optimization\n  and Randomization", "comments": "Work presented at MMDS 2014 (June 2014) and JSM 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to fully utilize \"big data\", it is often required to use \"big\nmodels\". Such models tend to grow with the complexity and size of the training\ndata, and do not make strong parametric assumptions upfront on the nature of\nthe underlying statistical dependencies. Kernel methods fit this need well, as\nthey constitute a versatile and principled statistical methodology for solving\na wide range of non-parametric modelling problems. However, their high\ncomputational costs (in storage and time) pose a significant barrier to their\nwidespread adoption in big data applications.\n  We propose an algorithmic framework and high-performance implementation for\nmassive-scale training of kernel-based statistical models, based on combining\ntwo key technical ingredients: (i) distributed general purpose convex\noptimization, and (ii) the use of randomization to improve the scalability of\nkernel methods. Our approach is based on a block-splitting variant of the\nAlternating Directions Method of Multipliers, carefully reconfigured to handle\nvery large random feature matrices, while exploiting hybrid parallelism\ntypically found in modern clusters of multicore machines. Our implementation\nsupports a variety of statistical learning tasks by enabling several loss\nfunctions, regularization schemes, kernels, and layers of randomized\napproximations for both dense and sparse datasets, in a highly extensible\nframework. We evaluate the ability of our framework to learn models on data\nfrom applications, and provide a comparison against existing sequential and\nparallel libraries.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 02:28:51 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 21:38:15 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2015 18:06:53 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Sindhwani", "Vikas", ""], ["Avron", "Haim", ""]]}, {"id": "1409.0964", "submitter": "Liansheng Zhuang", "authors": "Liansheng Zhuang, Shenghua Gao, Jinhui Tang, Jingjing Wang, Zhouchen\n  Lin, Yi Ma", "title": "Constructing a Non-Negative Low Rank and Sparse Graph with Data-Adaptive\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at constructing a good graph for discovering intrinsic data\nstructures in a semi-supervised learning setting. Firstly, we propose to build\na non-negative low-rank and sparse (referred to as NNLRS) graph for the given\ndata representation. Specifically, the weights of edges in the graph are\nobtained by seeking a nonnegative low-rank and sparse matrix that represents\neach data sample as a linear combination of others. The so-obtained NNLRS-graph\ncan capture both the global mixture of subspaces structure (by the low\nrankness) and the locally linear structure (by the sparseness) of the data,\nhence is both generative and discriminative. Secondly, as good features are\nextremely important for constructing a good graph, we propose to learn the data\nembedding matrix and construct the graph jointly within one framework, which is\ntermed as NNLRS with embedded features (referred to as NNLRS-EF). Extensive\nexperiments on three publicly available datasets demonstrate that the proposed\nmethod outperforms the state-of-the-art graph construction method by a large\nmargin for both semi-supervised classification and discriminative analysis,\nwhich verifies the effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 06:45:11 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Zhuang", "Liansheng", ""], ["Gao", "Shenghua", ""], ["Tang", "Jinhui", ""], ["Wang", "Jingjing", ""], ["Lin", "Zhouchen", ""], ["Ma", "Yi", ""]]}, {"id": "1409.1043", "submitter": "Uwe Aickelin", "authors": "Ian Dent, Tony Craig, Uwe Aickelin and Tom Rodden", "title": "Variability of Behaviour in Electricity Load Profile Clustering; Who\n  Does Things at the Same Time Each Day?", "comments": "Advances in Data Mining, pp. 70-84, Springer, Heidelberg, 2014, ISBN\n  978-3-319-08975-1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UK electricity market changes provide opportunities to alter households'\nelectricity usage patterns for the benefit of the overall electricity network.\nWork on clustering similar households has concentrated on daily load profiles\nand the variability in regular household behaviours has not been considered.\nThose households with most variability in regular activities may be the most\nreceptive to incentives to change timing.\n  Whether using the variability of regular behaviour allows the creation of\nmore consistent groupings of households is investigated and compared with daily\nload profile clustering. 204 UK households are analysed to find repeating\npatterns (motifs). Variability in the time of the motif is used as the basis\nfor clustering households. Different clustering algorithms are assessed by the\nconsistency of the results.\n  Findings show that variability of behaviour, using motifs, provides more\nconsistent groupings of households across different clustering algorithms and\nallows for more efficient targeting of behaviour change interventions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 11:42:33 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Dent", "Ian", ""], ["Craig", "Tony", ""], ["Aickelin", "Uwe", ""], ["Rodden", "Tom", ""]]}, {"id": "1409.1053", "submitter": "Uwe Aickelin", "authors": "Jenna M. Reps, Uwe Aickelin and Jonathan M. Garibaldi", "title": "Tuning a Multiple Classifier System for Side Effect Discovery using\n  Genetic Algorithms", "comments": "Proceedings of the 2014 World Congress on Computational Intelligence\n  (WCCI 2014), pp. 910-917, IEEE, Beijing, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous work, a novel supervised framework implementing a binary\nclassifier was presented that obtained excellent results for side effect\ndiscovery. Interestingly, unique side effects were identified when different\nbinary classifiers were used within the framework, prompting the investigation\nof applying a multiple classifier system. In this paper we investigate tuning a\nside effect multiple classifying system using genetic algorithms. The results\nof this research show that the novel framework implementing a multiple\nclassifying system trained using genetic algorithms can obtain a higher partial\narea under the receiver operating characteristic curve than implementing a\nsingle classifier. Furthermore, the framework is able to detect side effects\nefficiently and obtains a low false positive rate.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 12:11:54 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Reps", "Jenna M.", ""], ["Aickelin", "Uwe", ""], ["Garibaldi", "Jonathan M.", ""]]}, {"id": "1409.1057", "submitter": "Uwe Aickelin", "authors": "Alexandros Ladas, Jonathan M. Garibaldi, Rodrigo Scarpel and Uwe\n  Aickelin", "title": "Augmented Neural Networks for Modelling Consumer Indebtness", "comments": "Proceedings of the 2014 World Congress on Computational Intelligence\n  (WCCI 2014), pp. 3086-3093, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumer Debt has risen to be an important problem of modern societies,\ngenerating a lot of research in order to understand the nature of consumer\nindebtness, which so far its modelling has been carried out by statistical\nmodels. In this work we show that Computational Intelligence can offer a more\nholistic approach that is more suitable for the complex relationships an\nindebtness dataset has and Linear Regression cannot uncover. In particular, as\nour results show, Neural Networks achieve the best performance in modelling\nconsumer indebtness, especially when they manage to incorporate the significant\nand experimentally verified results of the Data Mining process in the model,\nexploiting the flexibility Neural Networks offer in designing their topology.\nThis novel method forms an elaborate framework to model Consumer indebtness\nthat can be extended to any other real world application.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 12:23:50 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Ladas", "Alexandros", ""], ["Garibaldi", "Jonathan M.", ""], ["Scarpel", "Rodrigo", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1409.1062", "submitter": "Fanhua Shang", "authors": "Fanhua Shang, Yuanyuan Liu, Hanghang Tong, James Cheng, Hong Cheng", "title": "Structured Low-Rank Matrix Factorization with Missing and Grossly\n  Corrupted Observations", "comments": "28 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering low-rank and sparse matrices from incomplete or corrupted\nobservations is an important problem in machine learning, statistics,\nbioinformatics, computer vision, as well as signal and image processing. In\ntheory, this problem can be solved by the natural convex joint/mixed\nrelaxations (i.e., l_{1}-norm and trace norm) under certain conditions.\nHowever, all current provable algorithms suffer from superlinear per-iteration\ncost, which severely limits their applicability to large-scale problems. In\nthis paper, we propose a scalable, provable structured low-rank matrix\nfactorization method to recover low-rank and sparse matrices from missing and\ngrossly corrupted data, i.e., robust matrix completion (RMC) problems, or\nincomplete and grossly corrupted measurements, i.e., compressive principal\ncomponent pursuit (CPCP) problems. Specifically, we first present two\nsmall-scale matrix trace norm regularized bilinear structured factorization\nmodels for RMC and CPCP problems, in which repetitively calculating SVD of a\nlarge-scale matrix is replaced by updating two much smaller factor matrices.\nThen, we apply the alternating direction method of multipliers (ADMM) to\nefficiently solve the RMC problems. Finally, we provide the convergence\nanalysis of our algorithm, and extend it to address general CPCP problems.\nExperimental results verified both the efficiency and effectiveness of our\nmethod compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 12:36:25 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Tong", "Hanghang", ""], ["Cheng", "James", ""], ["Cheng", "Hong", ""]]}, {"id": "1409.1200", "submitter": "Jim Jing-Yan Wang", "authors": "Jim Jing-Yan Wang", "title": "Domain Transfer Structured Output Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the problem of domain transfer structured output\nlearn- ing and the first solution to solve it. The problem is defined on two\ndifferent data domains sharing the same input and output spaces, named as\nsource domain and target domain. The outputs are structured, and for the data\nsamples of the source domain, the corresponding outputs are available, while\nfor most data samples of the target domain, the corresponding outputs are\nmissing. The input distributions of the two domains are significantly\ndifferent. The problem is to learn a predictor for the target domain to predict\nthe structured outputs from the input. Due to the limited number of outputs\navailable for the samples form the target domain, it is difficult to directly\nlearn the predictor from the target domain, thus it is necessary to use the\noutput information available in source domain. We propose to learn the target\ndomain predictor by adapting a auxiliary predictor trained by using source\ndomain data to the target domain. The adaptation is implemented by adding a\ndelta function on the basis of the auxiliary predictor. An algorithm is\ndeveloped to learn the parameter of the delta function to minimize loss\nfunctions associat- ed with the predicted outputs against the true outputs of\nthe data samples with available outputs of the target domain.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 19:18:05 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Wang", "Jim Jing-Yan", ""]]}, {"id": "1409.1257", "submitter": "KyungHyun Cho", "authors": "Jean Pouget-Abadie and Dzmitry Bahdanau and Bart van Merrienboer and\n  Kyunghyun Cho and Yoshua Bengio", "title": "Overcoming the Curse of Sentence Length for Neural Machine Translation\n  using Automatic Segmentation", "comments": "Eighth Workshop on Syntax, Semantics and Structure in Statistical\n  Translation (SSST-8)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors of (Cho et al., 2014a) have shown that the recently introduced\nneural network translation systems suffer from a significant drop in\ntranslation quality when translating long sentences, unlike existing\nphrase-based translation systems. In this paper, we propose a way to address\nthis issue by automatically segmenting an input sentence into phrases that can\nbe easily translated by the neural network translation model. Once each segment\nhas been independently translated by the neural machine translation model, the\ntranslated clauses are concatenated to form a final translation. Empirical\nresults show a significant improvement in translation quality for long\nsentences.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 21:00:49 GMT"}, {"version": "v2", "created": "Tue, 7 Oct 2014 18:09:37 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Pouget-Abadie", "Jean", ""], ["Bahdanau", "Dzmitry", ""], ["van Merrienboer", "Bart", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1409.1320", "submitter": "Wei Ping", "authors": "Wei Ping, Qiang Liu, Alexander Ihler", "title": "Marginal Structured SVM with Hidden Variables", "comments": "Accepted by the 31st International Conference on Machine Learning\n  (ICML 2014). 12 pages version with supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose the marginal structured SVM (MSSVM) for structured\nprediction with hidden variables. MSSVM properly accounts for the uncertainty\nof hidden variables, and can significantly outperform the previously proposed\nlatent structured SVM (LSSVM; Yu & Joachims (2009)) and other state-of-art\nmethods, especially when that uncertainty is large. Our method also results in\na smoother objective function, making gradient-based optimization of MSSVMs\nconverge significantly faster than for LSSVMs. We also show that our method\nconsistently outperforms hidden conditional random fields (HCRFs; Quattoni et\nal. (2007)) on both simulated and real-world datasets. Furthermore, we propose\na unified framework that includes both our and several other existing methods\nas special cases, and provides insights into the comparison of different models\nin practice.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 05:06:34 GMT"}, {"version": "v2", "created": "Fri, 5 Sep 2014 21:13:36 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Ping", "Wei", ""], ["Liu", "Qiang", ""], ["Ihler", "Alexander", ""]]}, {"id": "1409.1458", "submitter": "Martin Jaggi", "authors": "Martin Jaggi, Virginia Smith, Martin Tak\\'a\\v{c}, Jonathan Terhorst,\n  Sanjay Krishnan, Thomas Hofmann, Michael I. Jordan", "title": "Communication-Efficient Distributed Dual Coordinate Ascent", "comments": "NIPS 2014 version, including proofs. Published in Advances in Neural\n  Information Processing Systems 27 (NIPS 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication remains the most significant bottleneck in the performance of\ndistributed optimization algorithms for large-scale machine learning. In this\npaper, we propose a communication-efficient framework, CoCoA, that uses local\ncomputation in a primal-dual setting to dramatically reduce the amount of\nnecessary communication. We provide a strong convergence rate analysis for this\nclass of algorithms, as well as experiments on real-world distributed datasets\nwith implementations in Spark. In our experiments, we find that as compared to\nstate-of-the-art mini-batch versions of SGD and SDCA algorithms, CoCoA\nconverges to the same .001-accurate solution quality on average 25x as quickly.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 14:59:35 GMT"}, {"version": "v2", "created": "Mon, 29 Sep 2014 16:07:32 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Jaggi", "Martin", ""], ["Smith", "Virginia", ""], ["Tak\u00e1\u010d", "Martin", ""], ["Terhorst", "Jonathan", ""], ["Krishnan", "Sanjay", ""], ["Hofmann", "Thomas", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1409.1576", "submitter": "Amir Hajian", "authors": "Amir Hajian, Marcelo Alvarez, J. Richard Bond", "title": "Machine Learning Etudes in Astrophysics: Selection Functions for Mock\n  Cluster Catalogs", "comments": "Matches version to appear in JCAP. Discussions expanded, 2 figures\n  added. 16 pages, 8 figures", "journal-ref": null, "doi": "10.1088/1475-7516/2015/01/038", "report-no": null, "categories": "astro-ph.CO astro-ph.IM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making mock simulated catalogs is an important component of astrophysical\ndata analysis. Selection criteria for observed astronomical objects are often\ntoo complicated to be derived from first principles. However the existence of\nan observed group of objects is a well-suited problem for machine learning\nclassification. In this paper we use one-class classifiers to learn the\nproperties of an observed catalog of clusters of galaxies from ROSAT and to\npick clusters from mock simulations that resemble the observed ROSAT catalog.\nWe show how this method can be used to study the cross-correlations of thermal\nSunya'ev-Zeldovich signals with number density maps of X-ray selected cluster\ncatalogs. The method reduces the bias due to hand-tuning the selection function\nand is readily scalable to large catalogs with a high-dimensional space of\nastrophysical features.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 20:00:05 GMT"}, {"version": "v2", "created": "Wed, 21 Jan 2015 21:11:42 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Hajian", "Amir", ""], ["Alvarez", "Marcelo", ""], ["Bond", "J. Richard", ""]]}, {"id": "1409.1917", "submitter": "Rajib Rana", "authors": "Rajib Rana, Brano Kusy, Josh Wall, Wen Hu", "title": "Novel Methods for Activity Classification and Occupany Prediction\n  Enabling Fine-grained HVAC Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the energy consumption in buildings is due to HVAC systems, which has\nmotivated several recent studies on making these systems more energy-\nefficient. Occupancy and activity are two important aspects, which need to be\ncorrectly estimated for optimal HVAC control. However, state-of-the-art methods\nto estimate occupancy and classify activity require infrastructure and/or\nwearable sensors which suffers from lower acceptability due to higher cost.\nEncouragingly, with the advancement of the smartphones, these are becoming more\nachievable. Most of the existing occupancy estimation tech- niques have the\nunderlying assumption that the phone is always carried by its user. However,\nphones are often left at desk while attending meeting or other events, which\ngenerates estimation error for the existing phone based occupancy algorithms.\nSimilarly, in the recent days the emerging theory of Sparse Random Classifier\n(SRC) has been applied for activity classification on smartphone, however,\nthere are rooms to improve the on-phone process- ing. We propose a novel sensor\nfusion method which offers almost 100% accuracy for occupancy estimation. We\nalso propose an activity classifica- tion algorithm, which offers similar\naccuracy as of the state-of-the-art SRC algorithms while offering 50% reduction\nin processing.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 16:17:14 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Rana", "Rajib", ""], ["Kusy", "Brano", ""], ["Wall", "Josh", ""], ["Hu", "Wen", ""]]}, {"id": "1409.1976", "submitter": "Quan Zhou", "authors": "Quan Zhou, Wenlin Chen, Shiji Song, Jacob R. Gardner, Kilian Q.\n  Weinberger, Yixin Chen", "title": "A Reduction of the Elastic Net to Support Vector Machines with an\n  Application to GPU Computing", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past years have witnessed many dedicated open-source projects that built\nand maintain implementations of Support Vector Machines (SVM), parallelized for\nGPU, multi-core CPUs and distributed systems. Up to this point, no comparable\neffort has been made to parallelize the Elastic Net, despite its popularity in\nmany high impact applications, including genetics, neuroscience and systems\nbiology. The first contribution in this paper is of theoretical nature. We\nestablish a tight link between two seemingly different algorithms and prove\nthat Elastic Net regression can be reduced to SVM with squared hinge loss\nclassification. Our second contribution is to derive a practical algorithm\nbased on this reduction. The reduction enables us to utilize prior efforts in\nspeeding up and parallelizing SVMs to obtain a highly optimized and parallel\nsolver for the Elastic Net and Lasso. With a simple wrapper, consisting of only\n11 lines of MATLAB code, we obtain an Elastic Net implementation that naturally\nutilizes GPU and multi-core CPUs. We demonstrate on twelve real world data\nsets, that our algorithm yields identical results as the popular (and highly\noptimized) glmnet implementation but is one or several orders of magnitude\nfaster.\n", "versions": [{"version": "v1", "created": "Sat, 6 Sep 2014 03:12:39 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Zhou", "Quan", ""], ["Chen", "Wenlin", ""], ["Song", "Shiji", ""], ["Gardner", "Jacob R.", ""], ["Weinberger", "Kilian Q.", ""], ["Chen", "Yixin", ""]]}, {"id": "1409.2045", "submitter": "Aryan Mokhtari", "authors": "Aryan Mokhtari and Alejandro Ribeiro", "title": "Global Convergence of Online Limited Memory BFGS", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global convergence of an online (stochastic) limited memory version of the\nBroyden-Fletcher- Goldfarb-Shanno (BFGS) quasi-Newton method for solving\noptimization problems with stochastic objectives that arise in large scale\nmachine learning is established. Lower and upper bounds on the Hessian\neigenvalues of the sample functions are shown to suffice to guarantee that the\ncurvature approximation matrices have bounded determinants and traces, which,\nin turn, permits establishing convergence to optimal arguments with probability\n1. Numerical experiments on support vector machines with synthetic data\nshowcase reductions in convergence time relative to stochastic gradient descent\nalgorithms as well as reductions in storage and computation relative to other\nonline quasi-Newton methods. Experimental evaluation on a search engine\nadvertising problem corroborates that these advantages also manifest in\npractical applications.\n", "versions": [{"version": "v1", "created": "Sat, 6 Sep 2014 18:51:17 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Mokhtari", "Aryan", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1409.2177", "submitter": "Kamalika Chaudhuri", "authors": "Kamalika Chaudhuri and Daniel Hsu and Shuang Song", "title": "The Large Margin Mechanism for Differentially Private Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic problem in the design of privacy-preserving algorithms is the private\nmaximization problem: the goal is to pick an item from a universe that\n(approximately) maximizes a data-dependent function, all under the constraint\nof differential privacy. This problem has been used as a sub-routine in many\nprivacy-preserving algorithms for statistics and machine-learning.\n  Previous algorithms for this problem are either range-dependent---i.e., their\nutility diminishes with the size of the universe---or only apply to very\nrestricted function classes. This work provides the first general-purpose,\nrange-independent algorithm for private maximization that guarantees\napproximate differential privacy. Its applicability is demonstrated on two\nfundamental tasks in data mining and machine learning.\n", "versions": [{"version": "v1", "created": "Sun, 7 Sep 2014 23:51:00 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Chaudhuri", "Kamalika", ""], ["Hsu", "Daniel", ""], ["Song", "Shuang", ""]]}, {"id": "1409.2232", "submitter": "Jim Jing-Yan Wang", "authors": "Jim Jing-Yan Wang, Xuefeng Cui, Ge Yu, Lili Guo, Xin Gao", "title": "When coding meets ranking: A joint framework based on local learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding, which represents a data point as a sparse reconstruction code\nwith regard to a dictionary, has been a popular data representation method.\nMeanwhile, in database retrieval problems, learning the ranking scores from\ndata points plays an important role. Up to now, these two problems have always\nbeen considered separately, assuming that data coding and ranking are two\nindependent and irrelevant problems. However, is there any internal\nrelationship between sparse coding and ranking score learning? If yes, how to\nexplore and make use of this internal relationship? In this paper, we try to\nanswer these questions by developing the first joint sparse coding and ranking\nscore learning algorithm. To explore the local distribution in the sparse code\nspace, and also to bridge coding and ranking problems, we assume that in the\nneighborhood of each data point, the ranking scores can be approximated from\nthe corresponding sparse codes by a local linear function. By considering the\nlocal approximation error of ranking scores, the reconstruction error and\nsparsity of sparse coding, and the query information provided by the user, we\nconstruct a unified objective function for learning of sparse codes, the\ndictionary and ranking scores. We further develop an iterative algorithm to\nsolve this optimization problem.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 08:10:37 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 07:33:51 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Wang", "Jim Jing-Yan", ""], ["Cui", "Xuefeng", ""], ["Yu", "Ge", ""], ["Guo", "Lili", ""], ["Gao", "Xin", ""]]}, {"id": "1409.2287", "submitter": "Andreas Damianou Mr", "authors": "Andreas C. Damianou, Michalis K. Titsias, Neil D. Lawrence", "title": "Variational Inference for Uncertainty on the Inputs of Gaussian Process\n  Models", "comments": "51 pages (of which 10 is Appendix), 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian process latent variable model (GP-LVM) provides a flexible\napproach for non-linear dimensionality reduction that has been widely applied.\nHowever, the current approach for training GP-LVMs is based on maximum\nlikelihood, where the latent projection variables are maximized over rather\nthan integrated out. In this paper we present a Bayesian method for training\nGP-LVMs by introducing a non-standard variational inference framework that\nallows to approximately integrate out the latent variables and subsequently\ntrain a GP-LVM by maximizing an analytic lower bound on the exact marginal\nlikelihood. We apply this method for learning a GP-LVM from iid observations\nand for learning non-linear dynamical systems where the observations are\ntemporally correlated. We show that a benefit of the variational Bayesian\nprocedure is its robustness to overfitting and its ability to automatically\nselect the dimensionality of the nonlinear latent space. The resulting\nframework is generic, flexible and easy to extend for other purposes, such as\nGaussian process regression with uncertain inputs and semi-supervised Gaussian\nprocesses. We demonstrate our method on synthetic data and standard machine\nlearning benchmarks, as well as challenging real world datasets, including high\nresolution video data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 10:47:23 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Damianou", "Andreas C.", ""], ["Titsias", "Michalis K.", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1409.2552", "submitter": "Yan Li", "authors": "Yan Li", "title": "Sparse Additive Model using Symmetric Nonnegative Definite Smoothers", "comments": "This is a term project report and has been withdrawn by the authors;\n  arXiv admin note: author list has been modified due to misrepresentation of\n  authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new algorithm, called adaptive sparse backfitting algorithm,\nfor solving high dimensional Sparse Additive Model (SpAM) utilizing symmetric,\nnon-negative definite smoothers. Unlike the previous sparse backfitting\nalgorithm, our method is essentially a block coordinate descent algorithm that\nguarantees to converge to the optimal solution. It bridges the gap between the\npopulation backfitting algorithm and that of the data version. We also prove\nvariable selection consistency under suitable conditions. Numerical studies on\nboth synthesis and real data are conducted to show that adaptive sparse\nbackfitting algorithm outperforms previous sparse backfitting algorithm in\nfitting and predicting high dimensional nonparametric models.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 23:47:51 GMT"}, {"version": "v2", "created": "Mon, 29 Sep 2014 20:16:07 GMT"}, {"version": "v3", "created": "Fri, 3 Oct 2014 01:55:04 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Li", "Yan", ""]]}, {"id": "1409.2574", "submitter": "John Hershey", "authors": "John R. Hershey, Jonathan Le Roux, Felix Weninger", "title": "Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures", "comments": "Added sections on reducing belief propagation to network activation\n  functions, and on conversion between conventional network parameters and BP\n  potentials for binary MRFs. Some bugs and typos were also fixed, and notation\n  made a bit clearer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based methods and deep neural networks have both been tremendously\nsuccessful paradigms in machine learning. In model-based methods, problem\ndomain knowledge can be built into the constraints of the model, typically at\nthe expense of difficulties during inference. In contrast, deterministic deep\nneural networks are constructed in such a way that inference is\nstraightforward, but their architectures are generic and it is unclear how to\nincorporate knowledge. This work aims to obtain the advantages of both\napproaches. To do so, we start with a model-based approach and an associated\ninference algorithm, and \\emph{unfold} the inference iterations as layers in a\ndeep network. Rather than optimizing the original model, we \\emph{untie} the\nmodel parameters across layers, in order to create a more powerful network. The\nresulting architecture can be trained discriminatively to perform accurate\ninference within a fixed network size. We show how this framework allows us to\ninterpret conventional networks as mean-field inference in Markov random\nfields, and to obtain new architectures by instead using belief propagation as\nthe inference algorithm. We then show its application to a non-negative matrix\nfactorization model that incorporates the problem-domain knowledge that sound\nsources are additive. Deep unfolding of this model yields a new kind of\nnon-negative deep neural network, that can be trained using a multiplicative\nbackpropagation-style update algorithm. We present speech enhancement\nexperiments showing that our approach is competitive with conventional neural\nnetworks despite using far fewer parameters.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 02:31:11 GMT"}, {"version": "v2", "created": "Thu, 11 Sep 2014 22:59:52 GMT"}, {"version": "v3", "created": "Wed, 8 Oct 2014 23:50:51 GMT"}, {"version": "v4", "created": "Thu, 20 Nov 2014 01:52:53 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Hershey", "John R.", ""], ["Roux", "Jonathan Le", ""], ["Weninger", "Felix", ""]]}, {"id": "1409.2579", "submitter": "Gang Wu", "authors": "Ting-ting Feng, Gang Wu", "title": "A theoretical contribution to the fast implementation of null linear\n  discriminant analysis method using random matrix multiplication with scatter\n  matrices", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The null linear discriminant analysis method is a competitive approach for\ndimensionality reduction. The implementation of this method, however, is\ncomputationally expensive. Recently, a fast implementation of null linear\ndiscriminant analysis method using random matrix multiplication with scatter\nmatrices was proposed. However, if the random matrix is chosen arbitrarily, the\norientation matrix may be rank deficient, and some useful discriminant\ninformation will be lost. In this paper, we investigate how to choose the\nrandom matrix properly, such that the two criteria of the null LDA method are\nsatisfied theoretically. We give a necessary and sufficient condition to\nguarantee full column rank of the orientation matrix. Moreover, the geometric\ncharacterization of the condition is also described.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 11:46:40 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Feng", "Ting-ting", ""], ["Wu", "Gang", ""]]}, {"id": "1409.2620", "submitter": "Suyog Gupta", "authors": "Suyog Gupta, Vikas Sindhwani, Kailash Gopalakrishnan", "title": "Learning Machines Implemented on Non-Deterministic Hardware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper highlights new opportunities for designing large-scale machine\nlearning systems as a consequence of blurring traditional boundaries that have\nallowed algorithm designers and application-level practitioners to stay -- for\nthe most part -- oblivious to the details of the underlying hardware-level\nimplementations. The hardware/software co-design methodology advocated here\nhinges on the deployment of compute-intensive machine learning kernels onto\ncompute platforms that trade-off determinism in the computation for improvement\nin speed and/or energy efficiency. To achieve this, we revisit digital\nstochastic circuits for approximating matrix computations that are ubiquitous\nin machine learning algorithms. Theoretical and empirical evaluation is\nundertaken to assess the impact of the hardware-induced computational noise on\nalgorithm performance. As a proof-of-concept, a stochastic hardware simulator\nis employed for training deep neural networks for image recognition problems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 07:35:40 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Gupta", "Suyog", ""], ["Sindhwani", "Vikas", ""], ["Gopalakrishnan", "Kailash", ""]]}, {"id": "1409.2655", "submitter": "Lester Mackey", "authors": "Lester Mackey and Jordan Bryan and Man Yue Mo", "title": "Weighted Classification Cascades for Optimizing Discovery Significance\n  in the HiggsML Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a minorization-maximization approach to optimizing common\nmeasures of discovery significance in high energy physics. The approach\nalternates between solving a weighted binary classification problem and\nupdating class weights in a simple, closed-form manner. Moreover, an argument\nbased on convex duality shows that an improvement in weighted classification\nerror on any round yields a commensurate improvement in discovery significance.\nWe complement our derivation with experimental results from the 2014 Higgs\nboson machine learning challenge.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 09:52:59 GMT"}, {"version": "v2", "created": "Wed, 10 Sep 2014 13:48:21 GMT"}, {"version": "v3", "created": "Sat, 13 Dec 2014 23:00:52 GMT"}, {"version": "v4", "created": "Sun, 14 Jun 2015 22:25:40 GMT"}, {"version": "v5", "created": "Thu, 10 Sep 2015 06:57:59 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Mackey", "Lester", ""], ["Bryan", "Jordan", ""], ["Mo", "Man Yue", ""]]}, {"id": "1409.2752", "submitter": "Alireza Makhzani", "authors": "Alireza Makhzani, Brendan Frey", "title": "Winner-Take-All Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a winner-take-all method for learning hierarchical\nsparse representations in an unsupervised fashion. We first introduce\nfully-connected winner-take-all autoencoders which use mini-batch statistics to\ndirectly enforce a lifetime sparsity in the activations of the hidden units. We\nthen propose the convolutional winner-take-all autoencoder which combines the\nbenefits of convolutional architectures and autoencoders for learning\nshift-invariant sparse representations. We describe a way to train\nconvolutional autoencoders layer by layer, where in addition to lifetime\nsparsity, a spatial sparsity within each feature map is achieved using\nwinner-take-all activation functions. We will show that winner-take-all\nautoencoders can be used to to learn deep sparse representations from the\nMNIST, CIFAR-10, ImageNet, Street View House Numbers and Toronto Face datasets,\nand achieve competitive classification performance.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 14:38:43 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2015 18:28:22 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Makhzani", "Alireza", ""], ["Frey", "Brendan", ""]]}, {"id": "1409.2802", "submitter": "William March", "authors": "William B. March and George Biros", "title": "Far-Field Compression for Fast Kernel Summation Methods in High\n  Dimensions", "comments": "43 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider fast kernel summations in high dimensions: given a large set of\npoints in $d$ dimensions (with $d \\gg 3$) and a pair-potential function (the\n{\\em kernel} function), we compute a weighted sum of all pairwise kernel\ninteractions for each point in the set. Direct summation is equivalent to a\n(dense) matrix-vector multiplication and scales quadratically with the number\nof points. Fast kernel summation algorithms reduce this cost to log-linear or\nlinear complexity.\n  Treecodes and Fast Multipole Methods (FMMs) deliver tremendous speedups by\nconstructing approximate representations of interactions of points that are far\nfrom each other. In algebraic terms, these representations correspond to\nlow-rank approximations of blocks of the overall interaction matrix. Existing\napproaches require an excessive number of kernel evaluations with increasing\n$d$ and number of points in the dataset.\n  To address this issue, we use a randomized algebraic approach in which we\nfirst sample the rows of a block and then construct its approximate, low-rank\ninterpolative decomposition. We examine the feasibility of this approach\ntheoretically and experimentally. We provide a new theoretical result showing a\ntighter bound on the reconstruction error from uniformly sampling rows than the\nexisting state-of-the-art. We demonstrate that our sampling approach is\ncompetitive with existing (but prohibitively expensive) methods from the\nliterature. We also construct kernel matrices for the Laplacian, Gaussian, and\npolynomial kernels -- all commonly used in physics and data analysis. We\nexplore the numerical properties of blocks of these matrices, and show that\nthey are amenable to our approach. Depending on the data set, our randomized\nalgorithm can successfully compute low rank approximations in high dimensions.\nWe report results for data sets with ambient dimensions from four to 1,000.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 16:28:40 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2015 00:30:28 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["March", "William B.", ""], ["Biros", "George", ""]]}, {"id": "1409.2848", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate", "comments": "Fixed a minor bug in the proof of lemma 1 (which does not affect the\n  result)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and analyze a simple algorithm for principal component analysis\nand singular value decomposition, VR-PCA, which uses computationally cheap\nstochastic iterations, yet converges exponentially fast to the optimal\nsolution. In contrast, existing algorithms suffer either from slow convergence,\nor computationally intensive iterations whose runtime scales with the data\nsize. The algorithm builds on a recent variance-reduced stochastic gradient\ntechnique, which was previously analyzed for strongly convex optimization,\nwhereas here we apply it to an inherently non-convex problem, using a very\ndifferent analysis.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 19:31:52 GMT"}, {"version": "v2", "created": "Sun, 25 Jan 2015 08:28:03 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2015 14:19:08 GMT"}, {"version": "v4", "created": "Sun, 26 Apr 2015 12:20:12 GMT"}, {"version": "v5", "created": "Fri, 31 Jul 2015 04:41:42 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1409.2905", "submitter": "Sunsern Cheamanunkul", "authors": "Sunsern Cheamanunkul, Evan Ettinger and Yoav Freund", "title": "Non-Convex Boosting Overcomes Random Label Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sensitivity of Adaboost to random label noise is a well-studied problem.\nLogitBoost, BrownBoost and RobustBoost are boosting algorithms claimed to be\nless sensitive to noise than AdaBoost. We present the results of experiments\nevaluating these algorithms on both synthetic and real datasets. We compare the\nperformance on each of datasets when the labels are corrupted by different\nlevels of independent label noise. In presence of random label noise, we found\nthat BrownBoost and RobustBoost perform significantly better than AdaBoost and\nLogitBoost, while the difference between each pair of algorithms is\ninsignificant. We provide an explanation for the difference based on the margin\ndistributions of the algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 21:36:47 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Cheamanunkul", "Sunsern", ""], ["Ettinger", "Evan", ""], ["Freund", "Yoav", ""]]}, {"id": "1409.2944", "submitter": "Hao Wang", "authors": "Hao Wang and Naiyan Wang and Dit-Yan Yeung", "title": "Collaborative Deep Learning for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) is a successful approach commonly used by many\nrecommender systems. Conventional CF-based methods use the ratings given to\nitems by users as the sole source of information for learning to make\nrecommendation. However, the ratings are often very sparse in many\napplications, causing CF-based methods to degrade significantly in their\nrecommendation performance. To address this sparsity problem, auxiliary\ninformation such as item content information may be utilized. Collaborative\ntopic regression (CTR) is an appealing recent method taking this approach which\ntightly couples the two components that learn from two different sources of\ninformation. Nevertheless, the latent representation learned by CTR may not be\nvery effective when the auxiliary information is very sparse. To address this\nproblem, we generalize recent advances in deep learning from i.i.d. input to\nnon-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian\nmodel called collaborative deep learning (CDL), which jointly performs deep\nrepresentation learning for the content information and collaborative filtering\nfor the ratings (feedback) matrix. Extensive experiments on three real-world\ndatasets from different domains show that CDL can significantly advance the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 03:05:22 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 09:23:37 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Wang", "Hao", ""], ["Wang", "Naiyan", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1409.2993", "submitter": "Jian Tang", "authors": "Jian Tang, Ming Zhang, Qiaozhu Mei", "title": "\"Look Ma, No Hands!\" A Parameter-Free Topic Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has always been a burden to the users of statistical topic models to\npredetermine the right number of topics, which is a key parameter of most topic\nmodels. Conventionally, automatic selection of this parameter is done through\neither statistical model selection (e.g., cross-validation, AIC, or BIC) or\nBayesian nonparametric models (e.g., hierarchical Dirichlet process). These\nmethods either rely on repeated runs of the inference algorithm to search\nthrough a large range of parameter values which does not suit the mining of big\ndata, or replace this parameter with alternative parameters that are less\nintuitive and still hard to be determined. In this paper, we explore to\n\"eliminate\" this parameter from a new perspective. We first present a\nnonparametric treatment of the PLSA model named nonparametric probabilistic\nlatent semantic analysis (nPLSA). The inference procedure of nPLSA allows for\nthe exploration and comparison of different numbers of topics within a single\nexecution, yet remains as simple as that of PLSA. This is achieved by\nsubstituting the parameter of the number of topics with an alternative\nparameter that is the minimal goodness of fit of a document. We show that the\nnew parameter can be further eliminated by two parameter-free treatments:\neither by monitoring the diversity among the discovered topics or by a weak\nsupervision from users in the form of an exemplar topic. The parameter-free\ntopic model finds the appropriate number of topics when the diversity among the\ndiscovered topics is maximized, or when the granularity of the discovered\ntopics matches the exemplar topic. Experiments on both synthetic and real data\nprove that the parameter-free topic model extracts topics with a comparable\nquality comparing to classical topic models with \"manual transmission\". The\nquality of the topics outperforms those extracted through classical Bayesian\nnonparametric models.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 08:41:35 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Tang", "Jian", ""], ["Zhang", "Ming", ""], ["Mei", "Qiaozhu", ""]]}, {"id": "1409.3040", "submitter": "Balasubramanian Sivan", "authors": "Nick Gravin, Yuval Peres and Balasubramanian Sivan", "title": "Towards Optimal Algorithms for Prediction with Expert Advice", "comments": "The latest version (accepted to SODA 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classical problem of prediction with expert advice in the\nadversarial setting with a geometric stopping time. In 1965, Cover gave the\noptimal algorithm for the case of 2 experts. In this paper, we design the\noptimal algorithm, adversary and regret for the case of 3 experts. Further, we\nshow that the optimal algorithm for $2$ and $3$ experts is a probability\nmatching algorithm (analogous to Thompson sampling) against a particular\nrandomized adversary. Remarkably, our proof shows that the probability matching\nalgorithm is not only optimal against this particular randomized adversary, but\nalso minimax optimal.\n  Our analysis develops upper and lower bounds simultaneously, analogous to the\nprimal-dual method. Our analysis of the optimal adversary goes through delicate\nasymptotics of the random walk of a particle between multiple walls. We use the\nconnection we develop to random walks to derive an improved algorithm and\nregret bound for the case of $4$ experts, and, provide a general framework for\ndesigning the optimal algorithm and adversary for an arbitrary number of\nexperts.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 12:16:19 GMT"}, {"version": "v2", "created": "Mon, 20 Oct 2014 07:19:57 GMT"}, {"version": "v3", "created": "Thu, 6 Nov 2014 20:53:17 GMT"}, {"version": "v4", "created": "Mon, 10 Nov 2014 20:29:29 GMT"}, {"version": "v5", "created": "Mon, 11 Jul 2016 05:29:28 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Gravin", "Nick", ""], ["Peres", "Yuval", ""], ["Sivan", "Balasubramanian", ""]]}, {"id": "1409.3136", "submitter": "Remi Lajugie", "authors": "Damien Garreau (INRIA Paris - Rocquencourt, DI-ENS), R\\'emi Lajugie\n  (INRIA Paris - Rocquencourt, DI-ENS), Sylvain Arlot (INRIA Paris -\n  Rocquencourt, DI-ENS), Francis Bach (INRIA Paris - Rocquencourt, DI-ENS)", "title": "Metric Learning for Temporal Sequence Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to learn a Mahalanobis distance to perform\nalignment of multivariate time series. The learning examples for this task are\ntime series for which the true alignment is known. We cast the alignment\nproblem as a structured prediction task, and propose realistic losses between\nalignments for which the optimization is tractable. We provide experiments on\nreal data in the audio to audio context, where we show that the learning of a\nsimilarity measure leads to improvements in the performance of the alignment\ntask. We also propose to use this metric learning framework to perform feature\nselection and, from basic audio features, build a combination of these with\nbetter performance for the alignment.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 16:10:33 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Garreau", "Damien", "", "INRIA Paris - Rocquencourt, DI-ENS"], ["Lajugie", "R\u00e9mi", "", "INRIA Paris - Rocquencourt, DI-ENS"], ["Arlot", "Sylvain", "", "INRIA Paris -\n  Rocquencourt, DI-ENS"], ["Bach", "Francis", "", "INRIA Paris - Rocquencourt, DI-ENS"]]}, {"id": "1409.3215", "submitter": "Ilya Sutskever", "authors": "Ilya Sutskever and Oriol Vinyals and Quoc V. Le", "title": "Sequence to Sequence Learning with Neural Networks", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are powerful models that have achieved excellent\nperformance on difficult learning tasks. Although DNNs work well whenever large\nlabeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to\na vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to\nFrench translation task from the WMT'14 dataset, the translations produced by\nthe LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's\nBLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did\nnot have difficulty on long sentences. For comparison, a phrase-based SMT\nsystem achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on\nthis task. The LSTM also learned sensible phrase and sentence representations\nthat are sensitive to word order and are relatively invariant to the active and\nthe passive voice. Finally, we found that reversing the order of the words in\nall source sentences (but not target sentences) improved the LSTM's performance\nmarkedly, because doing so introduced many short term dependencies between the\nsource and the target sentence which made the optimization problem easier.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 19:55:35 GMT"}, {"version": "v2", "created": "Wed, 29 Oct 2014 12:13:17 GMT"}, {"version": "v3", "created": "Sun, 14 Dec 2014 20:59:51 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Sutskever", "Ilya", ""], ["Vinyals", "Oriol", ""], ["Le", "Quoc V.", ""]]}, {"id": "1409.3358", "submitter": "Lili Mou", "authors": "Lili Mou, Ge Li, Yuxuan Liu, Hao Peng, Zhi Jin, Yan Xu, Lu Zhang", "title": "Building Program Vector Representations for Deep Learning", "comments": "This paper was submitted to ICSE'14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has made significant breakthroughs in various fields of\nartificial intelligence. Advantages of deep learning include the ability to\ncapture highly complicated features, weak involvement of human engineering,\netc. However, it is still virtually impossible to use deep learning to analyze\nprograms since deep architectures cannot be trained effectively with pure back\npropagation. In this pioneering paper, we propose the \"coding criterion\" to\nbuild program vector representations, which are the premise of deep learning\nfor program analysis. Our representation learning approach directly makes deep\nlearning a reality in this new field. We evaluate the learned vector\nrepresentations both qualitatively and quantitatively. We conclude, based on\nthe experiments, the coding criterion is successful in building program\nrepresentations. To evaluate whether deep learning is beneficial for program\nanalysis, we feed the representations to deep neural networks, and achieve\nhigher accuracy in the program classification task than \"shallow\" methods, such\nas logistic regression and the support vector machine. This result confirms the\nfeasibility of deep learning to analyze programs. It also gives primary\nevidence of its success in this new field. We believe deep learning will become\nan outstanding technique for program analysis in the near future.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2014 08:44:28 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Mou", "Lili", ""], ["Li", "Ge", ""], ["Liu", "Yuxuan", ""], ["Peng", "Hao", ""], ["Jin", "Zhi", ""], ["Xu", "Yan", ""], ["Zhang", "Lu", ""]]}, {"id": "1409.3446", "submitter": "Haimonti Dutta", "authors": "Haimonti Dutta and Ashwin Srinivasan", "title": "Consensus-Based Modelling using Distributed Feature Construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A particularly successful role for Inductive Logic Programming (ILP) is as a\ntool for discovering useful relational features for subsequent use in a\npredictive model. Conceptually, the case for using ILP to construct relational\nfeatures rests on treating these features as functions, the automated discovery\nof which necessarily requires some form of first-order learning. Practically,\nthere are now several reports in the literature that suggest that augmenting\nany existing features with ILP-discovered relational features can substantially\nimprove the predictive power of a model. While the approach is straightforward\nenough, much still needs to be done to scale it up to explore more fully the\nspace of possible features that can be constructed by an ILP system. This is in\nprinciple, infinite and in practice, extremely large. Applications have been\nconfined to heuristic or random selections from this space. In this paper, we\naddress this computational difficulty by allowing features to be constructed in\na distributed manner. That is, there is a network of computational units, each\nof which employs an ILP engine to construct some small number of features and\nthen builds a (local) model. We then employ a consensus-based algorithm, in\nwhich neighboring nodes share information to update local models. For a\ncategory of models (those with convex loss functions), it can be shown that the\nalgorithm will result in all nodes converging to a consensus model. In\npractice, it may be slow to achieve this convergence. Nevertheless, our results\non synthetic and real datasets that suggests that in relatively short time the\n\"best\" node in the network reaches a model whose predictive accuracy is\ncomparable to that obtained using more computational effort in a\nnon-distributed setting (the best node is identified as the one whose weights\nconverge first).\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2014 14:11:02 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Dutta", "Haimonti", ""], ["Srinivasan", "Ashwin", ""]]}, {"id": "1409.3518", "submitter": "Do-kyum Kim", "authors": "Do-kyum Kim, Geoffrey M. Voelker, Lawrence K. Saul", "title": "Topic Modeling of Hierarchical Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of topic modeling in corpora whose documents are\norganized in a multi-level hierarchy. We explore a parametric approach to this\nproblem, assuming that the number of topics is known or can be estimated by\ncross-validation. The models we consider can be viewed as special\n(finite-dimensional) instances of hierarchical Dirichlet processes (HDPs). For\nthese models we show that there exists a simple variational approximation for\nprobabilistic inference. The approximation relies on a previously unexploited\ninequality that handles the conditional dependence between Dirichlet latent\nvariables in adjacent levels of the model's hierarchy. We compare our approach\nto existing implementations of nonparametric HDPs. On several benchmarks we\nfind that our approach is faster than Gibbs sampling and able to learn more\npredictive models than existing variational methods. Finally, we demonstrate\nthe large-scale viability of our approach on two newly available corpora from\nresearchers in computer security---one with 350,000 documents and over 6,000\ninternal subcategories, the other with a five-level deep hierarchy.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2014 18:00:59 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 06:29:46 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Kim", "Do-kyum", ""], ["Voelker", "Geoffrey M.", ""], ["Saul", "Lawrence K.", ""]]}, {"id": "1409.3660", "submitter": "Fei-Yun Zhu", "authors": "Feiyun Zhu, Bin Fan, Xinliang Zhu, Ying Wang, Shiming Xiang and\n  Chunhong Pan", "title": "10,000+ Times Accelerated Robust Subset Selection (ARSS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subset selection from massive data with noised information is increasingly\npopular for various applications. This problem is still highly challenging as\ncurrent methods are generally slow in speed and sensitive to outliers. To\naddress the above two issues, we propose an accelerated robust subset selection\n(ARSS) method. Specifically in the subset selection area, this is the first\nattempt to employ the $\\ell_{p}(0<p\\leq1)$-norm based measure for the\nrepresentation loss, preventing large errors from dominating our objective. As\na result, the robustness against outlier elements is greatly enhanced.\nActually, data size is generally much larger than feature length, i.e. $N\\gg\nL$. Based on this observation, we propose a speedup solver (via ALM and\nequivalent derivations) to highly reduce the computational cost, theoretically\nfrom $O(N^{4})$ to $O(N{}^{2}L)$. Extensive experiments on ten benchmark\ndatasets verify that our method not only outperforms state of the art methods,\nbut also runs 10,000+ times faster than the most related method.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 07:18:17 GMT"}, {"version": "v2", "created": "Fri, 19 Sep 2014 02:49:19 GMT"}, {"version": "v3", "created": "Mon, 13 Oct 2014 07:58:57 GMT"}, {"version": "v4", "created": "Mon, 17 Nov 2014 14:39:31 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Zhu", "Feiyun", ""], ["Fan", "Bin", ""], ["Zhu", "Xinliang", ""], ["Wang", "Ying", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1409.3768", "submitter": "Sang-Yun Oh", "authors": "Sang-Yun Oh, Onkar Dalal, Kshitij Khare, Bala Rajaratnam", "title": "Optimization Methods for Sparse Pseudo-Likelihood Graphical Model\n  Selection", "comments": "NIPS accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse high dimensional graphical model selection is a popular topic in\ncontemporary machine learning. To this end, various useful approaches have been\nproposed in the context of $\\ell_1$-penalized estimation in the Gaussian\nframework. Though many of these inverse covariance estimation approaches are\ndemonstrably scalable and have leveraged recent advances in convex\noptimization, they still depend on the Gaussian functional form. To address\nthis gap, a convex pseudo-likelihood based partial correlation graph estimation\nmethod (CONCORD) has been recently proposed. This method uses coordinate-wise\nminimization of a regression based pseudo-likelihood, and has been shown to\nhave robust model selection properties in comparison with the Gaussian\napproach. In direct contrast to the parallel work in the Gaussian setting\nhowever, this new convex pseudo-likelihood framework has not leveraged the\nextensive array of methods that have been proposed in the machine learning\nliterature for convex optimization. In this paper, we address this crucial gap\nby proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) for\nperforming $\\ell_1$-regularized inverse covariance matrix estimation in the\npseudo-likelihood framework. We present timing comparisons with coordinate-wise\nminimization and demonstrate that our approach yields tremendous payoffs for\n$\\ell_1$-penalized partial correlation graph estimation outside the Gaussian\nsetting, thus yielding the fastest and most scalable approach for such\nproblems. We undertake a theoretical analysis of our approach and rigorously\ndemonstrate convergence, and also derive rates thereof.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 15:25:07 GMT"}], "update_date": "2014-09-15", "authors_parsed": [["Oh", "Sang-Yun", ""], ["Dalal", "Onkar", ""], ["Khare", "Kshitij", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1409.3821", "submitter": "Andrea Montanari", "authors": "Andrea Montanari", "title": "Computational Implications of Reducing Data to Sufficient Statistics", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large dataset and an estimation task, it is common to pre-process the\ndata by reducing them to a set of sufficient statistics. This step is often\nregarded as straightforward and advantageous (in that it simplifies statistical\nanalysis). I show that -on the contrary- reducing data to sufficient statistics\ncan change a computationally tractable estimation problem into an intractable\none. I discuss connections with recent work in theoretical computer science,\nand implications for some techniques to estimate graphical models.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 18:57:01 GMT"}, {"version": "v2", "created": "Mon, 15 Sep 2014 16:39:26 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2015 19:35:44 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Montanari", "Andrea", ""]]}, {"id": "1409.3854", "submitter": "M. Emre Celebi", "authors": "M. Emre Celebi and Hassan A. Kingravi", "title": "Linear, Deterministic, and Order-Invariant Initialization Methods for\n  the K-Means Clustering Algorithm", "comments": "21 pages, 2 figures, 5 tables, Partitional Clustering Algorithms\n  (Springer, 2014). arXiv admin note: substantial text overlap with\n  arXiv:1304.7465, arXiv:1209.1960", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past five decades, k-means has become the clustering algorithm of\nchoice in many application domains primarily due to its simplicity, time/space\nefficiency, and invariance to the ordering of the data points. Unfortunately,\nthe algorithm's sensitivity to the initial selection of the cluster centers\nremains to be its most serious drawback. Numerous initialization methods have\nbeen proposed to address this drawback. Many of these methods, however, have\ntime complexity superlinear in the number of data points, which makes them\nimpractical for large data sets. On the other hand, linear methods are often\nrandom and/or sensitive to the ordering of the data points. These methods are\ngenerally unreliable in that the quality of their results is unpredictable.\nTherefore, it is common practice to perform multiple runs of such methods and\ntake the output of the run that produces the best results. Such a practice,\nhowever, greatly increases the computational requirements of the otherwise\nhighly efficient k-means algorithm. In this chapter, we investigate the\nempirical performance of six linear, deterministic (non-random), and\norder-invariant k-means initialization methods on a large and diverse\ncollection of data sets from the UCI Machine Learning Repository. The results\ndemonstrate that two relatively unknown hierarchical initialization methods due\nto Su and Dy outperform the remaining four methods with respect to two\nobjective effectiveness criteria. In addition, a recent method due to Erisoglu\net al. performs surprisingly poorly.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 20:11:36 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Celebi", "M. Emre", ""], ["Kingravi", "Hassan A.", ""]]}, {"id": "1409.3879", "submitter": "Qianli Liao", "authors": "Qianli Liao, Joel Z. Leibo, Tomaso Poggio", "title": "Unsupervised learning of clutter-resistant visual representations from\n  natural videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Populations of neurons in inferotemporal cortex (IT) maintain an explicit\ncode for object identity that also tolerates transformations of object\nappearance e.g., position, scale, viewing angle [1, 2, 3]. Though the learning\nrules are not known, recent results [4, 5, 6] suggest the operation of an\nunsupervised temporal-association-based method e.g., Foldiak's trace rule [7].\nSuch methods exploit the temporal continuity of the visual world by assuming\nthat visual experience over short timescales will tend to have invariant\nidentity content. Thus, by associating representations of frames from nearby\ntimes, a representation that tolerates whatever transformations occurred in the\nvideo may be achieved. Many previous studies verified that such rules can work\nin simple situations without background clutter, but the presence of visual\nclutter has remained problematic for this approach. Here we show that temporal\nassociation based on large class-specific filters (templates) avoids the\nproblem of clutter. Our system learns in an unsupervised way from natural\nvideos gathered from the internet, and is able to perform a difficult\nunconstrained face recognition task on natural images: Labeled Faces in the\nWild [8].\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 22:35:08 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 00:33:14 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Liao", "Qianli", ""], ["Leibo", "Joel Z.", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1409.3881", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and K. Vijay-Shanker", "title": "An Approach to Reducing Annotation Costs for BioNLP", "comments": "2 pages, 1 figure, 5 tables; appeared in Proceedings of the Workshop\n  on Current Trends in Biomedical Natural Language Processing at ACL\n  (Association for Computational Linguistics) 2008", "journal-ref": "In Proceedings of the Workshop on Current Trends in Biomedical\n  Natural Language Processing, pages 104-105, Columbus, Ohio, June 2008.\n  Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a broad range of BioNLP tasks for which active learning (AL) can\nsignificantly reduce annotation costs and a specific AL algorithm we have\ndeveloped is particularly effective in reducing annotation costs for these\ntasks. We have previously developed an AL algorithm called ClosestInitPA that\nworks best with tasks that have the following characteristics: redundancy in\ntraining material, burdensome annotation costs, Support Vector Machines (SVMs)\nwork well for the task, and imbalanced datasets (i.e. when set up as a binary\nclassification problem, one class is substantially rarer than the other). Many\nBioNLP tasks have these characteristics and thus our AL algorithm is a natural\napproach to apply to BioNLP tasks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 22:40:38 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Bloodgood", "Michael", ""], ["Vijay-Shanker", "K.", ""]]}, {"id": "1409.3912", "submitter": "Takafumi Kanamori Dr.", "authors": "Kota Matsui, Wataru Kumagai, Takafumi Kanamori", "title": "Parallel Distributed Block Coordinate Descent Methods based on Pairwise\n  Comparison Oracle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a block coordinate descent algorithm to solve\nunconstrained optimization problems. In our algorithm, computation of function\nvalues or gradients is not required. Instead, pairwise comparison of function\nvalues is used. Our algorithm consists of two steps; one is the direction\nestimate step and the other is the search step. Both steps require only\npairwise comparison of function values, which tells us only the order of\nfunction values over two points. In the direction estimate step, a Newton type\nsearch direction is estimated. A computation method like block coordinate\ndescent methods is used with the pairwise comparison. In the search step, a\nnumerical solution is updated along the estimated direction. The computation in\nthe direction estimate step can be easily parallelized, and thus, the algorithm\nworks efficiently to find the minimizer of the objective function. Also, we\nshow an upper bound of the convergence rate. In numerical experiments, we show\nthat our method efficiently finds the optimal solution compared to some\nexisting methods based on the pairwise comparison.\n", "versions": [{"version": "v1", "created": "Sat, 13 Sep 2014 05:45:32 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Matsui", "Kota", ""], ["Kumagai", "Wataru", ""], ["Kanamori", "Takafumi", ""]]}, {"id": "1409.3924", "submitter": "Yuguang Wang", "authors": "Yuguang Wang and Feilong Cao and Yubo Yuan", "title": "A study on effectiveness of extreme learning machine", "comments": null, "journal-ref": "Neurocomputing, 74(16):2483--2490, 2011", "doi": "10.1016/j.neucom.2010.11.030", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme learning machine (ELM), proposed by Huang et al., has been shown a\npromising learning algorithm for single-hidden layer feedforward neural\nnetworks (SLFNs). Nevertheless, because of the random choice of input weights\nand biases, the ELM algorithm sometimes makes the hidden layer output matrix H\nof SLFN not full column rank, which lowers the effectiveness of ELM. This paper\ndiscusses the effectiveness of ELM and proposes an improved algorithm called\nEELM that makes a proper selection of the input weights and bias before\ncalculating the output weights, which ensures the full column rank of H in\ntheory. This improves to some extend the learning rate (testing accuracy,\nprediction accuracy, learning time) and the robustness property of the\nnetworks. The experimental results based on both the benchmark function\napproximation and real-world problems including classification and regression\napplications show the good performances of EELM.\n", "versions": [{"version": "v1", "created": "Sat, 13 Sep 2014 07:47:37 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Wang", "Yuguang", ""], ["Cao", "Feilong", ""], ["Yuan", "Yubo", ""]]}, {"id": "1409.3970", "submitter": "Yin Zheng", "authors": "Yin Zheng, Yu-Jin Zhang, Hugo Larochelle", "title": "A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data", "comments": "24 pages, 10 figures. A version has been accepted by TPAMI on Aug\n  4th, 2015. Add footnote about how to train the model in practice in Section\n  5.1. arXiv admin note: substantial text overlap with arXiv:1305.5306", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2476802", "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling based on latent Dirichlet allocation (LDA) has been a\nframework of choice to deal with multimodal data, such as in image annotation\ntasks. Another popular approach to model the multimodal data is through deep\nneural networks, such as the deep Boltzmann machine (DBM). Recently, a new type\nof topic model called the Document Neural Autoregressive Distribution Estimator\n(DocNADE) was proposed and demonstrated state-of-the-art performance for text\ndocument modeling. In this work, we show how to successfully apply and extend\nthis model to multimodal data, such as simultaneous image classification and\nannotation. First, we propose SupDocNADE, a supervised extension of DocNADE,\nthat increases the discriminative power of the learned hidden topic features\nand show how to employ it to learn a joint representation from image visual\nwords, annotation words and class label information. We test our model on the\nLabelMe and UIUC-Sports data sets and show that it compares favorably to other\ntopic models. Second, we propose a deep extension of our model and provide an\nefficient way of training the deep model. Experimental results show that our\ndeep model outperforms its shallow version and reaches state-of-the-art\nperformance on the Multimedia Information Retrieval (MIR) Flickr data set.\n", "versions": [{"version": "v1", "created": "Sat, 13 Sep 2014 17:17:05 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2015 02:44:29 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2015 16:12:31 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Zheng", "Yin", ""], ["Zhang", "Yu-Jin", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1409.4018", "submitter": "Anna Goldenberg", "authors": "Daniel Hidru and Anna Goldenberg", "title": "EquiNMF: Graph Regularized Multiview Nonnegative Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) methods have proved to be powerful\nacross a wide range of real-world clustering applications. Integrating multiple\ntypes of measurements for the same objects/subjects allows us to gain a deeper\nunderstanding of the data and refine the clustering. We have developed a novel\nGraph-reguarized multiview NMF-based method for data integration called\nEquiNMF. The parameters for our method are set in a completely automated\ndata-specific unsupervised fashion, a highly desirable property in real-world\napplications. We performed extensive and comprehensive experiments on multiview\nimaging data. We show that EquiNMF consistently outperforms other single-view\nNMF methods used on concatenated data and multi-view NMF methods with different\ntypes of regularizations.\n", "versions": [{"version": "v1", "created": "Sun, 14 Sep 2014 05:15:18 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Hidru", "Daniel", ""], ["Goldenberg", "Anna", ""]]}, {"id": "1409.4044", "submitter": "Alain Tapp", "authors": "Alain Tapp", "title": "A new approach in machine learning", "comments": "Preliminary report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report we presented a novel approach to machine learning.\nOnce the new framework is presented, we will provide a simple and yet very\npowerful learning algorithm which will be benchmark on various dataset.\n  The framework we proposed is based on booleen circuits; more specifically the\nclassifier produced by our algorithm have that form. Using bits and boolean\ngates instead of real numbers and multiplication enable the the learning\nalgorithm and classifier to use very efficient boolean vector operations. This\nenable both the learning algorithm and classifier to be extremely efficient.\nThe accuracy of the classifier we obtain with our framework compares very\nfavorably those produced by conventional techniques, both in terms of\nefficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 14 Sep 2014 10:25:23 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Tapp", "Alain", ""]]}, {"id": "1409.4127", "submitter": "Yu-Chuan Su", "authors": "Yu-Chuan Su, Tzu-Hsuan Chiu, Chun-Yen Yeh, Hsin-Fu Huang, Winston H.\n  Hsu", "title": "Transfer Learning for Video Recognition with Scarce Training Data for\n  Deep Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unconstrained video recognition and Deep Convolution Network (DCN) are two\nactive topics in computer vision recently. In this work, we apply DCNs as\nframe-based recognizers for video recognition. Our preliminary studies,\nhowever, show that video corpora with complete ground truth are usually not\nlarge and diverse enough to learn a robust model. The networks trained directly\non the video data set suffer from significant overfitting and have poor\nrecognition rate on the test set. The same lack-of-training-sample problem\nlimits the usage of deep models on a wide range of computer vision problems\nwhere obtaining training data are difficult. To overcome the problem, we\nperform transfer learning from images to videos to utilize the knowledge in the\nweakly labeled image corpus for video recognition. The image corpus help to\nlearn important visual patterns for natural images, while these patterns are\nignored by models trained only on the video corpus. Therefore, the resultant\nnetworks have better generalizability and better recognition rate. We show that\nby means of transfer learning from image to video, we can learn a frame-based\nrecognizer with only 4k videos. Because the image corpus is weakly labeled, the\nentire learning process requires only 4k annotated instances, which is far less\nthan the million scale image data sets required by previous works. The same\napproach may be applied to other visual recognition tasks where only scarce\ntraining data is available, and it improves the applicability of DCNs in\nvarious computer vision problems. Our experiments also reveal the correlation\nbetween meta-parameters and the performance of DCNs, given the properties of\nthe target problem and data. These results lead to a heuristic for\nmeta-parameter selection for future researches, which does not rely on the time\nconsuming meta-parameter search.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 01:26:55 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 14:47:54 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Su", "Yu-Chuan", ""], ["Chiu", "Tzu-Hsuan", ""], ["Yeh", "Chun-Yen", ""], ["Huang", "Hsin-Fu", ""], ["Hsu", "Winston H.", ""]]}, {"id": "1409.4155", "submitter": "Sicheng Xiong", "authors": "Sicheng Xiong, R\\'omer Rosales, Yuanli Pei, Xiaoli Z. Fern", "title": "Active Metric Learning from Relative Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on active learning of distance metrics from relative\ncomparison information. A relative comparison specifies, for a data point\ntriplet $(x_i,x_j,x_k)$, that instance $x_i$ is more similar to $x_j$ than to\n$x_k$. Such constraints, when available, have been shown to be useful toward\ndefining appropriate distance metrics. In real-world applications, acquiring\nconstraints often require considerable human effort. This motivates us to study\nhow to select and query the most useful relative comparisons to achieve\neffective metric learning with minimum user effort. Given an underlying class\nconcept that is employed by the user to provide such constraints, we present an\ninformation-theoretic criterion that selects the triplet whose answer leads to\nthe highest expected gain in information about the classes of a set of\nexamples. Directly applying the proposed criterion requires examining $O(n^3)$\ntriplets with $n$ instances, which is prohibitive even for datasets of moderate\nsize. We show that a randomized selection strategy can be used to reduce the\nselection pool from $O(n^3)$ to $O(n)$, allowing us to scale up to larger-size\nproblems. Experiments show that the proposed method consistently outperforms\ntwo baseline policies.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 04:37:46 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Xiong", "Sicheng", ""], ["Rosales", "R\u00f3mer", ""], ["Pei", "Yuanli", ""], ["Fern", "Xiaoli Z.", ""]]}, {"id": "1409.4256", "submitter": "Stefan Engblom", "authors": "Tomas Ekeberg, Stefan Engblom, and Jing Liu", "title": "Machine learning for ultrafast X-ray diffraction patterns on large-scale\n  GPU clusters", "comments": null, "journal-ref": "Int. J. High Perf. Comput. Appl. 29(2):233--243 (2015)", "doi": "10.1177/1094342015572030", "report-no": null, "categories": "q-bio.BM cs.DC cs.LG physics.bio-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical method of determining the atomic structure of complex molecules\nby analyzing diffraction patterns is currently undergoing drastic developments.\nModern techniques for producing extremely bright and coherent X-ray lasers\nallow a beam of streaming particles to be intercepted and hit by an ultrashort\nhigh energy X-ray beam. Through machine learning methods the data thus\ncollected can be transformed into a three-dimensional volumetric intensity map\nof the particle itself. The computational complexity associated with this\nproblem is very high such that clusters of data parallel accelerators are\nrequired.\n  We have implemented a distributed and highly efficient algorithm for\ninversion of large collections of diffraction patterns targeting clusters of\nhundreds of GPUs. With the expected enormous amount of diffraction data to be\nproduced in the foreseeable future, this is the required scale to approach real\ntime processing of data at the beam site. Using both real and synthetic data we\nlook at the scaling properties of the application and discuss the overall\ncomputational viability of this exciting and novel imaging technique.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2014 20:26:23 GMT"}, {"version": "v2", "created": "Tue, 16 Dec 2014 12:53:13 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Ekeberg", "Tomas", ""], ["Engblom", "Stefan", ""], ["Liu", "Jing", ""]]}, {"id": "1409.4271", "submitter": "Xiangrong Zeng", "authors": "Xiangrong Zeng, and M\\'ario A. T. Figueiredo", "title": "The Ordered Weighted $\\ell_1$ Norm: Atomic Formulation, Projections, and\n  Algorithms", "comments": "13 pages, 17 figures. The latest version of this paper was submitted\n  to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ordered weighted $\\ell_1$ norm (OWL) was recently proposed, with two\ndifferent motivations: its good statistical properties as a sparsity promoting\nregularizer; the fact that it generalizes the so-called {\\it octagonal\nshrinkage and clustering algorithm for regression} (OSCAR), which has the\nability to cluster/group regression variables that are highly correlated. This\npaper contains several contributions to the study and application of OWL\nregularization: the derivation of the atomic formulation of the OWL norm; the\nderivation of the dual of the OWL norm, based on its atomic formulation; a new\nand simpler derivation of the proximity operator of the OWL norm; an efficient\nscheme to compute the Euclidean projection onto an OWL ball; the instantiation\nof the conditional gradient (CG, also known as Frank-Wolfe) algorithm for\nlinear regression problems under OWL regularization; the instantiation of\naccelerated projected gradient algorithms for the same class of problems.\nFinally, a set of experiments give evidence that accelerated projected gradient\nalgorithms are considerably faster than CG, for the class of problems\nconsidered.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 14:22:34 GMT"}, {"version": "v2", "created": "Tue, 16 Sep 2014 14:01:09 GMT"}, {"version": "v3", "created": "Mon, 22 Sep 2014 14:08:29 GMT"}, {"version": "v4", "created": "Tue, 10 Mar 2015 16:07:39 GMT"}, {"version": "v5", "created": "Fri, 10 Apr 2015 13:21:46 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Zeng", "Xiangrong", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1409.4276", "submitter": "Paul Vitanyi", "authors": "Rudi L. Cilibrasi (CWI, Amsterdam) and Paul M.B. Vitanyi (CWI and\n  University of Amsterdam)", "title": "A Fast Quartet Tree Heuristic for Hierarchical Clustering", "comments": "LaTeX, 40 pages, 11 figures; this paper has substantial overlap with\n  arXiv:cs/0606048 in cs.DS", "journal-ref": "Pattern Recognition, 44 (2011) 662-677", "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Quartet Tree Cost problem is to construct an optimal weight tree\nfrom the $3{n \\choose 4}$ weighted quartet topologies on $n$ objects, where\noptimality means that the summed weight of the embedded quartet topologies is\noptimal (so it can be the case that the optimal tree embeds all quartets as\nnonoptimal topologies). We present a Monte Carlo heuristic, based on randomized\nhill climbing, for approximating the optimal weight tree, given the quartet\ntopology weights. The method repeatedly transforms a dendrogram, with all\nobjects involved as leaves, achieving a monotonic approximation to the exact\nsingle globally optimal tree. The problem and the solution heuristic has been\nextensively used for general hierarchical clustering of nontree-like\n(non-phylogeny) data in various domains and across domains with heterogeneous\ndata. We also present a greatly improved heuristic, reducing the running time\nby a factor of order a thousand to ten thousand. All this is implemented and\navailable, as part of the CompLearn package. We compare performance and running\ntime of the original and improved versions with those of UPGMA, BioNJ, and NJ,\nas implemented in the SplitsTree package on genomic data for which the latter\nare optimized.\n  Keywords: Data and knowledge visualization, Pattern\nmatching--Clustering--Algorithms/Similarity measures, Hierarchical clustering,\nGlobal optimization, Quartet tree, Randomized hill-climbing,\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 15:55:25 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Cilibrasi", "Rudi L.", "", "CWI, Amsterdam"], ["Vitanyi", "Paul M. B.", "", "CWI and\n  University of Amsterdam"]]}, {"id": "1409.4326", "submitter": "Jure \\v{Z}bontar", "authors": "Jure \\v{Z}bontar and Yann LeCun", "title": "Computing the Stereo Matching Cost with a Convolutional Neural Network", "comments": "Conference on Computer Vision and Pattern Recognition (CVPR), June\n  2015", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298767", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for extracting depth information from a rectified image\npair. We train a convolutional neural network to predict how well two image\npatches match and use it to compute the stereo matching cost. The cost is\nrefined by cross-based cost aggregation and semiglobal matching, followed by a\nleft-right consistency check to eliminate errors in the occluded regions. Our\nstereo method achieves an error rate of 2.61 % on the KITTI stereo dataset and\nis currently (August 2014) the top performing method on this dataset.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 16:54:42 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 15:08:48 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["\u017dbontar", "Jure", ""], ["LeCun", "Yann", ""]]}, {"id": "1409.4566", "submitter": "Ethem Alpaydin", "authors": "Olcay Taner Yildiz, Ethem Alpaydin", "title": "Multivariate Comparison of Classification Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical tests that compare classification algorithms are univariate and\nuse a single performance measure, e.g., misclassification error, $F$ measure,\nAUC, and so on. In multivariate tests, comparison is done using multiple\nmeasures simultaneously. For example, error is the sum of false positives and\nfalse negatives and a univariate test on error cannot make a distinction\nbetween these two sources, but a 2-variate test can. Similarly, instead of\ncombining precision and recall in $F$ measure, we can have a 2-variate test on\n(precision, recall). We use Hotelling's multivariate $T^2$ test for comparing\ntwo algorithms, and when we have three or more algorithms we use the\nmultivariate analysis of variance (MANOVA) followed by pairwise post hoc tests.\nIn our experiments, we see that multivariate tests have higher power than\nunivariate tests, that is, they can detect differences that univariate tests\ncannot. We also discuss how multivariate analysis allows us to automatically\nextract performance measures that best distinguish the behavior of multiple\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 10:18:12 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Yildiz", "Olcay Taner", ""], ["Alpaydin", "Ethem", ""]]}, {"id": "1409.4689", "submitter": "Johannes Lederer", "authors": "Johannes Lederer and Sergio Guadarrama", "title": "Compute Less to Get More: Using ORC to Improve Sparse Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Filtering is a popular feature learning algorithm for image\nclassification pipelines. In this paper, we connect the performance of Sparse\nFiltering with spectral properties of the corresponding feature matrices. This\nconnection provides new insights into Sparse Filtering; in particular, it\nsuggests early stopping of Sparse Filtering. We therefore introduce the Optimal\nRoundness Criterion (ORC), a novel stopping criterion for Sparse Filtering. We\nshow that this stopping criterion is related with pre-processing procedures\nsuch as Statistical Whitening and demonstrate that it can make image\nclassification with Sparse Filtering considerably faster and more accurate.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 16:31:07 GMT"}, {"version": "v2", "created": "Sun, 24 May 2015 09:16:03 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Lederer", "Johannes", ""], ["Guadarrama", "Sergio", ""]]}, {"id": "1409.4698", "submitter": "Charmgil Hong", "authors": "Charmgil Hong, Iyad Batal, Milos Hauskrecht", "title": "A Mixtures-of-Experts Framework for Multi-Label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel probabilistic approach for multi-label classification that\nis based on the mixtures-of-experts architecture combined with recently\nintroduced conditional tree-structured Bayesian networks. Our approach captures\ndifferent input-output relations from multi-label data using the efficient\ntree-structured classifiers, while the mixtures-of-experts architecture aims to\ncompensate for the tree-structured restrictions and build a more accurate\nmodel. We develop and present algorithms for learning the model from data and\nfor performing multi-label predictions on future data instances. Experiments on\nmultiple benchmark datasets demonstrate that our approach achieves highly\ncompetitive results and outperforms the existing state-of-the-art multi-label\nclassification methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 16:52:14 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Hong", "Charmgil", ""], ["Batal", "Iyad", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1409.4747", "submitter": "Fabrice Rossi", "authors": "Tsirizo Rabenoro (SAMM), J\\'er\\^ome Lacaille, Marie Cottrell (SAMM),\n  Fabrice Rossi (SAMM)", "title": "Anomaly Detection Based on Indicators Aggregation", "comments": "International Joint Conference on Neural Networks (IJCNN 2014),\n  Beijing : China (2014). arXiv admin note: substantial text overlap with\n  arXiv:1407.0880", "journal-ref": null, "doi": "10.1109/IJCNN.2014.6889841", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic anomaly detection is a major issue in various areas. Beyond mere\ndetection, the identification of the source of the problem that produced the\nanomaly is also essential. This is particularly the case in aircraft engine\nhealth monitoring where detecting early signs of failure (anomalies) and\nhelping the engine owner to implement efficiently the adapted maintenance\noperations (fixing the source of the anomaly) are of crucial importance to\nreduce the costs attached to unscheduled maintenance. This paper introduces a\ngeneral methodology that aims at classifying monitoring signals into normal\nones and several classes of abnormal ones. The main idea is to leverage expert\nknowledge by generating a very large number of binary indicators. Each\nindicator corresponds to a fully parametrized anomaly detector built from\nparametric anomaly scores designed by experts. A feature selection method is\nused to keep only the most discriminant indicators which are used at inputs of\na Naive Bayes classifier. This give an interpretable classifier based on\ninterpretable anomaly detectors whose parameters have been optimized indirectly\nby the selection process. The proposed methodology is evaluated on simulated\ndata designed to reproduce some of the anomaly types observed in real world\nengines.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 19:44:31 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Rabenoro", "Tsirizo", "", "SAMM"], ["Lacaille", "J\u00e9r\u00f4me", "", "SAMM"], ["Cottrell", "Marie", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1409.4757", "submitter": "Katsuhiko Ishiguro", "authors": "Katsuhiko Ishiguro, Issei Sato, Naonori Ueda", "title": "Collapsed Variational Bayes Inference of Infinite Relational Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Infinite Relational Model (IRM) is a probabilistic model for relational\ndata clustering that partitions objects into clusters based on observed\nrelationships. This paper presents Averaged CVB (ACVB) solutions for IRM,\nconvergence-guaranteed and practically useful fast Collapsed Variational Bayes\n(CVB) inferences. We first derive ordinary CVB and CVB0 for IRM based on the\nlower bound maximization. CVB solutions yield deterministic iterative\nprocedures for inferring IRM given the truncated number of clusters. Our\nproposal includes CVB0 updates of hyperparameters including the concentration\nparameter of the Dirichlet Process, which has not been studied in the\nliterature. To make the CVB more practically useful, we further study the CVB\ninference in two aspects. First, we study the convergence issues and develop a\nconvergence-guaranteed algorithm for any CVB-based inferences called ACVB,\nwhich enables automatic convergence detection and frees non-expert\npractitioners from difficult and costly manual monitoring of inference\nprocesses. Second, we present a few techniques for speeding up IRM inferences.\nIn particular, we describe the linear time inference of CVB0, allowing the IRM\nfor larger relational data uses. The ACVB solutions of IRM showed comparable or\nbetter performance compared to existing inference methods in experiments, and\nprovide deterministic, faster, and easier convergence detection.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 08:06:36 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Ishiguro", "Katsuhiko", ""], ["Sato", "Issei", ""], ["Ueda", "Naonori", ""]]}, {"id": "1409.4835", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and K. Vijay-Shanker", "title": "Taking into Account the Differences between Actively and Passively\n  Acquired Data: The Case of Active Learning with Support Vector Machines for\n  Imbalanced Datasets", "comments": "4 pages, 5 figures; appeared in Proceedings of Human Language\n  Technologies: The 2009 Annual Conference of the North American Chapter of the\n  Association for Computational Linguistics, Companion Volume: Short Papers,\n  pages 137-140, Boulder, Colorado, June 2009. Association for Computational\n  Linguistics", "journal-ref": "Proceedings of HLT: The 2009 Annual Conference of the North\n  American Chapter of the Association for Computational Linguistics, Short\n  Papers, pages 137-140, Boulder, Colorado, June 2009. Association for\n  Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actively sampled data can have very different characteristics than passively\nsampled data. Therefore, it's promising to investigate using different\ninference procedures during AL than are used during passive learning (PL). This\ngeneral idea is explored in detail for the focused case of AL with\ncost-weighted SVMs for imbalanced data, a situation that arises for many HLT\ntasks. The key idea behind the proposed InitPA method for addressing imbalance\nis to base cost models during AL on an estimate of overall corpus imbalance\ncomputed via a small unbiased sample rather than the imbalance in the labeled\ntraining data, which is the leading method used during PL.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 00:00:11 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Bloodgood", "Michael", ""], ["Vijay-Shanker", "K.", ""]]}, {"id": "1409.4928", "submitter": "Christophe Sch\\\"ulke", "authors": "Ang\\'elique Dr\\'emeau, Christophe Sch\\\"ulke, Yingying Xu, Devavrat\n  Shah", "title": "Statistical inference with probabilistic graphical models", "comments": "Chapter of \"Statistical Physics, Optimization, Inference, and\n  Message-Passing Algorithms\", Eds.: F. Krzakala, F. Ricci-Tersenghi, L.\n  Zdeborova, R. Zecchina, E. W. Tramel, L. F. Cugliandolo (Oxford University\n  Press, to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are notes from the lecture of Devavrat Shah given at the autumn school\n\"Statistical Physics, Optimization, Inference, and Message-Passing Algorithms\",\nthat took place in Les Houches, France from Monday September 30th, 2013, till\nFriday October 11th, 2013. The school was organized by Florent Krzakala from\nUPMC & ENS Paris, Federico Ricci-Tersenghi from La Sapienza Roma, Lenka\nZdeborova from CEA Saclay & CNRS, and Riccardo Zecchina from Politecnico\nTorino. This lecture of Devavrat Shah (MIT) covers the basics of inference and\nlearning. It explains how inference problems are represented within structures\nknown as graphical models. The theoretical basis of the belief propagation\nalgorithm is then explained and derived. This lecture sets the stage for\ngeneralizations and applications of message passing algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 09:52:36 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Dr\u00e9meau", "Ang\u00e9lique", ""], ["Sch\u00fclke", "Christophe", ""], ["Xu", "Yingying", ""], ["Shah", "Devavrat", ""]]}, {"id": "1409.4936", "submitter": "Anthony Bagnall Dr", "authors": "Anthony Bagnall and Reda Younsi", "title": "Ensembles of Random Sphere Cover Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and evaluate alternative ensemble schemes for a new instance based\nlearning classifier, the Randomised Sphere Cover (RSC) classifier. RSC fuses\ninstances into spheres, then bases classification on distance to spheres rather\nthan distance to instances. The randomised nature of RSC makes it ideal for use\nin ensembles. We propose two ensemble methods tailored to the RSC classifier;\n$\\alpha \\beta$RSE, an ensemble based on instance resampling and $\\alpha$RSSE, a\nsubspace ensemble. We compare $\\alpha \\beta$RSE and $\\alpha$RSSE to tree based\nensembles on a set of UCI datasets and demonstrates that RSC ensembles perform\nsignificantly better than some of these ensembles, and not significantly worse\nthan the others. We demonstrate via a case study on six gene expression data\nsets that $\\alpha$RSSE can outperform other subspace ensemble methods on high\ndimensional data when used in conjunction with an attribute filter. Finally, we\nperform a set of Bias/Variance decomposition experiments to analyse the source\nof improvement in comparison to a base classifier.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 10:18:34 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Bagnall", "Anthony", ""], ["Younsi", "Reda", ""]]}, {"id": "1409.4988", "submitter": "Filippo Maria Bianchi", "authors": "Filippo Maria Bianchi, Enrico Maiorino, Lorenzo Livi, Antonello Rizzi\n  and Alireza Sadeghian", "title": "An Agent-Based Algorithm exploiting Multiple Local Dissimilarities for\n  Clusters Mining and Knowledge Discovery", "comments": null, "journal-ref": null, "doi": "10.1007/s00500-015-1876-1", "report-no": null, "categories": "cs.LG cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-agent algorithm able to automatically discover relevant\nregularities in a given dataset, determining at the same time the set of\nconfigurations of the adopted parametric dissimilarity measure yielding compact\nand separated clusters. Each agent operates independently by performing a\nMarkovian random walk on a suitable weighted graph representation of the input\ndataset. Such a weighted graph representation is induced by the specific\nparameter configuration of the dissimilarity measure adopted by the agent,\nwhich searches and takes decisions autonomously for one cluster at a time.\nResults show that the algorithm is able to discover parameter configurations\nthat yield a consistent and interpretable collection of clusters. Moreover, we\ndemonstrate that our algorithm shows comparable performances with other similar\nstate-of-the-art algorithms when facing specific clustering problems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 14:39:37 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Bianchi", "Filippo Maria", ""], ["Maiorino", "Enrico", ""], ["Livi", "Lorenzo", ""], ["Rizzi", "Antonello", ""], ["Sadeghian", "Alireza", ""]]}, {"id": "1409.5079", "submitter": "Bilal Ahmed", "authors": "Bilal Ahmed", "title": "Predictive Capacity of Meteorological Data - Will it rain tomorrow", "comments": "7 pages, 2 Result Sets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the availability of high precision digital sensors and cheap storage\nmedium, it is not uncommon to find large amounts of data collected on almost\nall measurable attributes, both in nature and man-made habitats. Weather in\nparticular has been an area of keen interest for researchers to develop more\naccurate and reliable prediction models. This paper presents a set of\nexperiments which involve the use of prevalent machine learning techniques to\nbuild models to predict the day of the week given the weather data for that\nparticular day i.e. temperature, wind, rain etc., and test their reliability\nacross four cities in Australia {Brisbane, Adelaide, Perth, Hobart}. The\nresults provide a comparison of accuracy of these machine learning techniques\nand their reliability to predict the day of the week by analysing the weather\ndata. We then apply the models to predict weather conditions based on the\navailable data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 13:09:23 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Ahmed", "Bilal", ""]]}, {"id": "1409.5165", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and K. Vijay-Shanker", "title": "A Method for Stopping Active Learning Based on Stabilizing Predictions\n  and the Need for User-Adjustable Stopping", "comments": "9 pages, 3 figures, 5 tables; appeared in Proceedings of the\n  Thirteenth Conference on Computational Natural Language Learning\n  (CoNLL-2009), June 2009", "journal-ref": "In Proceedings of the Thirteenth Conference on Computational\n  Natural Language Learning (CoNLL-2009), pages 39-47, Boulder, Colorado, June\n  2009. Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A survey of existing methods for stopping active learning (AL) reveals the\nneeds for methods that are: more widely applicable; more aggressive in saving\nannotations; and more stable across changing datasets. A new method for\nstopping AL based on stabilizing predictions is presented that addresses these\nneeds. Furthermore, stopping methods are required to handle a broad range of\ndifferent annotation/performance tradeoff valuations. Despite this, the\nexisting body of work is dominated by conservative methods with little (if any)\nattention paid to providing users with control over the behavior of stopping\nmethods. The proposed method is shown to fill a gap in the level of\naggressiveness available for stopping AL and supports providing users with\ncontrol over stopping behavior.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 23:28:59 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Bloodgood", "Michael", ""], ["Vijay-Shanker", "K.", ""]]}, {"id": "1409.5185", "submitter": "Zhuowen Tu", "authors": "Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen\n  Tu", "title": "Deeply-Supervised Nets", "comments": "Patent disclosure, UCSD Docket No. SD2014-313, filed on May 22, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Our proposed deeply-supervised nets (DSN) method simultaneously minimizes\nclassification error while making the learning process of hidden layers direct\nand transparent. We make an attempt to boost the classification performance by\nstudying a new formulation in deep networks. Three aspects in convolutional\nneural networks (CNN) style architectures are being looked at: (1) transparency\nof the intermediate layers to the overall classification; (2)\ndiscriminativeness and robustness of learned features, especially in the early\nlayers; (3) effectiveness in training due to the presence of the exploding and\nvanishing gradients. We introduce \"companion objective\" to the individual\nhidden layers, in addition to the overall objective at the output layer (a\ndifferent strategy to layer-wise pre-training). We extend techniques from\nstochastic gradient methods to analyze our algorithm. The advantage of our\nmethod is evident and our experimental result on benchmark datasets shows\nsignificant performance gain over existing methods (e.g. all state-of-the-art\nresults on MNIST, CIFAR-10, CIFAR-100, and SVHN).\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 04:08:25 GMT"}, {"version": "v2", "created": "Thu, 25 Sep 2014 05:03:06 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Lee", "Chen-Yu", ""], ["Xie", "Saining", ""], ["Gallagher", "Patrick", ""], ["Zhang", "Zhengyou", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1409.5209", "submitter": "Chunhua Shen", "authors": "Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel", "title": "Pedestrian Detection with Spatially Pooled Features and Structured\n  Ensemble Learning", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many typical applications of object detection operate within a prescribed\nfalse-positive range. In this situation the performance of a detector should be\nassessed on the basis of the area under the ROC curve over that range, rather\nthan over the full curve, as the performance outside the range is irrelevant.\nThis measure is labelled as the partial area under the ROC curve (pAUC). We\npropose a novel ensemble learning method which achieves a maximal detection\nrate at a user-defined range of false positive rates by directly optimizing the\npartial AUC using structured learning.\n  In order to achieve a high object detection performance, we propose a new\napproach to extract low-level visual features based on spatial pooling.\nIncorporating spatial pooling improves the translational invariance and thus\nthe robustness of the detection process. Experimental results on both synthetic\nand real-world data sets demonstrate the effectiveness of our approach, and we\nshow that it is possible to train state-of-the-art pedestrian detectors using\nthe proposed structured ensemble learning method with spatially pooled\nfeatures. The result is the current best reported performance on the\nCaltech-USA pedestrian detection dataset.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 07:14:33 GMT"}, {"version": "v2", "created": "Wed, 15 Oct 2014 02:35:33 GMT"}, {"version": "v3", "created": "Sun, 28 Jun 2015 10:15:37 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Paisitkriangkrai", "Sakrapee", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1409.5330", "submitter": "Shaobo Lin", "authors": "Jian Fang, Shaobo Lin, Zongben Xu", "title": "Learning and approximation capability of orthogonal super greedy\n  algorithm", "comments": "30 pages,14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the approximation capability of orthogonal super greedy\nalgorithms (OSGA) and its applications in supervised learning. OSGA is\nconcerned with selecting more than one atoms in each iteration step, which, of\ncourse, greatly reduces the computational burden when compared with the\nconventional orthogonal greedy algorithm (OGA). We prove that even for function\nclasses that are not the convex hull of the dictionary, OSGA does not degrade\nthe approximation capability of OGA provided the dictionary is incoherent.\nBased on this, we deduce a tight generalization error bound for OSGA learning.\nOur results show that in the realm of supervised learning, OSGA provides a\npossibility to further reduce the computational burden of OGA in the premise of\nmaintaining its prominent generalization capability.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 15:09:47 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Fang", "Jian", ""], ["Lin", "Shaobo", ""], ["Xu", "Zongben", ""]]}, {"id": "1409.5402", "submitter": "John Canny", "authors": "Huasha Zhao and Biye Jiang and John Canny", "title": "SAME but Different: Fast and High-Quality Gibbs Parameter Estimation", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs sampling is a workhorse for Bayesian inference but has several\nlimitations when used for parameter estimation, and is often much slower than\nnon-sampling inference methods. SAME (State Augmentation for Marginal\nEstimation) \\cite{Doucet99,Doucet02} is an approach to MAP parameter estimation\nwhich gives improved parameter estimates over direct Gibbs sampling. SAME can\nbe viewed as cooling the posterior parameter distribution and allows annealed\nsearch for the MAP parameters, often yielding very high quality (lower loss)\nestimates. But it does so at the expense of additional samples per iteration\nand generally slower performance. On the other hand, SAME dramatically\nincreases the parallelism in the sampling schedule, and is an excellent match\nfor modern (SIMD) hardware. In this paper we explore the application of SAME to\ngraphical model inference on modern hardware. We show that combining SAME with\nfactored sample representation (or approximation) gives throughput competitive\nwith the fastest symbolic methods, but with potentially better quality. We\ndescribe experiments on Latent Dirichlet Allocation, achieving speeds similar\nto the fastest reported methods (online Variational Bayes) and lower\ncross-validated loss than other LDA implementations. The method is simple to\nimplement and should be applicable to many other models.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 18:31:50 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Zhao", "Huasha", ""], ["Jiang", "Biye", ""], ["Canny", "John", ""]]}, {"id": "1409.5495", "submitter": "Hanzhang Hu", "authors": "Hanzhang Hu, Alexander Grubb, J. Andrew Bagnell, Martial Hebert", "title": "Efficient Feature Group Sequencing for Anytime Linear Prediction", "comments": "Published in UAI 2016, Proceedings of the Thirty-Second Conference on\n  Uncertainty in Artificial Intelligence, UAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider \\textit{anytime} linear prediction in the common machine learning\nsetting, where features are in groups that have costs. We achieve anytime (or\ninterruptible) predictions by sequencing the computation of feature groups and\nreporting results using the computed features at interruption. We extend\nOrthogonal Matching Pursuit (OMP) and Forward Regression (FR) to learn the\nsequencing greedily under this group setting with costs. We theoretically\nguarantee that our algorithms achieve near-optimal linear predictions at each\nbudget when a feature group is chosen. With a novel analysis of OMP, we improve\nits theoretical bound to the same strength as that of FR. In addition, we\ndevelop a novel algorithm that consumes cost $4B$ to approximate the optimal\nperformance of \\textit{any} cost $B$, and prove that with cost less than $4B$,\nsuch an approximation is impossible. To our knowledge, these are the first\nanytime bounds at \\textit{all} budgets. We test our algorithms on two\nreal-world data-sets and evaluate them in terms of anytime linear prediction\nperformance against cost-weighted Group Lasso and alternative greedy\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 00:37:42 GMT"}, {"version": "v2", "created": "Fri, 23 Jan 2015 16:25:08 GMT"}, {"version": "v3", "created": "Mon, 23 May 2016 20:58:25 GMT"}, {"version": "v4", "created": "Mon, 5 Dec 2016 21:19:03 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Hu", "Hanzhang", ""], ["Grubb", "Alexander", ""], ["Bagnell", "J. Andrew", ""], ["Hebert", "Martial", ""]]}, {"id": "1409.5616", "submitter": "Zhaohong Deng", "authors": "Zhaohong Deng, Kup-Sze Choi, Yizhang Jiang, Jun Wang, Shitong Wang", "title": "A Survey on Soft Subspace Clustering", "comments": "This paper has been published in Information Sciences Journal in 2016", "journal-ref": null, "doi": "10.1016/j.ins.2016.01.101", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering (SC) is a promising clustering technology to identify\nclusters based on their associations with subspaces in high dimensional spaces.\nSC can be classified into hard subspace clustering (HSC) and soft subspace\nclustering (SSC). While HSC algorithms have been extensively studied and well\naccepted by the scientific community, SSC algorithms are relatively new but\ngaining more attention in recent years due to better adaptability. In the\npaper, a comprehensive survey on existing SSC algorithms and the recent\ndevelopment are presented. The SSC algorithms are classified systematically\ninto three main categories, namely, conventional SSC (CSSC), independent SSC\n(ISSC) and extended SSC (XSSC). The characteristics of these algorithms are\nhighlighted and the potential future development of SSC is also discussed.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 12:01:08 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 02:08:55 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Deng", "Zhaohong", ""], ["Choi", "Kup-Sze", ""], ["Jiang", "Yizhang", ""], ["Wang", "Jun", ""], ["Wang", "Shitong", ""]]}, {"id": "1409.5671", "submitter": "Ebru Aydin Gol", "authors": "Ebru Aydin Gol and Ezio Bartocci and Calin Belta", "title": "A Formal Methods Approach to Pattern Synthesis in Reaction Diffusion\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.LG cs.LO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a technique to detect and generate patterns in a network of\nlocally interacting dynamical systems. Central to our approach is a novel\nspatial superposition logic, whose semantics is defined over the quad-tree of a\npartitioned image. We show that formulas in this logic can be efficiently\nlearned from positive and negative examples of several types of patterns. We\nalso demonstrate that pattern detection, which is implemented as a model\nchecking algorithm, performs very well for test data sets different from the\nlearning sets. We define a quantitative semantics for the logic and integrate\nthe model checking algorithm with particle swarm optimization in a\ncomputational framework for synthesis of parameters leading to desired patterns\nin reaction-diffusion systems.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 05:21:06 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Gol", "Ebru Aydin", ""], ["Bartocci", "Ezio", ""], ["Belta", "Calin", ""]]}, {"id": "1409.5686", "submitter": "Zhaohong Deng", "authors": "Zhaohong Deng, Yizhang Jiang, Fu-Lai Chung, Hisao Ishibuchi, Kup-Sze\n  Choi, Shitong Wang", "title": "Transfer Prototype-based Fuzzy Clustering", "comments": "The manuscript has been accepted by IEEE Trans. Fuzzy Systmes in 2015", "journal-ref": null, "doi": "10.1109/TFUZZ.2015.2505330", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional prototype based clustering methods, such as the well-known\nfuzzy c-mean (FCM) algorithm, usually need sufficient data to find a good\nclustering partition. If the available data is limited or scarce, most of the\nexisting prototype based clustering algorithms will no longer be effective.\nWhile the data for the current clustering task may be scarce, there is usually\nsome useful knowledge available in the related scenes/domains. In this study,\nthe concept of transfer learning is applied to prototype based fuzzy clustering\n(PFC). Specifically, the idea of leveraging knowledge from the source domain is\nexploited to develop a set of transfer prototype based fuzzy clustering (TPFC)\nalgorithms. Three prototype based fuzzy clustering algorithms, namely, FCM,\nfuzzy k-plane clustering (FKPC) and fuzzy subspace clustering (FSC), have been\nchosen to incorporate with knowledge leveraging mechanism to develop the\ncorresponding transfer clustering algorithms. Novel objective functions are\nproposed to integrate the knowledge of source domain with the data of target\ndomain for clustering in the target domain. The proposed algorithms have been\nvalidated on different synthetic and real-world datasets and the results\ndemonstrate their effectiveness when compared with both the original prototype\nbased fuzzy clustering algorithms and the related clustering algorithms like\nmulti-task clustering and co-clustering.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 14:58:56 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 09:43:45 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Deng", "Zhaohong", ""], ["Jiang", "Yizhang", ""], ["Chung", "Fu-Lai", ""], ["Ishibuchi", "Hisao", ""], ["Choi", "Kup-Sze", ""], ["Wang", "Shitong", ""]]}, {"id": "1409.5705", "submitter": "Pengtao Xie", "authors": "Pengtao Xie, Jin Kyu Kim, Yi Zhou, Qirong Ho, Abhimanu Kumar, Yaoliang\n  Yu, Eric Xing", "title": "Distributed Machine Learning via Sufficient Factor Broadcasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix-parametrized models, including multiclass logistic regression and\nsparse coding, are used in machine learning (ML) applications ranging from\ncomputer vision to computational biology. When these models are applied to\nlarge-scale ML problems starting at millions of samples and tens of thousands\nof classes, their parameter matrix can grow at an unexpected rate, resulting in\nhigh parameter synchronization costs that greatly slow down distributed\nlearning. To address this issue, we propose a Sufficient Factor Broadcasting\n(SFB) computation model for efficient distributed learning of a large family of\nmatrix-parameterized models, which share the following property: the parameter\nupdate computed on each data sample is a rank-1 matrix, i.e., the outer product\nof two \"sufficient factors\" (SFs). By broadcasting the SFs among worker\nmachines and reconstructing the update matrices locally at each worker, SFB\nimproves communication efficiency --- communication costs are linear in the\nparameter matrix's dimensions, rather than quadratic --- without affecting\ncomputational correctness. We present a theoretical convergence analysis of\nSFB, and empirically corroborate its efficiency on four different\nmatrix-parametrized ML models.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 15:42:28 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2015 12:14:30 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Xie", "Pengtao", ""], ["Kim", "Jin Kyu", ""], ["Zhou", "Yi", ""], ["Ho", "Qirong", ""], ["Kumar", "Abhimanu", ""], ["Yu", "Yaoliang", ""], ["Xing", "Eric", ""]]}, {"id": "1409.5718", "submitter": "Lili Mou", "authors": "Lili Mou, Ge Li, Lu Zhang, Tao Wang, Zhi Jin", "title": "Convolutional Neural Networks over Tree Structures for Programming\n  Language Processing", "comments": "Accepted at AAAI-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming language processing (similar to natural language processing) is a\nhot research topic in the field of software engineering; it has also aroused\ngrowing interest in the artificial intelligence community. However, different\nfrom a natural language sentence, a program contains rich, explicit, and\ncomplicated structural information. Hence, traditional NLP models may be\ninappropriate for programs. In this paper, we propose a novel tree-based\nconvolutional neural network (TBCNN) for programming language processing, in\nwhich a convolution kernel is designed over programs' abstract syntax trees to\ncapture structural information. TBCNN is a generic architecture for programming\nlanguage processing; our experiments show its effectiveness in two different\nprogram analysis tasks: classifying programs according to functionality, and\ndetecting code snippets of certain patterns. TBCNN outperforms baseline\nmethods, including several neural models for NLP.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 06:50:52 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 12:31:51 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Mou", "Lili", ""], ["Li", "Ge", ""], ["Zhang", "Lu", ""], ["Wang", "Tao", ""], ["Jin", "Zhi", ""]]}, {"id": "1409.5743", "submitter": "Matteo Rucco", "authors": "Matteo Rucco, David M. S. Rodrigues, Emanuela Merelli, Jeffrey H.\n  Johnson, Lorenzo Falsetti, Cinzia Nitti and Aldo Salvi", "title": "Neural Hypernetwork Approach for Pulmonary Embolism diagnosis", "comments": "16 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.LG physics.data-an q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces an integrative approach based on Q-analysis with machine\nlearning. The new approach, called Neural Hypernetwork, has been applied to a\ncase study of pulmonary embolism diagnosis. The objective of the application of\nneural hyper-network to pulmonary embolism (PE) is to improve diagnose for\nreducing the number of CT-angiography needed. Hypernetworks, based on\ntopological simplicial complex, generalize the concept of two-relation to\nmany-body relation. Furthermore, Hypernetworks provide a significant\ngeneralization of network theory, enabling the integration of relational\nstructure, logic and analytic dynamics. Another important results is that\nQ-analysis stays close to the data, while other approaches manipulate data,\nprojecting them into metric spaces or applying some filtering functions to\nhighlight the intrinsic relations. A pulmonary embolism (PE) is a blockage of\nthe main artery of the lung or one of its branches, frequently fatal. Our study\nuses data on 28 diagnostic features of 1,427 people considered to be at risk of\nPE. The resulting neural hypernetwork correctly recognized 94% of those\ndeveloping a PE. This is better than previous results that have been obtained\nwith other methods (statistical selection of features, partial least squares\nregression, topological data analysis in a metric space).\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 18:04:11 GMT"}, {"version": "v2", "created": "Mon, 13 Oct 2014 14:22:48 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Rucco", "Matteo", ""], ["Rodrigues", "David M. S.", ""], ["Merelli", "Emanuela", ""], ["Johnson", "Jeffrey H.", ""], ["Falsetti", "Lorenzo", ""], ["Nitti", "Cinzia", ""], ["Salvi", "Aldo", ""]]}, {"id": "1409.5774", "submitter": "Uwe Aickelin", "authors": "Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin and Daniele Soria,\n  Jack E. Gibson and Richard B. Hubbard", "title": "Attributes for Causal Inference in Longitudinal Observational Databases", "comments": "The 26th IEEE International Symposium on Computer-Based Medical\n  Systems, Porto, pp. 548 - 549, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pharmaceutical industry is plagued by the problem of side effects that\ncan occur anytime a prescribed medication is ingested. There has been a recent\ninterest in using the vast quantities of medical data available in longitudinal\nobservational databases to identify causal relationships between drugs and\nmedical events. Unfortunately the majority of existing post marketing\nsurveillance algorithms measure how dependant or associated an event is on the\npresence of a drug rather than measuring causality. In this paper we\ninvestigate potential attributes that can be used in causal inference to\nidentify side effects based on the Bradford-Hill causality criteria. Potential\nattributes are developed by considering five of the causality criteria and\nfeature selection is applied to identify the most suitable of these attributes\nfor detecting side effects. We found that attributes based on the specificity\ncriterion may improve side effect signalling algorithms but the experiment and\ndosage criteria attributes investigated in this paper did not offer sufficient\nadditional information.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 09:07:53 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Reps", "Jenna", ""], ["Garibaldi", "Jonathan M.", ""], ["Aickelin", "Uwe", ""], ["Soria", "Daniele", ""], ["Gibson", "Jack E.", ""], ["Hubbard", "Richard B.", ""]]}, {"id": "1409.5834", "submitter": "Tim Roughgarden", "authors": "Amir Globerson and Tim Roughgarden and David Sontag and Cafer Yildirim", "title": "Tight Error Bounds for Structured Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured prediction tasks in machine learning involve the simultaneous\nprediction of multiple labels. This is typically done by maximizing a score\nfunction on the space of labels, which decomposes as a sum of pairwise\nelements, each depending on two specific labels. Intuitively, the more pairwise\nterms are used, the better the expected accuracy. However, there is currently\nno theoretical account of this intuition. This paper takes a significant step\nin this direction.\n  We formulate the problem as classifying the vertices of a known graph\n$G=(V,E)$, where the vertices and edges of the graph are labelled and correlate\nsemi-randomly with the ground truth. We show that the prospects for achieving\nlow expected Hamming error depend on the structure of the graph $G$ in\ninteresting ways. For example, if $G$ is a very poor expander, like a path,\nthen large expected Hamming error is inevitable. Our main positive result shows\nthat, for a wide class of graphs including 2D grid graphs common in machine\nvision applications, there is a polynomial-time algorithm with small and\ninformation-theoretically near-optimal expected error. Our results provide a\nfirst step toward a theoretical justification for the empirical success of the\nefficient approximate inference algorithms that are used for structured\nprediction in models where exact inference is intractable.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 23:51:09 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Globerson", "Amir", ""], ["Roughgarden", "Tim", ""], ["Sontag", "David", ""], ["Yildirim", "Cafer", ""]]}, {"id": "1409.5887", "submitter": "Tanmay Sinha", "authors": "Tanmay Sinha, Nan Li, Patrick Jermann, Pierre Dillenbourg", "title": "Capturing \"attrition intensifying\" structural traits from didactic\n  interaction sequences of MOOC learners", "comments": "\"Shared Task\" submission for EMNLP 2014 Workshop on Modeling Large\n  Scale Social Interaction in Massively Open Online Courses", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is an attempt to discover hidden structural configurations in\nlearning activity sequences of students in Massive Open Online Courses (MOOCs).\nLeveraging combined representations of video clickstream interactions and forum\nactivities, we seek to fundamentally understand traits that are predictive of\ndecreasing engagement over time. Grounded in the interdisciplinary field of\nnetwork science, we follow a graph based approach to successfully extract\nindicators of active and passive MOOC participation that reflect persistence\nand regularity in the overall interaction footprint. Using these rich\neducational semantics, we focus on the problem of predicting student attrition,\none of the major highlights of MOOC literature in the recent years. Our results\nindicate an improvement over a baseline ngram based approach in capturing\n\"attrition intensifying\" features from the learning activities that MOOC\nlearners engage in. Implications for some compelling future research are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Sat, 20 Sep 2014 14:22:20 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Sinha", "Tanmay", ""], ["Li", "Nan", ""], ["Jermann", "Patrick", ""], ["Dillenbourg", "Pierre", ""]]}, {"id": "1409.5937", "submitter": "Jiashi Feng", "authors": "Jiashi Feng, Huan Xu, Shie Mannor", "title": "Distributed Robust Learning", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for distributed robust statistical learning on {\\em\nbig contaminated data}. The Distributed Robust Learning (DRL) framework can\nreduce the computational time of traditional robust learning methods by several\norders of magnitude. We analyze the robustness property of DRL, showing that\nDRL not only preserves the robustness of the base robust learning method, but\nalso tolerates contaminations on a constant fraction of results from computing\nnodes (node failures). More precisely, even in presence of the most adversarial\noutlier distribution over computing nodes, DRL still achieves a breakdown point\nof at least $ \\lambda^*/2 $, where $ \\lambda^* $ is the break down point of\ncorresponding centralized algorithm. This is in stark contrast with naive\ndivision-and-averaging implementation, which may reduce the breakdown point by\na factor of $ k $ when $ k $ computing nodes are used. We then specialize the\nDRL framework for two concrete cases: distributed robust principal component\nanalysis and distributed robust regression. We demonstrate the efficiency and\nthe robustness advantages of DRL through comprehensive simulations and\npredicting image tags on a large-scale image set.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 03:53:20 GMT"}, {"version": "v2", "created": "Sat, 7 Feb 2015 15:55:06 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Feng", "Jiashi", ""], ["Xu", "Huan", ""], ["Mannor", "Shie", ""]]}, {"id": "1409.6041", "submitter": "Muhammad Ghifary", "authors": "Muhammad Ghifary and W. Bastiaan Kleijn and Mengjie Zhang", "title": "Domain Adaptive Neural Networks for Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple neural network model to deal with the domain adaptation\nproblem in object recognition. Our model incorporates the Maximum Mean\nDiscrepancy (MMD) measure as a regularization in the supervised learning to\nreduce the distribution mismatch between the source and target domains in the\nlatent space. From experiments, we demonstrate that the MMD regularization is\nan effective tool to provide good domain adaptation models on both SURF\nfeatures and raw image pixels of a particular image data set. We also show that\nour proposed model, preceded by the denoising auto-encoder pretraining,\nachieves better performance than recent benchmark models on the same data sets.\nThis work represents the first study of MMD measure in the context of neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 20:42:00 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Ghifary", "Muhammad", ""], ["Kleijn", "W. Bastiaan", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1409.6045", "submitter": "Paul Honeine", "authors": "Paul Honeine", "title": "Analyzing sparse dictionaries for online learning with kernels", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many signal processing and machine learning methods share essentially the\nsame linear-in-the-parameter model, with as many parameters as available\nsamples as in kernel-based machines. Sparse approximation is essential in many\ndisciplines, with new challenges emerging in online learning with kernels. To\nthis end, several sparsity measures have been proposed in the literature to\nquantify sparse dictionaries and constructing relevant ones, the most prolific\nones being the distance, the approximation, the coherence and the Babel\nmeasures. In this paper, we analyze sparse dictionaries based on these\nmeasures. By conducting an eigenvalue analysis, we show that these sparsity\nmeasures share many properties, including the linear independence condition and\ninducing a well-posed optimization problem. Furthermore, we prove that there\nexists a quasi-isometry between the parameter (i.e., dual) space and the\ndictionary's induced feature space.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 21:46:19 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Honeine", "Paul", ""]]}, {"id": "1409.6046", "submitter": "Paul Honeine", "authors": "Paul Honeine", "title": "Approximation errors of online sparsification criteria", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/TSP.2015.2442960", "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG cs.NE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning frameworks, such as resource-allocating networks,\nkernel-based methods, Gaussian processes, and radial-basis-function networks,\nrequire a sparsification scheme in order to address the online learning\nparadigm. For this purpose, several online sparsification criteria have been\nproposed to restrict the model definition on a subset of samples. The most\nknown criterion is the (linear) approximation criterion, which discards any\nsample that can be well represented by the already contributing samples, an\noperation with excessive computational complexity. Several computationally\nefficient sparsification criteria have been introduced in the literature, such\nas the distance, the coherence and the Babel criteria. In this paper, we\nprovide a framework that connects these sparsification criteria to the issue of\napproximating samples, by deriving theoretical bounds on the approximation\nerrors. Moreover, we investigate the error of approximating any feature, by\nproposing upper-bounds on the approximation error for each of the\naforementioned sparsification criteria. Two classes of features are described\nin detail, the empirical mean and the principal axes in the kernel principal\ncomponent analysis.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 21:53:08 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Honeine", "Paul", ""]]}, {"id": "1409.6075", "submitter": "Tyler Ward", "authors": "Tyler Ward", "title": "The Information Theoretically Efficient Model (ITEM): A model for\n  computerized analysis of large datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document discusses the Information Theoretically Efficient Model (ITEM),\na computerized system to generate an information theoretically efficient\nmultinomial logistic regression from a general dataset. More specifically, this\nmodel is designed to succeed even where the logit transform of the dependent\nvariable is not necessarily linear in the independent variables. This research\nshows that for large datasets, the resulting models can be produced on modern\ncomputers in a tractable amount of time. These models are also resistant to\noverfitting, and as such they tend to produce interpretable models with only a\nlimited number of features, all of which are designed to be well behaved.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 03:39:23 GMT"}, {"version": "v2", "created": "Mon, 6 Oct 2014 11:12:07 GMT"}, {"version": "v3", "created": "Tue, 4 Nov 2014 05:41:04 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Ward", "Tyler", ""]]}, {"id": "1409.6110", "submitter": "Marta Soare", "authors": "Marta Soare, Alessandro Lazaric, R\\'emi Munos", "title": "Best-Arm Identification in Linear Bandits", "comments": "In Advances in Neural Information Processing Systems 27 (NIPS), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the best-arm identification problem in linear bandit, where the\nrewards of the arms depend linearly on an unknown parameter $\\theta^*$ and the\nobjective is to return the arm with the largest reward. We characterize the\ncomplexity of the problem and introduce sample allocation strategies that pull\narms to identify the best arm with a fixed confidence, while minimizing the\nsample budget. In particular, we show the importance of exploiting the global\nlinear structure to improve the estimate of the reward of near-optimal arms. We\nanalyze the proposed strategies and compare their empirical performance.\nFinally, as a by-product of our analysis, we point out the connection to the\n$G$-optimality criterion used in optimal experimental design.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 08:41:02 GMT"}, {"version": "v2", "created": "Tue, 4 Nov 2014 14:21:28 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Soare", "Marta", ""], ["Lazaric", "Alessandro", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1409.6111", "submitter": "Xiaochuan Zhao", "authors": "Xiaochuan Zhao and Ali H. Sayed", "title": "Distributed Clustering and Learning Over Networks", "comments": "47 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.MA cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed processing over networks relies on in-network processing and\ncooperation among neighboring agents. Cooperation is beneficial when agents\nshare a common objective. However, in many applications agents may belong to\ndifferent clusters that pursue different objectives. Then, indiscriminate\ncooperation will lead to undesired results. In this work, we propose an\nadaptive clustering and learning scheme that allows agents to learn which\nneighbors they should cooperate with and which other neighbors they should\nignore. In doing so, the resulting algorithm enables the agents to identify\ntheir clusters and to attain improved learning and estimation accuracy over\nnetworks. We carry out a detailed mean-square analysis and assess the error\nprobabilities of Types I and II, i.e., false alarm and mis-detection, for the\nclustering mechanism. Among other results, we establish that these\nprobabilities decay exponentially with the step-sizes so that the probability\nof correct clustering can be made arbitrarily close to one.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 08:47:11 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Zhao", "Xiaochuan", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1409.6440", "submitter": "Rashid Khogali", "authors": "Rashid Khogali", "title": "A non-linear learning & classification algorithm that achieves full\n  training accuracy with stellar classification accuracy", "comments": "43 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fast Non-linear and non-iterative learning and classification algorithm is\nsynthesized and validated. This algorithm named the \"Reverse Ripple\nEffect(R.R.E)\", achieves 100% learning accuracy but is computationally\nexpensive upon classification. The R.R.E is a (deterministic) algorithm that\nsuper imposes Gaussian weighted functions on training points. In this work, the\nR.R.E algorithm is compared against known learning and classification\ntechniques/algorithms such as: the Perceptron Criterion algorithm, Linear\nSupport Vector machines, the Linear Fisher Discriminant and a simple Neural\nNetwork. The classification accuracy of the R.R.E algorithm is evaluated using\nsimulations conducted in MATLAB. The R.R.E algorithm's behaviour is analyzed\nunder linearly and non-linearly separable data sets. For the comparison with\nthe Neural Network, the classical XOR problem is considered.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2014 08:17:07 GMT"}, {"version": "v2", "created": "Fri, 31 Oct 2014 17:46:07 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Khogali", "Rashid", ""]]}, {"id": "1409.6448", "submitter": "Bo Han", "authors": "Bo Han, Bo He, Tingting Sun, Mengmeng Ma, Amaury Lendasse", "title": "HSR: L1/2 Regularized Sparse Representation for Fast Face Recognition\n  using Hierarchical Feature Selection", "comments": "Submitted to IEEE Computational Intelligence Magazine in 09/2014", "journal-ref": null, "doi": "10.1007/s00521-015-1907-y", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method for fast face recognition called\nL1/2 Regularized Sparse Representation using Hierarchical Feature Selection\n(HSR). By employing hierarchical feature selection, we can compress the scale\nand dimension of global dictionary, which directly contributes to the decrease\nof computational cost in sparse representation that our approach is strongly\nrooted in. It consists of Gabor wavelets and Extreme Learning Machine\nAuto-Encoder (ELM-AE) hierarchically. For Gabor wavelets part, local features\ncan be extracted at multiple scales and orientations to form Gabor-feature\nbased image, which in turn improves the recognition rate. Besides, in the\npresence of occluded face image, the scale of Gabor-feature based global\ndictionary can be compressed accordingly because redundancies exist in\nGabor-feature based occlusion dictionary. For ELM-AE part, the dimension of\nGabor-feature based global dictionary can be compressed because\nhigh-dimensional face images can be rapidly represented by low-dimensional\nfeature. By introducing L1/2 regularization, our approach can produce sparser\nand more robust representation compared to regularized Sparse Representation\nbased Classification (SRC), which also contributes to the decrease of the\ncomputational cost in sparse representation. In comparison with related work\nsuch as SRC and Gabor-feature based SRC (GSRC), experimental results on a\nvariety of face databases demonstrate the great advantage of our method for\ncomputational cost. Moreover, we also achieve approximate or even better\nrecognition rate.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2014 08:36:05 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Han", "Bo", ""], ["He", "Bo", ""], ["Sun", "Tingting", ""], ["Ma", "Mengmeng", ""], ["Lendasse", "Amaury", ""]]}, {"id": "1409.6805", "submitter": "Siting Ren", "authors": "Siting Ren, Sheng Gao", "title": "Improving Cross-domain Recommendation through Probabilistic\n  Cluster-level Latent Factor Model--Extended Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain recommendation has been proposed to transfer user behavior\npattern by pooling together the rating data from multiple domains to alleviate\nthe sparsity problem appearing in single rating domains. However, previous\nmodels only assume that multiple domains share a latent common rating pattern\nbased on the user-item co-clustering. To capture diversities among different\ndomains, we propose a novel Probabilistic Cluster-level Latent Factor (PCLF)\nmodel to improve the cross-domain recommendation performance. Experiments on\nseveral real world datasets demonstrate that our proposed model outperforms the\nstate-of-the-art methods for the cross-domain recommendation task.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 02:55:31 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Ren", "Siting", ""], ["Gao", "Sheng", ""]]}, {"id": "1409.6981", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Unsupervised learning of regression mixture models with unknown number\n  of components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression mixture models are widely studied in statistics, machine learning\nand data analysis. Fitting regression mixtures is challenging and is usually\nperformed by maximum likelihood by using the expectation-maximization (EM)\nalgorithm. However, it is well-known that the initialization is crucial for EM.\nIf the initialization is inappropriately performed, the EM algorithm may lead\nto unsatisfactory results. The EM algorithm also requires the number of\nclusters to be given a priori; the problem of selecting the number of mixture\ncomponents requires using model selection criteria to choose one from a set of\npre-estimated candidate models. We propose a new fully unsupervised algorithm\nto learn regression mixture models with unknown number of components. The\ndeveloped unsupervised learning approach consists in a penalized maximum\nlikelihood estimation carried out by a robust expectation-maximization (EM)\nalgorithm for fitting polynomial, spline and B-spline regressions mixtures. The\nproposed learning approach is fully unsupervised: 1) it simultaneously infers\nthe model parameters and the optimal number of the regression mixture\ncomponents from the data as the learning proceeds, rather than in a two-fold\nscheme as in standard model-based clustering using afterward model selection\ncriteria, and 2) it does not require accurate initialization unlike the\nstandard EM for regression mixtures. The developed approach is applied to curve\nclustering problems. Numerical experiments on simulated data show that the\nproposed robust EM algorithm performs well and provides accurate results in\nterms of robustness with regard initialization and retrieving the optimal\npartition with the actual number of clusters. An application to real data in\nthe framework of functional data clustering, confirms the benefit of the\nproposed approach for practical applications.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 14:55:00 GMT"}], "update_date": "2014-09-25", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1409.7074", "submitter": "Charles Fisher", "authors": "Charles K. Fisher", "title": "Variational Pseudolikelihood for Regularized Ising Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I propose a variational approach to maximum pseudolikelihood inference of the\nIsing model. The variational algorithm is more computationally efficient, and\ndoes a better job predicting out-of-sample correlations than $L_2$ regularized\nmaximum pseudolikelihood inference as well as mean field and isolated spin pair\napproximations with pseudocount regularization. The key to the approach is a\nvariational energy that regularizes the inference problem by shrinking the\ncouplings towards zero, while still allowing some large couplings to explain\nstrong correlations. The utility of the variational pseudolikelihood approach\nis illustrated by training an Ising model to represent the letters A-J using\nsamples of letters from different computer fonts.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 20:01:15 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Fisher", "Charles K.", ""]]}, {"id": "1409.7085", "submitter": "Michael Bloodgood", "authors": "Kathryn Baker, Michael Bloodgood, Chris Callison-Burch, Bonnie J.\n  Dorr, Nathaniel W. Filardo, Lori Levin, Scott Miller and Christine Piatko", "title": "Semantically-Informed Syntactic Machine Translation: A Tree-Grafting\n  Approach", "comments": "10 pages, 7 figures, 3 tables; appeared in Proceedings of the Ninth\n  Conference of the Association for Machine Translation in the Americas (AMTA),\n  October 2010", "journal-ref": "In Proceedings of the Ninth Conference of the Association for\n  Machine Translation in the Americas (AMTA), Denver, Colorado, October 2010", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a unified and coherent syntactic framework for supporting a\nsemantically-informed syntactic approach to statistical machine translation.\nSemantically enriched syntactic tags assigned to the target-language training\ntexts improved translation quality. The resulting system significantly\noutperformed a linguistically naive baseline model (Hiero), and reached the\nhighest scores yet reported on the NIST 2009 Urdu-English translation task.\nThis finding supports the hypothesis (posed by many researchers in the MT\ncommunity, e.g., in DARPA GALE) that both syntactic and semantic information\nare critical for improving translation quality---and further demonstrates that\nlarge gains can be achieved for low-resource languages with different word\norder than English.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 20:16:49 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Baker", "Kathryn", ""], ["Bloodgood", "Michael", ""], ["Callison-Burch", "Chris", ""], ["Dorr", "Bonnie J.", ""], ["Filardo", "Nathaniel W.", ""], ["Levin", "Lori", ""], ["Miller", "Scott", ""], ["Piatko", "Christine", ""]]}, {"id": "1409.7165", "submitter": "Liang Wu", "authors": "Liang Wu, Hui Xiong, Liang Du, Bo Liu, Guandong Xu, Yong Ge, Yanjie\n  Fu, Yuanchun Zhou, Jianhui Li", "title": "Heterogeneous Metric Learning with Content-based Regularization for\n  Software Artifact Retrieval", "comments": "to appear in IEEE International Conference on Data Mining (ICDM),\n  Shen Zhen, China, December 2014", "journal-ref": null, "doi": "10.1109/ICDM.2014.147", "report-no": null, "categories": "cs.LG cs.IR cs.SE", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The problem of software artifact retrieval has the goal to effectively locate\nsoftware artifacts, such as a piece of source code, in a large code repository.\nThis problem has been traditionally addressed through the textual query. In\nother words, information retrieval techniques will be exploited based on the\ntextual similarity between queries and textual representation of software\nartifacts, which is generated by collecting words from comments, identifiers,\nand descriptions of programs. However, in addition to these semantic\ninformation, there are rich information embedded in source codes themselves.\nThese source codes, if analyzed properly, can be a rich source for enhancing\nthe efforts of software artifact retrieval. To this end, in this paper, we\ndevelop a feature extraction method on source codes. Specifically, this method\ncan capture both the inherent information in the source codes and the semantic\ninformation hidden in the comments, descriptions, and identifiers of the source\ncodes. Moreover, we design a heterogeneous metric learning approach, which\nallows to integrate code features and text features into the same latent\nsemantic space. This, in turn, can help to measure the artifact similarity by\nexploiting the joint power of both code and text features. Finally, extensive\nexperiments on real-world data show that the proposed method can help to\nimprove the performances of software artifact retrieval with a significant\nmargin.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 06:33:57 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Wu", "Liang", ""], ["Xiong", "Hui", ""], ["Du", "Liang", ""], ["Liu", "Bo", ""], ["Xu", "Guandong", ""], ["Ge", "Yong", ""], ["Fu", "Yanjie", ""], ["Zhou", "Yuanchun", ""], ["Li", "Jianhui", ""]]}, {"id": "1409.7202", "submitter": "Tofigh Naghibi", "authors": "Tofigh Naghibi, Beat Pfister", "title": "A Boosting Framework on Grounds of Online Learning", "comments": "Accepted in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By exploiting the duality between boosting and online learning, we present a\nboosting framework which proves to be extremely powerful thanks to employing\nthe vast knowledge available in the online learning area. Using this framework,\nwe develop various algorithms to address multiple practically and theoretically\ninteresting questions including sparse boosting, smooth-distribution boosting,\nagnostic learning and some generalization to double-projection online learning\nalgorithms, as a by-product.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 10:02:01 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 11:05:29 GMT"}, {"version": "v3", "created": "Sun, 23 Nov 2014 14:46:24 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Naghibi", "Tofigh", ""], ["Pfister", "Beat", ""]]}, {"id": "1409.7384", "submitter": "Tofigh Naghibi", "authors": "Tofigh Naghibi, Sarah Hoffmann and Beat Pfister", "title": "A Semidefinite Programming Based Search Strategy for Feature Selection\n  with Mutual Information Measure", "comments": "IEEETrans On Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature subset selection, as a special case of the general subset selection\nproblem, has been the topic of a considerable number of studies due to the\ngrowing importance of data-mining applications. In the feature subset selection\nproblem there are two main issues that need to be addressed: (i) Finding an\nappropriate measure function than can be fairly fast and robustly computed for\nhigh-dimensional data. (ii) A search strategy to optimize the measure over the\nsubset space in a reasonable amount of time. In this article mutual information\nbetween features and class labels is considered to be the measure function. Two\nseries expansions for mutual information are proposed, and it is shown that\nmost heuristic criteria suggested in the literature are truncated\napproximations of these expansions. It is well-known that searching the whole\nsubset space is an NP-hard problem. Here, instead of the conventional\nsequential search algorithms, we suggest a parallel search strategy based on\nsemidefinite programming (SDP) that can search through the subset space in\npolynomial time. By exploiting the similarities between the proposed algorithm\nand an instance of the maximum-cut problem in graph theory, the approximation\nratio of this algorithm is derived and is compared with the approximation ratio\nof the backward elimination method. The experiments show that it can be\nmisleading to judge the quality of a measure solely based on the classification\naccuracy, without taking the effect of the non-optimum search strategy into\naccount.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 11:57:48 GMT"}, {"version": "v2", "created": "Wed, 12 Nov 2014 13:56:54 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Naghibi", "Tofigh", ""], ["Hoffmann", "Sarah", ""], ["Pfister", "Beat", ""]]}, {"id": "1409.7461", "submitter": "Ozan \\.Irsoy", "authors": "Ozan \\.Irsoy, Ethem Alpayd{\\i}n", "title": "Autoencoder Trees", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss an autoencoder model in which the encoding and decoding functions\nare implemented by decision trees. We use the soft decision tree where internal\nnodes realize soft multivariate splits given by a gating function and the\noverall output is the average of all leaves weighted by the gating values on\ntheir path. The encoder tree takes the input and generates a lower dimensional\nrepresentation in the leaves and the decoder tree takes this and reconstructs\nthe original input. Exploiting the continuity of the trees, autoencoder trees\nare trained with stochastic gradient descent. On handwritten digit and news\ndata, we see that the autoencoder trees yield good reconstruction error\ncompared to traditional autoencoder perceptrons. We also see that the\nautoencoder tree captures hierarchical representations at different\ngranularities of the data on its different levels and the leaves capture the\nlocalities in the input space.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 02:27:04 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["\u0130rsoy", "Ozan", ""], ["Alpayd\u0131n", "Ethem", ""]]}, {"id": "1409.7476", "submitter": "Michel Fliess", "authors": "C\\'edric Join (INRIA Lille - Nord Europe, CRAN, AL.I.E.N.), Cyril\n  Voyant (SPE), Michel Fliess (AL.I.E.N., LIX), Marc Muselli (SPE), Marie Laure\n  Nivet (SPE), Christophe Paoli, Fr\\'ed\\'eric Chaxel (CRAN)", "title": "Short-term solar irradiance and irradiation forecasts via different time\n  series techniques: A preliminary study", "comments": null, "journal-ref": "3rd International Symposium on Environment-Friendly Energies and\n  Applications (EFEA 2014), Pars : France (2014)", "doi": null, "report-no": null, "categories": "cs.LG physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This communication is devoted to solar irradiance and irradiation short-term\nforecasts, which are useful for electricity production. Several different time\nseries approaches are employed. Our results and the corresponding numerical\nsimulations show that techniques which do not need a large amount of historical\ndata behave better than those which need them, especially when those data are\nquite noisy.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 06:27:30 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Join", "C\u00e9dric", "", "INRIA Lille - Nord Europe, CRAN, AL.I.E.N."], ["Voyant", "Cyril", "", "SPE"], ["Fliess", "Michel", "", "AL.I.E.N., LIX"], ["Muselli", "Marc", "", "SPE"], ["Nivet", "Marie Laure", "", "SPE"], ["Paoli", "Christophe", "", "CRAN"], ["Chaxel", "Fr\u00e9d\u00e9ric", "", "CRAN"]]}, {"id": "1409.7480", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny, Ahmed Elgammal", "title": "Generalized Twin Gaussian Processes using Sharma-Mittal Divergence", "comments": "This work got accepted for Publication in the Machine Learning\n  Journal 2015. The work is scheduled for presentation at ECML-PKDD 2015\n  journal track papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a growing interest in mutual information measures due to their\nwide range of applications in Machine Learning and Computer Vision. In this\npaper, we present a generalized structured regression framework based on\nShama-Mittal divergence, a relative entropy measure, which is introduced to the\nMachine Learning community in this work. Sharma-Mittal (SM) divergence is a\ngeneralized mutual information measure for the widely used R\\'enyi, Tsallis,\nBhattacharyya, and Kullback-Leibler (KL) relative entropies. Specifically, we\nstudy Sharma-Mittal divergence as a cost function in the context of the Twin\nGaussian Processes (TGP)~\\citep{Bo:2010}, which generalizes over the\nKL-divergence without computational penalty. We show interesting properties of\nSharma-Mittal TGP (SMTGP) through a theoretical analysis, which covers missing\ninsights in the traditional TGP formulation. However, we generalize this theory\nbased on SM-divergence instead of KL-divergence which is a special case.\nExperimentally, we evaluated the proposed SMTGP framework on several datasets.\nThe results show that SMTGP reaches better predictions than KL-based TGP, since\nit offers a bigger class of models through its parameters that we learn from\nthe data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 06:46:38 GMT"}, {"version": "v2", "created": "Wed, 1 Oct 2014 13:32:50 GMT"}, {"version": "v3", "created": "Fri, 3 Oct 2014 03:54:41 GMT"}, {"version": "v4", "created": "Mon, 6 Oct 2014 03:47:51 GMT"}, {"version": "v5", "created": "Mon, 1 Jun 2015 06:30:29 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1409.7495", "submitter": "Yaroslav Ganin", "authors": "Yaroslav Ganin, Victor Lempitsky", "title": "Unsupervised Domain Adaptation by Backpropagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-performing deep architectures are trained on massive amounts of labeled\ndata. In the absence of labeled data for a certain task, domain adaptation\noften provides an attractive option given that labeled data of similar nature\nbut from a different domain (e.g. synthetic images) are available. Here, we\npropose a new approach to domain adaptation in deep architectures that can be\ntrained on large amount of labeled data from the source domain and large amount\nof unlabeled data from the target domain (no labeled target-domain data is\nnecessary).\n  As the training progresses, the approach promotes the emergence of \"deep\"\nfeatures that are (i) discriminative for the main learning task on the source\ndomain and (ii) invariant with respect to the shift between the domains. We\nshow that this adaptation behaviour can be achieved in almost any feed-forward\nmodel by augmenting it with few standard layers and a simple new gradient\nreversal layer. The resulting augmented architecture can be trained using\nstandard backpropagation.\n  Overall, the approach can be implemented with little effort using any of the\ndeep-learning packages. The method performs very well in a series of image\nclassification experiments, achieving adaptation effect in the presence of big\ndomain shifts and outperforming previous state-of-the-art on Office datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 08:22:21 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 14:54:37 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Ganin", "Yaroslav", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1409.7552", "submitter": "Johannes Kulick", "authors": "Johannes Kulick, Robert Lieck and Marc Toussaint", "title": "The Advantage of Cross Entropy over Entropy in Iterative Information\n  Gathering", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gathering the most information by picking the least amount of data is a\ncommon task in experimental design or when exploring an unknown environment in\nreinforcement learning and robotics. A widely used measure for quantifying the\ninformation contained in some distribution of interest is its entropy. Greedily\nminimizing the expected entropy is therefore a standard method for choosing\nsamples in order to gain strong beliefs about the underlying random variables.\nWe show that this approach is prone to temporally getting stuck in local optima\ncorresponding to wrongly biased beliefs. We suggest instead maximizing the\nexpected cross entropy between old and new belief, which aims at challenging\nrefutable beliefs and thereby avoids these local optima. We show that both\ncriteria are closely related and that their difference can be traced back to\nthe asymmetry of the Kullback-Leibler divergence. In illustrative examples as\nwell as simulated and real-world experiments we demonstrate the advantage of\ncross entropy over simple entropy for practical applications.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 12:25:33 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 10:03:11 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Kulick", "Johannes", ""], ["Lieck", "Robert", ""], ["Toussaint", "Marc", ""]]}, {"id": "1409.7612", "submitter": "Rushdi Shams Mr", "authors": "Rushdi Shams", "title": "Semi-supervised Classification for Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised classification is an interesting idea where classification\nmodels are learned from both labeled and unlabeled data. It has several\nadvantages over supervised classification in natural language processing\ndomain. For instance, supervised classification exploits only labeled data that\nare expensive, often difficult to get, inadequate in quantity, and require\nhuman experts for annotation. On the other hand, unlabeled data are inexpensive\nand abundant. Despite the fact that many factors limit the wide-spread use of\nsemi-supervised classification, it has become popular since its level of\nperformance is empirically as good as supervised classification. This study\nexplores the possibilities and achievements as well as complexity and\nlimitations of semi-supervised classification for several natural langue\nprocessing tasks like parsing, biomedical information processing, text\nclassification, and summarization.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 15:18:44 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Shams", "Rushdi", ""]]}, {"id": "1409.7780", "submitter": "Jim Jing-Yan Wang", "authors": "Jim Jing-Yan Wang, Yi Wang, Shiguang Zhao, Xin Gao", "title": "Maximum mutual information regularized classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel pattern classification approach is proposed by\nregularizing the classifier learning to maximize mutual information between the\nclassification response and the true class label. We argue that, with the\nlearned classifier, the uncertainty of the true class label of a data sample\nshould be reduced by knowing its classification response as much as possible.\nThe reduced uncertainty is measured by the mutual information between the\nclassification response and the true class label. To this end, when learning a\nlinear classifier, we propose to maximize the mutual information between\nclassification responses and true class labels of training samples, besides\nminimizing the classification error and reduc- ing the classifier complexity.\nAn objective function is constructed by modeling mutual information with\nentropy estimation, and it is optimized by a gradi- ent descend method in an\niterative algorithm. Experiments on two real world pattern classification\nproblems show the significant improvements achieved by maximum mutual\ninformation regularization.\n", "versions": [{"version": "v1", "created": "Sat, 27 Sep 2014 07:45:56 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Wang", "Jim Jing-Yan", ""], ["Wang", "Yi", ""], ["Zhao", "Shiguang", ""], ["Gao", "Xin", ""]]}, {"id": "1409.7794", "submitter": "Steven C.H. Hoi", "authors": "Yue Wu, Steven C. H. Hoi, Tao Mei, Nenghai Yu", "title": "Large-scale Online Feature Selection for Ultra-high Dimensional Sparse\n  Data", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection with large-scale high-dimensional data is important yet\nvery challenging in machine learning and data mining. Online feature selection\nis a promising new paradigm that is more efficient and scalable than batch\nfeature section methods, but the existing online approaches usually fall short\nin their inferior efficacy as compared with batch approaches. In this paper, we\npresent a novel second-order online feature selection scheme that is simple yet\neffective, very fast and extremely scalable to deal with large-scale ultra-high\ndimensional sparse data streams. The basic idea is to improve the existing\nfirst-order online feature selection methods by exploiting second-order\ninformation for choosing the subset of important features with high confidence\nweights. However, unlike many second-order learning methods that often suffer\nfrom extra high computational cost, we devise a novel smart algorithm for\nsecond-order online feature selection using a MaxHeap-based approach, which is\nnot only more effective than the existing first-order approaches, but also\nsignificantly more efficient and scalable for large-scale feature selection\nwith ultra-high dimensional sparse data, as validated from our extensive\nexperiments. Impressively, on a billion-scale synthetic dataset (1-billion\ndimensions, 1-billion nonzero features, and 1-million samples), our new\nalgorithm took only 8 minutes on a single PC, which is orders of magnitudes\nfaster than traditional batch approaches. \\url{http://arxiv.org/abs/1409.7794}\n", "versions": [{"version": "v1", "created": "Sat, 27 Sep 2014 10:58:09 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 12:49:16 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2015 14:21:21 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Wu", "Yue", ""], ["Hoi", "Steven C. H.", ""], ["Mei", "Tao", ""], ["Yu", "Nenghai", ""]]}, {"id": "1409.7930", "submitter": "Weijia Han", "authors": "Weijia Han, Huiyan Sang, Min Sheng, Jiandong Li, and Shuguang Cui", "title": "Cognitive Learning of Statistical Primary Patterns via Bayesian Network", "comments": "This paper has been refreshed with a new version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cognitive radio (CR) technology, the trend of sensing is no longer to only\ndetect the presence of active primary users. A large number of applications\ndemand for more comprehensive knowledge on primary user behaviors in spatial,\ntemporal, and frequency domains. To satisfy such requirements, we study the\nstatistical relationship among primary users by introducing a Bayesian network\n(BN) based framework. How to learn such a BN structure is a long standing\nissue, not fully understood even in the statistical learning community.\nBesides, another key problem in this learning scenario is that the CR has to\nidentify how many variables are in the BN, which is usually considered as prior\nknowledge in statistical learning applications. To solve such two issues\nsimultaneously, this paper proposes a BN structure learning scheme consisting\nof an efficient structure learning algorithm and a blind variable\nidentification scheme. The proposed approach incurs significantly lower\ncomputational complexity compared with previous ones, and is capable of\ndetermining the structure without assuming much prior knowledge about\nvariables. With this result, cognitive users could efficiently understand the\nstatistical pattern of primary networks, such that more efficient cognitive\nprotocols could be designed across different network layers.\n", "versions": [{"version": "v1", "created": "Sun, 28 Sep 2014 16:36:06 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 17:24:30 GMT"}, {"version": "v3", "created": "Fri, 10 Oct 2014 23:39:38 GMT"}, {"version": "v4", "created": "Fri, 19 Dec 2014 02:57:49 GMT"}, {"version": "v5", "created": "Mon, 9 Feb 2015 13:01:07 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Han", "Weijia", ""], ["Sang", "Huiyan", ""], ["Sheng", "Min", ""], ["Li", "Jiandong", ""], ["Cui", "Shuguang", ""]]}, {"id": "1409.7935", "submitter": "Lior Shamir", "authors": "Evan Kuminski, Joe George, John Wallin, Lior Shamir", "title": "Combining human and machine learning for morphological analysis of\n  galaxy images", "comments": "PASP, accepted", "journal-ref": null, "doi": "10.1086/678977", "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing importance of digital sky surveys collecting many millions of\ngalaxy images has reinforced the need for robust methods that can perform\nmorphological analysis of large galaxy image databases. Citizen science\ninitiatives such as Galaxy Zoo showed that large datasets of galaxy images can\nbe analyzed effectively by non-scientist volunteers, but since databases\ngenerated by robotic telescopes grow much faster than the processing power of\nany group of citizen scientists, it is clear that computer analysis is\nrequired. Here we propose to use citizen science data for training machine\nlearning systems, and show experimental results demonstrating that machine\nlearning systems can be trained with citizen science data. Our findings show\nthat the performance of machine learning depends on the quality of the data,\nwhich can be improved by using samples that have a high degree of agreement\nbetween the citizen scientists. The source code of the method is publicly\navailable.\n", "versions": [{"version": "v1", "created": "Sun, 28 Sep 2014 17:47:35 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Kuminski", "Evan", ""], ["George", "Joe", ""], ["Wallin", "John", ""], ["Shamir", "Lior", ""]]}, {"id": "1409.7938", "submitter": "Baharan Mirzasoleiman", "authors": "Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan\n  Vondrak, and Andreas Krause", "title": "Lazier Than Lazy Greedy", "comments": "In Proc. Conference on Artificial Intelligence (AAAI), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to maximize a monotone submodular function faster than the\nwidely used lazy greedy algorithm (also known as accelerated greedy), both in\ntheory and practice? In this paper, we develop the first linear-time algorithm\nfor maximizing a general monotone submodular function subject to a cardinality\nconstraint. We show that our randomized algorithm, STOCHASTIC-GREEDY, can\nachieve a $(1-1/e-\\varepsilon)$ approximation guarantee, in expectation, to the\noptimum solution in time linear in the size of the data and independent of the\ncardinality constraint. We empirically demonstrate the effectiveness of our\nalgorithm on submodular functions arising in data summarization, including\ntraining large-scale kernel methods, exemplar-based clustering, and sensor\nplacement. We observe that STOCHASTIC-GREEDY practically achieves the same\nutility value as lazy greedy but runs much faster. More surprisingly, we\nobserve that in many practical scenarios STOCHASTIC-GREEDY does not evaluate\nthe whole fraction of data points even once and still achieves\nindistinguishable results compared to lazy greedy.\n", "versions": [{"version": "v1", "created": "Sun, 28 Sep 2014 18:06:23 GMT"}, {"version": "v2", "created": "Wed, 26 Nov 2014 08:45:32 GMT"}, {"version": "v3", "created": "Fri, 28 Nov 2014 13:06:54 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Mirzasoleiman", "Baharan", ""], ["Badanidiyuru", "Ashwinkumar", ""], ["Karbasi", "Amin", ""], ["Vondrak", "Jan", ""], ["Krause", "Andreas", ""]]}, {"id": "1409.7963", "submitter": "Arjun Jain", "authors": "Arjun Jain, Jonathan Tompson, Yann LeCun and Christoph Bregler", "title": "MoDeep: A Deep Learning Framework Using Motion Features for Human Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel and efficient method for articulated human\npose estimation in videos using a convolutional network architecture, which\nincorporates both color and motion features. We propose a new human body pose\ndataset, FLIC-motion, that extends the FLIC dataset with additional motion\nfeatures. We apply our architecture to this dataset and report significantly\nbetter performance than current state-of-the-art pose detection systems.\n", "versions": [{"version": "v1", "created": "Sun, 28 Sep 2014 21:32:15 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Jain", "Arjun", ""], ["Tompson", "Jonathan", ""], ["LeCun", "Yann", ""], ["Bregler", "Christoph", ""]]}, {"id": "1409.7985", "submitter": "Yanchuan Sim", "authors": "Yanchuan Sim and Bryan Routledge and Noah A. Smith", "title": "The Utility of Text: The Case of Amicus Briefs and the Supreme Court", "comments": "Working draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.GT cs.LG", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We explore the idea that authoring a piece of text is an act of maximizing\none's expected utility. To make this idea concrete, we consider the societally\nimportant decisions of the Supreme Court of the United States. Extensive past\nwork in quantitative political science provides a framework for empirically\nmodeling the decisions of justices and how they relate to text. We incorporate\ninto such a model texts authored by amici curiae (\"friends of the court\"\nseparate from the litigants) who seek to weigh in on the decision, then\nexplicitly model their goals in a random utility model. We demonstrate the\nbenefits of this approach in improved vote prediction and the ability to\nperform counterfactual analysis.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 03:04:26 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 14:54:53 GMT"}, {"version": "v3", "created": "Tue, 7 Oct 2014 18:47:43 GMT"}, {"version": "v4", "created": "Fri, 10 Oct 2014 00:20:33 GMT"}, {"version": "v5", "created": "Tue, 25 Nov 2014 21:29:15 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Sim", "Yanchuan", ""], ["Routledge", "Bryan", ""], ["Smith", "Noah A.", ""]]}, {"id": "1409.8185", "submitter": "Theodoros Tsiligkaridis", "authors": "Theodoros Tsiligkaridis, Keith W. Forsythe", "title": "Adaptive Low-Complexity Sequential Inference for Dirichlet Process\n  Mixture Models", "comments": "25 pages, To appear in Advances in Neural Information Processing\n  Systems (NIPS) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a sequential low-complexity inference procedure for Dirichlet\nprocess mixtures of Gaussians for online clustering and parameter estimation\nwhen the number of clusters are unknown a-priori. We present an easily\ncomputable, closed form parametric expression for the conditional likelihood,\nin which hyperparameters are recursively updated as a function of the streaming\ndata assuming conjugate priors. Motivated by large-sample asymptotics, we\npropose a novel adaptive low-complexity design for the Dirichlet process\nconcentration parameter and show that the number of classes grow at most at a\nlogarithmic rate. We further prove that in the large-sample limit, the\nconditional likelihood and data predictive distribution become asymptotically\nGaussian. We demonstrate through experiments on synthetic and real data sets\nthat our approach is superior to other online state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 16:47:44 GMT"}, {"version": "v2", "created": "Thu, 5 Feb 2015 15:55:06 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2015 20:07:31 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Tsiligkaridis", "Theodoros", ""], ["Forsythe", "Keith W.", ""]]}, {"id": "1409.8191", "submitter": "Djallel Bouneffouf", "authors": "Robin Allesiardo, Raphael Feraud and Djallel Bouneffouf", "title": "A Neural Networks Committee for the Contextual Bandit Problem", "comments": "21st International Conference on Neural Information Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new contextual bandit algorithm, NeuralBandit, which\ndoes not need hypothesis on stationarity of contexts and rewards. Several\nneural networks are trained to modelize the value of rewards knowing the\ncontext. Two variants, based on multi-experts approach, are proposed to choose\nonline the parameters of multi-layer perceptrons. The proposed algorithms are\nsuccessfully tested on a large dataset with and without stationarity of\nrewards.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 17:08:21 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Allesiardo", "Robin", ""], ["Feraud", "Raphael", ""], ["Bouneffouf", "Djallel", ""]]}, {"id": "1409.8202", "submitter": "Matteo De Felice", "authors": "Matteo De Felice, Marcello Petitta, Paolo M. Ruti", "title": "Short-Term Predictability of Photovoltaic Production over Italy", "comments": "Submitted to Renewable Energy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photovoltaic (PV) power production increased drastically in Europe throughout\nthe last years. About the 6% of electricity in Italy comes from PV and for an\nefficient management of the power grid an accurate and reliable forecasting of\nproduction would be needed. Starting from a dataset of electricity production\nof 65 Italian solar plants for the years 2011-2012 we investigate the\npossibility to forecast daily production from one to ten days of lead time\nwithout using on site measurements. Our study is divided in two parts: an\nassessment of the predictability of meteorological variables using weather\nforecasts and an analysis on the application of data-driven modelling in\npredicting solar power production. We calibrate a SVM model using available\nobservations and then we force the same model with the predicted variables from\nweather forecasts with a lead time from one to ten days. As expected, solar\npower production is strongly influenced by cloudiness and clear sky, in fact we\nobserve that while during summer we obtain a general error under the 10%\n(slightly lower in south Italy), during winter the error is abundantly above\nthe 20%.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 17:24:29 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["De Felice", "Matteo", ""], ["Petitta", "Marcello", ""], ["Ruti", "Paolo M.", ""]]}, {"id": "1409.8211", "submitter": "Pavel Kuksa", "authors": "Pavel P. Kuksa", "title": "Efficient multivariate sequence classification", "comments": "multivariate sequence classification, string kernels, vector\n  quantization, direct feature quantization, music classification, protein\n  classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based approaches for sequence classification have been successfully\napplied to a variety of domains, including the text categorization, image\nclassification, speech analysis, biological sequence analysis, time series and\nmusic classification, where they show some of the most accurate results.\n  Typical kernel functions for sequences in these domains (e.g., bag-of-words,\nmismatch, or subsequence kernels) are restricted to {\\em discrete univariate}\n(i.e. one-dimensional) string data, such as sequences of words in the text\nanalysis, codeword sequences in the image analysis, or nucleotide or amino acid\nsequences in the DNA and protein sequence analysis. However, original sequence\ndata are often of real-valued multivariate nature, i.e. are not univariate and\ndiscrete as required by typical $k$-mer based sequence kernel functions.\n  In this work, we consider the problem of the {\\em multivariate} sequence\nclassification such as classification of multivariate music sequences, or\nmultidimensional protein sequence representations. To this end, we extend {\\em\nunivariate} kernel functions typically used in sequence analysis and propose\nefficient {\\em multivariate} similarity kernel method (MVDFQ-SK) based on (1) a\ndirect feature quantization (DFQ) of each sequence dimension in the original\n{\\em real-valued} multivariate sequences and (2) applying novel multivariate\ndiscrete kernel measures on these multivariate discrete DFQ sequence\nrepresentations to more accurately capture similarity relationships among\nsequences and improve classification performance.\n  Experiments using the proposed MVDFQ-SK kernel method show excellent\nclassification performance on three challenging music classification tasks as\nwell as protein sequence classification with significant 25-40% improvements\nover univariate kernel methods and existing state-of-the-art sequence\nclassification methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 18:03:22 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 14:46:42 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Kuksa", "Pavel P.", ""]]}, {"id": "1409.8276", "submitter": "Beyza Ermis Ms", "authors": "Beyza Ermis, A. Taylan Cemgil", "title": "A Bayesian Tensor Factorization Model via Variational Inference for Link\n  Prediction", "comments": "arXiv admin note: substantial text overlap with arXiv:1409.8083", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic approaches for tensor factorization aim to extract meaningful\nstructure from incomplete data by postulating low rank constraints. Recently,\nvariational Bayesian (VB) inference techniques have successfully been applied\nto large scale models. This paper presents full Bayesian inference via VB on\nboth single and coupled tensor factorization models. Our method can be run even\nfor very large models and is easily implemented. It exhibits better prediction\nperformance than existing approaches based on maximum likelihood on several\nreal-world datasets for missing link prediction problem.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 12:29:21 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Ermis", "Beyza", ""], ["Cemgil", "A. Taylan", ""]]}, {"id": "1409.8309", "submitter": "Youssef Hassan", "authors": "Youssef Hassan, Mohamed Aly and Amir Atiya", "title": "Arabic Spelling Correction using Supervised Learning", "comments": "System description paper that is submitted in the EMNLP 2014\n  conference shared task \"Automatic Arabic Error Correction\" (Mohit et al.,\n  2014) in the Arabic NLP workshop. 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of spelling correction in the Arabic\nlanguage utilizing the new corpus provided by QALB (Qatar Arabic Language Bank)\nproject which is an annotated corpus of sentences with errors and their\ncorrections. The corpus contains edit, add before, split, merge, add after,\nmove and other error types. We are concerned with the first four error types as\nthey contribute more than 90% of the spelling errors in the corpus. The\nproposed system has many models to address each error type on its own and then\nintegrating all the models to provide an efficient and robust system that\nachieves an overall recall of 0.59, precision of 0.58 and F1 score of 0.58\nincluding all the error types on the development set. Our system participated\nin the QALB 2014 shared task \"Automatic Arabic Error Correction\" and achieved\nan F1 score of 0.6, earning the sixth place out of nine participants.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 20:18:12 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Hassan", "Youssef", ""], ["Aly", "Mohamed", ""], ["Atiya", "Amir", ""]]}, {"id": "1409.8327", "submitter": "Alessandro Chiuso", "authors": "Giulia Prando and Alessandro Chiuso and Gianluigi Pillonetto", "title": "Bayesian and regularization approaches to multivariable linear system\n  identification: the role of rank penalties", "comments": "to appear in IEEE Conference on Decision and Control, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in linear system identification have proposed the use of\nnon-parameteric methods, relying on regularization strategies, to handle the\nso-called bias/variance trade-off. This paper introduces an impulse response\nestimator which relies on an $\\ell_2$-type regularization including a\nrank-penalty derived using the log-det heuristic as a smooth approximation to\nthe rank function. This allows to account for different properties of the\nestimated impulse response (e.g. smoothness and stability) while also\npenalizing high-complexity models. This also allows to account and enforce\ncoupling between different input-output channels in MIMO systems. According to\nthe Bayesian paradigm, the parameters defining the relative weight of the two\nregularization terms as well as the structure of the rank penalty are estimated\noptimizing the marginal likelihood. Once these hyperameters have been\nestimated, the impulse response estimate is available in closed form.\nExperiments show that the proposed method is superior to the estimator relying\non the \"classic\" $\\ell_2$-regularization alone as well as those based in atomic\nand nuclear norm.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 21:08:54 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Prando", "Giulia", ""], ["Chiuso", "Alessandro", ""], ["Pillonetto", "Gianluigi", ""]]}, {"id": "1409.8428", "submitter": "Ohad Shamir", "authors": "Noga Alon, Nicol\\`o Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay\n  Mansour and Ohad Shamir", "title": "Nonstochastic Multi-Armed Bandits with Graph-Structured Feedback", "comments": "Preliminary versions of parts of this paper appeared in [1,20], and\n  also as arXiv papers arXiv:1106.2436 and arXiv:1307.4564", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and study a partial-information model of online learning, where a\ndecision maker repeatedly chooses from a finite set of actions, and observes\nsome subset of the associated losses. This naturally models several situations\nwhere the losses of different actions are related, and knowing the loss of one\naction provides information on the loss of other actions. Moreover, it\ngeneralizes and interpolates between the well studied full-information setting\n(where all losses are revealed) and the bandit setting (where only the loss of\nthe action chosen by the player is revealed). We provide several algorithms\naddressing different variants of our setting, and provide tight regret bounds\ndepending on combinatorial properties of the information feedback structure.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 08:29:13 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Alon", "Noga", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""], ["Gentile", "Claudio", ""], ["Mannor", "Shie", ""], ["Mansour", "Yishay", ""], ["Shamir", "Ohad", ""]]}, {"id": "1409.8484", "submitter": "Christian Napoli", "authors": "Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana", "title": "An agent-driven semantical identifier using radial basis neural networks\n  and reinforcement learning", "comments": "Published on: Proceedings of the XV Workshop \"Dagli Oggetti agli\n  Agenti\" (WOA 2014), Catania, Italy, Sepember. 25-26, 2014", "journal-ref": "Proceedings of the XV Workshop \"Dagli Oggetti agli Agenti\" (WOA\n  2014), on CEUR-WS, volume 1260, ISSN: 1613-073, Catania, Italy, Sepember.\n  25-26, 2014. http://ceur-ws.org/Vol-1260/", "doi": "10.13140/2.1.1446.7843", "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the huge availability of documents in digital form, and the deception\npossibility raise bound to the essence of digital documents and the way they\nare spread, the authorship attribution problem has constantly increased its\nrelevance. Nowadays, authorship attribution,for both information retrieval and\nanalysis, has gained great importance in the context of security, trust and\ncopyright preservation. This work proposes an innovative multi-agent driven\nmachine learning technique that has been developed for authorship attribution.\nBy means of a preprocessing for word-grouping and time-period related analysis\nof the common lexicon, we determine a bias reference level for the recurrence\nfrequency of the words within analysed texts, and then train a Radial Basis\nNeural Networks (RBPNN)-based classifier to identify the correct author. The\nmain advantage of the proposed approach lies in the generality of the semantic\nanalysis, which can be applied to different contexts and lexical domains,\nwithout requiring any modification. Moreover, the proposed system is able to\nincorporate an external input, meant to tune the classifier, and then\nself-adjust by means of continuous learning reinforcement.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 11:10:23 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Napoli", "Christian", ""], ["Pappalardo", "Giuseppe", ""], ["Tramontana", "Emiliano", ""]]}, {"id": "1409.8498", "submitter": "Jacob Crandall", "authors": "Jacob W. Crandall", "title": "Non-myopic learning in repeated stochastic games", "comments": null, "journal-ref": "Robust Learning for Repeated Stochastic Games via Meta-Gaming,\n  Proceedings of the 24th International Joint Conference on Artificial\n  Intelligence (IJCAI), pp. 3416-3422, 2015", "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In repeated stochastic games (RSGs), an agent must quickly adapt to the\nbehavior of previously unknown associates, who may themselves be learning. This\nmachine-learning problem is particularly challenging due, in part, to the\npresence of multiple (even infinite) equilibria and inherently large strategy\nspaces. In this paper, we introduce a method to reduce the strategy space of\ntwo-player general-sum RSGs to a handful of expert strategies. This process,\ncalled Mega, effectually reduces an RSG to a bandit problem. We show that the\nresulting strategy space preserves several important properties of the original\nRSG, thus enabling a learner to produce robust strategies within a reasonably\nsmall number of interactions. To better establish strengths and weaknesses of\nthis approach, we empirically evaluate the resulting learning system against\nother algorithms in three different RSGs.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 11:46:29 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 15:30:04 GMT"}, {"version": "v3", "created": "Fri, 19 Jan 2018 13:54:51 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Crandall", "Jacob W.", ""]]}, {"id": "1409.8558", "submitter": "Prasanna Kumar Muthukumar", "authors": "Prasanna Kumar Muthukumar and Alan W. Black", "title": "A Deep Learning Approach to Data-driven Parameterizations for\n  Statistical Parametric Speech Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearly all Statistical Parametric Speech Synthesizers today use Mel Cepstral\ncoefficients as the vocal tract parameterization of the speech signal. Mel\nCepstral coefficients were never intended to work in a parametric speech\nsynthesis framework, but as yet, there has been little success in creating a\nbetter parameterization that is more suited to synthesis. In this paper, we use\ndeep learning algorithms to investigate a data-driven parameterization\ntechnique that is designed for the specific requirements of synthesis. We\ncreate an invertible, low-dimensional, noise-robust encoding of the Mel Log\nSpectrum by training a tapered Stacked Denoising Autoencoder (SDA). This SDA is\nthen unwrapped and used as the initialization for a Multi-Layer Perceptron\n(MLP). The MLP is fine-tuned by training it to reconstruct the input at the\noutput layer. This MLP is then split down the middle to form encoding and\ndecoding networks. These networks produce a parameterization of the Mel Log\nSpectrum that is intended to better fulfill the requirements of synthesis.\nResults are reported for experiments conducted using this resulting\nparameterization with the ClusterGen speech synthesizer.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 14:20:29 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Muthukumar", "Prasanna Kumar", ""], ["Black", "Alan W.", ""]]}, {"id": "1409.8572", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Freshness-Aware Thompson Sampling", "comments": "21st International Conference on Neural Information Processing. arXiv\n  admin note: text overlap with arXiv:1409.7729", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To follow the dynamicity of the user's content, researchers have recently\nstarted to model interactions between users and the Context-Aware Recommender\nSystems (CARS) as a bandit problem where the system needs to deal with\nexploration and exploitation dilemma. In this sense, we propose to study the\nfreshness of the user's content in CARS through the bandit problem. We\nintroduce in this paper an algorithm named Freshness-Aware Thompson Sampling\n(FA-TS) that manages the recommendation of fresh document according to the\nuser's risk of the situation. The intensive evaluation and the detailed\nanalysis of the experimental results reveals several important discoveries in\nthe exploration/exploitation (exr/exp) behaviour.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 17:17:52 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "1409.8576", "submitter": "Huseyin Ozkan", "authors": "Huseyin Ozkan, Ozgun S. Pelvan and Suleyman S. Kozat", "title": "Data Imputation through the Identification of Local Anomalies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a comprehensive and statistical framework in a model free\nsetting for a complete treatment of localized data corruptions due to severe\nnoise sources, e.g., an occluder in the case of a visual recording. Within this\nframework, we propose i) a novel algorithm to efficiently separate, i.e.,\ndetect and localize, possible corruptions from a given suspicious data instance\nand ii) a Maximum A Posteriori (MAP) estimator to impute the corrupted data. As\na generalization to Euclidean distance, we also propose a novel distance\nmeasure, which is based on the ranked deviations among the data attributes and\nempirically shown to be superior in separating the corruptions. Our algorithm\nfirst splits the suspicious instance into parts through a binary partitioning\ntree in the space of data attributes and iteratively tests those parts to\ndetect local anomalies using the nominal statistics extracted from an\nuncorrupted (clean) reference data set. Once each part is labeled as anomalous\nvs normal, the corresponding binary patterns over this tree that characterize\ncorruptions are identified and the affected attributes are imputed. Under a\ncertain conditional independency structure assumed for the binary patterns, we\nanalytically show that the false alarm rate of the introduced algorithm in\ndetecting the corruptions is independent of the data and can be directly set\nwithout any parameter tuning. The proposed framework is tested over several\nwell-known machine learning data sets with synthetically generated corruptions;\nand experimentally shown to produce remarkable improvements in terms of\nclassification purposes with strong corruption separation capabilities. Our\nexperiments also indicate that the proposed algorithms outperform the typical\napproaches and are robust to varying training phase conditions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 14:55:07 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Ozkan", "Huseyin", ""], ["Pelvan", "Ozgun S.", ""], ["Kozat", "Suleyman S.", ""]]}, {"id": "1409.8606", "submitter": "Shahin Shahrampour", "authors": "Shahin Shahrampour, Alexander Rakhlin, Ali Jadbabaie", "title": "Distributed Detection : Finite-time Analysis and Impact of Network\n  Topology", "comments": "29 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of distributed detection in multi-agent\nnetworks. Agents receive private signals about an unknown state of the world.\nThe underlying state is globally identifiable, yet informative signals may be\ndispersed throughout the network. Using an optimization-based framework, we\ndevelop an iterative local strategy for updating individual beliefs. In\ncontrast to the existing literature which focuses on asymptotic learning, we\nprovide a finite-time analysis. Furthermore, we introduce a Kullback-Leibler\ncost to compare the efficiency of the algorithm to its centralized counterpart.\nOur bounds on the cost are expressed in terms of network size, spectral gap,\ncentrality of each agent and relative entropy of agents' signal structures. A\nkey observation is that distributing more informative signals to central agents\nresults in a faster learning rate. Furthermore, optimizing the weights, we can\nspeed up learning by improving the spectral gap. We also quantify the effect of\nlink failures on learning speed in symmetric networks. We finally provide\nnumerical simulations which verify our theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 15:49:59 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Shahrampour", "Shahin", ""], ["Rakhlin", "Alexander", ""], ["Jadbabaie", "Ali", ""]]}]