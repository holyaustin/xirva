[{"id": "1811.00002", "submitter": "Rafael Valle", "authors": "Ryan Prenger, Rafael Valle, Bryan Catanzaro", "title": "WaveGlow: A Flow-based Generative Network for Speech Synthesis", "comments": "5 pages, 1 figure, 1 table, 13 equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose WaveGlow: a flow-based network capable of generating\nhigh quality speech from mel-spectrograms. WaveGlow combines insights from Glow\nand WaveNet in order to provide fast, efficient and high-quality audio\nsynthesis, without the need for auto-regression. WaveGlow is implemented using\nonly a single network, trained using only a single cost function: maximizing\nthe likelihood of the training data, which makes the training procedure simple\nand stable. Our PyTorch implementation produces audio samples at a rate of more\nthan 500 kHz on an NVIDIA V100 GPU. Mean Opinion Scores show that it delivers\naudio quality as good as the best publicly available WaveNet implementation.\nAll code will be made publicly available online.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 03:22:25 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Prenger", "Ryan", ""], ["Valle", "Rafael", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1811.00003", "submitter": "Bhalaji Nagarajan Mr", "authors": "Bhalaji Nagarajan, V Ramana Murthy Oruganti", "title": "Deep Net Features for Complex Emotion Recognition", "comments": "Conflict of interest", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates the influence of different acoustic features,\naudio-events based features and automatic speech translation based lexical\nfeatures in complex emotion recognition such as curiosity. Pretrained networks,\nnamely, AudioSet Net, VoxCeleb Net and Deep Speech Net trained extensively for\ndifferent speech based applications are studied for this objective. Information\nfrom deep layers of these networks are considered as descriptors and encoded\ninto feature vectors. Experimental results on the EmoReact dataset consisting\nof 8 complex emotions show the effectiveness, yielding highest F1 score of 0.85\nas against the baseline of 0.69 in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 04:52:18 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 04:46:09 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Nagarajan", "Bhalaji", ""], ["Oruganti", "V Ramana Murthy", ""]]}, {"id": "1811.00006", "submitter": "Kevin Kilgour", "authors": "David B. Ramsay, Kevin Kilgour, Dominik Roblek and Matthew Sharifi", "title": "Low-Dimensional Bottleneck Features for On-Device Continuous Speech\n  Recognition", "comments": "Submitted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low power digital signal processors (DSPs) typically have a very limited\namount of memory in which to cache data. In this paper we develop efficient\nbottleneck feature (BNF) extractors that can be run on a DSP, and retrain a\nbaseline large-vocabulary continuous speech recognition (LVCSR) system to use\nthese BNFs with only a minimal loss of accuracy. The small BNFs allow the DSP\nchip to cache more audio features while the main application processor is\nsuspended, thereby reducing the overall battery usage. Our presented system is\nable to reduce the footprint of standard, fixed point DSP spectral features by\na factor of 10 without any loss in word error rate (WER) and by a factor of 64\nwith only a 5.8% relative increase in WER.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 14:20:24 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Ramsay", "David B.", ""], ["Kilgour", "Kevin", ""], ["Roblek", "Dominik", ""], ["Sharifi", "Matthew", ""]]}, {"id": "1811.00007", "submitter": "Stefan Bauer", "authors": "Raphael Suter, {\\DJ}or{\\dj}e Miladinovi\\'c, Bernhard Sch\\\"olkopf,\n  Stefan Bauer", "title": "Robustly Disentangled Causal Mechanisms: Validating Deep Representations\n  for Interventional Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to learn disentangled representations that split underlying\nsources of variation in high dimensional, unstructured data is important for\ndata efficient and robust use of neural networks. While various approaches\naiming towards this goal have been proposed in recent times, a commonly\naccepted definition and validation procedure is missing. We provide a causal\nperspective on representation learning which covers disentanglement and domain\nshift robustness as special cases. Our causal framework allows us to introduce\na new metric for the quantitative evaluation of deep latent variable models. We\nshow how this metric can be estimated from labeled observational data and\nfurther provide an efficient estimation algorithm that scales linearly in the\ndataset size.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 14:42:01 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 18:29:03 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Suter", "Raphael", ""], ["Miladinovi\u0107", "\u0110or\u0111e", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Bauer", "Stefan", ""]]}, {"id": "1811.00052", "submitter": "Shrey Gadiya", "authors": "Shrey Gadiya, Deepak Anand and Amit Sethi", "title": "Some New Layer Architectures for Graph CNN", "comments": "5 pages, 1 figure, submitted to ICASSP 2019 Special Session on\n  Learning Methods in Complex and Hypercomplex Domains, Brighton, United\n  Kingdom, May 12-17, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While convolutional neural networks (CNNs) have recently made great strides\nin supervised classification of data structured on a grid (e.g. images composed\nof pixel grids), in several interesting datasets, the relations between\nfeatures can be better represented as a general graph instead of a regular\ngrid. Although recent algorithms that adapt CNNs to graphs have shown promising\nresults, they mostly neglect learning explicit operations for edge features\nwhile focusing on vertex features alone. We propose new formulations for\nconvolutional, pooling, and fully connected layers for neural networks that\nmake more comprehensive use of the information available in multi-dimensional\ngraphs. Using these layers led to an improvement in classification accuracy\nover the state-of-the-art methods on benchmark graph datasets.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 18:24:58 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Gadiya", "Shrey", ""], ["Anand", "Deepak", ""], ["Sethi", "Amit", ""]]}, {"id": "1811.00062", "submitter": "Niek Tax", "authors": "Niek Tax, Irene Teinemaa, Sebastiaan J. van Zelst", "title": "An Interdisciplinary Comparison of Sequence Modeling Methods for\n  Next-Element Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data of sequential nature arise in many application domains in forms of, e.g.\ntextual data, DNA sequences, and software execution traces. Different research\ndisciplines have developed methods to learn sequence models from such datasets:\n(i) in the machine learning field methods such as (hidden) Markov models and\nrecurrent neural networks have been developed and successfully applied to a\nwide-range of tasks, (ii) in process mining process discovery techniques aim to\ngenerate human-interpretable descriptive models, and (iii) in the grammar\ninference field the focus is on finding descriptive models in the form of\nformal grammars. Despite their different focuses, these fields share a common\ngoal - learning a model that accurately describes the behavior in the\nunderlying data. Those sequence models are generative, i.e, they can predict\nwhat elements are likely to occur after a given unfinished sequence. So far,\nthese fields have developed mainly in isolation from each other and no\ncomparison exists. This paper presents an interdisciplinary experimental\nevaluation that compares sequence modeling techniques on the task of\nnext-element prediction on four real-life sequence datasets. The results\nindicate that machine learning techniques that generally have no aim at\ninterpretability in terms of accuracy outperform techniques from the process\nmining and grammar inference fields that aim to yield interpretable models.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 18:54:27 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Tax", "Niek", ""], ["Teinemaa", "Irene", ""], ["van Zelst", "Sebastiaan J.", ""]]}, {"id": "1811.00073", "submitter": "Prashnna Gyawali", "authors": "Prashnna K Gyawali, Cameron Knight, Sandesh Ghimire, B. Milan Horacek,\n  John L. Sapp and Linwei Wang", "title": "Deep Generative Model with Beta Bernoulli Process for Modeling and\n  Learning Confounding Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While deep representation learning has become increasingly capable of\nseparating task-relevant representations from other confounding factors in the\ndata, two significant challenges remain. First, there is often an unknown and\npotentially infinite number of confounding factors coinciding in the data.\nSecond, not all of these factors are readily observable. In this paper, we\npresent a deep conditional generative model that learns to disentangle a\ntask-relevant representation from an unknown number of confounding factors that\nmay grow infinitely. This is achieved by marrying the representational power of\ndeep generative models with Bayesian non-parametric factor models, where a\nsupervised deterministic encoder learns task-related representation and a\nprobabilistic encoder with an Indian Buffet Process (IBP) learns the unknown\nnumber of unobservable confounding factors. We tested the presented model in\ntwo datasets: a handwritten digit dataset (MNIST) augmented with colored digits\nand a clinical ECG dataset with significant inter-subject variations and\naugmented with signal artifacts. These diverse data sets highlighted the\nability of the presented model to grow with the complexity of the data and\nidentify the absence or presence of unobserved confounding factors.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 19:19:49 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 20:02:28 GMT"}, {"version": "v3", "created": "Sun, 5 May 2019 20:36:21 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Gyawali", "Prashnna K", ""], ["Knight", "Cameron", ""], ["Ghimire", "Sandesh", ""], ["Horacek", "B. Milan", ""], ["Sapp", "John L.", ""], ["Wang", "Linwei", ""]]}, {"id": "1811.00075", "submitter": "Anthony Bagnall Dr", "authors": "Anthony Bagnall and Hoang Anh Dau and Jason Lines and Michael Flynn\n  and James Large and Aaron Bostrom and Paul Southam and Eamonn Keogh", "title": "The UEA multivariate time series classification archive, 2018", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2002, the UCR time series classification archive was first released with\nsixteen datasets. It gradually expanded, until 2015 when it increased in size\nfrom 45 datasets to 85 datasets. In October 2018 more datasets were added,\nbringing the total to 128. The new archive contains a wide range of problems,\nincluding variable length series, but it still only contains univariate time\nseries classification problems. One of the motivations for introducing the\narchive was to encourage researchers to perform a more rigorous evaluation of\nnewly proposed time series classification (TSC) algorithms. It has worked: most\nrecent research into TSC uses all 85 datasets to evaluate algorithmic advances.\nResearch into multivariate time series classification, where more than one\nseries are associated with each class label, is in a position where univariate\nTSC research was a decade ago. Algorithms are evaluated using very few datasets\nand claims of improvement are not based on statistical comparisons. We aim to\naddress this problem by forming the first iteration of the MTSC archive, to be\nhosted at the website www.timeseriesclassification.com. Like the univariate\narchive, this formulation was a collaborative effort between researchers at the\nUniversity of East Anglia (UEA) and the University of California, Riverside\n(UCR). The 2018 vintage consists of 30 datasets with a wide range of cases,\ndimensions and series lengths. For this first iteration of the archive we\nformat all data to be of equal length, include no series with missing data and\nprovide train/test splits.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 19:24:20 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Bagnall", "Anthony", ""], ["Dau", "Hoang Anh", ""], ["Lines", "Jason", ""], ["Flynn", "Michael", ""], ["Large", "James", ""], ["Bostrom", "Aaron", ""], ["Southam", "Paul", ""], ["Keogh", "Eamonn", ""]]}, {"id": "1811.00102", "submitter": "Mayank Baranwal", "authors": "Amber Srivastava, Mayank Baranwal and Srinivasa Salapaka", "title": "On the Persistence of Clustering Solutions and True Number of Clusters\n  in a Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically clustering algorithms provide clustering solutions with\nprespecified number of clusters. The lack of a priori knowledge on the true\nnumber of underlying clusters in the dataset makes it important to have a\nmetric to compare the clustering solutions with different number of clusters.\nThis article quantifies a notion of persistence of clustering solutions that\nenables comparing solutions with different number of clusters. The persistence\nrelates to the range of data-resolution scales over which a clustering solution\npersists; it is quantified in terms of the maximum over two-norms of all the\nassociated cluster-covariance matrices. Thus we associate a persistence value\nfor each element in a set of clustering solutions with different number of\nclusters. We show that the datasets where natural clusters are a priori known,\nthe clustering solutions that identify the natural clusters are most persistent\n- in this way, this notion can be used to identify solutions with true number\nof clusters. Detailed experiments on a variety of standard and synthetic\ndatasets demonstrate that the proposed persistence-based indicator outperforms\nthe existing approaches, such as, gap-statistic method, $X$-means, $G$-means,\n$PG$-means, dip-means algorithms and information-theoretic method, in\naccurately identifying the clustering solutions with true number of clusters.\nInterestingly, our method can be explained in terms of the phase-transition\nphenomenon in the deterministic annealing algorithm, where the number of\ndistinct cluster centers changes (bifurcates) with respect to an annealing\nparameter.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 20:27:16 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 20:45:25 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Srivastava", "Amber", ""], ["Baranwal", "Mayank", ""], ["Salapaka", "Srinivasa", ""]]}, {"id": "1811.00103", "submitter": "Samira Samadi", "authors": "Samira Samadi, Uthaipon Tantipongpipat, Jamie Morgenstern, Mohit\n  Singh, Santosh Vempala", "title": "The Price of Fair PCA: One Extra Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate whether the standard dimensionality reduction technique of PCA\ninadvertently produces data representations with different fidelity for two\ndifferent populations. We show on several real-world data sets, PCA has higher\nreconstruction error on population A than on B (for example, women versus men\nor lower- versus higher-educated individuals). This can happen even when the\ndata set has a similar number of samples from A and B. This motivates our study\nof dimensionality reduction techniques which maintain similar fidelity for A\nand B. We define the notion of Fair PCA and give a polynomial-time algorithm\nfor finding a low dimensional representation of the data which is\nnearly-optimal with respect to this measure. Finally, we show on real-world\ndata sets that our algorithm can be used to efficiently generate a fair low\ndimensional representation of the data.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 20:32:00 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Samadi", "Samira", ""], ["Tantipongpipat", "Uthaipon", ""], ["Morgenstern", "Jamie", ""], ["Singh", "Mohit", ""], ["Vempala", "Santosh", ""]]}, {"id": "1811.00112", "submitter": "Daniel S\\'aez Trigueros", "authors": "Daniel S\\'aez Trigueros, Li Meng, Margaret Hartnett", "title": "Generating Photo-Realistic Training Data to Improve Face Recognition\n  Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the feasibility of using synthetic data to\naugment face datasets. In particular, we propose a novel generative adversarial\nnetwork (GAN) that can disentangle identity-related attributes from\nnon-identity-related attributes. This is done by training an embedding network\nthat maps discrete identity labels to an identity latent space that follows a\nsimple prior distribution, and training a GAN conditioned on samples from that\ndistribution. Our proposed GAN allows us to augment face datasets by generating\nboth synthetic images of subjects in the training set and synthetic images of\nnew subjects not in the training set. By using recent advances in GAN training,\nwe show that the synthetic images generated by our model are photo-realistic,\nand that training with augmented datasets can indeed increase the accuracy of\nface recognition models as compared with models trained with real images alone.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 20:53:25 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Trigueros", "Daniel S\u00e1ez", ""], ["Meng", "Li", ""], ["Hartnett", "Margaret", ""]]}, {"id": "1811.00115", "submitter": "Yik Chau Lui", "authors": "Kry Yik Chau Lui, Gavin Weiguang Ding, Ruitong Huang, Robert J. McCann", "title": "Dimensionality Reduction has Quantifiable Imperfections: Two Geometric\n  Bounds", "comments": "32nd Conference on Neural Information Processing Systems (NIPS 2018),\n  Montreal, Canada", "journal-ref": "Neural Information Processing Systems (NIPS 2018)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate Dimensionality reduction (DR) maps in an\ninformation retrieval setting from a quantitative topology point of view. In\nparticular, we show that no DR maps can achieve perfect precision and perfect\nrecall simultaneously. Thus a continuous DR map must have imperfect precision.\nWe further prove an upper bound on the precision of Lipschitz continuous DR\nmaps. While precision is a natural measure in an information retrieval setting,\nit does not measure `how' wrong the retrieved data is. We therefore propose a\nnew measure based on Wasserstein distance that comes with similar theoretical\nguarantee. A key technical step in our proofs is a particular optimization\nproblem of the $L_2$-Wasserstein distance over a constrained set of\ndistributions. We provide a complete solution to this optimization problem,\nwhich can be of independent interest on the technical side.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 20:56:14 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Lui", "Kry Yik Chau", ""], ["Ding", "Gavin Weiguang", ""], ["Huang", "Ruitong", ""], ["McCann", "Robert J.", ""]]}, {"id": "1811.00121", "submitter": "George Kesidis", "authors": "David J. Miller, Xinyi Hu, Zhen Xiang, and George Kesidis", "title": "A Mixture Model Based Defense for Data Poisoning Attacks Against Naive\n  Bayes Spam Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naive Bayes spam filters are highly susceptible to data poisoning attacks.\nHere, known spam sources/blacklisted IPs exploit the fact that their received\nemails will be treated as (ground truth) labeled spam examples, and used for\nclassifier training (or re-training). The attacking source thus generates\nemails that will skew the spam model, potentially resulting in great\ndegradation in classifier accuracy. Such attacks are successful mainly because\nof the poor representation power of the naive Bayes (NB) model, with only a\nsingle (component) density to represent spam (plus a possible attack). We\npropose a defense based on the use of a mixture of NB models. We demonstrate\nthat the learned mixture almost completely isolates the attack in a second NB\ncomponent, with the original spam component essentially unchanged by the\nattack. Our approach addresses both the scenario where the classifier is being\nre-trained in light of new data and, significantly, the more challenging\nscenario where the attack is embedded in the original spam training set. Even\nfor weak attack strengths, BIC-based model order selection chooses a\ntwo-component solution, which invokes the mixture-based defense. Promising\nresults are presented on the TREC 2005 spam corpus.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 21:04:43 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Miller", "David J.", ""], ["Hu", "Xinyi", ""], ["Xiang", "Zhen", ""], ["Kesidis", "George", ""]]}, {"id": "1811.00128", "submitter": "Kavosh Asadi", "authors": "Kavosh Asadi, Evan Cater, Dipendra Misra, Michael L. Littman", "title": "Towards a Simple Approach to Multi-step Model-based Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When environmental interaction is expensive, model-based reinforcement\nlearning offers a solution by planning ahead and avoiding costly mistakes.\nModel-based agents typically learn a single-step transition model. In this\npaper, we propose a multi-step model that predicts the outcome of an action\nsequence with variable length. We show that this model is easy to learn, and\nthat the model can make policy-conditional predictions. We report preliminary\nresults that show a clear advantage for the multi-step model compared to its\none-step counterpart.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 21:31:59 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Asadi", "Kavosh", ""], ["Cater", "Evan", ""], ["Misra", "Dipendra", ""], ["Littman", "Michael L.", ""]]}, {"id": "1811.00135", "submitter": "Yijun Xiao", "authors": "Yijun Xiao, Tiancheng Zhao, William Yang Wang", "title": "Dirichlet Variational Autoencoder for Text Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an improved variational autoencoder (VAE) for text modeling with\ntopic information explicitly modeled as a Dirichlet latent variable. By\nproviding the proposed model topic awareness, it is more superior at\nreconstructing input texts. Furthermore, due to the inherent interactions\nbetween the newly introduced Dirichlet variable and the conventional\nmultivariate Gaussian variable, the model is less prone to KL divergence\nvanishing. We derive the variational lower bound for the new model and conduct\nexperiments on four different data sets. The results show that the proposed\nmodel is superior at text reconstruction across the latent space and\nclassifications on learned representations have higher test accuracies.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 22:04:22 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Xiao", "Yijun", ""], ["Zhao", "Tiancheng", ""], ["Wang", "William Yang", ""]]}, {"id": "1811.00139", "submitter": "Nathaniel Harms", "authors": "Nathaniel Harms", "title": "Testing Halfspaces over Rotation-Invariant Distributions", "comments": "36 pages, 2 figures, to appear in SODA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for testing halfspaces over arbitrary, unknown\nrotation-invariant distributions. Using $\\tilde O(\\sqrt{n}\\epsilon^{-7})$\nrandom examples of an unknown function $f$, the algorithm determines with high\nprobability whether $f$ is of the form $f(x) = sign(\\sum_i w_ix_i-t)$ or is\n$\\epsilon$-far from all such functions. This sample size is significantly\nsmaller than the well-known requirement of $\\Omega(n)$ samples for learning\nhalfspaces, and known lower bounds imply that our sample size is optimal (in\nits dependence on $n$) up to logarithmic factors. The algorithm is\ndistribution-free in the sense that it requires no knowledge of the\ndistribution aside from the promise of rotation invariance. To prove the\ncorrectness of this algorithm we present a theorem relating the distance\nbetween a function and a halfspace to the distance between their centers of\nmass, that applies to arbitrary distributions.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 22:25:27 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Harms", "Nathaniel", ""]]}, {"id": "1811.00143", "submitter": "Minghuang Ma", "authors": "Minghuang Ma, Hadi Pouransari, Daniel Chao, Saurabh Adya, Santiago\n  Akle Serrano, Yi Qin, Dan Gimnicher, Dominic Walsh", "title": "Democratizing Production-Scale Distributed Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interest and demand for training deep neural networks have been\nexperiencing rapid growth, spanning a wide range of applications in both\nacademia and industry. However, training them distributed and at scale remains\ndifficult due to the complex ecosystem of tools and hardware involved. One\nconsequence is that the responsibility of orchestrating these complex\ncomponents is often left to one-off scripts and glue code customized for\nspecific problems. To address these restrictions, we introduce \\emph{Alchemist}\n- an internal service built at Apple from the ground up for \\emph{easy},\n\\emph{fast}, and \\emph{scalable} distributed training. We discuss its design,\nimplementation, and examples of running different flavors of distributed\ntraining. We also present case studies of its internal adoption in the\ndevelopment of autonomous systems, where training times have been reduced by\n10x to keep up with the ever-growing data collection.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 22:39:59 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 05:47:41 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Ma", "Minghuang", ""], ["Pouransari", "Hadi", ""], ["Chao", "Daniel", ""], ["Adya", "Saurabh", ""], ["Serrano", "Santiago Akle", ""], ["Qin", "Yi", ""], ["Gimnicher", "Dan", ""], ["Walsh", "Dominic", ""]]}, {"id": "1811.00145", "submitter": "Aman Sinha", "authors": "Matthew O'Kelly, Aman Sinha, Hongseok Namkoong, John Duchi, Russ\n  Tedrake", "title": "Scalable End-to-End Autonomous Vehicle Testing via Rare-event Simulation", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent developments in autonomous vehicle (AV) technology highlight\nsubstantial progress, we lack tools for rigorous and scalable testing.\nReal-world testing, the $\\textit{de facto}$ evaluation environment, places the\npublic in danger, and, due to the rare nature of accidents, will require\nbillions of miles in order to statistically validate performance claims. We\nimplement a simulation framework that can test an entire modern autonomous\ndriving system, including, in particular, systems that employ deep-learning\nperception and control algorithms. Using adaptive importance-sampling methods\nto accelerate rare-event probability evaluation, we estimate the probability of\nan accident under a base distribution governing standard traffic behavior. We\ndemonstrate our framework on a highway scenario, accelerating system evaluation\nby $2$-$20$ times over naive Monte Carlo sampling methods and $10$-$300\n\\mathsf{P}$ times (where $\\mathsf{P}$ is the number of processors) over\nreal-world testing.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 22:47:22 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 05:32:44 GMT"}, {"version": "v3", "created": "Sat, 12 Jan 2019 19:27:45 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["O'Kelly", "Matthew", ""], ["Sinha", "Aman", ""], ["Namkoong", "Hongseok", ""], ["Duchi", "John", ""], ["Tedrake", "Russ", ""]]}, {"id": "1811.00147", "submitter": "Haoyu Wang", "authors": "Haoyu Wang, Vivek Kulkarni, William Yang Wang", "title": "DOLORES: Deep Contextualized Knowledge Graph Embeddings", "comments": "10 pages, 6 figures", "journal-ref": "Automated Knowledge Base Construction (AKBC), 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method DOLORES for learning knowledge graph embeddings\nthat effectively captures contextual cues and dependencies among entities and\nrelations. First, we note that short paths on knowledge graphs comprising of\nchains of entities and relations can encode valuable information regarding\ntheir contextual usage. We operationalize this notion by representing knowledge\ngraphs not as a collection of triples but as a collection of entity-relation\nchains, and learn embeddings for entities and relations using deep neural\nmodels that capture such contextual usage. In particular, our model is based on\nBi-Directional LSTMs and learn deep representations of entities and relations\nfrom constructed entity-relation chains. We show that these representations can\nvery easily be incorporated into existing models to significantly advance the\nstate of the art on several knowledge graph prediction tasks like link\nprediction, triple classification, and missing relation type prediction (in\nsome cases by at least 9.5%).\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 22:59:57 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Wang", "Haoyu", ""], ["Kulkarni", "Vivek", ""], ["Wang", "William Yang", ""]]}, {"id": "1811.00148", "submitter": "Hongyang Zhang", "authors": "Hongyang Zhang, Vatsal Sharan, Moses Charikar, Yingyu Liang", "title": "Recovery Guarantees for Quadratic Tensors with Limited Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the tensor completion problem of predicting the missing entries\nof a tensor. The commonly used CP model has a triple product form, but an\nalternate family of quadratic models which are the sum of pairwise products\ninstead of a triple product have emerged from applications such as\nrecommendation systems. Non-convex methods are the method of choice for\nlearning quadratic models, and this work examines their sample complexity and\nerror guarantee. Our main result is that with the number of samples being only\nlinear in the dimension, all local minima of the mean squared error objective\nare global minima and recover the original tensor accurately. The techniques\nlead to simple proofs showing that convex relaxation can recover quadratic\ntensors provided with linear number of samples. We substantiate our theoretical\nresults with experiments on synthetic and real-world data, showing that\nquadratic models have better performance than CP models in scenarios where\nthere are limited amount of observations available.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 23:05:22 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Zhang", "Hongyang", ""], ["Sharan", "Vatsal", ""], ["Charikar", "Moses", ""], ["Liang", "Yingyu", ""]]}, {"id": "1811.00152", "submitter": "Hamid Eghbal-zadeh", "authors": "Hamid Eghbal-zadeh, Werner Zellinger, Gerhard Widmer", "title": "Mixture Density Generative Adversarial Networks", "comments": "Accepted at the third workshop on Bayesian Deep Learning (NeurIPS\n  2018), Montr\\'eal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks have surprising ability for generating sharp\nand realistic images, though they are known to suffer from the so-called mode\ncollapse problem. In this paper, we propose a new GAN variant called Mixture\nDensity GAN that while being capable of generating high-quality images,\novercomes this problem by encouraging the Discriminator to form clusters in its\nembedding space, which in turn leads the Generator to exploit these and\ndiscover different modes in the data. This is achieved by positioning Gaussian\ndensity functions in the corners of a simplex, using the resulting Gaussian\nmixture as a likelihood function over discriminator embeddings, and formulating\nan objective function for GAN training that is based on these likelihoods. We\ndemonstrate empirically (1) the quality of the generated images in Mixture\nDensity GAN and their strong similarity to real images, as measured by the\nFr\\'echet Inception Distance (FID), which compares very favourably with\nstate-of-the-art methods, and (2) the ability to avoid mode collapse and\ndiscover all data modes.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 23:21:21 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 16:50:02 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Eghbal-zadeh", "Hamid", ""], ["Zellinger", "Werner", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1811.00155", "submitter": "Avner May", "authors": "Jian Zhang, Avner May, Tri Dao, Christopher R\\'e", "title": "Low-Precision Random Fourier Features for Memory-Constrained Kernel\n  Approximation", "comments": "International Conference on Artificial Intelligence and Statistics\n  (AISTATS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate how to train kernel approximation methods that generalize well\nunder a memory budget. Building on recent theoretical work, we define a measure\nof kernel approximation error which we find to be more predictive of the\nempirical generalization performance of kernel approximation methods than\nconventional metrics. An important consequence of this definition is that a\nkernel approximation matrix must be high rank to attain close approximation.\nBecause storing a high-rank approximation is memory intensive, we propose using\na low-precision quantization of random Fourier features (LP-RFFs) to build a\nhigh-rank approximation under a memory budget. Theoretically, we show\nquantization has a negligible effect on generalization performance in important\nsettings. Empirically, we demonstrate across four benchmark datasets that\nLP-RFFs can match the performance of full-precision RFFs and the Nystr\\\"{o}m\nmethod, with 3x-10x and 50x-460x less memory, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 23:24:51 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 05:50:12 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Zhang", "Jian", ""], ["May", "Avner", ""], ["Dao", "Tri", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1811.00159", "submitter": "Gaurush Hiranandani", "authors": "Gaurush Hiranandani, Raghav Somani, Oluwasanmi Koyejo, Sreangsu\n  Acharyya", "title": "Clustered Monotone Transforms for Rating Factorization", "comments": "The first two authors contributed equally to the paper. The paper to\n  appear in WSDM 2019", "journal-ref": null, "doi": "10.1145/3289600.3291005", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting low-rank structure of the user-item rating matrix has been the\ncrux of many recommendation engines. However, existing recommendation engines\nforce raters with heterogeneous behavior profiles to map their intrinsic rating\nscales to a common rating scale (e.g. 1-5). This non-linear transformation of\nthe rating scale shatters the low-rank structure of the rating matrix,\ntherefore resulting in a poor fit and consequentially, poor recommendations. In\nthis paper, we propose Clustered Monotone Transforms for Rating Factorization\n(CMTRF), a novel approach to perform regression up to unknown monotonic\ntransforms over unknown population segments. Essentially, for recommendation\nsystems, the technique searches for monotonic transformations of the rating\nscales resulting in a better fit. This is combined with an underlying matrix\nfactorization regression model that couples the user-wise ratings to exploit\nshared low dimensional structure. The rating scale transformations can be\ngenerated for each user, for a cluster of users, or for all the users at once,\nforming the basis of three simple and efficient algorithms proposed in this\npaper, all of which alternate between transformation of the rating scales and\nmatrix factorization regression. Despite the non-convexity, CMTRF is\ntheoretically shown to recover a unique solution under mild conditions.\nExperimental results on two synthetic and seven real-world datasets show that\nCMTRF outperforms other state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 23:53:24 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Hiranandani", "Gaurush", ""], ["Somani", "Raghav", ""], ["Koyejo", "Oluwasanmi", ""], ["Acharyya", "Sreangsu", ""]]}, {"id": "1811.00162", "submitter": "Shang-Yu Su", "authors": "Yu-An Wang, Yu-Kai Huang, Tzu-Chuan Lin, Shang-Yu Su, Yun-Nung Chen", "title": "Modeling Melodic Feature Dependency with Modularized Variational\n  Auto-Encoder", "comments": "The first three authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic melody generation has been a long-time aspiration for both AI\nresearchers and musicians. However, learning to generate euphonious melodies\nhas turned out to be highly challenging. This paper introduces 1) a new variant\nof variational autoencoder (VAE), where the model structure is designed in a\nmodularized manner in order to model polyphonic and dynamic music with domain\nknowledge, and 2) a hierarchical encoding/decoding strategy, which explicitly\nmodels the dependency between melodic features. The proposed framework is\ncapable of generating distinct melodies that sounds natural, and the\nexperiments for evaluating generated music clips show that the proposed model\noutperforms the baselines in human evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 23:59:04 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Wang", "Yu-An", ""], ["Huang", "Yu-Kai", ""], ["Lin", "Tzu-Chuan", ""], ["Su", "Shang-Yu", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "1811.00164", "submitter": "Noam Brown", "authors": "Noam Brown, Adam Lerer, Sam Gross, Tuomas Sandholm", "title": "Deep Counterfactual Regret Minimization", "comments": null, "journal-ref": "International Conference on Machine Learning (ICML), 2019", "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual Regret Minimization (CFR) is the leading framework for solving\nlarge imperfect-information games. It converges to an equilibrium by\niteratively traversing the game tree. In order to deal with extremely large\ngames, abstraction is typically applied before running CFR. The abstracted game\nis solved with tabular CFR, and its solution is mapped back to the full game.\nThis process can be problematic because aspects of abstraction are often manual\nand domain specific, abstraction algorithms may miss important strategic\nnuances of the game, and there is a chicken-and-egg problem because determining\na good abstraction requires knowledge of the equilibrium of the game. This\npaper introduces Deep Counterfactual Regret Minimization, a form of CFR that\nobviates the need for abstraction by instead using deep neural networks to\napproximate the behavior of CFR in the full game. We show that Deep CFR is\nprincipled and achieves strong performance in large poker games. This is the\nfirst non-tabular variant of CFR to be successful in large games.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 00:07:02 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 18:44:39 GMT"}, {"version": "v3", "created": "Wed, 22 May 2019 17:53:39 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Brown", "Noam", ""], ["Lerer", "Adam", ""], ["Gross", "Sam", ""], ["Sandholm", "Tuomas", ""]]}, {"id": "1811.00170", "submitter": "Charalampos Patrikakis", "authors": "Panagiotis Kasnesis, Charalampos Z. Patrikakis, Iakovos S. Venieris", "title": "PerceptionNet: A Deep Convolutional Neural Network for Late Sensor\n  Fusion", "comments": "This article has been accepted for publication in the proceedings of\n  Intelligent Systems Conference (IntelliSys) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Activity Recognition (HAR) based on motion sensors has drawn a lot of\nattention over the last few years, since perceiving the human status enables\ncontext-aware applications to adapt their services on users' needs. However,\nmotion sensor fusion and feature extraction have not reached their full\npotentials, remaining still an open issue. In this paper, we introduce\nPerceptionNet, a deep Convolutional Neural Network (CNN) that applies a late 2D\nconvolution to multimodal time-series sensor data, in order to extract\nautomatically efficient features for HAR. We evaluate our approach on two\npublic available HAR datasets to demonstrate that the proposed model fuses\neffectively multimodal sensors and improves the performance of HAR. In\nparticular, PerceptionNet surpasses the performance of state-of-the-art HAR\nmethods based on: (i) features extracted from humans, (ii) deep CNNs exploiting\nearly fusion approaches, and (iii) Long Short-Term Memory (LSTM), by an average\naccuracy of more than 3%.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 00:29:16 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Kasnesis", "Panagiotis", ""], ["Patrikakis", "Charalampos Z.", ""], ["Venieris", "Iakovos S.", ""]]}, {"id": "1811.00178", "submitter": "Anuj Sharma Dr", "authors": "Charanjeet, Anuj Sharma", "title": "Online learning using multiple times weight updating", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning makes sequence of decisions with partial data arrival where\nnext movement of data is unknown. In this paper, we have presented a new\ntechnique as multiple times weight updating that update the weight iteratively\nforsame instance. The proposed technique analyzed with popular state-of-art\nalgorithms from literature and experimented using established tool. The results\nindicates that mistake rate reduces to zero or close to zero for various\ndatasets and algorithms. The overhead running cost is not too expensive and\nachieving mistake rate close to zero further strengthen the proposed technique.\nThe present work include bound nature of weight updating for single instance\nand achieve optimal weight value. This proposed work could be extended to big\ndatasets problems to reduce mistake rate in online learning environment. Also,\nthe proposed technique could be helpful to meet real life challenges.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 12:02:46 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 12:49:17 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Charanjeet", "", ""], ["Sharma", "Anuj", ""]]}, {"id": "1811.00181", "submitter": "Jayaraman J. Thiagarajan", "authors": "Uday Shankar Shanthamallu, Jayaraman J. Thiagarajan and Andreas\n  Spanias", "title": "A Regularized Attention Mechanism for Graph Attention Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models that can exploit the inherent structure in data have\ngained prominence. In particular, there is a surge in deep learning solutions\nfor graph-structured data, due to its wide-spread applicability in several\nfields. Graph attention networks (GAT), a recent addition to the broad class of\nfeature learning models in graphs, utilizes the attention mechanism to\nefficiently learn continuous vector representations for semi-supervised\nlearning problems. In this paper, we perform a detailed analysis of GAT models,\nand present interesting insights into their behavior. In particular, we show\nthat the models are vulnerable to heterogeneous rogue nodes and hence propose\nnovel regularization strategies to improve the robustness of GAT models. Using\nbenchmark datasets, we demonstrate performance improvements on semi-supervised\nlearning, using the proposed robust variant of GAT.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 01:45:31 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 21:57:24 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Shanthamallu", "Uday Shankar", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Spanias", "Andreas", ""]]}, {"id": "1811.00183", "submitter": "Jayaraman J. Thiagarajan", "authors": "Vivek Sivaraman Narayanaswamy, Jayaraman J. Thiagarajan, Huan Song and\n  Andreas Spanias", "title": "Designing an Effective Metric Learning Pipeline for Speaker Diarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art speaker diarization systems utilize knowledge from external\ndata, in the form of a pre-trained distance metric, to effectively determine\nrelative speaker identities to unseen data. However, much of recent focus has\nbeen on choosing the appropriate feature extractor, ranging from pre-trained\n$i-$vectors to representations learned via different sequence modeling\narchitectures (e.g. 1D-CNNs, LSTMs, attention models), while adopting\noff-the-shelf metric learning solutions. In this paper, we argue that,\nregardless of the feature extractor, it is crucial to carefully design a metric\nlearning pipeline, namely the loss function, the sampling strategy and the\ndiscrimnative margin parameter, for building robust diarization systems.\nFurthermore, we propose to adopt a fine-grained validation process to obtain a\ncomprehensive evaluation of the generalization power of metric learning\npipelines. To this end, we measure diarization performance across different\nlanguage speakers, and variations in the number of speakers in a recording.\nUsing empirical studies, we provide interesting insights into the effectiveness\nof different design choices and make recommendations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 01:51:17 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Narayanaswamy", "Vivek Sivaraman", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Song", "Huan", ""], ["Spanias", "Andreas", ""]]}, {"id": "1811.00189", "submitter": "Jiayang Liu", "authors": "Jiayang Liu, Dongdong Hou, Weiming Zhang and Nenghai Yu", "title": "Reversible Adversarial Examples", "comments": "arXiv admin note: text overlap with arXiv:1806.09186", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks have recently led to significant improvement in many\nfields such as image classification and speech recognition. However, these\nmachine learning models are vulnerable to adversarial examples which can\nmislead machine learning classifiers to give incorrect classifications. In this\npaper, we take advantage of reversible data hiding to construct reversible\nadversarial examples which are still misclassified by Deep Neural Networks.\nFurthermore, the proposed method can recover original images from reversible\nadversarial examples with no distortion.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 02:28:31 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 14:30:54 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Liu", "Jiayang", ""], ["Hou", "Dongdong", ""], ["Zhang", "Weiming", ""], ["Yu", "Nenghai", ""]]}, {"id": "1811.00196", "submitter": "Qingyu Yin", "authors": "Hui Liu, Qingyu Yin, William Yang Wang", "title": "Towards Explainable NLP: A Generative Explanation Framework for Text\n  Classification", "comments": "Accepted to ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building explainable systems is a critical problem in the field of Natural\nLanguage Processing (NLP), since most machine learning models provide no\nexplanations for the predictions. Existing approaches for explainable machine\nlearning systems tend to focus on interpreting the outputs or the connections\nbetween inputs and outputs. However, the fine-grained information is often\nignored, and the systems do not explicitly generate the human-readable\nexplanations. To better alleviate this problem, we propose a novel generative\nexplanation framework that learns to make classification decisions and generate\nfine-grained explanations at the same time. More specifically, we introduce the\nexplainable factor and the minimum risk training approach that learn to\ngenerate more reasonable explanations. We construct two new datasets that\ncontain summaries, rating scores, and fine-grained reasons. We conduct\nexperiments on both datasets, comparing with several strong neural network\nbaseline systems. Experimental results show that our method surpasses all\nbaselines on both datasets, and is able to generate concise explanations at the\nsame time.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 02:45:57 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 13:12:58 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Liu", "Hui", ""], ["Yin", "Qingyu", ""], ["Wang", "William Yang", ""]]}, {"id": "1811.00198", "submitter": "Yu Hao", "authors": "Hao Yu, Vivek Kulkarni, William Wang", "title": "MOHONE: Modeling Higher Order Network Effects in KnowledgeGraphs via\n  Network Infused Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many knowledge graph embedding methods operate on triples and are therefore\nimplicitly limited by a very local view of the entire knowledge graph. We\npresent a new framework MOHONE to effectively model higher order network\neffects in knowledge-graphs, thus enabling one to capture varying degrees of\nnetwork connectivity (from the local to the global). Our framework is generic,\nexplicitly models the network scale, and captures two different aspects of\nsimilarity in networks: (a) shared local neighborhood and (b) structural\nrole-based similarity. First, we introduce methods that learn network\nrepresentations of entities in the knowledge graph capturing these varied\naspects of similarity. We then propose a fast, efficient method to incorporate\nthe information captured by these network representations into existing\nknowledge graph embeddings. We show that our method consistently and\nsignificantly improves the performance on link prediction of several different\nknowledge-graph embedding methods including TRANSE, TRANSD, DISTMULT, and\nCOMPLEX(by at least 4 points or 17% in some cases).\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 03:04:09 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Yu", "Hao", ""], ["Kulkarni", "Vivek", ""], ["Wang", "William", ""]]}, {"id": "1811.00200", "submitter": "Christopher Mohri", "authors": "Christopher Mohri", "title": "Online Learning Algorithms for Statistical Arbitrage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical arbitrage is a class of financial trading strategies using mean\nreversion models. The corresponding techniques rely on a number of assumptions\nwhich may not hold for general non-stationary stochastic processes. This paper\npresents an alternative technique for statistical arbitrage based on online\nlearning which does not require such assumptions and which benefits from strong\nlearning guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 03:12:26 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Mohri", "Christopher", ""]]}, {"id": "1811.00208", "submitter": "Xu Chu", "authors": "Xu Chu, Yang Lin, Jingyue Gao, Jiangtao Wang, Yasha Wang, Leye Wang", "title": "Multi-Label Robust Factorization Autoencoder and its Application in\n  Predicting Drug-Drug Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drug-drug interactions (DDIs) are a major cause of preventable\nhospitalizations and deaths. Predicting the occurrence of DDIs helps drug\nsafety professionals allocate investigative resources and take appropriate\nregulatory action promptly. Traditional DDI prediction methods predict DDIs\nbased on the similarity between drugs. Recently, researchers revealed that\npredictive performance can be improved by better modeling the interactions\nbetween drug pairs with bilinear forms. However, the shallow models leveraging\nbilinear forms suffer from limitations on capturing complicated nonlinear\ninteractions between drug pairs. To this end, we propose Multi-Label Robust\nFactorization Autoencoder (abbreviated to MuLFA) for DDI prediction, which\nlearns a representation of interactions between drug pairs and has the\ncapability of characterizing complicated nonlinear interactions more precisely.\nMoreover, a novel loss called CuXCov is designed to effectively learn the\nparameters of MuLFA. Furthermore, the decoder is able to generate high-risk\nchemical structures of drug pairs for specific DDIs, assisting pharmacists to\nbetter understand the relationship between drug chemistry and DDI. Experimental\nresults on real-world datasets demonstrate that MuLFA consistently outperforms\nstate-of-the-art methods; particularly, it increases 21:3% predictive\nperformance compared to the best baseline for top 50 frequent DDIs.We also\nillustrate various case studies to demonstrate the efficacy of the chemical\nstructures generated by MuLFA in DDI diagnosis.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 03:50:45 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Chu", "Xu", ""], ["Lin", "Yang", ""], ["Gao", "Jingyue", ""], ["Wang", "Jiangtao", ""], ["Wang", "Yasha", ""], ["Wang", "Leye", ""]]}, {"id": "1811.00210", "submitter": "Jie Chen", "authors": "Tengfei Ma, Patrick Ferber, Siyu Huo, Jie Chen, Michael Katz", "title": "Online Planner Selection with Graph Neural Networks and Adaptive\n  Scheduling", "comments": "AAAI 2020. Code is released at\n  https://github.com/matenure/GNN_planner. Data set is released at\n  https://github.com/IBM/IPC-graph-data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated planning is one of the foundational areas of AI. Since no single\nplanner can work well for all tasks and domains, portfolio-based techniques\nhave become increasingly popular in recent years. In particular, deep learning\nemerges as a promising methodology for online planner selection. Owing to the\nrecent development of structural graph representations of planning tasks, we\npropose a graph neural network (GNN) approach to selecting candidate planners.\nGNNs are advantageous over a straightforward alternative, the convolutional\nneural networks, in that they are invariant to node permutations and that they\nincorporate node labels for better inference.\n  Additionally, for cost-optimal planning, we propose a two-stage adaptive\nscheduling method to further improve the likelihood that a given task is solved\nin time. The scheduler may switch at halftime to a different planner,\nconditioned on the observed performance of the first one. Experimental results\nvalidate the effectiveness of the proposed method against strong baselines,\nboth deep learning and non-deep learning based.\n  The code is available at \\url{https://github.com/matenure/GNN_planner}.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 03:51:32 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 02:15:12 GMT"}, {"version": "v3", "created": "Tue, 26 Feb 2019 20:32:03 GMT"}, {"version": "v4", "created": "Wed, 20 Nov 2019 16:16:36 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Ma", "Tengfei", ""], ["Ferber", "Patrick", ""], ["Huo", "Siyu", ""], ["Chen", "Jie", ""], ["Katz", "Michael", ""]]}, {"id": "1811.00217", "submitter": "Rafael Menelau Oliveira E Cruz", "authors": "Rafael M. O Cruz, Robert Sabourin and George D. C. Cavalcanti", "title": "META-DES.Oracle: Meta-learning and feature selection for ensemble\n  selection", "comments": "Paper published on Information Fusion", "journal-ref": "Volume 38, November 2017, Pages 84-103", "doi": "10.1016/j.inffus.2017.02.010", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key issue in Dynamic Ensemble Selection (DES) is defining a suitable\ncriterion for calculating the classifiers' competence. There are several\ncriteria available to measure the level of competence of base classifiers, such\nas local accuracy estimates and ranking. However, using only one criterion may\nlead to a poor estimation of the classifier's competence. In order to deal with\nthis issue, we have proposed a novel dynamic ensemble selection framework using\nmeta-learning, called META-DES. An important aspect of the META-DES framework\nis that multiple criteria can be embedded in the system encoded as different\nsets of meta-features. However, some DES criteria are not suitable for every\nclassification problem. For instance, local accuracy estimates may produce poor\nresults when there is a high degree of overlap between the classes. Moreover, a\nhigher classification accuracy can be obtained if the performance of the\nmeta-classifier is optimized for the corresponding data. In this paper, we\npropose a novel version of the META-DES framework based on the formal\ndefinition of the Oracle, called META-DES.Oracle. The Oracle is an abstract\nmethod that represents an ideal classifier selection scheme. A meta-feature\nselection scheme using an overfitting cautious Binary Particle Swarm\nOptimization (BPSO) is proposed for improving the performance of the\nmeta-classifier. The difference between the outputs obtained by the\nmeta-classifier and those presented by the Oracle is minimized. Thus, the\nmeta-classifier is expected to obtain results that are similar to the Oracle.\nExperiments carried out using 30 classification problems demonstrate that the\noptimization procedure based on the Oracle definition leads to a significant\nimprovement in classification accuracy when compared to previous versions of\nthe META-DES framework and other state-of-the-art DES techniques.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 04:20:28 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Cruz", "Rafael M. O", ""], ["Sabourin", "Robert", ""], ["Cavalcanti", "George D. C.", ""]]}, {"id": "1811.00231", "submitter": "Benjamin Lansdell", "authors": "Benjamin James Lansdell and Konrad Paul Kording", "title": "Towards learning-to-learn", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In good old-fashioned artificial intelligence (GOFAI), humans specified\nsystems that solved problems. Much of the recent progress in AI has come from\nreplacing human insights by learning. However, learning itself is still usually\nbuilt by humans -- specifically the choice that parameter updates should follow\nthe gradient of a cost function. Yet, in analogy with GOFAI, there is no reason\nto believe that humans are particularly good at defining such learning systems:\nwe may expect learning itself to be better if we learn it. Recent research in\nmachine learning has started to realize the benefits of that strategy. We\nshould thus expect this to be relevant for neuroscience: how could the correct\nlearning rules be acquired? Indeed, cognitive science has long shown that\nhumans learn-to-learn, which is potentially responsible for their impressive\nlearning abilities. Here we discuss ideas across machine learning,\nneuroscience, and cognitive science that matter for the principle of\nlearning-to-learn.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 05:07:49 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 14:33:21 GMT"}, {"version": "v3", "created": "Wed, 9 Jan 2019 15:45:11 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Lansdell", "Benjamin James", ""], ["Kording", "Konrad Paul", ""]]}, {"id": "1811.00239", "submitter": "Lili Mou", "authors": "Nabiha Asghar, Lili Mou, Kira A. Selby, Kevin D. Pantasdo, Pascal\n  Poupart, Xin Jiang", "title": "Progressive Memory Banks for Incremental Domain Adaptation", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of incremental domain adaptation (IDA) in\nnatural language processing (NLP). We assume each domain comes one after\nanother, and that we could only access data in the current domain. The goal of\nIDA is to build a unified model performing well on all the domains that we have\nencountered. We adopt the recurrent neural network (RNN) widely used in NLP,\nbut augment it with a directly parameterized memory bank, which is retrieved by\nan attention mechanism at each step of RNN transition. The memory bank provides\na natural way of IDA: when adapting our model to a new domain, we progressively\nadd new slots to the memory bank, which increases the number of parameters, and\nthus the model capacity. We learn the new memory slots and fine-tune existing\nparameters by back-propagation. Experimental results show that our approach\nachieves significantly better performance than fine-tuning alone. Compared with\nexpanding hidden states, our approach is more robust for old domains, shown by\nboth empirical and theoretical results. Our model also outperforms previous\nwork of IDA including elastic weight consolidation and progressive neural\nnetworks in the experiments.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 05:22:01 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 04:04:14 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Asghar", "Nabiha", ""], ["Mou", "Lili", ""], ["Selby", "Kira A.", ""], ["Pantasdo", "Kevin D.", ""], ["Poupart", "Pascal", ""], ["Jiang", "Xin", ""]]}, {"id": "1811.00246", "submitter": "Jinwon An", "authors": "Jinwon An, Sungwon Lyu, Sungzoon Cho", "title": "SARN: Relational Reasoning through Sequential Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an attention module augmented relational network called\nSARN(Sequential Attention Relational Network) that can carry out relational\nreasoning by extracting reference objects and making efficient pairing between\nobjects. SARN greatly reduces the computational and memory requirements of the\nrelational network, which computes all object pairs. It also shows high\naccuracy on the Sort-of-CLEVR dataset compared to other models, especially on\nrelational questions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 05:45:43 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["An", "Jinwon", ""], ["Lyu", "Sungwon", ""], ["Cho", "Sungzoon", ""]]}, {"id": "1811.00247", "submitter": "Padala Manisha Miss", "authors": "Padala Manisha, Sujit Gujar", "title": "FNNC: Achieving Fairness through Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classification models fairness can be ensured by solving a constrained\noptimization problem. We focus on fairness constraints like Disparate Impact,\nDemographic Parity, and Equalized Odds, which are non-decomposable and\nnon-convex. Researchers define convex surrogates of the constraints and then\napply convex optimization frameworks to obtain fair classifiers. Surrogates\nserve only as an upper bound to the actual constraints, and convexifying\nfairness constraints might be challenging.\n  We propose a neural network-based framework, \\emph{FNNC}, to achieve fairness\nwhile maintaining high accuracy in classification. The above fairness\nconstraints are included in the loss using Lagrangian multipliers. We prove\nbounds on generalization errors for the constrained losses which asymptotically\ngo to zero. The network is optimized using two-step mini-batch stochastic\ngradient descent. Our experiments show that FNNC performs as good as the state\nof the art, if not better. The experimental evidence supplements our\ntheoretical guarantees. In summary, we have an automated solution to achieve\nfairness in classification, which is easily extendable to many fairness\nconstraints.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 05:49:40 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 11:14:39 GMT"}, {"version": "v3", "created": "Thu, 7 May 2020 06:17:20 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Manisha", "Padala", ""], ["Gujar", "Sujit", ""]]}, {"id": "1811.00255", "submitter": "Masaaki Takada Mr.", "authors": "Masaaki Takada, Hironori Fujisawa, Takeichiro Nishikawa", "title": "HMLasso: Lasso with High Missing Rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse regression such as the Lasso has achieved great success in handling\nhigh-dimensional data. However, one of the biggest practical problems is that\nhigh-dimensional data often contain large amounts of missing values. Convex\nConditioned Lasso (CoCoLasso) has been proposed for dealing with\nhigh-dimensional data with missing values, but it performs poorly when there\nare many missing values, so that the high missing rate problem has not been\nresolved. In this paper, we propose a novel Lasso-type regression method for\nhigh-dimensional data with high missing rates. We effectively incorporate mean\nimputed covariance, overcoming its inherent estimation bias. The result is an\noptimally weighted modification of CoCoLasso according to missing ratios. We\ntheoretically and experimentally show that our proposed method is highly\neffective even when there are many missing values.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 06:44:53 GMT"}, {"version": "v2", "created": "Tue, 25 Dec 2018 04:51:53 GMT"}, {"version": "v3", "created": "Tue, 7 May 2019 06:42:00 GMT"}, {"version": "v4", "created": "Wed, 19 Jun 2019 09:05:04 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Takada", "Masaaki", ""], ["Fujisawa", "Hironori", ""], ["Nishikawa", "Takeichiro", ""]]}, {"id": "1811.00260", "submitter": "Edoardo Conti", "authors": "Jason Gauci, Edoardo Conti, Yitao Liang, Kittipat Virochsiri, Yuchen\n  He, Zachary Kaden, Vivek Narayanan, Xiaohui Ye, Zhengxing Chen, Scott\n  Fujimoto", "title": "Horizon: Facebook's Open Source Applied Reinforcement Learning Platform", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present Horizon, Facebook's open source applied\nreinforcement learning (RL) platform. Horizon is an end-to-end platform\ndesigned to solve industry applied RL problems where datasets are large\n(millions to billions of observations), the feedback loop is slow (vs. a\nsimulator), and experiments must be done with care because they don't run in a\nsimulator. Unlike other RL platforms, which are often designed for fast\nprototyping and experimentation, Horizon is designed with production use cases\nas top of mind. The platform contains workflows to train popular deep RL\nalgorithms and includes data preprocessing, feature transformation, distributed\ntraining, counterfactual policy evaluation, optimized serving, and a\nmodel-based data understanding tool. We also showcase and describe real\nexamples where reinforcement learning models trained with Horizon significantly\noutperformed and replaced supervised learning systems at Facebook.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 07:02:45 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 22:49:50 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 05:57:02 GMT"}, {"version": "v4", "created": "Thu, 30 May 2019 20:47:42 GMT"}, {"version": "v5", "created": "Wed, 4 Sep 2019 19:30:00 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Gauci", "Jason", ""], ["Conti", "Edoardo", ""], ["Liang", "Yitao", ""], ["Virochsiri", "Kittipat", ""], ["He", "Yuchen", ""], ["Kaden", "Zachary", ""], ["Narayanan", "Vivek", ""], ["Ye", "Xiaohui", ""], ["Chen", "Zhengxing", ""], ["Fujimoto", "Scott", ""]]}, {"id": "1811.00264", "submitter": "Yaqiang Yao", "authors": "Yaqiang Yao, Huanhuan Chen", "title": "Multiple Kernel $k$-Means Clustering by Selecting Representative Kernels", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To cluster data that are not linearly separable in the original feature\nspace, $k$-means clustering was extended to the kernel version. However, the\nperformance of kernel $k$-means clustering largely depends on the choice of\nkernel function. To mitigate this problem, multiple kernel learning has been\nintroduced into the $k$-means clustering to obtain an optimal kernel\ncombination for clustering. Despite the success of multiple kernel $k$-means\nclustering in various scenarios, few of the existing work update the\ncombination coefficients based on the diversity of kernels, which leads to the\nresult that the selected kernels contain high redundancy and would degrade the\nclustering performance and efficiency. In this paper, we propose a simple but\nefficient strategy that selects a diverse subset from the pre-specified kernels\nas the representative kernels, and then incorporate the subset selection\nprocess into the framework of multiple $k$-means clustering. The representative\nkernels can be indicated as the significant combination weights. Due to the\nnon-convexity of the obtained objective function, we develop an alternating\nminimization method to optimize the combination coefficients of the selected\nkernels and the cluster membership alternatively. We evaluate the proposed\napproach on several benchmark and real-world datasets. The experimental results\ndemonstrate the competitiveness of our approach in comparison with the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 07:18:15 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Yao", "Yaqiang", ""], ["Chen", "Huanhuan", ""]]}, {"id": "1811.00274", "submitter": "Cho Ying Wu", "authors": "Cho Ying Wu, Ulrich Neumann", "title": "Efficient Multi-Domain Dictionary Learning with GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the multi-domain dictionary learning (MDDL) to make\ndictionary learning-based classification more robust to data representing in\ndifferent domains. We use adversarial neural networks to generate data in\ndifferent styles, and collect all the generated data into a miscellaneous\ndictionary. To tackle the dictionary learning with many samples, we compute the\nweighting matrix that compress the miscellaneous dictionary from multi-sample\nper class to single sample per class. We show that the time complexity solving\nthe proposed MDDL with weighting matrix is the same as solving the dictionary\nwith single sample per class. Moreover, since the weighting matrix could help\nthe solver rely more on the training data, which possibly lie in the same\ndomain with the testing data, the classification could be more accurate.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 07:52:19 GMT"}], "update_date": "2018-11-04", "authors_parsed": [["Wu", "Cho Ying", ""], ["Neumann", "Ulrich", ""]]}, {"id": "1811.00293", "submitter": "Arnu Pretorius", "authors": "Arnu Pretorius, Elan Van Biljon, Steve Kroon, Herman Kamper", "title": "Critical initialisation for deep signal propagation in noisy rectifier\n  neural networks", "comments": "20 pages, 11 figures, accepted at the 32nd Conference on Neural\n  Information Processing Systems (NeurIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic regularisation is an important weapon in the arsenal of a deep\nlearning practitioner. However, despite recent theoretical advances, our\nunderstanding of how noise influences signal propagation in deep neural\nnetworks remains limited. By extending recent work based on mean field theory,\nwe develop a new framework for signal propagation in stochastic regularised\nneural networks. Our noisy signal propagation theory can incorporate several\ncommon noise distributions, including additive and multiplicative Gaussian\nnoise as well as dropout. We use this framework to investigate initialisation\nstrategies for noisy ReLU networks. We show that no critical initialisation\nstrategy exists using additive noise, with signal propagation exploding\nregardless of the selected noise distribution. For multiplicative noise (e.g.\ndropout), we identify alternative critical initialisation strategies that\ndepend on the second moment of the noise distribution. Simulations and\nexperiments on real-world data confirm that our proposed initialisation is able\nto stably propagate signals in deep networks, while using an initialisation\ndisregarding noise fails to do so. Furthermore, we analyse correlation dynamics\nbetween inputs. Stronger noise regularisation is shown to reduce the depth to\nwhich discriminatory information about the inputs to a noisy ReLU network is\nable to propagate, even when initialised at criticality. We support our\ntheoretical predictions for these trainable depths with simulations, as well as\nwith experiments on MNIST and CIFAR-10\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 09:58:34 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 11:39:03 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Pretorius", "Arnu", ""], ["Van Biljon", "Elan", ""], ["Kroon", "Steve", ""], ["Kamper", "Herman", ""]]}, {"id": "1811.00321", "submitter": "Ramin M. Hasani", "authors": "Ramin M. Hasani, Mathias Lechner, Alexander Amini, Daniela Rus, Radu\n  Grosu", "title": "Liquid Time-constant Recurrent Neural Networks as Universal\n  Approximators", "comments": "This short report introduces the universal approximation capabilities\n  of liquid time-constant (LTC) recurrent neural networks, and provides\n  theoretical bounds for its dynamics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the notion of liquid time-constant (LTC)\nrecurrent neural networks (RNN)s, a subclass of continuous-time RNNs, with\nvarying neuronal time-constant realized by their nonlinear synaptic\ntransmission model. This feature is inspired by the communication principles in\nthe nervous system of small species. It enables the model to approximate\ncontinuous mapping with a small number of computational units. We show that any\nfinite trajectory of an $n$-dimensional continuous dynamical system can be\napproximated by the internal state of the hidden units and $n$ output units of\nan LTC network. Here, we also theoretically find bounds on their neuronal\nstates and varying time-constant.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 11:36:56 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Hasani", "Ramin M.", ""], ["Lechner", "Mathias", ""], ["Amini", "Alexander", ""], ["Rus", "Daniela", ""], ["Grosu", "Radu", ""]]}, {"id": "1811.00333", "submitter": "Diogo R. Ferreira", "authors": "Diogo R. Ferreira (on behalf of JET Contributors)", "title": "Applications of Deep Learning to Nuclear Fusion Research", "comments": "This paper is based on a talk presented at NVIDIA's GPU Technology\n  Conference, which took place at the International Congress Center in Munich,\n  Germany, October 9--11, 2018 (GTC Europe 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.plasm-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclear fusion is the process that powers the sun, and it is one of the best\nhopes to achieve a virtually unlimited energy source for the future of\nhumanity. However, reproducing sustainable nuclear fusion reactions here on\nEarth is a tremendous scientific and technical challenge. Special devices --\ncalled tokamaks -- have been built around the world, with JET (Joint European\nTorus, in the UK) being the largest tokamak currently in operation. Such\ndevices confine matter and heat it up to extremely high temperatures, creating\na plasma where fusion reactions begin to occur. JET has over one hundred\ndiagnostic systems to monitor what happens inside the plasma, and each\n30-second experiment (or pulse) generates about 50 GB of data. In this work, we\nshow how convolutional neural networks (CNNs) can be used to reconstruct the 2D\nplasma profile inside the device based on data coming from those diagnostics.\nWe also discuss how recurrent neural networks (RNNs) can be used to predict\nplasma disruptions, which are one of the major problems affecting tokamaks\ntoday. Training of such networks is done on NVIDIA GPUs.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 11:58:59 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Ferreira", "Diogo R.", "", "on behalf of JET Contributors"]]}, {"id": "1811.00338", "submitter": "Qin Zou", "authors": "Qin Zou, Yanling Wang, Qian Wang, Yi Zhao, Qingquan Li", "title": "Deep Learning-Based Gait Recognition Using Smartphones in the Wild", "comments": "IEEE Transactions on Information Forensics and Security, 15(1), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to other biometrics, gait is difficult to conceal and has the\nadvantage of being unobtrusive. Inertial sensors, such as accelerometers and\ngyroscopes, are often used to capture gait dynamics. These inertial sensors are\ncommonly integrated into smartphones and are widely used by the average person,\nwhich makes gait data convenient and inexpensive to collect. In this paper, we\nstudy gait recognition using smartphones in the wild. In contrast to\ntraditional methods, which often require a person to walk along a specified\nroad and/or at a normal walking speed, the proposed method collects inertial\ngait data under unconstrained conditions without knowing when, where, and how\nthe user walks. To obtain good person identification and authentication\nperformance, deep-learning techniques are presented to learn and model the gait\nbiometrics based on walking data. Specifically, a hybrid deep neural network is\nproposed for robust gait feature representation, where features in the space\nand time domains are successively abstracted by a convolutional neural network\nand a recurrent neural network. In the experiments, two datasets collected by\nsmartphones for a total of 118 subjects are used for evaluations. The\nexperiments show that the proposed method achieves higher than 93.5\\% and\n93.7\\% accuracies in person identification and authentication, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 12:20:37 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 17:48:49 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 15:47:09 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Zou", "Qin", ""], ["Wang", "Yanling", ""], ["Wang", "Qian", ""], ["Zhao", "Yi", ""], ["Li", "Qingquan", ""]]}, {"id": "1811.00401", "submitter": "J\\\"orn-Henrik Jacobsen", "authors": "J\\\"orn-Henrik Jacobsen, Jens Behrmann, Richard Zemel, Matthias Bethge", "title": "Excessive Invariance Causes Adversarial Vulnerability", "comments": null, "journal-ref": "Proceedings of the 7th International Conference on Learning\n  Representations (ICLR), 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their impressive performance, deep neural networks exhibit striking\nfailures on out-of-distribution inputs. One core idea of adversarial example\nresearch is to reveal neural network errors under such distribution shifts. We\ndecompose these errors into two complementary sources: sensitivity and\ninvariance. We show deep networks are not only too sensitive to task-irrelevant\nchanges of their input, as is well-known from epsilon-adversarial examples, but\nare also too invariant to a wide range of task-relevant changes, thus making\nvast regions in input space vulnerable to adversarial attacks. We show such\nexcessive invariance occurs across various tasks and architecture types. On\nMNIST and ImageNet one can manipulate the class-specific content of almost any\nimage without changing the hidden activations. We identify an insufficiency of\nthe standard cross-entropy loss as a reason for these failures. Further, we\nextend this objective based on an information-theoretic analysis so it\nencourages the model to consider all task-dependent features in its decision.\nThis provides the first approach tailored explicitly to overcome excessive\ninvariance and resulting vulnerabilities.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 14:14:03 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 03:26:21 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 04:12:20 GMT"}, {"version": "v4", "created": "Sun, 12 Jul 2020 07:26:06 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Jacobsen", "J\u00f6rn-Henrik", ""], ["Behrmann", "Jens", ""], ["Zemel", "Richard", ""], ["Bethge", "Matthias", ""]]}, {"id": "1811.00403", "submitter": "Herman Kamper", "authors": "Herman Kamper", "title": "Truly unsupervised acoustic word embeddings using weak top-down\n  constraints in encoder-decoder models", "comments": "5 pages, 3 figures, 2 tables; accepted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate unsupervised models that can map a variable-duration speech\nsegment to a fixed-dimensional representation. In settings where unlabelled\nspeech is the only available resource, such acoustic word embeddings can form\nthe basis for \"zero-resource\" speech search, discovery and indexing systems.\nMost existing unsupervised embedding methods still use some supervision, such\nas word or phoneme boundaries. Here we propose the encoder-decoder\ncorrespondence autoencoder (EncDec-CAE), which, instead of true word segments,\nuses automatically discovered segments: an unsupervised term discovery system\nfinds pairs of words of the same unknown type, and the EncDec-CAE is trained to\nreconstruct one word given the other as input. We compare it to a standard\nencoder-decoder autoencoder (AE), a variational AE with a prior over its latent\nembedding, and downsampling. EncDec-CAE outperforms its closest competitor by\n24% relative in average precision on two languages in a word discrimination\ntask.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 14:17:01 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 14:28:07 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Kamper", "Herman", ""]]}, {"id": "1811.00410", "submitter": "Elliot J. Crowley", "authors": "Antreas Antoniou, Agnieszka S{\\l}owik, Elliot J. Crowley, Amos Storkey", "title": "Dilated DenseNets for Relational Reasoning", "comments": "Extended Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite their impressive performance in many tasks, deep neural networks\noften struggle at relational reasoning. This has recently been remedied with\nthe introduction of a plug-in relational module that considers relations\nbetween pairs of objects. Unfortunately, this is combinatorially expensive. In\nthis extended abstract, we show that a DenseNet incorporating dilated\nconvolutions excels at relational reasoning on the Sort-of-CLEVR dataset,\nallowing us to forgo this relational module and its associated expense.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 14:38:44 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Antoniou", "Antreas", ""], ["S\u0142owik", "Agnieszka", ""], ["Crowley", "Elliot J.", ""], ["Storkey", "Amos", ""]]}, {"id": "1811.00414", "submitter": "Ewin Tang", "authors": "Ewin Tang", "title": "Quantum-inspired classical algorithms for principal component analysis\n  and supervised clustering", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe classical analogues to Lloyd et al.'s quantum algorithms for\nprincipal component analysis and nearest-centroid clustering. We introduce a\nclassical algorithm model that assumes we can efficiently perform $\\ell^2$-norm\nsamples of input data, a natural analogue to quantum algorithms assuming\nefficient state preparation. In this model, our classical algorithms run in\ntime polylogarithmic in input size, matching the runtime of the quantum\nalgorithms with only polynomial slowdown. These algorithms indicate that their\ncorresponding problems do not yield exponential quantum speedups.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 03:23:52 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 10:22:32 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Tang", "Ewin", ""]]}, {"id": "1811.00416", "submitter": "Avanti Shrikumar", "authors": "Avanti Shrikumar, Katherine Tian, \\v{Z}iga Avsec, Anna Shcherbina,\n  Abhimanyu Banerjee, Mahfuza Sharmin, Surag Nair, Anshul Kundaje", "title": "Technical Note on Transcription Factor Motif Discovery from Importance\n  Scores (TF-MoDISco) version 0.5.6.5", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TF-MoDISco (Transcription Factor Motif Discovery from Importance Scores) is\nan algorithm for identifying motifs from basepair-level importance scores\ncomputed on genomic sequence data. This technical note focuses on version\nv0.5.6.5. The implementation is available at\nhttps://github.com/kundajelab/tfmodisco/tree/v0.5.6.5\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 16:22:49 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 03:37:19 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2020 10:57:12 GMT"}, {"version": "v4", "created": "Fri, 3 Jan 2020 04:08:40 GMT"}, {"version": "v5", "created": "Thu, 30 Apr 2020 11:50:44 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Shrikumar", "Avanti", ""], ["Tian", "Katherine", ""], ["Avsec", "\u017diga", ""], ["Shcherbina", "Anna", ""], ["Banerjee", "Abhimanyu", ""], ["Sharmin", "Mahfuza", ""], ["Nair", "Surag", ""], ["Kundaje", "Anshul", ""]]}, {"id": "1811.00423", "submitter": "Daniel Tait", "authors": "Daniel J. Tait and Bruce J. Worton", "title": "Multiplicative Latent Force Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian modelling of dynamic systems must achieve a compromise between\nproviding a complete mechanistic specification of the process while retaining\nthe flexibility to handle those situations in which data is sparse relative to\nmodel complexity, or a full specification is hard to motivate. Latent force\nmodels achieve this dual aim by specifying a parsimonious linear evolution\nequation which an additive latent Gaussian process (GP) forcing term. In this\nwork we extend the latent force framework to allow for multiplicative\ninteractions between the GP and the latent states leading to more control over\nthe geometry of the trajectories. Unfortunately inference is no longer\nstraightforward and so we introduce an approximation based on the method of\nsuccessive approximations and examine its performance using a simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 15:08:46 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Tait", "Daniel J.", ""], ["Worton", "Bruce J.", ""]]}, {"id": "1811.00424", "submitter": "Daniel Rodriguez", "authors": "Raul-Jose Palma-Mendoza, Daniel Rodriguez, Luis de-Marcos", "title": "Distributed ReliefF based Feature Selection in Spark", "comments": null, "journal-ref": null, "doi": "10.1007/s10115-017-1145-y", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection (FS) is a key research area in the machine learning and\ndata mining fields, removing irrelevant and redundant features usually helps to\nreduce the effort required to process a dataset while maintaining or even\nimproving the processing algorithm's accuracy. However, traditional algorithms\ndesigned for executing on a single machine lack scalability to deal with the\nincreasing amount of data that has become available in the current Big Data\nera. ReliefF is one of the most important algorithms successfully implemented\nin many FS applications. In this paper, we present a completely redesigned\ndistributed version of the popular ReliefF algorithm based on the novel Spark\ncluster computing model that we have called DiReliefF. Spark is increasing its\npopularity due to its much faster processing times compared with Hadoop's\nMapReduce model implementation. The effectiveness of our proposal is tested on\nfour publicly available datasets, all of them with a large number of instances\nand two of them with also a large number of features. Subsets of these datasets\nwere also used to compare the results to a non-distributed implementation of\nthe algorithm. The results show that the non-distributed implementation is\nunable to handle such large volumes of data without specialized hardware, while\nour design can process them in a scalable way with much better processing times\nand memory usage.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 15:11:32 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Palma-Mendoza", "Raul-Jose", ""], ["Rodriguez", "Daniel", ""], ["de-Marcos", "Luis", ""]]}, {"id": "1811.00429", "submitter": "Pierre Thodoroff", "authors": "Pierre Thodoroff, Audrey Durand, Joelle Pineau, Doina Precup", "title": "Temporal Regularization in Markov Decision Process", "comments": "Published as a conference paper at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several applications of Reinforcement Learning suffer from instability due to\nhigh variance. This is especially prevalent in high dimensional domains.\nRegularization is a commonly used technique in machine learning to reduce\nvariance, at the cost of introducing some bias. Most existing regularization\ntechniques focus on spatial (perceptual) regularization. Yet in reinforcement\nlearning, due to the nature of the Bellman equation, there is an opportunity to\nalso exploit temporal regularization based on smoothness in value estimates\nover trajectories. This paper explores a class of methods for temporal\nregularization. We formally characterize the bias induced by this technique\nusing Markov chain concepts. We illustrate the various characteristics of\ntemporal regularization via a sequence of simple discrete and continuous MDPs,\nand show that the technique provides improvement even in high-dimensional Atari\ngames.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 15:21:45 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 23:03:53 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Thodoroff", "Pierre", ""], ["Durand", "Audrey", ""], ["Pineau", "Joelle", ""], ["Precup", "Doina", ""]]}, {"id": "1811.00454", "submitter": "Emad Grais", "authors": "Emad M. Grais, Hagen Wierstorf, Dominic Ward, Russell Mason, Mark D.\n  Plumbley", "title": "Referenceless Performance Evaluation of Audio Source Separation using\n  Deep Neural Networks", "comments": null, "journal-ref": "This paper will be presented at EUSIPCO 2019", "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current performance evaluation for audio source separation depends on\ncomparing the processed or separated signals with reference signals. Therefore,\ncommon performance evaluation toolkits are not applicable to real-world\nsituations where the ground truth audio is unavailable. In this paper, we\npropose a performance evaluation technique that does not require reference\nsignals in order to assess separation quality. The proposed technique uses a\ndeep neural network (DNN) to map the processed audio into its quality score.\nOur experiment results show that the DNN is capable of predicting the\nsources-to-artifacts ratio from the blind source separation evaluation toolkit\nwithout the need for reference signals.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 15:50:42 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Grais", "Emad M.", ""], ["Wierstorf", "Hagen", ""], ["Ward", "Dominic", ""], ["Mason", "Russell", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1811.00458", "submitter": "Di Chen", "authors": "Di Chen, Carla P. Gomes", "title": "Bias Reduction via End-to-End Shift Learning: Application to Citizen\n  Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citizen science projects are successful at gathering rich datasets for\nvarious applications. However, the data collected by citizen scientists are\noften biased --- in particular, aligned more with the citizens' preferences\nthan with scientific objectives. We propose the Shift Compensation Network\n(SCN), an end-to-end learning scheme which learns the shift from the scientific\nobjectives to the biased data while compensating for the shift by re-weighting\nthe training data. Applied to bird observational data from the citizen science\nproject eBird, we demonstrate how SCN quantifies the data distribution shift\nand outperforms supervised learning models that do not address the data bias.\nCompared with competing models in the context of covariate shift, we further\ndemonstrate the advantage of SCN in both its effectiveness and its capability\nof handling massive high-dimensional data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 15:54:30 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 21:32:48 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 17:36:08 GMT"}, {"version": "v4", "created": "Wed, 14 Nov 2018 05:35:37 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Chen", "Di", ""], ["Gomes", "Carla P.", ""]]}, {"id": "1811.00464", "submitter": "Yue Li", "authors": "Yue Li, Manolis Kellis", "title": "A latent topic model for mining heterogenous non-randomly missing\n  electronic health records data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health records (EHR) are rich heterogeneous collection of patient\nhealth information, whose broad adoption provides great opportunities for\nsystematic health data mining. However, heterogeneous EHR data types and biased\nascertainment impose computational challenges. Here, we present mixEHR, an\nunsupervised generative model integrating collaborative filtering and latent\ntopic models, which jointly models the discrete distributions of data\nobservation bias and actual data using latent disease-topic distributions. We\napply mixEHR on 12.8 million phenotypic observations from the MIMIC dataset,\nand use it to reveal latent disease topics, interpret EHR results, impute\nmissing data, and predict mortality in intensive care units. Using both\nsimulation and real data, we show that mixEHR outperforms previous methods and\nreveals meaningful multi-disease insights.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 16:04:58 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Li", "Yue", ""], ["Kellis", "Manolis", ""]]}, {"id": "1811.00472", "submitter": "Weidi Xie", "authors": "Erika Lu, Weidi Xie and Andrew Zisserman", "title": "Class-Agnostic Counting", "comments": "Asian Conference on Computer Vision (ACCV), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nearly all existing counting methods are designed for a specific object\nclass. Our work, however, aims to create a counting model able to count any\nclass of object. To achieve this goal, we formulate counting as a matching\nproblem, enabling us to exploit the image self-similarity property that\nnaturally exists in object counting problems. We make the following three\ncontributions: first, a Generic Matching Network (GMN) architecture that can\npotentially count any object in a class-agnostic manner; second, by\nreformulating the counting problem as one of matching objects, we can take\nadvantage of the abundance of video data labeled for tracking, which contains\nnatural repetitions suitable for training a counting model. Such data enables\nus to train the GMN. Third, to customize the GMN to different user\nrequirements, an adapter module is used to specialize the model with minimal\neffort, i.e. using a few labeled examples, and adapting only a small fraction\nof the trained parameters. This is a form of few-shot learning, which is\npractical for domains where labels are limited due to requiring expert\nknowledge (e.g. microbiology). We demonstrate the flexibility of our method on\na diverse set of existing counting benchmarks: specifically cells, cars, and\nhuman crowds. The model achieves competitive performance on cell and crowd\ncounting datasets, and surpasses the state-of-the-art on the car dataset using\nonly three training images. When training on the entire dataset, the proposed\nmethod outperforms all previous methods by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 16:11:42 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Lu", "Erika", ""], ["Xie", "Weidi", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1811.00497", "submitter": "Xiaoran Xu", "authors": "Xiaoran Xu, Songpeng Zu, Chengliang Gao, Yuan Zhang, and Wei Feng", "title": "Modeling Attention Flow on Graphs", "comments": "NeurIPS 2018 Relational Representation Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world scenarios demand reasoning about process, more than final outcome\nprediction, to discover latent causal chains and better understand complex\nsystems. It requires the learning algorithms to offer both accurate predictions\nand clear interpretations. We design a set of trajectory reasoning tasks on\ngraphs with only the source and the destination observed. We present the\nattention flow mechanism to explicitly model the reasoning process, leveraging\nthe relational inductive biases by basing our models on graph networks. We\nstudy the way attention flow can effectively act on the underlying information\nflow implemented by message passing. Experiments demonstrate that the attention\nflow driven by and interacting with graph networks can provide higher accuracy\nin prediction and better interpretation for trajectory reasoning.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 16:59:54 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 08:59:30 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Xu", "Xiaoran", ""], ["Zu", "Songpeng", ""], ["Gao", "Chengliang", ""], ["Zhang", "Yuan", ""], ["Feng", "Wei", ""]]}, {"id": "1811.00512", "submitter": "Renato Negrinho", "authors": "Renato Negrinho, Matthew R. Gormley, Geoffrey J. Gordon", "title": "Learning Beam Search Policies via Imitation Learning", "comments": "Published in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beam search is widely used for approximate decoding in structured prediction\nproblems. Models often use a beam at test time but ignore its existence at\ntrain time, and therefore do not explicitly learn how to use the beam. We\ndevelop an unifying meta-algorithm for learning beam search policies using\nimitation learning. In our setting, the beam is part of the model, and not just\nan artifact of approximate decoding. Our meta-algorithm captures existing\nlearning algorithms and suggests new ones. It also lets us show novel no-regret\nguarantees for learning beam search policies.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 17:31:10 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 17:15:37 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Negrinho", "Renato", ""], ["Gormley", "Matthew R.", ""], ["Gordon", "Geoffrey J.", ""]]}, {"id": "1811.00513", "submitter": "Congzheng Song", "authors": "Congzheng Song, Vitaly Shmatikov", "title": "Auditing Data Provenance in Text-Generation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To help enforce data-protection regulations such as GDPR and detect\nunauthorized uses of personal data, we develop a new \\emph{model auditing}\ntechnique that helps users check if their data was used to train a machine\nlearning model. We focus on auditing deep-learning models that generate\nnatural-language text, including word prediction and dialog generation. These\nmodels are at the core of popular online services and are often trained on\npersonal data such as users' messages, searches, chats, and comments.\n  We design and evaluate a black-box auditing method that can detect, with very\nfew queries to a model, if a particular user's texts were used to train it\n(among thousands of other users). We empirically show that our method can\nsuccessfully audit well-generalized models that are not overfitted to the\ntraining data. We also analyze how text-generation models memorize word\nsequences and explain why this memorization makes them amenable to auditing.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 17:32:44 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 18:47:05 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Song", "Congzheng", ""], ["Shmatikov", "Vitaly", ""]]}, {"id": "1811.00521", "submitter": "Bryan He", "authors": "Bryan He, James Zou", "title": "Minimizing Close-k Aggregate Loss Improves Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classification, the de facto method for aggregating individual losses is\nthe average loss. When the actual metric of interest is 0-1 loss, it is common\nto minimize the average surrogate loss for some well-behaved (e.g. convex)\nsurrogate. Recently, several other aggregate losses such as the maximal loss\nand average top-$k$ loss were proposed as alternative objectives to address\nshortcomings of the average loss. However, we identify common classification\nsettings, e.g. the data is imbalanced, has too many easy or ambiguous examples,\netc., when average, maximal and average top-$k$ all suffer from suboptimal\ndecision boundaries, even on an infinitely large training set. To address this\nproblem, we propose a new classification objective called the close-$k$\naggregate loss, where we adaptively minimize the loss for points close to the\ndecision boundary. We provide theoretical guarantees for the 0-1 accuracy when\nwe optimize close-$k$ aggregate loss. We also conduct systematic experiments\nacross the PMLB and OpenML benchmark datasets. Close-$k$ achieves significant\ngains in 0-1 test accuracy, improvements of $\\geq 2$% and $p<0.05$, in over 25%\nof the datasets compared to average, maximal and average top-$k$. In contrast,\nthe previous aggregate losses outperformed close-$k$ in less than 2% of the\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 17:42:18 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 17:39:14 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["He", "Bryan", ""], ["Zou", "James", ""]]}, {"id": "1811.00525", "submitter": "Marc Khoury", "authors": "Marc Khoury and Dylan Hadfield-Menell", "title": "On the Geometry of Adversarial Examples", "comments": "Improvements to clarity and presentation over initial submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are a pervasive phenomenon of machine learning models\nwhere seemingly imperceptible perturbations to the input lead to\nmisclassifications for otherwise statistically accurate models. We propose a\ngeometric framework, drawing on tools from the manifold reconstruction\nliterature, to analyze the high-dimensional geometry of adversarial examples.\nIn particular, we highlight the importance of codimension: for low-dimensional\ndata manifolds embedded in high-dimensional space there are many directions off\nthe manifold in which to construct adversarial examples. Adversarial examples\nare a natural consequence of learning a decision boundary that classifies the\nlow-dimensional data manifold well, but classifies points near the manifold\nincorrectly. Using our geometric framework we prove (1) a tradeoff between\nrobustness under different norms, (2) that adversarial training in balls around\nthe data is sample inefficient, and (3) sufficient sampling conditions under\nwhich nearest neighbor classifiers and ball-based adversarial training are\nrobust.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 17:47:10 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 21:43:30 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Khoury", "Marc", ""], ["Hadfield-Menell", "Dylan", ""]]}, {"id": "1811.00539", "submitter": "Colin Graber", "authors": "Colin Graber, Ofer Meshi, Alexander Schwing", "title": "Deep Structured Prediction with Nonlinear Output Transformations", "comments": "Appearing in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep structured models are widely used for tasks like semantic segmentation,\nwhere explicit correlations between variables provide important prior\ninformation which generally helps to reduce the data needs of deep nets.\nHowever, current deep structured models are restricted by oftentimes very local\nneighborhood structure, which cannot be increased for computational complexity\nreasons, and by the fact that the output configuration, or a representation\nthereof, cannot be transformed further. Very recent approaches which address\nthose issues include graphical model inference inside deep nets so as to permit\nsubsequent non-linear output space transformations. However, optimization of\nthose formulations is challenging and not well understood. Here, we develop a\nnovel model which generalizes existing approaches, such as structured\nprediction energy networks, and discuss a formulation which maintains\napplicability of existing inference techniques.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 17:59:58 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Graber", "Colin", ""], ["Meshi", "Ofer", ""], ["Schwing", "Alexander", ""]]}, {"id": "1811.00542", "submitter": "Daniel Emaasit", "authors": "Daniel Emaasit", "title": "Pymc-learn: Practical Probabilistic Machine Learning in Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  $\\textit{Pymc-learn}$ is a Python package providing a variety of\nstate-of-the-art probabilistic models for supervised and unsupervised machine\nlearning. It is inspired by $\\textit{scikit-learn}$ and focuses on bringing\nprobabilistic machine learning to non-specialists. It uses a general-purpose\nhigh-level language that mimics $\\textit{scikit-learn}$. Emphasis is put on\nease of use, productivity, flexibility, performance, documentation, and an API\nconsistent with $\\textit{scikit-learn}$. It depends on $\\textit{scikit-learn}$\nand $\\textit{pymc3}$ and is distributed under the new BSD-3 license,\nencouraging its use in both academia and industry. Source code, binaries, and\ndocumentation are available on http://github.com/pymc-learn/pymc-learn.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 22:54:12 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Emaasit", "Daniel", ""]]}, {"id": "1811.00548", "submitter": "Erva Ulu", "authors": "Erva Ulu", "title": "Enhancing the Structural Performance of Additively Manufactured Objects", "comments": "PhD Thesis 2018, Carnegie Mellon University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.CG cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately quantify the performance an additively manufactured\n(AM) product is important for a widespread industry adoption of AM as the\ndesign is required to: (1) satisfy geometrical constraints, (2) satisfy\nstructural constraints dictated by its intended function, and (3) be cost\neffective compared to traditional manufacturing methods. Optimization\ntechniques offer design aids in creating cost-effective structures that meet\nthe prescribed structural objectives. The fundamental problem in existing\napproaches lies in the difficulty to quantify the structural performance as\neach unique design leads to a new set of analyses to determine the structural\nrobustness and such analyses can be very costly due to the complexity of in-use\nforces experienced by the structure. This work develops computationally\ntractable methods tailored to maximize the structural performance of AM\nproducts. A geometry preserving build orientation optimization method as well\nas data-driven shape optimization approaches to structural design are\npresented. Proposed methods greatly enhance the value of AM technology by\ntaking advantage of the design space enabled by it for a broad class of\nproblems involving complex in-use loads.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 17:09:29 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Ulu", "Erva", ""]]}, {"id": "1811.00552", "submitter": "Sandeep Subramanian", "authors": "Sandeep Subramanian, Guillaume Lample, Eric Michael Smith, Ludovic\n  Denoyer, Marc'Aurelio Ranzato, Y-Lan Boureau", "title": "Multiple-Attribute Text Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant approach to unsupervised \"style transfer\" in text is based on\nthe idea of learning a latent representation, which is independent of the\nattributes specifying its \"style\". In this paper, we show that this condition\nis not necessary and is not always met in practice, even with domain\nadversarial training that explicitly aims at learning such disentangled\nrepresentations. We thus propose a new model that controls several factors of\nvariation in textual data where this condition on disentanglement is replaced\nwith a simpler mechanism based on back-translation. Our method allows control\nover multiple attributes, like gender, sentiment, product type, etc., and a\nmore fine-grained control on the trade-off between content preservation and\nchange of style with a pooling operator in the latent space. Our experiments\ndemonstrate that the fully entangled model produces better generations, even\nwhen tested on new and more challenging benchmarks comprising reviews with\nmultiple sentences and multiple attributes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 18:00:00 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 04:13:22 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Subramanian", "Sandeep", ""], ["Lample", "Guillaume", ""], ["Smith", "Eric Michael", ""], ["Denoyer", "Ludovic", ""], ["Ranzato", "Marc'Aurelio", ""], ["Boureau", "Y-Lan", ""]]}, {"id": "1811.00570", "submitter": "Wasi Ahmad", "authors": "Wasi Uddin Ahmad, Zhisong Zhang, Xuezhe Ma, Eduard Hovy, Kai-Wei\n  Chang, Nanyun Peng", "title": "On Difficulties of Cross-Lingual Transfer with Order Differences: A Case\n  Study on Dependency Parsing", "comments": "Accepted by NAACL-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different languages might have different word orders. In this paper, we\ninvestigate cross-lingual transfer and posit that an order-agnostic model will\nperform better when transferring to distant foreign languages. To test our\nhypothesis, we train dependency parsers on an English corpus and evaluate their\ntransfer performance on 30 other languages. Specifically, we compare encoders\nand decoders based on Recurrent Neural Networks (RNNs) and modified\nself-attentive architectures. The former relies on sequential information while\nthe latter is more flexible at modeling word order. Rigorous experiments and\ndetailed analysis shows that RNN-based architectures transfer well to languages\nthat are close to English, while self-attentive models have better overall\ncross-lingual transferability and perform especially well on distant languages.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 18:11:01 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2019 19:45:14 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 01:20:22 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Ahmad", "Wasi Uddin", ""], ["Zhang", "Zhisong", ""], ["Ma", "Xuezhe", ""], ["Hovy", "Eduard", ""], ["Chang", "Kai-Wei", ""], ["Peng", "Nanyun", ""]]}, {"id": "1811.00573", "submitter": "Emmanouil Theodosis", "authors": "Emmanouil Theodosis and Petros Maragos", "title": "Tropical Modeling of Weighted Transducer Algorithms on Graphs", "comments": "Under review for the International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.RA cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted Finite State Transducers (WFSTs) are versatile data structures that\ncan model a great number of problems, ranging from Automatic Speech Recognition\nto DNA sequencing. Traditional computer science algorithms are employed when\nworking with these structures in order to optimise their size, but also the\nruntime of decoding algorithms. However, these algorithms are not unified under\na common framework that would allow for their treatment as a whole. Moreover,\nthe inherent geometrical representation of WFSTs, coupled with the\ntopology-preserving algorithms that operate on them make the structures ideal\nfor tropical analysis. The benefits of such analysis have a twofold nature;\nfirst, matrix operations offer a connection to nonlinear vector space and\nspectral theory, and, second, tropical algebra offers a connection to tropical\ngeometry. In this work we model some of the most frequently used algorithms in\nWFSTs by using tropical algebra; this provides a theoretical unification and\nallows us to also analyse aspects of their tropical geometry. Further, we\nprovide insights via numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 18:19:38 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Theodosis", "Emmanouil", ""], ["Maragos", "Petros", ""]]}, {"id": "1811.00576", "submitter": "Jean Thierry-Mieg", "authors": "Jean Thierry-Mieg", "title": "Connections between physics, mathematics and deep learning", "comments": "Version 1 and 2 title was: How the fundamental concepts of\n  mathematics and physics explain deep learning. Version 3 with the new title\n  is accepted in LHEP. It is enriched by a new chapter on the Bayesian\n  Information criterion seen as an application of renormalisation theory. 19\n  pages, 22 references, no figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG hep-th stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting from the Fermat's principle of least action, which governs classical\nand quantum mechanics and from the theory of exterior differential forms, which\ngoverns the geometry of curved manifolds, we show how to derive the equations\ngoverning neural networks in an intrinsic, coordinate invariant way, where the\nloss function plays the role of the Hamiltonian. To be covariant, these\nequations imply a layer metric which is instrumental in pretraining and\nexplains the role of conjugation when using complex numbers. The differential\nformalism also clarifies the relation of the gradient descent optimizer with\nAristotelian and Newtonian mechanics and why large learning steps break the\nlogic of the linearization procedure. We hope that this formal presentation of\nthe differential geometry of neural networks will encourage some physicists to\ndive into deep learning, and reciprocally, that the specialists of deep\nlearning will better appreciate the close interconnection of their subject with\nthe foundations of classical and quantum field theory.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 18:21:42 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 18:14:00 GMT"}, {"version": "v3", "created": "Sun, 25 Aug 2019 14:30:52 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Thierry-Mieg", "Jean", ""]]}, {"id": "1811.00577", "submitter": "Luiz F. O. Chamon", "authors": "Luiz F. O. Chamon and Yonina C. Eldar and Alejandro Ribeiro", "title": "Functional Nonlinear Sparse Models", "comments": "Accepted for publication on the IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signal processing is rich in inherently continuous and often nonlinear\napplications, such as spectral estimation, optical imaging, and\nsuper-resolution microscopy, in which sparsity plays a key role in obtaining\nstate-of-the-art results. Coping with the infinite dimensionality and\nnon-convexity of these problems typically involves discretization and convex\nrelaxations, e.g., using atomic norms. Nevertheless, grid mismatch and other\ncoherence issues often lead to discretized versions of sparse signals that are\nnot sparse. Even if they are, recovering sparse solutions using convex\nrelaxations requires assumptions that may be hard to meet in practice. What is\nmore, problems involving nonlinear measurements remain non-convex even after\nrelaxing the sparsity objective. We address these issues by directly tackling\nthe continuous, nonlinear problem cast as a sparse functional optimization\nprogram. We prove that when these problems are non-atomic, they have no duality\ngap and can therefore be solved efficiently using duality and~(stochastic)\nconvex optimization methods. We illustrate the wide range of applications of\nthis approach by formulating and solving problems from nonlinear spectral\nestimation and robust classification.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 18:24:25 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 14:08:06 GMT"}, {"version": "v3", "created": "Wed, 18 Sep 2019 23:17:30 GMT"}, {"version": "v4", "created": "Fri, 20 Mar 2020 15:24:04 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Chamon", "Luiz F. O.", ""], ["Eldar", "Yonina C.", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1811.00596", "submitter": "Valery Kharitonov", "authors": "Valery Kharitonov, Dmitry Molchanov, Dmitry Vetrov", "title": "Variational Dropout via Empirical Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Automatic Relevance Determination procedure applied to deep\nneural networks. We show that ARD applied to Bayesian DNNs with Gaussian\napproximate posterior distributions leads to a variational bound similar to\nthat of variational dropout, and in the case of a fixed dropout rate,\nobjectives are exactly the same. Experimental results show that the two\napproaches yield comparable results in practice even when the dropout rates are\ntrained. This leads to an alternative Bayesian interpretation of dropout and\nmitigates some of the theoretical issues that arise with the use of improper\npriors in the variational dropout model. Additionally, we explore the use of\nthe hierarchical priors in ARD and show that it helps achieve higher sparsity\nfor the same accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 19:22:39 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 10:56:21 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Kharitonov", "Valery", ""], ["Molchanov", "Dmitry", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "1811.00620", "submitter": "Hongyuan Zhan", "authors": "Hongyuan Zhan, Gabriel Gomes, Xiaoye S. Li, Kamesh Madduri, Kesheng Wu", "title": "Efficient Online Hyperparameter Optimization for Kernel Ridge Regression\n  with Applications to Traffic Time Series Prediction", "comments": "An extended version of \"Efficient Online Hyperparameter Learning for\n  Traffic Flow Prediction\" published in The 21st IEEE International Conference\n  on Intelligent Transportation Systems (ITSC 2018)", "journal-ref": "H. Zhan, G. Gomes, X. S. Li, K. Madduri, and K. Wu. Efficient\n  Online Hyperparameter Learning for Traffic Flow Prediction. In 2018 IEEE 21th\n  International Conference on Intelligent Transportation Systems (ITSC), pages\n  1-6. IEEE, 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational efficiency is an important consideration for deploying machine\nlearning models for time series prediction in an online setting. Machine\nlearning algorithms adjust model parameters automatically based on the data,\nbut often require users to set additional parameters, known as hyperparameters.\nHyperparameters can significantly impact prediction accuracy. Traffic\nmeasurements, typically collected online by sensors, are serially correlated.\nMoreover, the data distribution may change gradually. A typical adaptation\nstrategy is periodically re-tuning the model hyperparameters, at the cost of\ncomputational burden. In this work, we present an efficient and principled\nonline hyperparameter optimization algorithm for Kernel Ridge regression\napplied to traffic prediction problems. In tests with real traffic measurement\ndata, our approach requires as little as one-seventh of the computation time of\nother tuning methods, while achieving better or similar prediction accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 20:14:49 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Zhan", "Hongyuan", ""], ["Gomes", "Gabriel", ""], ["Li", "Xiaoye S.", ""], ["Madduri", "Kamesh", ""], ["Wu", "Kesheng", ""]]}, {"id": "1811.00621", "submitter": "Chirag Agarwal", "authors": "Chirag Agarwal, Anh Nguyen, Dan Schonfeld", "title": "Improving Adversarial Robustness by Encouraging Discriminative Features", "comments": "This article corresponds to the accepted version at IEEE ICIP 2019.\n  We will link the DOI as soon as it is available", "journal-ref": "2019 26th IEEE International Conference on Image Processing (ICIP)", "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved state-of-the-art results in various\npattern recognition tasks. However, they perform poorly on out-of-distribution\nadversarial examples i.e. inputs that are specifically crafted by an adversary\nto cause DNNs to misbehave, questioning the security and reliability of\napplications. In this paper, we encourage DNN classifiers to learn more\ndiscriminative features by imposing a center loss in addition to the regular\nsoftmax cross-entropy loss. Intuitively, the center loss encourages DNNs to\nsimultaneously learns a center for the deep features of each class, and\nminimize the distances between the intra-class deep features and their\ncorresponding class centers. We hypothesize that minimizing distances between\nintra-class features and maximizing the distances between inter-class features\nat the same time would improve a classifier's robustness to adversarial\nexamples. Our results on state-of-the-art architectures on MNIST, CIFAR-10, and\nCIFAR-100 confirmed that intuition and highlight the importance of\ndiscriminative features.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 20:15:56 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 16:15:04 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Agarwal", "Chirag", ""], ["Nguyen", "Anh", ""], ["Schonfeld", "Dan", ""]]}, {"id": "1811.00628", "submitter": "Zois Boukouvalas", "authors": "Zois Boukouvalas, Daniel C. Elton, Peter W. Chung, Mark D. Fuge", "title": "Independent Vector Analysis for Data Fusion Prior to Molecular Property\n  Prediction with Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.mtrl-sci cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its high computational speed and accuracy compared to ab-initio\nquantum chemistry and forcefield modeling, the prediction of molecular\nproperties using machine learning has received great attention in the fields of\nmaterials design and drug discovery. A main ingredient required for machine\nlearning is a training dataset consisting of molecular features\\textemdash for\nexample fingerprint bits, chemical descriptors, etc. that adequately\ncharacterize the corresponding molecules. However, choosing features for any\napplication is highly non-trivial. No \"universal\" method for feature selection\nexists. In this work, we propose a data fusion framework that uses Independent\nVector Analysis to exploit underlying complementary information contained in\ndifferent molecular featurization methods, bringing us a step closer to\nautomated feature generation. Our approach takes an arbitrary number of\nindividual feature vectors and automatically generates a single, compact (low\ndimensional) set of molecular features that can be used to enhance the\nprediction performance of regression models. At the same time our methodology\nretains the possibility of interpreting the generated features to discover\nrelationships between molecular structures and properties. We demonstrate this\non the QM7b dataset for the prediction of several properties such as\natomization energy, polarizability, frontier orbital eigenvalues, ionization\npotential, electron affinity, and excitation energies. In addition, we show how\nour method helps improve the prediction of experimental binding affinities for\na set of human BACE-1 inhibitors. To encourage more widespread use of IVA we\nhave developed the PyIVA Python package, an open source code which is available\nfor download on Github.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 20:34:31 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Boukouvalas", "Zois", ""], ["Elton", "Daniel C.", ""], ["Chung", "Peter W.", ""], ["Fuge", "Mark D.", ""]]}, {"id": "1811.00631", "submitter": "Rados{\\l}aw Piliszek", "authors": "Rados{\\l}aw Piliszek, Krzysztof Mnich, Szymon Migacz, Pawe{\\l}\n  Tabaszewski, Andrzej Su{\\l}ecki, Aneta Polewko-Klim and Witold Rudnicki", "title": "MDFS - MultiDimensional Feature Selection", "comments": "12 pages, 3 figures, 5 tables, license: CC-BY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identification of informative variables in an information system is often\nperformed using simple one-dimensional filtering procedures that discard\ninformation about interactions between variables. Such approach may result in\nremoving some relevant variables from consideration. Here we present an R\npackage MDFS (MultiDimensional Feature Selection) that performs identification\nof informative variables taking into account synergistic interactions between\nmultiple descriptors and the decision variable. MDFS is an implementation of an\nalgorithm based on information theory. Computational kernel of the package is\nimplemented in C++. A high-performance version implemented in CUDA C is also\navailable. The applications of MDFS are demonstrated using the well-known\nMadelon dataset that has synergistic variables by design. The dataset comes\nfrom the UCI Machine Learning Repository. It is shown that multidimensional\nanalysis is more sensitive than one-dimensional tests and returns more reliable\nrankings of importance.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 12:22:14 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Piliszek", "Rados\u0142aw", ""], ["Mnich", "Krzysztof", ""], ["Migacz", "Szymon", ""], ["Tabaszewski", "Pawe\u0142", ""], ["Su\u0142ecki", "Andrzej", ""], ["Polewko-Klim", "Aneta", ""], ["Rudnicki", "Witold", ""]]}, {"id": "1811.00636", "submitter": "Jerry Li", "authors": "Brandon Tran, Jerry Li, Aleksander Madry", "title": "Spectral Signatures in Backdoor Attacks", "comments": "16 pages, accepted to NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent line of work has uncovered a new form of data poisoning: so-called\n\\emph{backdoor} attacks. These attacks are particularly dangerous because they\ndo not affect a network's behavior on typical, benign data. Rather, the network\nonly deviates from its expected output when triggered by a perturbation planted\nby an adversary.\n  In this paper, we identify a new property of all known backdoor attacks,\nwhich we call \\emph{spectral signatures}. This property allows us to utilize\ntools from robust statistics to thwart the attacks. We demonstrate the efficacy\nof these signatures in detecting and removing poisoned examples on real image\nsets and state of the art neural network architectures. We believe that\nunderstanding spectral signatures is a crucial first step towards designing ML\nsystems secure against such backdoor attacks\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 21:12:01 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Tran", "Brandon", ""], ["Li", "Jerry", ""], ["Madry", "Aleksander", ""]]}, {"id": "1811.00639", "submitter": "Alexander Shekhovtsov", "authors": "Alexander Shekhovtsov and Boris Flach", "title": "Stochastic Normalizations as Bayesian Learning", "comments": "Accepted to ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we investigate the reasons why Batch Normalization (BN) improves\nthe generalization performance of deep networks. We argue that one major\nreason, distinguishing it from data-independent normalization methods, is\nrandomness of batch statistics. This randomness appears in the parameters\nrather than in activations and admits an interpretation as a practical Bayesian\nlearning. We apply this idea to other (deterministic) normalization techniques\nthat are oblivious to the batch size. We show that their generalization\nperformance can be improved significantly by Bayesian learning of the same\nform. We obtain test performance comparable to BN and, at the same time, better\nvalidation losses suitable for subsequent output uncertainty estimation through\napproximate Bayesian posterior.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 21:30:39 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Shekhovtsov", "Alexander", ""], ["Flach", "Boris", ""]]}, {"id": "1811.00641", "submitter": "Anish Acharya", "authors": "Anish Acharya, Rahul Goel, Angeliki Metallinou, Inderjit Dhillon", "title": "Online Embedding Compression for Text Classification using Low Rank\n  Matrix Factorization", "comments": "Accepted in Thirty-Third AAAI Conference on Artificial Intelligence\n  (AAAI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have become state of the art for natural language\nprocessing (NLP) tasks, however deploying these models in production system\nposes significant memory constraints. Existing compression methods are either\nlossy or introduce significant latency. We propose a compression method that\nleverages low rank matrix factorization during training,to compress the word\nembedding layer which represents the size bottleneck for most NLP models. Our\nmodels are trained, compressed and then further re-trained on the downstream\ntask to recover accuracy while maintaining the reduced size. Empirically, we\nshow that the proposed method can achieve 90% compression with minimal impact\nin accuracy for sentence classification tasks, and outperforms alternative\nmethods like fixed-point quantization or offline word embedding compression. We\nalso analyze the inference time and storage space for our method through FLOP\ncalculations, showing that we can compress DNN models by a configurable ratio\nand regain accuracy loss without introducing additional latency compared to\nfixed point quantization. Finally, we introduce a novel learning rate schedule,\nthe Cyclically Annealed Learning Rate (CALR), which we empirically demonstrate\nto outperform other popular adaptive learning rate algorithms on a sentence\nclassification benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 21:38:18 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Acharya", "Anish", ""], ["Goel", "Rahul", ""], ["Metallinou", "Angeliki", ""], ["Dhillon", "Inderjit", ""]]}, {"id": "1811.00648", "submitter": "Matthias Rottmann", "authors": "Matthias Rottmann, Pascal Colling, Thomas-Paul Hack, Robin Chan,\n  Fabian H\\\"uger, Peter Schlicht, Hanno Gottschalk", "title": "Prediction Error Meta Classification in Semantic Segmentation: Detection\n  via Aggregated Dispersion Measures of Softmax Probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that \"meta\" classifies whether seg-ments predicted by a\nsemantic segmentation neural networkintersect with the ground truth. For this\npurpose, we employ measures of dispersion for predicted pixel-wise class\nprobability distributions, like classification entropy, that yield heat maps of\nthe input scene's size. We aggregate these dispersion measures segment-wise and\nderive metrics that are well-correlated with the segment-wise IoU of prediction\nand ground truth. This procedure yields an almost plug and play post-processing\ntool to rate the prediction quality of semantic segmentation networks on\nsegment level. This is especially relevant for monitoring neural networks in\nonline applications like automated driving or medical imaging where reliability\nis of utmost importance. In our tests, we use publicly available\nstate-of-the-art networks trained on the Cityscapes dataset and the BraTS2017\ndataset and analyze the predictive power of different metrics as well as\ndifferent sets of metrics. To this end, we compute logistic LASSO regression\nfits for the task of classifying IoU=0 vs. IoU>0 per segment and obtain AUROC\nvalues of up to 91.55%. We complement these tests with linear regression fits\nto predict the segment-wise IoU and obtain prediction standard deviations of\ndown to 0.130 as well as $R^2$ values of up to 84.15%. We show that these\nresults clearly outperform standard approaches.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 22:00:00 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 14:38:24 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Rottmann", "Matthias", ""], ["Colling", "Pascal", ""], ["Hack", "Thomas-Paul", ""], ["Chan", "Robin", ""], ["H\u00fcger", "Fabian", ""], ["Schlicht", "Peter", ""], ["Gottschalk", "Hanno", ""]]}, {"id": "1811.00659", "submitter": "Deren Lei", "authors": "Deren Lei, Zichen Sun, Yijun Xiao, William Yang Wang", "title": "Implicit Regularization of Stochastic Gradient Descent in Natural\n  Language Processing: Observations and Implications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks with remarkably strong generalization performances are\nusually over-parameterized. Despite explicit regularization strategies are used\nfor practitioners to avoid over-fitting, the impacts are often small. Some\ntheoretical studies have analyzed the implicit regularization effect of\nstochastic gradient descent (SGD) on simple machine learning models with\ncertain assumptions. However, how it behaves practically in state-of-the-art\nmodels and real-world datasets is still unknown. To bridge this gap, we study\nthe role of SGD implicit regularization in deep learning systems. We show pure\nSGD tends to converge to minimas that have better generalization performances\nin multiple natural language processing (NLP) tasks. This phenomenon coexists\nwith dropout, an explicit regularizer. In addition, neural network's finite\nlearning capability does not impact the intrinsic nature of SGD's implicit\nregularization effect. Specifically, under limited training samples or with\ncertain corrupted labels, the implicit regularization effect remains strong. We\nfurther analyze the stability by varying the weight initialization range. We\ncorroborate these experimental findings with a decision boundary visualization\nusing a 3-layer neural network for interpretation. Altogether, our work enables\na deepened understanding on how implicit regularization affects the deep\nlearning model and sheds light on the future study of the over-parameterized\nmodel's generalization ability.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 22:24:25 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Lei", "Deren", ""], ["Sun", "Zichen", ""], ["Xiao", "Yijun", ""], ["Wang", "William Yang", ""]]}, {"id": "1811.00669", "submitter": "Rafael Menelau Oliveira E Cruz", "authors": "Rafael M. O. Cruz, George D. C. Cavalcanti, Tsang Ing Ren", "title": "A Method For Dynamic Ensemble Selection Based on a Filter and an\n  Adaptive Distance to Improve the Quality of the Regions of Competence", "comments": "Paper published on IJCNN 2011", "journal-ref": null, "doi": "10.1109/IJCNN.2011.6033350", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic classifier selection systems aim to select a group of classifiers\nthat is most adequate for a specific query pattern. This is done by defining a\nregion around the query pattern and analyzing the competence of the classifiers\nin this region. However, the regions are often surrounded by noise which can\ndifficult the classifier selection. This fact makes the performance of most\ndynamic selection systems no better than static selections. In this paper, we\ndemonstrate that the performance dynamic selection systems end up limited by\nthe quality of the regions extracted. Thereafter, we propose a new dynamic\nclassifier selection that improves the regions of competence in order to\nachieve higher recognition rates. obtained from several classification\ndatabases show the proposed method not only increase the recognition\nperformance but also decreases the computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 23:03:53 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Cruz", "Rafael M. O.", ""], ["Cavalcanti", "George D. C.", ""], ["Ren", "Tsang Ing", ""]]}, {"id": "1811.00677", "submitter": "Rafael Menelau Oliveira E Cruz", "authors": "Rafael M. O. Cruz, Robert Sabourin, George D. C. Cavalcanti", "title": "Analyzing different prototype selection techniques for dynamic\n  classifier and ensemble selection", "comments": null, "journal-ref": "Published on the International Joint Conference on Neural\n  Networks, 2017, 3959-3966", "doi": "10.1109/IJCNN.2017.7966355", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dynamic selection (DS) techniques, only the most competent classifiers,\nfor the classification of a specific test sample are selected to predict the\nsample's class labels. The more important step in DES techniques is estimating\nthe competence of the base classifiers for the classification of each specific\ntest sample. The classifiers' competence is usually estimated using the\nneighborhood of the test sample defined on the validation samples, called the\nregion of competence. Thus, the performance of DS techniques is sensitive to\nthe distribution of the validation set. In this paper, we evaluate six\nprototype selection techniques that work by editing the validation data in\norder to remove noise and redundant instances. Experiments conducted using\nseveral state-of-the-art DS techniques over 30 classification problems\ndemonstrate that by using prototype selection techniques we can improve the\nclassification accuracy of DS techniques and also significantly reduce the\ncomputational cost involved.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 23:34:10 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Cruz", "Rafael M. O.", ""], ["Sabourin", "Robert", ""], ["Cavalcanti", "George D. C.", ""]]}, {"id": "1811.00683", "submitter": "Marius Hofert", "authors": "Marius Hofert and Avinash Prasad and Mu Zhu", "title": "Quasi-random sampling for multivariate distributions via generative\n  neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative moment matching networks (GMMNs) are introduced for generating\nquasi-random samples from multivariate models with any underlying copula in\norder to compute estimates under variance reduction. So far, quasi-random\nsampling for multivariate distributions required a careful design, exploiting\nspecific properties (such as conditional distributions) of the implied\nparametric copula or the underlying quasi-Monte Carlo (QMC) point set, and was\nonly tractable for a small number of models. Utilizing GMMNs allows one to\nconstruct quasi-random samples for a much larger variety of multivariate\ndistributions without such restrictions, including empirical ones from real\ndata with dependence structures not well captured by parametric copulas. Once\ntrained on pseudo-random samples from a parametric model or on real data, these\nneural networks only require a multivariate standard uniform randomized QMC\npoint set as input and are thus fast in estimating expectations of interest\nunder dependence with variance reduction. Numerical examples are considered to\ndemonstrate the approach, including applications inspired by risk management\npractice. All results are reproducible with the demos GMMN_QMC_paper,\nGMMN_QMC_data and GMMN_QMC_timings as part of the R package gnn.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 23:59:41 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 16:11:45 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 21:02:04 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Hofert", "Marius", ""], ["Prasad", "Avinash", ""], ["Zhu", "Mu", ""]]}, {"id": "1811.00686", "submitter": "Martin Jankowiak", "authors": "Martin Jankowiak", "title": "Closed Form Variational Objectives For Bayesian Neural Networks with a\n  Single Hidden Layer", "comments": "Bayesian Deep Learning Workshop @ NeurIPS 2018; 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we consider setups in which variational objectives for Bayesian\nneural networks can be computed in closed form. In particular we focus on\nsingle-layer networks in which the activation function is piecewise polynomial\n(e.g. ReLU). In this case we show that for a Normal likelihood and structured\nNormal variational distributions one can compute a variational lower bound in\nclosed form. In addition we compute the predictive mean and variance in closed\nform. Finally, we also show how to compute approximate lower bounds for other\nlikelihoods (e.g. softmax classification). In experiments we show how the\nresulting variational objectives can help improve training and provide fast\ntest time predictions.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 00:23:12 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 21:24:56 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Jankowiak", "Martin", ""]]}, {"id": "1811.00688", "submitter": "Ruochen Yang", "authors": "Ruochen Yang, Gaurav Gupta, Paul Bogdan", "title": "Data-driven Perception of Neuron Point Process with Unknown Unknowns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of patterns from discrete data time-series for statistical\ninference, threat detection, social opinion dynamics, brain activity prediction\nhas received recent momentum. In addition to the huge data size, the associated\nchallenges are, for example, (i) missing data to construct a closed\ntime-varying complex network, and (ii) contribution of unknown sources which\nare not probed. Towards this end, the current work focuses on statistical\nneuron system model with multi-covariates and unknown inputs. Previous research\nof neuron activity analysis is mainly limited with effects from the spiking\nhistory of target neuron and the interaction with other neurons in the system\nwhile ignoring the influence of unknown stimuli. We propose to use unknown\nunknowns, which describes the effect of unknown stimuli, undetected neuron\nactivities and all other hidden sources of error. The maximum likelihood\nestimation with the fixed-point iteration method is implemented. The\nfixed-point iterations converge fast, and the proposed methods can be\nefficiently parallelized and offer computational advantage especially when the\ninput spiking trains are over long time-horizon. The developed framework\nprovides an intuition into the meaning of having extra degrees-of-freedom in\nthe data to support the need for unknowns. The proposed algorithm is applied to\nsimulated spike trains and on real-world experimental data of mouse\nsomatosensory, mouse retina and cat retina. The model shows a successful\nincreasing of system likelihood with respect to the conditional intensity\nfunction, and it also reveals the convergence with iterations. Results suggest\nthat the neural connection model with unknown unknowns can efficiently estimate\nthe statistical properties of the process by increasing the network likelihood.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 00:42:25 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 06:12:36 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Yang", "Ruochen", ""], ["Gupta", "Gaurav", ""], ["Bogdan", "Paul", ""]]}, {"id": "1811.00703", "submitter": "Gaurav Gupta", "authors": "Gaurav Gupta, Sergio Pequito, Paul Bogdan", "title": "Learning Latent Fractional dynamics with Unknown Unknowns", "comments": "8 pages, 5 figures, American Control Conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant effort in understanding complex systems (CS), we lack a\ntheory for modeling, inference, analysis and efficient control of time-varying\ncomplex networks (TVCNs) in uncertain environments. From brain activity\ndynamics to microbiome, and even chromatin interactions within the genome\narchitecture, many such TVCNs exhibits a pronounced spatio-temporal fractality.\nMoreover, for many TVCNs only limited information (e.g., few variables) is\naccessible for modeling, which hampers the capabilities of analytical tools to\nuncover the true degrees of freedom and infer the CS model, the hidden states\nand their parameters. Another fundamental limitation is that of understanding\nand unveiling of unknown drivers of the dynamics that could sporadically excite\nthe network in ways that straightforward modeling does not work due to our\ninability to model non-stationary processes. Towards addressing these\nchallenges, in this paper, we consider the problem of learning the fractional\ndynamical complex networks under unknown unknowns (i.e., hidden drivers) and\npartial observability (i.e., only partial data is available). More precisely,\nwe consider a generalized modeling approach of TVCNs consisting of\ndiscrete-time fractional dynamical equations and propose an iterative framework\nto determine the network parameterization and predict the state of the system.\nWe showcase the performance of the proposed framework in the context of task\nclassification using real electroencephalogram data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 02:01:11 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 20:14:20 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Gupta", "Gaurav", ""], ["Pequito", "Sergio", ""], ["Bogdan", "Paul", ""]]}, {"id": "1811.00706", "submitter": "Luis Borges", "authors": "Lu\\'is Borges, Bruno Martins, P\\'avel Calado", "title": "Combining Similarity Features and Deep Representation Learning for\n  Stance Detection in the Context of Checking Fake News", "comments": "Accepted for publication in the special issue of the ACM Journal of\n  Data and Information Quality (ACM JDIQ) on Combating Digital Misinformation\n  and Disinformation", "journal-ref": "Journal of Data and Information Quality (JDIQ) 11 (3), 1-26, 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fake news are nowadays an issue of pressing concern, given their recent rise\nas a potential threat to high-quality journalism and well-informed public\ndiscourse. The Fake News Challenge (FNC-1) was organized in 2017 to encourage\nthe development of machine learning-based classification systems for stance\ndetection (i.e., for identifying whether a particular news article agrees,\ndisagrees, discusses, or is unrelated to a particular news headline), thus\nhelping in the detection and analysis of possible instances of fake news. This\narticle presents a new approach to tackle this stance detection problem, based\non the combination of string similarity features with a deep neural\narchitecture that leverages ideas previously advanced in the context of\nlearning efficient text representations, document classification, and natural\nlanguage inference. Specifically, we use bi-directional Recurrent Neural\nNetworks, together with max-pooling over the temporal/sequential dimension and\nneural attention, for representing (i) the headline, (ii) the first two\nsentences of the news article, and (iii) the entire news article. These\nrepresentations are then combined/compared, complemented with similarity\nfeatures inspired on other FNC-1 approaches, and passed to a final layer that\npredicts the stance of the article towards the headline. We also explore the\nuse of external sources of information, specifically large datasets of sentence\npairs originally proposed for training and evaluating natural language\ninference methods, in order to pre-train specific components of the neural\nnetwork architecture (e.g., the RNNs used for encoding sentences). The obtained\nresults attest to the effectiveness of the proposed ideas and show that our\nmodel, particularly when considering pre-training and the combination of neural\nrepresentations together with similarity features, slightly outperforms the\nprevious state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 02:13:52 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Borges", "Lu\u00eds", ""], ["Martins", "Bruno", ""], ["Calado", "P\u00e1vel", ""]]}, {"id": "1811.00707", "submitter": "Boris Ginsburg", "authors": "Jason Li, Ravi Gadde, Boris Ginsburg, Vitaly Lavrukhin", "title": "Training Neural Speech Recognition Systems with Synthetic Speech\n  Augmentation", "comments": "Pre-print. Work in progress, 5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building an accurate automatic speech recognition (ASR) system requires a\nlarge dataset that contains many hours of labeled speech samples produced by a\ndiverse set of speakers. The lack of such open free datasets is one of the main\nissues preventing advancements in ASR research. To address this problem, we\npropose to augment a natural speech dataset with synthetic speech. We train\nvery large end-to-end neural speech recognition models using the LibriSpeech\ndataset augmented with synthetic speech. These new models achieve state of the\nart Word Error Rate (WER) for character-level based models without an external\nlanguage model.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 02:15:46 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Li", "Jason", ""], ["Gadde", "Ravi", ""], ["Ginsburg", "Boris", ""], ["Lavrukhin", "Vitaly", ""]]}, {"id": "1811.00717", "submitter": "He Zhao", "authors": "He Zhao, Lan Du, Wray Buntine, Mingyuan Zhou", "title": "Dirichlet belief networks for topic structure learning", "comments": "accepted in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, considerable research effort has been devoted to developing deep\narchitectures for topic models to learn topic structures. Although several deep\nmodels have been proposed to learn better topic proportions of documents, how\nto leverage the benefits of deep structures for learning word distributions of\ntopics has not yet been rigorously studied. Here we propose a new multi-layer\ngenerative process on word distributions of topics, where each layer consists\nof a set of topics and each topic is drawn from a mixture of the topics of the\nlayer above. As the topics in all layers can be directly interpreted by words,\nthe proposed model is able to discover interpretable topic hierarchies. As a\nself-contained module, our model can be flexibly adapted to different kinds of\ntopic models to improve their modelling accuracy and interpretability.\nExtensive experiments on text corpora demonstrate the advantages of the\nproposed model.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 02:54:39 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Zhao", "He", ""], ["Du", "Lan", ""], ["Buntine", "Wray", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1811.00739", "submitter": "Xuan Zhang", "authors": "Xuan Zhang, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy\n  Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, Marine Carpuat", "title": "An Empirical Exploration of Curriculum Learning for Neural Machine\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation systems based on deep neural networks are expensive to\ntrain. Curriculum learning aims to address this issue by choosing the order in\nwhich samples are presented during training to help train better models faster.\nWe adopt a probabilistic view of curriculum learning, which lets us flexibly\nevaluate the impact of curricula design, and perform an extensive exploration\non a German-English translation task. Results show that it is possible to\nimprove convergence time at no loss in translation quality. However, results\nare highly sensitive to the choice of sample difficulty criteria, curriculum\nschedule and other hyperparameters.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 05:05:26 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Zhang", "Xuan", ""], ["Kumar", "Gaurav", ""], ["Khayrallah", "Huda", ""], ["Murray", "Kenton", ""], ["Gwinnup", "Jeremy", ""], ["Martindale", "Marianna J", ""], ["McNamee", "Paul", ""], ["Duh", "Kevin", ""], ["Carpuat", "Marine", ""]]}, {"id": "1811.00740", "submitter": "Xiaoyu Wang", "authors": "Xiaoyu Wang, Cailian Chen, Yang Min, Jianping He, Bo Yang, Yang Zhang", "title": "Efficient Metropolitan Traffic Prediction Based on Graph Recurrent\n  Neural Network", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic prediction is a fundamental and vital task in Intelligence\nTransportation System (ITS), but it is very challenging to get high accuracy\nwhile containing low computational complexity due to the spatiotemporal\ncharacteristics of traffic flow, especially under the metropolitan\ncircumstances. In this work, a new topological framework, called Linkage\nNetwork, is proposed to model the road networks and present the propagation\npatterns of traffic flow. Based on the Linkage Network model, a novel online\npredictor, named Graph Recurrent Neural Network (GRNN), is designed to learn\nthe propagation patterns in the graph. It could simultaneously predict traffic\nflow for all road segments based on the information gathered from the whole\ngraph, which thus reduces the computational complexity significantly from O(nm)\nto O(n+m), while keeping the high accuracy. Moreover, it can also predict the\nvariations of traffic trends. Experiments based on real-world data demonstrate\nthat the proposed method outperforms the existing prediction methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 05:08:40 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Wang", "Xiaoyu", ""], ["Chen", "Cailian", ""], ["Min", "Yang", ""], ["He", "Jianping", ""], ["Yang", "Bo", ""], ["Zhang", "Yang", ""]]}, {"id": "1811.00741", "submitter": "Pang Wei Koh", "authors": "Pang Wei Koh, Jacob Steinhardt, Percy Liang", "title": "Stronger Data Poisoning Attacks Break Data Sanitization Defenses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models trained on data from the outside world can be\ncorrupted by data poisoning attacks that inject malicious points into the\nmodels' training sets. A common defense against these attacks is data\nsanitization: first filter out anomalous training points before training the\nmodel. Can data poisoning attacks break data sanitization defenses? In this\npaper, we develop three new attacks that can all bypass a broad range of data\nsanitization defenses, including commonly-used anomaly detectors based on\nnearest neighbors, training loss, and singular-value decomposition. For\nexample, our attacks successfully increase the test error on the Enron spam\ndetection dataset from 3% to 24% and on the IMDB sentiment classification\ndataset from 12% to 29% by adding just 3% poisoned data. In contrast, many\nexisting attacks from the literature do not explicitly consider defenses, and\nwe show that those attacks are ineffective in the presence of the defenses we\nconsider. Our attacks are based on two ideas: (i) we coordinate our attacks to\nplace poisoned points near one another, which fools some anomaly detectors, and\n(ii) we formulate each attack as a constrained optimization problem, with\nconstraints designed to ensure that the poisoned points evade detection. While\nthis optimization involves solving an expensive bilevel problem, we explore and\ndevelop three efficient approximations to this problem based on influence\nfunctions; minimax duality; and the Karush-Kuhn-Tucker (KKT) conditions. Our\nresults underscore the urgent need to develop more sophisticated and robust\ndefenses against data poisoning attacks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 05:19:07 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Koh", "Pang Wei", ""], ["Steinhardt", "Jacob", ""], ["Liang", "Percy", ""]]}, {"id": "1811.00749", "submitter": "Shuo Yang", "authors": "Shuo Yang", "title": "Effective Learning of Probabilistic Models for Clinical Predictions from\n  Longitudinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the expeditious advancement of information technologies, health-related\ndata presented unprecedented potentials for medical and health discoveries but\nat the same time significant challenges for machine learning techniques both in\nterms of size and complexity. Those challenges include: the structured data\nwith various storage formats and value types caused by heterogeneous data\nsources; the uncertainty widely existing in every aspect of medical diagnosis\nand treatments; the high dimensionality of the feature space; the longitudinal\nmedical records data with irregular intervals between adjacent observations;\nthe richness of relations existing among objects with similar genetic factors,\nlocation or socio-demographic background. This thesis aims to develop advanced\nStatistical Relational Learning approaches in order to effectively exploit such\nhealth-related data and facilitate the discoveries in medical research. It\npresents the work on cost-sensitive statistical relational learning for mining\nstructured imbalanced data, the first continuous-time probabilistic logic model\nfor predicting sequential events from longitudinal structured data as well as\nhybrid probabilistic relational models for learning from heterogeneous\nstructured data. It also demonstrates the outstanding performance of these\nproposed models as well as other state of the art machine learning models when\napplied to medical research problems and other real-world large-scale systems,\nreveals the great potential of statistical relational learning for exploring\nthe structured health-related data to facilitate medical research.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 05:59:35 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Yang", "Shuo", ""]]}, {"id": "1811.00753", "submitter": "Kartik Ahuja", "authors": "Kartik Ahuja, Mihaela van der Schaar", "title": "Risk-Stratify: Confident Stratification Of Patients Based On Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A clinician desires to use a risk-stratification method that achieves\nconfident risk-stratification - the risk estimates of the different patients\nreflect the true risks with a high probability. This allows him/her to use\nthese risks to make accurate predictions about prognosis and decisions about\nscreening, treatments for the current patient. We develop Risk-stratify - a two\nphase algorithm that is designed to achieve confident risk-stratification. In\nthe first phase, we grow a tree to partition the covariate space. Each node in\nthe tree is split using statistical tests that determine if the risks of the\nchild nodes are different or not. The choice of the statistical tests depends\non whether the data is censored (Log-rank test) or not (U-test). The set of the\nleaves of the tree form a partition. The risk distribution of patients that\nbelong to a leaf is different from the sibling leaf but not the rest of the\nleaves. Therefore, some of the leaves that have similar underlying risks are\nincorrectly specified to have different risks. In the second phase, we develop\na novel recursive graph decomposition approach to address this problem. We\nmerge the leaves of the tree that have similar risks to form new leaves that\nform the final output. We apply Risk-stratify on a cohort of patients (with no\nhistory of cardiovascular disease) from UK Biobank and assess their risk for\ncardiovascular disease. Risk-stratify significantly improves\nrisk-stratification, i.e., a lower fraction of the groups have over/under\nestimated risks (measured in terms of false discovery rate; 33% reduction) in\ncomparison to state-of-the-art methods for cardiovascular prediction (Random\nforests, Cox model, etc.). We find that the Cox model significantly over\nestimates the risk of 21,621 patients out of 216,211 patients. Risk-stratify\ncan accurately categorize 2,987 of these 21,621 patients as low-risk\nindividuals.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 06:30:52 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Ahuja", "Kartik", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1811.00755", "submitter": "Jialin Song", "authors": "Jialin Song, Yuxin Chen, Yisong Yue", "title": "A General Framework for Multi-fidelity Bayesian Optimization with\n  Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we efficiently gather information to optimize an unknown function,\nwhen presented with multiple, mutually dependent information sources with\ndifferent costs? For example, when optimizing a robotic system, intelligently\ntrading off computer simulations and real robot testings can lead to\nsignificant savings. Existing methods, such as multi-fidelity GP-UCB or Entropy\nSearch-based approaches, either make simplistic assumptions on the interaction\namong different fidelities or use simple heuristics that lack theoretical\nguarantees. In this paper, we study multi-fidelity Bayesian optimization with\ncomplex structural dependencies among multiple outputs, and propose\nMF-MI-Greedy, a principled algorithmic framework for addressing this problem.\nIn particular, we model different fidelities using additive Gaussian processes\nbased on shared latent structures with the target function. Then we use\ncost-sensitive mutual information gain for efficient Bayesian global\noptimization. We propose a simple notion of regret which incorporates the cost\nof different fidelities, and prove that MF-MI-Greedy achieves low regret. We\ndemonstrate the strong empirical performance of our algorithm on both synthetic\nand real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 06:36:13 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Song", "Jialin", ""], ["Chen", "Yuxin", ""], ["Yue", "Yisong", ""]]}, {"id": "1811.00761", "submitter": "Riza Ozcelik", "authors": "R{\\i}za \\\"Oz\\c{c}elik, Hakime \\\"Ozt\\\"urk, Arzucan \\\"Ozg\\\"ur, Elif\n  Ozkirimli", "title": "ChemBoost: A chemical language based approach for protein-ligand binding\n  affinity prediction", "comments": "Molecular Informatics", "journal-ref": null, "doi": "10.1002/minf.202000212", "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of high affinity drug-target interactions is a major research\nquestion in drug discovery. Proteins are generally represented by their\nstructures or sequences. However, structures are available only for a small\nsubset of biomolecules and sequence similarity is not always correlated with\nfunctional similarity. We propose ChemBoost, a chemical language based approach\nfor affinity prediction using SMILES syntax. We hypothesize that SMILES is a\ncodified language and ligands are documents composed of chemical words. These\ndocuments can be used to learn chemical word vectors that represent words in\nsimilar contexts with similar vectors. In ChemBoost, the ligands are\nrepresented via chemical word embeddings, while the proteins are represented\nthrough sequence-based features and/or chemical words of their ligands. Our aim\nis to process the patterns in SMILES as a language to predict protein-ligand\naffinity, even when we cannot infer the function from the sequence. We used\neXtreme Gradient Boosting to predict protein-ligand affinities in KIBA and\nBindingDB data sets. ChemBoost was able to predict drug-target binding affinity\nas well as or better than state-of-the-art machine learning systems. When\npowered with ligand-centric representations, ChemBoost was more robust to the\nchanges in protein sequence similarity and successfully captured the\ninteractions between a protein and a ligand, even if the protein has low\nsequence similarity to the known targets of the ligand.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 07:29:56 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 10:28:40 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 21:48:20 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["\u00d6z\u00e7elik", "R\u0131za", ""], ["\u00d6zt\u00fcrk", "Hakime", ""], ["\u00d6zg\u00fcr", "Arzucan", ""], ["Ozkirimli", "Elif", ""]]}, {"id": "1811.00764", "submitter": "Naoki Sakamoto", "authors": "Naoki Sakamoto, Youhei Akimoto", "title": "Adaptive Ranking Based Constraint Handling for Explicitly Constrained\n  Black-Box Optimization", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel explicit constraint handling technique for the covariance matrix\nadaptation evolution strategy (CMA-ES) is proposed. The proposed constraint\nhandling exhibits two invariance properties. One is the invariance to arbitrary\nelement-wise increasing transformation of the objective and constraint\nfunctions. The other is the invariance to arbitrary affine transformation of\nthe search space. The proposed technique virtually transforms a constrained\noptimization problem into an unconstrained optimization problem by considering\nan adaptive weighted sum of the ranking of the objective function values and\nthe ranking of the constraint violations that are measured by the Mahalanobis\ndistance between each candidate solution to its projection onto the boundary of\nthe constraints. Simulation results are presented and show that the CMA-ES with\nthe proposed constraint handling exhibits the affine invariance and performs\nsimilarly to the CMA-ES on unconstrained counterparts.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 07:40:54 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 07:10:15 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Sakamoto", "Naoki", ""], ["Akimoto", "Youhei", ""]]}, {"id": "1811.00778", "submitter": "Jun Jie Sim", "authors": "Ahmad Al Badawi, Jin Chao, Jie Lin, Chan Fook Mun, Jun Jie Sim,\n  Benjamin Hong Meng Tan, Xiao Nan, Khin Mi Mi Aung, Vijay Ramaseshan\n  Chandrasekhar", "title": "Towards the AlexNet Moment for Homomorphic Encryption: HCNN, theFirst\n  Homomorphic CNN on Encrypted Data with GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning as a Service (DLaaS) stands as a promising solution for\ncloud-based inference applications. In this setting, the cloud has a\npre-learned model whereas the user has samples on which she wants to run the\nmodel. The biggest concern with DLaaS is user privacy if the input samples are\nsensitive data. We provide here an efficient privacy-preserving system by\nemploying high-end technologies such as Fully Homomorphic Encryption (FHE),\nConvolutional Neural Networks (CNNs) and Graphics Processing Units (GPUs). FHE,\nwith its widely-known feature of computing on encrypted data, empowers a wide\nrange of privacy-concerned applications. This comes at high cost as it requires\nenormous computing power. In this paper, we show how to accelerate the\nperformance of running CNNs on encrypted data with GPUs. We evaluated two CNNs\nto classify homomorphically the MNIST and CIFAR-10 datasets. Our solution\nachieved a sufficient security level (> 80 bit) and reasonable classification\naccuracy (99%) and (77.55%) for MNIST and CIFAR-10, respectively. In terms of\nlatency, we could classify an image in 5.16 seconds and 304.43 seconds for\nMNIST and CIFAR-10, respectively. Our system can also classify a batch of\nimages (> 8,000) without extra overhead.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 08:44:21 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 04:23:59 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 18:21:38 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Badawi", "Ahmad Al", ""], ["Chao", "Jin", ""], ["Lin", "Jie", ""], ["Mun", "Chan Fook", ""], ["Sim", "Jun Jie", ""], ["Tan", "Benjamin Hong Meng", ""], ["Nan", "Xiao", ""], ["Aung", "Khin Mi Mi", ""], ["Chandrasekhar", "Vijay Ramaseshan", ""]]}, {"id": "1811.00780", "submitter": "Edwin D. Simpson", "authors": "Edwin Simpson and Iryna Gurevych", "title": "A Bayesian Approach for Sequence Tagging with Crowds", "comments": "Accepted for EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Current methods for sequence tagging, a core task in NLP, are data hungry,\nwhich motivates the use of crowdsourcing as a cheap way to obtain labelled\ndata. However, annotators are often unreliable and current aggregation methods\ncannot capture common types of span annotation errors. To address this, we\npropose a Bayesian method for aggregating sequence tags that reduces errors by\nmodelling sequential dependencies between the annotations as well as the\nground-truth labels. By taking a Bayesian approach, we account for uncertainty\nin the model due to both annotator errors and the lack of data for modelling\nannotators who complete few tasks. We evaluate our model on crowdsourced data\nfor named entity recognition, information extraction and argument mining,\nshowing that our sequential model outperforms the previous state of the art. We\nalso find that our approach can reduce crowdsourcing costs through more\neffective active learning, as it better captures uncertainty in the sequence\nlabels when there are few annotations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 08:50:11 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 13:14:02 GMT"}, {"version": "v3", "created": "Fri, 6 Sep 2019 07:31:01 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Simpson", "Edwin", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1811.00784", "submitter": "Jamie Caldwell", "authors": "J. R. Caldwell, R. A. Watson, C. Thies and J. D. Knowles", "title": "Deep Optimisation: Solving Combinatorial Optimisation Problems using\n  Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Optimisation (DO) combines evolutionary search with Deep Neural Networks\n(DNNs) in a novel way - not for optimising a learning algorithm, but for\nfinding a solution to an optimisation problem. Deep learning has been\nsuccessfully applied to classification, regression, decision and generative\ntasks and in this paper we extend its application to solving optimisation\nproblems. Model Building Optimisation Algorithms (MBOAs), a branch of\nevolutionary algorithms, have been successful in combining machine learning\nmethods and evolutionary search but, until now, they have not utilised DNNs. DO\nis the first algorithm to use a DNN to learn and exploit the problem structure\nto adapt the variation operator (changing the neighbourhood structure of the\nsearch process). We demonstrate the performance of DO using two theoretical\noptimisation problems within the MAXSAT class. The Hierarchical Transformation\nOptimisation Problem (HTOP) has controllable deep structure that provides a\nclear evaluation of how DO works and why using a layerwise technique is\nessential for learning and exploiting problem structure. The Parity Modular\nConstraint Problem (MCparity) is a simplistic example of a problem containing\nhigher-order dependencies (greater than pairwise) which DO can solve and state\nof the art MBOAs cannot. Further, we show that DO can exploit deep structure in\nTSP instances. Together these results show that there exists problems that DO\ncan find and exploit deep problem structure that other algorithms cannot.\nMaking this connection between DNNs and optimisation allows for the utilisation\nof advanced tools applicable to DNNs that current MBOAs are unable to use.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 09:04:29 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Caldwell", "J. R.", ""], ["Watson", "R. A.", ""], ["Thies", "C.", ""], ["Knowles", "J. D.", ""]]}, {"id": "1811.00796", "submitter": "Mitsuru Kusumoto", "authors": "Mitsuru Kusumoto, Keisuke Yahata, Masahiro Sakai", "title": "Automated Theorem Proving in Intuitionistic Propositional Logic by Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem-solving in automated theorem proving (ATP) can be interpreted as\na search problem where the prover constructs a proof tree step by step. In this\npaper, we propose a deep reinforcement learning algorithm for proof search in\nintuitionistic propositional logic. The most significant challenge in the\napplication of deep learning to the ATP is the absence of large, public theorem\ndatabase. We, however, overcame this issue by applying a novel data\naugmentation procedure at each iteration of the reinforcement learning. We also\nimprove the efficiency of the algorithm by representing the syntactic structure\nof formulas by a novel compact graph representation. Using the large volume of\naugmented data, we train highly accurate graph neural networks that approximate\nthe value function for the set of the syntactic structures of formulas. Our\nmethod is also cost-efficient in terms of computational time. We will show that\nour prover outperforms Coq's $\\texttt{tauto}$ tactic, a prover based on\nhuman-engineered heuristics. Within the specified time limit, our prover solved\n84% of the theorems in a benchmark library, while $\\texttt{tauto}$ was able to\nsolve only 52%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 09:49:18 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Kusumoto", "Mitsuru", ""], ["Yahata", "Keisuke", ""], ["Sakai", "Masahiro", ""]]}, {"id": "1811.00821", "submitter": "Mireille El Gheche", "authors": "Mireille El Gheche and Giovanni Chierchia and Pascal Frossard", "title": "OrthoNet: Multilayer Network Data Clustering", "comments": null, "journal-ref": "IEEE Transactions on Signal and Information Processing over\n  Networks, 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data appears in very diverse applications, like biological, social,\nor sensor networks. Clustering of network nodes into categories or communities\nhas thus become a very common task in machine learning and data mining. Network\ndata comes with some information about the network edges. In some cases, this\nnetwork information can even be given with multiple views or multiple layers,\neach one representing a different type of relationship between the network\nnodes. Increasingly often, network nodes also carry a feature vector. We\npropose in this paper to extend the node clustering problem, that commonly\nconsiders only the network information, to a problem where both the network\ninformation and the node features are considered together for learning a\nclustering-friendly representation of the feature space. Specifically, we\ndesign a generic two-step algorithm for multilayer network data clustering. The\nfirst step aggregates the different layers of network information into a graph\nrepresentation given by the geometric mean of the network Laplacian matrices.\nThe second step uses a neural net to learn a feature embedding that is\nconsistent with the structure given by the network layers. We propose a novel\nalgorithm for efficiently training the neural net via stochastic gradient\ndescent, which encourages the neural net outputs to span the leading\neigenvectors of the aggregated Laplacian matrix, in order to capture the\npairwise interactions on the network, and provide a clustering-friendly\nrepresentation of the feature space. We demonstrate with an extensive set of\nexperiments on synthetic and real datasets that our method leads to a\nsignificant improvement w.r.t. state-of-the-art multilayer graph clustering\nalgorithms, as it judiciously combines nodes features and network information\nin the node embedding algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 11:12:49 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 13:52:55 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2019 08:25:56 GMT"}, {"version": "v4", "created": "Tue, 2 Jul 2019 12:05:43 GMT"}, {"version": "v5", "created": "Thu, 23 Jan 2020 09:13:20 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Gheche", "Mireille El", ""], ["Chierchia", "Giovanni", ""], ["Frossard", "Pascal", ""]]}, {"id": "1811.00836", "submitter": "Shayan Aziznejad", "authors": "Shayan Aziznejad, Michael Unser", "title": "Multi-Kernel Regression with Sparsity Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a Banach-space formulation of supervised learning\nwith generalized total-variation (gTV) regularization. We identify the class of\nkernel functions that are admissible in this framework. Then, we propose a\nvariation of supervised learning in a continuous-domain hybrid search space\nwith gTV regularization. We show that the solution admits a multi-kernel\nexpansion with adaptive positions. In this representation, the number of active\nkernels is upper-bounded by the number of data points while the gTV\nregularization imposes an $\\ell_1$ penalty on the kernel coefficients. Finally,\nwe illustrate numerically the outcome of our theory.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 12:34:12 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 15:37:03 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2019 14:34:09 GMT"}, {"version": "v4", "created": "Thu, 17 Dec 2020 10:17:35 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Aziznejad", "Shayan", ""], ["Unser", "Michael", ""]]}, {"id": "1811.00839", "submitter": "Jiankai Sun", "authors": "Jiankai Sun, Bortik Bandyopadhyay, Armin Bashizade, Jiongqian Liang,\n  P. Sadayappan and Srinivasan Parthasarathy", "title": "ATP: Directed Graph Embedding with Asymmetric Transitivity Preservation", "comments": "has been accepted to the Thirty-Third AAAI Conference on Artificial\n  Intelligence (AAAI 2019), acceptance rate: 1150/7095 = 16.2%", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed graphs have been widely used in Community Question Answering\nservices (CQAs) to model asymmetric relationships among different types of\nnodes in CQA graphs, e.g., question, answer, user. Asymmetric transitivity is\nan essential property of directed graphs, since it can play an important role\nin downstream graph inference and analysis. Question difficulty and user\nexpertise follow the characteristic of asymmetric transitivity. Maintaining\nsuch properties, while reducing the graph to a lower dimensional vector\nembedding space, has been the focus of much recent research. In this paper, we\ntackle the challenge of directed graph embedding with asymmetric transitivity\npreservation and then leverage the proposed embedding method to solve a\nfundamental task in CQAs: how to appropriately route and assign newly posted\nquestions to users with the suitable expertise and interest in CQAs. The\ntechnique incorporates graph hierarchy and reachability information naturally\nby relying on a non-linear transformation that operates on the core\nreachability and implicit hierarchy within such graphs. Subsequently, the\nmethodology levers a factorization-based approach to generate two embedding\nvectors for each node within the graph, to capture the asymmetric transitivity.\nExtensive experiments show that our framework consistently and significantly\noutperforms the state-of-the-art baselines on two diverse real-world tasks:\nlink prediction, and question difficulty estimation and expert finding in\nonline forums like Stack Exchange. Particularly, our framework can support\ninductive embedding learning for newly posted questions (unseen nodes during\ntraining), and therefore can properly route and assign these kinds of questions\nto experts in CQAs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 12:45:16 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 14:25:49 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Sun", "Jiankai", ""], ["Bandyopadhyay", "Bortik", ""], ["Bashizade", "Armin", ""], ["Liang", "Jiongqian", ""], ["Sadayappan", "P.", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1811.00849", "submitter": "Yasha Singh Ms", "authors": "Yasha Singh, Dhruv Srivastava, P.S. Chandranand, Dr. Surinder Singh", "title": "Algorithms for screening of Cervical Cancer: A chronological review", "comments": "This critical review of various machine learning algorithms for\n  Cervical Cancer Screening was completed at National Institute of\n  Biologicals(NIB), India by B.Tech final year Computer Science students at\n  JSSATE, Noida, India under the supervision of Director at NIB Dr. Surinder\n  Singh and Jr. Scientist Sh. P.S. Chandranand", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are various algorithms and methodologies used for automated screening\nof cervical cancer by segmenting and classifying cervical cancer cells into\ndifferent categories. This study presents a critical review of different\nresearch papers published that integrated AI methods in screening cervical\ncancer via different approaches analyzed in terms of typical metrics like\ndataset size, drawbacks, accuracy etc. An attempt has been made to furnish the\nreader with an insight of Machine Learning algorithms like SVM (Support Vector\nMachines), GLCM (Gray Level Co-occurrence Matrix), k-NN (k-Nearest Neighbours),\nMARS (Multivariate Adaptive Regression Splines), CNNs (Convolutional Neural\nNetworks), spatial fuzzy clustering algorithms, PNNs (Probabilistic Neural\nNetworks), Genetic Algorithm, RFT (Random Forest Trees), C5.0, CART\n(Classification and Regression Trees) and Hierarchical clustering algorithm for\nfeature extraction, cell segmentation and classification. This paper also\ncovers the publicly available datasets related to cervical cancer. It presents\na holistic review on the computational methods that have evolved over the\nperiod of time, in chronological order in detection of malignant cells.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 13:31:27 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Singh", "Yasha", ""], ["Srivastava", "Dhruv", ""], ["Chandranand", "P. S.", ""], ["Singh", "Dr. Surinder", ""]]}, {"id": "1811.00852", "submitter": "Daniel Goldfarb", "authors": "Daniel Goldfarb", "title": "Understanding Deep Neural Networks Using Topological Data Analysis", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) are black box algorithms. They are trained using a\ngradient descent back propagation technique which trains weights in each layer\nfor the sole goal of minimizing training error. Hence, the resulting weights\ncannot be directly explained. Using Topological Data Analysis (TDA) we can get\nan insight on how the neural network is thinking, specifically by analyzing the\nactivation values of validation images as they pass through each layer.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 22:05:11 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Goldfarb", "Daniel", ""]]}, {"id": "1811.00855", "submitter": "Yanqiao Zhu", "authors": "Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, Tieniu Tan", "title": "Session-based Recommendation with Graph Neural Networks", "comments": "9 pages, 4 figures, accepted by AAAI Conference on Artificial\n  Intelligence (AAAI-19)", "journal-ref": null, "doi": "10.1609/aaai.v33i01.3301346", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of session-based recommendation aims to predict user actions\nbased on anonymous sessions. Previous methods model a session as a sequence and\nestimate user representations besides item representations to make\nrecommendations. Though achieved promising results, they are insufficient to\nobtain accurate user vectors in sessions and neglect complex transitions of\nitems. To obtain accurate item embedding and take complex transitions of items\ninto account, we propose a novel method, i.e. Session-based Recommendation with\nGraph Neural Networks, SR-GNN for brevity. In the proposed method, session\nsequences are modeled as graph-structured data. Based on the session graph, GNN\ncan capture complex transitions of items, which are difficult to be revealed by\nprevious conventional sequential methods. Each session is then represented as\nthe composition of the global preference and the current interest of that\nsession using an attention network. Extensive experiments conducted on two real\ndatasets show that SR-GNN evidently outperforms the state-of-the-art\nsession-based recommendation methods consistently.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 02:44:16 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 04:47:18 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2018 04:41:34 GMT"}, {"version": "v4", "created": "Thu, 24 Jan 2019 08:12:19 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Wu", "Shu", ""], ["Tang", "Yuyuan", ""], ["Zhu", "Yanqiao", ""], ["Wang", "Liang", ""], ["Xie", "Xing", ""], ["Tan", "Tieniu", ""]]}, {"id": "1811.00866", "submitter": "Tsui-Wei Weng", "authors": "Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel", "title": "Efficient Neural Network Robustness Certification with General\n  Activation Functions", "comments": "Accepted by NIPS 2018. Huan Zhang and Tsui-Wei Weng contributed\n  equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding minimum distortion of adversarial examples and thus certifying\nrobustness in neural network classifiers for given data points is known to be a\nchallenging problem. Nevertheless, recently it has been shown to be possible to\ngive a non-trivial certified lower bound of minimum adversarial distortion, and\nsome recent progress has been made towards this direction by exploiting the\npiece-wise linear nature of ReLU activations. However, a generic robustness\ncertification for general activation functions still remains largely\nunexplored. To address this issue, in this paper we introduce CROWN, a general\nframework to certify robustness of neural networks with general activation\nfunctions for given input data points. The novelty in our algorithm consists of\nbounding a given activation function with linear and quadratic functions, hence\nallowing it to tackle general activation functions including but not limited to\nfour popular choices: ReLU, tanh, sigmoid and arctan. In addition, we\nfacilitate the search for a tighter certified lower bound by adaptively\nselecting appropriate surrogates for each neuron activation. Experimental\nresults show that CROWN on ReLU networks can notably improve the certified\nlower bounds compared to the current state-of-the-art algorithm Fast-Lin, while\nhaving comparable computational efficiency. Furthermore, CROWN also\ndemonstrates its effectiveness and flexibility on networks with general\nactivation functions, including tanh, sigmoid and arctan.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 14:03:25 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Zhang", "Huan", ""], ["Weng", "Tsui-Wei", ""], ["Chen", "Pin-Yu", ""], ["Hsieh", "Cho-Jui", ""], ["Daniel", "Luca", ""]]}, {"id": "1811.00869", "submitter": "Durmus Sahin", "authors": "Durmus Ozkan Sahin, Erdal Kilic", "title": "Comparison of Classification Algorithms Used Medical Documents\n  Categorization", "comments": "International Conference on Computer Science and Engineering 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volume of text based documents have been increasing day by day. Medical\ndocuments are located within this growing text documents. In this study, the\ntechniques used for text classification applied on medical documents and\nevaluated classification performance. Used data sets are multi class and multi\nlabelled. Chi Square (CHI) technique was used for feature selection also SMO,\nNB, C4.5, RF and KNN algorithms was used for classification. The aim of this\nstudy, success of various classifiers is evaluated on multi class and multi\nlabel data sets consisting of medical documents. The first 400 features, while\nthe most successful in the KNN classifier, feature number 400 and after the SMO\nhas become the most successful classifier.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 14:08:53 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Sahin", "Durmus Ozkan", ""], ["Kilic", "Erdal", ""]]}, {"id": "1811.00883", "submitter": "Bin Liu", "authors": "Bin Liu, Shuai Nie, Yaping Zhang, Shan Liang, Wenju Liu", "title": "Deep Segment Attentive Embedding for Duration Robust Speaker\n  Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSTM-based speaker verification usually uses a fixed-length local segment\nrandomly truncated from an utterance to learn the utterance-level speaker\nembedding, while using the average embedding of all segments of a test\nutterance to verify the speaker, which results in a critical mismatch between\ntesting and training. This mismatch degrades the performance of speaker\nverification, especially when the durations of training and testing utterances\nare very different. To alleviate this issue, we propose the deep segment\nattentive embedding method to learn the unified speaker embeddings for\nutterances of variable duration. Each utterance is segmented by a sliding\nwindow and LSTM is used to extract the embedding of each segment. Instead of\nonly using one local segment, we use the whole utterance to learn the\nutterance-level embedding by applying an attentive pooling to the embeddings of\nall segments. Moreover, the similarity loss of segment-level embeddings is\nintroduced to guide the segment attention to focus on the segments with more\nspeaker discriminations, and jointly optimized with the similarity loss of\nutterance-level embeddings. Systematic experiments on Tongdun and VoxCeleb show\nthat the proposed method significantly improves robustness of duration variant\nand achieves the relative Equal Error Rate reduction of 50% and 11.54% ,\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 01:21:41 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Liu", "Bin", ""], ["Nie", "Shuai", ""], ["Zhang", "Yaping", ""], ["Liang", "Shan", ""], ["Liu", "Wenju", ""]]}, {"id": "1811.00894", "submitter": "Anthony Bagnall Dr", "authors": "James Large and Paul Southam and Anthony Bagnall", "title": "Can automated smoothing significantly improve benchmark time series\n  classification algorithms?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  tl;dr: no, it cannot, at least not on average on the standard archive\nproblems. We assess whether using six smoothing algorithms (moving average,\nexponential smoothing, Gaussian filter, Savitzky-Golay filter, Fourier\napproximation and a recursive median sieve) could be automatically applied to\ntime series classification problems as a preprocessing step to improve the\nperformance of three benchmark classifiers (1-Nearest Neighbour with Euclidean\nand Dynamic Time Warping distances, and Rotation Forest). We found no\nsignificant improvement over unsmoothed data even when we set the smoothing\nparameter through cross validation. We are not claiming smoothing has no worth.\nIt has an important role in exploratory analysis and helps with specific\nclassification problems where domain knowledge can be exploited. What we\nobserve is that the automatic application does not help and that we cannot\nexplain the improvement of other time series classification algorithms over the\nbaseline classifiers simply as a function of the absence of smoothing.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 12:41:24 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Large", "James", ""], ["Southam", "Paul", ""], ["Bagnall", "Anthony", ""]]}, {"id": "1811.00907", "submitter": "Ilia Kulikov", "authors": "Ilia Kulikov, Alexander H. Miller, Kyunghyun Cho, Jason Weston", "title": "Importance of Search and Evaluation Strategies in Neural Dialogue\n  Modeling", "comments": "iNLG 2019 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the impact of search strategies in neural dialogue modeling.\nWe first compare two standard search algorithms, greedy and beam search, as\nwell as our newly proposed iterative beam search which produces a more diverse\nset of candidate responses. We evaluate these strategies in realistic full\nconversations with humans and propose a model-based Bayesian calibration to\naddress annotator bias. These conversations are analyzed using two automatic\nmetrics: log-probabilities assigned by the model and utterance diversity. Our\nexperiments reveal that better search algorithms lead to higher rated\nconversations. However, finding the optimal selection mechanism to choose from\na more diverse set of candidates is still an open question.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 14:54:50 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 10:11:54 GMT"}, {"version": "v3", "created": "Sun, 3 Nov 2019 11:21:56 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Kulikov", "Ilia", ""], ["Miller", "Alexander H.", ""], ["Cho", "Kyunghyun", ""], ["Weston", "Jason", ""]]}, {"id": "1811.00908", "submitter": "Natasa Tagasovska", "authors": "Natasa Tagasovska, David Lopez-Paz", "title": "Single-Model Uncertainties for Deep Learning", "comments": "To appear in NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide single-model estimates of aleatoric and epistemic uncertainty for\ndeep neural networks. To estimate aleatoric uncertainty, we propose\nSimultaneous Quantile Regression (SQR), a loss function to learn all the\nconditional quantiles of a given target variable. These quantiles can be used\nto compute well-calibrated prediction intervals. To estimate epistemic\nuncertainty, we propose Orthonormal Certificates (OCs), a collection of diverse\nnon-constant functions that map all training samples to zero. These\ncertificates map out-of-distribution examples to non-zero values, signaling\nepistemic uncertainty. Our uncertainty estimators are computationally\nattractive, as they do not require ensembling or retraining deep models, and\nachieve competitive performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 14:55:07 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 16:45:59 GMT"}, {"version": "v3", "created": "Fri, 6 Sep 2019 13:38:18 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Tagasovska", "Natasa", ""], ["Lopez-Paz", "David", ""]]}, {"id": "1811.00911", "submitter": "Gaurush Hiranandani", "authors": "Prakhar Gupta, Gaurush Hiranandani, Harvineet Singh, Branislav Kveton,\n  Zheng Wen, Iftikhar Ahamath Burhanuddin", "title": "Online Diverse Learning to Rank from Partial-Click Feedback", "comments": "The first three authors contributed equally to this work. 24 pages, 4\n  figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to rank is an important problem in machine learning and recommender\nsystems. In a recommender system, a user is typically recommended a list of\nitems. Since the user is unlikely to examine the entire recommended list,\npartial feedback arises naturally. At the same time, diverse recommendations\nare important because it is challenging to model all tastes of the user in\npractice. In this paper, we propose the first algorithm for online learning to\nrank diverse items from partial-click feedback. We assume that the user\nexamines the list of recommended items until the user is attracted by an item,\nwhich is clicked, and does not examine the rest of the items. This model of\nuser behavior is known as the cascade model. We propose an online learning\nalgorithm, cascadelsb, for solving our problem. The algorithm actively explores\nthe tastes of the user with the objective of learning to recommend the optimal\ndiverse list. We analyze the algorithm and prove a gap-free upper bound on its\nn-step regret. We evaluate cascadelsb on both synthetic and real-world\ndatasets, compare it to various baselines, and show that it learns even when\nour modeling assumptions do not hold exactly.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 03:10:00 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 17:36:49 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Gupta", "Prakhar", ""], ["Hiranandani", "Gaurush", ""], ["Singh", "Harvineet", ""], ["Kveton", "Branislav", ""], ["Wen", "Zheng", ""], ["Burhanuddin", "Iftikhar Ahamath", ""]]}, {"id": "1811.00915", "submitter": "Matthias Eberlein", "authors": "Matthias Eberlein, Raphael Hildebrand, Ronald Tetzlaff, Nico Hoffmann,\n  Levin Kuhlmann, Benjamin Brinkmann and Jens M\\\"uller", "title": "Convolutional Neural Networks for Epileptic Seizure Prediction", "comments": "accepted for MLESP 2018", "journal-ref": "2018 IEEE International Conference on Bioinformatics and\n  Biomedicine (BIBM)", "doi": "10.1109/BIBM.2018.8621225", "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epilepsy is the most common neurological disorder and an accurate forecast of\nseizures would help to overcome the patient's uncertainty and helplessness. In\nthis contribution, we present and discuss a novel methodology for the\nclassification of intracranial electroencephalography (iEEG) for seizure\nprediction. Contrary to previous approaches, we categorically refrain from an\nextraction of hand-crafted features and use a convolutional neural network\n(CNN) topology instead for both the determination of suitable signal\ncharacteristics and the binary classification of preictal and interictal\nsegments. Three different models have been evaluated on public datasets with\nlong-term recordings from four dogs and three patients. Overall, our findings\ndemonstrate the general applicability. In this work we discuss the strengths\nand limitations of our methodology.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 15:09:18 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 13:58:56 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Eberlein", "Matthias", ""], ["Hildebrand", "Raphael", ""], ["Tetzlaff", "Ronald", ""], ["Hoffmann", "Nico", ""], ["Kuhlmann", "Levin", ""], ["Brinkmann", "Benjamin", ""], ["M\u00fcller", "Jens", ""]]}, {"id": "1811.00928", "submitter": "Micha\\\"el Perrot", "authors": "Debarghya Ghoshdastidar, Micha\\\"el Perrot, Ulrike von Luxburg", "title": "Foundations of Comparison-Based Hierarchical Clustering", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the classical problem of hierarchical clustering, but in a\nframework where one does not have access to a representation of the objects or\ntheir pairwise similarities. Instead, we assume that only a set of comparisons\nbetween objects is available, that is, statements of the form \"objects $i$ and\n$j$ are more similar than objects $k$ and $l$.\" Such a scenario is commonly\nencountered in crowdsourcing applications. The focus of this work is to develop\ncomparison-based hierarchical clustering algorithms that do not rely on the\nprinciples of ordinal embedding. We show that single and complete linkage are\ninherently comparison-based and we develop variants of average linkage. We\nprovide statistical guarantees for the different methods under a planted\nhierarchical partition model. We also empirically demonstrate the performance\nof the proposed approaches on several datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 15:17:48 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 13:37:27 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Ghoshdastidar", "Debarghya", ""], ["Perrot", "Micha\u00ebl", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1811.00937", "submitter": "Alon Talmor", "authors": "Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense\n  Knowledge", "comments": "accepted as a long paper at NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When answering a question, people often draw upon their rich world knowledge\nin addition to the particular context. Recent work has focused primarily on\nanswering questions given some relevant document or context, and required very\nlittle general background. To investigate question answering with prior\nknowledge, we present CommonsenseQA: a challenging new dataset for commonsense\nquestion answering. To capture common sense beyond associations, we extract\nfrom ConceptNet (Speer et al., 2017) multiple target concepts that have the\nsame semantic relation to a single source concept. Crowd-workers are asked to\nauthor multiple-choice questions that mention the source concept and\ndiscriminate in turn between each of the target concepts. This encourages\nworkers to create questions with complex semantics that often require prior\nknowledge. We create 12,247 questions through this procedure and demonstrate\nthe difficulty of our task with a large number of strong baselines. Our best\nbaseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy,\nwell below human performance, which is 89%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 15:34:29 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 18:02:58 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Talmor", "Alon", ""], ["Herzig", "Jonathan", ""], ["Lourie", "Nicholas", ""], ["Berant", "Jonathan", ""]]}, {"id": "1811.00944", "submitter": "Alexander Wein", "authors": "Ankur Moitra and Alexander S. Wein", "title": "Spectral Methods from Tensor Networks", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tensor network is a diagram that specifies a way to \"multiply\" a collection\nof tensors together to produce another tensor (or matrix). Many existing\nalgorithms for tensor problems (such as tensor decomposition and tensor PCA),\nalthough they are not presented this way, can be viewed as spectral methods on\nmatrices built from simple tensor networks. In this work we leverage the full\npower of this abstraction to design new algorithms for certain continuous\ntensor decomposition problems.\n  An important and challenging family of tensor problems comes from orbit\nrecovery, a class of inference problems involving group actions (inspired by\napplications such as cryo-electron microscopy). Orbit recovery problems over\nfinite groups can often be solved via standard tensor methods. However, for\ninfinite groups, no general algorithms are known. We give a new spectral\nalgorithm based on tensor networks for one such problem: continuous\nmulti-reference alignment over the infinite group SO(2). Our algorithm extends\nto the more general heterogeneous case.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 15:52:35 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Moitra", "Ankur", ""], ["Wein", "Alexander S.", ""]]}, {"id": "1811.00956", "submitter": "Shahina Rahman", "authors": "Shahina Rahman and Valen E. Johnson", "title": "A Fast Algorithm for Clustering High Dimensional Feature Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for clustering high dimensional data. If $P$ features\nfor $N$ objects are represented in an $N\\times P$ matrix ${\\bf X}$, where $N\\ll\nP$, the method is based on exploiting the cluster-dependent structure of the\n$N\\times N$ matrix ${\\bf XX}^T$. Computational burden thus depends primarily on\n$N$, the number of objects to be clustered, rather than $P$, the number of\nfeatures that are measured. This makes the method particularly useful in high\ndimensional settings, where it is substantially faster than a number of other\npopular clustering algorithms. Aside from an upper bound on the number of\npotential clusters, the method is independent of tuning parameters. When\ncompared to $16$ other clustering algorithms on $32$ genomic datasets with gold\nstandards, we show that it provides the most accurate cluster configuration\nmore than twice as often than its closest competitors. We illustrate the method\non data taken from highly cited genomic studies.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 16:11:31 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Rahman", "Shahina", ""], ["Johnson", "Valen E.", ""]]}, {"id": "1811.00958", "submitter": "Bo Liu", "authors": "Bo Liu and Luwan Zhang and Ji Liu", "title": "Dantzig Selector with an Approximately Optimal Denoising Matrix and its\n  Application to Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dantzig Selector (DS) is widely used in compressed sensing and sparse\nlearning for feature selection and sparse signal recovery. Since the DS\nformulation is essentially a linear programming optimization, many existing\nlinear programming solvers can be simply applied for scaling up. The DS\nformulation can be explained as a basis pursuit denoising problem, wherein the\ndata matrix (or measurement matrix) is employed as the denoising matrix to\neliminate the observation noise. However, we notice that the data matrix may\nnot be the optimal denoising matrix, as shown by a simple counter-example. This\nmotivates us to pursue a better denoising matrix for defining a general DS\nformulation. We first define the optimal denoising matrix through a minimax\noptimization, which turns out to be an NPhard problem. To make the problem\ncomputationally tractable, we propose a novel algorithm, termed as Optimal\nDenoising Dantzig Selector (ODDS), to approximately estimate the optimal\ndenoising matrix. Empirical experiments validate the proposed method. Finally,\na novel sparse reinforcement learning algorithm is formulated by extending the\nproposed ODDS algorithm to temporal difference learning, and empirical\nexperimental results demonstrate to outperform the conventional vanilla DS-TD\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 16:15:14 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Liu", "Bo", ""], ["Zhang", "Luwan", ""], ["Liu", "Ji", ""]]}, {"id": "1811.00961", "submitter": "Eurika Kaiser", "authors": "Eurika Kaiser and J. Nathan Kutz and Steven L. Brunton", "title": "Discovering conservation laws from data for control", "comments": "7 pages, 2 figures, 57th IEEE Conference on Decision and Control (CDC\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.LG cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conserved quantities, i.e. constants of motion, are critical for\ncharacterizing many dynamical systems in science and engineering. These\nquantities are related to underlying symmetries and they provide fundamental\nknowledge about physical laws, describe the evolution of the system, and enable\nsystem reduction. In this work, we formulate a data-driven architecture for\ndiscovering conserved quantities based on Koopman theory. The Koopman operator\nhas emerged as a principled linear embedding of nonlinear dynamics, and its\neigenfunctions establish intrinsic coordinates along which the dynamics behave\nlinearly. Interestingly, eigenfunctions of the Koopman operator associated with\nvanishing eigenvalues correspond to conserved quantities of the underlying\nsystem. In this paper, we show that these invariants may be identified with\ndata-driven regression and power series expansions, based on the infinitesimal\ngenerator of the Koopman operator. We further establish a connection between\nthe Koopman framework, conserved quantities, and the Lie-Poisson bracket. This\ndata-driven method for discovering conserved quantities is demonstrated on the\nthree-dimensional rigid body equations, where we simultaneously discover the\ntotal energy and angular momentum and use these intrinsic coordinates to\ndevelop a model predictive controller to track a given reference value.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 16:24:34 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Kaiser", "Eurika", ""], ["Kutz", "J. Nathan", ""], ["Brunton", "Steven L.", ""]]}, {"id": "1811.00971", "submitter": "Eren Balevi", "authors": "Eren Balevi and Jeffrey G. Andrews", "title": "One-Bit OFDM Receivers via Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops novel deep learning-based architectures and design\nmethodologies for an orthogonal frequency division multiplexing (OFDM) receiver\nunder the constraint of one-bit complex quantization. Single bit quantization\ngreatly reduces complexity and power consumption, but makes accurate channel\nestimation and data detection difficult. This is particularly true for\nmulticarrier waveforms, which have high peak-to-average ratio in the time\ndomain and fragile subcarrier orthogonality in the frequency domain. The severe\ndistortion for one-bit quantization typically results in an error floor even at\nmoderately low signal-to-noise-ratio (SNR) such as 5 dB. For channel estimation\n(using pilots), we design a novel generative supervised deep neural network\n(DNN) that can be trained with a reasonable number of pilots. After channel\nestimation, a neural network-based receiver -- specifically, an autoencoder --\njointly learns a precoder and decoder for data symbol detection. Since\nquantization prevents end-to-end training, we propose a two-step sequential\ntraining policy for this model. With synthetic data, our deep learning-based\nchannel estimation can outperform least squares (LS) channel estimation for\nunquantized (full-resolution) OFDM at average SNRs up to 14 dB. For data\ndetection, our proposed design achieves lower bit error rate (BER) in fading\nthan unquantized OFDM at average SNRs up to 10 dB.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 16:36:25 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 23:33:46 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Balevi", "Eren", ""], ["Andrews", "Jeffrey G.", ""]]}, {"id": "1811.00972", "submitter": "Naman Deep Singh", "authors": "Naman D. Singh and Abhinav Dhall", "title": "Clustering and Learning from Imbalanced Data", "comments": "9 pages, To Appear at NIPS 2018 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A learning classifier must outperform a trivial solution, in case of\nimbalanced data, this condition usually does not hold true. To overcome this\nproblem, we propose a novel data level resampling method - Clustering Based\nOversampling for improved learning from class imbalanced datasets. The\nessential idea behind the proposed method is to use the distance between a\nminority class sample and its respective cluster centroid to infer the number\nof new sample points to be generated for that minority class sample. The\nproposed algorithm has very less dependence on the technique used for finding\ncluster centroids and does not effect the majority class learning in any way.\nIt also improves learning from imbalanced data by incorporating the\ndistribution structure of minority class samples in generation of new data\nsamples. The newly generated minority class data is handled in a way as to\nprevent outlier production and overfitting. Implementation analysis on\ndifferent datasets using deep neural networks as the learning classifier shows\nthe effectiveness of this method as compared to other synthetic data resampling\ntechniques across several evaluation metrics.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 16:37:09 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 06:05:58 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Singh", "Naman D.", ""], ["Dhall", "Abhinav", ""]]}, {"id": "1811.00974", "submitter": "Pawel Chilinski", "authors": "Pawel Chilinski and Ricardo Silva", "title": "Neural Likelihoods via Cumulative Distribution Functions", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We leverage neural networks as universal approximators of monotonic functions\nto build a parameterization of conditional cumulative distribution functions\n(CDFs). By the application of automatic differentiation with respect to\nresponse variables and then to parameters of this CDF representation, we are\nable to build black box CDF and density estimators. A suite of families is\nintroduced as alternative constructions for the multivariate case. At one\nextreme, the simplest construction is a competitive density estimator against\nstate-of-the-art deep learning methods, although it does not provide an easily\ncomputable representation of multivariate CDFs. At the other extreme, we have a\nflexible construction from which multivariate CDF evaluations and\nmarginalizations can be obtained by a simple forward pass in a deep neural net,\nbut where the computation of the likelihood scales exponentially with\ndimensionality. Alternatives in between the extremes are discussed. We evaluate\nthe different representations empirically on a variety of tasks involving tail\narea probabilities, tail dependence and (partial) density estimation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 16:40:21 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 12:08:43 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Chilinski", "Pawel", ""], ["Silva", "Ricardo", ""]]}, {"id": "1811.00986", "submitter": "Loek Tonnaer", "authors": "Nazly Rocio Santos Buitrago (1), Loek Tonnaer (1), Vlado Menkovski\n  (1), Dimitrios Mavroeidis (2) ((1) Eindhoven University of Technology,\n  Eindhoven, The Netherlands, (2) Royal Philips B.V., Eindhoven, The\n  Netherlands)", "title": "Anomaly Detection for imbalanced datasets with Deep Generative Models", "comments": "15 pages, 13 figures, accepted by Benelearn 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important data analysis applications present with severely imbalanced\ndatasets with respect to the target variable. A typical example is medical\nimage analysis, where positive samples are scarce, while performance is\ncommonly estimated against the correct detection of these positive examples. We\napproach this challenge by formulating the problem as anomaly detection with\ngenerative models. We train a generative model without supervision on the\n`negative' (common) datapoints and use this model to estimate the likelihood of\nunseen data. A successful model allows us to detect the `positive' case as low\nlikelihood datapoints.\n  In this position paper, we present the use of state-of-the-art deep\ngenerative models (GAN and VAE) for the estimation of a likelihood of the data.\nOur results show that on the one hand both GANs and VAEs are able to separate\nthe `positive' and `negative' samples in the MNIST case. On the other hand, for\nthe NLST case, neither GANs nor VAEs were able to capture the complexity of the\ndata and discriminate anomalies at the level that this task requires. These\nresults show that even though there are a number of successes presented in the\nliterature for using generative models in similar applications, there remain\nfurther challenges for broad successful implementation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 17:08:31 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Buitrago", "Nazly Rocio Santos", ""], ["Tonnaer", "Loek", ""], ["Menkovski", "Vlado", ""], ["Mavroeidis", "Dimitrios", ""]]}, {"id": "1811.00995", "submitter": "J\\\"orn-Henrik Jacobsen", "authors": "Jens Behrmann, Will Grathwohl, Ricky T. Q. Chen, David Duvenaud,\n  J\\\"orn-Henrik Jacobsen", "title": "Invertible Residual Networks", "comments": null, "journal-ref": "Proceedings of the International Conference on Machine Learning\n  (ICML), 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that standard ResNet architectures can be made invertible, allowing\nthe same model to be used for classification, density estimation, and\ngeneration. Typically, enforcing invertibility requires partitioning dimensions\nor restricting network architectures. In contrast, our approach only requires\nadding a simple normalization step during training, already available in\nstandard frameworks. Invertible ResNets define a generative model which can be\ntrained by maximum likelihood on unlabeled data. To compute likelihoods, we\nintroduce a tractable approximation to the Jacobian log-determinant of a\nresidual block. Our empirical evaluation shows that invertible ResNets perform\ncompetitively with both state-of-the-art image classifiers and flow-based\ngenerative models, something that has not been previously achieved with a\nsingle architecture.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 17:17:55 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 17:18:26 GMT"}, {"version": "v3", "created": "Sat, 18 May 2019 18:19:33 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Behrmann", "Jens", ""], ["Grathwohl", "Will", ""], ["Chen", "Ricky T. Q.", ""], ["Duvenaud", "David", ""], ["Jacobsen", "J\u00f6rn-Henrik", ""]]}, {"id": "1811.00998", "submitter": "James O' Neill", "authors": "James O' Neill, Danushka Bollegala", "title": "Analysing Dropout and Compounding Errors in Neural Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper carries out an empirical analysis of various dropout techniques\nfor language modelling, such as Bernoulli dropout, Gaussian dropout, Curriculum\nDropout, Variational Dropout and Concrete Dropout. Moreover, we propose an\nextension of variational dropout to concrete dropout and curriculum dropout\nwith varying schedules. We find these extensions to perform well when compared\nto standard dropout approaches, particularly variational curriculum dropout\nwith a linear schedule. Largest performance increases are made when applying\ndropout on the decoder layer. Lastly, we analyze where most of the errors occur\nat test time as a post-analysis step to determine if the well-known problem of\ncompounding errors is apparent and to what end do the proposed methods mitigate\nthis issue for each dataset. We report results on a 2-hidden layer LSTM, GRU\nand Highway network with embedding dropout, dropout on the gated hidden layers\nand the output projection layer for each model. We report our results on\nPenn-TreeBank and WikiText-2 word-level language modelling datasets, where the\nformer reduces the long-tail distribution through preprocessing and one which\npreserves rare words in the training and test set.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 17:31:53 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Neill", "James O'", ""], ["Bollegala", "Danushka", ""]]}, {"id": "1811.01001", "submitter": "Mirac Suzgun", "authors": "Mirac Suzgun, Yonatan Belinkov, Stuart M. Shieber", "title": "On Evaluating the Generalization of LSTM Models in Formal Languages", "comments": "Proceedings of the Society for Computation in Linguistics (SCiL) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are theoretically Turing-complete and\nestablished themselves as a dominant model for language processing. Yet, there\nstill remains an uncertainty regarding their language learning capabilities. In\nthis paper, we empirically evaluate the inductive learning capabilities of Long\nShort-Term Memory networks, a popular extension of simple RNNs, to learn simple\nformal languages, in particular $a^nb^n$, $a^nb^nc^n$, and $a^nb^nc^nd^n$. We\ninvestigate the influence of various aspects of learning, such as training data\nregimes and model capacity, on the generalization to unobserved samples. We\nfind striking differences in model performances under different training\nsettings and highlight the need for careful analysis and assessment when making\nclaims about the learning capabilities of neural network models.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 17:37:39 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Suzgun", "Mirac", ""], ["Belinkov", "Yonatan", ""], ["Shieber", "Stuart M.", ""]]}, {"id": "1811.01017", "submitter": "Emmanouil Theodosis", "authors": "Emmanouil Theodosis and Petros Maragos", "title": "An Adaptive Pruning Algorithm for Spoofing Localisation Based on\n  Tropical Geometry", "comments": "Under review for the International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of spoofing attacks is increasingly relevant as digital systems\nare becoming more ubiquitous. Thus the detection of such attacks and the\nlocalisation of attackers have been objects of recent study. After an attack\nhas been detected, various algorithms have been proposed in order to localise\nthe attacker. In this work we propose a new adaptive pruning algorithm inspired\nby the tropical and geometrical analysis of the traditional Viterbi pruning\nalgorithm to solve the localisation problem. In particular, the proposed\nalgorithm tries to localise the attacker by adapting the leniency parameter\nbased on estimates about the state of the solution space. These estimates stem\nfrom the enclosed volume and the entropy of the solution space, as they were\nintroduced in our previous works.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 18:19:42 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Theodosis", "Emmanouil", ""], ["Maragos", "Petros", ""]]}, {"id": "1811.01027", "submitter": "Lingwei Chen", "authors": "Yanfang Ye, Shifu Hou, Lingwei Chen, Jingwei Lei, Wenqiang Wan, Jiabin\n  Wang, Qi Xiong, Fudong Shao", "title": "AiDroid: When Heterogeneous Information Network Marries Deep Neural\n  Network for Real-time Android Malware Detection", "comments": "The revised version will be published in IJCAI'2019 entitled\n  \"Out-of-sample Node Representation Learning for Heterogeneous Graph in\n  Real-time Android Malware Detection\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive growth and increasing sophistication of Android malware call\nfor new defensive techniques that are capable of protecting mobile users\nagainst novel threats. In this paper, we first extract the runtime Application\nProgramming Interface (API) call sequences from Android apps, and then analyze\nhigher-level semantic relations within the ecosystem to comprehensively\ncharacterize the apps. To model different types of entities (i.e., app, API,\nIMEI, signature, affiliation) and the rich semantic relations among them, we\nthen construct a structural heterogeneous information network (HIN) and present\nmeta-path based approach to depict the relatedness over apps. To efficiently\nclassify nodes (e.g., apps) in the constructed HIN, we propose the HinLearning\nmethod to first obtain in-sample node embeddings and then learn representations\nof out-of-sample nodes without rerunning/adjusting HIN embeddings at the first\nattempt. Afterwards, we design a deep neural network (DNN) classifier taking\nthe learned HIN representations as inputs for Android malware detection. A\ncomprehensive experimental study on the large-scale real sample collections\nfrom Tencent Security Lab is performed to compare various baselines. Promising\nexperimental results demonstrate that our developed system AiDroid which\nintegrates our proposed method outperforms others in real-time Android malware\ndetection. AiDroid has already been incorporated into Tencent Mobile Security\nproduct that serves millions of users worldwide.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 18:11:55 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 20:34:49 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Ye", "Yanfang", ""], ["Hou", "Shifu", ""], ["Chen", "Lingwei", ""], ["Lei", "Jingwei", ""], ["Wan", "Wenqiang", ""], ["Wang", "Jiabin", ""], ["Xiong", "Qi", ""], ["Shao", "Fudong", ""]]}, {"id": "1811.01031", "submitter": "Faiq Khalid", "authors": "Faiq Khalid, Muhammad Abdullah Hanif, Semeen Rehman, Rehan Ahmed,\n  Muhammad Shafique", "title": "TrISec: Training Data-Unaware Imperceptible Security Attacks on Deep\n  Neural Networks", "comments": null, "journal-ref": "2019 IEEE 25th International Symposium on On-Line Testing and\n  Robust System Design (IOLTS), Rhodes, Greece, 2019, pp. 188-193", "doi": "10.1109/IOLTS.2019.8854425", "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the data manipulation attacks on deep neural networks (DNNs) during\nthe training stage introduce a perceptible noise that can be catered by\npreprocessing during inference or can be identified during the validation\nphase. Therefore, data poisoning attacks during inference (e.g., adversarial\nattacks) are becoming more popular. However, many of them do not consider the\nimperceptibility factor in their optimization algorithms, and can be detected\nby correlation and structural similarity analysis, or noticeable (e.g., by\nhumans) in a multi-level security system. Moreover, the majority of the\ninference attack relies on some knowledge about the training dataset. In this\npaper, we propose a novel methodology which automatically generates\nimperceptible attack images by using the back-propagation algorithm on\npre-trained DNNs, without requiring any information about the training dataset\n(i.e., completely training data-unaware). We present a case study on traffic\nsign detection using the VGGNet trained on the German Traffic Sign Recognition\nBenchmarks dataset in an autonomous driving use case. Our results demonstrate\nthat the generated attack images successfully perform misclassification while\nremaining imperceptible in both \"subjective\" and \"objective\" quality tests.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 18:21:17 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 22:51:49 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 10:20:06 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Khalid", "Faiq", ""], ["Hanif", "Muhammad Abdullah", ""], ["Rehman", "Semeen", ""], ["Ahmed", "Rehan", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1811.01054", "submitter": "Kaiyi Ji", "authors": "Kaiyi Ji and Yingbin Liang", "title": "Minimax Estimation of Neural Net Distance", "comments": "To appear in Proc. NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important class of distance metrics proposed for training generative\nadversarial networks (GANs) is the integral probability metric (IPM), in which\nthe neural net distance captures the practical GAN training via two neural\nnetworks. This paper investigates the minimax estimation problem of the neural\nnet distance based on samples drawn from the distributions. We develop the\nfirst known minimax lower bound on the estimation error of the neural net\ndistance, and an upper bound tighter than an existing bound on the estimator\nerror for the empirical neural net distance. Our lower and upper bounds match\nnot only in the order of the sample size but also in terms of the norm of the\nparameter matrices of neural networks, which justifies the empirical neural net\ndistance as a good approximation of the true neural net distance for training\nGANs in practice.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 19:00:27 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Ji", "Kaiyi", ""], ["Liang", "Yingbin", ""]]}, {"id": "1811.01057", "submitter": "Aditi Raghunathan", "authors": "Aditi Raghunathan, Jacob Steinhardt, Percy Liang", "title": "Semidefinite relaxations for certifying robustness to adversarial\n  examples", "comments": "To appear at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their impressive performance on diverse tasks, neural networks fail\ncatastrophically in the presence of adversarial inputs---imperceptibly but\nadversarially perturbed versions of natural inputs. We have witnessed an arms\nrace between defenders who attempt to train robust networks and attackers who\ntry to construct adversarial examples. One promise of ending the arms race is\ndeveloping certified defenses, ones which are provably robust against all\nattackers in some family. These certified defenses are based on convex\nrelaxations which construct an upper bound on the worst case loss over all\nattackers in the family. Previous relaxations are loose on networks that are\nnot trained against the respective relaxation. In this paper, we propose a new\nsemidefinite relaxation for certifying robustness that applies to arbitrary\nReLU networks. We show that our proposed relaxation is tighter than previous\nrelaxations and produces meaningful robustness guarantees on three different\n\"foreign networks\" whose training objectives are agnostic to our proposed\nrelaxation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 19:08:04 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Raghunathan", "Aditi", ""], ["Steinhardt", "Jacob", ""], ["Liang", "Percy", ""]]}, {"id": "1811.01092", "submitter": "Huy Phan", "authors": "Huy Phan, Oliver Y. Ch\\'en, Philipp Koch, Lam Pham, Ian McLoughlin,\n  Alfred Mertins, Maarten De Vos", "title": "Unifying Isolated and Overlapping Audio Event Detection with Multi-Label\n  Multi-Task Convolutional Recurrent Neural Networks", "comments": "Accepted for the 44th International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-label multi-task framework based on a convolutional\nrecurrent neural network to unify detection of isolated and overlapping audio\nevents. The framework leverages the power of convolutional recurrent neural\nnetwork architectures; convolutional layers learn effective features over which\nhigher recurrent layers perform sequential modelling. Furthermore, the output\nlayer is designed to handle arbitrary degrees of event overlap. At each time\nstep in the recurrent output sequence, an output triple is dedicated to each\nevent category of interest to jointly model event occurrence and temporal\nboundaries. That is, the network jointly determines whether an event of this\ncategory occurs, and when it occurs, by estimating onset and offset positions\nat each recurrent time step. We then introduce three sequential losses for\nnetwork training: multi-label classification loss, distance estimation loss,\nand confidence loss. We demonstrate good generalization on two datasets:\nITC-Irst for isolated audio event detection, and TUT-SED-Synthetic-2016 for\noverlapping audio event detection.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 21:16:54 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 21:36:18 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Phan", "Huy", ""], ["Ch\u00e9n", "Oliver Y.", ""], ["Koch", "Philipp", ""], ["Pham", "Lam", ""], ["McLoughlin", "Ian", ""], ["Mertins", "Alfred", ""], ["De Vos", "Maarten", ""]]}, {"id": "1811.01095", "submitter": "Huy Phan", "authors": "Huy Phan, Oliver Y. Ch\\'en, Philipp Koch, Lam Pham, Ian McLoughlin,\n  Alfred Mertins, Maarten De Vos", "title": "Beyond Equal-Length Snippets: How Long is Sufficient to Recognize an\n  Audio Scene?", "comments": "Accepted to 2019 AES Conference on Audio Forensics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the variability in characteristics of audio scenes, some scenes can\nnaturally be recognized earlier than others. In this work, rather than using\nequal-length snippets for all scene categories, as is common in the literature,\nwe study to which temporal extent an audio scene can be reliably recognized\ngiven state-of-the-art models. Moreover, as model fusion with deep network\nensemble is prevalent in audio scene classification, we further study whether,\nand if so, when model fusion is necessary for this task. To achieve these\ngoals, we employ two single-network systems relying on a convolutional neural\nnetwork and a recurrent neural network for classification as well as early\nfusion and late fusion of these networks. Experimental results on the\nLITIS-Rouen dataset show that some scenes can be reliably recognized with a few\nseconds while other scenes require significantly longer durations. In addition,\nmodel fusion is shown to be the most beneficial when the signal length is\nshort.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 21:22:58 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 19:00:37 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Phan", "Huy", ""], ["Ch\u00e9n", "Oliver Y.", ""], ["Koch", "Philipp", ""], ["Pham", "Lam", ""], ["McLoughlin", "Ian", ""], ["Mertins", "Alfred", ""], ["De Vos", "Maarten", ""]]}, {"id": "1811.01118", "submitter": "Priyansh Trivedi", "authors": "Gaurav Maheshwari, Priyansh Trivedi, Denis Lukovnikov, Nilesh\n  Chakraborty, Asja Fischer, Jens Lehmann", "title": "Learning to Rank Query Graphs for Complex Question Answering over\n  Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we conduct an empirical investigation of neural query graph\nranking approaches for the task of complex question answering over knowledge\ngraphs. We experiment with six different ranking models and propose a novel\nself-attention based slot matching model which exploits the inherent structure\nof query graphs, our logical form of choice. Our proposed model generally\noutperforms the other models on two QA datasets over the DBpedia knowledge\ngraph, evaluated in different settings. In addition, we show that transfer\nlearning from the larger of those QA datasets to the smaller dataset yields\nsubstantial improvements, effectively offsetting the general lack of training\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 22:59:31 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Maheshwari", "Gaurav", ""], ["Trivedi", "Priyansh", ""], ["Lukovnikov", "Denis", ""], ["Chakraborty", "Nilesh", ""], ["Fischer", "Asja", ""], ["Lehmann", "Jens", ""]]}, {"id": "1811.01122", "submitter": "Gunnar Carlsson", "authors": "Gunnar Carlsson and Rickard Br\\\"uel Gabrielsson", "title": "Topological Approaches to Deep Learning", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform topological data analysis on the internal states of convolutional\ndeep neural networks to develop an understanding of the computations that they\nperform. We apply this understanding to modify the computations so as to (a)\nspeed up computations and (b) improve generalization from one data set of\ndigits to another. One byproduct of the analysis is the production of a\ngeometry on new sets of features on data sets of images, and use this\nobservation to develop a methodology for constructing analogues of CNN's for\nmany other geometries, including the graph structures constructed by\ntopological data analysis.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 23:18:03 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Carlsson", "Gunnar", ""], ["Gabrielsson", "Rickard Br\u00fcel", ""]]}, {"id": "1811.01124", "submitter": "Marco Cuturi", "authors": "Jean Alaux, Edouard Grave, Marco Cuturi, Armand Joulin", "title": "Unsupervised Hyperalignment for Multilingual Word Embeddings", "comments": "ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of aligning continuous word representations, learned\nin multiple languages, to a common space. It was recently shown that, in the\ncase of two languages, it is possible to learn such a mapping without\nsupervision. This paper extends this line of work to the problem of aligning\nmultiple languages to a common space. A solution is to independently map all\nlanguages to a pivot language. Unfortunately, this degrades the quality of\nindirect word translation. We thus propose a novel formulation that ensures\ncomposable mappings, leading to better alignments. We evaluate our method by\njointly aligning word vectors in eleven languages, showing consistent\nimprovement with indirect mappings while maintaining competitive performance on\ndirect word translation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 23:30:54 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 11:21:19 GMT"}, {"version": "v3", "created": "Tue, 4 Jun 2019 20:58:06 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Alaux", "Jean", ""], ["Grave", "Edouard", ""], ["Cuturi", "Marco", ""], ["Joulin", "Armand", ""]]}, {"id": "1811.01132", "submitter": "Matthew Fellows", "authors": "Matthew Fellows, Anuj Mahajan, Tim G. J. Rudner and Shimon Whiteson", "title": "VIREL: A Variational Inference Framework for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying probabilistic models to reinforcement learning (RL) enables the\napplication of powerful optimisation tools such as variational inference to RL.\nHowever, existing inference frameworks and their algorithms pose significant\nchallenges for learning optimal policies, e.g., the absence of mode capturing\nbehaviour in pseudo-likelihood methods and difficulties learning deterministic\npolicies in maximum entropy RL based approaches. We propose VIREL, a novel,\ntheoretically grounded probabilistic inference framework for RL that utilises a\nparametrised action-value function to summarise future dynamics of the\nunderlying MDP. This gives VIREL a mode-seeking form of KL divergence, the\nability to learn deterministic optimal polices naturally from inference and the\nability to optimise value functions and policies in separate, iterative steps.\nIn applying variational expectation-maximisation to VIREL we thus show that the\nactor-critic algorithm can be reduced to expectation-maximisation, with policy\nimprovement equivalent to an E-step and policy evaluation to an M-step. We then\nderive a family of actor-critic methods from VIREL, including a scheme for\nadaptive exploration. Finally, we demonstrate that actor-critic algorithms from\nthis family outperform state-of-the-art methods based on soft value functions\nin several domains.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 00:15:48 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 09:57:25 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 12:04:20 GMT"}, {"version": "v4", "created": "Sat, 8 Dec 2018 00:37:04 GMT"}, {"version": "v5", "created": "Tue, 29 Jan 2019 19:21:58 GMT"}, {"version": "v6", "created": "Mon, 9 Sep 2019 15:04:25 GMT"}, {"version": "v7", "created": "Tue, 10 Dec 2019 15:10:16 GMT"}, {"version": "v8", "created": "Mon, 20 Jan 2020 19:00:56 GMT"}, {"version": "v9", "created": "Thu, 16 Jul 2020 14:42:28 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Fellows", "Matthew", ""], ["Mahajan", "Anuj", ""], ["Rudner", "Tim G. J.", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1811.01135", "submitter": "Lajanugen Logeswaran", "authors": "Lajanugen Logeswaran, Honglak Lee, Samy Bengio", "title": "Content preserving text generation with attribute controls", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of modifying textual attributes of\nsentences. Given an input sentence and a set of attribute labels, we attempt to\ngenerate sentences that are compatible with the conditioning information. To\nensure that the model generates content compatible sentences, we introduce a\nreconstruction loss which interpolates between auto-encoding and\nback-translation loss components. We propose an adversarial loss to enforce\ngenerated samples to be attribute compatible and realistic. Through\nquantitative, qualitative and human evaluations we demonstrate that our model\nis capable of generating fluent sentences that better reflect the conditioning\ninformation compared to prior methods. We further demonstrate that the model is\ncapable of simultaneously controlling multiple attributes.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 00:29:41 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Logeswaran", "Lajanugen", ""], ["Lee", "Honglak", ""], ["Bengio", "Samy", ""]]}, {"id": "1811.01136", "submitter": "Mikel Artetxe", "authors": "Mikel Artetxe, Holger Schwenk", "title": "Margin-based Parallel Corpus Mining with Multilingual Sentence\n  Embeddings", "comments": "ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation is highly sensitive to the size and quality of the\ntraining data, which has led to an increasing interest in collecting and\nfiltering large parallel corpora. In this paper, we propose a new method for\nthis task based on multilingual sentence embeddings. In contrast to previous\napproaches, which rely on nearest neighbor retrieval with a hard threshold over\ncosine similarity, our proposed method accounts for the scale inconsistencies\nof this measure, considering the margin between a given sentence pair and its\nclosest candidates instead. Our experiments show large improvements over\nexisting methods. We outperform the best published results on the BUCC mining\ntask and the UN reconstruction task by more than 10 F1 and 30 precision points,\nrespectively. Filtering the English-German ParaCrawl corpus with our approach,\nwe obtain 31.2 BLEU points on newstest2014, an improvement of more than one\npoint over the best official filtered version.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 00:34:05 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 22:17:09 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Artetxe", "Mikel", ""], ["Schwenk", "Holger", ""]]}, {"id": "1811.01146", "submitter": "Amanda Sofie Rios", "authors": "Amanda Rios and Laurent Itti", "title": "Closed-Loop Memory GAN for Continual Learning", "comments": "Proceedings of the Twenty-Eighth International Joint Conference on\n  Artificial Intelligence (IJCAI-2019). https://doi.org/10.24963/ijcai.2019/462", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential learning of tasks using gradient descent leads to an unremitting\ndecline in the accuracy of tasks for which training data is no longer\navailable, termed catastrophic forgetting. Generative models have been explored\nas a means to approximate the distribution of old tasks and bypass storage of\nreal data. Here we propose a cumulative closed-loop memory replay GAN (CloGAN)\nprovided with external regularization by a small memory unit selected for\nmaximum sample diversity. We evaluate incremental class learning using a\nnotoriously hard paradigm, single-headed learning, in which each task is a\ndisjoint subset of classes in the overall dataset, and performance is evaluated\non all previous classes. First, we show that when constructing a dynamic memory\nunit to preserve sample heterogeneity, model performance asymptotically\napproaches training on the full dataset. We then show that using a stochastic\ngenerator to continuously output fresh new images during training increases\nperformance significantly further meanwhile generating quality images. We\ncompare our approach to several baselines including fine-tuning by gradient\ndescent (FGD), Elastic Weight Consolidation (EWC), Deep Generative Replay (DGR)\nand Memory Replay GAN (MeRGAN). Our method has very low long-term memory cost,\nthe memory unit, as well as negligible intermediate memory storage.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 02:40:01 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 21:17:48 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 21:44:55 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Rios", "Amanda", ""], ["Itti", "Laurent", ""]]}, {"id": "1811.01158", "submitter": "Lifang He", "authors": "Lifang He, Kun Chen, Wanwan Xu, Jiayu Zhou, Fei Wang", "title": "Boosted Sparse and Low-Rank Tensor Regression", "comments": "10 pages, 5 figures, NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sparse and low-rank tensor regression model to relate a\nunivariate outcome to a feature tensor, in which each unit-rank tensor from the\nCP decomposition of the coefficient tensor is assumed to be sparse. This\nstructure is both parsimonious and highly interpretable, as it implies that the\noutcome is related to the features through a few distinct pathways, each of\nwhich may only involve subsets of feature dimensions. We take a\ndivide-and-conquer strategy to simplify the task into a set of sparse unit-rank\ntensor regression problems. To make the computation efficient and scalable, for\nthe unit-rank tensor regression, we propose a stagewise estimation procedure to\nefficiently trace out its entire solution path. We show that as the step size\ngoes to zero, the stagewise solution paths converge exactly to those of the\ncorresponding regularized regression. The superior performance of our approach\nis demonstrated on various real-world and synthetic examples.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 04:45:57 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["He", "Lifang", ""], ["Chen", "Kun", ""], ["Xu", "Wanwan", ""], ["Zhou", "Jiayu", ""], ["Wang", "Fei", ""]]}, {"id": "1811.01159", "submitter": "Haitao Liu", "authors": "Haitao Liu, Jianfei Cai, Yew-Soon Ong, Yi Wang", "title": "Understanding and Comparing Scalable Gaussian Process Regression for Big\n  Data", "comments": "25 pages, 15 figures, preprint submitted to KBS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a non-parametric Bayesian model which produces informative predictive\ndistribution, Gaussian process (GP) has been widely used in various fields,\nlike regression, classification and optimization. The cubic complexity of\nstandard GP however leads to poor scalability, which poses challenges in the\nera of big data. Hence, various scalable GPs have been developed in the\nliterature in order to improve the scalability while retaining desirable\nprediction accuracy. This paper devotes to investigating the methodological\ncharacteristics and performance of representative global and local scalable GPs\nincluding sparse approximations and local aggregations from four main\nperspectives: scalability, capability, controllability and robustness. The\nnumerical experiments on two toy examples and five real-world datasets with up\nto 250K points offer the following findings. In terms of scalability, most of\nthe scalable GPs own a time complexity that is linear to the training size. In\nterms of capability, the sparse approximations capture the long-term spatial\ncorrelations, the local aggregations capture the local patterns but suffer from\nover-fitting in some scenarios. In terms of controllability, we could improve\nthe performance of sparse approximations by simply increasing the inducing\nsize. But this is not the case for local aggregations. In terms of robustness,\nlocal aggregations are robust to various initializations of hyperparameters due\nto the local attention mechanism. Finally, we highlight that the proper hybrid\nof global and local scalable GPs may be a promising way to improve both the\nmodel capability and scalability for big data.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 04:46:58 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Liu", "Haitao", ""], ["Cai", "Jianfei", ""], ["Ong", "Yew-Soon", ""], ["Wang", "Yi", ""]]}, {"id": "1811.01165", "submitter": "Jiequn Han", "authors": "Jiequn Han, Jihao Long", "title": "Convergence of the Deep BSDE Method for Coupled FBSDEs", "comments": null, "journal-ref": "Probability,Uncertaintyand Quantitative Risk, 5(1), 1-33 (2020)", "doi": "10.1186/s41546-020-00047-w", "report-no": null, "categories": "math.PR cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed numerical algorithm, deep BSDE method, has shown\nremarkable performance in solving high-dimensional forward-backward stochastic\ndifferential equations (FBSDEs) and parabolic partial differential equations\n(PDEs). This article lays a theoretical foundation for the deep BSDE method in\nthe general case of coupled FBSDEs. In particular, a posteriori error\nestimation of the solution is provided and it is proved that the error\nconverges to zero given the universal approximation capability of neural\nnetworks. Numerical results are presented to demonstrate the accuracy of the\nanalyzed algorithm in solving high-dimensional coupled FBSDEs.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 06:08:23 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 04:55:30 GMT"}, {"version": "v3", "created": "Sat, 5 Oct 2019 04:34:43 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Han", "Jiequn", ""], ["Long", "Jihao", ""]]}, {"id": "1811.01171", "submitter": "Mayank Sharma", "authors": "Mayank Sharma, Jayadeva, Sumit Soman", "title": "Radius-margin bounds for deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Explaining the unreasonable effectiveness of deep learning has eluded\nresearchers around the globe. Various authors have described multiple metrics\nto evaluate the capacity of deep architectures. In this paper, we allude to the\nradius margin bounds described for a support vector machine (SVM) with hinge\nloss, apply the same to the deep feed-forward architectures and derive the\nVapnik-Chervonenkis (VC) bounds which are different from the earlier bounds\nproposed in terms of number of weights of the network. In doing so, we also\nrelate the effectiveness of techniques like Dropout and Dropconnect in bringing\ndown the capacity of the network. Finally, we describe the effect of maximizing\nthe input as well as the output margin to achieve an input noise-robust deep\narchitecture.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 07:56:16 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Sharma", "Mayank", ""], ["Jayadeva", "", ""], ["Soman", "Sumit", ""]]}, {"id": "1811.01174", "submitter": "Jian Gao", "authors": "Jian Gao, Deep Chakraborty, Hamidou Tembine, Olaitan Olaleye", "title": "Nonparallel Emotional Speech Conversion", "comments": "Published in INTERSPEECH 2019, 5 pages, 6 figures. Simulation\n  available at http://www.jian-gao.org/emogan", "journal-ref": "https://www.isca-speech.org/archive/Interspeech_2019/abstracts/2878.html", "doi": "10.21437/Interspeech.2019-2878", "report-no": null, "categories": "cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparallel data-driven emotional speech conversion method. It\nenables the transfer of emotion-related characteristics of a speech signal\nwhile preserving the speaker's identity and linguistic content. Most existing\napproaches require parallel data and time alignment, which is not available in\nmost real applications. We achieve nonparallel training based on an\nunsupervised style transfer technique, which learns a translation model between\ntwo distributions instead of a deterministic one-to-one mapping between paired\nexamples. The conversion model consists of an encoder and a decoder for each\nemotion domain. We assume that the speech signal can be decomposed into an\nemotion-invariant content code and an emotion-related style code in latent\nspace. Emotion conversion is performed by extracting and recombining the\ncontent code of the source speech and the style code of the target emotion. We\ntested our method on a nonparallel corpora with four emotions. Both subjective\nand objective evaluations show the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 08:28:04 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 17:04:58 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 08:03:25 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Gao", "Jian", ""], ["Chakraborty", "Deep", ""], ["Tembine", "Hamidou", ""], ["Olaleye", "Olaitan", ""]]}, {"id": "1811.01179", "submitter": "Haitao Liu", "authors": "Haitao Liu, Yew-Soon Ong, Jianfei Cai", "title": "Large-scale Heteroscedastic Regression via Gaussian Process", "comments": "14 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heteroscedastic regression considering the varying noises among observations\nhas many applications in the fields like machine learning and statistics. Here\nwe focus on the heteroscedastic Gaussian process (HGP) regression which\nintegrates the latent function and the noise function together in a unified\nnon-parametric Bayesian framework. Though showing remarkable performance, HGP\nsuffers from the cubic time complexity, which strictly limits its application\nto big data. To improve the scalability, we first develop a variational sparse\ninference algorithm, named VSHGP, to handle large-scale datasets. Furthermore,\ntwo variants are developed to improve the scalability and capability of VSHGP.\nThe first is stochastic VSHGP (SVSHGP) which derives a factorized evidence\nlower bound, thus enhancing efficient stochastic variational inference. The\nsecond is distributed VSHGP (DVSHGP) which (i) follows the Bayesian committee\nmachine formalism to distribute computations over multiple local VSHGP experts\nwith many inducing points; and (ii) adopts hybrid parameters for experts to\nguard against over-fitting and capture local variety. The superiority of DVSHGP\nand SVSHGP as compared to existing scalable heteroscedastic/homoscedastic GPs\nis then extensively verified on various datasets.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 09:04:35 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 11:45:26 GMT"}, {"version": "v3", "created": "Tue, 21 Jan 2020 08:27:58 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Liu", "Haitao", ""], ["Ong", "Yew-Soon", ""], ["Cai", "Jianfei", ""]]}, {"id": "1811.01182", "submitter": "Conghui Tan", "authors": "Conghui Tan, Tong Zhang, Shiqian Ma, Ji Liu", "title": "Stochastic Primal-Dual Method for Empirical Risk Minimization with\n  $\\mathcal{O}(1)$ Per-Iteration Complexity", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized empirical risk minimization problem with linear predictor appears\nfrequently in machine learning. In this paper, we propose a new stochastic\nprimal-dual method to solve this class of problems. Different from existing\nmethods, our proposed methods only require O(1) operations in each iteration.\nWe also develop a variance-reduction variant of the algorithm that converges\nlinearly. Numerical experiments suggest that our methods are faster than\nexisting ones such as proximal SGD, SVRG and SAGA on high-dimensional problems.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 09:18:54 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Tan", "Conghui", ""], ["Zhang", "Tong", ""], ["Ma", "Shiqian", ""], ["Liu", "Ji", ""]]}, {"id": "1811.01198", "submitter": "En-Liang Hu", "authors": "En-Liang Hu", "title": "Low-Rank Semidefinite Programs via Bilinear Factorization", "comments": "This submission also needs major revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning problems can be reduced to learning a low-rank positive\nsemidefinite matrix (denoted as $Z$), which encounters semidefinite program\n(SDP). Existing SDP solvers are often expensive for large-scale learning. To\navoid directly solving SDP, some works convert SDP into a nonconvex program by\nfactorizing $Z$ \\textit{quadraticly} as $XX^\\top$. However, this would bring\nhigher-order nonlinearity, resulting in scarcity of structure in subsequent\noptimization. In this paper, we propose a novel surrogate for SDP learning, in\nwhich the structure of subproblem is exploited. More specifically, we surrogate\nunconstrained SDP by a biconvex problem, through factorizing $Z$\n\\textit{bilinearly} as $XY^\\top$ and using a Courant penalty to penalize the\ndifference of $X$ and $Y$, in which the resultant subproblems in terms of $X$\nand $Y$ are convex respectively. Furthermore, we provide a theoretical bound\nfor the associated penalty parameter under the assumption that the subobjective\nfunction of $X$ or $Y$ is $L$-Lipschitz-smooth and $\\sigma$-strongly convex,\nsuch that the proposed surrogate will solve the original SDP when the penalty\nparameter is larger than this bound (that is $\\gamma>\\frac{1}{4}(L-\\sigma)$).\nExperiments on two SDP-related applications demonstrate that the proposed\nalgorithm is as accurate as the state-of-the-art, but is faster on large-scale\nlearning.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 12:15:42 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 03:32:56 GMT"}, {"version": "v3", "created": "Wed, 25 Dec 2019 03:13:44 GMT"}, {"version": "v4", "created": "Sat, 7 Mar 2020 11:02:05 GMT"}, {"version": "v5", "created": "Wed, 20 Jan 2021 11:19:37 GMT"}, {"version": "v6", "created": "Mon, 1 Feb 2021 06:03:46 GMT"}, {"version": "v7", "created": "Tue, 2 Feb 2021 02:59:16 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Hu", "En-Liang", ""]]}, {"id": "1811.01213", "submitter": "Zhehui Chen", "authors": "Haoming Jiang, Zhehui Chen, Yuyang Shi, Bo Dai, and Tuo Zhao", "title": "Learning to Defend by Learning to Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training provides a principled approach for training robust\nneural networks. From an optimization perspective, adversarial training is\nessentially solving a bilevel optimization problem. The leader problem is\ntrying to learn a robust classifier, while the follower problem is trying to\ngenerate adversarial samples. Unfortunately, such a bilevel problem is\ndifficult to solve due to its highly complicated structure. This work proposes\na new adversarial training method based on a generic learning-to-learn (L2L)\nframework. Specifically, instead of applying existing hand-designed algorithms\nfor the inner problem, we learn an optimizer, which is parametrized as a\nconvolutional neural network. At the same time, a robust classifier is learned\nto defense the adversarial attack generated by the learned optimizer.\nExperiments over CIFAR-10 and CIFAR-100 datasets demonstrate that L2L\noutperforms existing adversarial training methods in both classification\naccuracy and computational efficiency. Moreover, our L2L framework can be\nextended to generative adversarial imitation learning and stabilize the\ntraining.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 13:33:23 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 15:13:28 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 23:48:28 GMT"}, {"version": "v4", "created": "Tue, 10 Mar 2020 22:42:13 GMT"}, {"version": "v5", "created": "Sun, 2 May 2021 14:28:02 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Jiang", "Haoming", ""], ["Chen", "Zhehui", ""], ["Shi", "Yuyang", ""], ["Dai", "Bo", ""], ["Zhao", "Tuo", ""]]}, {"id": "1811.01216", "submitter": "Anindya De", "authors": "Anindya De, Ryan O'Donnell and Rocco Servedio", "title": "Learning sparse mixtures of rankings from noisy information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning an unknown mixture of $k$ rankings over $n$\nelements, given access to noisy samples drawn from the unknown mixture. We\nconsider a range of different noise models, including natural variants of the\n\"heat kernel\" noise framework and the Mallows model. For each of these noise\nmodels we give an algorithm which, under mild assumptions, learns the unknown\nmixture to high accuracy and runs in $n^{O(\\log k)}$ time. The best previous\nalgorithms for closely related problems have running times which are\nexponential in $k$.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 13:36:12 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["De", "Anindya", ""], ["O'Donnell", "Ryan", ""], ["Servedio", "Rocco", ""]]}, {"id": "1811.01225", "submitter": "Xiaoyi Dong", "authors": "Xiaoyi Dong, Weiming Zhang, Nenghai Yu", "title": "CAAD 2018: Powerful None-Access Black-Box Attack Based on Adversarial\n  Transformation Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an improvement of Adversarial Transformation\nNetworks(ATN) to generate adversarial examples, which can fool white-box models\nand black-box models with a state of the art performance and won the 2rd place\nin the non-target task in CAAD 2018.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 14:18:40 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Dong", "Xiaoyi", ""], ["Zhang", "Weiming", ""], ["Yu", "Nenghai", ""]]}, {"id": "1811.01237", "submitter": "Jun Feng", "authors": "Jun Feng, Minlie Huang, Yijie Zhang, Yang Yang and Xiaoyan Zhu", "title": "Relation Mention Extraction from Noisy Data with Hierarchical\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address a task of relation mention extraction from noisy\ndata: extracting representative phrases for a particular relation from noisy\nsentences that are collected via distant supervision. Despite its significance\nand value in many downstream applications, this task is less studied on noisy\ndata. The major challenges exists in 1) the lack of annotation on mention\nphrases, and more severely, 2) handling noisy sentences which do not express a\nrelation at all. To address the two challenges, we formulate the task as a\nsemi-Markov decision process and propose a novel hierarchical reinforcement\nlearning model. Our model consists of a top-level sentence selector to remove\nnoisy sentences, a low-level mention extractor to extract relation mentions,\nand a reward estimator to provide signals to guide data denoising and mention\nextraction without explicit annotations. Experimental results show that our\nmodel is effective to extract relation mentions from noisy data.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 15:50:27 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Feng", "Jun", ""], ["Huang", "Minlie", ""], ["Zhang", "Yijie", ""], ["Yang", "Yang", ""], ["Zhu", "Xiaoyan", ""]]}, {"id": "1811.01247", "submitter": "Jiwoong Im", "authors": "Daniel Jiwoong Im, Nakul Verma, Kristin Branson", "title": "Stochastic Neighbor Embedding under f-divergences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The t-distributed Stochastic Neighbor Embedding (t-SNE) is a powerful and\npopular method for visualizing high-dimensional data. It minimizes the\nKullback-Leibler (KL) divergence between the original and embedded data\ndistributions. In this work, we propose extending this method to other\nf-divergences. We analytically and empirically evaluate the types of latent\nstructure-manifold, cluster, and hierarchical-that are well-captured using both\nthe original KL-divergence as well as the proposed f-divergence generalization,\nand find that different divergences perform better for different types of\nstructure.\n  A common concern with $t$-SNE criterion is that it is optimized using\ngradient descent, and can become stuck in poor local minima. We propose\noptimizing the f-divergence based loss criteria by minimizing a variational\nbound. This typically performs better than optimizing the primal form, and our\nexperiments show that it can improve upon the embedding results obtained from\nthe original $t$-SNE criterion as well.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 16:56:15 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Im", "Daniel Jiwoong", ""], ["Verma", "Nakul", ""], ["Branson", "Kristin", ""]]}, {"id": "1811.01249", "submitter": "Mohammad Kachuee Mr.", "authors": "Mohammad Kachuee, Sajad Darabi, Babak Moatamed, Majid Sarrafzadeh", "title": "Dynamic Feature Acquisition Using Denoising Autoencoders", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 2018", "doi": "10.1109/TNNLS.2018.2880403", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world scenarios, different features have different acquisition costs\nat test-time which necessitates cost-aware methods to optimize the cost and\nperformance trade-off. This paper introduces a novel and scalable approach for\ncost-aware feature acquisition at test-time. The method incrementally asks for\nfeatures based on the available context that are known feature values. The\nproposed method is based on sensitivity analysis in neural networks and density\nestimation using denoising autoencoders with binary representation layers. In\nthe proposed architecture, a denoising autoencoder is used to handle unknown\nfeatures (i.e., features that are yet to be acquired), and the sensitivity of\npredictions with respect to each unknown feature is used as a context-dependent\nmeasure of informativeness. We evaluated the proposed method on eight different\nreal-world datasets as well as one synthesized dataset and compared its\nperformance with several other approaches in the literature. According to the\nresults, the suggested method is capable of efficiently acquiring features at\ntest-time in a cost- and context-aware fashion.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 17:14:28 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Kachuee", "Mohammad", ""], ["Darabi", "Sajad", ""], ["Moatamed", "Babak", ""], ["Sarrafzadeh", "Majid", ""]]}, {"id": "1811.01287", "submitter": "Petar Veli\\v{c}kovi\\'c", "authors": "C\\u{a}t\\u{a}lina Cangea, Petar Veli\\v{c}kovi\\'c, Nikola Jovanovi\\'c,\n  Thomas Kipf, Pietro Li\\`o", "title": "Towards Sparse Hierarchical Graph Classifiers", "comments": "To appear in the Workshop on Relational Representation Learning (R2L)\n  at NIPS 2018. 6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in representation learning on graphs, mainly leveraging graph\nconvolutional networks, have brought a substantial improvement on many\ngraph-based benchmark tasks. While novel approaches to learning node embeddings\nare highly suitable for node classification and link prediction, their\napplication to graph classification (predicting a single label for the entire\ngraph) remains mostly rudimentary, typically using a single global pooling step\nto aggregate node features or a hand-designed, fixed heuristic for hierarchical\ncoarsening of the graph structure. An important step towards ameliorating this\nis differentiable graph coarsening---the ability to reduce the size of the\ngraph in an adaptive, data-dependent manner within a graph neural network\npipeline, analogous to image downsampling within CNNs. However, the previous\nprominent approach to pooling has quadratic memory requirements during training\nand is therefore not scalable to large graphs. Here we combine several recent\nadvances in graph neural network design to demonstrate that competitive\nhierarchical graph classification results are possible without sacrificing\nsparsity. Our results are verified on several established graph classification\nbenchmarks, and highlight an important direction for future research in\ngraph-based neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 21:39:43 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Cangea", "C\u0103t\u0103lina", ""], ["Veli\u010dkovi\u0107", "Petar", ""], ["Jovanovi\u0107", "Nikola", ""], ["Kipf", "Thomas", ""], ["Li\u00f2", "Pietro", ""]]}, {"id": "1811.01302", "submitter": "Peter Henderson", "authors": "Peter Henderson, Koustuv Sinha, Rosemary Nan Ke, Joelle Pineau", "title": "Adversarial Gain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples can be defined as inputs to a model which induce a\nmistake - where the model output is different than that of an oracle, perhaps\nin surprising or malicious ways. Original models of adversarial attacks are\nprimarily studied in the context of classification and computer vision tasks.\nWhile several attacks have been proposed in natural language processing (NLP)\nsettings, they often vary in defining the parameters of an attack and what a\nsuccessful attack would look like. The goal of this work is to propose a\nunifying model of adversarial examples suitable for NLP tasks in both\ngenerative and classification settings. We define the notion of adversarial\ngain: based in control theory, it is a measure of the change in the output of a\nsystem relative to the perturbation of the input (caused by the so-called\nadversary) presented to the learner. This definition, as we show, can be used\nunder different feature spaces and distance conditions to determine attack or\ndefense effectiveness across different intuitive manifolds. This notion of\nadversarial gain not only provides a useful way for evaluating adversaries and\ndefenses, but can act as a building block for future work in robustness under\nadversaries due to its rooted nature in stability and manifold theory.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 00:02:42 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Henderson", "Peter", ""], ["Sinha", "Koustuv", ""], ["Ke", "Rosemary Nan", ""], ["Pineau", "Joelle", ""]]}, {"id": "1811.01305", "submitter": "Yuefeng Liang", "authors": "Yuefeng Liang, Cho-Jui Hsieh, Thomas C.M. Lee", "title": "Block-wise Partitioning for Extreme Multi-label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme multi-label classification aims to learn a classifier that annotates\nan instance with a relevant subset of labels from an extremely large label set.\nMany existing solutions embed the label matrix to a low-dimensional linear\nsubspace, or examine the relevance of a test instance to every label via a\nlinear scan. In practice, however, those approaches can be computationally\nexorbitant. To alleviate this drawback, we propose a Block-wise Partitioning\n(BP) pretreatment that divides all instances into disjoint clusters, to each of\nwhich the most frequently tagged label subset is attached. One multi-label\nclassifier is trained on one pair of instance and label clusters, and the label\nset of a test instance is predicted by first delivering it to the most\nappropriate instance cluster. Experiments on benchmark multi-label data sets\nreveal that BP pretreatment significantly reduces prediction time, and retains\nalmost the same level of prediction accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 00:54:11 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Liang", "Yuefeng", ""], ["Hsieh", "Cho-Jui", ""], ["Lee", "Thomas C. M.", ""]]}, {"id": "1811.01307", "submitter": "Yu-An Chung", "authors": "Yu-An Chung and Wei-Hung Weng and Schrasing Tong and James Glass", "title": "Towards Unsupervised Speech-to-Text Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for building speech-to-text translation (ST) systems\nusing only monolingual speech and text corpora, in other words, speech\nutterances from a source language and independent text from a target language.\nAs opposed to traditional cascaded systems and end-to-end architectures, our\nsystem does not require any labeled data (i.e., transcribed source audio or\nparallel source and target text corpora) during training, making it especially\napplicable to language pairs with very few or even zero bilingual resources.\nThe framework initializes the ST system with a cross-modal bilingual dictionary\ninferred from the monolingual corpora, that maps every source speech segment\ncorresponding to a spoken word to its target text translation. For unseen\nsource speech utterances, the system first performs word-by-word translation on\neach speech segment in the utterance. The translation is improved by leveraging\na language model and a sequence denoising autoencoder to provide prior\nknowledge about the target language. Experimental results show that our\nunsupervised system achieves comparable BLEU scores to supervised end-to-end\nmodels despite the lack of supervision. We also provide an ablation analysis to\nexamine the utility of each component in our system.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 01:23:47 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Chung", "Yu-An", ""], ["Weng", "Wei-Hung", ""], ["Tong", "Schrasing", ""], ["Glass", "James", ""]]}, {"id": "1811.01312", "submitter": "Rahul Aralikatte", "authors": "Shreya Khare, Rahul Aralikatte, Senthil Mani", "title": "Adversarial Black-Box Attacks on Automatic Speech Recognition Systems\n  using Multi-Objective Evolutionary Optimization", "comments": "Published in Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fooling deep neural networks with adversarial input have exposed a\nsignificant vulnerability in the current state-of-the-art systems in multiple\ndomains. Both black-box and white-box approaches have been used to either\nreplicate the model itself or to craft examples which cause the model to fail.\nIn this work, we propose a framework which uses multi-objective evolutionary\noptimization to perform both targeted and un-targeted black-box attacks on\nAutomatic Speech Recognition (ASR) systems. We apply this framework on two ASR\nsystems: Deepspeech and Kaldi-ASR, which increases the Word Error Rates (WER)\nof these systems by upto 980%, indicating the potency of our approach. During\nboth un-targeted and targeted attacks, the adversarial samples maintain a high\nacoustic similarity of 0.98 and 0.97 with the original audio.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 02:05:16 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 09:49:56 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Khare", "Shreya", ""], ["Aralikatte", "Rahul", ""], ["Mani", "Senthil", ""]]}, {"id": "1811.01315", "submitter": "Xilei Zhao", "authors": "Xilei Zhao, Xiang Yan, Alan Yu, Pascal Van Hentenryck", "title": "Modeling Stated Preference for Mobility-on-Demand Transit: A Comparison\n  of Machine Learning and Logit Models", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logit models are usually applied when studying individual travel behavior,\ni.e., to predict travel mode choice and to gain behavioral insights on traveler\npreferences. Recently, some studies have applied machine learning to model\ntravel mode choice and reported higher out-of-sample predictive accuracy than\ntraditional logit models (e.g., multinomial logit). However, little research\nfocuses on comparing the interpretability of machine learning with logit\nmodels. In other words, how to draw behavioral insights from the\nhigh-performance \"black-box\" machine-learning models remains largely unsolved\nin the field of travel behavior modeling.\n  This paper aims at providing a comprehensive comparison between the two\napproaches by examining the key similarities and differences in model\ndevelopment, evaluation, and behavioral interpretation between logit and\nmachine-learning models for travel mode choice modeling. To complement the\ntheoretical discussions, the paper also empirically evaluates the two\napproaches on the stated-preference survey data for a new type of transit\nsystem integrating high-frequency fixed-route services and ridesourcing. The\nresults show that machine learning can produce significantly higher predictive\naccuracy than logit models. Moreover, machine learning and logit models largely\nagree on many aspects of behavioral interpretations. In addition, machine\nlearning can automatically capture the nonlinear relationship between the input\nfeatures and choice outcomes. The paper concludes that there is great potential\nin merging ideas from machine learning and conventional statistical methods to\ndevelop refined models for travel behavior research and suggests some new\nresearch directions.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 02:55:49 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 19:40:52 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Zhao", "Xilei", ""], ["Yan", "Xiang", ""], ["Yu", "Alan", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "1811.01316", "submitter": "Huiling Zhen", "authors": "Hui-Ling Zhen, Xi Lin, Alan Z. Tang, Zhenhua Li, Qingfu Zhang and Sam\n  Kwong", "title": "Nonlinear Collaborative Scheme for Deep Neural Networks", "comments": "11 pages, 3 figures (20 subfigures), prepared to submit to IEEE\n  Trans. on Neural Networks and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional research attributes the improvements of generalization ability\nof deep neural networks either to powerful optimizers or the new network\ndesign. Different from them, in this paper, we aim to link the generalization\nability of a deep network to optimizing a new objective function. To this end,\nwe propose a \\textit{nonlinear collaborative scheme} for deep network training,\nwith the key technique as combining different loss functions in a nonlinear\nmanner. We find that after adaptively tuning the weights of different loss\nfunctions, the proposed objective function can efficiently guide the\noptimization process. What is more, we demonstrate that, from the mathematical\nperspective, the nonlinear collaborative scheme can lead to (i) smaller KL\ndivergence with respect to optimal solutions; (ii) data-driven stochastic\ngradient descent; (iii) tighter PAC-Bayes bound. We also prove that its\nadvantage can be strengthened by nonlinearity increasing. To some extent, we\nbridge the gap between learning (i.e., minimizing the new objective function)\nand generalization (i.e., minimizing a PAC-Bayes bound) in the new scheme. We\nalso interpret our findings through the experiments on Residual Networks and\nDenseNet, showing that our new scheme performs superior to single-loss and\nmulti-loss schemes no matter with randomization or not.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 03:24:29 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Zhen", "Hui-Ling", ""], ["Lin", "Xi", ""], ["Tang", "Alan Z.", ""], ["Li", "Zhenhua", ""], ["Zhang", "Qingfu", ""], ["Kwong", "Sam", ""]]}, {"id": "1811.01336", "submitter": "Rajesh Dachiraju", "authors": "Rajesh Dachiraju", "title": "A Function Fitting Method", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we describe a function fitting method that has potential\napplications in machine learning and also prove relevant theorems. The\ndescribed function fitting method is a convex minimization problem and can be\nsolved using a gradient descent algorithm. We also provide qualitative analysis\non fitness to data of this function fitting method. The function fitting\nproblem is also shown to be a solution of a linear, weak partial differential\nequation(PDE). We describe a way to fit a Sobolev function by giving a method\nto choose the optimal $\\lambda$ parameter. We describe a closed-form solution\nto the derived PDE, which enables the parametrization of the solution function.\nWe describe a simple numerical solution using a gradient descent algorithm,\nthat converges uniformly to the actual solution. As the functional of the\nminimization problem is a quadratic form, there also exists a numerical method\nusing linear algebra. Lastly, we give some numerical examples and also\nnumerically demonstrate its application to a binary classification problem.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 08:36:01 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 01:43:16 GMT"}, {"version": "v3", "created": "Sun, 13 Jan 2019 07:56:54 GMT"}, {"version": "v4", "created": "Sat, 19 Jan 2019 03:34:50 GMT"}, {"version": "v5", "created": "Sat, 14 Dec 2019 12:54:47 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Dachiraju", "Rajesh", ""]]}, {"id": "1811.01338", "submitter": "Ashish Ranjan", "authors": "Ashish Ranjan, Md Shah Fahad, David Fernandez-Baca, Akshay Deepak and\n  Sudhakar Tripathi", "title": "Deep Robust Framework for Protein Function Prediction using\n  Variable-Length Protein Sequences", "comments": null, "journal-ref": "IEEE/ACM Transactions on Computational Biology and Bioinformatics,\n  2019", "doi": "10.1109/TCBB.2019.2911609", "report-no": null, "categories": "cs.LG q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amino acid sequence portrays most intrinsic form of a protein and expresses\nprimary structure of protein. The order of amino acids in a sequence enables a\nprotein to acquire a particular stable conformation that is responsible for the\nfunctions of the protein. This relationship between a sequence and its function\nmotivates the need to analyse the sequences for predicting protein functions.\nEarly generation computational methods using BLAST, FASTA, etc. perform\nfunction transfer based on sequence similarity with existing databases and are\ncomputationally slow. Although machine learning based approaches are fast, they\nfail to perform well for long protein sequences (i.e., protein sequences with\nmore than 300 amino acid residues). In this paper, we introduce a novel method\nfor construction of two separate feature sets for protein sequences based on\nanalysis of 1) single fixed-sized segments and 2) multi-sized segments, using\nbi-directional long short-term memory network. Further, model based on proposed\nfeature set is combined with the state of the art Multi-lable Linear\nDiscriminant Analysis (MLDA) features based model to improve the accuracy.\nExtensive evaluations using separate datasets for biological processes and\nmolecular functions demonstrate promising results for both single-sized and\nmulti-sized segments based feature sets. While former showed an improvement of\n+3.37% and +5.48%, the latter produces an improvement of +5.38% and +8.00%\nrespectively for two datasets over the state of the art MLDA based classifier.\nAfter combining two models, there is a significant improvement of +7.41% and\n+9.21% respectively for two datasets compared to MLDA based classifier.\nSpecifically, the proposed approach performed well for the long protein\nsequences and superior overall performance.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 08:49:38 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 08:37:28 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Ranjan", "Ashish", ""], ["Fahad", "Md Shah", ""], ["Fernandez-Baca", "David", ""], ["Deepak", "Akshay", ""], ["Tripathi", "Sudhakar", ""]]}, {"id": "1811.01339", "submitter": "Ahmadreza Ahmadi", "authors": "Ahmadreza Ahmadi and Jun Tani", "title": "A Novel Predictive-Coding-Inspired Variational RNN Model for Online\n  Prediction and Recognition", "comments": "The paper is accepted in Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study introduces PV-RNN, a novel variational RNN inspired by the\npredictive-coding ideas. The model learns to extract the probabilistic\nstructures hidden in fluctuating temporal patterns by dynamically changing the\nstochasticity of its latent states. Its architecture attempts to address two\nmajor concerns of variational Bayes RNNs: how can latent variables learn\nmeaningful representations and how can the inference model transfer future\nobservations to the latent variables. PV-RNN does both by introducing adaptive\nvectors mirroring the training data, whose values can then be adapted\ndifferently during evaluation. Moreover, prediction errors during\nbackpropagation, rather than external inputs during the forward computation,\nare used to convey information to the network about the external data. For\ntesting, we introduce error regression for predicting unseen sequences as\ninspired by predictive coding that leverages those mechanisms. The model\nintroduces a weighting parameter, the meta-prior, to balance the optimization\npressure placed on two terms of a lower bound on the marginal likelihood of the\nsequential data. We test the model on two datasets with probabilistic\nstructures and show that with high values of the meta-prior the network\ndevelops deterministic chaos through which the data's randomness is imitated.\nFor low values, the model behaves as a random process. The network performs\nbest on intermediate values, and is able to capture the latent probabilistic\nstructure with good generalization. Analyzing the meta-prior's impact on the\nnetwork allows to precisely study the theoretical value and practical benefits\nof incorporating stochastic dynamics in our model. We demonstrate better\nprediction performance on a robot imitation task with our model using error\nregression compared to a standard variational Bayes model lacking such a\nprocedure.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 08:56:50 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 14:59:40 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 02:08:58 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Ahmadi", "Ahmadreza", ""], ["Tani", "Jun", ""]]}, {"id": "1811.01376", "submitter": "Kohki Mametani", "authors": "Kohki Mametani, Tsuneo Kato, Seiichi Yamamoto", "title": "Investigating context features hidden in End-to-End TTS", "comments": "Accepted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have introduced end-to-end TTS, which integrates the\nproduction of context and acoustic features in statistical parametric speech\nsynthesis. As a result, a single neural network replaced laborious feature\nengineering with automated feature learning. However, little is known about\nwhat types of context information end-to-end TTS extracts from text input\nbefore synthesizing speech, and the previous knowledge about context features\nis barely utilized. In this work, we first point out the model similarity\nbetween end-to-end TTS and parametric TTS. Based on the similarity, we evaluate\nthe quality of encoder outputs from an end-to-end TTS system against eight\ncriteria that are derived from a standard set of context information used in\nparametric TTS. We conduct experiments using an evaluation procedure that has\nbeen newly developed in the machine learning literature for quantitative\nanalysis of neural representations, while adapting it to the TTS domain.\nExperimental results show that the encoder outputs reflect both linguistic and\nphonetic contexts, such as vowel reduction at phoneme level, lexical stress at\nsyllable level, and part-of-speech at word level, possibly due to the joint\noptimization of context and acoustic features.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 14:22:37 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 14:55:07 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Mametani", "Kohki", ""], ["Kato", "Tsuneo", ""], ["Yamamoto", "Seiichi", ""]]}, {"id": "1811.01382", "submitter": "Kai Hu", "authors": "Kai Hu, Zhijian Ou, Min Hu, Junlan Feng", "title": "Neural CRF transducers for sequence labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional random fields (CRFs) have been shown to be one of the most\nsuccessful approaches to sequence labeling. Various linear-chain neural CRFs\n(NCRFs) are developed to implement the non-linear node potentials in CRFs, but\nstill keeping the linear-chain hidden structure. In this paper, we propose NCRF\ntransducers, which consists of two RNNs, one extracting features from\nobservations and the other capturing (theoretically infinite) long-range\ndependencies between labels. Different sequence labeling methods are evaluated\nover POS tagging, chunking and NER (English, Dutch). Experiment results show\nthat NCRF transducers achieve consistent improvements over linear-chain NCRFs\nand RNN transducers across all the four tasks, and can improve state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 14:45:50 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Hu", "Kai", ""], ["Ou", "Zhijian", ""], ["Hu", "Min", ""], ["Feng", "Junlan", ""]]}, {"id": "1811.01394", "submitter": "Koichi Tojo", "authors": "Koichi Tojo, Taro Yoshino", "title": "A method to construct exponential families by representation theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give a method to construct \"good\" exponential families\nsystematically by representation theory. More precisely, we consider a\nhomogeneous space $G/H$ as a sample space and construct an exponential family\ninvariant under the transformation group $G$ by using a representation of $G$.\nThe method generates widely used exponential families such as normal, gamma,\nBernoulli, categorical, Wishart, von Mises, Fisher-Bingham and hyperboloid\ndistributions.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 16:08:07 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 12:17:34 GMT"}, {"version": "v3", "created": "Sat, 17 Aug 2019 08:05:07 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Tojo", "Koichi", ""], ["Yoshino", "Taro", ""]]}, {"id": "1811.01437", "submitter": "Faiq Khalid", "authors": "Faiq Khalid, Hassan Ali, Hammad Tariq, Muhammad Abdullah Hanif, Semeen\n  Rehman, Rehan Ahmed and Muhammad Shafique", "title": "QuSecNets: Quantization-based Defense Mechanism for Securing Deep Neural\n  Network against Adversarial Attacks", "comments": null, "journal-ref": "2019 IEEE 25th International Symposium on On-Line Testing and\n  Robust System Design (IOLTS), Rhodes, Greece, 2019, pp. 182-187", "doi": "10.1109/IOLTS.2019.8854377", "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples have emerged as a significant threat to machine learning\nalgorithms, especially to the convolutional neural networks (CNNs). In this\npaper, we propose two quantization-based defense mechanisms, Constant\nQuantization (CQ) and Trainable Quantization (TQ), to increase the robustness\nof CNNs against adversarial examples. CQ quantizes input pixel intensities\nbased on a \"fixed\" number of quantization levels, while in TQ, the quantization\nlevels are \"iteratively learned during the training phase\", thereby providing a\nstronger defense mechanism. We apply the proposed techniques on undefended CNNs\nagainst different state-of-the-art adversarial attacks from the open-source\n\\textit{Cleverhans} library. The experimental results demonstrate 50%-96% and\n10%-50% increase in the classification accuracy of the perturbed images\ngenerated from the MNIST and the CIFAR-10 datasets, respectively, on commonly\nused CNN (Conv2D(64, 8x8) - Conv2D(128, 6x6) - Conv2D(128, 5x5) - Dense(10) -\nSoftmax()) available in \\textit{Cleverhans} library.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 21:25:38 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 09:30:01 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Khalid", "Faiq", ""], ["Ali", "Hassan", ""], ["Tariq", "Hammad", ""], ["Hanif", "Muhammad Abdullah", ""], ["Rehman", "Semeen", ""], ["Ahmed", "Rehan", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1811.01442", "submitter": "Zhao Song", "authors": "Zhao Song, David P. Woodruff, Peilin Zhong", "title": "Towards a Zero-One Law for Column Subset Selection", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are a number of approximation algorithms for NP-hard versions of low\nrank approximation, such as finding a rank-$k$ matrix $B$ minimizing the sum of\nabsolute values of differences to a given $n$-by-$n$ matrix $A$,\n$\\min_{\\textrm{rank-}k~B}\\|A-B\\|_1$, or more generally finding a rank-$k$\nmatrix $B$ which minimizes the sum of $p$-th powers of absolute values of\ndifferences, $\\min_{\\textrm{rank-}k~B}\\|A-B\\|_p^p$. Many of these algorithms\nare linear time columns subset selection algorithms, returning a subset of\n$\\mathrm{poly}(k \\log n)$ columns whose cost is no more than a\n$\\mathrm{poly}(k)$ factor larger than the cost of the best rank-$k$ matrix. The\nabove error measures are special cases of the following general entrywise low\nrank approximation problem: given an arbitrary function $g:\\mathbb{R}\n\\rightarrow \\mathbb{R}_{\\geq 0}$, find a rank-$k$ matrix $B$ which minimizes\n$\\|A-B\\|_g = \\sum_{i,j}g(A_{i,j}-B_{i,j})$. A natural question is which\nfunctions $g$ admit efficient approximation algorithms? Indeed, this is a\ncentral question of recent work studying generalized low rank models. In this\nwork we give approximation algorithms for $\\textit{every}$ function $g$ which\nis approximately monotone and satisfies an approximate triangle inequality, and\nwe show both of these conditions are necessary. Further, our algorithm is\nefficient if the function $g$ admits an efficient approximate regression\nalgorithm. Our approximation algorithms handle functions which are not even\nscale-invariant, such as the Huber loss function, which we show have very\ndifferent structural properties than $\\ell_p$-norms, e.g., one can show the\nlack of scale-invariance causes any column subset selection algorithm to\nprovably require a $\\sqrt{\\log n}$ factor larger number of columns than\n$\\ell_p$-norms; nevertheless we design the first efficient column subset\nselection algorithms for such error measures.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 21:43:55 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 23:53:54 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Song", "Zhao", ""], ["Woodruff", "David P.", ""], ["Zhong", "Peilin", ""]]}, {"id": "1811.01443", "submitter": "Faiq Khalid", "authors": "Hassan Ali, Faiq Khalid, Hammad Tariq, Muhammad Abdullah Hanif, Semeen\n  Rehman, Rehan Ahmed and Muhammad Shafique", "title": "SSCNets: Robustifying DNNs using Secure Selective Convolutional Filters", "comments": null, "journal-ref": "IEEE Design & Test, vol. 37, no. 2, pp. 58-65, April 2020", "doi": "10.1109/MDAT.2019.2961325", "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel technique based on the Secure Selective\nConvolutional (SSC) techniques in the training loop that increases the\nrobustness of a given DNN by allowing it to learn the data distribution based\non the important edges in the input image. We validate our technique on\nConvolutional DNNs against the state-of-the-art attacks from the open-source\nCleverhans library using the MNIST, the CIFAR-10, and the CIFAR-100 datasets.\nOur experimental results show that the attack success rate, as well as the\nimperceptibility of the adversarial images, can be significantly reduced by\nadding effective pre-processing functions, i.e., Sobel filtering.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 21:54:11 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 03:44:31 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Ali", "Hassan", ""], ["Khalid", "Faiq", ""], ["Tariq", "Hammad", ""], ["Hanif", "Muhammad Abdullah", ""], ["Rehman", "Semeen", ""], ["Ahmed", "Rehan", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1811.01444", "submitter": "Faiq Khalid", "authors": "Faiq Khalid, Muhammmad Abdullah Hanif, Semeen Rehman, Junaid Qadir,\n  Muhammad Shafique", "title": "FAdeML: Understanding the Impact of Pre-Processing Noise Filtering on\n  Adversarial Machine Learning", "comments": "Accepted in Design, Automation and Test in Europe 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN)-based machine learning (ML) algorithms have\nrecently emerged as the leading ML paradigm particularly for the task of\nclassification due to their superior capability of learning efficiently from\nlarge datasets. The discovery of a number of well-known attacks such as dataset\npoisoning, adversarial examples, and network manipulation (through the addition\nof malicious nodes) has, however, put the spotlight squarely on the lack of\nsecurity in DNN-based ML systems. In particular, malicious actors can use these\nwell-known attacks to cause random/targeted misclassification, or cause a\nchange in the prediction confidence, by only slightly but systematically\nmanipulating the environmental parameters, inference data, or the data\nacquisition block. Most of the prior adversarial attacks have, however, not\naccounted for the pre-processing noise filters commonly integrated with the\nML-inference module. Our contribution in this work is to show that this is a\nmajor omission since these noise filters can render ineffective the majority of\nthe existing attacks, which rely essentially on introducing adversarial noise.\nApart from this, we also extend the state of the art by proposing a novel\npre-processing noise Filter-aware Adversarial ML attack called FAdeML. To\ndemonstrate the effectiveness of the proposed methodology, we generate an\nadversarial attack image by exploiting the \"VGGNet\" DNN trained for the \"German\nTraffic Sign Recognition Benchmarks (GTSRB\" dataset, which despite having no\nvisual noise, can cause a classifier to misclassify even in the presence of\npre-processing noise filters.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 21:56:33 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Khalid", "Faiq", ""], ["Hanif", "Muhammmad Abdullah", ""], ["Rehman", "Semeen", ""], ["Qadir", "Junaid", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1811.01457", "submitter": "Viral Shah", "authors": "Michael Innes, Elliot Saba, Keno Fischer, Dhairya Gandhi, Marco\n  Concetto Rudilosso, Neethu Mariya Joy, Tejan Karmali, Avik Pal, Viral Shah", "title": "Fashionable Modelling with Flux", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning as a discipline has seen an incredible surge of interest in\nrecent years due in large part to a perfect storm of new theory, superior\ntooling, renewed interest in its capabilities. We present in this paper a\nframework named Flux that shows how further refinement of the core ideas of\nmachine learning, built upon the foundation of the Julia programming language,\ncan yield an environment that is simple, easily modifiable, and performant. We\ndetail the fundamental principles of Flux as a framework for differentiable\nprogramming, give examples of models that are implemented within Flux to\ndisplay many of the language and framework-level features that contribute to\nits ease of use and high productivity, display internal compiler techniques\nused to enable the acceleration and performance that lies at the heart of Flux,\nand finally give an overview of the larger ecosystem that Flux fits inside of.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 00:32:27 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 15:04:35 GMT"}, {"version": "v3", "created": "Sat, 10 Nov 2018 18:49:35 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Innes", "Michael", ""], ["Saba", "Elliot", ""], ["Fischer", "Keno", ""], ["Gandhi", "Dhairya", ""], ["Rudilosso", "Marco Concetto", ""], ["Joy", "Neethu Mariya", ""], ["Karmali", "Tejan", ""], ["Pal", "Avik", ""], ["Shah", "Viral", ""]]}, {"id": "1811.01458", "submitter": "Jakob Foerster", "authors": "Jakob N. Foerster, Francis Song, Edward Hughes, Neil Burch, Iain\n  Dunning, Shimon Whiteson, Matthew Botvinick, Michael Bowling", "title": "Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When observing the actions of others, humans make inferences about why they\nacted as they did, and what this implies about the world; humans also use the\nfact that their actions will be interpreted in this manner, allowing them to\nact informatively and thereby communicate efficiently with others. Although\nlearning algorithms have recently achieved superhuman performance in a number\nof two-player, zero-sum games, scalable multi-agent reinforcement learning\nalgorithms that can discover effective strategies and conventions in complex,\npartially observable settings have proven elusive. We present the Bayesian\naction decoder (BAD), a new multi-agent learning method that uses an\napproximate Bayesian update to obtain a public belief that conditions on the\nactions taken by all agents in the environment. BAD introduces a new Markov\ndecision process, the public belief MDP, in which the action space consists of\nall deterministic partial policies, and exploits the fact that an agent acting\nonly on this public belief state can still learn to use its private information\nif the action space is augmented to be over all partial policies mapping\nprivate information into environment actions. The Bayesian update is closely\nrelated to the theory of mind reasoning that humans carry out when observing\nothers' actions. We first validate BAD on a proof-of-principle two-step matrix\ngame, where it outperforms policy gradient methods; we then evaluate BAD on the\nchallenging, cooperative partial-information card game Hanabi, where, in the\ntwo-player setting, it surpasses all previously published learning and\nhand-coded approaches, establishing a new state of the art.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 23:43:54 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 16:51:31 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2019 21:21:00 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Foerster", "Jakob N.", ""], ["Song", "Francis", ""], ["Hughes", "Edward", ""], ["Burch", "Neil", ""], ["Dunning", "Iain", ""], ["Whiteson", "Shimon", ""], ["Botvinick", "Matthew", ""], ["Bowling", "Michael", ""]]}, {"id": "1811.01459", "submitter": "Xinshao Wang Mr", "authors": "Xinshao Wang, Yang Hua, Elyor Kodirov, Guosheng Hu, Neil M. Robertson", "title": "Deep Metric Learning by Online Soft Mining and Class-Aware Attention", "comments": "Learning Robust Representations, Deep Metric Learning, Person\n  Re-identification (AAAI 2019 Oral) Code:\n  https://github.com/XinshaoAmosWang/OSM_CAA_WeightedContrastiveLoss", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning aims to learn a deep embedding that can capture the\nsemantic similarity of data points. Given the availability of massive training\nsamples, deep metric learning is known to suffer from slow convergence due to a\nlarge fraction of trivial samples. Therefore, most existing methods generally\nresort to sample mining strategies for selecting nontrivial samples to\naccelerate convergence and improve performance. In this work, we identify two\ncritical limitations of the sample mining methods, and provide solutions for\nboth of them. First, previous mining methods assign one binary score to each\nsample, i.e., dropping or keeping it, so they only selects a subset of relevant\nsamples in a mini-batch. Therefore, we propose a novel sample mining method,\ncalled Online Soft Mining (OSM), which assigns one continuous score to each\nsample to make use of all samples in the mini-batch. OSM learns extended\nmanifolds that preserve useful intraclass variances by focusing on more similar\npositives. Second, the existing methods are easily influenced by outliers as\nthey are generally included in the mined subset. To address this, we introduce\nClass-Aware Attention (CAA) that assigns little attention to abnormal data\nsamples. Furthermore, by combining OSM and CAA, we propose a novel weighted\ncontrastive loss to learn discriminative embeddings. Extensive experiments on\ntwo fine-grained visual categorisation datasets and two video-based person\nre-identification benchmarks show that our method significantly outperforms the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 23:47:18 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 20:40:29 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 22:36:39 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Wang", "Xinshao", ""], ["Hua", "Yang", ""], ["Kodirov", "Elyor", ""], ["Hu", "Guosheng", ""], ["Robertson", "Neil M.", ""]]}, {"id": "1811.01463", "submitter": "Faiq Khalid", "authors": "Faiq Khalid, Muhammad Abdullah Hanif, Semeen Rehman, Muhammad Shafique", "title": "Security for Machine Learning-based Systems: Attacks and Challenges\n  during Training and Inference", "comments": null, "journal-ref": "International Conference on Frontiers of Information Technology\n  (FIT) 2018", "doi": "10.1109/FIT.2018.00064", "report-no": "INSPEC Accession Number: 18398499", "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential increase in dependencies between the cyber and physical world\nleads to an enormous amount of data which must be efficiently processed and\nstored. Therefore, computing paradigms are evolving towards machine learning\n(ML)-based systems because of their ability to efficiently and accurately\nprocess the enormous amount of data. Although ML-based solutions address the\nefficient computing requirements of big data, they introduce (new) security\nvulnerabilities into the systems, which cannot be addressed by traditional\nmonitoring-based security measures. Therefore, this paper first presents a\nbrief overview of various security threats in machine learning, their\nrespective threat models and associated research challenges to develop robust\nsecurity measures. To illustrate the security vulnerabilities of ML during\ntraining, inferencing and hardware implementation, we demonstrate some key\nsecurity threats on ML using LeNet and VGGNet for MNIST and German Traffic Sign\nRecognition Benchmarks (GTSRB), respectively. Moreover, based on the security\nanalysis of ML-training, we also propose an attack that has a very less impact\non the inference accuracy. Towards the end, we highlight the associated\nresearch challenges in developing security measures and provide a brief\noverview of the techniques used to mitigate such security threats.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 00:30:21 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Khalid", "Faiq", ""], ["Hanif", "Muhammad Abdullah", ""], ["Rehman", "Semeen", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1811.01464", "submitter": "Ke Sun", "authors": "Ke Sun", "title": "Intrinsic Universal Measurements of Non-linear Embeddings", "comments": "work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic problem in machine learning is to find a mapping $f$ from a low\ndimensional latent space to a high dimensional observation space. Equipped with\nthe representation power of non-linearity, a learner can easily find a mapping\nwhich perfectly fits all the observations. However such a mapping is often not\nconsidered as good as it is not simple enough and over-fits. How to define\nsimplicity? This paper tries to make such a formal definition of the amount of\ninformation imposed by a non-linear mapping. This definition is based on\ninformation geometry and is independent of observations, nor specific\nparametrizations. We prove these basic properties and discuss relationships\nwith parametric and non-parametric embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 00:32:28 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Sun", "Ke", ""]]}, {"id": "1811.01466", "submitter": "Vu Nguyen", "authors": "Vu Nguyen, Sunil Gupta, Santu Rana, Cheng Li, Svetha Venkatesh", "title": "Practical Batch Bayesian Optimization for Less Expensive Functions", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization (BO) and its batch extensions are successful for\noptimizing expensive black-box functions. However, these traditional BO\napproaches are not yet ideal for optimizing less expensive functions when the\ncomputational cost of BO can dominate the cost of evaluating the blackbox\nfunction. Examples of these less expensive functions are cheap machine learning\nmodels, inexpensive physical experiment through simulators, and acquisition\nfunction optimization in Bayesian optimization. In this paper, we consider a\nbatch BO setting for situations where function evaluations are less expensive.\nOur model is based on a new exploration strategy using geometric distance that\nprovides an alternative way for exploration, selecting a point far from the\nobserved locations. Using that intuition, we propose to use Sobol sequence to\nguide exploration that will get rid of running multiple global optimization\nsteps as used in previous works. Based on the proposed distance exploration, we\npresent an efficient batch BO approach. We demonstrate that our approach\noutperforms other baselines and global optimization methods when the function\nevaluations are less expensive.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 00:49:31 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Nguyen", "Vu", ""], ["Gupta", "Sunil", ""], ["Rana", "Santu", ""], ["Li", "Cheng", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1811.01476", "submitter": "Alexander Wong", "authors": "Mohammad Saeed Shafiee, Mohammad Javad Shafiee, and Alexander Wong", "title": "Dynamic Representations Toward Efficient Inference on Deep Neural\n  Networks by Decision Gates", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks extract rich features from the input data, the\ncurrent trade-off between depth and computational cost makes it difficult to\nadopt deep neural networks for many industrial applications, especially when\ncomputing power is limited. Here, we are inspired by the idea that, while\ndeeper embeddings are needed to discriminate difficult samples (i.e.,\nfine-grained discrimination), a large number of samples can be well\ndiscriminated via much shallower embeddings (i.e., coarse-grained\ndiscrimination). In this study, we introduce the simple yet effective concept\nof decision gates (d-gate), modules trained to decide whether a sample needs to\nbe projected into a deeper embedding or if an early prediction can be made at\nthe d-gate, thus enabling the computation of dynamic representations at\ndifferent depths. The proposed d-gate modules can be integrated with any deep\nneural network and reduces the average computational cost of the deep neural\nnetworks while maintaining modeling accuracy. The proposed d-gate framework is\nexamined via different network architectures and datasets, with experimental\nresults showing that leveraging the proposed d-gate modules led to a ~43%\nspeed-up and 44% FLOPs reduction on ResNet-101 and 55% speed-up and 39% FLOPs\nreduction on DenseNet-201 trained on the CIFAR10 dataset with only ~2% drop in\naccuracy. Furthermore, experiments where d-gate modules are integrated into\nResNet-101 trained on the ImageNet dataset demonstrate that it is possible to\nreduce the computational cost of the network by 1.5 GFLOPs without any drop in\nthe modeling accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 01:37:39 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 02:55:19 GMT"}, {"version": "v3", "created": "Wed, 7 Nov 2018 02:47:56 GMT"}, {"version": "v4", "created": "Sat, 11 May 2019 16:35:49 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Shafiee", "Mohammad Saeed", ""], ["Shafiee", "Mohammad Javad", ""], ["Wong", "Alexander", ""]]}, {"id": "1811.01480", "submitter": "Jixue Liu", "authors": "Jixue Liu, Jiuyong Li, Lin Liu, Thuc Duy Le, Feiyue Ye, Gefei Li", "title": "FairMod - Making Predictive Models Discrimination Aware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive models such as decision trees and neural networks may produce\ndiscrimination in their predictions. This paper proposes a method to\npost-process the predictions of a predictive model to make the processed\npredictions non-discriminatory. The method considers multiple protected\nvariables together. Multiple protected variables make the problem more\nchallenging than a simple protected variable. The method uses a well-cited\ndiscrimination metric and adapts it to allow the specification of explanatory\nvariables, such as position, profession, education, that describe the contexts\nof the applications. It models the post-processing of predictions problem as a\nnonlinear optimization problem to find best adjustments to the predictions so\nthat the discrimination constraints of all protected variables are all met at\nthe same time. The proposed method is independent of classification methods. It\ncan handle the cases that existing methods cannot handle: satisfying multiple\nprotected attributes at the same time, allowing multiple explanatory\nattributes, and being independent of classification model types. An evaluation\nusing four real world data sets shows that the proposed method is as\neffectively as existing methods, in addition to its extra power.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 02:07:03 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Liu", "Jixue", ""], ["Li", "Jiuyong", ""], ["Liu", "Lin", ""], ["Le", "Thuc Duy", ""], ["Ye", "Feiyue", ""], ["Li", "Gefei", ""]]}, {"id": "1811.01483", "submitter": "Jongwook Choi", "authors": "Jongwook Choi, Yijie Guo, Marcin Moczulski, Junhyuk Oh, Neal Wu,\n  Mohammad Norouzi, Honglak Lee", "title": "Contingency-Aware Exploration in Reinforcement Learning", "comments": "In ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates whether learning contingency-awareness and\ncontrollable aspects of an environment can lead to better exploration in\nreinforcement learning. To investigate this question, we consider an\ninstantiation of this hypothesis evaluated on the Arcade Learning Element\n(ALE). In this study, we develop an attentive dynamics model (ADM) that\ndiscovers controllable elements of the observations, which are often associated\nwith the location of the character in Atari games. The ADM is trained in a\nself-supervised fashion to predict the actions taken by the agent. The learned\ncontingency information is used as a part of the state representation for\nexploration purposes. We demonstrate that combining actor-critic algorithm with\ncount-based exploration using our representation achieves impressive results on\na set of notoriously challenging Atari games due to sparse rewards. For\nexample, we report a state-of-the-art score of >11,000 points on Montezuma's\nRevenge without using expert demonstrations, explicit high-level information\n(e.g., RAM states), or supervisory data. Our experiments confirm that\ncontingency-awareness is indeed an extremely powerful concept for tackling\nexploration problems in reinforcement learning and opens up interesting\nresearch questions for further investigations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 02:12:11 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 05:12:36 GMT"}, {"version": "v3", "created": "Mon, 4 Mar 2019 18:55:24 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Choi", "Jongwook", ""], ["Guo", "Yijie", ""], ["Moczulski", "Marcin", ""], ["Oh", "Junhyuk", ""], ["Wu", "Neal", ""], ["Norouzi", "Mohammad", ""], ["Lee", "Honglak", ""]]}, {"id": "1811.01501", "submitter": "Zhouchen Lin", "authors": "Jia Li, Cong Fang, Zhouchen Lin", "title": "Lifted Proximal Operator Machines", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new optimization method for training feed-forward neural\nnetworks. By rewriting the activation function as an equivalent proximal\noperator, we approximate a feed-forward neural network by adding the proximal\noperators to the objective function as penalties, hence we call the lifted\nproximal operator machine (LPOM). LPOM is block multi-convex in all layer-wise\nweights and activations. This allows us to use block coordinate descent to\nupdate the layer-wise weights and activations in parallel. Most notably, we\nonly use the mapping of the activation function itself, rather than its\nderivatives, thus avoiding the gradient vanishing or blow-up issues in gradient\nbased training methods. So our method is applicable to various non-decreasing\nLipschitz continuous activation functions, which can be saturating and\nnon-differentiable. LPOM does not require more auxiliary variables than the\nlayer-wise activations, thus using roughly the same amount of memory as\nstochastic gradient descent (SGD) does. We further prove the convergence of\nupdating the layer-wise weights and activations. Experiments on MNIST and\nCIFAR-10 datasets testify to the advantages of LPOM.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 03:33:24 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Li", "Jia", ""], ["Fang", "Cong", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1811.01506", "submitter": "Connie Kou", "authors": "Connie Kou, Hwee Kuan Lee, Jorge Sanz, Teck Khim Ng", "title": "Theoretical and Experimental Analysis on the Generalizability of\n  Distribution Regression Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is emerging interest in performing regression between distributions. In\ncontrast to prediction on single instances, these machine learning methods can\nbe useful for population-based studies or on problems that are inherently\nstatistical in nature. The recently proposed distribution regression network\n(DRN) has shown superior performance for the distribution-to-distribution\nregression task compared to conventional neural networks. However, in Kou et\nal. (2018) and some other works on distribution regression, there is a lack of\ncomprehensive comparative study on both theoretical basis and generalization\nabilities of the methods. We derive some mathematical properties of DRN and\nqualitatively compare it to conventional neural networks. We also perform\ncomprehensive experiments to study the generalizability of distribution\nregression models, by studying their robustness to limited training data, data\nsampling noise and task difficulty. DRN consistently outperforms conventional\nneural networks, requiring fewer training data and maintaining robust\nperformance with noise. Furthermore, the theoretical properties of DRN can be\nused to provide some explanation on the ability of DRN to achieve better\ngeneralization performance than conventional neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 04:09:49 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 06:56:39 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 07:23:21 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Kou", "Connie", ""], ["Lee", "Hwee Kuan", ""], ["Sanz", "Jorge", ""], ["Ng", "Teck Khim", ""]]}, {"id": "1811.01531", "submitter": "Efthymios Tzinis", "authors": "Efthymios Tzinis, Shrikant Venkataramani and Paris Smaragdis", "title": "Unsupervised Deep Clustering for Source Separation: Direct Learning from\n  Mixtures using Spatial Information", "comments": "Submitted to ICASSP 2019 (v1: November 5th 2018)", "journal-ref": "ICASSP 2019 - 2019 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP)", "doi": "10.1109/ICASSP.2019.8683201", "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a monophonic source separation system that is trained by only\nobserving mixtures with no ground truth separation information. We use a deep\nclustering approach which trains on multi-channel mixtures and learns to\nproject spectrogram bins to source clusters that correlate with various spatial\nfeatures. We show that using such a training process we can obtain separation\nperformance that is as good as making use of ground truth separation\ninformation. Once trained, this system is capable of performing sound\nseparation on monophonic inputs, despite having learned how to do so using\nmulti-channel recordings.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 07:00:12 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 07:39:34 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Tzinis", "Efthymios", ""], ["Venkataramani", "Shrikant", ""], ["Smaragdis", "Paris", ""]]}, {"id": "1811.01533", "submitter": "Hassan Ismail Fawaz", "authors": "Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane\n  Idoumghar, Pierre-Alain Muller", "title": "Transfer learning for time series classification", "comments": "Accepted at IEEE International Conference on Big Data 2018", "journal-ref": null, "doi": "10.1109/BigData.2018.8621990", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning for deep neural networks is the process of first training a\nbase network on a source dataset, and then transferring the learned features\n(the network's weights) to a second network to be trained on a target dataset.\nThis idea has been shown to improve deep neural network's generalization\ncapabilities in many computer vision tasks such as image recognition and object\nlocalization. Apart from these applications, deep Convolutional Neural Networks\n(CNNs) have also recently gained popularity in the Time Series Classification\n(TSC) community. However, unlike for image recognition problems, transfer\nlearning techniques have not yet been investigated thoroughly for the TSC task.\nThis is surprising as the accuracy of deep learning models for TSC could\npotentially be improved if the model is fine-tuned from a pre-trained neural\nnetwork instead of training it from scratch. In this paper, we fill this gap by\ninvestigating how to transfer deep CNNs for the TSC task. To evaluate the\npotential of transfer learning, we performed extensive experiments using the\nUCR archive which is the largest publicly available TSC benchmark containing 85\ndatasets. For each dataset in the archive, we pre-trained a model and then\nfine-tuned it on the other datasets resulting in 7140 different deep neural\nnetworks. These experiments revealed that transfer learning can improve or\ndegrade the model's predictions depending on the dataset used for transfer.\nTherefore, in an effort to predict the best source dataset for a given target\ndataset, we propose a new method relying on Dynamic Time Warping to measure\ninter-datasets similarities. We describe how our method can guide the transfer\nto choose the best source dataset leading to an improvement in accuracy on 71\nout of 85 datasets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 07:06:32 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Fawaz", "Hassan Ismail", ""], ["Forestier", "Germain", ""], ["Weber", "Jonathan", ""], ["Idoumghar", "Lhassane", ""], ["Muller", "Pierre-Alain", ""]]}, {"id": "1811.01545", "submitter": "Ping Guo", "authors": "P. Guo, K. Wang, and X. L. Zhou", "title": "PILAE: A Non-gradient Descent Learning Scheme for Deep Feedforward\n  Neural Networks", "comments": "This work is our effort toward to realize AutoML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a non-gradient descent learning scheme is proposed for deep\nfeedforward neural networks (DNN). As we known, autoencoder can be used as the\nbuilding blocks of the multi-layer perceptron (MLP) deep neural network. So,\nthe MLP will be taken as an example to illustrate the proposed scheme of\npseudoinverse learning algorithm for autoencoder (PILAE) training. The PILAE\nwith low rank approximation is a non-gradient based learning algorithm, and the\nencoder weight matrix is set to be the low rank approximation of the\npseudoinverse of the input matrix, while the decoder weight matrix is\ncalculated by the pseudoinverse learning algorithm. It is worth to note that\nonly few network structure hyperparameters need to be tuned. Hence, the\nproposed algorithm can be regarded as a quasi-automated training algorithm\nwhich can be utilized in autonomous machine learning research field. The\nexperimental results show that the proposed learning scheme for DNN can achieve\nbetter performance on considering the tradeoff between training efficiency and\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 08:14:11 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 01:30:10 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Guo", "P.", ""], ["Wang", "K.", ""], ["Zhou", "X. L.", ""]]}, {"id": "1811.01557", "submitter": "Chin-Chia Michael Yeh", "authors": "Chin-Chia Michael Yeh, Yan Zhu, Evangelos E. Papalexakis, Abdullah\n  Mueen, Eamonn Keogh", "title": "Representation Learning by Reconstructing Neighborhoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its introduction, unsupervised representation learning has attracted a\nlot of attention from the research community, as it is demonstrated to be\nhighly effective and easy-to-apply in tasks such as dimension reduction,\nclustering, visualization, information retrieval, and semi-supervised learning.\nIn this work, we propose a novel unsupervised representation learning framework\ncalled neighbor-encoder, in which domain knowledge can be easily incorporated\ninto the learning process without modifying the general encoder-decoder\narchitecture of the classic autoencoder.In contrast to autoencoder, which\nreconstructs the input data itself, neighbor-encoder reconstructs the input\ndata's neighbors. As the proposed representation learning problem is\nessentially a neighbor reconstruction problem, domain knowledge can be easily\nincorporated in the form of an appropriate definition of similarity between\nobjects. Based on that observation, our framework can leverage any\noff-the-shelf similarity search algorithms or side information to find the\nneighbor of an input object. Applications of other algorithms (e.g.,\nassociation rule mining) in our framework are also possible, given that the\nappropriate definition of neighbor can vary in different contexts. We have\ndemonstrated the effectiveness of our framework in many diverse domains,\nincluding images, text, and time series, and for various data mining tasks\nincluding classification, clustering, and visualization. Experimental results\nshow that neighbor-encoder not only outperforms autoencoder in most of the\nscenarios we consider, but also achieves the state-of-the-art performance on\ntext document clustering.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 08:56:21 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 06:31:16 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Yeh", "Chin-Chia Michael", ""], ["Zhu", "Yan", ""], ["Papalexakis", "Evangelos E.", ""], ["Mueen", "Abdullah", ""], ["Keogh", "Eamonn", ""]]}, {"id": "1811.01558", "submitter": "Qianxiao Li", "authors": "Qianxiao Li, Cheng Tai, Weinan E", "title": "Stochastic Modified Equations and Dynamics of Stochastic Gradient\n  Algorithms I: Mathematical Foundations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the mathematical foundations of the stochastic modified equations\n(SME) framework for analyzing the dynamics of stochastic gradient algorithms,\nwhere the latter is approximated by a class of stochastic differential\nequations with small noise parameters. We prove that this approximation can be\nunderstood mathematically as an weak approximation, which leads to a number of\nprecise and useful results on the approximations of stochastic gradient descent\n(SGD), momentum SGD and stochastic Nesterov's accelerated gradient method in\nthe general setting of stochastic objectives. We also demonstrate through\nexplicit calculations that this continuous-time approach can uncover important\nanalytical insights into the stochastic gradient algorithms under consideration\nthat may not be easy to obtain in a purely discrete-time setting.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 09:00:29 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Li", "Qianxiao", ""], ["Tai", "Cheng", ""], ["E", "Weinan", ""]]}, {"id": "1811.01564", "submitter": "Nikolas Ioannou", "authors": "Nikolas Ioannou, Celestine D\\\"unner, Kornilios Kourtis, and Thomas\n  Parnell", "title": "Parallel training of linear models without compromising convergence", "comments": "Presented at the Workshop on Systems for ML and Open Source Software\n  at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze, evaluate, and improve the performance of training\ngeneralized linear models on modern CPUs. We start with a state-of-the-art\nasynchronous parallel training algorithm, identify system-level performance\nbottlenecks, and apply optimizations that improve data parallelism, cache line\nlocality, and cache line prefetching of the algorithm. These modifications\nreduce the per-epoch run-time significantly, but take a toll on algorithm\nconvergence in terms of the required number of epochs. To alleviate these\nshortcomings of our systems-optimized version, we propose a novel, dynamic data\npartitioning scheme across threads which allows us to approach the convergence\nof the sequential version. The combined set of optimizations result in a\nconsistent bottom line speedup in convergence of up to 12x compared to the\ninitial asynchronous parallel training algorithm and up to 42x, compared to\nstate of the art implementations (scikit-learn and h2o) on a range of\nmulti-core CPU architectures.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 09:23:41 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 16:12:12 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Ioannou", "Nikolas", ""], ["D\u00fcnner", "Celestine", ""], ["Kourtis", "Kornilios", ""], ["Parnell", "Thomas", ""]]}, {"id": "1811.01567", "submitter": "Naiyan Wang", "authors": "Xinbang Zhang, Zehao Huang, Naiyan Wang", "title": "You Only Search Once: Single Shot Neural Architecture Search via Direct\n  Sparse Optimization", "comments": "ICLR2019 Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Neural Architecture Search (NAS) has aroused great interest in both\nacademia and industry, however it remains challenging because of its huge and\nnon-continuous search space. Instead of applying evolutionary algorithm or\nreinforcement learning as previous works, this paper proposes a Direct Sparse\nOptimization NAS (DSO-NAS) method. In DSO-NAS, we provide a novel model pruning\nview to NAS problem. In specific, we start from a completely connected block,\nand then introduce scaling factors to scale the information flow between\noperations. Next, we impose sparse regularizations to prune useless connections\nin the architecture. Lastly, we derive an efficient and theoretically sound\noptimization method to solve it. Our method enjoys both advantages of\ndifferentiability and efficiency, therefore can be directly applied to large\ndatasets like ImageNet. Particularly, On CIFAR-10 dataset, DSO-NAS achieves an\naverage test error 2.84\\%, while on the ImageNet dataset DSO-NAS achieves\n25.4\\% test error under 600M FLOPs with 8 GPUs in 18 hours.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 09:28:13 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Zhang", "Xinbang", ""], ["Huang", "Zehao", ""], ["Wang", "Naiyan", ""]]}, {"id": "1811.01574", "submitter": "Kaihui Liu", "authors": "Kaihui Liu, Jiayi Wang, Zhengli Xing, Linxiao Yang, and Jun Fang", "title": "Low-Rank Phase Retrieval via Variational Bayesian Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of low-rank phase retrieval whose\nobjective is to estimate a complex low-rank matrix from magnitude-only\nmeasurements. We propose a hierarchical prior model for low-rank phase\nretrieval, in which a Gaussian-Wishart hierarchical prior is placed on the\nunderlying low-rank matrix to promote the low-rankness of the matrix. Based on\nthe proposed hierarchical model, a variational expectation-maximization (EM)\nalgorithm is developed. The proposed method is less sensitive to the choice of\nthe initialization point and works well with random initialization. Simulation\nresults are provided to illustrate the effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 09:47:31 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Liu", "Kaihui", ""], ["Wang", "Jiayi", ""], ["Xing", "Zhengli", ""], ["Yang", "Linxiao", ""], ["Fang", "Jun", ""]]}, {"id": "1811.01587", "submitter": "Xiaoliang Song", "authors": "Yiyang Wang, Risheng Liu, Long Ma and Xiaoliang Song", "title": "Task Embedded Coordinate Update: A Realizable Framework for Multivariate\n  Non-convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We in this paper propose a realizable framework TECU, which embeds\ntask-specific strategies into update schemes of coordinate descent, for\noptimizing multivariate non-convex problems with coupled objective functions.\nOn one hand, TECU is capable of improving algorithm efficiencies through\nembedding productive numerical algorithms, for optimizing univariate\nsub-problems with nice properties. From the other side, it also augments\nprobabilities to receive desired results, by embedding advanced techniques in\noptimizations of realistic tasks. Integrating both numerical algorithms and\nadvanced techniques together, TECU is proposed in a unified framework for\nsolving a class of non-convex problems. Although the task embedded strategies\nbring inaccuracies in sub-problem optimizations, we provide a realizable\ncriterion to control the errors, meanwhile, to ensure robust performances with\nrigid theoretical analyses. By respectively embedding ADMM and a residual-type\nCNN in our algorithm framework, the experimental results verify both efficiency\nand effectiveness of embedding task-oriented strategies in coordinate descent\nfor solving practical problems.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 10:15:43 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 06:01:18 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Wang", "Yiyang", ""], ["Liu", "Risheng", ""], ["Ma", "Long", ""], ["Song", "Xiaoliang", ""]]}, {"id": "1811.01609", "submitter": "Hirokazu Kameoka", "authors": "Hirokazu Kameoka, Kou Tanaka, Damian Kwasny, Takuhiro Kaneko,\n  Nobukatsu Hojo", "title": "ConvS2S-VC: Fully convolutional sequence-to-sequence voice conversion", "comments": "Published in IEEE/ACM Trans. ASLP\n  https://ieeexplore.ieee.org/document/9113442", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a voice conversion (VC) method using sequence-to-sequence\n(seq2seq or S2S) learning, which flexibly converts not only the voice\ncharacteristics but also the pitch contour and duration of input speech. The\nproposed method, called ConvS2S-VC, has three key features. First, it uses a\nmodel with a fully convolutional architecture. This is particularly\nadvantageous in that it is suitable for parallel computations using GPUs. It is\nalso beneficial since it enables effective normalization techniques such as\nbatch normalization to be used for all the hidden layers in the networks.\nSecond, it achieves many-to-many conversion by simultaneously learning mappings\namong multiple speakers using only a single model instead of separately\nlearning mappings between each speaker pair using a different model. This\nenables the model to fully utilize available training data collected from\nmultiple speakers by capturing common latent features that can be shared across\ndifferent speakers. Owing to this structure, our model works reasonably well\neven without source speaker information, thus making it able to handle\nany-to-many conversion tasks. Third, we introduce a mechanism, called the\nconditional batch normalization that switches batch normalization layers in\naccordance with the target speaker. This particular mechanism has been found to\nbe extremely effective for our many-to-many conversion model. We conducted\nspeaker identity conversion experiments and found that ConvS2S-VC obtained\nhigher sound quality and speaker similarity than baseline methods. We also\nfound from audio examples that it could perform well in various tasks including\nemotional expression conversion, electrolaryngeal speech enhancement, and\nEnglish accent conversion.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 11:02:29 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 09:28:54 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 19:14:50 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Kameoka", "Hirokazu", ""], ["Tanaka", "Kou", ""], ["Kwasny", "Damian", ""], ["Kaneko", "Takuhiro", ""], ["Hojo", "Nobukatsu", ""]]}, {"id": "1811.01640", "submitter": "Michele Alberti", "authors": "Vinaychandran Pondenkandath, Michele Alberti, Sammer Puran, Rolf\n  Ingold, Marcus Liwicki", "title": "Leveraging Random Label Memorization for Unsupervised Pre-Training", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to leverage large unlabeled datasets by\npre-training state-of-the-art deep neural networks on randomly-labeled\ndatasets. Specifically, we train the neural networks to memorize arbitrary\nlabels for all the samples in a dataset and use these pre-trained networks as a\nstarting point for regular supervised learning. Our assumption is that the\n\"memorization infrastructure\" learned by the network during the random-label\ntraining proves to be beneficial for the conventional supervised learning as\nwell. We test the effectiveness of our pre-training on several video action\nrecognition datasets (HMDB51, UCF101, Kinetics) by comparing the results of the\nsame network with and without the random label pre-training. Our approach\nyields an improvement - ranging from 1.5% on UCF-101 to 5% on Kinetics - in\nclassification accuracy, which calls for further research in this direction.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 12:27:14 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Pondenkandath", "Vinaychandran", ""], ["Alberti", "Michele", ""], ["Puran", "Sammer", ""], ["Ingold", "Rolf", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1811.01661", "submitter": "Stanislaw Gorlow", "authors": "Pedro J. Villasana T. and Stanislaw Gorlow", "title": "Exact multiplicative updates for convolutional $\\beta$-NMF in 2D", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend the $\\beta$-CNMF to two dimensions and derive exact\nmultiplicative updates for its factors. The new updates generalize and correct\nthe nonnegative matrix factor deconvolution previously proposed by Schmidt and\nM{\\o}rup. We show by simulation that the updates lead to a monotonically\ndecreasing $\\beta$-divergence in terms of the mean and the standard deviation\nand that the corresponding convergence curves are consistent across the most\ncommon values for $\\beta$.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 13:17:55 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["T.", "Pedro J. Villasana", ""], ["Gorlow", "Stanislaw", ""]]}, {"id": "1811.01662", "submitter": "Tien Huu Do", "authors": "Tien Huu Do, Duc Minh Nguyen, Evaggelia Tsiligianni, Angel Lopez\n  Aguirre, Valerio Panzica La Manna, Frank Pasveer, Wilfried Philips, Nikos\n  Deligiannis", "title": "Matrix Completion With Variational Graph Autoencoders: Application in\n  Hyperlocal Air Quality Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring air quality from a limited number of observations is an essential\ntask for monitoring and controlling air pollution. Existing inference methods\ntypically use low spatial resolution data collected by fixed monitoring\nstations and infer the concentration of air pollutants using additional types\nof data, e.g., meteorological and traffic information. In this work, we focus\non street-level air quality inference by utilizing data collected by mobile\nstations. We formulate air quality inference in this setting as a graph-based\nmatrix completion problem and propose a novel variational model based on graph\nconvolutional autoencoders. Our model captures effectively the spatio-temporal\ncorrelation of the measurements and does not depend on the availability of\nadditional information apart from the street-network topology. Experiments on a\nreal air quality dataset, collected with mobile stations, shows that the\nproposed model outperforms state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 13:18:32 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Do", "Tien Huu", ""], ["Nguyen", "Duc Minh", ""], ["Tsiligianni", "Evaggelia", ""], ["Aguirre", "Angel Lopez", ""], ["La Manna", "Valerio Panzica", ""], ["Pasveer", "Frank", ""], ["Philips", "Wilfried", ""], ["Deligiannis", "Nikos", ""]]}, {"id": "1811.01686", "submitter": "Arash Khoeini", "authors": "Arash Khoeini, Bita Shams, Saman Haratizadeh", "title": "GEMRank: Global Entity Embedding For Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, word embedding algorithms have been applied to map the entities of\nrecommender systems, such as users and items, to new feature spaces using\ntextual element-context relations among them. Unlike many other domains, this\napproach has not achieved a desired performance in collaborative filtering\nproblems, probably due to unavailability of appropriate textual data. In this\npaper we propose a new recommendation framework, called GEMRank that can be\napplied when the user-item matrix is the sole available souce of information.\nIt uses the concept of profile co-occurrence for defining relations among\nentities and applies a factorization method for embedding the users and items.\nGEMRank then feeds the extracted representations to a neural network model to\npredict user-item like/dislike relations which the final recommendations are\nmade based on. We evaluated GEMRank in an extensive set of experiments against\nstate of the art recommendation methods. The results show that GEMRank\nsignificantly outperforms the baseline algorithms in a variety of data sets\nwith different degrees of density.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 13:54:20 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Khoeini", "Arash", ""], ["Shams", "Bita", ""], ["Haratizadeh", "Saman", ""]]}, {"id": "1811.01700", "submitter": "Junjie Zeng", "authors": "Junjie Zeng, Long Qin, Yue Hu, Cong Hu and Quanjun Yin", "title": "Combining Subgoal Graphs with Reinforcement Learning to Build a Rational\n  Pathfinder", "comments": "20 pages", "journal-ref": null, "doi": "10.3390/app9020323", "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a hierarchical path planning framework called SG-RL\n(subgoal graphs-reinforcement learning), to plan rational paths for agents\nmaneuvering in continuous and uncertain environments. By \"rational\", we mean\n(1) efficient path planning to eliminate first-move lags; (2) collision-free\nand smooth for agents with kinematic constraints satisfied. SG-RL works in a\ntwo-level manner. At the first level, SG-RL uses a geometric path-planning\nmethod, i.e., Simple Subgoal Graphs (SSG), to efficiently find optimal abstract\npaths, also called subgoal sequences. At the second level, SG-RL uses an RL\nmethod, i.e., Least-Squares Policy Iteration (LSPI), to learn near-optimal\nmotion-planning policies which can generate kinematically feasible and\ncollision-free trajectories between adjacent subgoals. The first advantage of\nthe proposed method is that SSG can solve the limitations of sparse reward and\nlocal minima trap for RL agents; thus, LSPI can be used to generate paths in\ncomplex environments. The second advantage is that, when the environment\nchanges slightly (i.e., unexpected obstacles appearing), SG-RL does not need to\nreconstruct subgoal graphs and replan subgoal sequences using SSG, since LSPI\ncan deal with uncertainties by exploiting its generalization ability to handle\nchanges in environments. Simulation experiments in representative scenarios\ndemonstrate that, compared with existing methods, SG-RL can work well on\nlarge-scale maps with relatively low action-switching frequencies and shorter\npath lengths, and SG-RL can deal with small changes in environments. We further\ndemonstrate that the design of reward functions and the types of training\nenvironments are important factors for learning feasible policies.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 14:12:14 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Zeng", "Junjie", ""], ["Qin", "Long", ""], ["Hu", "Yue", ""], ["Hu", "Cong", ""], ["Yin", "Quanjun", ""]]}, {"id": "1811.01704", "submitter": "Ahmed Taha Elthakeb", "authors": "Ahmed T. Elthakeb, Prannoy Pilligundla, FatemehSadat Mireshghallah,\n  Amir Yazdanbakhsh, Hadi Esmaeilzadeh", "title": "ReLeQ: A Reinforcement Learning Approach for Deep Quantization of Neural\n  Networks", "comments": "Presented as a spotlight paper at NeurIPS Workshop on ML for Systems\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) typically require massive amount of computation\nresource in inference tasks for computer vision applications. Quantization can\nsignificantly reduce DNN computation and storage by decreasing the bitwidth of\nnetwork encodings. Recent research affirms that carefully selecting the\nquantization levels for each layer can preserve the accuracy while pushing the\nbitwidth below eight bits. However, without arduous manual effort, this deep\nquantization can lead to significant accuracy loss, leaving it in a position of\nquestionable utility. As such, deep quantization opens a large hyper-parameter\nspace (bitwidth of the layers), the exploration of which is a major challenge.\nWe propose a systematic approach to tackle this problem, by automating the\nprocess of discovering the quantization levels through an end-to-end deep\nreinforcement learning framework (ReLeQ). We adapt policy optimization methods\nto the problem of quantization, and focus on finding the best design decisions\nin choosing the state and action spaces, network architecture and training\nframework, as well as the tuning of various hyperparamters. We show how ReLeQ\ncan balance speed and quality, and provide an asymmetric general solution for\nquantization of a large variety of deep networks (AlexNet, CIFAR-10, LeNet,\nMobileNet-V1, ResNet-20, SVHN, and VGG-11) that virtually preserves the\naccuracy (=< 0.3% loss) while minimizing the computation and storage cost. With\nthese DNNs, ReLeQ enables conventional hardware to achieve 2.2x speedup over\n8-bit execution. Similarly, a custom DNN accelerator achieves 2.0x speedup and\nenergy reduction compared to 8-bit runs. These encouraging results mark ReLeQ\nas the initial step towards automating the deep quantization of neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 14:18:06 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 23:49:36 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 02:02:27 GMT"}, {"version": "v4", "created": "Thu, 16 Apr 2020 17:17:43 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Elthakeb", "Ahmed T.", ""], ["Pilligundla", "Prannoy", ""], ["Mireshghallah", "FatemehSadat", ""], ["Yazdanbakhsh", "Amir", ""], ["Esmaeilzadeh", "Hadi", ""]]}, {"id": "1811.01710", "submitter": "Shankar Kumar", "authors": "Jared Lichtarge, Christopher Alberti, Shankar Kumar, Noam Shazeer,\n  Niki Parmar", "title": "Weakly Supervised Grammatical Error Correction using Iterative Decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an approach to Grammatical Error Correction (GEC) that is\neffective at making use of models trained on large amounts of weakly supervised\nbitext. We train the Transformer sequence-to-sequence model on 4B tokens of\nWikipedia revisions and employ an iterative decoding strategy that is tailored\nto the loosely-supervised nature of the Wikipedia training corpus. Finetuning\non the Lang-8 corpus and ensembling yields an F0.5 of 58.3 on the CoNLL'14\nbenchmark and a GLEU of 62.4 on JFLEG. The combination of weakly supervised\ntraining and iterative decoding obtains an F0.5 of 48.2 on CoNLL'14 even\nwithout using any labeled GEC data.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 01:31:10 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Lichtarge", "Jared", ""], ["Alberti", "Christopher", ""], ["Kumar", "Shankar", ""], ["Shazeer", "Noam", ""], ["Parmar", "Niki", ""]]}, {"id": "1811.01713", "submitter": "Lingfei Wu", "authors": "Lingfei Wu, Ian E.H. Yen, Kun Xu, Fangli Xu, Avinash Balakrishnan,\n  Pin-Yu Chen, Pradeep Ravikumar, Michael J. Witbrock", "title": "Word Mover's Embedding: From Word2Vec to Document Embedding", "comments": "EMNLP'18 Camera-Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the celebrated Word2Vec technique yields semantically rich\nrepresentations for individual words, there has been relatively less success in\nextending to generate unsupervised sentences or documents embeddings. Recent\nwork has demonstrated that a distance measure between documents called\n\\emph{Word Mover's Distance} (WMD) that aligns semantically similar words,\nyields unprecedented KNN classification accuracy. However, WMD is expensive to\ncompute, and it is hard to extend its use beyond a KNN classifier. In this\npaper, we propose the \\emph{Word Mover's Embedding } (WME), a novel approach to\nbuilding an unsupervised document (sentence) embedding from pre-trained word\nembeddings. In our experiments on 9 benchmark text classification datasets and\n22 textual similarity tasks, the proposed technique consistently matches or\noutperforms state-of-the-art techniques, with significantly higher accuracy on\nproblems of short length.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 19:43:17 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Wu", "Lingfei", ""], ["Yen", "Ian E. H.", ""], ["Xu", "Kun", ""], ["Xu", "Fangli", ""], ["Balakrishnan", "Avinash", ""], ["Chen", "Pin-Yu", ""], ["Ravikumar", "Pradeep", ""], ["Witbrock", "Michael J.", ""]]}, {"id": "1811.01715", "submitter": "Siwei Wang", "authors": "Siwei Wang, Longbo Huang", "title": "Multi-armed Bandits with Compensation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study the known-compensation multi-arm bandit (KCMAB) problem,\nwhere a system controller offers a set of arms to many short-term players for\n$T$ steps. In each step, one short-term player arrives to the system. Upon\narrival, the player aims to select an arm with the current best average reward\nand receives a stochastic reward associated with the arm. In order to\nincentivize players to explore other arms, the controller provides a proper\npayment compensation to players. The objective of the controller is to maximize\nthe total reward collected by players while minimizing the compensation. We\nfirst provide a compensation lower bound $\\Theta(\\sum_i {\\Delta_i\\log T\\over\nKL_i})$, where $\\Delta_i$ and $KL_i$ are the expected reward gap and\nKullback-Leibler (KL) divergence between distributions of arm $i$ and the best\narm, respectively. We then analyze three algorithms to solve the KCMAB problem,\nand obtain their regrets and compensations. We show that the algorithms all\nachieve $O(\\log T)$ regret and $O(\\log T)$ compensation that match the\ntheoretical lower bound. Finally, we present experimental results to\ndemonstrate the performance of the algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 14:24:46 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Wang", "Siwei", ""], ["Huang", "Longbo", ""]]}, {"id": "1811.01721", "submitter": "Jeff Johnson", "authors": "Jeff Johnson", "title": "Rethinking floating point for deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing hardware overhead of neural networks for faster or lower power\ninference and training is an active area of research. Uniform quantization\nusing integer multiply-add has been thoroughly investigated, which requires\nlearning many quantization parameters, fine-tuning training or other\nprerequisites. Little effort is made to improve floating point relative to this\nbaseline; it remains energy inefficient, and word size reduction yields drastic\nloss in needed dynamic range. We improve floating point to be more energy\nefficient than equivalent bit width integer hardware on a 28 nm ASIC process\nwhile retaining accuracy in 8 bits with a novel hybrid log multiply/linear add,\nKulisch accumulation and tapered encodings from Gustafson's posit format. With\nno network retraining, and drop-in replacement of all math and float32\nparameters via round-to-nearest-even only, this open-sourced 8-bit log float is\nwithin 0.9% top-1 and 0.2% top-5 accuracy of the original float32 ResNet-50 CNN\nmodel on ImageNet. Unlike int8 quantization, it is still a general purpose\nfloating point arithmetic, interpretable out-of-the-box. Our 8/38-bit log float\nmultiply-add is synthesized and power profiled at 28 nm at 0.96x the power and\n1.12x the area of 8/32-bit integer multiply-add. In 16 bits, our log float\nmultiply-add is 0.59x the power and 0.68x the area of IEEE 754 float16 fused\nmultiply-add, maintaining the same signficand precision and dynamic range,\nproving useful for training ASICs as well.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 22:13:57 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Johnson", "Jeff", ""]]}, {"id": "1811.01727", "submitter": "Ronghui You", "authors": "Ronghui You, Zihan Zhang, Ziye Wang, Suyang Dai, Hiroshi Mamitsuka and\n  Shanfeng Zhu", "title": "AttentionXML: Label Tree-based Attention-Aware Deep Model for\n  High-Performance Extreme Multi-Label Text Classification", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme multi-label text classification (XMTC) is an important problem in the\nera of big data, for tagging a given text with the most relevant multiple\nlabels from an extremely large-scale label set. XMTC can be found in many\napplications, such as item categorization, web page tagging, and news\nannotation. Traditionally most methods used bag-of-words (BOW) as inputs,\nignoring word context as well as deep semantic information. Recent attempts to\novercome the problems of BOW by deep learning still suffer from 1) failing to\ncapture the important subtext for each label and 2) lack of scalability against\nthe huge number of labels. We propose a new label tree-based deep learning\nmodel for XMTC, called AttentionXML, with two unique features: 1) a multi-label\nattention mechanism with raw text as input, which allows to capture the most\nrelevant part of text to each label; and 2) a shallow and wide probabilistic\nlabel tree (PLT), which allows to handle millions of labels, especially for\n\"tail labels\". We empirically compared the performance of AttentionXML with\nthose of eight state-of-the-art methods over six benchmark datasets, including\nAmazon-3M with around 3 million labels. AttentionXML outperformed all competing\nmethods under all experimental settings. Experimental results also show that\nAttentionXML achieved the best performance against tail labels among label\ntree-based methods. The code and datasets are available at\nhttp://github.com/yourh/AttentionXML .\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 16:30:33 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 13:29:08 GMT"}, {"version": "v3", "created": "Mon, 4 Nov 2019 15:51:52 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["You", "Ronghui", ""], ["Zhang", "Zihan", ""], ["Wang", "Ziye", ""], ["Dai", "Suyang", ""], ["Mamitsuka", "Hiroshi", ""], ["Zhu", "Shanfeng", ""]]}, {"id": "1811.01734", "submitter": "Radu Tudor Ionescu", "authors": "Radu Tudor Ionescu and Andrei M. Butnaru", "title": "Transductive Learning with String Kernels for Cross-Domain Text\n  Classification", "comments": "Accepted at ICONIP 2018. arXiv admin note: substantial text overlap\n  with arXiv:1808.08409", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many text classification tasks, there is a major problem posed by the\nlack of labeled data in a target domain. Although classifiers for a target\ndomain can be trained on labeled text data from a related source domain, the\naccuracy of such classifiers is usually lower in the cross-domain setting.\nRecently, string kernels have obtained state-of-the-art results in various text\nclassification tasks such as native language identification or automatic essay\nscoring. Moreover, classifiers based on string kernels have been found to be\nrobust to the distribution gap between different domains. In this paper, we\nformally describe an algorithm composed of two simple yet effective\ntransductive learning approaches to further improve the results of string\nkernels in cross-domain settings. By adapting string kernels to the test set\nwithout using the ground-truth test labels, we report significantly better\naccuracy rates in cross-domain English polarity classification.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 08:05:36 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Ionescu", "Radu Tudor", ""], ["Butnaru", "Andrei M.", ""]]}, {"id": "1811.01742", "submitter": "Rafael Menelau Oliveira E Cruz", "authors": "Rafael M. O. Cruz, Robert Sabourin and George D. C. Cavalcanti", "title": "META-DES.H: a dynamic ensemble selection technique using meta-learning\n  and a dynamic weighting approach", "comments": "arXiv admin note: substantial text overlap with arXiv:1509.00825,\n  arXiv:1810.01270, arXiv:1811.00217", "journal-ref": "Published on the International Joint Conference on Neural Networks\n  (IJCNN), 2015, pp. 1-8", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Dynamic Ensemble Selection (DES) techniques, only the most competent\nclassifiers are selected to classify a given query sample. Hence, the key issue\nin DES is how to estimate the competence of each classifier in a pool to select\nthe most competent ones. In order to deal with this issue, we proposed a novel\ndynamic ensemble selection framework using meta-learning, called META-DES. The\nframework is divided into three steps. In the first step, the pool of\nclassifiers is generated from the training data. In the second phase the\nmeta-features are computed using the training data and used to train a\nmeta-classifier that is able to predict whether or not a base classifier from\nthe pool is competent enough to classify an input instance. In this paper, we\npropose improvements to the training and generalization phase of the META-DES\nframework. In the training phase, we evaluate four different algorithms for the\ntraining of the meta-classifier. For the generalization phase, three\ncombination approaches are evaluated: Dynamic selection, where only the\nclassifiers that attain a certain competence level are selected; Dynamic\nweighting, where the meta-classifier estimates the competence of each\nclassifier in the pool, and the outputs of all classifiers in the pool are\nweighted based on their level of competence; and a hybrid approach, in which\nfirst an ensemble with the most competent classifiers is selected, after which\nthe weights of the selected classifiers are estimated in order to be used in a\nweighted majority voting scheme. Experiments are carried out on 30\nclassification datasets. Experimental results demonstrate that the changes\nproposed in this paper significantly improve the recognition accuracy of the\nsystem in several datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 23:28:01 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Cruz", "Rafael M. O.", ""], ["Sabourin", "Robert", ""], ["Cavalcanti", "George D. C.", ""]]}, {"id": "1811.01743", "submitter": "Rafael Menelau Oliveira E Cruz", "authors": "Rafael M. O. Cruz, Robert Sabourin, George D. C. Cavalcanti", "title": "On Meta-Learning for Dynamic Ensemble Selection", "comments": "arXiv admin note: substantial text overlap with arXiv:1810.01270;\n  text overlap with arXiv:1509.00825", "journal-ref": "Published on the International Conference on Pattern Recognition\n  (ICPR), 2014, pp. 1230-1235", "doi": "10.1109/ICPR.2014.221", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel dynamic ensemble selection framework using\nmeta-learning. The framework is divided into three steps. In the first step,\nthe pool of classifiers is generated from the training data. The second phase\nis responsible to extract the meta-features and train the meta-classifier. Five\ndistinct sets of meta-features are proposed, each one corresponding to a\ndifferent criterion to measure the level of competence of a classifier for the\nclassification of a given query sample. The meta-features are computed using\nthe training data and used to train a meta-classifier that is able to predict\nwhether or not a base classifier from the pool is competent enough to classify\nan input instance. Three different training scenarios for the training of the\nmeta-classifier are considered: problem-dependent, problem-independent and\nhybrid. Experimental results show that the problem-dependent scenario provides\nthe best result. In addition, the performance of the problem-dependent scenario\nis strongly correlated with the recognition rate of the system. A comparison\nwith state-of-the-art techniques shows that the proposed-dependent approach\noutperforms current dynamic ensemble selection techniques.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 23:13:38 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Cruz", "Rafael M. O.", ""], ["Sabourin", "Robert", ""], ["Cavalcanti", "George D. C.", ""]]}, {"id": "1811.01747", "submitter": "Ali Emami Mr.", "authors": "Ali Emami, Paul Trichelair, Adam Trischler, Kaheer Suleman, Hannes\n  Schulz and Jackie Chi Kit Cheung", "title": "The Knowref Coreference Corpus: Removing Gender and Number Cues for\n  Difficult Pronominal Anaphora Resolution", "comments": "9 pages (excluding references), accepted for ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new benchmark for coreference resolution and NLI, Knowref,\nthat targets common-sense understanding and world knowledge. Previous\ncoreference resolution tasks can largely be solved by exploiting the number and\ngender of the antecedents, or have been handcrafted and do not reflect the\ndiversity of naturally occurring text. We present a corpus of over 8,000\nannotated text passages with ambiguous pronominal anaphora. These instances are\nboth challenging and realistic. We show that various coreference systems,\nwhether rule-based, feature-rich, or neural, perform significantly worse on the\ntask than humans, who display high inter-annotator agreement. To explain this\nperformance gap, we show empirically that state-of-the art models often fail to\ncapture context, instead relying on the gender or number of candidate\nantecedents to make a decision. We then use problem-specific insights to\npropose a data-augmentation trick called antecedent switching to alleviate this\ntendency in models. Finally, we show that antecedent switching yields promising\nresults on other tasks as well: we use it to achieve state-of-the-art results\non the GAP coreference task.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 16:41:26 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 22:16:35 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 20:06:32 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Emami", "Ali", ""], ["Trichelair", "Paul", ""], ["Trischler", "Adam", ""], ["Suleman", "Kaheer", ""], ["Schulz", "Hannes", ""], ["Cheung", "Jackie Chi Kit", ""]]}, {"id": "1811.01749", "submitter": "Bertrand Girard", "authors": "David Vigouroux, Sylvain Picard", "title": "FUNN: Flexible Unsupervised Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have demonstrated high accuracy in image classification\ntasks. However, they were shown to be weak against adversarial examples: a\nsmall perturbation in the image which changes the classification output\ndramatically. In recent years, several defenses have been proposed to solve\nthis issue in supervised classification tasks. We propose a method to obtain\nrobust features in unsupervised learning tasks against adversarial attacks. Our\nmethod differs from existing solutions by directly learning the robust features\nwithout the need to project the adversarial examples in the original examples\ndistribution space. A first auto-encoder A1 is in charge of perturbing the\ninput image to fool another auto-encoder A2 which is in charge of regenerating\nthe original image. A1 tries to find the less perturbed image under the\nconstraint that the error in the output of A2 should be at least equal to a\nthreshold. Thanks to this training, the encoder of A2 will be robust against\nadversarial attacks and could be used in different tasks like classification.\nUsing state-of-art network architectures, we demonstrate the robustness of the\nfeatures obtained thanks to this method in classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 14:42:02 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Vigouroux", "David", ""], ["Picard", "Sylvain", ""]]}, {"id": "1811.01753", "submitter": "Patrick Krauss", "authors": "Achim Schilling, Claus Metzner, Jonas Rietsch, Richard Gerum, Holger\n  Schulze, Patrick Krauss", "title": "How deep is deep enough? -- Quantifying class separability in the hidden\n  layers of deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks typically outperform more traditional machine learning\nmodels in their ability to classify complex data, and yet is not clear how the\nindividual hidden layers of a deep network contribute to the overall\nclassification performance. We thus introduce a Generalized Discrimination\nValue (GDV) that measures, in a non-invasive manner, how well different data\nclasses separate in each given network layer. The GDV can be used for the\nautomatic tuning of hyper-parameters, such as the width profile and the total\ndepth of a network. Moreover, the layer-dependent GDV(L) provides new insights\ninto the data transformations that self-organize during training: In the case\nof multi-layer perceptrons trained with error backpropagation, we find that\nclassification of highly complex data sets requires a temporal {\\em reduction}\nof class separability, marked by a characteristic 'energy barrier' in the\ninitial part of the GDV(L) curve. Even more surprisingly, for a given data set,\nthe GDV(L) is running through a fixed 'master curve', independently from the\ntotal number of network layers. Furthermore, applying the GDV to Deep Belief\nNetworks reveals that also unsupervised training with the Contrastive\nDivergence method can systematically increase class separability over tens of\nlayers, even though the system does not 'know' the desired class labels. These\nresults indicate that the GDV may become a useful tool to open the black box of\ndeep learning.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 14:46:12 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 13:09:32 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Schilling", "Achim", ""], ["Metzner", "Claus", ""], ["Rietsch", "Jonas", ""], ["Gerum", "Richard", ""], ["Schulze", "Holger", ""], ["Krauss", "Patrick", ""]]}, {"id": "1811.01757", "submitter": "Nikolaos Passalis", "authors": "Angeliki Papadimitriou, Nikolaos Passalis and Anastasios Tefas", "title": "Decoding Generic Visual Representations From Human Brain Activity using\n  Machine Learning", "comments": "Accepted at 1st Workshop on Brain-Driven Computer Vision - ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the most impressive recent applications of neural decoding is the\nvisual representation decoding, where the category of an object that a subject\neither sees or imagines is inferred by observing his/her brain activity. Even\nthough there is an increasing interest in the aforementioned visual\nrepresentation decoding task, there is no extensive study of the effect of\nusing different machine learning models on the decoding accuracy. In this paper\nwe provide an extensive evaluation of several machine learning models, along\nwith different similarity metrics, for the aforementioned task, drawing many\ninteresting conclusions. That way, this paper a) paves the way for developing\nmore advanced and accurate methods and b) provides an extensive and easily\nreproducible baseline for the aforementioned decoding task.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 14:48:15 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Papadimitriou", "Angeliki", ""], ["Passalis", "Nikolaos", ""], ["Tefas", "Anastasios", ""]]}, {"id": "1811.01760", "submitter": "Junhong Lin", "authors": "Junhong Lin and Volkan Cevher", "title": "Kernel Conjugate Gradient Methods with Random Projections", "comments": "43 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.FA math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study kernel conjugate gradient methods (KCGM) with random\nprojections for least-squares regression over a separable Hilbert space.\nConsidering two types of random projections generated by randomized sketches\nand Nystr\\\"{o}m subsampling, we prove optimal statistical results with respect\nto variants of norms for the algorithms under a suitable stopping rule.\nParticularly, our results show that if the projection dimension is proportional\nto the effective dimension of the problem, KCGM with randomized sketches can\ngeneralize optimally, while achieving a computational advantage. As a\ncorollary, we derive optimal rates for classic KCGM in the case that the target\nfunction may not be in the hypothesis space, filling a theoretical gap.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 14:50:58 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Lin", "Junhong", ""], ["Cevher", "Volkan", ""]]}, {"id": "1811.01778", "submitter": "Paul Trichelair", "authors": "Paul Trichelair and Ali Emami and Adam Trischler and Kaheer Suleman\n  and Jackie Chi Kit Cheung", "title": "How Reasonable are Common-Sense Reasoning Tasks: A Case-Study on the\n  Winograd Schema Challenge and SWAG", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have significantly improved the state-of-the-art on\ncommon-sense reasoning (CSR) benchmarks like the Winograd Schema Challenge\n(WSC) and SWAG. The question we ask in this paper is whether improved\nperformance on these benchmarks represents genuine progress towards\ncommon-sense-enabled systems. We make case studies of both benchmarks and\ndesign protocols that clarify and qualify the results of previous work by\nanalyzing threats to the validity of previous experimental designs. Our\nprotocols account for several properties prevalent in common-sense benchmarks\nincluding size limitations, structural regularities, and variable instance\ndifficulty.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 15:11:24 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 06:28:44 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Trichelair", "Paul", ""], ["Emami", "Ali", ""], ["Trischler", "Adam", ""], ["Suleman", "Kaheer", ""], ["Cheung", "Jackie Chi Kit", ""]]}, {"id": "1811.01811", "submitter": "Kemal Davaslioglu", "authors": "Yi Shi, Yalin E. Sagduyu, Kemal Davaslioglu, and Jason H. Li", "title": "Active Deep Learning Attacks under Strict Rate Limitations for Online\n  API Calls", "comments": "Presented at 2018 IEEE International Symposium on Technologies for\n  Homeland Security (HST) on October 23 2018. Received the Best Paper Award in\n  Cyber Security Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has been applied to a broad range of applications and some\nof them are available online as application programming interfaces (APIs) with\neither free (trial) or paid subscriptions. In this paper, we study adversarial\nmachine learning in the form of back-box attacks on online classifier APIs. We\nstart with a deep learning based exploratory (inference) attack, which aims to\nbuild a classifier that can provide similar classification results (labels) as\nthe target classifier. To minimize the difference between the labels returned\nby the inferred classifier and the target classifier, we show that the deep\nlearning based exploratory attack requires a large number of labeled training\ndata samples. These labels can be collected by calling the online API, but\nusually there is some strict rate limitation on the number of allowed API\ncalls. To mitigate the impact of limited training data, we develop an active\nlearning approach that first builds a classifier based on a small number of API\ncalls and uses this classifier to select samples to further collect their\nlabels. Then, a new classifier is built using more training data samples. This\nupdating process can be repeated multiple times. We show that this active\nlearning approach can build an adversarial classifier with a small statistical\ndifference from the target classifier using only a limited number of training\ndata samples. We further consider evasion and causative (poisoning) attacks\nbased on the inferred classifier that is built by the exploratory attack.\nEvasion attack determines samples that the target classifier is likely to\nmisclassify, whereas causative attack provides erroneous training data samples\nto reduce the reliability of the re-trained classifier. The success of these\nattacks show that adversarial machine learning emerges as a feasible threat in\nthe realistic case with limited training data.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 15:50:30 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Shi", "Yi", ""], ["Sagduyu", "Yalin E.", ""], ["Davaslioglu", "Kemal", ""], ["Li", "Jason H.", ""]]}, {"id": "1811.01824", "submitter": "Patrick Fernandes", "authors": "Patrick Fernandes, Miltiadis Allamanis, Marc Brockschmidt", "title": "Structured Neural Summarization", "comments": "Published in ICLR 2019 https://openreview.net/forum?id=H1ersoRqtm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summarization of long sequences into a concise statement is a core problem in\nnatural language processing, requiring non-trivial understanding of the input.\nBased on the promising results of graph neural networks on highly structured\ndata, we develop a framework to extend existing sequence encoders with a graph\ncomponent that can reason about long-distance relationships in weakly\nstructured data such as text. In an extensive evaluation, we show that the\nresulting hybrid sequence-graph models outperform both pure sequence models as\nwell as pure graph models on a range of summarization tasks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 16:12:04 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 13:22:59 GMT"}, {"version": "v3", "created": "Mon, 4 May 2020 11:47:10 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2021 12:43:02 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Fernandes", "Patrick", ""], ["Allamanis", "Miltiadis", ""], ["Brockschmidt", "Marc", ""]]}, {"id": "1811.01837", "submitter": "Micha Livne", "authors": "Micha Livne, David J. Fleet", "title": "TzK Flow - Conditional Generative Model", "comments": "5 pages, 4 figures, Accepted to Bayesian Deep Learning Workshop NIPS\n  2018, camera ready NOTE: This workshop paper has been replaced. Please refer\n  to the following work: arXiv:1902.01893", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce TzK (pronounced \"task\"), a conditional probability flow-based\nmodel that exploits attributes (e.g., style, class membership, or other side\ninformation) in order to learn tight conditional prior around manifolds of the\ntarget observations. The model is trained via approximated ML, and offers\nefficient approximation of arbitrary data sample distributions (similar to GAN\nand flow-based ML), and stable training (similar to VAE and ML), while avoiding\nvariational approximations. TzK exploits meta-data to facilitate a bottleneck,\nsimilar to autoencoders, thereby producing a low-dimensional representation.\nUnlike autoencoders, the bottleneck does not limit model expressiveness,\nsimilar to flow-based ML. Supervised, unsupervised, and semi-supervised\nlearning are supported by replacing missing observations with samples from\nlearned priors. We demonstrate TzK by training jointly on MNIST and Omniglot\ndatasets with minimal preprocessing, and weak supervision, with results\ncomparable to state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 16:44:37 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 21:03:44 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2018 22:34:26 GMT"}, {"version": "v4", "created": "Tue, 19 Feb 2019 22:57:39 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Livne", "Micha", ""], ["Fleet", "David J.", ""]]}, {"id": "1811.01838", "submitter": "Marius Jahrens", "authors": "Marius Jahrens, Thomas Martinetz", "title": "Multi-layer Relation Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational Networks (RN) as introduced by Santoro et al. (2017) have\ndemonstrated strong relational reasoning capabilities with a rather shallow\narchitecture. Its single-layer design, however, only considers pairs of\ninformation objects, making it unsuitable for problems requiring reasoning\nacross a higher number of facts. To overcome this limitation, we propose a\nmulti-layer relation network architecture which enables successive refinements\nof relational information through multiple layers. We show that the increased\ndepth allows for more complex relational reasoning by applying it to the bAbI\n20 QA dataset, solving all 20 tasks with joint training and surpassing the\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 16:44:54 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Jahrens", "Marius", ""], ["Martinetz", "Thomas", ""]]}, {"id": "1811.01845", "submitter": "Siddhartha Dhar Choudhury", "authors": "Siddhartha Dhar Choudhury, Shashank Pandey, Kunal Mehrotra", "title": "Deep Genetic Network", "comments": "The paper has some major flaws and needs to be re written, it will\n  take time so cannot be replaced soon enough", "journal-ref": null, "doi": "10.35940/ijeat.A1128.109119", "report-no": "A1128109119", "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimizing a neural network's performance is a tedious and time taking\nprocess, this iterative process does not have any defined solution which can\nwork for all the problems. Optimization can be roughly categorized into -\nArchitecture and Hyperparameter optimization. Many algorithms have been devised\nto address this problem. In this paper we introduce a neural network\narchitecture (Deep Genetic Network) which will optimize its parameters during\ntraining based on its fitness. Deep Genetic Net uses genetic algorithms along\nwith deep neural networks to address the hyperparameter optimization problem,\nthis approach uses ideas like mating and mutation which are key to genetic\nalgorithms which help the neural net architecture to learn to optimize its\nhyperparameters by itself rather than depending on a person to explicitly set\nthe values. Using genetic algorithms for this problem proved to work\nexceptionally well when given enough time to train the network. The proposed\narchitecture is found to work well in optimizing hyperparameters in affine,\nconvolutional and recurrent layers proving to be a good choice for conventional\nsupervised learning tasks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 17:04:02 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 17:02:45 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Choudhury", "Siddhartha Dhar", ""], ["Pandey", "Shashank", ""], ["Mehrotra", "Kunal", ""]]}, {"id": "1811.01846", "submitter": "Cong Feng", "authors": "Cong Feng and Jie Zhang", "title": "Reinforcement Learning based Dynamic Model Selection for Short-Term Load\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing prevalence of smart grid technology, short-term load\nforecasting (STLF) becomes particularly important in power system operations.\nThere is a large collection of methods developed for STLF, but selecting a\nsuitable method under varying conditions is still challenging. This paper\ndevelops a novel reinforcement learning based dynamic model selection (DMS)\nmethod for STLF. A forecasting model pool is first built, including ten\nstate-of-the-art machine learning based forecasting models. Then a Q-learning\nagent learns the optimal policy of selecting the best forecasting model for the\nnext time step, based on the model performance. The optimal DMS policy is\napplied to select the best model at each time step with a moving window.\nNumerical simulations on two-year load and weather data show that the\nQ-learning algorithm converges fast, resulting in effective and efficient DMS.\nThe developed STLF model with Q-learning based DMS improves the forecasting\naccuracy by approximately 50%, compared to the state-of-the-art machine\nlearning based STLF models.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 17:04:35 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Feng", "Cong", ""], ["Zhang", "Jie", ""]]}, {"id": "1811.01848", "submitter": "Aravind Rajeswaran", "authors": "Kendall Lowrey, Aravind Rajeswaran, Sham Kakade, Emanuel Todorov, Igor\n  Mordatch", "title": "Plan Online, Learn Offline: Efficient Learning and Exploration via\n  Model-Based Control", "comments": "The first two authors contributed equally. Accepted at ICLR 2019.\n  Supplementary videos available at: https://sites.google.com/view/polo-mpc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a plan online and learn offline (POLO) framework for the setting\nwhere an agent, with an internal model, needs to continually act and learn in\nthe world. Our work builds on the synergistic relationship between local\nmodel-based control, global value function learning, and exploration. We study\nhow local trajectory optimization can cope with approximation errors in the\nvalue function, and can stabilize and accelerate value function learning.\nConversely, we also study how approximate value functions can help reduce the\nplanning horizon and allow for better policies beyond local solutions. Finally,\nwe also demonstrate how trajectory optimization can be used to perform\ntemporally coordinated exploration in conjunction with estimating uncertainty\nin value function approximation. This exploration is critical for fast and\nstable learning of the value function. Combining these components enable\nsolutions to complex simulated control tasks, like humanoid locomotion and\ndexterous in-hand manipulation, in the equivalent of a few minutes of\nexperience in the real world.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 17:09:18 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 20:54:47 GMT"}, {"version": "v3", "created": "Mon, 28 Jan 2019 16:39:23 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Lowrey", "Kendall", ""], ["Rajeswaran", "Aravind", ""], ["Kakade", "Sham", ""], ["Todorov", "Emanuel", ""], ["Mordatch", "Igor", ""]]}, {"id": "1811.01850", "submitter": "Olga Slizovskaia", "authors": "Olga Slizovskaia, Leo Kim, Gloria Haro, Emilia Gomez", "title": "End-to-End Sound Source Separation Conditioned On Instrument Labels", "comments": "5 pages, 2 figures, 2 tables, ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we perform an end-to-end music source separation with a variable number\nof sources using a deep learning model? We present an extension of the\nWave-U-Net model which allows end-to-end monaural source separation with a\nnon-fixed number of sources. Furthermore, we propose multiplicative\nconditioning with instrument labels at the bottleneck of the Wave-U-Net and\nshow its effect on the separation results. This approach leads to other types\nof conditioning such as audio-visual source separation and score-informed\nsource separation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 17:12:54 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 16:55:38 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Slizovskaia", "Olga", ""], ["Kim", "Leo", ""], ["Haro", "Gloria", ""], ["Gomez", "Emilia", ""]]}, {"id": "1811.01885", "submitter": "Ainesh Bakshi", "authors": "Ainesh Bakshi, Rajesh Jayaram and David P. Woodruff", "title": "Learning Two Layer Rectified Neural Networks in Polynomial Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following fundamental learning problem: given input examples $x\n\\in \\mathbb{R}^d$ and their vector-valued labels, as defined by an underlying\ngenerative neural network, recover the weight matrices of this network. We\nconsider two-layer networks, mapping $\\mathbb{R}^d$ to $\\mathbb{R}^m$, with $k$\nnon-linear activation units $f(\\cdot)$, where $f(x) = \\max \\{x , 0\\}$ is the\nReLU. Such a network is specified by two weight matrices, $\\mathbf{U}^* \\in\n\\mathbb{R}^{m \\times k}, \\mathbf{V}^* \\in \\mathbb{R}^{k \\times d}$, such that\nthe label of an example $x \\in \\mathbb{R}^{d}$ is given by $\\mathbf{U}^*\nf(\\mathbf{V}^* x)$, where $f(\\cdot)$ is applied coordinate-wise. Given $n$\nsamples as a matrix $\\mathbf{X} \\in \\mathbb{R}^{d \\times n}$ and the (possibly\nnoisy) labels $\\mathbf{U}^* f(\\mathbf{V}^* \\mathbf{X}) + \\mathbf{E}$ of the\nnetwork on these samples, where $\\mathbf{E}$ is a noise matrix, our goal is to\nrecover the weight matrices $\\mathbf{U}^*$ and $\\mathbf{V}^*$.\n  In this work, we develop algorithms and hardness results under varying\nassumptions on the input and noise. Although the problem is NP-hard even for\n$k=2$, by assuming Gaussian marginals over the input $\\mathbf{X}$ we are able\nto develop polynomial time algorithms for the approximate recovery of\n$\\mathbf{U}^*$ and $\\mathbf{V}^*$. Perhaps surprisingly, in the noiseless case\nour algorithms recover $\\mathbf{U}^*,\\mathbf{V}^*$ exactly, i.e., with no\nerror. To the best of the our knowledge, this is the first algorithm to\naccomplish exact recovery. For the noisy case, we give the first polynomial\ntime algorithm that approximately recovers the weights in the presence of\nmean-zero noise $\\mathbf{E}$. Our algorithms generalize to a larger class of\nrectified activation functions, $f(x) = 0$ when $x\\leq 0$, and $f(x) > 0$\notherwise.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 18:03:56 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Bakshi", "Ainesh", ""], ["Jayaram", "Rajesh", ""], ["Woodruff", "David P.", ""]]}, {"id": "1811.01900", "submitter": "Ryan Murphy", "authors": "Ryan L. Murphy, Balasubramaniam Srinivasan, Vinayak Rao, Bruno Ribeiro", "title": "Janossy Pooling: Learning Deep Permutation-Invariant Functions for\n  Variable-Size Inputs", "comments": "This version clarifies and adds detail to some of the arguments", "journal-ref": "ICLR 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a simple and overarching representation for permutation-invariant\nfunctions of sequences (or multiset functions). Our approach, which we call\nJanossy pooling, expresses a permutation-invariant function as the average of a\npermutation-sensitive function applied to all reorderings of the input\nsequence. This allows us to leverage the rich and mature literature on\npermutation-sensitive functions to construct novel and flexible\npermutation-invariant functions. If carried out naively, Janossy pooling can be\ncomputationally prohibitive. To allow computational tractability, we consider\nthree kinds of approximations: canonical orderings of sequences, functions with\n$k$-order interactions, and stochastic optimization algorithms with random\npermutations. Our framework unifies a variety of existing work in the\nliterature, and suggests possible modeling and algorithmic extensions. We\nexplore a few in our experiments, which demonstrate improved performance over\ncurrent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 18:26:41 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 14:35:49 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 21:13:26 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Murphy", "Ryan L.", ""], ["Srinivasan", "Balasubramaniam", ""], ["Rao", "Vinayak", ""], ["Ribeiro", "Bruno", ""]]}, {"id": "1811.01903", "submitter": "Jelena Diakonikolas", "authors": "Jelena Diakonikolas and Crist\\'obal Guzm\\'an", "title": "Lower Bounds for Parallel and Randomized Convex Optimization", "comments": "In Proc. COLT'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question of whether parallelization in the exploration of the\nfeasible set can be used to speed up convex optimization, in the local oracle\nmodel of computation. We show that the answer is negative for both\ndeterministic and randomized algorithms applied to essentially any of the\ninteresting geometries and nonsmooth, weakly-smooth, or smooth objective\nfunctions. In particular, we show that it is not possible to obtain a\npolylogarithmic (in the sequential complexity of the problem) number of\nparallel rounds with a polynomial (in the dimension) number of queries per\nround. In the majority of these settings and when the dimension of the space is\npolynomial in the inverse target accuracy, our lower bounds match the oracle\ncomplexity of sequential convex optimization, up to at most a logarithmic\nfactor in the dimension, which makes them (nearly) tight. Prior to our work,\nlower bounds for parallel convex optimization algorithms were only known in a\nsmall fraction of the settings considered in this paper, mainly applying to\nEuclidean ($\\ell_2$) and $\\ell_\\infty$ spaces. Our work provides a more general\napproach for proving lower bounds in the setting of parallel convex\noptimization.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 18:32:06 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 14:29:38 GMT"}, {"version": "v3", "created": "Wed, 19 Jun 2019 21:41:52 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Diakonikolas", "Jelena", ""], ["Guzm\u00e1n", "Crist\u00f3bal", ""]]}, {"id": "1811.01907", "submitter": "Tianyun Zhang", "authors": "Shaokai Ye, Tianyun Zhang, Kaiqi Zhang, Jiayu Li, Jiaming Xie, Yun\n  Liang, Sijia Liu, Xue Lin and Yanzhi Wang", "title": "A Unified Framework of DNN Weight Pruning and Weight\n  Clustering/Quantization Using ADMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many model compression techniques of Deep Neural Networks (DNNs) have been\ninvestigated, including weight pruning, weight clustering and quantization,\netc. Weight pruning leverages the redundancy in the number of weights in DNNs,\nwhile weight clustering/quantization leverages the redundancy in the number of\nbit representations of weights. They can be effectively combined in order to\nexploit the maximum degree of redundancy. However, there lacks a systematic\ninvestigation in literature towards this direction.\n  In this paper, we fill this void and develop a unified, systematic framework\nof DNN weight pruning and clustering/quantization using Alternating Direction\nMethod of Multipliers (ADMM), a powerful technique in optimization theory to\ndeal with non-convex optimization problems. Both DNN weight pruning and\nclustering/quantization, as well as their combinations, can be solved in a\nunified manner. For further performance improvement in this framework, we adopt\nmultiple techniques including iterative weight quantization and retraining,\njoint weight clustering training and centroid updating, weight clustering\nretraining, etc. The proposed framework achieves significant improvements both\nin individual weight pruning and clustering/quantization problems, as well as\ntheir combinations. For weight pruning alone, we achieve 167x weight reduction\nin LeNet-5, 24.7x in AlexNet, and 23.4x in VGGNet, without any accuracy loss.\nFor the combination of DNN weight pruning and clustering/quantization, we\nachieve 1,910x and 210x storage reduction of weight data on LeNet-5 and\nAlexNet, respectively, without accuracy loss. Our codes and models are released\nat the link http://bit.ly/2D3F0np\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 18:34:17 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Ye", "Shaokai", ""], ["Zhang", "Tianyun", ""], ["Zhang", "Kaiqi", ""], ["Li", "Jiayu", ""], ["Xie", "Jiaming", ""], ["Liang", "Yun", ""], ["Liu", "Sijia", ""], ["Lin", "Xue", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1811.01908", "submitter": "David Cortes", "authors": "David Cortes", "title": "Fast Non-Bayesian Poisson Factorization for Implicit-Feedback\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores non-negative matrix factorization based on regularized\nPoisson models for recommender systems with implicit-feedback data. The\nproperties of Poisson likelihood allow a shortcut for very fast computation and\noptimization over elements with zero-value when the latent-factor matrices are\nnon-negative, making it a more suitable approach than squared loss for very\nsparse inputs such as implicit-feedback data. A simple and embarrassingly\nparallel optimization approach based on proximal gradients is presented, which\nin large datasets converges 2-3 orders of magnitude faster than its Bayesian\ncounterpart (Hierarchical Poisson Factorization) fit through variational\ninference techniques, and 1 order of magnitude faster than implicit-ALS fit\nwith the Conjugate Gradient method.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 18:35:22 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 17:30:10 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 13:13:36 GMT"}, {"version": "v4", "created": "Sat, 23 May 2020 09:21:51 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Cortes", "David", ""]]}, {"id": "1811.01910", "submitter": "Nikolai Rozanov", "authors": "Edward Collins, Nikolai Rozanov, Bingbing Zhang", "title": "Evolutionary Data Measures: Understanding the Difficulty of Text\n  Classification Tasks", "comments": "27 pages, 6 tables, 3 figures (submitted for publication in June\n  2018), CoNLL 2018", "journal-ref": "ACL, CoNLL(K18-1037), 22, 380--391, (2018)", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classification tasks are usually analysed and improved through new model\narchitectures or hyperparameter optimisation but the underlying properties of\ndatasets are discovered on an ad-hoc basis as errors occur. However,\nunderstanding the properties of the data is crucial in perfecting models. In\nthis paper we analyse exactly which characteristics of a dataset best determine\nhow difficult that dataset is for the task of text classification. We then\npropose an intuitive measure of difficulty for text classification datasets\nwhich is simple and fast to calculate. We show that this measure generalises to\nunseen data by comparing it to state-of-the-art datasets and results. This\nmeasure can be used to analyse the precise source of errors in a dataset and\nallows fast estimation of how difficult a dataset is to learn. We searched for\nthis measure by training 12 classical and neural network based models on 78\nreal-world datasets, then use a genetic algorithm to discover the best measure\nof difficulty. Our difficulty-calculating code ( https://github.com/Wluper/edm\n) and datasets ( http://data.wluper.com ) are publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 18:39:54 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 10:07:20 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Collins", "Edward", ""], ["Rozanov", "Nikolai", ""], ["Zhang", "Bingbing", ""]]}, {"id": "1811.01926", "submitter": "Robin van Emden", "authors": "Robin van Emden, Maurits Kaptein", "title": "contextual: Evaluating Contextual Multi-Armed Bandit Problems in R", "comments": "55 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, contextual bandit algorithms have been gaining in\npopularity due to their effectiveness and flexibility in solving sequential\ndecision problems---from online advertising and finance to clinical trial\ndesign and personalized medicine. At the same time, there are, as of yet,\nsurprisingly few options that enable researchers and practitioners to simulate\nand compare the wealth of new and existing bandit algorithms in a standardized\nway. To help close this gap between analytical research and empirical\nevaluation the current paper introduces the object-oriented R package\n\"contextual\": a user-friendly and, through its object-oriented structure,\neasily extensible framework that facilitates parallelized comparison of\ncontextual and context-free bandit policies through both simulation and offline\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 08:37:03 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 16:55:30 GMT"}, {"version": "v3", "created": "Mon, 8 Jul 2019 13:33:36 GMT"}, {"version": "v4", "created": "Wed, 1 Jan 2020 14:31:34 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["van Emden", "Robin", ""], ["Kaptein", "Maurits", ""]]}, {"id": "1811.01931", "submitter": "Robert Giaquinto", "authors": "Robert Giaquinto and Arindam Banerjee", "title": "DAPPER: Scaling Dynamic Author Persona Topic Model to Billion Word\n  Corpora", "comments": "Published in IEEE International Conference on Data Mining, November\n  2018, Singapore", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting common narratives from multi-author dynamic text corpora requires\ncomplex models, such as the Dynamic Author Persona (DAP) topic model. However,\nsuch models are complex and can struggle to scale to large corpora, often\nbecause of challenging non-conjugate terms. To overcome such challenges, in\nthis paper we adapt new ideas in approximate inference to the DAP model,\nresulting in the DAP Performed Exceedingly Rapidly (DAPPER) topic model.\nSpecifically, we develop Conjugate-Computation Variational Inference (CVI)\nbased variational Expectation-Maximization (EM) for learning the model,\nyielding fast, closed form updates for each document, replacing iterative\noptimization in earlier work. Our results show significant improvements in\nmodel fit and training time without needing to compromise the model's temporal\nstructure or the application of Regularized Variation Inference (RVI). We\ndemonstrate the scalability and effectiveness of the DAPPER model by extracting\nhealth journeys from the CaringBridge corpus --- a collection of 9 million\njournals written by 200,000 authors during health crises.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 21:27:56 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Giaquinto", "Robert", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1811.02002", "submitter": "Ya-Ping Hsieh", "authors": "Ya-Ping Hsieh, Chen Liu, Volkan Cevher", "title": "Finding Mixed Nash Equilibria of Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reconsider the training objective of Generative Adversarial Networks\n(GANs) from the mixed Nash Equilibria (NE) perspective. Inspired by the\nclassical prox methods, we develop a novel algorithmic framework for GANs via\nan infinite-dimensional two-player game and prove rigorous convergence rates to\nthe mixed NE, resolving the longstanding problem that no provably convergent\nalgorithm exists for general GANs. We then propose a principled procedure to\nreduce our novel prox methods to simple sampling routines, leading to\npractically efficient algorithms. Finally, we provide experimental evidence\nthat our approach outperforms methods that seek pure strategy equilibria, such\nas SGD, Adam, and RMSProp, both in speed and quality.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 13:07:18 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Hsieh", "Ya-Ping", ""], ["Liu", "Chen", ""], ["Cevher", "Volkan", ""]]}, {"id": "1811.02017", "submitter": "Taco Cohen", "authors": "Taco Cohen, Mario Geiger, Maurice Weiler", "title": "A General Theory of Equivariant CNNs on Homogeneous Spaces", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 32 (NeurIPS\n  2019) 9142-9153", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general theory of Group equivariant Convolutional Neural\nNetworks (G-CNNs) on homogeneous spaces such as Euclidean space and the sphere.\nFeature maps in these networks represent fields on a homogeneous base space,\nand layers are equivariant maps between spaces of fields. The theory enables a\nsystematic classification of all existing G-CNNs in terms of their symmetry\ngroup, base space, and field type. We also consider a fundamental question:\nwhat is the most general kind of equivariant linear map between feature spaces\n(fields) of given types? Following Mackey, we show that such maps correspond\none-to-one with convolutions using equivariant kernels, and characterize the\nspace of such kernels.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 20:22:10 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 14:59:52 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Cohen", "Taco", ""], ["Geiger", "Mario", ""], ["Weiler", "Maurice", ""]]}, {"id": "1811.02033", "submitter": "Liu Yang", "authors": "Liu Yang, Dongkun Zhang, George Em Karniadakis", "title": "Physics-Informed Generative Adversarial Networks for Stochastic\n  Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.AP math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a new class of physics-informed generative adversarial networks\n(PI-GANs) to solve in a unified manner forward, inverse and mixed stochastic\nproblems based on a limited number of scattered measurements. Unlike standard\nGANs relying only on data for training, here we encoded into the architecture\nof GANs the governing physical laws in the form of stochastic differential\nequations (SDEs) using automatic differentiation. In particular, we applied\nWasserstein GANs with gradient penalty (WGAN-GP) for its enhanced stability\ncompared to vanilla GANs. We first tested WGAN-GP in approximating Gaussian\nprocesses of different correlation lengths based on data realizations collected\nfrom simultaneous reads at sparsely placed sensors. We obtained good\napproximation of the generated stochastic processes to the target ones even for\na mismatch between the input noise dimensionality and the effective\ndimensionality of the target stochastic processes. We also studied the\noverfitting issue for both the discriminator and generator, and we found that\noverfitting occurs also in the generator in addition to the discriminator as\npreviously reported. Subsequently, we considered the solution of elliptic SDEs\nrequiring approximations of three stochastic processes, namely the solution,\nthe forcing, and the diffusion coefficient. We used three generators for the\nPI-GANs, two of them were feed forward deep neural networks (DNNs) while the\nother one was the neural network induced by the SDE. Depending on the data, we\nemployed one or multiple feed forward DNNs as the discriminators in PI-GANs.\nHere, we have demonstrated the accuracy and effectiveness of PI-GANs in solving\nSDEs for up to 30 dimensions, but in principle, PI-GANs could tackle very high\ndimensional problems given more sensor data with low-polynomial growth in\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 21:01:29 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Yang", "Liu", ""], ["Zhang", "Dongkun", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1811.02050", "submitter": "Ye Jia", "authors": "Ye Jia, Melvin Johnson, Wolfgang Macherey, Ron J. Weiss, Yuan Cao,\n  Chung-Cheng Chiu, Naveen Ari, Stella Laurenzo, Yonghui Wu", "title": "Leveraging Weakly Supervised Data to Improve End-to-End Speech-to-Text\n  Translation", "comments": "ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end Speech Translation (ST) models have many potential advantages when\ncompared to the cascade of Automatic Speech Recognition (ASR) and text Machine\nTranslation (MT) models, including lowered inference latency and the avoidance\nof error compounding. However, the quality of end-to-end ST is often limited by\na paucity of training data, since it is difficult to collect large parallel\ncorpora of speech and translated transcript pairs. Previous studies have\nproposed the use of pre-trained components and multi-task learning in order to\nbenefit from weakly supervised training data, such as speech-to-transcript or\ntext-to-foreign-text pairs. In this paper, we demonstrate that using\npre-trained MT or text-to-speech (TTS) synthesis models to convert weakly\nsupervised data into speech-to-translation pairs for ST training can be more\neffective than multi-task learning. Furthermore, we demonstrate that a high\nquality end-to-end ST model can be trained using only weakly supervised\ndatasets, and that synthetic data sourced from unlabeled monolingual text or\nspeech can be used to improve performance. Finally, we discuss methods for\navoiding overfitting to synthetic speech with a quantitative ablation study.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 21:57:09 GMT"}, {"version": "v2", "created": "Sun, 10 Feb 2019 19:47:34 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Jia", "Ye", ""], ["Johnson", "Melvin", ""], ["Macherey", "Wolfgang", ""], ["Weiss", "Ron J.", ""], ["Cao", "Yuan", ""], ["Chiu", "Chung-Cheng", ""], ["Ari", "Naveen", ""], ["Laurenzo", "Stella", ""], ["Wu", "Yonghui", ""]]}, {"id": "1811.02052", "submitter": "Charalampos Andriotis", "authors": "C.P. Andriotis, K.G. Papakonstantinou", "title": "Managing engineering systems with large state and action spaces through\n  deep reinforcement learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision-making for engineering systems can be efficiently formulated as a\nMarkov Decision Process (MDP) or a Partially Observable MDP (POMDP). Typical\nMDP and POMDP solution procedures utilize offline knowledge about the\nenvironment and provide detailed policies for relatively small systems with\ntractable state and action spaces. However, in large multi-component systems\nthe sizes of these spaces easily explode, as system states and actions scale\nexponentially with the number of components, whereas environment dynamics are\ndifficult to be described in explicit forms for the entire system and may only\nbe accessible through numerical simulators. In this work, to address these\nissues, an integrated Deep Reinforcement Learning (DRL) framework is\nintroduced. The Deep Centralized Multi-agent Actor Critic (DCMAC) is developed,\nan off-policy actor-critic DRL approach, providing efficient life-cycle\npolicies for large multi-component systems operating in high-dimensional\nspaces. Apart from deep function approximations that parametrize large state\nspaces, DCMAC also adopts a factorized representation of the system actions,\nbeing able to designate individualized component- and subsystem-level\ndecisions, while maintaining a centralized value function for the entire\nsystem. DCMAC compares well against Deep Q-Network (DQN) solutions and exact\npolicies, where applicable, and outperforms optimized baselines that are based\non time-based, condition-based and periodic policies.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 22:01:54 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Andriotis", "C. P.", ""], ["Papakonstantinou", "K. G.", ""]]}, {"id": "1811.02054", "submitter": "Varun Chandrasekaran", "authors": "Varun Chandrasekaran, Kamalika Chaudhuri, Irene Giacomelli, Somesh Jha\n  and Songbai Yan", "title": "Exploring Connections Between Active Learning and Model Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is being increasingly used by individuals, research\ninstitutions, and corporations. This has resulted in the surge of Machine\nLearning-as-a-Service (MLaaS) - cloud services that provide (a) tools and\nresources to learn the model, and (b) a user-friendly query interface to access\nthe model. However, such MLaaS systems raise privacy concerns such as model\nextraction. In model extraction attacks, adversaries maliciously exploit the\nquery interface to steal the model. More precisely, in a model extraction\nattack, a good approximation of a sensitive or proprietary model held by the\nserver is extracted (i.e. learned) by a dishonest user who interacts with the\nserver only via the query interface. This attack was introduced by Tramer et\nal. at the 2016 USENIX Security Symposium, where practical attacks for various\nmodels were shown. We believe that better understanding the efficacy of model\nextraction attacks is paramount to designing secure MLaaS systems. To that end,\nwe take the first step by (a) formalizing model extraction and discussing\npossible defense strategies, and (b) drawing parallels between model extraction\nand established area of active learning. In particular, we show that recent\nadvancements in the active learning domain can be used to implement powerful\nmodel extraction attacks, and investigate possible defense strategies.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 22:06:12 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 16:25:32 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 09:32:36 GMT"}, {"version": "v4", "created": "Sat, 2 Mar 2019 06:30:45 GMT"}, {"version": "v5", "created": "Tue, 5 Mar 2019 14:58:47 GMT"}, {"version": "v6", "created": "Wed, 20 Nov 2019 04:20:13 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Chandrasekaran", "Varun", ""], ["Chaudhuri", "Kamalika", ""], ["Giacomelli", "Irene", ""], ["Jha", "Somesh", ""], ["Yan", "Songbai", ""]]}, {"id": "1811.02061", "submitter": "Vassilis N. Ioannidis", "authors": "Vassilis N. Ioannidis, Antonio G. Marques, and Georgios B. Giannakis", "title": "A Recurrent Graph Neural Network for Multi-Relational Data", "comments": "Submitted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The era of data deluge has sparked the interest in graph-based learning\nmethods in a number of disciplines such as sociology, biology, neuroscience, or\nengineering. In this paper, we introduce a graph recurrent neural network\n(GRNN) for scalable semi-supervised learning from multi-relational data. Key\naspects of the novel GRNN architecture are the use of multi-relational graphs,\nthe dynamic adaptation to the different relations via learnable weights, and\nthe consideration of graph-based regularizers to promote smoothness and\nalleviate over-parametrization. Our ultimate goal is to design a powerful\nlearning architecture able to: discover complex and highly non-linear data\nassociations, combine (and select) multiple types of relations, and scale\ngracefully with respect to the size of the graph. Numerical tests with real\ndata sets corroborate the design goals and illustrate the performance gains\nrelative to competing alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 22:21:43 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 17:30:03 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 01:26:51 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Ioannidis", "Vassilis N.", ""], ["Marques", "Antonio G.", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1811.02067", "submitter": "Christopher Snyder", "authors": "Christopher Snyder and Sriram Vishwanath", "title": "Sample Compression, Support Vectors, and Generalization in Deep Learning", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though Deep Neural Networks (DNNs) are widely celebrated for their\npractical performance, they possess many intriguing properties related to depth\nthat are difficult to explain both theoretically and intuitively. Understanding\nhow weights in deep networks coordinate together across layers to form useful\nlearners has proven challenging, in part because the repeated composition of\nnonlinearities has proved intractable. This paper presents a reparameterization\nof DNNs as a linear function of a feature map that is locally independent of\nthe weights. This feature map transforms depth-dependencies into simple tensor\nproducts and maps each input to a discrete subset of the feature space. Then,\nusing a max-margin assumption, the paper develops a sample compression\nrepresentation of the neural network in terms of the discrete activation state\nof neurons induced by s ``support vectors\". The paper shows that the number of\nsupport vectors s relates with learning guarantees for neural networks through\nsample compression bounds, yielding a sample complexity of O(ns/epsilon) for\nnetworks with n neurons. Finally, the number of support vectors s is found to\nmonotonically increase with width and label noise but decrease with depth.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 22:32:15 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 20:43:01 GMT"}, {"version": "v3", "created": "Mon, 21 Oct 2019 19:37:01 GMT"}, {"version": "v4", "created": "Tue, 17 Mar 2020 16:54:10 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Snyder", "Christopher", ""], ["Vishwanath", "Sriram", ""]]}, {"id": "1811.02073", "submitter": "Shangtong Zhang", "authors": "Shangtong Zhang, Borislav Mavrin, Linglong Kong, Bo Liu, Hengshuai Yao", "title": "QUOTA: The Quantile Option Architecture for Reinforcement Learning", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the Quantile Option Architecture (QUOTA) for\nexploration based on recent advances in distributional reinforcement learning\n(RL). In QUOTA, decision making is based on quantiles of a value distribution,\nnot only the mean. QUOTA provides a new dimension for exploration via making\nuse of both optimism and pessimism of a value distribution. We demonstrate the\nperformance advantage of QUOTA in both challenging video games and physical\nrobot simulators.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 22:49:03 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 19:52:47 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Zhang", "Shangtong", ""], ["Mavrin", "Borislav", ""], ["Kong", "Linglong", ""], ["Liu", "Bo", ""], ["Yao", "Hengshuai", ""]]}, {"id": "1811.02084", "submitter": "Noam Shazeer", "authors": "Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani,\n  Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff\n  Young, Ryan Sepassi, Blake Hechtman", "title": "Mesh-TensorFlow: Deep Learning for Supercomputers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch-splitting (data-parallelism) is the dominant distributed Deep Neural\nNetwork (DNN) training strategy, due to its universal applicability and its\namenability to Single-Program-Multiple-Data (SPMD) programming. However,\nbatch-splitting suffers from problems including the inability to train very\nlarge models (due to memory constraints), high latency, and inefficiency at\nsmall batch sizes. All of these can be solved by more general distribution\nstrategies (model-parallelism). Unfortunately, efficient model-parallel\nalgorithms tend to be complicated to discover, describe, and to implement,\nparticularly on large clusters. We introduce Mesh-TensorFlow, a language for\nspecifying a general class of distributed tensor computations. Where\ndata-parallelism can be viewed as splitting tensors and operations along the\n\"batch\" dimension, in Mesh-TensorFlow, the user can specify any\ntensor-dimensions to be split across any dimensions of a multi-dimensional mesh\nof processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting\nof parallel operations coupled with collective communication primitives such as\nAllreduce. We use Mesh-TensorFlow to implement an efficient data-parallel,\nmodel-parallel version of the Transformer sequence-to-sequence model. Using TPU\nmeshes of up to 512 cores, we train Transformer models with up to 5 billion\nparameters, surpassing state of the art results on WMT'14 English-to-French\ntranslation task and the one-billion-word language modeling benchmark.\nMesh-Tensorflow is available at https://github.com/tensorflow/mesh .\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 23:25:02 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Shazeer", "Noam", ""], ["Cheng", "Youlong", ""], ["Parmar", "Niki", ""], ["Tran", "Dustin", ""], ["Vaswani", "Ashish", ""], ["Koanantakool", "Penporn", ""], ["Hawkins", "Peter", ""], ["Lee", "HyoukJoong", ""], ["Hong", "Mingsheng", ""], ["Young", "Cliff", ""], ["Sepassi", "Ryan", ""], ["Hechtman", "Blake", ""]]}, {"id": "1811.02089", "submitter": "Pan Li", "authors": "Pan Li, Gregory J. Puleo, Olgica Milenkovic", "title": "Motif and Hypergraph Correlation Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in social and biological network analysis, we\nintroduce a new form of agnostic clustering termed~\\emph{motif correlation\nclustering}, which aims to minimize the cost of clustering errors associated\nwith both edges and higher-order network structures. The problem may be\nsuccinctly described as follows: Given a complete graph $G$, partition the\nvertices of the graph so that certain predetermined `important' subgraphs\nmostly lie within the same cluster, while `less relevant' subgraphs are allowed\nto lie across clusters. Our contributions are as follows: We first introduce\nseveral variants of motif correlation clustering and then show that these\nclustering problems are NP-hard. We then proceed to describe polynomial-time\nclustering algorithms that provide constant approximation guarantees for the\nproblems at hand. Despite following the frequently used LP relaxation and\nrounding procedure, the algorithms involve a sophisticated and carefully\ndesigned neighborhood growing step that combines information about both edge\nand motif structures. We conclude with several examples illustrating the\nperformance of the developed algorithms on synthetic and real networks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 23:40:03 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Li", "Pan", ""], ["Puleo", "Gregory J.", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1811.02091", "submitter": "Dustin Tran", "authors": "Dustin Tran, Matthew Hoffman, Dave Moore, Christopher Suter, Srinivas\n  Vasudevan, Alexey Radul, Matthew Johnson, Rif A. Saurous", "title": "Simple, Distributed, and Accelerated Probabilistic Programming", "comments": "Appears in Neural Information Processing Systems, 2018. Code\n  available at http://bit.ly/2JpFipt", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a simple, low-level approach for embedding probabilistic\nprogramming in a deep learning ecosystem. In particular, we distill\nprobabilistic programming down to a single abstraction---the random variable.\nOur lightweight implementation in TensorFlow enables numerous applications: a\nmodel-parallel variational auto-encoder (VAE) with 2nd-generation tensor\nprocessing units (TPUv2s); a data-parallel autoregressive model (Image\nTransformer) with TPUv2s; and multi-GPU No-U-Turn Sampler (NUTS). For both a\nstate-of-the-art VAE on 64x64 ImageNet and Image Transformer on 256x256\nCelebA-HQ, our approach achieves an optimal linear speedup from 1 to 256 TPUv2\nchips. With NUTS, we see a 100x speedup on GPUs over Stan and 37x over PyMC3.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 23:53:59 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 02:56:29 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Tran", "Dustin", ""], ["Hoffman", "Matthew", ""], ["Moore", "Dave", ""], ["Suter", "Christopher", ""], ["Vasudevan", "Srinivas", ""], ["Radul", "Alexey", ""], ["Johnson", "Matthew", ""], ["Saurous", "Rif A.", ""]]}, {"id": "1811.02095", "submitter": "Like Hui", "authors": "Like Hui, Siyuan Ma, Mikhail Belkin", "title": "Kernel Machines Beat Deep Neural Networks on Mask-based Single-channel\n  Speech Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply a fast kernel method for mask-based single-channel speech\nenhancement. Specifically, our method solves a kernel regression problem\nassociated to a non-smooth kernel function (exponential power kernel) with a\nhighly efficient iterative method (EigenPro). Due to the simplicity of this\nmethod, its hyper-parameters such as kernel bandwidth can be automatically and\nefficiently selected using line search with subsamples of training data. We\nobserve an empirical correlation between the regression loss (mean square\nerror) and regular metrics for speech enhancement. This observation justifies\nour training target and motivates us to achieve lower regression loss by\ntraining separate kernel model per frequency subband. We compare our method\nwith the state-of-the-art deep neural networks on mask-based HINT and TIMIT.\nExperimental results show that our kernel method consistently outperforms deep\nneural networks while requiring less training time.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 00:04:55 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Hui", "Like", ""], ["Ma", "Siyuan", ""], ["Belkin", "Mikhail", ""]]}, {"id": "1811.02096", "submitter": "Po-Ling Loh", "authors": "Po-Ling Loh", "title": "Scale calibration for high-dimensional robust regression", "comments": "43 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for high-dimensional linear regression when a scale\nparameter of the additive errors is unknown. The proposed estimator is based on\na penalized Huber $M$-estimator, for which theoretical results on estimation\nerror have recently been proposed in high-dimensional statistics literature.\nHowever, the variance of the error term in the linear model is intricately\nconnected to the optimal parameter used to define the shape of the Huber loss.\nOur main idea is to use an adaptive technique, based on Lepski's method, to\novercome the difficulties in solving a joint nonconvex optimization problem\nwith respect to the location and scale parameters.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 00:07:17 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Loh", "Po-Ling", ""]]}, {"id": "1811.02102", "submitter": "Yu Li", "authors": "Yu Li, Hu Wang, Xinjian Deng", "title": "Image-Based Reconstruction for a 3D-PFHS Heat Transfer Problem by\n  ReConNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The heat transfer performance of Plate Fin Heat Sink (PFHS) has been\ninvestigated experimentally and extensively. Commonly, the objective function\nof the PFHS design is based on the responses of simulations. Compared with\nexisting studies, the purpose of this study is to transfer from analysis-based\nmodel to image-based one for heat sink designs. Compared with the popular\nobjective function based on maximum, mean, variance values etc., more\ninformation should be involved in image-based and thus a more objective model\nshould be constructed. It means that the sequential optimization should be\nbased on images instead of responses and more reasonable solutions should be\nobtained. Therefore, an image-based reconstruction model of a heat transfer\nprocess for a 3D-PFHS is established. Unlike image recognition, such procedure\ncannot be implemented by existing recognition algorithms (e.g. Convolutional\nNeural Network) directly. Therefore, a Reconstructive Neural Network (ReConNN),\nintegrated supervised learning and unsupervised learning techniques, is\nsuggested and improved to achieve higher accuracy. According to the\nexperimental results, the heat transfer process can be observed more detailed\nand clearly, and the reconstructed results are meaningful for the further\noptimizations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 00:38:35 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 00:58:47 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Li", "Yu", ""], ["Wang", "Hu", ""], ["Deng", "Xinjian", ""]]}, {"id": "1811.02113", "submitter": "German I. Parisi", "authors": "German I. Parisi, Xu Ji, Stefan Wermter", "title": "On the role of neurogenesis in overcoming catastrophic forgetting", "comments": "Accepted to NIPS'18 Workshop on Continual Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifelong learning capabilities are crucial for artificial autonomous agents\noperating on real-world data, which is typically non-stationary and temporally\ncorrelated. In this work, we demonstrate that dynamically grown networks\noutperform static networks in incremental learning scenarios, even when bounded\nby the same amount of memory in both cases. Learning is unsupervised in our\nmodels, a condition that additionally makes training more challenging whilst\nincreasing the realism of the study, since humans are able to learn without\ndense manual annotation. Our results on artificial neural networks reinforce\nthat structural plasticity constitutes effective prevention against\ncatastrophic forgetting in non-stationary environments, as well as empirically\nsupporting the importance of neurogenesis in the mammalian brain.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 01:38:26 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 04:41:56 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Parisi", "German I.", ""], ["Ji", "Xu", ""], ["Wermter", "Stefan", ""]]}, {"id": "1811.02114", "submitter": "Ingoo Lee", "authors": "Ingoo Lee, Jongsoo Keum, Hojung Nam", "title": "DeepConv-DTI: Prediction of drug-target interactions via deep learning\n  with convolution on protein sequences", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1007129", "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of drug-target interactions (DTIs) plays a key role in drug\ndiscovery. The high cost and labor-intensive nature of in vitro and in vivo\nexperiments have highlighted the importance of in silico-based DTI prediction\napproaches. In several computational models, conventional protein descriptors\nare shown to be not informative enough to predict accurate DTIs. Thus, in this\nstudy, we employ a convolutional neural network (CNN) on raw protein sequences\nto capture local residue patterns participating in DTIs. With CNN on protein\nsequences, our model performs better than previous protein descriptor-based\nmodels. In addition, our model performs better than the previous deep learning\nmodel for massive prediction of DTIs. By examining the pooled convolution\nresults, we found that our model can detect binding sites of proteins for DTIs.\nIn conclusion, our prediction model for detecting local residue patterns of\ntarget proteins successfully enriches the protein features of a raw protein\nsequence, yielding better prediction results than previous approaches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 01:39:02 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Lee", "Ingoo", ""], ["Keum", "Jongsoo", ""], ["Nam", "Hojung", ""]]}, {"id": "1811.02117", "submitter": "Sha Yuan", "authors": "Sha Yuan, Yu Zhang, Jie Tang, Huawei Shen, Xingxing Wei", "title": "Modeling and Predicting Popularity Dynamics via Deep Learning Attention\n  Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ability to predict the popularity dynamics of individual items within a\ncomplex evolving system has important implications in a wide range of domains.\nHere we propose a deep learning attention mechanism to model the process\nthrough which individual items gain their popularity. We analyze the\ninterpretability of the model with the four key phenomena confirmed\nindependently in the previous studies of long-term popularity dynamics\nquantification, including the intrinsic quality, the aging effect, the recency\neffect and the Matthew effect. We analyze the effectiveness of introducing\nattention model in popularity dynamics prediction. Extensive experiments on a\nreal-large citation data set demonstrate that the designed deep learning\nattention mechanism possesses remarkable power at predicting the long-term\npopularity dynamics. It consistently outperforms the existing methods, and\nachieves a significant performance improvement.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 01:44:13 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Yuan", "Sha", ""], ["Zhang", "Yu", ""], ["Tang", "Jie", ""], ["Shen", "Huawei", ""], ["Wei", "Xingxing", ""]]}, {"id": "1811.02122", "submitter": "Younggun Lee", "authors": "Younggun Lee and Taesu Kim", "title": "Robust and fine-grained prosody control of end-to-end speech synthesis", "comments": "ICASSP 2019, best viewed in color", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose prosody embeddings for emotional and expressive speech synthesis\nnetworks. The proposed methods introduce temporal structures in the embedding\nnetworks, thus enabling fine-grained control of the speaking style of the\nsynthesized speech. The temporal structures can be designed either on the\nspeech side or the text side, leading to different control resolutions in time.\nThe prosody embedding networks are plugged into end-to-end speech synthesis\nnetworks and trained without any other supervision except for the target speech\nfor synthesizing. It is demonstrated that the prosody embedding networks\nlearned to extract prosodic features. By adjusting the learned prosody\nfeatures, we could change the pitch and amplitude of the synthesized speech\nboth at the frame level and the phoneme level. We also introduce the temporal\nnormalization of prosody embeddings, which shows better robustness against\nspeaker perturbations during prosody transfer tasks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 01:54:22 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 06:33:37 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Lee", "Younggun", ""], ["Kim", "Taesu", ""]]}, {"id": "1811.02130", "submitter": "Prem Seetharaman", "authors": "Prem Seetharaman, Gordon Wichern, Jonathan Le Roux, Bryan Pardo", "title": "Bootstrapping single-channel source separation via unsupervised spatial\n  clustering on stereo mixtures", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Separating an audio scene into isolated sources is a fundamental problem in\ncomputer audition, analogous to image segmentation in visual scene analysis.\nSource separation systems based on deep learning are currently the most\nsuccessful approaches for solving the underdetermined separation problem, where\nthere are more sources than channels. Traditionally, such systems are trained\non sound mixtures where the ground truth decomposition is already known. Since\nmost real-world recordings do not have such a decomposition available, this\nlimits the range of mixtures one can train on, and the range of mixtures the\nlearned models may successfully separate. In this work, we use a simple blind\nspatial source separation algorithm to generate estimated decompositions of\nstereo mixtures. These estimates, together with a weighting scheme in the\ntime-frequency domain, based on confidence in the separation quality, are used\nto train a deep learning model that can be used for single-channel separation,\nwhere no source direction information is available. This demonstrates how a\nsimple cue such as the direction of origin of source can be used to bootstrap a\nmodel for source separation that can be used in situations where that cue is\nnot available.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 02:20:40 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Seetharaman", "Prem", ""], ["Wichern", "Gordon", ""], ["Roux", "Jonathan Le", ""], ["Pardo", "Bryan", ""]]}, {"id": "1811.02132", "submitter": "Guoqiang Zhong", "authors": "Jinxuan Sun, Guoqiang Zhong, Yang Chen, Yongbin Liu, Tao Li, Zhongwen\n  Guo", "title": "Student's t-Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have a great performance in image\ngeneration, but they need a large scale of data to train the entire framework,\nand often result in nonsensical results. We propose a new method referring to\nconditional GAN, which equipments the latent noise with mixture of Student's\nt-distribution with attention mechanism in addition to class information.\nStudent's t-distribution has long tails that can provide more diversity to the\nlatent noise. Meanwhile, the discriminator in our model implements two tasks\nsimultaneously, judging whether the images come from the true data\ndistribution, and identifying the class of each generated images. The\nparameters of the mixture model can be learned along with those of GANs.\nMoreover, we mathematically prove that any multivariate Student's\nt-distribution can be obtained by a linear transformation of a normal\nmultivariate Student's t-distribution. Experiments comparing the proposed\nmethod with typical GAN, DeliGAN and DCGAN indicate that, our method has a\ngreat performance on generating diverse and legible objects with limited data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 02:31:28 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Sun", "Jinxuan", ""], ["Zhong", "Guoqiang", ""], ["Chen", "Yang", ""], ["Liu", "Yongbin", ""], ["Li", "Tao", ""], ["Guo", "Zhongwen", ""]]}, {"id": "1811.02141", "submitter": "Matias Carrasco Kind", "authors": "Sahand Hariri, Matias Carrasco Kind, Robert J. Brunner", "title": "Extended Isolation Forest", "comments": "12 pages; 21 figures, Published. Open source code in\n  https://github.com/sahandha/eif", "journal-ref": null, "doi": "10.1109/TKDE.2019.2947676", "report-no": null, "categories": "cs.LG astro-ph.IM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an extension to the model-free anomaly detection algorithm,\nIsolation Forest. This extension, named Extended Isolation Forest (EIF),\nresolves issues with assignment of anomaly score to given data points. We\nmotivate the problem using heat maps for anomaly scores. These maps suffer from\nartifacts generated by the criteria for branching operation of the binary tree.\nWe explain this problem in detail and demonstrate the mechanism by which it\noccurs visually. We then propose two different approaches for improving the\nsituation. First we propose transforming the data randomly before creation of\neach tree, which results in averaging out the bias. Second, which is the\npreferred way, is to allow the slicing of the data to use hyperplanes with\nrandom slopes. This approach results in remedying the artifact seen in the\nanomaly score heat maps. We show that the robustness of the algorithm is much\nimproved using this method by looking at the variance of scores of data points\ndistributed along constant level sets. We report AUROC and AUPRC for our\nsynthetic datasets, along with real-world benchmark datasets. We find no\nappreciable difference in the rate of convergence nor in computation time\nbetween the standard Isolation Forest and EIF.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 03:02:13 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 23:20:56 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 05:38:57 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Hariri", "Sahand", ""], ["Kind", "Matias Carrasco", ""], ["Brunner", "Robert J.", ""]]}, {"id": "1811.02144", "submitter": "Li Tang", "authors": "Li Tang, Konstantinos Konstantinidis and Aditya Ramamoorthy", "title": "Erasure coding for distributed matrix multiplication for matrices with\n  bounded entries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed matrix multiplication is widely used in several scientific\ndomains. It is well recognized that computation times on distributed clusters\nare often dominated by the slowest workers (called stragglers). Recent work has\ndemonstrated that straggler mitigation can be viewed as a problem of designing\nerasure codes. For matrices $\\mathbf A$ and $\\mathbf B$, the technique\nessentially maps the computation of $\\mathbf A^T \\mathbf B$ into the\nmultiplication of smaller (coded) submatrices. The stragglers are treated as\nerasures in this process. The computation can be completed as long as a certain\nnumber of workers (called the recovery threshold) complete their assigned\ntasks.\n  We present a novel coding strategy for this problem when the absolute values\nof the matrix entries are sufficiently small. We demonstrate a tradeoff between\nthe assumed absolute value bounds on the matrix entries and the recovery\nthreshold. At one extreme, we are optimal with respect to the recovery\nthreshold and on the other extreme, we match the threshold of prior work.\nExperimental results on cloud-based clusters validate the benefits of our\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 03:24:06 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 17:34:47 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Tang", "Li", ""], ["Konstantinidis", "Konstantinos", ""], ["Ramamoorthy", "Aditya", ""]]}, {"id": "1811.02161", "submitter": "Aadirupa Saha", "authors": "Aadirupa Saha, Rakesh Shivanna, Chiranjib Bhattacharyya", "title": "How Many Pairwise Preferences Do We Need to Rank A Graph Consistently?", "comments": "In Thirty-Third AAAI Conference on Artificial Intelligence, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of optimal recovery of true ranking of $n$ items from\na randomly chosen subset of their pairwise preferences. It is well known that\nwithout any further assumption, one requires a sample size of $\\Omega(n^2)$ for\nthe purpose. We analyze the problem with an additional structure of relational\ngraph $G([n],E)$ over the $n$ items added with an assumption of\n\\emph{locality}: Neighboring items are similar in their rankings. Noting the\npreferential nature of the data, we choose to embed not the graph, but, its\n\\emph{strong product} to capture the pairwise node relationships. Furthermore,\nunlike existing literature that uses Laplacian embedding for graph based\nlearning problems, we use a richer class of graph\nembeddings---\\emph{orthonormal representations}---that includes (normalized)\nLaplacian as its special case. Our proposed algorithm, {\\it Pref-Rank},\npredicts the underlying ranking using an SVM based approach over the chosen\nembedding of the product graph, and is the first to provide \\emph{statistical\nconsistency} on two ranking losses: \\emph{Kendall's tau} and \\emph{Spearman's\nfootrule}, with a required sample complexity of $O(n^2\n\\chi(\\bar{G}))^{\\frac{2}{3}}$ pairs, $\\chi(\\bar{G})$ being the \\emph{chromatic\nnumber} of the complement graph $\\bar{G}$. Clearly, our sample complexity is\nsmaller for dense graphs, with $\\chi(\\bar G)$ characterizing the degree of node\nconnectivity, which is also intuitive due to the locality assumption e.g.\n$O(n^\\frac{4}{3})$ for union of $k$-cliques, or $O(n^\\frac{5}{3})$ for random\nand power law graphs etc.---a quantity much smaller than the fundamental limit\nof $\\Omega(n^2)$ for large $n$. This, for the first time, relates ranking\ncomplexity to structural properties of the graph. We also report experimental\nevaluations on different synthetic and real datasets, where our algorithm is\nshown to outperform the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 04:54:18 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 20:26:20 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Saha", "Aadirupa", ""], ["Shivanna", "Rakesh", ""], ["Bhattacharyya", "Chiranjib", ""]]}, {"id": "1811.02166", "submitter": "Shun Zheng", "authors": "Shun Zheng, Xu Han, Yankai Lin, Peilin Yu, Lu Chen, Ling Huang,\n  Zhiyuan Liu and Wei Xu", "title": "DIAG-NRE: A Neural Pattern Diagnosis Framework for Distantly Supervised\n  Neural Relation Extraction", "comments": "Accepted by ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern-based labeling methods have achieved promising results in alleviating\nthe inevitable labeling noises of distantly supervised neural relation\nextraction. However, these methods require significant expert labor to write\nrelation-specific patterns, which makes them too sophisticated to generalize\nquickly.To ease the labor-intensive workload of pattern writing and enable the\nquick generalization to new relation types, we propose a neural pattern\ndiagnosis framework, DIAG-NRE, that can automatically summarize and refine\nhigh-quality relational patterns from noise data with human experts in the\nloop. To demonstrate the effectiveness of DIAG-NRE, we apply it to two\nreal-world datasets and present both significant and interpretable improvements\nover state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 05:08:59 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 09:57:21 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Zheng", "Shun", ""], ["Han", "Xu", ""], ["Lin", "Yankai", ""], ["Yu", "Peilin", ""], ["Chen", "Lu", ""], ["Huang", "Ling", ""], ["Liu", "Zhiyuan", ""], ["Xu", "Wei", ""]]}, {"id": "1811.02172", "submitter": "Chong Wang", "authors": "Jiangtao Feng, Lingpeng Kong, Po-Sen Huang, Chong Wang, Da Huang,\n  Jiayuan Mao, Kan Qiao, Dengyong Zhou", "title": "Neural Phrase-to-Phrase Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Neural Phrase-to-Phrase Machine Translation\n(NP$^2$MT). Our model uses a phrase attention mechanism to discover relevant\ninput (source) segments that are used by a decoder to generate output (target)\nphrases. We also design an efficient dynamic programming algorithm to decode\nsegments that allows the model to be trained faster than the existing neural\nphrase-based machine translation method by Huang et al. (2018). Furthermore,\nour method can naturally integrate with external phrase dictionaries during\ndecoding. Empirical experiments show that our method achieves comparable\nperformance with the state-of-the art methods on benchmark datasets. However,\nwhen the training and testing data are from different distributions or domains,\nour method performs better.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 05:47:52 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Feng", "Jiangtao", ""], ["Kong", "Lingpeng", ""], ["Huang", "Po-Sen", ""], ["Wang", "Chong", ""], ["Huang", "Da", ""], ["Mao", "Jiayuan", ""], ["Qiao", "Kan", ""], ["Zhou", "Dengyong", ""]]}, {"id": "1811.02182", "submitter": "Geonmin Kim", "authors": "Geonmin Kim, Hwaran Lee, Bo-Kyeong Kim, Sang-Hoon Oh, and Soo-Young\n  Lee", "title": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for\n  Speech Recognition", "comments": "will be published in IEEE Signal Processing Letter", "journal-ref": null, "doi": "10.1109/LSP.2018.2880285", "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many speech enhancement methods try to learn the relationship between noisy\nand clean speech, obtained using an acoustic room simulator. We point out\nseveral limitations of enhancement methods relying on clean speech targets; the\ngoal of this work is proposing an alternative learning algorithm, called\nacoustic and adversarial supervision (AAS). AAS makes the enhanced output both\nmaximizing the likelihood of transcription on the pre-trained acoustic model\nand having general characteristics of clean speech, which improve\ngeneralization on unseen noisy speeches. We employ the connectionist temporal\nclassification and the unpaired conditional boundary equilibrium generative\nadversarial network as the loss function of AAS. AAS is tested on two datasets\nincluding additive noise without and with reverberation, Librispeech + DEMAND\nand CHiME-4. By visualizing the enhanced speech with different loss\ncombinations, we demonstrate the role of each supervision. AAS achieves a lower\nword error rate than other state-of-the-art methods using the clean speech\ntarget in both datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 06:23:57 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Kim", "Geonmin", ""], ["Lee", "Hwaran", ""], ["Kim", "Bo-Kyeong", ""], ["Oh", "Sang-Hoon", ""], ["Lee", "Soo-Young", ""]]}, {"id": "1811.02184", "submitter": "Jonathan Lee", "authors": "Jonathan N. Lee, Michael Laskey, Ajay Kumar Tanwani, Anil Aswani, Ken\n  Goldberg", "title": "Dynamic Regret Convergence Analysis and an Adaptive Regularization\n  Algorithm for On-Policy Robot Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-policy imitation learning algorithms such as DAgger evolve a robot control\npolicy by executing it, measuring performance (loss), obtaining corrective\nfeedback from a supervisor, and generating the next policy. As the loss between\niterations can vary unpredictably, a fundamental question is under what\nconditions this process will eventually achieve a converged policy. If one\nassumes the underlying trajectory distribution is static (stationary), it is\npossible to prove convergence for DAgger. However, in more realistic models for\nrobotics, the underlying trajectory distribution is dynamic because it is a\nfunction of the policy. Recent results show it is possible to prove convergence\nof DAgger when a regularity condition on the rate of change of the trajectory\ndistributions is satisfied. In this article, we reframe this result using\ndynamic regret theory from the field of online optimization and show that\ndynamic regret can be applied to any on-policy algorithm to analyze its\nconvergence and optimality. These results inspire a new algorithm, Adaptive\nOn-Policy Regularization (AOR), that ensures the conditions for convergence. We\npresent simulation results with cart-pole balancing and locomotion benchmarks\nthat suggest AOR can significantly decrease dynamic regret and chattering as\nthe robot learns. To our knowledge, this the first application of dynamic\nregret theory to imitation learning.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 06:34:39 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 23:07:13 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Lee", "Jonathan N.", ""], ["Laskey", "Michael", ""], ["Tanwani", "Ajay Kumar", ""], ["Aswani", "Anil", ""], ["Goldberg", "Ken", ""]]}, {"id": "1811.02196", "submitter": "Utkarsh Porwal", "authors": "Utkarsh Porwal and Smruthi Mukund", "title": "Credit Card Fraud Detection in e-Commerce: An Outlier Detection Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often the challenge associated with tasks like fraud and spam detection is\nthe lack of all likely patterns needed to train suitable supervised learning\nmodels. This problem accentuates when the fraudulent patterns are not only\nscarce, they also change over time. Change in fraudulent pattern is because\nfraudsters continue to innovate novel ways to circumvent measures put in place\nto prevent fraud. Limited data and continuously changing patterns makes\nlearning significantly difficult. We hypothesize that good behavior does not\nchange with time and data points representing good behavior have consistent\nspatial signature under different groupings. Based on this hypothesis we are\nproposing an approach that detects outliers in large data sets by assigning a\nconsistency score to each data point using an ensemble of clustering methods.\nOur main contribution is proposing a novel method that can detect outliers in\nlarge datasets and is robust to changing patterns. We also argue that area\nunder the ROC curve, although a commonly used metric to evaluate outlier\ndetection methods is not the right metric. Since outlier detection problems\nhave a skewed distribution of classes, precision-recall curves are better\nsuited because precision compares false positives to true positives (outliers)\nrather than true negatives (inliers) and therefore is not affected by the\nproblem of class imbalance. We show empirically that area under the\nprecision-recall curve is a better than ROC as an evaluation metric. The\nproposed approach is tested on the modified version of the Landsat satellite\ndataset, the modified version of the ann-thyroid dataset and a large real world\ncredit card fraud detection dataset available through Kaggle where we show\nsignificant improvement over the baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 07:06:38 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 00:00:30 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Porwal", "Utkarsh", ""], ["Mukund", "Smruthi", ""]]}, {"id": "1811.02198", "submitter": "Dongsheng Li", "authors": "Dongsheng Li and Chao Chen and Qin Lv and Junchi Yan and Li Shang and\n  Stephen M. Chu", "title": "Collaborative Filtering with Stability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) is a popular technique in today's recommender\nsystems, and matrix approximation-based CF methods have achieved great success\nin both rating prediction and top-N recommendation tasks. However, real-world\nuser-item rating matrices are typically sparse, incomplete and noisy, which\nintroduce challenges to the algorithm stability of matrix approximation, i.e.,\nsmall changes in the training data may significantly change the models. As a\nresult, existing matrix approximation solutions yield low generalization\nperformance, exhibiting high error variance on the training data, and\nminimizing the training error may not guarantee error reduction on the test\ndata. This paper investigates the algorithm stability problem of matrix\napproximation methods and how to achieve stable collaborative filtering via\nstable matrix approximation. We present a new algorithm design framework, which\n(1) introduces new optimization objectives to guide stable matrix approximation\nalgorithm design, and (2) solves the optimization problem to obtain stable\napproximation solutions with good generalization performance. Experimental\nresults on real-world datasets demonstrate that the proposed method can achieve\nbetter accuracy compared with state-of-the-art matrix approximation methods and\nensemble methods in both rating prediction and top-N recommendation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 07:13:23 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Li", "Dongsheng", ""], ["Chen", "Chao", ""], ["Lv", "Qin", ""], ["Yan", "Junchi", ""], ["Shang", "Li", ""], ["Chu", "Stephen M.", ""]]}, {"id": "1811.02213", "submitter": "Wieslaw Kopec", "authors": "Wies{\\l}aw Kope\\'c, Marcin Skibi\\'nski, Cezary Biele, Kinga Skorupska,\n  Dominika Tkaczyk, Anna Jaskulska, Katarzyna Abramczuk, Piotr Gago, Krzysztof\n  Marasek", "title": "Hybrid Approach to Automation, RPA and Machine Learning: a Method for\n  the Human-centered Design of Software Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CY cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the more prominent trends within Industry 4.0 is the drive to employ\nRobotic Process Automation (RPA), especially as one of the elements of the Lean\napproach. The full implementation of RPA is riddled with challenges relating\nboth to the reality of everyday business operations, from SMEs to SSCs and\nbeyond, and the social effects of the changing job market. To successfully\naddress these points there is a need to develop a solution that would adjust to\nthe existing business operations and at the same time lower the negative social\nimpact of the automation process.\n  To achieve these goals we propose a hybrid, human-centered approach to the\ndevelopment of software robots. This design and implementation method combines\nthe Living Lab approach with empowerment through participatory design to\nkick-start the co-development and co-maintenance of hybrid software robots\nwhich, supported by variety of AI methods and tools, including interactive and\ncollaborative ML in the cloud, transform menial job posts into higher-skilled\npositions, allowing former employees to stay on as robot co-designers and\nmaintainers, i.e. as co-programmers who supervise the machine learning\nprocesses with the use of tailored high-level RPA Domain Specific Languages\n(DSLs) to adjust the functioning of the robots and maintain operational\nflexibility.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 08:05:24 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Kope\u0107", "Wies\u0142aw", ""], ["Skibi\u0144ski", "Marcin", ""], ["Biele", "Cezary", ""], ["Skorupska", "Kinga", ""], ["Tkaczyk", "Dominika", ""], ["Jaskulska", "Anna", ""], ["Abramczuk", "Katarzyna", ""], ["Gago", "Piotr", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1811.02225", "submitter": "Pierre Ablin", "authors": "Pierre Ablin (PARIETAL), Dylan Fagot (IRIT), Herwig Wendt (IRIT),\n  Alexandre Gramfort (PARIETAL), C\\'edric F\\'evotte (IRIT)", "title": "A Quasi-Newton algorithm on the orthogonal manifold for NMF with\n  transform learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) is a popular method for audio spectral\nunmixing. While NMF is traditionally applied to off-the-shelf time-frequency\nrepresentations based on the short-time Fourier or Cosine transforms, the\nability to learn transforms from raw data attracts increasing attention.\nHowever, this adds an important computational overhead. When assumed orthogonal\n(like the Fourier or Cosine transforms), learning the transform yields a\nnon-convex optimization problem on the orthogonal matrix manifold. In this\npaper, we derive a quasi-Newton method on the manifold using sparse\napproximations of the Hessian. Experiments on synthetic and real audio data\nshow that the proposed algorithm out-performs state-of-the-art first-order and\ncoordinate-descent methods by orders of magnitude. A Python package for fast\nTL-NMF is released online at https://github.com/pierreablin/tlnmf.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 08:49:40 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Ablin", "Pierre", "", "PARIETAL"], ["Fagot", "Dylan", "", "IRIT"], ["Wendt", "Herwig", "", "IRIT"], ["Gramfort", "Alexandre", "", "PARIETAL"], ["F\u00e9votte", "C\u00e9dric", "", "IRIT"]]}, {"id": "1811.02228", "submitter": "Bo Dai", "authors": "Bo Dai, Hanjun Dai, Arthur Gretton, Le Song, Dale Schuurmans, Niao He", "title": "Kernel Exponential Family Estimation via Doubly Dual Embedding", "comments": "22 pages, 20 figures; AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate penalized maximum log-likelihood estimation for exponential\nfamily distributions whose natural parameter resides in a reproducing kernel\nHilbert space. Key to our approach is a novel technique, doubly dual embedding,\nthat avoids computation of the partition function. This technique also allows\nthe development of a flexible sampling strategy that amortizes the cost of\nMonte-Carlo sampling in the inference stage. The resulting estimator can be\neasily generalized to kernel conditional exponential families. We establish a\nconnection between kernel exponential family estimation and MMD-GANs, revealing\na new perspective for understanding GANs. Compared to the score matching based\nestimators, the proposed method improves both memory and time efficiency while\nenjoying stronger statistical properties, such as fully capturing smoothness in\nits statistical convergence rate while the score matching estimator appears to\nsaturate. Finally, we show that the proposed estimator empirically outperforms\nstate-of-the-art\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 08:51:51 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 23:21:40 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2019 06:26:10 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Dai", "Bo", ""], ["Dai", "Hanjun", ""], ["Gretton", "Arthur", ""], ["Song", "Le", ""], ["Schuurmans", "Dale", ""], ["He", "Niao", ""]]}, {"id": "1811.02234", "submitter": "Maxime Bucher", "authors": "Maxime Bucher (Palaiseau), St\\'ephane Herbin (Palaiseau), Fr\\'ed\\'eric\n  Jurie", "title": "Semantic bottleneck for computer vision tasks", "comments": null, "journal-ref": "Asian Conference on Computer Vision (ACCV), Dec 2018, Perth,\n  Australia", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel method for the representation of images that is\nsemantic by nature, addressing the question of computation intelligibility in\ncomputer vision tasks. More specifically, our proposition is to introduce what\nwe call a semantic bottleneck in the processing pipeline, which is a crossing\npoint in which the representation of the image is entirely expressed with\nnatural language , while retaining the efficiency of numerical representations.\nWe show that our approach is able to generate semantic representations that\ngive state-of-the-art results on semantic content-based image retrieval and\nalso perform very well on image classification tasks. Intelligibility is\nevaluated through user centered experiments for failure detection.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 09:01:02 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Bucher", "Maxime", "", "Palaiseau"], ["Herbin", "St\u00e9phane", "", "Palaiseau"], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1811.02248", "submitter": "Apostolos Modas", "authors": "Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard", "title": "SparseFool: a few pixels make a big difference", "comments": "In Proceedings of IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks have achieved extraordinary results on image\nclassification tasks, but have been shown to be vulnerable to attacks with\ncarefully crafted perturbations of the input data. Although most attacks\nusually change values of many image's pixels, it has been shown that deep\nnetworks are also vulnerable to sparse alterations of the input. However, no\ncomputationally efficient method has been proposed to compute sparse\nperturbations. In this paper, we exploit the low mean curvature of the decision\nboundary, and propose SparseFool, a geometry inspired sparse attack that\ncontrols the sparsity of the perturbations. Extensive evaluations show that our\napproach computes sparse perturbations very fast, and scales efficiently to\nhigh dimensional data. We further analyze the transferability and the visual\neffects of the perturbations, and show the existence of shared semantic\ninformation across the images and the networks. Finally, we show that\nadversarial training can only slightly improve the robustness against sparse\nadditive perturbations computed with SparseFool.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 09:30:34 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 11:56:58 GMT"}, {"version": "v3", "created": "Sun, 18 Nov 2018 19:41:22 GMT"}, {"version": "v4", "created": "Mon, 27 May 2019 16:33:41 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Modas", "Apostolos", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Frossard", "Pascal", ""]]}, {"id": "1811.02284", "submitter": "Johan Barthelemy", "authors": "Johan Barth\\'elemy and Morgane Dumont and Timoteo Carletti", "title": "Comparison of Discrete Choice Models and Artificial Neural Networks in\n  Presence of Missing Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification, the process of assigning a label (or class) to an observation\ngiven its features, is a common task in many applications. Nonetheless in most\nreal-life applications, the labels can not be fully explained by the observed\nfeatures. Indeed there can be many factors hidden to the modellers. The\nunexplained variation is then treated as some random noise which is handled\ndifferently depending on the method retained by the practitioner. This work\nfocuses on two simple and widely used supervised classification algorithms:\ndiscrete choice models and artificial neural networks in the context of binary\nclassification.\n  Through various numerical experiments involving continuous or discrete\nexplanatory features, we present a comparison of the retained methods'\nperformance in presence of missing variables. The impact of the distribution of\nthe two classes in the training data is also investigated. The outcomes of\nthose experiments highlight the fact that artificial neural networks\noutperforms the discrete choice models, except when the distribution of the\nclasses in the training data is highly unbalanced.\n  Finally, this work provides some guidelines for choosing the right classifier\nwith respect to the training data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 11:03:04 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Barth\u00e9lemy", "Johan", ""], ["Dumont", "Morgane", ""], ["Carletti", "Timoteo", ""]]}, {"id": "1811.02290", "submitter": "Zhaofei Yu", "authors": "Qi Yan, Yajing Zheng, Shanshan Jia, Yichen Zhang, Zhaofei Yu, Feng\n  Chen, Yonghong Tian, Tiejun Huang, Jian K. Liu", "title": "Revealing Fine Structures of the Retinal Receptive Field by Deep\n  Learning Networks", "comments": "updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have demonstrated impressive\nperformance on many visual tasks. Recently, they became useful models for the\nvisual system in neuroscience. However, it is still not clear what are learned\nby CNNs in terms of neuronal circuits. When a deep CNN with many layers is used\nfor the visual system, it is not easy to compare the structure components of\nCNNs with possible neuroscience underpinnings due to highly complex circuits\nfrom the retina to higher visual cortex. Here we address this issue by focusing\non single retinal ganglion cells with biophysical models and recording data\nfrom animals. By training CNNs with white noise images to predict neuronal\nresponses, we found that fine structures of the retinal receptive field can be\nrevealed. Specifically, convolutional filters learned are resembling biological\ncomponents of the retinal circuit. This suggests that a CNN learning from one\nsingle retinal cell reveals a minimal neural network carried out in this cell.\nFurthermore, when CNNs learned from different cells are transferred between\ncells, there is a diversity of transfer learning performance, which indicates\nthat CNNs are cell-specific. Moreover, when CNNs are transferred between\ndifferent types of input images, here white noise v.s. natural images, transfer\nlearning shows a good performance, which implies that CNNs indeed capture the\nfull computational ability of a single retinal cell for different inputs. Taken\ntogether, these results suggest that CNNs could be used to reveal structure\ncomponents of neuronal circuits, and provide a powerful model for neural system\nidentification.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 11:20:46 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 11:49:38 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Yan", "Qi", ""], ["Zheng", "Yajing", ""], ["Jia", "Shanshan", ""], ["Zhang", "Yichen", ""], ["Yu", "Zhaofei", ""], ["Chen", "Feng", ""], ["Tian", "Yonghong", ""], ["Huang", "Tiejun", ""], ["Liu", "Jian K.", ""]]}, {"id": "1811.02314", "submitter": "Arun Venkitaraman", "authors": "Arun Venkitaraman, Pascal Frossard, and Saikat Chatterjee", "title": "Kernel Regression for Graph Signal Prediction in Presence of Sparse\n  Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In presence of sparse noise we propose kernel regression for predicting\noutput vectors which are smooth over a given graph. Sparse noise models the\ntraining outputs being corrupted either with missing samples or large\nperturbations. The presence of sparse noise is handled using appropriate use of\n$\\ell_1$-norm along-with use of $\\ell_2$-norm in a convex cost function. For\noptimization of the cost function, we propose an iteratively reweighted\nleast-squares (IRLS) approach that is suitable for kernel substitution or\nkernel trick due to availability of a closed form solution. Simulations using\nreal-world temperature data show efficacy of our proposed method, mainly for\nlimited-size training datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 12:19:58 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Venkitaraman", "Arun", ""], ["Frossard", "Pascal", ""], ["Chatterjee", "Saikat", ""]]}, {"id": "1811.02316", "submitter": "Wouter van Loon", "authors": "Wouter van Loon, Marjolein Fokkema, Botond Szabo, Mark de Rooij", "title": "Stacked Penalized Logistic Regression for Selecting Views in Multi-View\n  Learning", "comments": "26 pages, 9 figures. Accepted manuscript", "journal-ref": "Information Fusion 61 (2020) 113-123", "doi": "10.1016/j.inffus.2020.03.007", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In biomedical research, many different types of patient data can be\ncollected, such as various types of omics data and medical imaging modalities.\nApplying multi-view learning to these different sources of information can\nincrease the accuracy of medical classification models compared with\nsingle-view procedures. However, collecting biomedical data can be expensive\nand/or burdening for patients, so that it is important to reduce the amount of\nrequired data collection. It is therefore necessary to develop multi-view\nlearning methods which can accurately identify those views that are most\nimportant for prediction. In recent years, several biomedical studies have used\nan approach known as multi-view stacking (MVS), where a model is trained on\neach view separately and the resulting predictions are combined through\nstacking. In these studies, MVS has been shown to increase classification\naccuracy. However, the MVS framework can also be used for selecting a subset of\nimportant views. To study the view selection potential of MVS, we develop a\nspecial case called stacked penalized logistic regression (StaPLR). Compared\nwith existing view-selection methods, StaPLR can make use of faster\noptimization algorithms and is easily parallelized. We show that nonnegativity\nconstraints on the parameters of the function which combines the views play an\nimportant role in preventing unimportant views from entering the model. We\ninvestigate the performance of StaPLR through simulations, and consider two\nreal data examples. We compare the performance of StaPLR with an existing view\nselection method called the group lasso and observe that, in terms of view\nselection, StaPLR is often more conservative and has a consistently lower false\npositive rate.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 12:23:52 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 13:18:44 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 14:26:11 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["van Loon", "Wouter", ""], ["Fokkema", "Marjolein", ""], ["Szabo", "Botond", ""], ["de Rooij", "Mark", ""]]}, {"id": "1811.02318", "submitter": "Lingbing Guo", "authors": "Lingbing Guo, Zequn Sun, Ermei Cao, Wei Hu", "title": "Recurrent Skipping Networks for Entity Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning knowledge graph (KG) embeddings for\nentity alignment (EA). Current methods use the embedding models mainly focusing\non triple-level learning, which lacks the ability of capturing long-term\ndependencies existing in KGs. Consequently, the embedding-based EA methods\nheavily rely on the amount of prior (known) alignment, due to the identity\ninformation in the prior alignment cannot be efficiently propagated from one KG\nto another. In this paper, we propose RSN4EA (recurrent skipping networks for\nEA), which leverages biased random walk sampling for generating long paths\nacross KGs and models the paths with a novel recurrent skipping network (RSN).\nRSN integrates the conventional recurrent neural network (RNN) with residual\nlearning and can largely improve the convergence speed and performance with\nonly a few more parameters. We evaluated RSN4EA on a series of datasets\nconstructed from real-world KGs. Our experimental results showed that it\noutperformed a number of state-of-the-art embedding-based EA methods and also\nachieved comparable performance for KG completion.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 12:28:58 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Guo", "Lingbing", ""], ["Sun", "Zequn", ""], ["Cao", "Ermei", ""], ["Hu", "Wei", ""]]}, {"id": "1811.02319", "submitter": "Yang Li", "authors": "Yang Li, Jiawei Jiang, Yingxia Shao and Bin Cui", "title": "Fast Hyperparameter Optimization of Deep Neural Networks via Ensembling\n  Multiple Surrogates", "comments": "More mature method is developed in the paper - MFES-HB", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of deep neural networks crucially depends on good\nhyperparameter configurations. Bayesian optimization is a powerful framework\nfor optimizing the hyperparameters of DNNs. These methods need sufficient\nevaluation data to approximate and minimize the validation error function of\nhyperparameters. However, the expensive evaluation cost of DNNs leads to very\nfew evaluation data within a limited time, which greatly reduces the efficiency\nof Bayesian optimization. Besides, the previous researches focus on using the\ncomplete evaluation data to conduct Bayesian optimization, and ignore the\nintermediate evaluation data generated by early stopping methods. To alleviate\nthe insufficient evaluation data problem, we propose a fast hyperparameter\noptimization method, HOIST, that utilizes both the complete and intermediate\nevaluation data to accelerate the hyperparameter optimization of DNNs.\nSpecifically, we train multiple basic surrogates to gather information from the\nmixed evaluation data, and then combine all basic surrogates using weighted\nbagging to provide an accurate ensemble surrogate. Our empirical studies show\nthat HOIST outperforms the state-of-the-art approaches on a wide range of DNNs,\nincluding feed forward neural networks, convolutional neural networks,\nrecurrent neural networks, and variational autoencoder.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 12:29:02 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 07:56:01 GMT"}, {"version": "v3", "created": "Wed, 21 Jul 2021 09:04:03 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Li", "Yang", ""], ["Jiang", "Jiawei", ""], ["Shao", "Yingxia", ""], ["Cui", "Bin", ""]]}, {"id": "1811.02320", "submitter": "Sihao Xue", "authors": "Yixiao Qu, Sihao Xue, Zhenyi Ying, Hang Zhou, Jue Sun", "title": "Hierarchical Neural Network Architecture In Keyword Spotting", "comments": "To be submitted in part to IEEE ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword Spotting (KWS) provides the start signal of ASR problem, and thus it\nis essential to ensure a high recall rate. However, its real-time property\nrequires low computation complexity. This contradiction inspires people to find\na suitable model which is small enough to perform well in multi environments.\nTo deal with this contradiction, we implement the Hierarchical Neural\nNetwork(HNN), which is proved to be effective in many speech recognition\nproblems. HNN outperforms traditional DNN and CNN even though its model size\nand computation complexity are slightly less. Also, its simple topology\nstructure makes easy to deploy on any device.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 12:32:27 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Qu", "Yixiao", ""], ["Xue", "Sihao", ""], ["Ying", "Zhenyi", ""], ["Zhou", "Hang", ""], ["Sun", "Jue", ""]]}, {"id": "1811.02322", "submitter": "Michael Kaufmann", "authors": "Michael Kaufmann, Thomas Parnell, Kornilios Kourtis", "title": "Elastic CoCoA: Scaling In to Improve Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we experimentally analyze the convergence behavior of CoCoA and\nshow, that the number of workers required to achieve the highest convergence\nrate at any point in time, changes over the course of the training. Based on\nthis observation, we build Chicle, an elastic framework that dynamically\nadjusts the number of workers based on feedback from the training algorithm, in\norder to select the number of workers that results in the highest convergence\nrate. In our evaluation of 6 datasets, we show that Chicle is able to\naccelerate the time-to-accuracy by a factor of up to 5.96x compared to the best\nstatic setting, while being robust enough to find an optimal or near-optimal\nsetting automatically in most cases.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 12:35:28 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Kaufmann", "Michael", ""], ["Parnell", "Thomas", ""], ["Kourtis", "Kornilios", ""]]}, {"id": "1811.02353", "submitter": "Mengying Lei", "authors": "Xian-Rui Zhang, Meng-Ying Lei, Yang Li", "title": "An amplitudes-perturbation data augmentation method in convolutional\n  neural networks for EEG decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-Computer Interface (BCI) system provides a pathway between humans and\nthe outside world by analyzing brain signals which contain potential neural\ninformation. Electroencephalography (EEG) is one of most commonly used brain\nsignals and EEG recognition is an important part of BCI system. Recently,\nconvolutional neural networks (ConvNet) in deep learning are becoming the new\ncutting edge tools to tackle the problem of EEG recognition. However, training\nan effective deep learning model requires a big number of data, which limits\nthe application of EEG datasets with a small number of samples. In order to\nsolve the issue of data insufficiency in deep learning for EEG decoding, we\npropose a novel data augmentation method that add perturbations to amplitudes\nof EEG signals after transform them to frequency domain. In experiments, we\nexplore the performance of signal recognition with the state-of-the-art models\nbefore and after data augmentation on BCI Competition IV dataset 2a and our\nlocal dataset. The results show that our data augmentation technique can\nimprove the accuracy of EEG recognition effectively.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 14:00:05 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Zhang", "Xian-Rui", ""], ["Lei", "Meng-Ying", ""], ["Li", "Yang", ""]]}, {"id": "1811.02361", "submitter": "Honglin Li", "authors": "Honglin Li, Frieder Ganz, Shirin Enshaeifar, Payam Barnaghi", "title": "Kalman Filter Modifier for Neural Networks in Non-stationary\n  Environments", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in a non-stationary environment is an inevitable problem when\napplying machine learning algorithm to real world environment. Learning new\ntasks without forgetting the previous knowledge is a challenge issue in machine\nlearning. We propose a Kalman Filter based modifier to maintain the performance\nof Neural Network models under non-stationary environments. The result shows\nthat our proposed model can preserve the key information and adapts better to\nthe changes. The accuracy of proposed model decreases by 0.4% in our\nexperiments, while the accuracy of conventional model decreases by 90% in the\ndrifts environment.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 14:16:29 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Li", "Honglin", ""], ["Ganz", "Frieder", ""], ["Enshaeifar", "Shirin", ""], ["Barnaghi", "Payam", ""]]}, {"id": "1811.02373", "submitter": "Vasily Morzhakov", "authors": "Vasily Morzhakov", "title": "Sets of autoencoders with shared latent spaces", "comments": "13 pages,16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoders receive latent models of input data. It was shown in recent\nworks that they also estimate probability density functions of the input. This\nfact makes using the Bayesian decision theory possible. If we obtain latent\nmodels of input data for each class or for some points in the space of\nparameters in a parameter estimation task, we are able to estimate likelihood\nfunctions for those classes or points in parameter space. We show how the set\nof autoencoders solves the recognition problem. Each autoencoder describes its\nown model or context, a latent vector that presents input data in the latent\nspace may be called treatment in its context. Sharing latent spaces of\nautoencoders gives a very important property that is the ability to separate\ntreatment and context where the input information is treated through the set of\nautoencoders. There are two remarkable and most valuable results of this work:\na mechanism that shows a possible way of forming abstract concepts and a way of\nreducing dataset's size during training. These results are confirmed by tests\npresented in the article.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 14:42:36 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Morzhakov", "Vasily", ""]]}, {"id": "1811.02384", "submitter": "Chunna Li", "authors": "Chun-Na Li, Yuan-Hai Shao, Zhen Wang, Nai-Yang Deng", "title": "Robust Bhattacharyya bound linear discriminant analysis through adaptive\n  algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel linear discriminant analysis criterion via\nthe Bhattacharyya error bound estimation based on a novel L1-norm (L1BLDA) and\nL2-norm (L2BLDA). Both L1BLDA and L2BLDA maximize the between-class scatters\nwhich are measured by the weighted pairwise distances of class means and\nmeanwhile minimize the within-class scatters under the L1-norm and L2-norm,\nrespectively. The proposed models can avoid the small sample size (SSS) problem\nand have no rank limit that may encounter in LDA. It is worth mentioning that,\nthe employment of L1-norm gives a robust performance of L1BLDA, and L1BLDA is\nsolved through an effective non-greedy alternating direction method of\nmultipliers (ADMM), where all the projection vectors can be obtained once for\nall. In addition, the weighting constants of L1BLDA and L2BLDA between the\nbetween-class and within-class terms are determined by the involved data set,\nwhich makes our L1BLDA and L2BLDA adaptive. The experimental results on both\nbenchmark data sets as well as the handwritten digit databases demonstrate the\neffectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 14:55:14 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Li", "Chun-Na", ""], ["Shao", "Yuan-Hai", ""], ["Wang", "Zhen", ""], ["Deng", "Nai-Yang", ""]]}, {"id": "1811.02438", "submitter": "Yuma Koizumi", "authors": "Yuma Koizumi, Noboru Harada, Yoichi Haneda", "title": "Trainable Adaptive Window Switching for Speech Enhancement", "comments": "accepted to the 44th International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes a trainable adaptive window switching (AWS) method and\napply it to a deep-neural-network (DNN) for speech enhancement in the modified\ndiscrete cosine transform domain. Time-frequency (T-F) mask processing in the\nshort-time Fourier transform (STFT)-domain is a typical speech enhancement\nmethod. To recover the target signal precisely, DNN-based short-time frequency\ntransforms have recently been investigated and used instead of the STFT.\nHowever, since such a fixed-resolution short-time frequency transform method\nhas a T-F resolution problem based on the uncertainty principle, not only the\nshort-time frequency transform but also the length of the windowing function\nshould be optimized. To overcome this problem, we incorporate AWS into the\nspeech enhancement procedure, and the windowing function of each time-frame is\nmanipulated using a DNN depending on the input signal. We confirmed that the\nproposed method achieved a higher signal-to-distortion ratio than conventional\nspeech enhancement methods in fixed-resolution frequency domains.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 12:25:42 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 09:14:01 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 05:45:31 GMT"}, {"version": "v4", "created": "Tue, 19 Feb 2019 23:56:50 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Koizumi", "Yuma", ""], ["Harada", "Noboru", ""], ["Haneda", "Yoichi", ""]]}, {"id": "1811.02454", "submitter": "Zhao Zhong", "authors": "Chen Lin, Zhao Zhong, Wei Wu, Junjie Yan", "title": "Synaptic Strength For Convolutional Neural Network", "comments": "Accepted by NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks(CNNs) are both computation and memory intensive\nwhich hindered their deployment in mobile devices. Inspired by the relevant\nconcept in neural science literature, we propose Synaptic Pruning: a\ndata-driven method to prune connections between input and output feature maps\nwith a newly proposed class of parameters called Synaptic Strength. Synaptic\nStrength is designed to capture the importance of a connection based on the\namount of information it transports. Experiment results show the effectiveness\nof our approach. On CIFAR-10, we prune connections for various CNN models with\nup to 96% , which results in significant size reduction and computation saving.\nFurther evaluation on ImageNet demonstrates that synaptic pruning is able to\ndiscover efficient models which is competitive to state-of-the-art compact CNNs\nsuch as MobileNet-V2 and NasNet-Mobile. Our contribution is summarized as\nfollowing: (1) We introduce Synaptic Strength, a new class of parameters for\nCNNs to indicate the importance of each connections. (2) Our approach can prune\nvarious CNNs with high compression without compromising accuracy. (3) Further\ninvestigation shows, the proposed Synaptic Strength is a better indicator for\nkernel pruning compared with the previous approach in both empirical result and\ntheoretical analysis.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 16:06:49 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Lin", "Chen", ""], ["Zhong", "Zhao", ""], ["Wu", "Wei", ""], ["Yan", "Junjie", ""]]}, {"id": "1811.02456", "submitter": "Robert Martorano", "authors": "Robert Frank Martorano III", "title": "Semantic Term \"Blurring\" and Stochastic \"Barcoding\" for Improved\n  Unsupervised Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of text data being produced in the modern age makes it\nincreasingly important to intuitively group, categorize, or classify text data\nby theme for efficient retrieval and search. Yet, the high dimensionality and\nimprecision of text data, or more generally language as a whole, prove to be\nchallenging when attempting to perform unsupervised document clustering. In\nthis thesis, we present two novel methods for improving unsupervised document\nclustering/classification by theme. The first is to improve document\nrepresentations. We look to exploit \"term neighborhoods\" and \"blur\" semantic\nweight across neighboring terms. These neighborhoods are located in the\nsemantic space afforded by \"word embeddings.\" The second method is for cluster\nrevision, based on what we deem as \"stochastic barcoding\", or \"S- Barcode\"\npatterns. Text data is inherently high dimensional, yet clustering typically\ntakes place in a low dimensional representation space. Our method utilizes\nlower dimension clustering results as initial cluster configurations, and\niteratively revises the configuration in the high dimensional space. We show\nwith experimental results how both of the two methods improve the quality of\ndocument clustering. While this thesis elaborates on the two new conceptual\ncontributions, a joint thesis by David Yan details the feature transformation\nand software architecture we developed for unsupervised document\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 16:08:31 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Martorano", "Robert Frank", "III"]]}, {"id": "1811.02459", "submitter": "Daniel Hernandez Diaz", "authors": "Daniel Hernandez, Antonio Khalil Moretti, Ziqiang Wei, Shreya Saxena,\n  John Cunningham and Liam Paninski", "title": "Nonlinear Evolution via Spatially-Dependent Linear Dynamics for\n  Electrophysiology and Calcium Data", "comments": "8 figs, Accepted at NBDT", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models have been widely applied for the analysis of time\nseries resulting from experimental neuroscience techniques. In these datasets,\nobservations are relatively smooth and possibly nonlinear. We present\nVariational Inference for Nonlinear Dynamics (VIND), a variational inference\nframework that is able to uncover nonlinear, smooth latent dynamics from\nsequential data. The framework is a direct extension of PfLDS; including a\nstructured approximate posterior describing spatially-dependent linear\ndynamics, as well as an algorithm that relies on the fixed-point iteration\nmethod to achieve convergence. We apply VIND to electrophysiology, single-cell\nvoltage and widefield imaging datasets with state-of-the-art results in\nreconstruction error. In single-cell voltage data, VIND finds a 5D latent\nspace, with variables akin to those of Hodgkin-Huxley-like models. VIND's\nlearned dynamics are further quantified by predicting future neural activity.\nVIND excels in this task, in some cases substantially outperforming current\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 16:10:56 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 16:18:42 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 18:00:58 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Hernandez", "Daniel", ""], ["Moretti", "Antonio Khalil", ""], ["Wei", "Ziqiang", ""], ["Saxena", "Shreya", ""], ["Cunningham", "John", ""], ["Paninski", "Liam", ""]]}, {"id": "1811.02471", "submitter": "Marc Ru{\\ss}wurm", "authors": "Marc Ru{\\ss}wurm and Marco K\\\"orner", "title": "Convolutional LSTMs for Cloud-Robust Segmentation of Remote Sensing\n  Imagery", "comments": "Cameraready version to NeurIPS 2018 Spatiotemporal Workshop.\n  Openreview: https://openreview.net/forum?id=Sye7df9CK7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clouds frequently cover the Earth's surface and pose an omnipresent challenge\nto optical Earth observation methods. The vast majority of remote sensing\napproaches either selectively choose single cloud-free observations or employ a\npre-classification strategy to identify and mask cloudy pixels. We follow a\ndifferent strategy and treat cloud coverage as noise that is inherent to the\nobserved satellite data. In prior work, we directly employed a straightforward\n\\emph{convolutional long short-term memory} network for vegetation\nclassification without explicit cloud filtering and achieved state-of-the-art\nclassification accuracies. In this work, we investigate this cloud-robustness\nfurther by visualizing internal cell activations and performing an ablation\nexperiment on datasets of different cloud coverage. In the visualizations of\nnetwork states, we identified some cells in which modulation and input gates\nclosed on cloudy pixels. This indicates that the network has internalized a\ncloud-filtering mechanism without being specifically trained on cloud labels.\nOverall, our results question the necessity of sophisticated pre-processing\npipelines for multi-temporal deep learning approaches.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 17:58:22 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 11:30:38 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Ru\u00dfwurm", "Marc", ""], ["K\u00f6rner", "Marco", ""]]}, {"id": "1811.02480", "submitter": "Giovanni Morrone", "authors": "Giovanni Morrone, Luca Pasa, Vadim Tikhanoff, Sonia Bergamaschi,\n  Luciano Fadiga, Leonardo Badino", "title": "Face Landmark-based Speaker-Independent Audio-Visual Speech Enhancement\n  in Multi-Talker Environments", "comments": "Proceedings of 2019 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP)", "journal-ref": null, "doi": "10.1109/ICASSP.2019.8682061", "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of enhancing the speech of a speaker of\ninterest in a cocktail party scenario when visual information of the speaker of\ninterest is available. Contrary to most previous studies, we do not learn\nvisual features on the typically small audio-visual datasets, but use an\nalready available face landmark detector (trained on a separate image dataset).\nThe landmarks are used by LSTM-based models to generate time-frequency masks\nwhich are applied to the acoustic mixed-speech spectrogram. Results show that:\n(i) landmark motion features are very effective features for this task, (ii)\nsimilarly to previous work, reconstruction of the target speaker's spectrogram\nmediated by masking is significantly more accurate than direct spectrogram\nreconstruction, and (iii) the best masks depend on both motion landmark\nfeatures and the input mixed-speech spectrogram. To the best of our knowledge,\nour proposed models are the first models trained and evaluated on the limited\nsize GRID and TCD-TIMIT datasets, that achieve speaker-independent speech\nenhancement in a multi-talker setting.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 16:35:01 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 10:40:59 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 16:27:49 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Morrone", "Giovanni", ""], ["Pasa", "Luca", ""], ["Tikhanoff", "Vadim", ""], ["Bergamaschi", "Sonia", ""], ["Fadiga", "Luciano", ""], ["Badino", "Leonardo", ""]]}, {"id": "1811.02483", "submitter": "Yufei Wang", "authors": "Yufei Wang, Zheyuan Ryan Shi, Lantao Yu, Yi Wu, Rohit Singh, Lucas\n  Joppa, Fei Fang", "title": "Deep Reinforcement Learning for Green Security Games with Real-Time\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Green Security Games (GSGs) have been proposed and applied to optimize\npatrols conducted by law enforcement agencies in green security domains such as\ncombating poaching, illegal logging and overfishing. However, real-time\ninformation such as footprints and agents' subsequent actions upon receiving\nthe information, e.g., rangers following the footprints to chase the poacher,\nhave been neglected in previous work. To fill the gap, we first propose a new\ngame model GSG-I which augments GSGs with sequential movement and the vital\nelement of real-time information. Second, we design a novel deep reinforcement\nlearning-based algorithm, DeDOL, to compute a patrolling strategy that adapts\nto the real-time information against a best-responding attacker. DeDOL is built\nupon the double oracle framework and the policy-space response oracle, solving\na restricted game and iteratively adding best response strategies to it through\ntraining deep Q-networks. Exploring the game structure, DeDOL uses\ndomain-specific heuristic strategies as initial strategies and constructs\nseveral local modes for efficient and parallelized training. To our knowledge,\nthis is the first attempt to use Deep Q-Learning for security games.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 16:43:47 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Wang", "Yufei", ""], ["Shi", "Zheyuan Ryan", ""], ["Yu", "Lantao", ""], ["Wu", "Yi", ""], ["Singh", "Rohit", ""], ["Joppa", "Lucas", ""], ["Fang", "Fei", ""]]}, {"id": "1811.02486", "submitter": "Igor Mordatch", "authors": "Igor Mordatch", "title": "Concept Learning with Energy-Based Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many hallmarks of human intelligence, such as generalizing from limited\nexperience, abstract reasoning and planning, analogical reasoning, creative\nproblem solving, and capacity for language require the ability to consolidate\nexperience into concepts, which act as basic building blocks of understanding\nand reasoning. We present a framework that defines a concept by an energy\nfunction over events in the environment, as well as an attention mask over\nentities participating in the event. Given few demonstration events, our method\nuses inference-time optimization procedure to generate events involving similar\nconcepts or identify entities involved in the concept. We evaluate our\nframework on learning visual, quantitative, relational, temporal concepts from\ndemonstration events in an unsupervised manner. Our approach is able to\nsuccessfully generate and identify concepts in a few-shot setting and resulting\nlearned concepts can be reused across environments. Example videos of our\nresults are available at sites.google.com/site/energyconceptmodels\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 16:52:20 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Mordatch", "Igor", ""]]}, {"id": "1811.02489", "submitter": "William Wilkinson", "authors": "William J. Wilkinson, Michael Riis Andersen, Joshua D. Reiss, Dan\n  Stowell, and Arno Solin", "title": "Unifying Probabilistic Models for Time-Frequency Analysis", "comments": "Accepted to International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In audio signal processing, probabilistic time-frequency models have many\nbenefits over their non-probabilistic counterparts. They adapt to the incoming\nsignal, quantify uncertainty, and measure correlation between the signal's\namplitude and phase information, making time domain resynthesis\nstraightforward. However, these models are still not widely used since they\ncome at a high computational cost, and because they are formulated in such a\nway that it can be difficult to interpret all the modelling assumptions. By\nshowing their equivalence to Spectral Mixture Gaussian processes, we illuminate\nthe underlying model assumptions and provide a general framework for\nconstructing more complex models that better approximate real-world signals.\nOur interpretation makes it intuitive to inspect, compare, and alter the models\nsince all prior knowledge is encoded in the Gaussian process kernel functions.\nWe utilise a state space representation to perform efficient inference via\nKalman smoothing, and we demonstrate how our interpretation allows for\nefficient parameter learning in the frequency domain.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 17:00:19 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 09:13:19 GMT"}, {"version": "v3", "created": "Thu, 8 Nov 2018 08:15:06 GMT"}, {"version": "v4", "created": "Fri, 9 Nov 2018 13:36:07 GMT"}, {"version": "v5", "created": "Mon, 12 Nov 2018 15:34:51 GMT"}, {"version": "v6", "created": "Tue, 12 Feb 2019 11:08:21 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Wilkinson", "William J.", ""], ["Andersen", "Michael Riis", ""], ["Reiss", "Joshua D.", ""], ["Stowell", "Dan", ""], ["Solin", "Arno", ""]]}, {"id": "1811.02491", "submitter": "Iqbal H. Sarker", "authors": "Iqbal H. Sarker", "title": "Mobile Data Science: Towards Understanding Data-Driven Intelligent\n  Mobile Applications", "comments": "Journal, 11 pages, Double Column", "journal-ref": "EAI Endorsed Transactions on Scalable Information Systems, 2018", "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the popularity of smart mobile phones and context-aware technology,\nvarious contextual data relevant to users' diverse activities with mobile\nphones is available around us. This enables the study on mobile phone data and\ncontext-awareness in computing, for the purpose of building data-driven\nintelligent mobile applications, not only on a single device but also in a\ndistributed environment for the benefit of end users. Based on the availability\nof mobile phone data, and the usefulness of data-driven applications, in this\npaper, we discuss about mobile data science that involves in collecting the\nmobile phone data from various sources and building data-driven models using\nmachine learning techniques, in order to make dynamic decisions intelligently\nin various day-to-day situations of the users. For this, we first discuss the\nfundamental concepts and the potentiality of mobile data science to build\nintelligent applications. We also highlight the key elements and explain\nvarious key modules involving in the process of mobile data science. This\narticle is the first in the field to draw a big picture, and thinking about\nmobile data science, and it's potentiality in developing various data-driven\nintelligent mobile applications. We believe this study will help both the\nresearchers and application developers for building smart data-driven mobile\napplications, to assist the end mobile phone users in their daily activities.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 20:19:46 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Sarker", "Iqbal H.", ""]]}, {"id": "1811.02496", "submitter": "Konstantinos Kamnitsas", "authors": "Chaitanya Baweja, Ben Glocker, Konstantinos Kamnitsas", "title": "Towards continual learning in medical imaging", "comments": "Accepted in Medical Imaging meets NIPS Workshop, NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates continual learning of two segmentation tasks in brain\nMRI with neural networks. To explore in this context the capabilities of\ncurrent methods for countering catastrophic forgetting of the first task when a\nnew one is learned, we investigate elastic weight consolidation, a recently\nproposed method based on Fisher information, originally evaluated on\nreinforcement learning of Atari games. We use it to sequentially learn\nsegmentation of normal brain structures and then segmentation of white matter\nlesions. Our findings show this recent method reduces catastrophic forgetting,\nwhile large room for improvement exists in these challenging settings for\ncontinual learning.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 17:09:17 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Baweja", "Chaitanya", ""], ["Glocker", "Ben", ""], ["Kamnitsas", "Konstantinos", ""]]}, {"id": "1811.02506", "submitter": "Viet Hung Tran", "authors": "Viet Hung Tran", "title": "Variational Bayes Inference in Digital Receivers", "comments": "PhD thesis, Trinity College Dublin, Ireland (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The digital telecommunications receiver is an important context for inference\nmethodology, the key objective being to minimize the expected loss function in\nrecovering the transmitted information. For that criterion, the optimal\ndecision is the Bayesian minimum-risk estimator. However, the computational\nload of the Bayesian estimator is often prohibitive and, hence, efficient\ncomputational schemes are required. The design of novel schemes, striking new\nbalances between accuracy and computational load, is the primary concern of\nthis thesis. Two popular techniques, one exact and one approximate, will be\nstudied.\n  The exact scheme is a recursive one, namely the generalized distributive law\n(GDL), whose purpose is to distribute all operators across the conditionally\nindependent (CI) factors of the joint model, so as to reduce the total number\nof operators required. In a novel theorem derived in this thesis, GDL, if\napplicable, will be shown to guarantee such a reduction in all cases. An\nassociated lemma also quantifies this reduction. For practical use, two novel\nalgorithms, namely the no-longer-needed (NLN) algorithm and the generalized\nform of the Markovian Forward-Backward (FB) algorithm, recursively factorizes\nand computes the CI factors of an arbitrary model, respectively.\n  The approximate scheme is an iterative one, namely the Variational Bayes (VB)\napproximation, whose purpose is to find the independent (i.e. zero-order\nMarkov) model closest to the true joint model in the minimum Kullback-Leibler\ndivergence (KLD) sense. Despite being computationally efficient, this naive\nmean field approximation confers only modest performance for highly correlated\nmodels. A novel approximation, namely Transformed Variational Bayes (TVB), will\nbe designed in the thesis in order to relax the zero-order constraint in the VB\napproximation, further reducing the KLD of the optimal approximation.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 12:42:15 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Tran", "Viet Hung", ""]]}, {"id": "1811.02525", "submitter": "Kin Gutierrez", "authors": "Kin Gutierrez, Jin Li, Cristian Challu, Artur Dubrawski", "title": "Double Adaptive Stochastic Gradient Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive moment methods have been remarkably successful in deep learning\noptimization, particularly in the presence of noisy and/or sparse gradients. We\nfurther the advantages of adaptive moment techniques by proposing a family of\ndouble adaptive stochastic gradient methods~\\textsc{DASGrad}. They leverage the\ncomplementary ideas of the adaptive moment algorithms widely used by deep\nlearning community, and recent advances in adaptive probabilistic algorithms.We\nanalyze the theoretical convergence improvements of our approach in a\nstochastic convex optimization setting, and provide empirical validation of our\nfindings with convex and non convex objectives. We observe that the benefits\nof~\\textsc{DASGrad} increase with the model complexity and variability of the\ngradients, and we explore the resulting utility in extensions of\ndistribution-matching multitask learning.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 17:47:34 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Gutierrez", "Kin", ""], ["Li", "Jin", ""], ["Challu", "Cristian", ""], ["Dubrawski", "Artur", ""]]}, {"id": "1811.02528", "submitter": "R\\'emi Francis", "authors": "R\\'emi Francis, Tom Ash, Will Williams", "title": "Discriminative training of RNNLMs with the average word error criterion", "comments": "Sumbitted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In automatic speech recognition (ASR), recurrent neural language models\n(RNNLM) are typically used to refine hypotheses in the form of lattices or\nn-best lists, which are generated by a beam search decoder with a weaker\nlanguage model. The RNNLMs are usually trained generatively using the\nperplexity (PPL) criterion on large corpora of grammatically correct text.\nHowever, the hypotheses are noisy, and the RNNLM doesn't always make the\nchoices that minimise the metric we optimise for, the word error rate (WER). To\naddress this mismatch we propose to use a task specific loss to train an RNNLM\nto discriminate between multiple hypotheses within lattice rescoring scenario.\nBy fine-tuning the RNNLM on lattices with the average edit distance loss, we\nshow that we obtain a 1.9% relative improvement in word error rate over a\npurely generatively trained model.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 17:53:54 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 16:50:45 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Francis", "R\u00e9mi", ""], ["Ash", "Tom", ""], ["Williams", "Will", ""]]}, {"id": "1811.02540", "submitter": "Gabriele Farina", "authors": "Gabriele Farina, Christian Kroer, Tuomas Sandholm", "title": "Regret Circuits: Composability of Regret Minimizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.GT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regret minimization is a powerful tool for solving large-scale problems; it\nwas recently used in breakthrough results for large-scale extensive-form game\nsolving. This was achieved by composing simplex regret minimizers into an\noverall regret-minimization framework for extensive-form game strategy spaces.\nIn this paper we study the general composability of regret minimizers. We\nderive a calculus for constructing regret minimizers for composite convex sets\nthat are obtained from convexity-preserving operations on simpler convex sets.\nWe show that local regret minimizers for the simpler sets can be combined with\nadditional regret minimizers into an aggregate regret minimizer for the\ncomposite set. As one application, we show that the CFR framework can be\nconstructed easily from our framework. We also show ways to include curtailing\n(constraining) operations into our framework. For one, they enables the\nconstruction of CFR generalization for extensive-form games with general convex\nstrategy constraints that can cut across decision points.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 18:30:27 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 20:30:43 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Farina", "Gabriele", ""], ["Kroer", "Christian", ""], ["Sandholm", "Tuomas", ""]]}, {"id": "1811.02549", "submitter": "Massimo Caccia", "authors": "Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle\n  Pineau, Laurent Charlin", "title": "Language GANs Falling Short", "comments": null, "journal-ref": "ICLR 2020 - Proceedings of the Seventh International Conference on\n  Learning Representation", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating high-quality text with sufficient diversity is essential for a\nwide range of Natural Language Generation (NLG) tasks. Maximum-Likelihood (MLE)\nmodels trained with teacher forcing have consistently been reported as weak\nbaselines, where poor performance is attributed to exposure bias (Bengio et\nal., 2015; Ranzato et al., 2015); at inference time, the model is fed its own\nprediction instead of a ground-truth token, which can lead to accumulating\nerrors and poor samples. This line of reasoning has led to an outbreak of\nadversarial based approaches for NLG, on the account that GANs do not suffer\nfrom exposure bias. In this work, we make several surprising observations which\ncontradict common beliefs. First, we revisit the canonical evaluation framework\nfor NLG, and point out fundamental flaws with quality-only evaluation: we show\nthat one can outperform such metrics using a simple, well-known temperature\nparameter to artificially reduce the entropy of the model's conditional\ndistributions. Second, we leverage the control over the quality / diversity\ntrade-off given by this parameter to evaluate models over the whole\nquality-diversity spectrum and find MLE models constantly outperform the\nproposed GAN variants over the whole quality-diversity space. Our results have\nseveral implications: 1) The impact of exposure bias on sample quality is less\nsevere than previously thought, 2) temperature tuning provides a better quality\n/ diversity trade-off than adversarial training while being easier to train,\neasier to cross-validate, and less computationally expensive. Code to reproduce\nthe experiments is available at github.com/pclucas14/GansFallingShort\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 18:44:11 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 05:29:16 GMT"}, {"version": "v3", "created": "Thu, 8 Nov 2018 04:40:20 GMT"}, {"version": "v4", "created": "Wed, 3 Apr 2019 19:55:38 GMT"}, {"version": "v5", "created": "Sat, 17 Aug 2019 00:01:05 GMT"}, {"version": "v6", "created": "Wed, 19 Feb 2020 22:44:37 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Caccia", "Massimo", ""], ["Caccia", "Lucas", ""], ["Fedus", "William", ""], ["Larochelle", "Hugo", ""], ["Pineau", "Joelle", ""], ["Charlin", "Laurent", ""]]}, {"id": "1811.02553", "submitter": "Andrew Ilyas", "authors": "Andrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras,\n  Firdaus Janoos, Larry Rudolph, Aleksander Madry", "title": "A Closer Look at Deep Policy Gradients", "comments": "ICLR 2020 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how the behavior of deep policy gradient algorithms reflects the\nconceptual framework motivating their development. To this end, we propose a\nfine-grained analysis of state-of-the-art methods based on key elements of this\nframework: gradient estimation, value prediction, and optimization landscapes.\nOur results show that the behavior of deep policy gradient algorithms often\ndeviates from what their motivating framework would predict: the surrogate\nobjective does not match the true reward landscape, learned value estimators\nfail to fit the true value function, and gradient estimates poorly correlate\nwith the \"true\" gradient. The mismatch between predicted and empirical behavior\nwe uncover highlights our poor understanding of current methods, and indicates\nthe need to move beyond current benchmark-centric evaluation methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 18:54:21 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 18:54:30 GMT"}, {"version": "v3", "created": "Sun, 2 Dec 2018 02:45:35 GMT"}, {"version": "v4", "created": "Mon, 25 May 2020 16:24:26 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Ilyas", "Andrew", ""], ["Engstrom", "Logan", ""], ["Santurkar", "Shibani", ""], ["Tsipras", "Dimitris", ""], ["Janoos", "Firdaus", ""], ["Rudolph", "Larry", ""], ["Madry", "Aleksander", ""]]}, {"id": "1811.02564", "submitter": "Siyuan Ma", "authors": "Raef Bassily, Mikhail Belkin, Siyuan Ma", "title": "On exponential convergence of SGD in non-convex over-parametrized\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large over-parametrized models learned via stochastic gradient descent (SGD)\nmethods have become a key element in modern machine learning. Although SGD\nmethods are very effective in practice, most theoretical analyses of SGD\nsuggest slower convergence than what is empirically observed. In our recent\nwork [8] we analyzed how interpolation, common in modern over-parametrized\nlearning, results in exponential convergence of SGD with constant step size for\nconvex loss functions. In this note, we extend those results to a much broader\nnon-convex function class satisfying the Polyak-Lojasiewicz (PL) condition. A\nnumber of important non-convex problems in machine learning, including some\nclasses of neural networks, have been recently shown to satisfy the PL\ncondition. We argue that the PL condition provides a relevant and attractive\nsetting for many machine learning problems, particularly in the\nover-parametrized regime.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 00:05:00 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Bassily", "Raef", ""], ["Belkin", "Mikhail", ""], ["Ma", "Siyuan", ""]]}, {"id": "1811.02566", "submitter": "Titouan Parcollet", "authors": "Titouan Parcollet, Mohamed Morchid, Georges Linar\\`es, Renato De Mori", "title": "Bidirectional Quaternion Long-Short Term Memory Recurrent Neural\n  Networks for Speech Recognition", "comments": "Submitted at ICASSP 2019. arXiv admin note: text overlap with\n  arXiv:1806.04418", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNN) are at the core of modern automatic speech\nrecognition (ASR) systems. In particular, long-short term memory (LSTM)\nrecurrent neural networks have achieved state-of-the-art results in many speech\nrecognition tasks, due to their efficient representation of long and short term\ndependencies in sequences of inter-dependent features. Nonetheless, internal\ndependencies within the element composing multidimensional features are weakly\nconsidered by traditional real-valued representations. We propose a novel\nquaternion long-short term memory (QLSTM) recurrent neural network that takes\ninto account both the external relations between the features composing a\nsequence, and these internal latent structural dependencies with the quaternion\nalgebra. QLSTMs are compared to LSTMs during a memory copy-task and a realistic\napplication of speech recognition on the Wall Street Journal (WSJ) dataset.\nQLSTM reaches better performances during the two experiments with up to $2.8$\ntimes less learning parameters, leading to a more expressive representation of\nthe information.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 21:17:34 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Parcollet", "Titouan", ""], ["Morchid", "Mohamed", ""], ["Linar\u00e8s", "Georges", ""], ["De Mori", "Renato", ""]]}, {"id": "1811.02579", "submitter": "Dallas Card", "authors": "Dallas Card and Michael Zhang and Noah A. Smith", "title": "Deep Weighted Averaging Classifiers", "comments": "13 pages, 8 figures, 5 tables, added DOI and updated to meet ACM\n  formatting requirements, In Proceedings of FAT* (2019)", "journal-ref": null, "doi": "10.1145/3287560.3287595", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have achieved impressive gains in\nclassification accuracy on a variety of types of data, including images and\ntext. Despite these gains, however, concerns have been raised about the\ncalibration, robustness, and interpretability of these models. In this paper we\npropose a simple way to modify any conventional deep architecture to\nautomatically provide more transparent explanations for classification\ndecisions, as well as an intuitive notion of the credibility of each\nprediction. Specifically, we draw on ideas from nonparametric kernel\nregression, and propose to predict labels based on a weighted sum of training\ninstances, where the weights are determined by distance in a learned\ninstance-embedding space. Working within the framework of conformal methods, we\npropose a new measure of nonconformity suggested by our model, and\nexperimentally validate the accompanying theoretical expectations,\ndemonstrating improved transparency, controlled error rates, and robustness to\nout-of-domain data, without compromising on accuracy or calibration.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 19:00:06 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 20:00:55 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Card", "Dallas", ""], ["Zhang", "Michael", ""], ["Smith", "Noah A.", ""]]}, {"id": "1811.02597", "submitter": "Sina Ghiassian", "authors": "Sina Ghiassian, Andrew Patterson, Martha White, Richard S. Sutton,\n  Adam White", "title": "Online Off-policy Prediction", "comments": "68 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of online prediction learning, where\nlearning proceeds continuously as the agent interacts with an environment. The\npredictions made by the agent are contingent on a particular way of behaving,\nrepresented as a value function. However, the behavior used to select actions\nand generate the behavior data might be different from the one used to define\nthe predictions, and thus the samples are generated off-policy. The ability to\nlearn behavior-contingent predictions online and off-policy has long been\nadvocated as a key capability of predictive-knowledge learning systems but\nremained an open algorithmic challenge for decades. The issue lies with the\ntemporal difference (TD) learning update at the heart of most prediction\nalgorithms: combining bootstrapping, off-policy sampling and function\napproximation may cause the value estimate to diverge. A breakthrough came with\nthe development of a new objective function that admitted stochastic gradient\ndescent variants of TD. Since then, many sound online off-policy prediction\nalgorithms have been developed, but there has been limited empirical work\ninvestigating the relative merits of all the variants. This paper aims to fill\nthese empirical gaps and provide clarity on the key ideas behind each method.\nWe summarize the large body of literature on off-policy learning, focusing on\n1- methods that use computation linear in the number of features and are\nconvergent under off-policy sampling, and 2- other methods which have proven\nuseful with non-fixed, nonlinear function approximation. We provide an\nempirical study of off-policy prediction methods in two challenging\nmicroworlds. We report each method's parameter sensitivity, empirical\nconvergence rate, and final performance, providing new insights that should\nenable practitioners to successfully extend these new methods to large-scale\napplications.[Abridged abstract]\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 19:09:04 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Ghiassian", "Sina", ""], ["Patterson", "Andrew", ""], ["White", "Martha", ""], ["Sutton", "Richard S.", ""], ["White", "Adam", ""]]}, {"id": "1811.02598", "submitter": "Yannis Pantazis", "authors": "Yannis Pantazis, Dipjyoti Paul, Michail Fasoulakis, Yannis Stylianou", "title": "Training Generative Adversarial Networks with Weights", "comments": "6 pages, 3 figures, submitted to Icassp2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impressive success of Generative Adversarial Networks (GANs) is often\novershadowed by the difficulties in their training. Despite the continuous\nefforts and improvements, there are still open issues regarding their\nconvergence properties. In this paper, we propose a simple training variation\nwhere suitable weights are defined and assist the training of the Generator. We\nprovide theoretical arguments why the proposed algorithm is better than the\nbaseline training in the sense of speeding up the training process and of\ncreating a stronger Generator. Performance results showed that the new\nalgorithm is more accurate in both synthetic and image datasets resulting in\nimprovements ranging between 5% and 50%.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 19:10:33 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Pantazis", "Yannis", ""], ["Paul", "Dipjyoti", ""], ["Fasoulakis", "Michail", ""], ["Stylianou", "Yannis", ""]]}, {"id": "1811.02615", "submitter": "Iqbal H. Sarker", "authors": "Iqbal H. Sarker", "title": "Understanding the Role of Data-Centric Social Context in Personalized\n  Mobile Applications", "comments": "Journal, EAI Endorsed Transactions on Context-aware Systems and\n  Applications, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-awareness in personalized mobile applications is a growing area of\nstudy. Social context is one of the most important sources of information in\nhuman-activity based applications. In this paper, we mainly focus on social\nrelational context that represents the interpersonal relationship between\nindividuals, and the role or influence of such context on users' diverse phone\ncall activities in their real world life. Individuals different phone call\nactivities such as making a phone call to a particular person or responding an\nincoming call may differ from person-to-person based on their interpersonal\nrelationships such as family, friend, or colleague. However, it is very\ndifficult to make the device understandable about such semantic relationships\nbetween individuals and the relevant context-aware applications. To address\nthis issue, in this paper, we explore the data-centric social relational\ncontext that can play a significant role in building context-aware personalized\nmobile applications for various purposes in our real world life.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 14:43:16 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Sarker", "Iqbal H.", ""]]}, {"id": "1811.02616", "submitter": "Yiwei Sun", "authors": "Yiwei Sun, Ngot Bui, Tsung-Yu Hsieh, Vasant Honavar", "title": "Multi-View Network Embedding Via Graph Factorization Clustering and\n  Co-Regularized Multi-View Agreement", "comments": "ICDMW2018 -- IEEE International Conference on Data Mining workshop on\n  Graph Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world social networks and digital platforms are comprised of individuals\n(nodes) that are linked to other individuals or entities through multiple types\nof relationships (links). Sub-networks of such a network based on each type of\nlink correspond to distinct views of the underlying network. In real-world\napplications, each node is typically linked to only a small subset of other\nnodes. Hence, practical approaches to problems such as node labeling have to\ncope with the resulting sparse networks. While low-dimensional network\nembeddings offer a promising approach to this problem, most of the current\nnetwork embedding methods focus primarily on single view networks. We introduce\na novel multi-view network embedding (MVNE) algorithm for constructing\nlow-dimensional node embeddings from multi-view networks. MVNE adapts and\nextends an approach to single view network embedding (SVNE) using graph\nfactorization clustering (GFC) to the multi-view setting using an objective\nfunction that maximizes the agreement between views based on both the local and\nglobal structure of the underlying multi-view graph. Our experiments with\nseveral benchmark real-world single view networks show that GFC-based SVNE\nyields network embeddings that are competitive with or superior to those\nproduced by the state-of-the-art single view network embedding methods when the\nembeddings are used for labeling unlabeled nodes in the networks. Our\nexperiments with several multi-view networks show that MVNE substantially\noutperforms the single view methods on integrated view and the state-of-the-art\nmulti-view methods. We further show that even when the goal is to predict\nlabels of nodes within a single target view, MVNE outperforms its single-view\ncounterpart suggesting that the MVNE is able to extract the information that is\nuseful for labeling nodes in the target view from the all of the views.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 20:19:06 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 19:01:21 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 08:58:18 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Sun", "Yiwei", ""], ["Bui", "Ngot", ""], ["Hsieh", "Tsung-Yu", ""], ["Honavar", "Vasant", ""]]}, {"id": "1811.02617", "submitter": "Kieran Greer Dr", "authors": "Kieran Greer", "title": "Category Trees", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.07042", "journal-ref": "WSEAS Transactions on Computer Research, Vol. 6, pp. 49 - 54,\n  2018. E-ISSN: 2415-1521", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a batch classifier that has been improved from the\nearlier version and fixed a mistake in the earlier paper. Two important changes\nhave been made. Each category is represented by a classifier, where each\nclassifier classifies its own subset of data rows, using batch input values to\nrepresent the centroid. The first change is to use the category centroid as the\ndesired category output. When the classifier represents more than one category,\nit creates a new layer and splits, to represent each category separately in the\nnew layer. The second change therefore, is to allow the classifier to branch to\nnew levels when there is a split in the data, or when some data rows are\nincorrectly classified. Each layer can therefore branch like a tree - not for\ndistinguishing features, but for distinguishing categories. The paper then\nsuggests further innovations, by adding fixed value ranges through bands, for\neach column or feature of the input dataset. When considering features, it is\nshown that some of the data can be classified directly through fixed value\nranges, while the rest can be classified using the classifier technique. Tests\nshow that the method can successfully classify a diverse set of benchmark\ndatasets to better than the state-of-the-art. The paper also discusses a\nbiological analogy with neurons and neuron links.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 20:21:26 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 10:32:40 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2019 11:01:22 GMT"}, {"version": "v4", "created": "Mon, 18 Nov 2019 15:00:57 GMT"}, {"version": "v5", "created": "Thu, 23 Jan 2020 14:43:39 GMT"}, {"version": "v6", "created": "Tue, 19 May 2020 14:26:39 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Greer", "Kieran", ""]]}, {"id": "1811.02619", "submitter": "Yaqi Duan", "authors": "Yaqi Duan, Zheng Tracy Ke, Mengdi Wang", "title": "State Aggregation Learning from Markov Transition Data", "comments": "Accepted to NeurIPS, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State aggregation is a popular model reduction method rooted in optimal\ncontrol. It reduces the complexity of engineering systems by mapping the\nsystem's states into a small number of meta-states. The choice of aggregation\nmap often depends on the data analysts' knowledge and is largely ad hoc. In\nthis paper, we propose a tractable algorithm that estimates the probabilistic\naggregation map from the system's trajectory. We adopt a soft-aggregation\nmodel, where each meta-state has a signature raw state, called an anchor state.\nThis model includes several common state aggregation models as special cases.\nOur proposed method is a simple two-step algorithm: The first step is spectral\ndecomposition of empirical transition matrix, and the second step conducts a\nlinear transformation of singular vectors to find their approximate convex\nhull. It outputs the aggregation distributions and disaggregation distributions\nfor each meta-state in explicit forms, which are not obtainable by classical\nspectral methods. On the theoretical side, we prove sharp error bounds for\nestimating the aggregation and disaggregation distributions and for identifying\nanchor states. The analysis relies on a new entry-wise deviation bound for\nsingular vectors of the empirical transition matrix of a Markov process, which\nis of independent interest and cannot be deduced from existing literature. The\napplication of our method to Manhattan traffic data successfully generates a\ndata-driven state aggregation map with nice interpretations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 20:31:37 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 00:17:54 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 01:29:52 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Duan", "Yaqi", ""], ["Ke", "Zheng Tracy", ""], ["Wang", "Mengdi", ""]]}, {"id": "1811.02625", "submitter": "Shiqi Wang", "authors": "Shiqi Wang, Yizheng Chen, Ahmed Abdou, Suman Jana", "title": "MixTrain: Scalable Training of Verifiably Robust Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making neural networks robust against adversarial inputs has resulted in an\narms race between new defenses and attacks. The most promising defenses,\nadversarially robust training and verifiably robust training, have limitations\nthat restrict their practical applications. The adversarially robust training\nonly makes the networks robust against a subclass of attackers and we reveal\nsuch weaknesses by developing a new attack based on interval gradients. By\ncontrast, verifiably robust training provides protection against any L-p\nnorm-bounded attacker but incurs orders of magnitude more computational and\nmemory overhead than adversarially robust training.\n  We propose two novel techniques, stochastic robust approximation and dynamic\nmixed training, to drastically improve the efficiency of verifiably robust\ntraining without sacrificing verified robustness. We leverage two critical\ninsights: (1) instead of over the entire training set, sound\nover-approximations over randomly subsampled training data points are\nsufficient for efficiently guiding the robust training process; and (2) We\nobserve that the test accuracy and verifiable robustness often conflict after\ncertain training epochs. Therefore, we use a dynamic loss function to\nadaptively balance them for each epoch.\n  We designed and implemented our techniques as part of MixTrain and evaluated\nit on six networks trained on three popular datasets including MNIST, CIFAR,\nand ImageNet-200. Our evaluations show that MixTrain can achieve up to $95.2\\%$\nverified robust accuracy against $L_\\infty$ norm-bounded attackers while taking\n$15$ and $3$ times less training time than state-of-the-art verifiably robust\ntraining and adversarially robust training schemes, respectively. Furthermore,\nMixTrain easily scales to larger networks like the one trained on ImageNet-200,\nsignificantly outperforming the existing verifiably robust training methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 20:47:28 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 23:52:52 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Wang", "Shiqi", ""], ["Chen", "Yizheng", ""], ["Abdou", "Ahmed", ""], ["Jana", "Suman", ""]]}, {"id": "1811.02628", "submitter": "Dong Yul Oh", "authors": "Dong Yul Oh and Il Dong Yun", "title": "Learning Bone Suppression from Dual Energy Chest X-rays using\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Suppressing bones on chest X-rays such as ribs and clavicle is often expected\nto improve pathologies classification. These bones can interfere with a broad\nrange of diagnostic tasks on pulmonary disease except for musculoskeletal\nsystem. Current conventional method for acquisition of bone suppressed X-rays\nis dual energy imaging, which captures two radiographs at a very short interval\nwith different energy levels; however, the patient is exposed to radiation\ntwice and the artifacts arise due to heartbeats between two shots. In this\npaper, we introduce a deep generative model trained to predict bone suppressed\nimages on single energy chest X-rays, analyzing a finite set of previously\nacquired dual energy chest X-rays. Since the relatively small amount of data is\navailable, such approach relies on the methodology maximizing the data\nutilization. Here we integrate the following two approaches. First, we use a\nconditional generative adversarial network that complements the traditional\nregression method minimizing the pairwise image difference. Second, we use Haar\n2D wavelet decomposition to offer a perceptual guideline in frequency details\nto allow the model to converge quickly and efficiently. As a result, we achieve\nstate-of-the-art performance on bone suppression as compared to the existing\napproaches with dual energy chest X-rays.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 01:01:31 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Oh", "Dong Yul", ""], ["Yun", "Il Dong", ""]]}, {"id": "1811.02629", "submitter": "Spyridon Bakas", "authors": "Spyridon Bakas, Mauricio Reyes, Andras Jakab, Stefan Bauer, Markus\n  Rempfler, Alessandro Crimi, Russell Takeshi Shinohara, Christoph Berger, Sung\n  Min Ha, Martin Rozycki, Marcel Prastawa, Esther Alberts, Jana Lipkova, John\n  Freymann, Justin Kirby, Michel Bilello, Hassan Fathallah-Shaykh, Roland\n  Wiest, Jan Kirschke, Benedikt Wiestler, Rivka Colen, Aikaterini Kotrotsou,\n  Pamela Lamontagne, Daniel Marcus, Mikhail Milchenko, Arash Nazeri, Marc-Andre\n  Weber, Abhishek Mahajan, Ujjwal Baid, Elizabeth Gerstner, Dongjin Kwon, Gagan\n  Acharya, Manu Agarwal, Mahbubul Alam, Alberto Albiol, Antonio Albiol,\n  Francisco J. Albiol, Varghese Alex, Nigel Allinson, Pedro H. A. Amorim,\n  Abhijit Amrutkar, Ganesh Anand, Simon Andermatt, Tal Arbel, Pablo Arbelaez,\n  Aaron Avery, Muneeza Azmat, Pranjal B., W Bai, Subhashis Banerjee, Bill\n  Barth, Thomas Batchelder, Kayhan Batmanghelich, Enzo Battistella, Andrew\n  Beers, Mikhail Belyaev, Martin Bendszus, Eze Benson, Jose Bernal, Halandur\n  Nagaraja Bharath, George Biros, Sotirios Bisdas, James Brown, Mariano\n  Cabezas, Shilei Cao, Jorge M. Cardoso, Eric N Carver, Adri\\`a Casamitjana,\n  Laura Silvana Castillo, Marcel Cat\\`a, Philippe Cattin, Albert Cerigues,\n  Vinicius S. Chagas, Siddhartha Chandra, Yi-Ju Chang, Shiyu Chang, Ken Chang,\n  Joseph Chazalon, Shengcong Chen, Wei Chen, Jefferson W Chen, Zhaolin Chen,\n  Kun Cheng, Ahana Roy Choudhury, Roger Chylla, Albert Cl\\'erigues, Steven\n  Colleman, Ramiro German Rodriguez Colmeiro, Marc Combalia, Anthony Costa,\n  Xiaomeng Cui, Zhenzhen Dai, Lutao Dai, Laura Alexandra Daza, Eric Deutsch,\n  Changxing Ding, Chao Dong, Shidu Dong, Wojciech Dudzik, Zach Eaton-Rosen,\n  Gary Egan, Guilherme Escudero, Th\\'eo Estienne, Richard Everson, Jonathan\n  Fabrizio, Yong Fan, Longwei Fang, Xue Feng, Enzo Ferrante, Lucas Fidon,\n  Martin Fischer, Andrew P. French, Naomi Fridman, Huan Fu, David Fuentes,\n  Yaozong Gao, Evan Gates, David Gering, Amir Gholami, Willi Gierke, Ben\n  Glocker, Mingming Gong, Sandra Gonz\\'alez-Vill\\'a, T. Grosges, Yuanfang Guan,\n  Sheng Guo, Sudeep Gupta, Woo-Sup Han, Il Song Han, Konstantin Harmuth,\n  Huiguang He, Aura Hern\\'andez-Sabat\\'e, Evelyn Herrmann, Naveen Himthani,\n  Winston Hsu, Cheyu Hsu, Xiaojun Hu, Xiaobin Hu, Yan Hu, Yifan Hu, Rui Hua,\n  Teng-Yi Huang, Weilin Huang, Sabine Van Huffel, Quan Huo, Vivek HV, Khan M.\n  Iftekharuddin, Fabian Isensee, Mobarakol Islam, Aaron S. Jackson, Sachin R.\n  Jambawalikar, Andrew Jesson, Weijian Jian, Peter Jin, V Jeya Maria Jose,\n  Alain Jungo, B Kainz, Konstantinos Kamnitsas, Po-Yu Kao, Ayush Karnawat,\n  Thomas Kellermeier, Adel Kermi, Kurt Keutzer, Mohamed Tarek Khadir, Mahendra\n  Khened, Philipp Kickingereder, Geena Kim, Nik King, Haley Knapp, Urspeter\n  Knecht, Lisa Kohli, Deren Kong, Xiangmao Kong, Simon Koppers, Avinash Kori,\n  Ganapathy Krishnamurthi, Egor Krivov, Piyush Kumar, Kaisar Kushibar, Dmitrii\n  Lachinov, Tryphon Lambrou, Joon Lee, Chengen Lee, Yuehchou Lee, M Lee,\n  Szidonia Lefkovits, Laszlo Lefkovits, James Levitt, Tengfei Li, Hongwei Li,\n  Wenqi Li, Hongyang Li, Xiaochuan Li, Yuexiang Li, Heng Li, Zhenye Li, Xiaoyu\n  Li, Zeju Li, XiaoGang Li, Wenqi Li, Zheng-Shen Lin, Fengming Lin, Pietro Lio,\n  Chang Liu, Boqiang Liu, Xiang Liu, Mingyuan Liu, Ju Liu, Luyan Liu, Xavier\n  Llado, Marc Moreno Lopez, Pablo Ribalta Lorenzo, Zhentai Lu, Lin Luo, Zhigang\n  Luo, Jun Ma, Kai Ma, Thomas Mackie, Anant Madabushi, Issam Mahmoudi, Klaus H.\n  Maier-Hein, Pradipta Maji, CP Mammen, Andreas Mang, B. S. Manjunath, Michal\n  Marcinkiewicz, S McDonagh, Stephen McKenna, Richard McKinley, Miriam Mehl,\n  Sachin Mehta, Raghav Mehta, Raphael Meier, Christoph Meinel, Dorit Merhof,\n  Craig Meyer, Robert Miller, Sushmita Mitra, Aliasgar Moiyadi, David\n  Molina-Garcia, Miguel A.B. Monteiro, Grzegorz Mrukwa, Andriy Myronenko, Jakub\n  Nalepa, Thuyen Ngo, Dong Nie, Holly Ning, Chen Niu, Nicholas K Nuechterlein,\n  Eric Oermann, Arlindo Oliveira, Diego D. C. Oliveira, Arnau Oliver, Alexander\n  F. I. Osman, Yu-Nian Ou, Sebastien Ourselin, Nikos Paragios, Moo Sung Park,\n  Brad Paschke, J. Gregory Pauloski, Kamlesh Pawar, Nick Pawlowski, Linmin Pei,\n  Suting Peng, Silvio M. Pereira, Julian Perez-Beteta, Victor M. Perez-Garcia,\n  Simon Pezold, Bao Pham, Ashish Phophalia, Gemma Piella, G.N. Pillai, Marie\n  Piraud, Maxim Pisov, Anmol Popli, Michael P. Pound, Reza Pourreza, Prateek\n  Prasanna, Vesna Prkovska, Tony P. Pridmore, Santi Puch, \\'Elodie Puybareau,\n  Buyue Qian, Xu Qiao, Martin Rajchl, Swapnil Rane, Michael Rebsamen, Hongliang\n  Ren, Xuhua Ren, Karthik Revanuru, Mina Rezaei, Oliver Rippel, Luis Carlos\n  Rivera, Charlotte Robert, Bruce Rosen, Daniel Rueckert, Mohammed Safwan,\n  Mostafa Salem, Joaquim Salvi, Irina Sanchez, Irina S\\'anchez, Heitor M.\n  Santos, Emmett Sartor, Dawid Schellingerhout, Klaudius Scheufele, Matthew R.\n  Scott, Artur A. Scussel, Sara Sedlar, Juan Pablo Serrano-Rubio, N. Jon Shah,\n  Nameetha Shah, Mazhar Shaikh, B. Uma Shankar, Zeina Shboul, Haipeng Shen,\n  Dinggang Shen, Linlin Shen, Haocheng Shen, Varun Shenoy, Feng Shi, Hyung Eun\n  Shin, Hai Shu, Diana Sima, M Sinclair, Orjan Smedby, James M. Snyder,\n  Mohammadreza Soltaninejad, Guidong Song, Mehul Soni, Jean Stawiaski, Shashank\n  Subramanian, Li Sun, Roger Sun, Jiawei Sun, Kay Sun, Yu Sun, Guoxia Sun,\n  Shuang Sun, Yannick R Suter, Laszlo Szilagyi, Sanjay Talbar, Dacheng Tao,\n  Dacheng Tao, Zhongzhao Teng, Siddhesh Thakur, Meenakshi H Thakur, Sameer\n  Tharakan, Pallavi Tiwari, Guillaume Tochon, Tuan Tran, Yuhsiang M. Tsai,\n  Kuan-Lun Tseng, Tran Anh Tuan, Vadim Turlapov, Nicholas Tustison, Maria\n  Vakalopoulou, Sergi Valverde, Rami Vanguri, Evgeny Vasiliev, Jonathan\n  Ventura, Luis Vera, Tom Vercauteren, C. A. Verrastro, Lasitha Vidyaratne,\n  Veronica Vilaplana, Ajeet Vivekanandan, Guotai Wang, Qian Wang, Chiatse J.\n  Wang, Weichung Wang, Duo Wang, Ruixuan Wang, Yuanyuan Wang, Chunliang Wang,\n  Guotai Wang, Ning Wen, Xin Wen, Leon Weninger, Wolfgang Wick, Shaocheng Wu,\n  Qiang Wu, Yihong Wu, Yong Xia, Yanwu Xu, Xiaowen Xu, Peiyuan Xu, Tsai-Ling\n  Yang, Xiaoping Yang, Hao-Yu Yang, Junlin Yang, Haojin Yang, Guang Yang,\n  Hongdou Yao, Xujiong Ye, Changchang Yin, Brett Young-Moxon, Jinhua Yu,\n  Xiangyu Yue, Songtao Zhang, Angela Zhang, Kun Zhang, Xuejie Zhang, Lichi\n  Zhang, Xiaoyue Zhang, Yazhuo Zhang, Lei Zhang, Jianguo Zhang, Xiang Zhang,\n  Tianhao Zhang, Sicheng Zhao, Yu Zhao, Xiaomei Zhao, Liang Zhao, Yefeng Zheng,\n  Liming Zhong, Chenhong Zhou, Xiaobing Zhou, Fan Zhou, Hongtu Zhu, Jin Zhu,\n  Ying Zhuge, Weiwei Zong, Jayashree Kalpathy-Cramer, Keyvan Farahani, Christos\n  Davatzikos, Koen van Leemput, Bjoern Menze", "title": "Identifying the Best Machine Learning Algorithms for Brain Tumor\n  Segmentation, Progression Assessment, and Overall Survival Prediction in the\n  BRATS Challenge", "comments": "The International Multimodal Brain Tumor Segmentation (BraTS)\n  Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gliomas are the most common primary brain malignancies, with different\ndegrees of aggressiveness, variable prognosis and various heterogeneous\nhistologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic\ncore, active and non-enhancing core. This intrinsic heterogeneity is also\nportrayed in their radio-phenotype, as their sub-regions are depicted by\nvarying intensity profiles disseminated across multi-parametric magnetic\nresonance imaging (mpMRI) scans, reflecting varying biological properties.\nTheir heterogeneous shape, extent, and location are some of the factors that\nmake these tumors difficult to resect, and in some cases inoperable. The amount\nof resected tumor is a factor also considered in longitudinal scans, when\nevaluating the apparent tumor for potential diagnosis of progression.\nFurthermore, there is mounting evidence that accurate segmentation of the\nvarious tumor sub-regions can offer the basis for quantitative image analysis\ntowards prediction of patient overall survival. This study assesses the\nstate-of-the-art machine learning (ML) methods used for brain tumor image\nanalysis in mpMRI scans, during the last seven instances of the International\nBrain Tumor Segmentation (BraTS) challenge, i.e., 2012-2018. Specifically, we\nfocus on i) evaluating segmentations of the various glioma sub-regions in\npre-operative mpMRI scans, ii) assessing potential tumor progression by virtue\nof longitudinal growth of tumor sub-regions, beyond use of the RECIST/RANO\ncriteria, and iii) predicting the overall survival from pre-operative mpMRI\nscans of patients that underwent gross total resection. Finally, we investigate\nthe challenge of identifying the best ML algorithms for each of these tasks,\nconsidering that apart from being diverse on each instance of the challenge,\nthe multi-institutional mpMRI BraTS dataset has also been a continuously\nevolving/growing dataset.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 05:10:18 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 23:18:19 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2019 13:35:04 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Bakas", "Spyridon", ""], ["Reyes", "Mauricio", ""], ["Jakab", "Andras", ""], ["Bauer", "Stefan", ""], ["Rempfler", "Markus", ""], ["Crimi", "Alessandro", ""], ["Shinohara", "Russell Takeshi", ""], ["Berger", "Christoph", ""], ["Ha", "Sung Min", ""], ["Rozycki", "Martin", ""], ["Prastawa", "Marcel", ""], ["Alberts", "Esther", ""], ["Lipkova", "Jana", ""], ["Freymann", "John", ""], ["Kirby", "Justin", ""], ["Bilello", "Michel", ""], ["Fathallah-Shaykh", "Hassan", ""], ["Wiest", "Roland", ""], ["Kirschke", "Jan", ""], ["Wiestler", "Benedikt", ""], ["Colen", "Rivka", ""], ["Kotrotsou", "Aikaterini", ""], ["Lamontagne", "Pamela", ""], ["Marcus", "Daniel", ""], ["Milchenko", "Mikhail", ""], ["Nazeri", "Arash", ""], ["Weber", "Marc-Andre", ""], ["Mahajan", "Abhishek", ""], ["Baid", "Ujjwal", ""], ["Gerstner", "Elizabeth", ""], ["Kwon", "Dongjin", ""], ["Acharya", "Gagan", ""], ["Agarwal", "Manu", ""], ["Alam", "Mahbubul", ""], ["Albiol", "Alberto", ""], ["Albiol", "Antonio", ""], ["Albiol", "Francisco J.", ""], ["Alex", "Varghese", ""], ["Allinson", "Nigel", ""], ["Amorim", "Pedro H. A.", ""], ["Amrutkar", "Abhijit", ""], ["Anand", "Ganesh", ""], ["Andermatt", "Simon", ""], ["Arbel", "Tal", ""], ["Arbelaez", "Pablo", ""], ["Avery", "Aaron", ""], ["Azmat", "Muneeza", ""], ["B.", "Pranjal", ""], ["Bai", "W", ""], ["Banerjee", "Subhashis", ""], ["Barth", "Bill", ""], ["Batchelder", "Thomas", ""], ["Batmanghelich", "Kayhan", ""], ["Battistella", "Enzo", ""], ["Beers", "Andrew", ""], ["Belyaev", "Mikhail", ""], ["Bendszus", "Martin", ""], ["Benson", "Eze", ""], ["Bernal", "Jose", ""], ["Bharath", "Halandur Nagaraja", ""], ["Biros", "George", ""], ["Bisdas", "Sotirios", ""], ["Brown", "James", ""], ["Cabezas", "Mariano", ""], ["Cao", "Shilei", ""], ["Cardoso", "Jorge M.", ""], ["Carver", "Eric N", ""], ["Casamitjana", "Adri\u00e0", ""], ["Castillo", "Laura Silvana", ""], ["Cat\u00e0", "Marcel", ""], ["Cattin", "Philippe", ""], ["Cerigues", "Albert", ""], ["Chagas", "Vinicius S.", ""], ["Chandra", "Siddhartha", ""], ["Chang", "Yi-Ju", ""], ["Chang", "Shiyu", ""], ["Chang", "Ken", ""], ["Chazalon", "Joseph", ""], ["Chen", "Shengcong", ""], ["Chen", "Wei", ""], ["Chen", "Jefferson W", ""], ["Chen", "Zhaolin", ""], ["Cheng", "Kun", ""], ["Choudhury", "Ahana Roy", ""], ["Chylla", "Roger", ""], ["Cl\u00e9rigues", "Albert", ""], ["Colleman", "Steven", ""], ["Colmeiro", "Ramiro German Rodriguez", ""], ["Combalia", "Marc", ""], ["Costa", "Anthony", ""], ["Cui", "Xiaomeng", ""], ["Dai", "Zhenzhen", ""], ["Dai", "Lutao", ""], ["Daza", "Laura Alexandra", ""], ["Deutsch", "Eric", ""], ["Ding", "Changxing", ""], ["Dong", "Chao", ""], ["Dong", "Shidu", ""], ["Dudzik", "Wojciech", ""], ["Eaton-Rosen", "Zach", ""], ["Egan", "Gary", ""], ["Escudero", "Guilherme", ""], ["Estienne", "Th\u00e9o", ""], ["Everson", "Richard", ""], ["Fabrizio", "Jonathan", ""], ["Fan", "Yong", ""], ["Fang", "Longwei", ""], ["Feng", "Xue", ""], ["Ferrante", "Enzo", ""], ["Fidon", "Lucas", ""], ["Fischer", "Martin", ""], ["French", "Andrew P.", ""], ["Fridman", "Naomi", ""], ["Fu", "Huan", ""], ["Fuentes", "David", ""], ["Gao", "Yaozong", ""], ["Gates", "Evan", ""], ["Gering", "David", ""], ["Gholami", "Amir", ""], ["Gierke", "Willi", ""], ["Glocker", "Ben", ""], ["Gong", "Mingming", ""], ["Gonz\u00e1lez-Vill\u00e1", "Sandra", ""], ["Grosges", "T.", ""], ["Guan", "Yuanfang", ""], ["Guo", "Sheng", ""], ["Gupta", "Sudeep", ""], ["Han", "Woo-Sup", ""], ["Han", "Il Song", ""], ["Harmuth", "Konstantin", ""], ["He", "Huiguang", ""], ["Hern\u00e1ndez-Sabat\u00e9", "Aura", ""], ["Herrmann", "Evelyn", ""], ["Himthani", "Naveen", ""], ["Hsu", "Winston", ""], ["Hsu", "Cheyu", ""], ["Hu", "Xiaojun", ""], ["Hu", "Xiaobin", ""], ["Hu", "Yan", ""], ["Hu", "Yifan", ""], ["Hua", "Rui", ""], ["Huang", "Teng-Yi", ""], ["Huang", "Weilin", ""], ["Van Huffel", "Sabine", ""], ["Huo", "Quan", ""], ["HV", "Vivek", ""], ["Iftekharuddin", "Khan M.", ""], ["Isensee", "Fabian", ""], ["Islam", "Mobarakol", ""], ["Jackson", "Aaron S.", ""], ["Jambawalikar", "Sachin R.", ""], ["Jesson", "Andrew", ""], ["Jian", "Weijian", ""], ["Jin", "Peter", ""], ["Jose", "V Jeya Maria", ""], ["Jungo", "Alain", ""], ["Kainz", "B", ""], ["Kamnitsas", "Konstantinos", ""], ["Kao", "Po-Yu", ""], ["Karnawat", "Ayush", ""], ["Kellermeier", "Thomas", ""], ["Kermi", "Adel", ""], ["Keutzer", "Kurt", ""], ["Khadir", "Mohamed Tarek", ""], ["Khened", "Mahendra", ""], ["Kickingereder", "Philipp", ""], ["Kim", "Geena", ""], ["King", "Nik", ""], ["Knapp", "Haley", ""], ["Knecht", "Urspeter", ""], ["Kohli", "Lisa", ""], ["Kong", "Deren", ""], ["Kong", "Xiangmao", ""], ["Koppers", "Simon", ""], ["Kori", "Avinash", ""], ["Krishnamurthi", "Ganapathy", ""], ["Krivov", "Egor", ""], ["Kumar", "Piyush", ""], ["Kushibar", "Kaisar", ""], ["Lachinov", "Dmitrii", ""], ["Lambrou", "Tryphon", ""], ["Lee", "Joon", ""], ["Lee", "Chengen", ""], ["Lee", "Yuehchou", ""], ["Lee", "M", ""], ["Lefkovits", "Szidonia", ""], ["Lefkovits", "Laszlo", ""], ["Levitt", "James", ""], ["Li", "Tengfei", ""], ["Li", "Hongwei", ""], ["Li", "Wenqi", ""], ["Li", "Hongyang", ""], ["Li", "Xiaochuan", ""], ["Li", "Yuexiang", ""], ["Li", "Heng", ""], ["Li", "Zhenye", ""], ["Li", "Xiaoyu", ""], ["Li", "Zeju", ""], ["Li", "XiaoGang", ""], ["Li", "Wenqi", ""], ["Lin", "Zheng-Shen", ""], ["Lin", "Fengming", ""], ["Lio", "Pietro", ""], ["Liu", "Chang", ""], ["Liu", "Boqiang", ""], ["Liu", "Xiang", ""], ["Liu", "Mingyuan", ""], ["Liu", "Ju", ""], ["Liu", "Luyan", ""], ["Llado", "Xavier", ""], ["Lopez", "Marc Moreno", ""], ["Lorenzo", "Pablo Ribalta", ""], ["Lu", "Zhentai", ""], ["Luo", "Lin", ""], ["Luo", "Zhigang", ""], ["Ma", "Jun", ""], ["Ma", "Kai", ""], ["Mackie", "Thomas", ""], ["Madabushi", "Anant", ""], ["Mahmoudi", "Issam", ""], ["Maier-Hein", "Klaus H.", ""], ["Maji", "Pradipta", ""], ["Mammen", "CP", ""], ["Mang", "Andreas", ""], ["Manjunath", "B. S.", ""], ["Marcinkiewicz", "Michal", ""], ["McDonagh", "S", ""], ["McKenna", "Stephen", ""], ["McKinley", "Richard", ""], ["Mehl", "Miriam", ""], ["Mehta", "Sachin", ""], ["Mehta", "Raghav", ""], ["Meier", "Raphael", ""], ["Meinel", "Christoph", ""], ["Merhof", "Dorit", ""], ["Meyer", "Craig", ""], ["Miller", "Robert", ""], ["Mitra", "Sushmita", ""], ["Moiyadi", "Aliasgar", ""], ["Molina-Garcia", "David", ""], ["Monteiro", "Miguel A. B.", ""], ["Mrukwa", "Grzegorz", ""], ["Myronenko", "Andriy", ""], ["Nalepa", "Jakub", ""], ["Ngo", "Thuyen", ""], ["Nie", "Dong", ""], ["Ning", "Holly", ""], ["Niu", "Chen", ""], ["Nuechterlein", "Nicholas K", ""], ["Oermann", "Eric", ""], ["Oliveira", "Arlindo", ""], ["Oliveira", "Diego D. C.", ""], ["Oliver", "Arnau", ""], ["Osman", "Alexander F. I.", ""], ["Ou", "Yu-Nian", ""], ["Ourselin", "Sebastien", ""], ["Paragios", "Nikos", ""], ["Park", "Moo Sung", ""], ["Paschke", "Brad", ""], ["Pauloski", "J. Gregory", ""], ["Pawar", "Kamlesh", ""], ["Pawlowski", "Nick", ""], ["Pei", "Linmin", ""], ["Peng", "Suting", ""], ["Pereira", "Silvio M.", ""], ["Perez-Beteta", "Julian", ""], ["Perez-Garcia", "Victor M.", ""], ["Pezold", "Simon", ""], ["Pham", "Bao", ""], ["Phophalia", "Ashish", ""], ["Piella", "Gemma", ""], ["Pillai", "G. N.", ""], ["Piraud", "Marie", ""], ["Pisov", "Maxim", ""], ["Popli", "Anmol", ""], ["Pound", "Michael P.", ""], ["Pourreza", "Reza", ""], ["Prasanna", "Prateek", ""], ["Prkovska", "Vesna", ""], ["Pridmore", "Tony P.", ""], ["Puch", "Santi", ""], ["Puybareau", "\u00c9lodie", ""], ["Qian", "Buyue", ""], ["Qiao", "Xu", ""], ["Rajchl", "Martin", ""], ["Rane", "Swapnil", ""], ["Rebsamen", "Michael", ""], ["Ren", "Hongliang", ""], ["Ren", "Xuhua", ""], ["Revanuru", "Karthik", ""], ["Rezaei", "Mina", ""], ["Rippel", "Oliver", ""], ["Rivera", "Luis Carlos", ""], ["Robert", "Charlotte", ""], ["Rosen", "Bruce", ""], ["Rueckert", "Daniel", ""], ["Safwan", "Mohammed", ""], ["Salem", "Mostafa", ""], ["Salvi", "Joaquim", ""], ["Sanchez", "Irina", ""], ["S\u00e1nchez", "Irina", ""], ["Santos", "Heitor M.", ""], ["Sartor", "Emmett", ""], ["Schellingerhout", "Dawid", ""], ["Scheufele", "Klaudius", ""], ["Scott", "Matthew R.", ""], ["Scussel", "Artur A.", ""], ["Sedlar", "Sara", ""], ["Serrano-Rubio", "Juan Pablo", ""], ["Shah", "N. Jon", ""], ["Shah", "Nameetha", ""], ["Shaikh", "Mazhar", ""], ["Shankar", "B. Uma", ""], ["Shboul", "Zeina", ""], ["Shen", "Haipeng", ""], ["Shen", "Dinggang", ""], ["Shen", "Linlin", ""], ["Shen", "Haocheng", ""], ["Shenoy", "Varun", ""], ["Shi", "Feng", ""], ["Shin", "Hyung Eun", ""], ["Shu", "Hai", ""], ["Sima", "Diana", ""], ["Sinclair", "M", ""], ["Smedby", "Orjan", ""], ["Snyder", "James M.", ""], ["Soltaninejad", "Mohammadreza", ""], ["Song", "Guidong", ""], ["Soni", "Mehul", ""], ["Stawiaski", "Jean", ""], ["Subramanian", "Shashank", ""], ["Sun", "Li", ""], ["Sun", "Roger", ""], ["Sun", "Jiawei", ""], ["Sun", "Kay", ""], ["Sun", "Yu", ""], ["Sun", "Guoxia", ""], ["Sun", "Shuang", ""], ["Suter", "Yannick R", ""], ["Szilagyi", "Laszlo", ""], ["Talbar", "Sanjay", ""], ["Tao", "Dacheng", ""], ["Tao", "Dacheng", ""], ["Teng", "Zhongzhao", ""], ["Thakur", "Siddhesh", ""], ["Thakur", "Meenakshi H", ""], ["Tharakan", "Sameer", ""], ["Tiwari", "Pallavi", ""], ["Tochon", "Guillaume", ""], ["Tran", "Tuan", ""], ["Tsai", "Yuhsiang M.", ""], ["Tseng", "Kuan-Lun", ""], ["Tuan", "Tran Anh", ""], ["Turlapov", "Vadim", ""], ["Tustison", "Nicholas", ""], ["Vakalopoulou", "Maria", ""], ["Valverde", "Sergi", ""], ["Vanguri", "Rami", ""], ["Vasiliev", "Evgeny", ""], ["Ventura", "Jonathan", ""], ["Vera", "Luis", ""], ["Vercauteren", "Tom", ""], ["Verrastro", "C. A.", ""], ["Vidyaratne", "Lasitha", ""], ["Vilaplana", "Veronica", ""], ["Vivekanandan", "Ajeet", ""], ["Wang", "Guotai", ""], ["Wang", "Qian", ""], ["Wang", "Chiatse J.", ""], ["Wang", "Weichung", ""], ["Wang", "Duo", ""], ["Wang", "Ruixuan", ""], ["Wang", "Yuanyuan", ""], ["Wang", "Chunliang", ""], ["Wang", "Guotai", ""], ["Wen", "Ning", ""], ["Wen", "Xin", ""], ["Weninger", "Leon", ""], ["Wick", "Wolfgang", ""], ["Wu", "Shaocheng", ""], ["Wu", "Qiang", ""], ["Wu", "Yihong", ""], ["Xia", "Yong", ""], ["Xu", "Yanwu", ""], ["Xu", "Xiaowen", ""], ["Xu", "Peiyuan", ""], ["Yang", "Tsai-Ling", ""], ["Yang", "Xiaoping", ""], ["Yang", "Hao-Yu", ""], ["Yang", "Junlin", ""], ["Yang", "Haojin", ""], ["Yang", "Guang", ""], ["Yao", "Hongdou", ""], ["Ye", "Xujiong", ""], ["Yin", "Changchang", ""], ["Young-Moxon", "Brett", ""], ["Yu", "Jinhua", ""], ["Yue", "Xiangyu", ""], ["Zhang", "Songtao", ""], ["Zhang", "Angela", ""], ["Zhang", "Kun", ""], ["Zhang", "Xuejie", ""], ["Zhang", "Lichi", ""], ["Zhang", "Xiaoyue", ""], ["Zhang", "Yazhuo", ""], ["Zhang", "Lei", ""], ["Zhang", "Jianguo", ""], ["Zhang", "Xiang", ""], ["Zhang", "Tianhao", ""], ["Zhao", "Sicheng", ""], ["Zhao", "Yu", ""], ["Zhao", "Xiaomei", ""], ["Zhao", "Liang", ""], ["Zheng", "Yefeng", ""], ["Zhong", "Liming", ""], ["Zhou", "Chenhong", ""], ["Zhou", "Xiaobing", ""], ["Zhou", "Fan", ""], ["Zhu", "Hongtu", ""], ["Zhu", "Jin", ""], ["Zhuge", "Ying", ""], ["Zong", "Weiwei", ""], ["Kalpathy-Cramer", "Jayashree", ""], ["Farahani", "Keyvan", ""], ["Davatzikos", "Christos", ""], ["van Leemput", "Koen", ""], ["Menze", "Bjoern", ""]]}, {"id": "1811.02633", "submitter": "Philippe Schwaller", "authors": "Philippe Schwaller, Teodoro Laino, Th\\'eophile Gaudin, Peter Bolgar,\n  Costas Bekas, Alpha A Lee", "title": "Molecular Transformer - A Model for Uncertainty-Calibrated Chemical\n  Reaction Prediction", "comments": "Machine Learning for Molecules and Materials workshop, NeurIPS 2018 /\n  Platform: https://rxn.res.ibm.com", "journal-ref": "ACS Central Science, 2019", "doi": "10.1021/acscentsci.9b00576", "report-no": null, "categories": "physics.chem-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organic synthesis is one of the key stumbling blocks in medicinal chemistry.\nA necessary yet unsolved step in planning synthesis is solving the forward\nproblem: given reactants and reagents, predict the products. Similar to other\nwork, we treat reaction prediction as a machine translation problem between\nSMILES strings of reactants-reagents and the products. We show that a\nmulti-head attention Molecular Transformer model outperforms all algorithms in\nthe literature, achieving a top-1 accuracy above 90% on a common benchmark\ndataset. Our algorithm requires no handcrafted rules, and accurately predicts\nsubtle chemical transformations. Crucially, our model can accurately estimate\nits own uncertainty, with an uncertainty score that is 89% accurate in terms of\nclassifying whether a prediction is correct. Furthermore, we show that the\nmodel is able to handle inputs without reactant-reagent split and including\nstereochemistry, which makes our method universally applicable.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 20:53:09 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 07:50:42 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Schwaller", "Philippe", ""], ["Laino", "Teodoro", ""], ["Gaudin", "Th\u00e9ophile", ""], ["Bolgar", "Peter", ""], ["Bekas", "Costas", ""], ["Lee", "Alpha A", ""]]}, {"id": "1811.02640", "submitter": "Kashyap Chitta", "authors": "Kashyap Chitta, Jose M. Alvarez, Adam Lesnikowski", "title": "Deep Probabilistic Ensembles: Approximate Variational Inference through\n  KL Regularization", "comments": "Workshop on Bayesian Deep Learning (NeurIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Deep Probabilistic Ensembles (DPEs), a scalable\ntechnique that uses a regularized ensemble to approximate a deep Bayesian\nNeural Network (BNN). We do so by incorporating a KL divergence penalty term\ninto the training objective of an ensemble, derived from the evidence lower\nbound used in variational inference. We evaluate the uncertainty estimates\nobtained from our models for active learning on visual classification. Our\napproach steadily improves upon active learning baselines as the annotation\nbudget is increased.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 20:59:51 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 21:12:55 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Chitta", "Kashyap", ""], ["Alvarez", "Jose M.", ""], ["Lesnikowski", "Adam", ""]]}, {"id": "1811.02642", "submitter": "Aman Rana", "authors": "Aman Rana, Gregory Yauney, Alarice Lowe, Pratik Shah", "title": "Computational Histological Staining and Destaining of Prostate Core\n  Biopsy RGB Images with Generative Adversarial Neural Networks", "comments": "Accepted for publication at 2018 IEEE International Conference on\n  Machine Learning and Applications (ICMLA)", "journal-ref": null, "doi": "10.1109/ICMLA.2018.00133", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Histopathology tissue samples are widely available in two states:\nparaffin-embedded unstained and non-paraffin-embedded stained whole slide RGB\nimages (WSRI). Hematoxylin and eosin stain (H&E) is one of the principal stains\nin histology but suffers from several shortcomings related to tissue\npreparation, staining protocols, slowness and human error. We report two novel\napproaches for training machine learning models for the computational H&E\nstaining and destaining of prostate core biopsy RGB images. The staining model\nuses a conditional generative adversarial network that learns hierarchical\nnon-linear mappings between whole slide RGB image (WSRI) pairs of prostate core\nbiopsy before and after H&E staining. The trained staining model can then\ngenerate computationally H&E-stained prostate core WSRIs using previously\nunseen non-stained biopsy images as input. The destaining model, by learning\nmappings between an H&E stained WSRI and a non-stained WSRI of the same biopsy,\ncan computationally destain previously unseen H&E-stained images. Structural\nand anatomical details of prostate tissue and colors, shapes, geometries,\nlocations of nuclei, stroma, vessels, glands and other cellular components were\ngenerated by both models with structural similarity indices of 0.68 (staining)\nand 0.84 (destaining). The proposed staining and destaining models can engender\ncomputational H&E staining and destaining of WSRI biopsies without additional\nequipment and devices.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 17:36:00 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 16:37:49 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Rana", "Aman", ""], ["Yauney", "Gregory", ""], ["Lowe", "Alarice", ""], ["Shah", "Pratik", ""]]}, {"id": "1811.02651", "submitter": "Emre E\\u{g}riboz", "authors": "Emre E\\u{g}riboz, Furkan Kaynar, Song\\\"ul Varl{\\i} Albayrak, Benan\n  M\\\"usellim, Tuba Sel\\c{c}uk", "title": "Finding and Following of Honeycombing Regions in Computed Tomography\n  Lung Images by Deep Learning", "comments": "4 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, besides the medical treatment methods in medical field,\nComputer Aided Diagnosis (CAD) systems which can facilitate the decision making\nphase of the physician and can detect the disease at an early stage have\nstarted to be used frequently. The diagnosis of Idiopathic Pulmonary Fibrosis\n(IPF) disease by using CAD systems is very important in that it can be followed\nby doctors and radiologists. It has become possible to diagnose and follow up\nthe disease with the help of CAD systems by the development of high resolution\ncomputed imaging scanners and increasing size of computation power. The purpose\nof this project is to design a tool that will help specialists diagnose and\nfollow up the IPF disease by identifying areas of honeycombing and ground glass\npatterns in High Resolution Computed Tomography (HRCT) lung images. Creating a\nprogram module that segments the lung pair and creating a self-learner deep\nlearning model from given Computed Tomography (CT) images for the specific\ndiseased regions thanks to doctors are the main purposes of this work. Through\nthe created model, program module will be able to find special regions in given\nnew CT images. In this study, the performance of lung segmentation was tested\nby the S{\\o}rensen-Dice coefficient method and the mean performance was\nmeasured as 90.7%, testing of the created model was performed with data not\nused in the training stage of the CNN network, and the average performance was\nmeasured as 87.8% for healthy regions, 73.3% for ground-glass areas and 69.1%\nfor honeycombing zones.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 18:29:45 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 21:25:09 GMT"}, {"version": "v3", "created": "Mon, 11 Feb 2019 10:24:10 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["E\u011friboz", "Emre", ""], ["Kaynar", "Furkan", ""], ["Albayrak", "Song\u00fcl Varl\u0131", ""], ["M\u00fcsellim", "Benan", ""], ["Sel\u00e7uk", "Tuba", ""]]}, {"id": "1811.02654", "submitter": "Ryan Sherman", "authors": "Ryan Sherman", "title": "A Volumetric Convolutional Neural Network for Brain Tumor Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain cancer can be very fatal, but chances of survival increase through\nearly detection and treatment. Doctors use Magnetic Resonance Imaging (MRI) to\ndetect and locate tumors in the brain, and very carefully analyze scans to\nsegment brain tumors. Manual segmentation is time consuming and tiring for\ndoctors, and it can be difficult for them to notice extremely small\nabnormalities. Automated segmentations performed by computers offer quicker\ndiagnoses, the ability to notice small details, and more accurate\nsegmentations. Advances in deep learning and computer hardware have allowed for\nhigh-performing automated segmentation approaches. However, several problems\npersist in practice: increased training time, class imbalance, and low\nperformance. In this paper, I propose applying V-Net, a volumetric, fully\nconvolutional neural network, to segment brain tumors in MRI scans from the\nBraTS Challenges. With this approach, I achieve a whole tumor dice score of\n0.89 and train the network in a short time while addressing class imbalance\nwith the use of a dice loss layer. Then, I propose applying an existing\ntechnique to improve automated segmentation performance in practice.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 02:28:54 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Sherman", "Ryan", ""]]}, {"id": "1811.02655", "submitter": "Alper Atamturk", "authors": "Alper Atamturk, Andres Gomez, Shaoning Han", "title": "Sparse and Smooth Signal Estimation: Convexification of L0 Formulations", "comments": "BCOL Research Report 18.05, IEOR, UC Berkeley", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signal estimation problems with smoothness and sparsity priors can be\nnaturally modeled as quadratic optimization with $\\ell_0$-\"norm\" constraints.\nSince such problems are non-convex and hard-to-solve, the standard approach is,\ninstead, to tackle their convex surrogates based on $\\ell_1$-norm relaxations.\nIn this paper, we propose a new iterative (convex) conic quadratic relaxations\nthat exploit not only the $\\ell_0$-\"norm\" terms, but also the fitness and\nsmoothness functions. The iterative convexification approach substantially\ncloses the gap between the $\\ell_0$-\"norm\" and its $\\ell_1$ surrogate. These\nstronger relaxations lead to significantly better estimators than $\\ell_1$-norm\napproaches and also allow one to utilize affine sparsity priors. In addition,\nthe parameters of the model and the resulting estimators are easily\ninterpretable. Experiments with a tailored Lagrangian decomposition method\nindicate that the proposed iterative convex relaxations \\rev{yield solutions\nwithin 1\\% of the exact $\\ell_0$ approach, and can tackle instances with up to\n100,000 variables under one minute.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 21:10:20 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 22:39:48 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 19:03:59 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Atamturk", "Alper", ""], ["Gomez", "Andres", ""], ["Han", "Shaoning", ""]]}, {"id": "1811.02656", "submitter": "Titouan Parcollet", "authors": "Titouan Parcollet, Mohamed Morchid, Georges Linar\\`es", "title": "Quaternion Convolutional Neural Networks for Heterogeneous Image\n  Processing", "comments": "Submitted at ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have recently achieved state-of-the-art\nresults in various applications. In the case of image recognition, an ideal\nmodel has to learn independently of the training data, both local dependencies\nbetween the three components (R,G,B) of a pixel, and the global relations\ndescribing edges or shapes, making it efficient with small or heterogeneous\ndatasets. Quaternion-valued convolutional neural networks (QCNN) solved this\nproblematic by introducing multidimensional algebra to CNN. This paper proposes\nto explore the fundamental reason of the success of QCNN over CNN, by\ninvestigating the impact of the Hamilton product on a color image\nreconstruction task performed from a gray-scale only training. By learning\nindependently both internal and external relations and with less parameters\nthan real valued convolutional encoder-decoder (CAE), quaternion convolutional\nencoder-decoders (QCAE) perfectly reconstructed unseen color images while CAE\nproduced worst and gray-scale versions.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 11:22:54 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Parcollet", "Titouan", ""], ["Morchid", "Mohamed", ""], ["Linar\u00e8s", "Georges", ""]]}, {"id": "1811.02657", "submitter": "Tan Nguyen", "authors": "Tan Nguyen, Nhat Ho, Ankit Patel, Anima Anandkumar, Michael I. Jordan,\n  Richard G. Baraniuk", "title": "A Bayesian Perspective of Convolutional Neural Networks through a\n  Deconvolutional Generative Model", "comments": "Keywords: neural nets, generative models, semi-supervised learning,\n  cross-entropy, statistical guarantees 80 pages, 7 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the success of Convolutional Neural Networks (CNNs) for\nsupervised prediction in images, we design the Deconvolutional Generative Model\n(DGM), a new probabilistic generative model whose inference calculations\ncorrespond to those in a given CNN architecture. The DGM uses a CNN to design\nthe prior distribution in the probabilistic model. Furthermore, the DGM\ngenerates images from coarse to finer scales. It introduces a small set of\nlatent variables at each scale, and enforces dependencies among all the latent\nvariables via a conjugate prior distribution. This conjugate prior yields a new\nregularizer based on paths rendered in the generative model for training\nCNNs-the Rendering Path Normalization (RPN). We demonstrate that this\nregularizer improves generalization, both in theory and in practice. In\naddition, likelihood estimation in the DGM yields training losses for CNNs, and\ninspired by this, we design a new loss termed as the Max-Min cross entropy\nwhich outperforms the traditional cross-entropy loss for object classification.\nThe Max-Min cross entropy suggests a new deep network architecture, namely the\nMax-Min network, which can learn from less labeled data while maintaining good\nprediction performance. Our experiments demonstrate that the DGM with the RPN\nand the Max-Min architecture exceeds or matches the-state-of-art on benchmarks\nincluding SVHN, CIFAR10, and CIFAR100 for semi-supervised and supervised\nlearning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 01:27:37 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 10:21:21 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Nguyen", "Tan", ""], ["Ho", "Nhat", ""], ["Patel", "Ankit", ""], ["Anandkumar", "Anima", ""], ["Jordan", "Michael I.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1811.02658", "submitter": "George Kesidis", "authors": "Yujia Wang, David J. Miller, George Kesidis", "title": "When Not to Classify: Detection of Reverse Engineering Attacks on DNN\n  Image Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses detection of a reverse engineering (RE) attack targeting\na deep neural network (DNN) image classifier; by querying, RE's aim is to\ndiscover the classifier's decision rule. RE can enable test-time evasion\nattacks, which require knowledge of the classifier. Recently, we proposed a\nquite effective approach (ADA) to detect test-time evasion attacks. In this\npaper, we extend ADA to detect RE attacks (ADA-RE). We demonstrate our method\nis successful in detecting \"stealthy\" RE attacks before they learn enough to\nlaunch effective test-time evasion attacks.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 20:59:49 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Wang", "Yujia", ""], ["Miller", "David J.", ""], ["Kesidis", "George", ""]]}, {"id": "1811.02659", "submitter": "Aman Rana", "authors": "Perikumar Javia, Aman Rana, Nathan Shapiro, Pratik Shah", "title": "Machine Learning Algorithms for Classification of Microcirculation\n  Images from Septic and Non-Septic Patients", "comments": "Accepted for publication at 2018 IEEE International Conference on\n  Machine Learning and Applications (IEEE ICMLA)", "journal-ref": null, "doi": "10.1109/ICMLA.2018.00097", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Sepsis is a life-threatening disease and one of the major causes of death in\nhospitals. Imaging of microcirculatory dysfunction is a promising approach for\nautomated diagnosis of sepsis. We report a machine learning classifier capable\nof distinguishing non-septic and septic images from dark field microcirculation\nvideos of patients. The classifier achieves an accuracy of 89.45%. The area\nunder the receiver operating characteristics of the classifier was 0.92, the\nprecision was 0.92 and the recall was 0.84. Codes representing the learned\nfeature space of trained classifier were visualized using t-SNE embedding and\nwere separable and distinguished between images from critically ill and\nnon-septic patients. Using an unsupervised convolutional autoencoder,\nindependent of the clinical diagnosis, we also report clustering of learned\nfeatures from a compressed representation associated with healthy images and\nthose with microcirculatory dysfunction. The feature space used by our trained\nclassifier to distinguish between images from septic and non-septic patients\nhas potential diagnostic application.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 15:34:18 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 16:50:47 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Javia", "Perikumar", ""], ["Rana", "Aman", ""], ["Shapiro", "Nathan", ""], ["Shah", "Pratik", ""]]}, {"id": "1811.02661", "submitter": "Trent Kyono", "authors": "Trent Kyono, Fiona J. Gilbert, Mihaela van der Schaar", "title": "MAMMO: A Deep Learning Solution for Facilitating Radiologist-Machine\n  Collaboration in Breast Cancer Diagnosis", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an aging and growing population, the number of women requiring either\nscreening or symptomatic mammograms is increasing. To reduce the number of\nmammograms that need to be read by a radiologist while keeping the diagnostic\naccuracy the same or better than current clinical practice, we develop Man and\nMachine Mammography Oracle (MAMMO) - a clinical decision support system capable\nof triaging mammograms into those that can be confidently classified by a\nmachine and those that cannot be, thus requiring the reading of a radiologist.\nThe first component of MAMMO is a novel multi-view convolutional neural network\n(CNN) with multi-task learning (MTL). MTL enables the CNN to learn the\nradiological assessments known to be associated with cancer, such as breast\ndensity, conspicuity, suspicion, etc., in addition to learning the primary task\nof cancer diagnosis. We show that MTL has two advantages: 1) learning refined\nfeature representations associated with cancer improves the classification\nperformance of the diagnosis task and 2) issuing radiological assessments\nprovides an additional layer of model interpretability that a radiologist can\nuse to debug and scrutinize the diagnoses provided by the CNN. The second\ncomponent of MAMMO is a triage network, which takes as input the radiological\nassessment and diagnostic predictions of the first network's MTL outputs and\ndetermines which mammograms can be correctly and confidently diagnosed by the\nCNN and which mammograms cannot, thus needing to be read by a radiologist.\nResults obtained on a private dataset of 8,162 patients show that MAMMO reduced\nthe number of radiologist readings by 42.8% while improving the overall\ndiagnostic accuracy in comparison to readings done by radiologists alone. We\nanalyze the triage of patients decided by MAMMO to gain a better understanding\nof what unique mammogram characteristics require radiologists' expertise.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 10:45:53 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Kyono", "Trent", ""], ["Gilbert", "Fiona J.", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1811.02662", "submitter": "Guixiang Ma", "authors": "Guixiang Ma, Nesreen K. Ahmed, Ted Willke, Dipanjan Sengupta, Michael\n  W. Cole, Nicholas B. Turk-Browne, Philip S. Yu", "title": "Similarity Learning with Higher-Order Graph Convolutions for Brain\n  Network Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a similarity metric has gained much attention recently, where the\ngoal is to learn a function that maps input patterns to a target space while\npreserving the semantic distance in the input space. While most related work\nfocused on images, we focus instead on learning a similarity metric for\nneuroimages, such as fMRI and DTI images. We propose an end-to-end similarity\nlearning framework called Higher-order Siamese GCN for multi-subject fMRI data\nanalysis. The proposed framework learns the brain network representations via a\nsupervised metric-based approach with siamese neural networks using two graph\nconvolutional networks as the twin networks. Our proposed framework performs\nhigher-order convolutions by incorporating higher-order proximity in graph\nconvolutional networks to characterize and learn the community structure in\nbrain connectivity networks. To the best of our knowledge, this is the first\ncommunity-preserving similarity learning framework for multi-subject brain\nnetwork analysis. Experimental results on four real fMRI datasets demonstrate\nthe potential use cases of the proposed framework for multi-subject brain\nanalysis in health and neuropsychiatric disorders. Our proposed approach\nachieves an average AUC gain of 75% compared to PCA, an average AUC gain of\n65.5% compared to Spectral Embedding, and an average AUC gain of 24.3% compared\nto S-GCN across the four datasets, indicating promising application in clinical\ninvestigation and brain disease diagnosis.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 03:51:45 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 03:49:36 GMT"}, {"version": "v3", "created": "Mon, 4 Mar 2019 16:01:11 GMT"}, {"version": "v4", "created": "Tue, 16 Apr 2019 06:16:46 GMT"}, {"version": "v5", "created": "Wed, 1 May 2019 21:10:27 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Ma", "Guixiang", ""], ["Ahmed", "Nesreen K.", ""], ["Willke", "Ted", ""], ["Sengupta", "Dipanjan", ""], ["Cole", "Michael W.", ""], ["Turk-Browne", "Nicholas B.", ""], ["Yu", "Philip S.", ""]]}, {"id": "1811.02667", "submitter": "Jakub Nalepa", "authors": "Pablo Ribalta Lorenzo, Lukasz Tulczyjew, Michal Marcinkiewicz, Jakub\n  Nalepa", "title": "Band Selection from Hyperspectral Images Using Attention-based\n  Convolutional Neural Networks", "comments": "This is an initial draft of the paper submitted to IEEE ACCESS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces new attention-based convolutional neural networks for\nselecting bands from hyperspectral images. The proposed approach re-uses\nconvolutional activations at different depths, identifying the most informative\nregions of the spectrum with the help of gating mechanisms. Our attention\ntechniques are modular and easy to implement, and they can be seamlessly\ntrained end-to-end using gradient descent. Our rigorous experiments showed that\ndeep models equipped with the attention mechanism deliver high-quality\nclassification, and repeatedly identify significant bands in the training data,\npermitting the creation of refined and extremely compact sets that retain the\nmost meaningful features.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 19:32:48 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 18:11:11 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 08:50:48 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Lorenzo", "Pablo Ribalta", ""], ["Tulczyjew", "Lukasz", ""], ["Marcinkiewicz", "Michal", ""], ["Nalepa", "Jakub", ""]]}, {"id": "1811.02668", "submitter": "Nghia (Andy) Nguyen", "authors": "Hanadi El Achi, Tatiana Belousova, Lei Chen, Amer Wahed, Iris Wang,\n  Zhihong Hu, Zeyad Kanaan, Adan Rios, Andy N.D. Nguyen", "title": "Automated Diagnosis of Lymphoma with Digital Pathology Images Using Deep\n  Learning", "comments": "13 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown promising results in using Deep Learning to detect\nmalignancy in whole slide imaging. However, they were limited to just\npredicting positive or negative finding for a specific neoplasm. We attempted\nto use Deep Learning with a convolutional neural network algorithm to build a\nlymphoma diagnostic model for four diagnostic categories: benign lymph node,\ndiffuse large B cell lymphoma, Burkitt lymphoma, and small lymphocytic\nlymphoma. Our software was written in Python language. We obtained digital\nwhole slide images of Hematoxylin and Eosin stained slides of 128 cases\nincluding 32 cases for each diagnostic category. Four sets of 5 representative\nimages, 40x40 pixels in dimension, were taken for each case. A total of 2,560\nimages were obtained from which 1,856 were used for training, 464 for\nvalidation and 240 for testing. For each test set of 5 images, the predicted\ndiagnosis was combined from prediction of 5 images. The test results showed\nexcellent diagnostic accuracy at 95% for image-by-image prediction and at 10%\nfor set-by-set prediction. This preliminary study provided a proof of concept\nfor incorporating automated lymphoma diagnostic screen into future pathology\nworkflow to augment the pathologists' productivity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 19:40:50 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Achi", "Hanadi El", ""], ["Belousova", "Tatiana", ""], ["Chen", "Lei", ""], ["Wahed", "Amer", ""], ["Wang", "Iris", ""], ["Hu", "Zhihong", ""], ["Kanaan", "Zeyad", ""], ["Rios", "Adan", ""], ["Nguyen", "Andy N. D.", ""]]}, {"id": "1811.02672", "submitter": "Michele Santacatterina", "authors": "Yi Su and Lequn Wang and Michele Santacatterina and Thorsten Joachims", "title": "CAB: Continuous Adaptive Blending Estimator for Policy Evaluation and\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to perform offline A/B-testing and off-policy learning using\nlogged contextual bandit feedback is highly desirable in a broad range of\napplications, including recommender systems, search engines, ad placement, and\npersonalized health care. Both offline A/B-testing and off-policy learning\nrequire a counterfactual estimator that evaluates how some new policy would\nhave performed, if it had been used instead of the logging policy. In this\npaper, we identify a family of counterfactual estimators which subsumes most\nsuch estimators proposed to date. Our analysis of this family identifies a new\nestimator - called Continuous Adaptive Blending (CAB) - which enjoys many\nadvantageous theoretical and practical properties. In particular, it can be\nsubstantially less biased than clipped Inverse Propensity Score (IPS) weighting\nand the Direct Method, and it can have less variance than Doubly Robust and IPS\nestimators. In addition, it is sub-differentiable such that it can be used for\nlearning, unlike the SWITCH estimator. Experimental results show that CAB\nprovides excellent evaluation accuracy and outperforms other counterfactual\nestimators in terms of learning performance.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 21:47:00 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 22:29:01 GMT"}, {"version": "v3", "created": "Tue, 14 May 2019 01:17:17 GMT"}, {"version": "v4", "created": "Wed, 28 Aug 2019 19:01:31 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Su", "Yi", ""], ["Wang", "Lequn", ""], ["Santacatterina", "Michele", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1811.02689", "submitter": "Kentaro Yoshioka", "authors": "Kentaro Yoshioka, Edward Lee, Mark Horowitz", "title": "Training Domain Specific Models for Energy-Efficient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end framework for training domain specific models (DSMs)\nto obtain both high accuracy and computational efficiency for object detection\ntasks. DSMs are trained with distillation \\cite{hinton2015distilling} and focus\non achieving high accuracy at a limited domain (e.g. fixed view of an\nintersection). We argue that DSMs can capture essential features well even with\na small model size, enabling higher accuracy and efficiency than traditional\ntechniques. In addition, we improve the training efficiency by reducing the\ndataset size by culling easy to classify images from the training set. For the\nlimited domain, we observed that compact DSMs significantly surpass the\naccuracy of COCO trained models of the same size. By training on a compact\ndataset, we show that with an accuracy drop of only 3.6\\%, the training time\ncan be reduced by 93\\%. The codes are uploaded in\nhttps://github.com/kentaroy47/training-domain-specific-models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 22:07:54 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 06:13:13 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Yoshioka", "Kentaro", ""], ["Lee", "Edward", ""], ["Horowitz", "Mark", ""]]}, {"id": "1811.02693", "submitter": "Jacob Rafati", "authors": "Jacob Rafati and Roummel F. Marcia", "title": "Deep Reinforcement Learning via L-BFGS Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning (RL) algorithms allow artificial agents to improve\ntheir action selections so as to increase rewarding experiences in their\nenvironments. Deep Reinforcement Learning algorithms require solving a\nnonconvex and nonlinear unconstrained optimization problem. Methods for solving\nthe optimization problems in deep RL are restricted to the class of first-order\nalgorithms, such as stochastic gradient descent (SGD). The major drawback of\nthe SGD methods is that they have the undesirable effect of not escaping saddle\npoints and their performance can be seriously obstructed by ill-conditioning.\nFurthermore, SGD methods require exhaustive trial and error to fine-tune many\nlearning parameters. Using second derivative information can result in improved\nconvergence properties, but computing the Hessian matrix for large-scale\nproblems is not practical. Quasi-Newton methods require only first-order\ngradient information, like SGD, but they can construct a low rank approximation\nof the Hessian matrix and result in superlinear convergence. The limited-memory\nBroyden-Fletcher-Goldfarb-Shanno (L-BFGS) approach is one of the most popular\nquasi-Newton methods that construct positive definite Hessian approximations.\nIn this paper, we introduce an efficient optimization method, based on the\nlimited memory BFGS quasi-Newton method using line search strategy -- as an\nalternative to SGD methods. Our method bridges the disparity between first\norder methods and second order methods by continuing to use gradient\ninformation to calculate a low-rank Hessian approximations. We provide formal\nconvergence analysis as well as empirical results on a subset of the classic\nATARI 2600 games. Our results show a robust convergence with preferred\ngeneralization characteristics, as well as fast training time and no need for\nthe experience replaying mechanism.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 22:18:21 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 20:52:38 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Rafati", "Jacob", ""], ["Marcia", "Roummel F.", ""]]}, {"id": "1811.02694", "submitter": "Ran Wang", "authors": "Ran Wang, Yao Wang, Adeen Flinker", "title": "Reconstructing Speech Stimuli From Human Auditory Cortex Activity Using\n  a WaveNet Approach", "comments": "6 pages, 3 figures. Conference of 2018 IEEE Signal Processing in\n  Medicine and Biology Symposium (SPMB 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The superior temporal gyrus (STG) region of cortex critically contributes to\nspeech recognition. In this work, we show that a proposed WaveNet, with limited\navailable data, is able to reconstruct speech stimuli from STG intracranial\nrecordings. We further investigate the impulse response of the fitted model for\neach recording electrode and observe phoneme level temporospectral tuning\nproperties for the recorded area of cortex. This discovery is consistent with\nprevious studies implicating the posterior STG (pSTG) in a phonetic\nrepresentation of speech and provides detailed acoustic features that certain\nelectrode sites possibly extract during speech recognition.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 22:19:28 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 02:17:16 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Wang", "Ran", ""], ["Wang", "Yao", ""], ["Flinker", "Adeen", ""]]}, {"id": "1811.02696", "submitter": "Shangtong Zhang", "authors": "Shangtong Zhang, Hao Chen, Hengshuai Yao", "title": "ACE: An Actor Ensemble Algorithm for Continuous Control with Tree Search", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an actor ensemble algorithm, named ACE, for\ncontinuous control with a deterministic policy in reinforcement learning. In\nACE, we use actor ensemble (i.e., multiple actors) to search the global maxima\nof the critic. Besides the ensemble perspective, we also formulate ACE in the\noption framework by extending the option-critic architecture with deterministic\nintra-option policies, revealing a relationship between ensemble and options.\nFurthermore, we perform a look-ahead tree search with those actors and a\nlearned value prediction model, resulting in a refined value estimation. We\ndemonstrate a significant performance boost of ACE over DDPG and its variants\nin challenging physical robot simulators.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 22:32:55 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Zhang", "Shangtong", ""], ["Chen", "Hao", ""], ["Yao", "Hengshuai", ""]]}, {"id": "1811.02702", "submitter": "Gary Cheng", "authors": "Gary Cheng and Armin Askari and Kannan Ramchandran and Laurent El\n  Ghaoui", "title": "Greedy Frank-Wolfe Algorithm for Exemplar Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of selecting representatives from a\ndata set for arbitrary supervised/unsupervised learning tasks. We identify a\nsubset $S$ of a data set $A$ such that 1) the size of $S$ is much smaller than\n$A$ and 2) $S$ efficiently describes the entire data set, in a way formalized\nvia convex optimization. In order to generate $|S| = k$ exemplars, our\nkernelizable algorithm, Frank-Wolfe Sparse Representation (FWSR), only needs to\nexecute $\\approx k$ iterations with a per-iteration cost that is quadratic in\nthe size of $A$. This is in contrast to other state of the art methods which\nneed to execute until convergence with each iteration costing an extra factor\nof $d$ (dimension of the data). Moreover, we also provide a proof of linear\nconvergence for our method. We support our results with empirical experiments;\nwe test our algorithm against current methods in three different experimental\nsetups on four different data sets. FWSR outperforms other exemplar finding\nmethods both in speed and accuracy in almost all scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 23:32:45 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 21:55:12 GMT"}, {"version": "v3", "created": "Sun, 23 Feb 2020 02:19:14 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Cheng", "Gary", ""], ["Askari", "Armin", ""], ["Ramchandran", "Kannan", ""], ["Ghaoui", "Laurent El", ""]]}, {"id": "1811.02722", "submitter": "Danny Doan", "authors": "Minh Tuan Doan, Jianzhong Qi, Sutharshan Rajasegarar, Christopher\n  Leckie", "title": "Scalable Bottom-up Subspace Clustering using FP-Trees for High\n  Dimensional Data", "comments": "Accepted to IEEE International Conference on Big Data 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering aims to find groups of similar objects (clusters) that\nexist in lower dimensional subspaces from a high dimensional dataset. It has a\nwide range of applications, such as analysing high dimensional sensor data or\nDNA sequences. However, existing algorithms have limitations in finding\nclusters in non-disjoint subspaces and scaling to large data, which impinge\ntheir applicability in areas such as bioinformatics and the Internet of Things.\nWe aim to address such limitations by proposing a subspace clustering algorithm\nusing a bottom-up strategy. Our algorithm first searches for base clusters in\nlow dimensional subspaces. It then forms clusters in higher-dimensional\nsubspaces using these base clusters, which we formulate as a frequent pattern\nmining problem. This formulation enables efficient search for clusters in\nhigher-dimensional subspaces, which is done using FP-trees. The proposed\nalgorithm is evaluated against traditional bottom-up clustering algorithms and\nstate-of-the-art subspace clustering algorithms. The experimental results show\nthat the proposed algorithm produces clusters with high accuracy, and scales\nwell to large volumes of data. We also demonstrate the algorithm's performance\nusing real-life data, including ten genomic datasets and a car parking\noccupancy dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 01:54:39 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Doan", "Minh Tuan", ""], ["Qi", "Jianzhong", ""], ["Rajasegarar", "Sutharshan", ""], ["Leckie", "Christopher", ""]]}, {"id": "1811.02728", "submitter": "Rizal Fathony", "authors": "Rizal Fathony, Ashkan Rezaei, Mohammad Ali Bashiri, Xinhua Zhang,\n  Brian D. Ziebart", "title": "Distributionally Robust Graphical Models", "comments": "Appears in Neural Information Processing Systems, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many structured prediction problems, complex relationships between\nvariables are compactly defined using graphical structures. The most prevalent\ngraphical prediction methods---probabilistic graphical models and large margin\nmethods---have their own distinct strengths but also possess significant\ndrawbacks. Conditional random fields (CRFs) are Fisher consistent, but they do\nnot permit integration of customized loss metrics into their learning process.\nLarge-margin models, such as structured support vector machines (SSVMs), have\nthe flexibility to incorporate customized loss metrics, but lack Fisher\nconsistency guarantees. We present adversarial graphical models (AGM), a\ndistributionally robust approach for constructing a predictor that performs\nrobustly for a class of data distributions defined using a graphical structure.\nOur approach enjoys both the flexibility of incorporating customized loss\nmetrics into its design as well as the statistical guarantee of Fisher\nconsistency. We present exact learning and prediction algorithms for AGM with\ntime complexity similar to existing graphical models and show the practical\nbenefits of our approach with experiments.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 02:18:32 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Fathony", "Rizal", ""], ["Rezaei", "Ashkan", ""], ["Bashiri", "Mohammad Ali", ""], ["Zhang", "Xinhua", ""], ["Ziebart", "Brian D.", ""]]}, {"id": "1811.02746", "submitter": "Osman Tursun", "authors": "Osman Tursun, Simon Denman, Sabesan Sivapalan, Sridha Sridharan,\n  Clinton Fookes and Sandra Mau", "title": "Component-based Attention for Large-scale Trademark Retrieval", "comments": "Fix typos related to authors' information", "journal-ref": null, "doi": "10.1109/TIFS.2019.2959921", "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for large-scale trademark retrieval (TR) systems has significantly\nincreased to combat the rise in international trademark infringement.\nUnfortunately, the ranking accuracy of current approaches using either\nhand-crafted or pre-trained deep convolution neural network (DCNN) features is\ninadequate for large-scale deployments. We show in this paper that the ranking\naccuracy of TR systems can be significantly improved by incorporating hard and\nsoft attention mechanisms, which direct attention to critical information such\nas figurative elements and reduce attention given to distracting and\nuninformative elements such as text and background. Our proposed approach\nachieves state-of-the-art results on a challenging large-scale trademark\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 03:33:28 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 06:25:55 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Tursun", "Osman", ""], ["Denman", "Simon", ""], ["Sivapalan", "Sabesan", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""], ["Mau", "Sandra", ""]]}, {"id": "1811.02756", "submitter": "Kursat Rasim Mestav", "authors": "Kursat Rasim Mestav, Jaime Luengo-Rozas and Lang Tong", "title": "Bayesian State Estimation for Unobservable Distribution Systems via Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of state estimation for unobservable distribution systems is\nconsidered. A deep learning approach to Bayesian state estimation is proposed\nfor real-time applications. The proposed technique consists of distribution\nlearning of stochastic power injection, a Monte Carlo technique for the\ntraining of a deep neural network for state estimation, and a Bayesian bad-data\ndetection and filtering algorithm. Structural characteristics of the deep\nneural networks are investigated. Simulations illustrate the accuracy of\nBayesian state estimation for unobservable systems and demonstrate the benefit\nof employing a deep neural network. Numerical results show the robustness of\nBayesian state estimation against modeling and estimation errors and the\npresence of bad and missing data. Comparing with pseudo-measurement techniques,\ndirect Bayesian state estimation via deep learning neural network outperforms\nexisting benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 04:37:33 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 05:11:27 GMT"}, {"version": "v3", "created": "Mon, 17 Dec 2018 20:04:58 GMT"}, {"version": "v4", "created": "Mon, 25 Feb 2019 00:37:22 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Mestav", "Kursat Rasim", ""], ["Luengo-Rozas", "Jaime", ""], ["Tong", "Lang", ""]]}, {"id": "1811.02757", "submitter": "Yikuan Li", "authors": "Yikuan Li, Liang Yao, Chengsheng Mao, Anand Srivastava, Xiaoqian Jiang\n  and Yuan Luo", "title": "Early Prediction of Acute Kidney Injury in Critical Care Setting Using\n  Clinical Notes", "comments": "4 pages, 3 figures, accepted by BIBM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Acute kidney injury (AKI) in critically ill patients is associated with\nsignificant morbidity and mortality. Development of novel methods to identify\npatients with AKI earlier will allow for testing of novel strategies to prevent\nor reduce the complications of AKI. We developed data-driven prediction models\nto estimate the risk of new AKI onset. We generated models from clinical notes\nwithin the first 24 hours following intensive care unit (ICU) admission\nextracted from Medical Information Mart for Intensive Care III (MIMIC-III).\nFrom the clinical notes, we generated clinically meaningful word and concept\nrepresentations and embeddings, respectively. Five supervised learning\nclassifiers and knowledge-guided deep learning architecture were used to\nconstruct prediction models. The best configuration yielded a competitive AUC\nof 0.779. Our work suggests that natural language processing of clinical notes\ncan be applied to assist clinicians in identifying the risk of incident AKI\nonset in critically ill patients upon admission to the ICU.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 04:45:55 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 18:34:48 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Li", "Yikuan", ""], ["Yao", "Liang", ""], ["Mao", "Chengsheng", ""], ["Srivastava", "Anand", ""], ["Jiang", "Xiaoqian", ""], ["Luo", "Yuan", ""]]}, {"id": "1811.02770", "submitter": "Murali Karthick Baskar", "authors": "Murali Karthick Baskar, Luk\\'a\\v{s} Burget, Shinji Watanabe, Martin\n  Karafi\\'at, Takaaki Hori, Jan Honza \\v{C}ernock\\'y", "title": "Promising Accurate Prefix Boosting for sequence-to-sequence ASR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present promising accurate prefix boosting (PAPB), a\ndiscriminative training technique for attention based sequence-to-sequence\n(seq2seq) ASR. PAPB is devised to unify the training and testing scheme in an\neffective manner. The training procedure involves maximizing the score of each\npartial correct sequence obtained during beam search compared to other\nhypotheses. The training objective also includes minimization of token\n(character) error rate. PAPB shows its efficacy by achieving 10.8\\% and 3.8\\%\nWER with and without RNNLM respectively on Wall Street Journal dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 05:53:21 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Baskar", "Murali Karthick", ""], ["Burget", "Luk\u00e1\u0161", ""], ["Watanabe", "Shinji", ""], ["Karafi\u00e1t", "Martin", ""], ["Hori", "Takaaki", ""], ["\u010cernock\u00fd", "Jan Honza", ""]]}, {"id": "1811.02783", "submitter": "Yaroslav Zharov", "authors": "Yaroslav Zharov, Denis Korzhenkov, Pavel Shvechikov, Alexander\n  Tuzhilin", "title": "YASENN: Explaining Neural Networks via Partitioning Activation Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach to feed-forward neural network interpretation\nbased on partitioning the space of sequences of neuron activations. In line\nwith this approach, we propose a model-specific interpretation method, called\nYASENN. Our method inherits many advantages of model-agnostic distillation,\nsuch as an ability to focus on the particular input region and to express an\nexplanation in terms of features different from those observed by a neural\nnetwork. Moreover, examination of distillation error makes the method\napplicable to the problems with low tolerance to interpretation mistakes.\nTechnically, YASENN distills the network with an ensemble of layer-wise\ngradient boosting decision trees and encodes the sequences of neuron\nactivations with leaf indices. The finite number of unique codes induces a\npartitioning of the input space. Each partition may be described in a variety\nof ways, including examination of an interpretable model (e.g. a logistic\nregression or a decision tree) trained to discriminate between objects of those\npartitions. Our experiments provide an intuition behind the method and\ndemonstrate revealed artifacts in neural network decision making.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 07:35:45 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Zharov", "Yaroslav", ""], ["Korzhenkov", "Denis", ""], ["Shvechikov", "Pavel", ""], ["Tuzhilin", "Alexander", ""]]}, {"id": "1811.02784", "submitter": "Jiancheng Lyu", "authors": "Spencer Sheen and Jiancheng Lyu", "title": "Median Binary-Connect Method and a Binary Convolutional Neural Nework\n  for Word Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study a new projection formula for training binary weight\nconvolutional neural networks. The projection formula measures the error in\napproximating a full precision (32 bit) vector by a 1-bit vector in the l_1\nnorm instead of the standard l_2 norm. The l_1 projector is in closed\nanalytical form and involves a median computation instead of an arithmatic\naverage in the l_2 projector. Experiments on 10 keywords classification show\nthat the l_1 (median) BinaryConnect (BC) method outperforms the regular BC,\nregardless of cold or warm start. The binary network trained by median BC and a\nrecent blending technique reaches test accuracy 92.4%, which is 1.1% lower than\nthe full-precision network accuracy 93.5%. On Android phone app, the trained\nbinary network doubles the speed of full-precision network in spoken keywords\nrecognition.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 07:46:56 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Sheen", "Spencer", ""], ["Lyu", "Jiancheng", ""]]}, {"id": "1811.02790", "submitter": "Ajay Mandlekar", "authors": "Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan Booher, Max Spero,\n  Albert Tung, Julian Gao, John Emmons, Anchit Gupta, Emre Orbay, Silvio\n  Savarese, Li Fei-Fei", "title": "RoboTurk: A Crowdsourcing Platform for Robotic Skill Learning through\n  Imitation", "comments": "Published at the Conference on Robot Learning (CoRL) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation Learning has empowered recent advances in learning robotic\nmanipulation tasks by addressing shortcomings of Reinforcement Learning such as\nexploration and reward specification. However, research in this area has been\nlimited to modest-sized datasets due to the difficulty of collecting large\nquantities of task demonstrations through existing mechanisms. This work\nintroduces RoboTurk to address this challenge. RoboTurk is a crowdsourcing\nplatform for high quality 6-DoF trajectory based teleoperation through the use\nof widely available mobile devices (e.g. iPhone). We evaluate RoboTurk on three\nmanipulation tasks of varying timescales (15-120s) and observe that our user\ninterface is statistically similar to special purpose hardware such as virtual\nreality controllers in terms of task completion times. Furthermore, we observe\nthat poor network conditions, such as low bandwidth and high delay links, do\nnot substantially affect the remote users' ability to perform task\ndemonstrations successfully on RoboTurk. Lastly, we demonstrate the efficacy of\nRoboTurk through the collection of a pilot dataset; using RoboTurk, we\ncollected 137.5 hours of manipulation data from remote workers, amounting to\nover 2200 successful task demonstrations in 22 hours of total system usage. We\nshow that the data obtained through RoboTurk enables policy learning on\nmulti-step manipulation tasks with sparse rewards and that using larger\nquantities of demonstrations during policy learning provides benefits in terms\nof both learning consistency and final performance. For additional results,\nvideos, and to download our pilot dataset, visit\n$\\href{http://roboturk.stanford.edu/}{\\texttt{roboturk.stanford.edu}}$\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 08:01:21 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Mandlekar", "Ajay", ""], ["Zhu", "Yuke", ""], ["Garg", "Animesh", ""], ["Booher", "Jonathan", ""], ["Spero", "Max", ""], ["Tung", "Albert", ""], ["Gao", "Julian", ""], ["Emmons", "John", ""], ["Gupta", "Anchit", ""], ["Orbay", "Emre", ""], ["Savarese", "Silvio", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1811.02798", "submitter": "Phi Vu Tran", "authors": "Phi Vu Tran", "title": "Multi-Task Graph Autoencoders", "comments": "NIPS 2018 Workshop on Relational Representation Learning. Short\n  version of arXiv:1802.08352", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We examine two fundamental tasks associated with graph representation\nlearning: link prediction and node classification. We present a new autoencoder\narchitecture capable of learning a joint representation of local graph\nstructure and available node features for the simultaneous multi-task learning\nof unsupervised link prediction and semi-supervised node classification. Our\nsimple, yet effective and versatile model is efficiently trained end-to-end in\na single stage, whereas previous related deep graph embedding methods require\nmultiple training steps that are difficult to optimize. We provide an empirical\nevaluation of our model on five benchmark relational, graph-structured datasets\nand demonstrate significant improvement over three strong baselines for graph\nrepresentation learning. Reference code and data are available at\nhttps://github.com/vuptran/graph-representation-learning\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 08:27:52 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Tran", "Phi Vu", ""]]}, {"id": "1811.02814", "submitter": "Weiping Zhang", "authors": "Ye Tian, Weiping Zhang", "title": "THORS: An Efficient Approach for Making Classifiers Cost-sensitive", "comments": "26 pages, 6 figures", "journal-ref": "IEEE ACCESS 7(1):97704-97718 2019", "doi": "10.1109/ACCESS.2019.2929078", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an effective THresholding method based on ORder\nStatistic, called THORS, to convert an arbitrary scoring-type classifier, which\ncan induce a continuous cumulative distribution function of the score, into a\ncost-sensitive one. The procedure, uses order statistic to find an optimal\nthreshold for classification, requiring almost no knowledge of classifiers\nitself. Unlike common data-driven methods, we analytically show that THORS has\ntheoretical guaranteed performance, theoretical bounds for the costs and lower\ntime complexity. Coupled with empirical results on several real-world data\nsets, we argue that THORS is the preferred cost-sensitive technique.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 10:06:00 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Tian", "Ye", ""], ["Zhang", "Weiping", ""]]}, {"id": "1811.02815", "submitter": "Peijie Sun", "authors": "Le Wu, Peijie Sun, Richang Hong, Yanjie Fu, Xiting Wang and Meng Wang", "title": "SocialGCN: An Efficient Graph Convolutional Network based Model for\n  Social Recommendation", "comments": "Our new version of this paper has been accepted by SIGIR 2019 and the\n  link to the new version is arXiv:1904.10322", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative Filtering (CF) is one of the most successful approaches for\nrecommender systems. With the emergence of online social networks, social\nrecommendation has become a popular research direction. Most of these social\nrecommendation models utilized each user's local neighbors' preferences to\nalleviate the data sparsity issue in CF. However, they only considered the\nlocal neighbors of each user and neglected the process that users' preferences\nare influenced as information diffuses in the social network. Recently, Graph\nConvolutional Networks~(GCN) have shown promising results by modeling the\ninformation diffusion process in graphs that leverage both graph structure and\nnode feature information. To this end, in this paper, we propose an effective\ngraph convolutional neural network based model for social recommendation. Based\non a classical CF model, the key idea of our proposed model is that we borrow\nthe strengths of GCNs to capture how users' preferences are influenced by the\nsocial diffusion process in social networks. The diffusion of users'\npreferences is built on a layer-wise diffusion manner, with the initial user\nembedding as a function of the current user's features and a free base user\nlatent vector that is not contained in the user feature. Similarly, each item's\nlatent vector is also a combination of the item's free latent vector, as well\nas its feature representation. Furthermore, we show that our proposed model is\nflexible when user and item features are not available. Finally, extensive\nexperimental results on two real-world datasets clearly show the effectiveness\nof our proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 10:07:52 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 03:06:02 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Wu", "Le", ""], ["Sun", "Peijie", ""], ["Hong", "Richang", ""], ["Fu", "Yanjie", ""], ["Wang", "Xiting", ""], ["Wang", "Meng", ""]]}, {"id": "1811.02820", "submitter": "Anton Belyy", "authors": "Anton Belyy", "title": "Construction and Quality Evaluation of Heterogeneous Hierarchical Topic\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In our work, we propose to represent HTM as a set of flat models, or layers,\nand a set of topical hierarchies, or edges. We suggest several quality measures\nfor edges of hierarchical models, resembling those proposed for flat models. We\nconduct an assessment experimentation and show strong correlation between the\nproposed measures and human judgement on topical edge quality. We also\nintroduce heterogeneous algorithm to build hierarchical topic models for\nheterogeneous data sources. We show how making certain adjustments to learning\nprocess helps to retain original structure of customized models while allowing\nfor slight coherent modifications for new documents. We evaluate this approach\nusing the proposed measures and show that the proposed heterogeneous algorithm\nsignificantly outperforms the baseline concat approach. Finally, we implement\nour own ESE called Rysearch, which demonstrates the potential of ARTM approach\nfor visualizing large heterogeneous document collections.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 10:32:50 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Belyy", "Anton", ""]]}, {"id": "1811.02827", "submitter": "Luca Ambrogioni", "authors": "Luca Ambrogioni, Umut Guclu and Marcel van Gerven", "title": "Wasserstein variational gradient descent: From semi-discrete optimal\n  transport to ensemble variational inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle-based variational inference offers a flexible way of approximating\ncomplex posterior distributions with a set of particles. In this paper we\nintroduce a new particle-based variational inference method based on the theory\nof semi-discrete optimal transport. Instead of minimizing the KL divergence\nbetween the posterior and the variational approximation, we minimize a\nsemi-discrete optimal transport divergence. The solution of the resulting\noptimal transport problem provides both a particle approximation and a set of\noptimal transportation densities that map each particle to a segment of the\nposterior distribution. We approximate these transportation densities by\nminimizing the KL divergence between a truncated distribution and the optimal\ntransport solution. The resulting algorithm can be interpreted as a form of\nensemble variational inference where each particle is associated with a local\nvariational approximation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 10:50:40 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 15:21:50 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Ambrogioni", "Luca", ""], ["Guclu", "Umut", ""], ["van Gerven", "Marcel", ""]]}, {"id": "1811.02834", "submitter": "Vayer Titouan", "authors": "Titouan Vayer, Laetita Chapel, R\\'emi Flamary, Romain Tavenard,\n  Nicolas Courty", "title": "Fused Gromov-Wasserstein distance for structured objects: theoretical\n  foundations and mathematical properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal transport theory has recently found many applications in machine\nlearning thanks to its capacity for comparing various machine learning objects\nconsidered as distributions. The Kantorovitch formulation, leading to the\nWasserstein distance, focuses on the features of the elements of the objects\nbut treat them independently, whereas the Gromov-Wasserstein distance focuses\nonly on the relations between the elements, depicting the structure of the\nobject, yet discarding its features.\n  In this paper we propose to extend these distances in order to encode\nsimultaneously both the feature and structure informations, resulting in the\nFused Gromov-Wasserstein distance. We develop the mathematical framework for\nthis novel distance, prove its metric and interpolation properties and provide\na concentration result for the convergence of finite samples. We also\nillustrate and interpret its use in various contexts where structured objects\nare involved.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 11:06:43 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Vayer", "Titouan", ""], ["Chapel", "Laetita", ""], ["Flamary", "R\u00e9mi", ""], ["Tavenard", "Romain", ""], ["Courty", "Nicolas", ""]]}, {"id": "1811.02850", "submitter": "Ilya Kamenshchikov", "authors": "Ilya Kamenshchikov, Matthias Krauledat", "title": "Effects of Dataset properties on the training of GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks are a new family of generative models,\nfrequently used for generating photorealistic images. The theory promises for\nthe GAN to eventually reach an equilibrium where generator produces pictures\nindistinguishable for the training set. In practice, however, a range of\nproblems frequently prevents the system from reaching this equilibrium, with\ntraining not progressing ahead due to instabilities or mode collapse. This\npaper describes a series of experiments trying to identify patterns in regard\nto the effect of the training set on the dynamics and eventual outcome of the\ntraining.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 11:49:51 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 09:48:12 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2018 08:29:37 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Kamenshchikov", "Ilya", ""], ["Krauledat", "Matthias", ""]]}, {"id": "1811.02934", "submitter": "Deyu Meng", "authors": "Hongwei Yong, Deyu Meng, Jinxing Li, Wangmeng Zuo, Lei Zhang", "title": "Model Inconsistent but Correlated Noise: Multi-view Subspace Learning\n  with Regularized Mixture of Gaussians", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view subspace learning (MSL) aims to find a low-dimensional subspace of\nthe data obtained from multiple views. Different from single view case, MSL\nshould take both common and specific knowledge among different views into\nconsideration. To enhance the robustness of model, the complexity,\nnon-consistency and similarity of noise in multi-view data should be fully\ntaken into consideration. Most current MSL methods only assume a simple\nGaussian or Laplacian distribution for the noise while neglect the complex\nnoise configurations in each view and noise correlations among different views\nof practical data. To this issue, this work initiates a MSL method by encoding\nthe multi-view-shared and single-view-specific noise knowledge in data.\nSpecifically, we model data noise in each view as a separated Mixture of\nGaussians (MoG), which can fit a wider range of complex noise types than\nconventional Gaussian/Laplacian. Furthermore, we link all single-view-noise as\na whole by regularizing them by a common MoG component, encoding the shared\nnoise knowledge among them. Such regularization component can be formulated as\na concise KL-divergence regularization term under a MAP framework, leading to\ngood interpretation of our model and simple EM-based solving strategy to the\nproblem. Experimental results substantiate the superiority of our method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 15:30:39 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Yong", "Hongwei", ""], ["Meng", "Deyu", ""], ["Li", "Jinxing", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Lei", ""]]}, {"id": "1811.02945", "submitter": "Marija Jegorova", "authors": "Marija Jegorova, St\\'ephane Doncieux, Timothy Hospedales", "title": "Behavioural Repertoire via Generative Adversarial Policy Networks", "comments": "In Proceedings of 2019 Joint IEEE 9th International Conference on\n  Development and Learning and Epigenetic Robotics (ICDL-EpiRob), pages 320 -\n  326", "journal-ref": "2019 Joint IEEE 9th International Conference on Development and\n  Learning and Epigenetic Robotics (ICDL-EpiRob)", "doi": "10.1109/ICDL-EpiRob44920.2019", "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning algorithms are enabling robots to solve increasingly challenging\nreal-world tasks. These approaches often rely on demonstrations and reproduce\nthe behavior shown. Unexpected changes in the environment may require using\ndifferent behaviors to achieve the same effect, for instance to reach and grasp\nan object in changing clutter. An emerging paradigm addressing this robustness\nissue is to learn a diverse set of successful behaviors for a given task, from\nwhich a robot can select the most suitable policy when faced with a new\nenvironment. In this paper, we explore a novel realization of this vision by\nlearning a generative model over policies. Rather than learning a single\npolicy, or a small fixed repertoire, our generative model for policies\ncompactly encodes an unbounded number of policies and allows novel controller\nvariants to be sampled. Leveraging our generative policy network, a robot can\nsample novel behaviors until it finds one that works for a new environment. We\ndemonstrate this idea with an application of robust ball-throwing in the\npresence of obstacles. We show that this approach achieves a greater diversity\nof behaviors than an existing evolutionary approach, while maintaining good\nefficacy of sampled behaviors, allowing a Baxter robot to hit targets more\noften when ball throwing in the presence of obstacles.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 15:47:48 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 17:11:05 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 17:37:28 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Jegorova", "Marija", ""], ["Doncieux", "St\u00e9phane", ""], ["Hospedales", "Timothy", ""]]}, {"id": "1811.02979", "submitter": "Benjamin Mark", "authors": "Benjamin Mark, Garvesh Raskutti, Rebecca Willett", "title": "Estimating Network Structure from Incomplete Event Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate Bernoulli autoregressive (BAR) processes model time series of\nevents in which the likelihood of current events is determined by the times and\nlocations of past events. These processes can be used to model nonlinear\ndynamical systems corresponding to criminal activity, responses of patients to\ndifferent medical treatment plans, opinion dynamics across social networks,\nepidemic spread, and more. Past work examines this problem under the assumption\nthat the event data is complete, but in many cases only a fraction of events\nare observed. Incomplete observations pose a significant challenge in this\nsetting because the unobserved events still govern the underlying dynamical\nsystem. In this work, we develop a novel approach to estimating the parameters\nof a BAR process in the presence of unobserved events via an unbiased estimator\nof the complete data log-likelihood function. We propose a computationally\nefficient estimation algorithm which approximates this estimator via Taylor\nseries truncation and establish theoretical results for both the statistical\nerror and optimization error of our algorithm. We further justify our approach\nby testing our method on both simulated data and a real data set consisting of\ncrimes recorded by the city of Chicago.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 16:44:48 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Mark", "Benjamin", ""], ["Raskutti", "Garvesh", ""], ["Willett", "Rebecca", ""]]}, {"id": "1811.02986", "submitter": "Loc Tran H", "authors": "Loc Hoang Tran, Linh Hoang Tran", "title": "Un-normalized hypergraph p-Laplacian based semi-supervised learning\n  methods", "comments": "13 pages, 3 figures, 2 tables. arXiv admin note: text overlap with\n  arXiv:1810.12743, arXiv:1212.0388", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most network-based machine learning methods assume that the labels of two\nadjacent samples in the network are likely to be the same. However, assuming\nthe pairwise relationship between samples is not complete. The information a\ngroup of samples that shows very similar pattern and tends to have similar\nlabels is missed. The natural way overcoming the information loss of the above\nassumption is to represent the feature dataset of samples as the hypergraph.\nThus, in this paper, we will present the un-normalized hypergraph p-Laplacian\nsemi-supervised learning methods. These methods will be applied to the zoo\ndataset and the tiny version of 20 newsgroups dataset. Experiment results show\nthat the accuracy performance measures of these un-normalized hypergraph\np-Laplacian based semi-supervised learning methods are significantly greater\nthan the accuracy performance measure of the un-normalized hypergraph Laplacian\nbased semi-supervised learning method (the current state of the art method\nhypergraph Laplacian based semi-supervised learning method for classification\nproblem with p=2).\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 03:46:32 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 13:21:46 GMT"}, {"version": "v3", "created": "Sun, 28 Apr 2019 10:51:40 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Tran", "Loc Hoang", ""], ["Tran", "Linh Hoang", ""]]}, {"id": "1811.02994", "submitter": "Jixue Liu", "authors": "Jixue Liu, Jiuyong Li, Feiyue Ye, Lin Liu, Thuc Duy Le, Ping Xiong", "title": "An exploration of algorithmic discrimination in data and classification", "comments": "arXiv admin note: text overlap with arXiv:1811.01480", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic discrimination is an important aspect when data is used for\npredictive purposes. This paper analyzes the relationships between\ndiscrimination and classification, data set partitioning, and decision models,\nas well as correlation. The paper uses real world data sets to demonstrate the\nexistence of discrimination and the independence between the discrimination of\ndata sets and the discrimination of classification models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 12:42:58 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Liu", "Jixue", ""], ["Li", "Jiuyong", ""], ["Ye", "Feiyue", ""], ["Liu", "Lin", ""], ["Le", "Thuc Duy", ""], ["Xiong", "Ping", ""]]}, {"id": "1811.03014", "submitter": "Adrienne Mendrik", "authors": "Adrienne M. Mendrik, Stephen R. Aylward", "title": "Beyond the Leaderboard: Insight and Deployment Challenges to Address\n  Research Problems", "comments": "This two-page abstract was accepted for the NIPS 2018 Challenges in\n  Machine Learning (CiML) workshop \"Machine Learning competitions \"in the\n  wild\": Playing in the real world or in real time\" on Saturday December 8,\n  2018 in Palais des congres de Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the medical image analysis field, organizing challenges with associated\nworkshops at international conferences began in 2007 and has grown to include\nover 150 challenges. Several of these challenges have had a major impact in the\nfield. However, whereas well-designed challenges have the potential to unite\nand focus the field on creating solutions to important problems, poorly\ndesigned and documented challenges can equally impede a field and lead to\npursuing incremental improvements in metric scores with no theoretic or\nclinical significance. This is supported by a critical assessment of challenges\nat the international MICCAI conference. In this assessment the main observation\nwas that small changes to the underlying challenge data can drastically change\nthe ranking order on the leaderboard. Related to this is the practice of\nleaderboard climbing, which is characterized by participants focusing on\nincrementally improving metric results rather than advancing science or solving\nthe driving problem of a challenge. In this abstract we look beyond the\nleaderboard of a challenge and instead look at the conclusions that can be\ndrawn from a challenge with respect to the research problem that it is\naddressing. Research study design is well described in other research areas and\ncan be translated to challenge design when viewing challenges as research\nstudies on algorithm performance that address a research problem. Based on the\ntwo main types of scientific research study design, we propose two main\nchallenge types, which we think would benefit other research areas as well: 1)\nan insight challenge that is based on a qualitative study design and 2) a\ndeployment challenge that is based on a quantitative study design. In addition\nwe briefly touch upon related considerations with respect to statistical\nsignificance versus practical significance, generalizability and data\nsaturation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 10:18:07 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Mendrik", "Adrienne M.", ""], ["Aylward", "Stephen R.", ""]]}, {"id": "1811.03016", "submitter": "Hadi Mansourifar", "authors": "Hadi Mansourifar, Weidong Shi", "title": "Toward Efficient Breast Cancer Diagnosis and Survival Prediction Using\n  L-Perceptron", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is the most frequently reported cancer type among the women\naround the globe and beyond that it has the second highest female fatality rate\namong all cancer types. Despite all the progresses made in prevention and early\nintervention, early prognosis and survival prediction rates are still\nunsatisfactory. In this paper, we propose a novel type of perceptron called\nL-Perceptron which outperforms all the previous supervised learning methods by\nreaching 97.42 \\% and 98.73 \\% in terms of accuracy and sensitivity,\nrespectively in Wisconsin Breast Cancer dataset. Experimental results on\nHaberman's Breast Cancer Survival dataset, show the superiority of proposed\nmethod by reaching 75.18 \\% and 83.86 \\% in terms of accuracy and F1 score,\nrespectively. The results are the best reported ones obtained in 10-fold cross\nvalidation in absence of any preprocessing or feature selection.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 05:17:08 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Mansourifar", "Hadi", ""], ["Shi", "Weidong", ""]]}, {"id": "1811.03056", "submitter": "Christoph Dann", "authors": "Christoph Dann, Lihong Li, Wei Wei, Emma Brunskill", "title": "Policy Certificates: Towards Accountable Reinforcement Learning", "comments": "article appearing at ICML 2019; full version including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of a reinforcement learning algorithm can vary drastically\nduring learning because of exploration. Existing algorithms provide little\ninformation about the quality of their current policy before executing it, and\nthus have limited use in high-stakes applications like healthcare. We address\nthis lack of accountability by proposing that algorithms output policy\ncertificates. These certificates bound the sub-optimality and return of the\npolicy in the next episode, allowing humans to intervene when the certified\nquality is not satisfactory. We further introduce two new algorithms with\ncertificates and present a new framework for theoretical analysis that\nguarantees the quality of their policies and certificates. For tabular MDPs, we\nshow that computing certificates can even improve the sample-efficiency of\noptimism-based exploration. As a result, one of our algorithms is the first to\nachieve minimax-optimal PAC bounds up to lower-order terms, and this algorithm\nalso matches (and in some settings slightly improves upon) existing minimax\nregret bounds.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 18:16:28 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 15:32:43 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 20:50:36 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Dann", "Christoph", ""], ["Li", "Lihong", ""], ["Wei", "Wei", ""], ["Brunskill", "Emma", ""]]}, {"id": "1811.03060", "submitter": "Raphael Tang", "authors": "Raphael Tang, Ashutosh Adhikari, Jimmy Lin", "title": "FLOPs as a Direct Optimization Objective for Learning Sparse Neural\n  Networks", "comments": "4 pages, accepted to the NIPS 2018 Workshop on Compact Deep Neural\n  Networks with Industrial Applications (CDNNRIA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a plethora of techniques for inducing structured sparsity in\nparametric models during the optimization process, with the final goal of\nresource-efficient inference. However, few methods target a specific number of\nfloating-point operations (FLOPs) as part of the optimization objective,\ndespite many reporting FLOPs as part of the results. Furthermore, a\none-size-fits-all approach ignores realistic system constraints, which differ\nsignificantly between, say, a GPU and a mobile phone -- FLOPs on the former\nincur less latency than on the latter; thus, it is important for practitioners\nto be able to specify a target number of FLOPs during model compression. In\nthis work, we extend a state-of-the-art technique to directly incorporate FLOPs\nas part of the optimization objective and show that, given a desired FLOPs\nrequirement, different neural networks can be successfully trained for image\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 18:21:25 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 18:20:10 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Tang", "Raphael", ""], ["Adhikari", "Ashutosh", ""], ["Lin", "Jimmy", ""]]}, {"id": "1811.03064", "submitter": "Chin-Chia Michael Yeh", "authors": "Chin-Chia Michael Yeh", "title": "Towards a Near Universal Time Series Data Mining Tool: Introducing the\n  Matrix Profile", "comments": "PhD dissertation (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has seen a flurry of research on all-pairs-similarity-search\n(or, self-join) for text, DNA, and a handful of other datatypes, and these\nsystems have been applied to many diverse data mining problems. Surprisingly,\nhowever, little progress has been made on addressing this problem for time\nseries subsequences. In this thesis, we have introduced a near universal time\nseries data mining tool called matrix profile which solves the\nall-pairs-similarity-search problem and caches the output in an easy-to-access\nfashion. The proposed algorithm is not only parameter-free, exact and scalable,\nbut also applicable for both single and multidimensional time series. By\nbuilding time series data mining methods on top of matrix profile, many time\nseries data mining tasks (e.g., motif discovery, discord discovery, shapelet\ndiscovery, semantic segmentation, and clustering) can be efficiently solved.\nBecause the same matrix profile can be shared by a diverse set of time series\ndata mining methods, matrix profile is versatile and\ncomputed-once-use-many-times data structure. We demonstrate the utility of\nmatrix profile for many time series data mining problems, including motif\ndiscovery, discord discovery, weakly labeled time series classification, and\nrepresentation learning on domains as diverse as seismology, entomology, music\nprocessing, bioinformatics, human activity monitoring, electrical power-demand\nmonitoring, and medicine. We hope the matrix profile is not the end but the\nbeginning of many more time series data mining projects.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 09:21:33 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 04:40:46 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Yeh", "Chin-Chia Michael", ""]]}, {"id": "1811.03066", "submitter": "Viraj Prabhu", "authors": "Viraj Prabhu, Anitha Kannan, Murali Ravuri, Manish Chablani, David\n  Sontag, Xavier Amatriain", "title": "Prototypical Clustering Networks for Dermatological Disease Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of image classification for the purpose of aiding\ndoctors in dermatological diagnosis. Dermatological diagnosis poses two major\nchallenges for standard off-the-shelf techniques: First, the data distribution\nis typically extremely long tailed. Second, intra-class variability is often\nlarge. To address the first issue, we formulate the problem as low-shot\nlearning, where once deployed, a base classifier must rapidly generalize to\ndiagnose novel conditions given very few labeled examples. To model diverse\nclasses effectively, we propose Prototypical Clustering Networks (PCN), an\nextension to Prototypical Networks that learns a mixture of prototypes for each\nclass. Prototypes are initialized for each class via clustering and refined via\nan online update scheme. Classification is performed by measuring similarity to\na weighted combination of prototypes within a class, where the weights are the\ninferred cluster responsibilities. We demonstrate the strengths of our approach\nin effective diagnosis on a realistic dataset of dermatological conditions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 18:27:41 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Prabhu", "Viraj", ""], ["Kannan", "Anitha", ""], ["Ravuri", "Murali", ""], ["Chablani", "Manish", ""], ["Sontag", "David", ""], ["Amatriain", "Xavier", ""]]}, {"id": "1811.03076", "submitter": "Prem Seetharaman", "authors": "Prem Seetharaman, Gordon Wichern, Shrikant Venkataramani, Jonathan Le\n  Roux", "title": "Class-conditional embeddings for music source separation", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Isolating individual instruments in a musical mixture has a myriad of\npotential applications, and seems imminently achievable given the levels of\nperformance reached by recent deep learning methods. While most musical source\nseparation techniques learn an independent model for each instrument, we\npropose using a common embedding space for the time-frequency bins of all\ninstruments in a mixture inspired by deep clustering and deep attractor\nnetworks. Additionally, an auxiliary network is used to generate parameters of\na Gaussian mixture model (GMM) where the posterior distribution over GMM\ncomponents in the embedding space can be used to create a mask that separates\nindividual sources from a mixture. In addition to outperforming a\nmask-inference baseline on the MUSDB-18 dataset, our embedding space is easily\ninterpretable and can be used for query-based separation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 18:49:34 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Seetharaman", "Prem", ""], ["Wichern", "Gordon", ""], ["Venkataramani", "Shrikant", ""], ["Roux", "Jonathan Le", ""]]}, {"id": "1811.03081", "submitter": "Ben Moews", "authors": "Levi Fussell and Ben Moews", "title": "Forging new worlds: high-resolution synthetic galaxies with chained\n  generative adversarial networks", "comments": "13 pages, 9 figures", "journal-ref": "Mon. Notices Royal Astron. Soc. 485(3) (2019) 3203-3214", "doi": "10.1093/mnras/stz602", "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astronomy of the 21st century increasingly finds itself with extreme\nquantities of data. This growth in data is ripe for modern technologies such as\ndeep image processing, which has the potential to allow astronomers to\nautomatically identify, classify, segment and deblend various astronomical\nobjects. In this paper, we explore the use of chained generative adversarial\nnetworks (GANs), a class of generative models that learn mappings from latent\nspaces to data distributions by modelling the joint distribution of the data,\nto produce physically realistic galaxy images as one use case of such models.\nIn cosmology, such datasets can aid in the calibration of shape measurements\nfor weak lensing by augmenting data with synthetic images. By measuring the\ndistributions of multiple physical properties, we show that images generated\nwith our approach closely follow the distributions of real galaxies, further\nestablishing state-of-the-art GAN architectures as a valuable tool for\nmodern-day astronomy.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 18:56:55 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 18:48:30 GMT"}, {"version": "v3", "created": "Sat, 16 Mar 2019 15:28:08 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Fussell", "Levi", ""], ["Moews", "Ben", ""]]}, {"id": "1811.03087", "submitter": "Antoine Labatie", "authors": "Antoine Labatie", "title": "Characterizing Well-Behaved vs. Pathological Deep Neural Networks", "comments": "Proceedings of ICML 2019 (with contact info updated and formatting\n  issues fixed). Code available at https://github.com/alabatie/moments-dnns", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach, requiring only mild assumptions, for the\ncharacterization of deep neural networks at initialization. Our approach\napplies both to fully-connected and convolutional networks and easily\nincorporates batch normalization and skip-connections. Our key insight is to\nconsider the evolution with depth of statistical moments of signal and noise,\nthereby characterizing the presence or absence of pathologies in the hypothesis\nspace encoded by the choice of hyperparameters. We establish: (i) for\nfeedforward networks, with and without batch normalization, the\nmultiplicativity of layer composition inevitably leads to ill-behaved moments\nand pathologies; (ii) for residual networks with batch normalization, on the\nother hand, skip-connections induce power-law rather than exponential\nbehaviour, leading to well-behaved moments and no pathology.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 18:59:37 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 17:27:38 GMT"}, {"version": "v3", "created": "Mon, 20 May 2019 11:10:24 GMT"}, {"version": "v4", "created": "Tue, 18 Jun 2019 17:49:07 GMT"}, {"version": "v5", "created": "Wed, 19 Jun 2019 12:43:23 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Labatie", "Antoine", ""]]}, {"id": "1811.03115", "submitter": "Mitchell Stern", "authors": "Mitchell Stern, Noam Shazeer, Jakob Uszkoreit", "title": "Blockwise Parallel Decoding for Deep Autoregressive Models", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep autoregressive sequence-to-sequence models have demonstrated impressive\nperformance across a wide variety of tasks in recent years. While common\narchitecture classes such as recurrent, convolutional, and self-attention\nnetworks make different trade-offs between the amount of computation needed per\nlayer and the length of the critical path at training time, generation still\nremains an inherently sequential process. To overcome this limitation, we\npropose a novel blockwise parallel decoding scheme in which we make predictions\nfor multiple time steps in parallel then back off to the longest prefix\nvalidated by a scoring model. This allows for substantial theoretical\nimprovements in generation speed when applied to architectures that can process\noutput sequences in parallel. We verify our approach empirically through a\nseries of experiments using state-of-the-art self-attention models for machine\ntranslation and image super-resolution, achieving iteration reductions of up to\n2x over a baseline greedy decoder with no loss in quality, or up to 7x in\nexchange for a slight decrease in performance. In terms of wall-clock time, our\nfastest models exhibit real-time speedups of up to 4x over standard greedy\ndecoding.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 19:09:40 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Stern", "Mitchell", ""], ["Shazeer", "Noam", ""], ["Uszkoreit", "Jakob", ""]]}, {"id": "1811.03129", "submitter": "Michael Wakin", "authors": "Zhihui Zhu, Qiuwei Li, Xinshuo Yang, Gongguo Tang, and Michael B.\n  Wakin", "title": "Global Optimality in Distributed Low-rank Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the convergence of a variant of distributed gradient descent (DGD)\non a distributed low-rank matrix approximation problem wherein some\noptimization variables are used for consensus (as in classical DGD) and some\noptimization variables appear only locally at a single node in the network. We\nterm the resulting algorithm DGD+LOCAL. Using algorithmic connections to\ngradient descent and geometric connections to the well-behaved landscape of the\ncentralized low-rank matrix approximation problem, we identify sufficient\nconditions where DGD+LOCAL is guaranteed to converge with exact consensus to a\nglobal minimizer of the original centralized problem. For the distributed\nlow-rank matrix approximation problem, these guarantees are stronger---in terms\nof consensus and optimality---than what appear in the literature for classical\nDGD and more general problems.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 19:48:20 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2018 20:03:10 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Zhu", "Zhihui", ""], ["Li", "Qiuwei", ""], ["Yang", "Xinshuo", ""], ["Tang", "Gongguo", ""], ["Wakin", "Michael B.", ""]]}, {"id": "1811.03146", "submitter": "Marvin Kennis", "authors": "Marvin Aron Kennis", "title": "Multi-channel discourse as an indicator for Bitcoin price and volume\n  movements", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research aims to identify how Bitcoin-related news publications and\nonline discourse are expressed in Bitcoin exchange movements of price and\nvolume. Being inherently digital, all Bitcoin-related fundamental data (from\nexchanges, as well as transactional data directly from the blockchain) is\navailable online, something that is not true for traditional businesses or\ncurrencies traded on exchanges. This makes Bitcoin an interesting subject for\nsuch research, as it enables the mapping of sentiment to fundamental events\nthat might otherwise be inaccessible. Furthermore, Bitcoin discussion largely\ntakes place on online forums and chat channels. In stock trading, the value of\nsentiment data in trading decisions has been demonstrated numerous times [1]\n[2] [3], and this research aims to determine whether there is value in such\ndata for Bitcoin trading models. To achieve this, data over the year 2015 has\nbeen collected from Bitcointalk.org, (the biggest Bitcoin forum in post\nvolume), established news sources such as Bloomberg and the Wall Street\nJournal, the complete /r/btc and /r/Bitcoin subreddits, and the bitcoin-otc and\nbitcoin-dev IRC channels. By analyzing this data on sentiment and volume, we\nfind weak to moderate correlations between forum, news, and Reddit sentiment\nand movements in price and volume from 1 to 5 days after the sentiment was\nexpressed. A Granger causality test confirms the predictive causality of the\nsentiment on the daily percentage price and volume movements, and at the same\ntime underscores the predictive causality of market movements on sentiment\nexpressions in online communities\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 18:39:00 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Kennis", "Marvin Aron", ""]]}, {"id": "1811.03149", "submitter": "Alireza Abdoli", "authors": "Alireza Abdoli, Amy C. Murillo, Chin-Chia M. Yeh, Alec C. Gerry,\n  Eamonn J. Keogh", "title": "Time Series Classification to Improve Poultry Welfare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poultry farms are an important contributor to the human food chain.\nWorldwide, humankind keeps an enormous number of domesticated birds (e.g.\nchickens) for their eggs and their meat, providing rich sources of low-fat\nprotein. However, around the world, there have been growing concerns about the\nquality of life for the livestock in poultry farms; and increasingly vocal\ndemands for improved standards of animal welfare. Recent advances in sensing\ntechnologies and machine learning allow the possibility of automatically\nassessing the health of some individual birds, and employing the lessons\nlearned to improve the welfare for all birds. This task superficially appears\nto be easy, given the dramatic progress in recent years in classifying human\nbehaviors, and given that human behaviors are presumably more complex. However,\nas we shall demonstrate, classifying chicken behaviors poses several unique\nchallenges, chief among which is creating a generalizable dictionary of\nbehaviors from sparse and noisy data. In this work we introduce a novel time\nseries dictionary learning algorithm that can robustly learn from weakly\nlabeled data sources.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 21:18:40 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Abdoli", "Alireza", ""], ["Murillo", "Amy C.", ""], ["Yeh", "Chin-Chia M.", ""], ["Gerry", "Alec C.", ""], ["Keogh", "Eamonn J.", ""]]}, {"id": "1811.03151", "submitter": "Kathleen Greene", "authors": "K. Gretchen Greene", "title": "DragonPaint: Rule based bootstrapping for small data with an application\n  to cartoon coloring", "comments": null, "journal-ref": "In Proceedings of the Fourth International Conference on\n  Predictive Applications and APIs, 82, 1-9, Boston, MA, USA, 2018. PMLR", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we confront the problem of deep learning's big labeled data\nrequirements, offer a rule based strategy for extreme augmentation of small\ndata sets and apply that strategy with the image to image translation model by\nIsola et al. (2016) to automate cel style cartoon coloring with very limited\ntraining data. While our experimental results using geometric rules and\ntransformations demonstrate the performance of our methods on an image\ntranslation task with industry applications in art, design and animation, we\nalso propose the use of rules on partial data sets as a generalizable small\ndata strategy, potentially applicable across data types and domains.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 21:23:31 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Greene", "K. Gretchen", ""]]}, {"id": "1811.03154", "submitter": "Francisco Ruiz", "authors": "Maryam Fatemi, Karl Granstr\\\"om, Lennart Svensson, Francisco J. R.\n  Ruiz, Lars Hammarstrand", "title": "Poisson Multi-Bernoulli Mapping Using Gibbs Sampling", "comments": "14 pages, 6 figures", "journal-ref": "IEEE Transactions on Signal Processing, Vol. 65, Issue 11, June\n  2017", "doi": "10.1109/TSP.2017.2675866", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the mapping problem. Using a conjugate prior form, we\nderive the exact theoretical batch multi-object posterior density of the map\ngiven a set of measurements. The landmarks in the map are modeled as extended\nobjects, and the measurements are described as a Poisson process, conditioned\non the map. We use a Poisson process prior on the map and prove that the\nposterior distribution is a hybrid Poisson, multi-Bernoulli mixture\ndistribution. We devise a Gibbs sampling algorithm to sample from the batch\nmulti-object posterior. The proposed method can handle uncertainties in the\ndata associations and the cardinality of the set of landmarks, and is\nparallelizable, making it suitable for large-scale problems. The performance of\nthe proposed method is evaluated on synthetic data and is shown to outperform a\nstate-of-the-art method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 21:30:55 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Fatemi", "Maryam", ""], ["Granstr\u00f6m", "Karl", ""], ["Svensson", "Lennart", ""], ["Ruiz", "Francisco J. R.", ""], ["Hammarstrand", "Lars", ""]]}, {"id": "1811.03166", "submitter": "Amir-Hossein Karimi", "authors": "Amir-Hossein Karimi, Alexander Wong, Ali Ghodsi", "title": "SRP: Efficient class-aware embedding learning for large-scale data via\n  supervised random projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised dimensionality reduction strategies have been of great interest.\nHowever, current supervised dimensionality reduction approaches are difficult\nto scale for situations characterized by large datasets given the high\ncomputational complexities associated with such methods. While stochastic\napproximation strategies have been explored for unsupervised dimensionality\nreduction to tackle this challenge, such approaches are not well-suited for\naccelerating computational speed for supervised dimensionality reduction.\nMotivated to tackle this challenge, in this study we explore a novel direction\nof directly learning optimal class-aware embeddings in a supervised manner via\nthe notion of supervised random projections (SRP). The key idea behind SRP is\nthat, rather than performing spectral decomposition (or approximations thereof)\nwhich are computationally prohibitive for large-scale data, we instead perform\na direct decomposition by leveraging kernel approximation theory and the\nsymmetry of the Hilbert-Schmidt Independence Criterion (HSIC) measure of\ndependence between the embedded data and the labels. Experimental results on\nfive different synthetic and real-world datasets demonstrate that the proposed\nSRP strategy for class-aware embedding learning can be very promising in\nproducing embeddings that are highly competitive with existing supervised\ndimensionality reduction methods (e.g., SPCA and KSPCA) while achieving 1-2\norders of magnitude better computational performance. As such, such an\nefficient approach to learning embeddings for dimensionality reduction can be a\npowerful tool for large-scale data analysis and visualization.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 22:09:23 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Karimi", "Amir-Hossein", ""], ["Wong", "Alexander", ""], ["Ghodsi", "Ali", ""]]}, {"id": "1811.03169", "submitter": "Stephane Fotso", "authors": "Stephane Fotso, Philip Spanoudes, Benjamin C. Ponedel, Brian Reynoso,\n  Janet Ko", "title": "Attention Fusion Networks: Combining Behavior and E-mail Content to\n  Improve Customer Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer support is a central objective at Square as it helps us build and\nmaintain great relationships with our sellers. In order to provide the best\nexperience, we strive to deliver the most accurate and quasi-instantaneous\nresponses to questions regarding our products.\n  In this work, we introduce the Attention Fusion Network model which combines\nsignals extracted from seller interactions on the Square product ecosystem,\nalong with submitted email questions, to predict the most relevant solution to\na seller's inquiry. We show that the innovative combination of two very\ndifferent data sources that are rarely used together, using state-of-the-art\ndeep learning systems outperforms, candidate models that are trained only on a\nsingle source.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 22:14:32 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 22:32:10 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Fotso", "Stephane", ""], ["Spanoudes", "Philip", ""], ["Ponedel", "Benjamin C.", ""], ["Reynoso", "Brian", ""], ["Ko", "Janet", ""]]}, {"id": "1811.03179", "submitter": "Tengyuan Liang", "authors": "Tengyuan Liang", "title": "How Well Generative Adversarial Networks Learn Distributions", "comments": "36 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the rates of convergence for learning distributions\nimplicitly with the adversarial framework and Generative Adversarial Networks\n(GAN), which subsume Wasserstein, Sobolev, MMD GAN, and Generalized/Simulated\nMethod of Moments (GMM/SMM) as special cases. We study a wide range of\nparametric and nonparametric target distributions, under a host of objective\nevaluation metrics. We investigate how to obtain a good statistical guarantee\nfor GANs through the lens of regularization. On the nonparametric end, we\nderive the optimal minimax rates for distribution estimation under the\nadversarial framework. On the parametric end, we establish a theory for general\nneural network classes (including deep leaky ReLU networks), that characterizes\nthe interplay on the choice of generator and discriminator pair. We discover\nand isolate a new notion of regularization, called the\ngenerator-discriminator-pair regularization, that sheds light on the advantage\nof GANs compared to classical parametric and nonparametric approaches for\nexplicit distribution estimation. We develop novel oracle inequalities as the\nmain technical tools for analyzing GANs, which is of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 23:14:45 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 15:08:06 GMT"}, {"version": "v3", "created": "Mon, 24 Aug 2020 14:57:20 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Liang", "Tengyuan", ""]]}, {"id": "1811.03188", "submitter": "Vahan Huroyan", "authors": "Vahan Huroyan, Gilad Lerman, and Hau-Tieng Wu", "title": "Solving Jigsaw Puzzles By the Graph Connection Laplacian", "comments": null, "journal-ref": "SIAM J. Imaging Sci. 13(4) (2020) 1717-1753", "doi": "10.1137/19M1290760", "report-no": null, "categories": "cs.CV cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel mathematical framework to address the problem of\nautomatically solving large jigsaw puzzles. This problem assumes a large image,\nwhich is cut into equal square pieces that are arbitrarily rotated and\nshuffled, and asks to recover the original image given the transformed pieces.\nThe main contribution of this work is a method for recovering the rotations of\nthe pieces when both shuffles and rotations are unknown. A major challenge of\nthis procedure is estimating the graph connection Laplacian without the\nknowledge of shuffles. A careful combination of our proposed method for\nestimating rotations with any existing method for estimating shuffles results\nin a practical solution for the jigsaw puzzle problem. Our theory guarantees,\nin a clean setting, that our basic idea of recovering rotations is robust to\nsome corruption of the connection graph. Numerical experiments demonstrate the\ncompetitive accuracy of this solution, its robustness to corruption and, its\ncomputational advantage for large puzzles.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 23:45:03 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 21:29:46 GMT"}, {"version": "v3", "created": "Fri, 8 Nov 2019 04:50:51 GMT"}, {"version": "v4", "created": "Mon, 27 Apr 2020 01:50:50 GMT"}, {"version": "v5", "created": "Sun, 1 Nov 2020 12:04:30 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Huroyan", "Vahan", ""], ["Lerman", "Gilad", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "1811.03194", "submitter": "Florian Tram\\`er", "authors": "Florian Tram\\`er, Pascal Dupr\\'e, Gili Rusak, Giancarlo Pellegrino,\n  Dan Boneh", "title": "AdVersarial: Perceptual Ad Blocking meets Adversarial Machine Learning", "comments": "17 pages, 14 figures", "journal-ref": "In 2019 ACM SIGSAC Conference on Computer and Communications\n  Security (CCS '19)", "doi": "10.1145/3319535.3354222", "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptual ad-blocking is a novel approach that detects online advertisements\nbased on their visual content. Compared to traditional filter lists, the use of\nperceptual signals is believed to be less prone to an arms race with web\npublishers and ad networks. We demonstrate that this may not be the case. We\ndescribe attacks on multiple perceptual ad-blocking techniques, and unveil a\nnew arms race that likely disfavors ad-blockers. Unexpectedly, perceptual\nad-blocking can also introduce new vulnerabilities that let an attacker bypass\nweb security boundaries and mount DDoS attacks.\n  We first analyze the design space of perceptual ad-blockers and present a\nunified architecture that incorporates prior academic and commercial work. We\nthen explore a variety of attacks on the ad-blocker's detection pipeline, that\nenable publishers or ad networks to evade or detect ad-blocking, and at times\neven abuse its high privilege level to bypass web security boundaries.\n  On one hand, we show that perceptual ad-blocking must visually classify\nrendered web content to escape an arms race centered on obfuscation of page\nmarkup. On the other, we present a concrete set of attacks on visual\nad-blockers by constructing adversarial examples in a real web page context.\nFor seven ad-detectors, we create perturbed ads, ad-disclosure logos, and\nnative web content that misleads perceptual ad-blocking with 100% success\nrates. In one of our attacks, we demonstrate how a malicious user can upload\nadversarial content, such as a perturbed image in a Facebook post, that fools\nthe ad-blocker into removing another users' non-ad content.\n  Moving beyond the Web and visual domain, we also build adversarial examples\nfor AdblockRadio, an open source radio client that uses machine learning to\ndetects ads in raw audio streams.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 00:20:12 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 09:02:47 GMT"}, {"version": "v3", "created": "Mon, 26 Aug 2019 10:27:39 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Tram\u00e8r", "Florian", ""], ["Dupr\u00e9", "Pascal", ""], ["Rusak", "Gili", ""], ["Pellegrino", "Giancarlo", ""], ["Boneh", "Dan", ""]]}, {"id": "1811.03195", "submitter": "Ilya Razenshteyn", "authors": "Konstantin Makarychev, Yury Makarychev, Ilya Razenshteyn", "title": "Performance of Johnson-Lindenstrauss Transform for k-Means and k-Medians\n  Clustering", "comments": "31 pages, an extended abstract appeared in the proceedings of STOC\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an instance of Euclidean $k$-means or $k$-medians clustering. We\nshow that the cost of the optimal solution is preserved up to a factor of\n$(1+\\varepsilon)$ under a projection onto a random $O(\\log(k / \\varepsilon) /\n\\varepsilon^2)$-dimensional subspace. Further, the cost of every clustering is\npreserved within $(1+\\varepsilon)$. More generally, our result applies to any\ndimension reduction map satisfying a mild sub-Gaussian-tail condition. Our\nbound on the dimension is nearly optimal. Additionally, our result applies to\nEuclidean $k$-clustering with the distances raised to the $p$-th power for any\nconstant $p$.\n  For $k$-means, our result resolves an open problem posed by Cohen, Elder,\nMusco, Musco, and Persu (STOC 2015); for $k$-medians, it answers a question\nraised by Kannan.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 00:24:23 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 23:48:43 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Makarychev", "Konstantin", ""], ["Makarychev", "Yury", ""], ["Razenshteyn", "Ilya", ""]]}, {"id": "1811.03205", "submitter": "Kiran Koshy Thekumparampil", "authors": "Kiran Koshy Thekumparampil, Ashish Khetan, Zinan Lin, Sewoong Oh", "title": "Robustness of Conditional GANs to Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning conditional generators from noisy labeled\nsamples, where the labels are corrupted by random noise. A standard training of\nconditional GANs will not only produce samples with wrong labels, but also\ngenerate poor quality samples. We consider two scenarios, depending on whether\nthe noise model is known or not. When the distribution of the noise is known,\nwe introduce a novel architecture which we call Robust Conditional GAN (RCGAN).\nThe main idea is to corrupt the label of the generated sample before feeding to\nthe adversarial discriminator, forcing the generator to produce samples with\nclean labels. This approach of passing through a matching noisy channel is\njustified by corresponding multiplicative approximation bounds between the loss\nof the RCGAN and the distance between the clean real distribution and the\ngenerator distribution. This shows that the proposed approach is robust, when\nused with a carefully chosen discriminator architecture, known as projection\ndiscriminator. When the distribution of the noise is not known, we provide an\nextension of our architecture, which we call RCGAN-U, that learns the noise\nmodel simultaneously while training the generator. We show experimentally on\nMNIST and CIFAR-10 datasets that both the approaches consistently improve upon\nbaseline approaches, and RCGAN-U closely matches the performance of RCGAN.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 01:07:17 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Thekumparampil", "Kiran Koshy", ""], ["Khetan", "Ashish", ""], ["Lin", "Zinan", ""], ["Oh", "Sewoong", ""]]}, {"id": "1811.03218", "submitter": "Michael Jacobs", "authors": "Michael A. Jacobs, Christopher Umbricht, Vishwa Parekh, Riham El\n  Khouli, Leslie Cope, Katarzyna J. Macura, Susan Harvey, Antonio C. Wolff", "title": "Advanced machine learning informatics modeling using clinical and\n  radiological imaging metrics for characterizing breast tumor characteristics\n  with the OncotypeDX gene array", "comments": "32 pages, 6 figures, Abstract number SSQ01-04:Radiological Society of\n  North America 2015 Scientific Assembly and Annual Meeting,Chicago IL", "journal-ref": null, "doi": null, "report-no": "SSQ01-04", "categories": "physics.med-ph cs.AI cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose-Optimal use of established and imaging methods, such as\nmultiparametric magnetic resonance imaging(mpMRI) can simultaneously identify\nkey functional parameters and provide unique imaging phenotypes of breast\ncancer. Therefore, we have developed and implemented a new machine-learning\ninformatic system that integrates clinical variables, derived from imaging and\nclinical health records, to compare with the 21-gene array assay, OncotypeDX.\nMaterials and methods-We tested our informatics modeling in a subset of\npatients (n=81) who had ER+ disease and underwent OncotypeDX gene expression\nand breast mpMRI testing. The machine-learning informatic method is termed\nIntegrated Radiomic Informatic System-IRIS was applied to the mpMRI, clinical\nand pathologic descriptors, as well as a gene array analysis. The IRIS method\nusing an advanced graph theoretic model and quantitative metrics. Summary\nstatistics (mean and standard deviations) for the quantitative imaging\nparameters were obtained. Sensitivity and specificity and Area Under the Curve\nwere calculated for the classification of the patients. Results-The OncotypeDX\nclassification by IRIS model had sensitivity of 95% and specificity of 89% with\nAUC of 0.92. The breast lesion size was larger for the high-risk groups and\nlower for both low risk and intermediate risk groups. There were significant\ndifferences in PK-DCE and ADC map values in each group. The ADC map values for\nhigh- and intermediate-risk groups were significantly lower than the low-risk\ngroup. Conclusion-These initial studies provide deeper understandings of\nimaging features and molecular gene array OncotypeDX score. This insight\nprovides the foundation to relate these imaging features to the assessment of\ntreatment response for improved personalized medicine.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 01:53:22 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Jacobs", "Michael A.", ""], ["Umbricht", "Christopher", ""], ["Parekh", "Vishwa", ""], ["Khouli", "Riham El", ""], ["Cope", "Leslie", ""], ["Macura", "Katarzyna J.", ""], ["Harvey", "Susan", ""], ["Wolff", "Antonio C.", ""]]}, {"id": "1811.03233", "submitter": "Byeongho Heo", "authors": "Byeongho Heo, Minsik Lee, Sangdoo Yun, Jin Young Choi", "title": "Knowledge Transfer via Distillation of Activation Boundaries Formed by\n  Hidden Neurons", "comments": "Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An activation boundary for a neuron refers to a separating hyperplane that\ndetermines whether the neuron is activated or deactivated. It has been long\nconsidered in neural networks that the activations of neurons, rather than\ntheir exact output values, play the most important role in forming\nclassification friendly partitions of the hidden feature space. However, as far\nas we know, this aspect of neural networks has not been considered in the\nliterature of knowledge transfer. In this paper, we propose a knowledge\ntransfer method via distillation of activation boundaries formed by hidden\nneurons. For the distillation, we propose an activation transfer loss that has\nthe minimum value when the boundaries generated by the student coincide with\nthose by the teacher. Since the activation transfer loss is not differentiable,\nwe design a piecewise differentiable loss approximating the activation transfer\nloss. By the proposed method, the student learns a separating boundary between\nactivation region and deactivation region formed by each neuron in the teacher.\nThrough the experiments in various aspects of knowledge transfer, it is\nverified that the proposed method outperforms the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 02:47:56 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 15:29:53 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Heo", "Byeongho", ""], ["Lee", "Minsik", ""], ["Yun", "Sangdoo", ""], ["Choi", "Jin Young", ""]]}, {"id": "1811.03250", "submitter": "Chi Wang", "authors": "Silu Huang, Chi Wang, Bolin Ding, Surajit Chaudhuri", "title": "ABC: Efficient Selection of Machine Learning Configuration on Large\n  Dataset", "comments": "Full version of an AAAI 2019 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A machine learning configuration refers to a combination of preprocessor,\nlearner, and hyperparameters. Given a set of configurations and a large dataset\nrandomly split into training and testing set, we study how to efficiently\nselect the best configuration with approximately the highest testing accuracy\nwhen trained from the training set. To guarantee small accuracy loss, we\ndevelop a solution using confidence interval (CI)-based progressive sampling\nand pruning strategy. Compared to using full data to find the exact best\nconfiguration, our solution achieves more than two orders of magnitude speedup,\nwhile the returned top configuration has identical or close test accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 03:44:11 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 08:11:52 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Huang", "Silu", ""], ["Wang", "Chi", ""], ["Ding", "Bolin", ""], ["Chaudhuri", "Surajit", ""]]}, {"id": "1811.03259", "submitter": "Shengjia Zhao", "authors": "Shengjia Zhao and Hongyu Ren and Arianna Yuan and Jiaming Song and\n  Noah Goodman and Stefano Ermon", "title": "Bias and Generalization in Deep Generative Models: An Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high dimensional settings, density estimation algorithms rely crucially on\ntheir inductive bias. Despite recent empirical success, the inductive bias of\ndeep generative models is not well understood. In this paper we propose a\nframework to systematically investigate bias and generalization in deep\ngenerative models of images. Inspired by experimental methods from cognitive\npsychology, we probe each learning algorithm with carefully designed training\ndatasets to characterize when and how existing models generate novel attributes\nand their combinations. We identify similarities to human psychology and verify\nthat these patterns are consistent across commonly used models and\narchitectures.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 04:15:28 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Zhao", "Shengjia", ""], ["Ren", "Hongyu", ""], ["Yuan", "Arianna", ""], ["Song", "Jiaming", ""], ["Goodman", "Noah", ""], ["Ermon", "Stefano", ""]]}, {"id": "1811.03270", "submitter": "Dacheng Tao", "authors": "Jingwei Zhang, Tongliang Liu, Dacheng Tao", "title": "An Optimal Transport View on Generalization", "comments": "27 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We derive upper bounds on the generalization error of learning algorithms\nbased on their \\emph{algorithmic transport cost}: the expected Wasserstein\ndistance between the output hypothesis and the output hypothesis conditioned on\nan input example. The bounds provide a novel approach to study the\ngeneralization of learning algorithms from an optimal transport view and impose\nless constraints on the loss function, such as sub-gaussian or bounded. We\nfurther provide several upper bounds on the algorithmic transport cost in terms\nof total variation distance, relative entropy (or KL-divergence), and VC\ndimension, thus further bridging optimal transport theory and information\ntheory with statistical learning theory. Moreover, we also study different\nconditions for loss functions under which the generalization error of a\nlearning algorithm can be upper bounded by different probability metrics\nbetween distributions relating to the output hypothesis and/or the input data.\nFinally, under our established framework, we analyze the generalization in deep\nlearning and conclude that the generalization error in deep neural networks\n(DNNs) decreases exponentially to zero as the number of layers increases. Our\nanalyses of generalization error in deep learning mainly exploit the\nhierarchical structure in DNNs and the contraction property of $f$-divergence,\nwhich may be of independent interest in analyzing other learning models with\nhierarchical structure.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 05:02:11 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Zhang", "Jingwei", ""], ["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1811.03305", "submitter": "Mahesh Subedar", "authors": "Ranganath Krishnan, Mahesh Subedar, Omesh Tickoo", "title": "BAR: Bayesian Activity Recognition using variational inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty estimation in deep neural networks is essential for designing\nreliable and robust AI systems. Applications such as video surveillance for\nidentifying suspicious activities are designed with deep neural networks\n(DNNs), but DNNs do not provide uncertainty estimates. Capturing reliable\nuncertainty estimates in safety and security critical applications will help to\nestablish trust in the AI system. Our contribution is to apply Bayesian deep\nlearning framework to visual activity recognition application and quantify\nmodel uncertainty along with principled confidence. We utilize the stochastic\nvariational inference technique while training the Bayesian DNNs to infer the\napproximate posterior distribution around model parameters and perform Monte\nCarlo sampling on the posterior of model parameters to obtain the predictive\ndistribution. We show that the Bayesian inference applied to DNNs provide\nreliable confidence measures for visual activity recognition task as compared\nto conventional DNNs. We also show that our method improves the visual activity\nrecognition precision-recall AUC by 6.2% compared to non-Bayesian baseline. We\nevaluate our models on Moments-In-Time (MiT) activity recognition dataset by\nselecting a subset of in- and out-of-distribution video samples.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 08:04:09 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 08:08:34 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Krishnan", "Ranganath", ""], ["Subedar", "Mahesh", ""], ["Tickoo", "Omesh", ""]]}, {"id": "1811.03307", "submitter": "Abhik Singla", "authors": "Abhik Singla, Sindhu Padakandla and Shalabh Bhatnagar", "title": "Memory-based Deep Reinforcement Learning for Obstacle Avoidance in UAV\n  with Limited Environment Knowledge", "comments": "Submitted to IEEE Transactions on Cybernetics. Supplementary Video:\n  https://www.youtube.com/watch?v=Lqh_B9U3Gv0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our method for enabling a UAV quadrotor, equipped with a\nmonocular camera, to autonomously avoid collisions with obstacles in\nunstructured and unknown indoor environments. When compared to obstacle\navoidance in ground vehicular robots, UAV navigation brings in additional\nchallenges because the UAV motion is no more constrained to a well-defined\nindoor ground or street environment. Horizontal structures in indoor and\noutdoor environments like decorative items, furnishings, ceiling fans,\nsign-boards, tree branches etc., also become relevant obstacles unlike those\nfor ground vehicular robots. Thus, methods of obstacle avoidance developed for\nground robots are clearly inadequate for UAV navigation. Current control\nmethods using monocular images for UAV obstacle avoidance are heavily dependent\non environment information. These controllers do not fully retain and utilize\nthe extensively available information about the ambient environment for\ndecision making. We propose a deep reinforcement learning based method for UAV\nobstacle avoidance (OA) and autonomous exploration which is capable of doing\nexactly the same. The crucial idea in our method is the concept of partial\nobservability and how UAVs can retain relevant information about the\nenvironment structure to make better future navigation decisions. Our OA\ntechnique uses recurrent neural networks with temporal attention and provides\nbetter results compared to prior works in terms of distance covered during\nnavigation without collisions. In addition, our technique has a high inference\nrate (a key factor in robotic applications) and is energy-efficient as it\nminimizes oscillatory motion of UAV and reduces power wastage.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 08:06:59 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Singla", "Abhik", ""], ["Padakandla", "Sindhu", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1811.03311", "submitter": "Eunwoo Song", "authors": "Eunwoo Song, Jin-Seob Kim, Kyungguen Byun, Hong-Goo Kang", "title": "Speaker-adaptive neural vocoders for parametric speech synthesis systems", "comments": "Accepted to the IEEE Workshop of MMSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes speaker-adaptive neural vocoders for parametric\ntext-to-speech (TTS) systems. Recently proposed WaveNet-based neural vocoding\nsystems successfully generate a time sequence of speech signal with an\nautoregressive framework. However, it remains a challenge to synthesize\nhigh-quality speech when the amount of a target speaker's training data is\ninsufficient. To generate more natural speech signals with the constraint of\nlimited training data, we propose a speaker adaptation task with an effective\nvariation of neural vocoding models. In the proposed method, a\nspeaker-independent training method is applied to capture universal attributes\nembedded in multiple speakers, and the trained model is then optimized to\nrepresent the specific characteristics of the target speaker. Experimental\nresults verify that the proposed TTS systems with speaker-adaptive neural\nvocoders outperform those with traditional source-filter model-based vocoders\nand those with WaveNet vocoders, trained either speaker-dependently or\nspeaker-independently. In particular, our TTS system achieves 3.80 and 3.77 MOS\nfor the Korean male and Korean female speakers, respectively, even though we\nuse only ten minutes' speech corpus for training the model.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 08:26:03 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 10:02:21 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 05:42:05 GMT"}, {"version": "v4", "created": "Fri, 28 Jun 2019 06:42:43 GMT"}, {"version": "v5", "created": "Sat, 1 Aug 2020 05:24:15 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Song", "Eunwoo", ""], ["Kim", "Jin-Seob", ""], ["Byun", "Kyungguen", ""], ["Kang", "Hong-Goo", ""]]}, {"id": "1811.03322", "submitter": "Daning Cheng", "authors": "Cheng Daning, Zhang Hanping, Xia Fen, Li Shigang, Zhang Yunquan", "title": "Using Known Information to Accelerate HyperParameters Optimization Based\n  on SMBO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automl is the key technology for machine learning problem. Current state of\nart hyperparameter optimization methods are based on traditional black-box\noptimization methods like SMBO (SMAC, TPE). The objective function of black-box\noptimization is non-smooth, or time-consuming to evaluate, or in some way\nnoisy. Recent years, many researchers offered the work about the properties of\nhyperparameters. However, traditional hyperparameter optimization methods do\nnot take those information into consideration. In this paper, we use gradient\ninformation and machine learning model analysis information to accelerate\ntraditional hyperparameter optimization methods SMBO. In our L2 norm\nexperiments, our method yielded state-of-the-art performance, and in many cases\noutperformed the previous best configuration approach.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 09:04:09 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 06:58:56 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Daning", "Cheng", ""], ["Hanping", "Zhang", ""], ["Fen", "Xia", ""], ["Shigang", "Li", ""], ["Yunquan", "Zhang", ""]]}, {"id": "1811.03356", "submitter": "Antonio Carta", "authors": "Davide Bacciu, Antonio Carta, Alessandro Sperduti", "title": "Linear Memory Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks can learn complex transduction problems that\nrequire maintaining and actively exploiting a memory of their inputs. Such\nmodels traditionally consider memory and input-output functionalities\nindissolubly entangled. We introduce a novel recurrent architecture based on\nthe conceptual separation between the functional input-output transformation\nand the memory mechanism, showing how they can be implemented through different\nneural components. By building on such conceptualization, we introduce the\nLinear Memory Network, a recurrent model comprising a feedforward neural\nnetwork, realizing the non-linear functional transformation, and a linear\nautoencoder for sequences, implementing the memory component. The resulting\narchitecture can be efficiently trained by building on closed-form solutions to\nlinear optimization problems. Further, by exploiting equivalence results\nbetween feedforward and recurrent neural networks we devise a pretraining\nschema for the proposed architecture. Experiments on polyphonic music datasets\nshow competitive results against gated recurrent networks and other state of\nthe art models.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 11:08:04 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Bacciu", "Davide", ""], ["Carta", "Antonio", ""], ["Sperduti", "Alessandro", ""]]}, {"id": "1811.03377", "submitter": "Pablo G. Camara", "authors": "Kiya W. Govek, Venkata S. Yamajala, Pablo G. Camara", "title": "Spectral Simplicial Theory for Feature Selection and Applications to\n  Genomics", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scale and complexity of modern data sets and the limitations associated\nwith testing large numbers of hypotheses underline the need for feature\nselection methods. Spectral techniques rank features according to their degree\nof consistency with an underlying metric structure, but their current\ngraph-based formulation restricts their applicability to point features. We\nextend spectral methods for feature selection to abstract simplicial complexes\nand present a general framework which can be applied to 2-point and\nhigher-order features. Combinatorial Laplacian scores take into account the\ntopology spanned by the data and reduce to the ordinary Laplacian score in the\ncase of point features. We demonstrate the utility of spectral simplicial\nmethods for feature selection with several examples of application to the\nanalysis of gene expression and multi-modal genomic data. Our results provide a\nunifying perspective on topological data analysis and manifold learning\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 12:27:49 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Govek", "Kiya W.", ""], ["Yamajala", "Venkata S.", ""], ["Camara", "Pablo G.", ""]]}, {"id": "1811.03378", "submitter": "Chigozie Nwankpa", "authors": "Chigozie Nwankpa, Winifred Ijomah, Anthony Gachagan, Stephen Marshall", "title": "Activation Functions: Comparison of trends in Practice and Research for\n  Deep Learning", "comments": "20 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been successfully used in diverse emerging domains\nto solve real world complex problems with may more deep learning(DL)\narchitectures, being developed to date. To achieve these state-of-the-art\nperformances, the DL architectures use activation functions (AFs), to perform\ndiverse computations between the hidden layers and the output layers of any\ngiven DL architecture. This paper presents a survey on the existing AFs used in\ndeep learning applications and highlights the recent trends in the use of the\nactivation functions for deep learning applications. The novelty of this paper\nis that it compiles majority of the AFs used in DL and outlines the current\ntrends in the applications and usage of these functions in practical deep\nlearning deployments against the state-of-the-art research results. This\ncompilation will aid in making effective decisions in the choice of the most\nsuitable and appropriate activation function for any given application, ready\nfor deployment. This paper is timely because most research papers on AF\nhighlights similar works and results while this paper will be the first, to\ncompile the trends in AF applications in practice against the research results\nfrom literature, found in deep learning research to date.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 12:28:43 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Nwankpa", "Chigozie", ""], ["Ijomah", "Winifred", ""], ["Gachagan", "Anthony", ""], ["Marshall", "Stephen", ""]]}, {"id": "1811.03388", "submitter": "Jill-J\\^enn Vie", "authors": "Jill-J\\^enn Vie and Hisashi Kashima", "title": "Knowledge Tracing Machines: Factorization Machines for Knowledge Tracing", "comments": "8 pages, 3 figures, 7 tables, to appear at the 33th AAAI Conference\n  on Artificial Intelligence (AAAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge tracing is a sequence prediction problem where the goal is to\npredict the outcomes of students over questions as they are interacting with a\nlearning platform. By tracking the evolution of the knowledge of some student,\none can optimize instruction. Existing methods are either based on temporal\nlatent variable models, or factor analysis with temporal features. We here show\nthat factorization machines (FMs), a model for regression or classification,\nencompasses several existing models in the educational literature as special\ncases, notably additive factor model, performance factor model, and\nmultidimensional item response theory. We show, using several real datasets of\ntens of thousands of users and items, that FMs can estimate student knowledge\naccurately and fast even when student data is sparsely observed, and handle\nside information such as multiple knowledge components and number of attempts\nat item or skill level. Our approach allows to fit student models of higher\ndimension than existing models, and provides a testbed to try new combinations\nof features in order to improve existing models.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 13:02:09 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 05:41:18 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Vie", "Jill-J\u00eann", ""], ["Kashima", "Hisashi", ""]]}, {"id": "1811.03390", "submitter": "Atilim Gunes Baydin", "authors": "Frank Soboczenski, Michael D. Himes, Molly D. O'Beirne, Simone Zorzan,\n  Atilim Gunes Baydin, Adam D. Cobb, Yarin Gal, Daniel Angerhausen, Massimo\n  Mascaro, Giada N. Arney, Shawn D. Domagal-Goldman", "title": "Bayesian Deep Learning for Exoplanet Atmospheric Retrieval", "comments": "Third workshop on Bayesian Deep Learning (NeurIPS 2018), Montreal,\n  Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.EP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, the study of extrasolar planets has evolved rapidly\nfrom plain detection and identification to comprehensive categorization and\ncharacterization of exoplanet systems and their atmospheres. Atmospheric\nretrieval, the inverse modeling technique used to determine an exoplanetary\natmosphere's temperature structure and composition from an observed spectrum,\nis both time-consuming and compute-intensive, requiring complex algorithms that\ncompare thousands to millions of atmospheric models to the observational data\nto find the most probable values and associated uncertainties for each model\nparameter. For rocky, terrestrial planets, the retrieved atmospheric\ncomposition can give insight into the surface fluxes of gaseous species\nnecessary to maintain the stability of that atmosphere, which may in turn\nprovide insight into the geological and/or biological processes active on the\nplanet. These atmospheres contain many molecules, some of them biosignatures,\nspectral fingerprints indicative of biological activity, which will become\nobservable with the next generation of telescopes. Runtimes of traditional\nretrieval models scale with the number of model parameters, so as more\nmolecular species are considered, runtimes can become prohibitively long.\nRecent advances in machine learning (ML) and computer vision offer new ways to\nreduce the time to perform a retrieval by orders of magnitude, given a\nsufficient data set to train with. Here we present an ML-based retrieval\nframework called Intelligent exoplaNet Atmospheric RetrievAl (INARA) that\nconsists of a Bayesian deep learning model for retrieval and a data set of\n3,000,000 synthetic rocky exoplanetary spectra generated using the NASA\nPlanetary Spectrum Generator. Our work represents the first ML retrieval model\nfor rocky, terrestrial exoplanets and the first synthetic data set of\nterrestrial spectra generated at this scale.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 13:03:08 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 13:01:22 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Soboczenski", "Frank", ""], ["Himes", "Michael D.", ""], ["O'Beirne", "Molly D.", ""], ["Zorzan", "Simone", ""], ["Baydin", "Atilim Gunes", ""], ["Cobb", "Adam D.", ""], ["Gal", "Yarin", ""], ["Angerhausen", "Daniel", ""], ["Mascaro", "Massimo", ""], ["Arney", "Giada N.", ""], ["Domagal-Goldman", "Shawn D.", ""]]}, {"id": "1811.03392", "submitter": "Ivan Olier", "authors": "Ivan Olier and Oghenejokpeme I. Orhobor and Joaquin Vanschoren and\n  Ross D. King", "title": "Transformative Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key to success in machine learning (ML) is the use of effective data\nrepresentations. Traditionally, data representations were hand-crafted.\nRecently it has been demonstrated that, given sufficient data, deep neural\nnetworks can learn effective implicit representations from simple input\nrepresentations. However, for most scientific problems, the use of deep\nlearning is not appropriate as the amount of available data is limited, and/or\nthe output models must be explainable. Nevertheless, many scientific problems\ndo have significant amounts of data available on related tasks, which makes\nthem amenable to multi-task learning, i.e. learning many related problems\nsimultaneously. Here we propose a novel and general representation learning\napproach for multi-task learning that works successfully with small amounts of\ndata. The fundamental new idea is to transform an input intrinsic data\nrepresentation (i.e., handcrafted features), to an extrinsic representation\nbased on what a pre-trained set of models predict about the examples. This\ntransformation has the dual advantages of producing significantly more accurate\npredictions, and providing explainable models. To demonstrate the utility of\nthis transformative learning approach, we have applied it to three real-world\nscientific problems: drug-design (quantitative structure activity relationship\nlearning), predicting human gene expression (across different tissue types and\ndrug treatments), and meta-learning for machine learning (predicting which\nmachine learning methods work best for a given problem). In all three problems,\ntransformative machine learning significantly outperforms the best intrinsic\nrepresentation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 13:09:05 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Olier", "Ivan", ""], ["Orhobor", "Oghenejokpeme I.", ""], ["Vanschoren", "Joaquin", ""], ["King", "Ross D.", ""]]}, {"id": "1811.03402", "submitter": "Steven Whang", "authors": "Yuji Roh, Geon Heo, Steven Euijong Whang", "title": "A Survey on Data Collection for Machine Learning: a Big Data -- AI\n  Integration Perspective", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data collection is a major bottleneck in machine learning and an active\nresearch topic in multiple communities. There are largely two reasons data\ncollection has recently become a critical issue. First, as machine learning is\nbecoming more widely-used, we are seeing new applications that do not\nnecessarily have enough labeled data. Second, unlike traditional machine\nlearning, deep learning techniques automatically generate features, which saves\nfeature engineering costs, but in return may require larger amounts of labeled\ndata. Interestingly, recent research in data collection comes not only from the\nmachine learning, natural language, and computer vision communities, but also\nfrom the data management community due to the importance of handling large\namounts of data. In this survey, we perform a comprehensive study of data\ncollection from a data management point of view. Data collection largely\nconsists of data acquisition, data labeling, and improvement of existing data\nor models. We provide a research landscape of these operations, provide\nguidelines on which technique to use when, and identify interesting research\nchallenges. The integration of machine learning and data management for data\ncollection is part of a larger trend of Big data and Artificial Intelligence\n(AI) integration and opens many opportunities for new research.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 13:37:46 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 13:48:36 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Roh", "Yuji", ""], ["Heo", "Geon", ""], ["Whang", "Steven Euijong", ""]]}, {"id": "1811.03403", "submitter": "Jarryd Son", "authors": "Jarryd Son, Amit Mishra", "title": "ExGate: Externally Controlled Gating for Feature-based Attention in\n  Artificial Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptual capabilities of artificial systems have come a long way since the\nadvent of deep learning. These methods have proven to be effective, however\nthey are not as efficient as their biological counterparts. Visual attention is\na set of mechanisms that are employed in biological visual systems to ease\ncomputational load by only processing pertinent parts of the stimuli. This\npaper addresses the implementation of top-down, feature-based attention in an\nartificial neural network by use of externally controlled neuron gating. Our\nresults showed a 5% increase in classification accuracy on the CIFAR-10 dataset\nversus a non-gated version, while adding very few parameters. Our gated model\nalso produces more reasonable errors in predictions by drastically reducing\nprediction of classes that belong to a different category to the true class.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 13:39:49 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Son", "Jarryd", ""], ["Mishra", "Amit", ""]]}, {"id": "1811.03407", "submitter": "Marco Cox", "authors": "Marco Cox, Thijs van de Laar, Bert de Vries", "title": "A Factor Graph Approach to Automated Design of Bayesian Signal\n  Processing Algorithms", "comments": "Accepted for publication in the International Journal of Approximate\n  Reasoning", "journal-ref": null, "doi": "10.1016/j.ijar.2018.11.002", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The benefits of automating design cycles for Bayesian inference-based\nalgorithms are becoming increasingly recognized by the machine learning\ncommunity. As a result, interest in probabilistic programming frameworks has\nmuch increased over the past few years. This paper explores a specific\nprobabilistic programming paradigm, namely message passing in Forney-style\nfactor graphs (FFGs), in the context of automated design of efficient Bayesian\nsignal processing algorithms. To this end, we developed \"ForneyLab\"\n(https://github.com/biaslab/ForneyLab.jl) as a Julia toolbox for message\npassing-based inference in FFGs. We show by example how ForneyLab enables\nautomatic derivation of Bayesian signal processing algorithms, including\nalgorithms for parameter estimation and model comparison. Crucially, due to the\nmodular makeup of the FFG framework, both the model specification and inference\nmethods are readily extensible in ForneyLab. In order to test this framework,\nwe compared variational message passing as implemented by ForneyLab with\nautomatic differentiation variational inference (ADVI) and Monte Carlo methods\nas implemented by state-of-the-art tools \"Edward\" and \"Stan\". In terms of\nperformance, extensibility and stability issues, ForneyLab appears to enjoy an\nedge relative to its competitors for automated inference in state-space models.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 13:53:46 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Cox", "Marco", ""], ["van de Laar", "Thijs", ""], ["de Vries", "Bert", ""]]}, {"id": "1811.03422", "submitter": "Wenbo Guo", "authors": "Wenbo Guo and Sui Huang and Yunzhe Tao and Xinyu Xing and Lin Lin", "title": "Explaining Deep Learning Models - A Bayesian Non-parametric Approach", "comments": "In Proceedings of the 32nd Conference on Neural Information\n  Processing Systems. arXiv admin note: text overlap with arXiv:1705.08564", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and interpreting how machine learning (ML) models make\ndecisions have been a big challenge. While recent research has proposed various\ntechnical approaches to provide some clues as to how an ML model makes\nindividual predictions, they cannot provide users with an ability to inspect a\nmodel as a complete entity. In this work, we propose a novel technical approach\nthat augments a Bayesian non-parametric regression mixture model with multiple\nelastic nets. Using the enhanced mixture model, we can extract generalizable\ninsights for a target model through a global approximation. To demonstrate the\nutility of our approach, we evaluate it on different ML models in the context\nof image recognition. The empirical results indicate that our proposed approach\nnot only outperforms the state-of-the-art techniques in explaining individual\ndecisions but also provides users with an ability to discover the\nvulnerabilities of the target ML models.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 16:26:32 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Guo", "Wenbo", ""], ["Huang", "Sui", ""], ["Tao", "Yunzhe", ""], ["Xing", "Xinyu", ""], ["Lin", "Lin", ""]]}, {"id": "1811.03423", "submitter": "Kory W Mathewson", "authors": "Markus Eger and Kory W. Mathewson", "title": "dAIrector: Automatic Story Beat Generation through Knowledge Synthesis", "comments": "10 pages with references, 1 figure. Accepted at Joint Workshop on\n  Intelligent Narrative Technologies and Intelligent Cinematography and Editing\n  at AAAI Conference on Artificial Intelligence and Interactive Digital\n  Entertainment (AIIDE'18). Edmonton, Alberta, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  dAIrector is an automated director which collaborates with humans\nstorytellers for live improvisational performances and writing assistance.\ndAIrector can be used to create short narrative arcs through contextual plot\ngeneration. In this work, we present the system architecture, a quantitative\nevaluation of design choices, and a case-study usage of the system which\nprovides qualitative feedback from a professional improvisational performer. We\npresent relevant metrics for the understudied domain of human-machine creative\ngeneration, specifically long-form narrative creation. We include, alongside\npublication, open-source code so that others may test, evaluate, and run the\ndAIrector.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 15:41:22 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Eger", "Markus", ""], ["Mathewson", "Kory W.", ""]]}, {"id": "1811.03431", "submitter": "I\\~nigo Urteaga", "authors": "I\\~nigo Urteaga, Mollie McKillop, Sharon Lipsky-Gorman and No\\'emie\n  Elhadad", "title": "Phenotyping Endometriosis through Mixed Membership Models of\n  Self-Tracking Data", "comments": "As presented in Machine Learning for Healthcare 2018,\n  https://www.mlforhc.org/2018-conference/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of self-tracking data and unsupervised\nmixed-membership models to phenotype endometriosis. Endometriosis is a\nsystemic, chronic condition of women in reproductive age and, at the same time,\na highly enigmatic condition with no known biomarkers to monitor its\nprogression and no established staging. We leverage data collected through a\nself-tracking app in an observational research study of over 2,800 women with\nendometriosis tracking their condition over a year and a half (456,900\nobservations overall). We extend a classical mixed-membership model to\naccommodate the idiosyncrasies of the data at hand (i.e., the multimodality of\nthe tracked variables). Our experiments show that our approach identifies\npotential subtypes that are robust in terms of biases of self-tracked data\n(e.g., wide variations in tracking frequency amongst participants), as well as\nto variations in hyperparameters of the model. Jointly modeling a wide range of\nobservations about participants (symptoms, quality of life, treatments) yields\nclinically meaningful subtypes that both validate what is already known about\nendometriosis and suggest new findings.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 17:54:18 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Urteaga", "I\u00f1igo", ""], ["McKillop", "Mollie", ""], ["Lipsky-Gorman", "Sharon", ""], ["Elhadad", "No\u00e9mie", ""]]}, {"id": "1811.03433", "submitter": "Qiao Zheng", "authors": "Qiao Zheng, Herv\\'e Delingette, Nicholas Ayache", "title": "Explainable cardiac pathology classification on cine MRI with motion\n  characterization by semi-supervised learning of apparent flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to classify cardiac pathology based on a novel approach\nto extract image derived features to characterize the shape and motion of the\nheart. An original semi-supervised learning procedure, which makes efficient\nuse of a large amount of non-segmented images and a small amount of images\nsegmented manually by experts, is developed to generate pixel-wise apparent\nflow between two time points of a 2D+t cine MRI image sequence. Combining the\napparent flow maps and cardiac segmentation masks, we obtain a local apparent\nflow corresponding to the 2D motion of myocardium and ventricular cavities.\nThis leads to the generation of time series of the radius and thickness of\nmyocardial segments to represent cardiac motion. These time series of motion\nfeatures are reliable and explainable characteristics of pathological cardiac\nmotion. Furthermore, they are combined with shape-related features to classify\ncardiac pathologies. Using only nine feature values as input, we propose an\nexplainable, simple and flexible model for pathology classification. On ACDC\ntraining set and testing set, the model achieves 95% and 94% respectively as\nclassification accuracy. Its performance is hence comparable to that of the\nstate-of-the-art. Comparison with various other models is performed to outline\nsome advantages of our model.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 14:22:05 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 20:52:47 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Zheng", "Qiao", ""], ["Delingette", "Herv\u00e9", ""], ["Ayache", "Nicholas", ""]]}, {"id": "1811.03435", "submitter": "Ben Green", "authors": "Ben Green", "title": "Data Science as Political Action: Grounding Data Science in a Politics\n  of Justice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In response to recent controversies, the field of data science has rushed to\nadopt codes of ethics. Such professional codes, however, are ill-equipped to\naddress broad matters of social justice. Instead of ethics codes, I argue, the\nfield must embrace politics. Data scientists must recognize themselves as\npolitical actors engaged in normative constructions of society and, as befits\npolitical work, evaluate their work according to its downstream material\nimpacts on people's lives. I justify this notion in two parts: first, by\narticulating why data scientists must recognize themselves as political actors,\nand second, by describing how the field can evolve toward a deliberative and\nrigorous grounding in a politics of social justice. Part 1 responds to three\narguments that are commonly invoked by data scientists when they are challenged\nto take political positions regarding their work. In confronting these\narguments, I will demonstrate why attempting to remain apolitical is itself a\npolitical stance--a fundamentally conservative one--and why the field's current\nattempts to promote \"social good\" dangerously rely on vague and unarticulated\npolitical assumptions. Part 2 proposes a framework for what a\npolitically-engaged data science could look like and how to achieve it,\nrecognizing the challenge of reforming the field in this manner. I\nconceptualize the process of incorporating politics into data science in four\nstages: becoming interested in directly addressing social issues, recognizing\nthe politics underlying these issues, redirecting existing methods toward new\napplications, and, finally, developing new practices and methods that orient\ndata science around a mission of social justice. The path ahead does not\nrequire data scientists to abandon their technical expertise, but it does\nentail expanding their notions of what problems to work on and how to engage\nwith society.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 03:11:09 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 21:22:35 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 22:48:01 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Green", "Ben", ""]]}, {"id": "1811.03436", "submitter": "Heeyoul Choi", "authors": "Hayoung Eom, Heeyoul Choi", "title": "Alpha-Integration Pooling for Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have achieved remarkable performance in\nmany applications, especially in image recognition tasks. As a crucial\ncomponent of CNNs, sub-sampling plays an important role for efficient training\nor invariance property, and max-pooling and arithmetic average-pooling are\ncommonly used sub-sampling methods. In addition to the two pooling methods,\nhowever, there could be many other pooling types, such as geometric average,\nharmonic average, and so on. Since it is not easy for algorithms to find the\nbest pooling method, usually the pooling types are assumed a priority, which\nmight not be optimal for different tasks. In line with the deep learning\nphilosophy, the type of pooling can be driven by data for a given task. In this\npaper, we propose {\\it $\\alpha$-integration pooling} ($\\alpha$I-pooling), which\nhas a trainable parameter $\\alpha$ to find the type of pooling.\n$\\alpha$I-pooling is a general pooling method including max-pooling and\narithmetic average-pooling as a special case, depending on the parameter\n$\\alpha$. Experiments show that $\\alpha$I-pooling outperforms other pooling\nmethods including max-pooling, in image recognition tasks. Also, it turns out\nthat each layer has different optimal pooling type.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 14:25:08 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 04:08:27 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 06:20:00 GMT"}, {"version": "v4", "created": "Sat, 14 Mar 2020 12:33:21 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Eom", "Hayoung", ""], ["Choi", "Heeyoul", ""]]}, {"id": "1811.03437", "submitter": "Mustafa Hajij", "authors": "Omar Elbagalati, Mustafa Hajij", "title": "Integrating Project Spatial Coordinates into Pavement Management\n  Prioritization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, pavement management software products and studies on optimizing the\nprioritization of pavement maintenance and rehabilitation (M&R) have been\nmainly focused on three parameters; the pre-treatment pavement condition, the\nrehabilitation cost, and the available budget. Yet, the role of the candidate\nprojects' spatial characteristics in the decision-making process has not been\ndeeply considered. Such a limitation, predominately, allows the recommended M&R\nprojects' schedule to involve simultaneously running but spatially scattered\nconstruction sites, which are very challenging to monitor and manage. This\nstudy introduces a novel approach to integrate pavement segments' spatial\ncoordinates into the M&R prioritization analysis. The introduced approach aims\nat combining the pavement segments with converged spatial coordinates to be\nrepaired in the same timeframe without compromising the allocated budget levels\nor the overall target Pavement Condition Index (PCI). Such a combination would\nresult in minimizing the routing of crews, materials and other equipment among\nthe construction sites and would provide better collaborations and\ncommunications between the pavement maintenance teams. Proposed herein is a\nnovel spatial clustering algorithm that automatically finds the projects within\na certain budget and spatial constrains. The developed algorithm was\nsuccessfully validated using 1,800 pavement maintenance projects from two\nreal-life examples of the City of Milton, GA and the City of Tyler, TX.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 18:38:28 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Elbagalati", "Omar", ""], ["Hajij", "Mustafa", ""]]}, {"id": "1811.03444", "submitter": "Heeyoul Choi", "authors": "Sangchul Hahn, Heeyoul Choi", "title": "Disentangling Latent Factors of Variational Auto-Encoder with Whitening", "comments": "ICANN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After deep generative models were successfully applied to image generation\ntasks, learning disentangled latent variables of data has become a crucial part\nof deep generative model research. Many models have been proposed to learn an\ninterpretable and factorized representation of latent variable by modifying\ntheir objective function or model architecture. To disentangle the latent\nvariable, some models show lower quality of reconstructed images and others\nincrease the model complexity which is hard to train. In this paper, we propose\na simple disentangling method based on a traditional whitening process. The\nproposed method is applied to the latent variables of variational auto-encoder\n(VAE), although it can be applied to any generative models with latent\nvariables. In experiment, we apply the proposed method to simple VAE models and\nexperiment results confirm that our method finds more interpretable factors\nfrom the latent space while keeping the reconstruction error the same as the\nconventional VAE's error.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 14:33:16 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 03:30:39 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Hahn", "Sangchul", ""], ["Choi", "Heeyoul", ""]]}, {"id": "1811.03450", "submitter": "Vincent Cheutet", "authors": "L. Chang, Yacine Ouzrout (DISP), Antoine Nongaillard (DISP), Abdelaziz\n  Bouras (DISP)", "title": "Optimized Hidden Markov Model based on Constrained Particle Swarm\n  Optimization", "comments": null, "journal-ref": "IEEE International Conference on Software, Knowledge Information,\n  Industrial Management and Applications SKIMA'12 International Conference, Sep\n  2012, Chengdu, China. 6 p", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of Bayesian analysis tools, Hidden Markov Model (HMM) has been used to\nin extensive applications. Most HMMs are solved by Baum-Welch algorithm (BWHMM)\nto predict the model parameters, which is difficult to find global optimal\nsolutions. This paper proposes an optimized Hidden Markov Model with Particle\nSwarm Optimization (PSO) algorithm and so is called PSOHMM. In order to\novercome the statistical constraints in HMM, the paper develops\nre-normalization and re-mapping mechanisms to ensure the constraints in HMM.\nThe experiments have shown that PSOHMM can search better solution than BWHMM,\nand has faster convergence speed.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 09:53:16 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Chang", "L.", "", "DISP"], ["Ouzrout", "Yacine", "", "DISP"], ["Nongaillard", "Antoine", "", "DISP"], ["Bouras", "Abdelaziz", "", "DISP"]]}, {"id": "1811.03451", "submitter": "Murali Karthick Baskar", "authors": "Martin Karafi\\'at, Murali Karthick Baskar, Shinji Watanabe, Takaaki\n  Hori, Matthew Wiesner and Jan \"Honza'' \\v{C}ernock\\'y", "title": "Analysis of Multilingual Sequence-to-Sequence speech recognition systems", "comments": "arXiv admin note: text overlap with arXiv:1810.03459", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the applications of various multilingual approaches\ndeveloped in conventional hidden Markov model (HMM) systems to\nsequence-to-sequence (seq2seq) automatic speech recognition (ASR). On a set\ncomposed of Babel data, we first show the effectiveness of multi-lingual\ntraining with stacked bottle-neck (SBN) features. Then we explore various\narchitectures and training strategies of multi-lingual seq2seq models based on\nCTC-attention networks including combinations of output layer, CTC and/or\nattention component re-training. We also investigate the effectiveness of\nlanguage-transfer learning in a very low resource scenario when the target\nlanguage is not included in the original multi-lingual training data.\nInterestingly, we found multilingual features superior to multilingual models,\nand this finding suggests that we can efficiently combine the benefits of the\nHMM system with the seq2seq system through these multilingual feature\ntechniques.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 09:59:24 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Karafi\u00e1t", "Martin", ""], ["Baskar", "Murali Karthick", ""], ["Watanabe", "Shinji", ""], ["Hori", "Takaaki", ""], ["Wiesner", "Matthew", ""], ["\u010cernock\u00fd", "Jan \"Honza''", ""]]}, {"id": "1811.03456", "submitter": "Jiayang Liu", "authors": "Jiayang Liu, Weiming Zhang, Nenghai Yu", "title": "CAAD 2018: Iterative Ensemble Adversarial Attack", "comments": "arXiv admin note: text overlap with arXiv:1811.00189", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have recently led to significant improvements in\nmany fields. However, DNNs are vulnerable to adversarial examples which are\nsamples with imperceptible perturbations while dramatically misleading the\nDNNs. Adversarial attacks can be used to evaluate the robustness of deep\nlearning models before they are deployed. Unfortunately, most of existing\nadversarial attacks can only fool a black-box model with a low success rate. To\nimprove the success rates for black-box adversarial attacks, we proposed an\niterated adversarial attack against an ensemble of image classifiers. With this\nmethod, we won the 5th place in CAAD 2018 Targeted Adversarial Attack\ncompetition.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 07:36:55 GMT"}], "update_date": "2018-11-11", "authors_parsed": [["Liu", "Jiayang", ""], ["Zhang", "Weiming", ""], ["Yu", "Nenghai", ""]]}, {"id": "1811.03491", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane", "title": "Degree-$d$ Chow Parameters Robustly Determine Degree-$d$ PTFs (and\n  Algorithmic Applications)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The degree-$d$ Chow parameters of a Boolean function $f: \\{-1,1\\}^n \\to\n\\mathbb{R}$ are its degree at most $d$ Fourier coefficients. It is well-known\nthat degree-$d$ Chow parameters uniquely characterize degree-$d$ polynomial\nthreshold functions (PTFs) within the space of all bounded functions. In this\npaper, we prove a robust version of this theorem: For $f$ any Boolean\ndegree-$d$ PTF and $g$ any bounded function, if the degree-$d$ Chow parameters\nof $f$ are close to the degree-$d$ Chow parameters of $g$ in $\\ell_2$-norm,\nthen $f$ is close to $g$ in $\\ell_1$-distance. Notably, our bound relating the\ntwo distances is completely independent of the dimension $n$. That is, we show\nthat Boolean degree-$d$ PTFs are {\\em robustly identifiable} from their\ndegree-$d$ Chow parameters. Results of this form had been shown for the $d=1$\ncase~\\cite{OS11:chow, DeDFS14}, but no non-trivial bound was previously known\nfor $d >1$.\n  Our robust identifiability result gives the following algorithmic\napplications: First, we show that Boolean degree-$d$ PTFs can be efficiently\napproximately reconstructed from approximations to their degree-$d$ Chow\nparameters. This immediately implies that degree-$d$ PTFs are efficiently\nlearnable in the uniform distribution $d$-RFA\nmodel~\\cite{BenDavidDichterman:98}. As a byproduct of our approach, we also\nobtain the first low integer-weight approximations of degree-$d$ PTFs, for\n$d>1$. As our second application, our robust identifiability result gives the\nfirst efficient algorithm, with dimension-independent error guarantees, for\nmalicious learning of Boolean degree-$d$ PTFs under the uniform distribution.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 17:59:16 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""]]}, {"id": "1811.03493", "submitter": "Gopal P. Sarma", "authors": "Gopal P. Sarma, Adam Safron, and Nick J. Hay", "title": "Integrative Biological Simulation, Neuropsychology, and AI Safety", "comments": "5 pages", "journal-ref": "Proceedings of the AAAI Workshop on Artificial Intelligence Safety\n  2019 co-located with the Thirty-Third AAAI Conference on Artificial\n  Intelligence 2019 (AAAI 2019)", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a biologically-inspired research agenda with parallel tracks\naimed at AI and AI safety. The bottom-up component consists of building a\nsequence of biophysically realistic simulations of simple organisms such as the\nnematode $Caenorhabditis$ $elegans$, the fruit fly $Drosophila$ $melanogaster$,\nand the zebrafish $Danio$ $rerio$ to serve as platforms for research into AI\nalgorithms and system architectures. The top-down component consists of an\napproach to value alignment that grounds AI goal structures in neuropsychology,\nbroadly considered. Our belief is that parallel pursuit of these tracks will\ninform the development of value-aligned AI systems that have been inspired by\nembodied organisms with sensorimotor integration. An important set of side\nbenefits is that the research trajectories we describe here are grounded in\nlong-standing intellectual traditions within existing research communities and\nfunding structures. In addition, these research programs overlap with\nsignificant contemporary themes in the biological and psychological sciences\nsuch as data/model integration and reproducibility.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 01:38:24 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2019 19:04:47 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Sarma", "Gopal P.", ""], ["Safron", "Adam", ""], ["Hay", "Nick J.", ""]]}, {"id": "1811.03508", "submitter": "Chen Cai", "authors": "Chen Cai, Yusu Wang", "title": "A simple yet effective baseline for non-attributed graph classification", "comments": "13 pages. Shorter version appears at 2019 ICLR Workshop:\n  Representation Learning on Graphs and Manifolds. arXiv admin note: text\n  overlap with arXiv:1810.00826 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graphs are complex objects that do not lend themselves easily to typical\nlearning tasks. Recently, a range of approaches based on graph kernels or graph\nneural networks have been developed for graph classification and for\nrepresentation learning on graphs in general. As the developed methodologies\nbecome more sophisticated, it is important to understand which components of\nthe increasingly complex methods are necessary or most effective.\n  As a first step, we develop a simple yet meaningful graph representation, and\nexplore its effectiveness in graph classification. We test our baseline\nrepresentation for the graph classification task on a range of graph datasets.\nInterestingly, this simple representation achieves similar performance as the\nstate-of-the-art graph kernels and graph neural networks for non-attributed\ngraph classification. Its performance on classifying attributed graphs is\nslightly weaker as it does not incorporate attributes. However, given its\nsimplicity and efficiency, we believe that it still serves as an effective\nbaseline for attributed graph classification. Our graph representation is\nefficient (linear-time) to compute. We also provide a simple connection with\nthe graph neural networks.\n  Note that these observations are only for the task of graph classification\nwhile existing methods are often designed for a broader scope including node\nembedding and link prediction. The results are also likely biased due to the\nlimited amount of benchmark datasets available. Nevertheless, the good\nperformance of our simple baseline calls for the development of new, more\ncomprehensive benchmark datasets so as to better evaluate and analyze different\ngraph learning methods. Furthermore, given the computational efficiency of our\ngraph summary, we believe that it is a good candidate as a baseline method for\nfuture graph classification (or even other graph learning) studies.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 15:53:37 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 19:15:19 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Cai", "Chen", ""], ["Wang", "Yusu", ""]]}, {"id": "1811.03516", "submitter": "Feryal Behbahani", "authors": "Feryal Behbahani, Kyriacos Shiarlis, Xi Chen, Vitaly Kurin, Sudhanshu\n  Kasewa, Ciprian Stirbu, Jo\\~ao Gomes, Supratik Paul, Frans A. Oliehoek,\n  Jo\\~ao Messias, Shimon Whiteson", "title": "Learning from Demonstration in the Wild", "comments": "Accepted to the IEEE International Conference on Robotics and\n  Automation (ICRA) 2019; extended version with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from demonstration (LfD) is useful in settings where hand-coding\nbehaviour or a reward function is impractical. It has succeeded in a wide range\nof problems but typically relies on manually generated demonstrations or\nspecially deployed sensors and has not generally been able to leverage the\ncopious demonstrations available in the wild: those that capture behaviours\nthat were occurring anyway using sensors that were already deployed for another\npurpose, e.g., traffic camera footage capturing demonstrations of natural\nbehaviour of vehicles, cyclists, and pedestrians. We propose Video to Behaviour\n(ViBe), a new approach to learn models of behaviour from unlabelled raw video\ndata of a traffic scene collected from a single, monocular, initially\nuncalibrated camera with ordinary resolution. Our approach calibrates the\ncamera, detects relevant objects, tracks them through time, and uses the\nresulting trajectories to perform LfD, yielding models of naturalistic\nbehaviour. We apply ViBe to raw videos of a traffic intersection and show that\nit can learn purely from videos, without additional expert knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 16:03:23 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 00:11:48 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Behbahani", "Feryal", ""], ["Shiarlis", "Kyriacos", ""], ["Chen", "Xi", ""], ["Kurin", "Vitaly", ""], ["Kasewa", "Sudhanshu", ""], ["Stirbu", "Ciprian", ""], ["Gomes", "Jo\u00e3o", ""], ["Paul", "Supratik", ""], ["Oliehoek", "Frans A.", ""], ["Messias", "Jo\u00e3o", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1811.03531", "submitter": "Zachary Charles", "authors": "Zachary Charles, Harrison Rosenberg, Dimitris Papailiopoulos", "title": "A Geometric Perspective on the Transferability of Adversarial Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art machine learning models frequently misclassify inputs that\nhave been perturbed in an adversarial manner. Adversarial perturbations\ngenerated for a given input and a specific classifier often seem to be\neffective on other inputs and even different classifiers. In other words,\nadversarial perturbations seem to transfer between different inputs, models,\nand even different neural network architectures. In this work, we show that in\nthe context of linear classifiers and two-layer ReLU networks, there provably\nexist directions that give rise to adversarial perturbations for many\nclassifiers and data points simultaneously. We show that these \"transferable\nadversarial directions\" are guaranteed to exist for linear separators of a\ngiven set, and will exist with high probability for linear classifiers trained\non independent sets drawn from the same distribution. We extend our results to\nlarge classes of two-layer ReLU networks. We further show that adversarial\ndirections for ReLU networks transfer to linear classifiers while the reverse\nneed not hold, suggesting that adversarial perturbations for more complex\nmodels are more likely to transfer to other classifiers. We validate our\nfindings empirically, even for deeper ReLU networks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 16:23:50 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Charles", "Zachary", ""], ["Rosenberg", "Harrison", ""], ["Papailiopoulos", "Dimitris", ""]]}, {"id": "1811.03537", "submitter": "Adish Singla", "authors": "Teresa Yeo, Parameswaran Kamalaruban, Adish Singla, Arpit Merchant,\n  Thibault Asselborn, Louis Faucon, Pierre Dillenbourg, Volkan Cevher", "title": "Iterative Classroom Teaching", "comments": "AAAI'19 (extended version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the machine teaching problem in a classroom-like setting wherein\nthe teacher has to deliver the same examples to a diverse group of students.\nTheir diversity stems from differences in their initial internal states as well\nas their learning rates. We prove that a teacher with full knowledge about the\nlearning dynamics of the students can teach a target concept to the entire\nclassroom using O(min{d,N} log(1/eps)) examples, where d is the ambient\ndimension of the problem, N is the number of learners, and eps is the accuracy\nparameter. We show the robustness of our teaching strategy when the teacher has\nlimited knowledge of the learners' internal dynamics as provided by a noisy\noracle. Further, we study the trade-off between the learners' workload and the\nteacher's cost in teaching the target concept. Our experiments validate our\ntheoretical results and suggest that appropriately partitioning the classroom\ninto homogenous groups provides a balance between these two objectives.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 16:34:14 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 16:16:23 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Yeo", "Teresa", ""], ["Kamalaruban", "Parameswaran", ""], ["Singla", "Adish", ""], ["Merchant", "Arpit", ""], ["Asselborn", "Thibault", ""], ["Faucon", "Louis", ""], ["Dillenbourg", "Pierre", ""], ["Cevher", "Volkan", ""]]}, {"id": "1811.03542", "submitter": "Kashyap Chitta", "authors": "Kashyap Chitta, Jianwei Feng, Martial Hebert", "title": "Adaptive Semantic Segmentation with a Strategic Curriculum of Proxy\n  Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep networks for semantic segmentation requires annotation of large\namounts of data, which can be time-consuming and expensive. Unfortunately,\nthese trained networks still generalize poorly when tested in domains not\nconsistent with the training data. In this paper, we show that by carefully\npresenting a mixture of labeled source domain and proxy-labeled target domain\ndata to a network, we can achieve state-of-the-art unsupervised domain\nadaptation results. With our design, the network progressively learns features\nspecific to the target domain using annotation from only the source domain. We\ngenerate proxy labels for the target domain using the network's own\npredictions. Our architecture then allows selective mining of easy samples from\nthis set of proxy labels, and hard samples from the annotated source domain. We\nconduct a series of experiments with the GTA5, Cityscapes and BDD100k datasets\non synthetic-to-real domain adaptation and geographic domain adaptation,\nshowing the advantages of our method over baselines and existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 16:44:59 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Chitta", "Kashyap", ""], ["Feng", "Jianwei", ""], ["Hebert", "Martial", ""]]}, {"id": "1811.03562", "submitter": "Mizanur Rahman", "authors": "Mizanur Rahman, Mashrur Chowdhury and Jerome McClendon", "title": "Real time Traffic Flow Parameters Prediction with Basic Safety Messages\n  at Low Penetration of Connected Vehicles", "comments": "16 pages, 15 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expected low market penetration of connected vehicles (CVs) in the near\nfuture could be a constraint in estimating traffic flow parameters, such as\naverage travel speed of a roadway segment and average space headway between\nvehicles from the CV broadcasted data. This estimated traffic flow parameters\nfrom low penetration of connected vehicles become noisy compared to 100 percent\npenetration of CVs, and such noise reduces the real time prediction accuracy of\na machine learning model, such as the accuracy of long short term memory (LSTM)\nmodel in terms of predicting traffic flow parameters. The accurate prediction\nof the parameters is important for future traffic condition assessment. To\nimprove the prediction accuracy using noisy traffic flow parameters, which is\nconstrained by limited CV market penetration and limited CV data, we developed\na real time traffic data prediction model that combines LSTM with Kalman filter\nbased Rauch Tung Striebel (RTS) noise reduction model. We conducted a case\nstudy using the Enhanced Next Generation Simulation (NGSIM) dataset, which\ncontains vehicle trajectory data for every one tenth of a second, to evaluate\nthe performance of this prediction model. Compared to a baseline LSTM model\nperformance, for only 5 percent penetration of CVs, the analyses revealed that\ncombined LSTM and RTS model reduced the mean absolute percentage error (MAPE)\nfrom 19 percent to 5 percent for speed prediction and from 27 percent to 9\npercent for space-headway prediction. The statistical significance test with a\n95 percent confidence interval confirmed no significant difference in predicted\naverage speed and average space headway using this LSTM and RTS combination\nwith only 5 percent CV penetration rate.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 17:34:14 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 15:25:34 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Rahman", "Mizanur", ""], ["Chowdhury", "Mashrur", ""], ["McClendon", "Jerome", ""]]}, {"id": "1811.03567", "submitter": "Honglin Chen", "authors": "Will Xiao, Honglin Chen, Qianli Liao and Tomaso Poggio", "title": "Biologically-plausible learning algorithms can scale to large datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The backpropagation (BP) algorithm is often thought to be biologically\nimplausible in the brain. One of the main reasons is that BP requires symmetric\nweight matrices in the feedforward and feedback pathways. To address this\n\"weight transport problem\" (Grossberg, 1987), two more biologically plausible\nalgorithms, proposed by Liao et al. (2016) and Lillicrap et al. (2016), relax\nBP's weight symmetry requirements and demonstrate comparable learning\ncapabilities to that of BP on small datasets. However, a recent study by\nBartunov et al. (2018) evaluate variants of target-propagation (TP) and\nfeedback alignment (FA) on MINIST, CIFAR, and ImageNet datasets, and find that\nalthough many of the proposed algorithms perform well on MNIST and CIFAR, they\nperform significantly worse than BP on ImageNet. Here, we additionally evaluate\nthe sign-symmetry algorithm (Liao et al., 2016), which differs from both BP and\nFA in that the feedback and feedforward weights share signs but not magnitudes.\nWe examine the performance of sign-symmetry and feedback alignment on ImageNet\nand MS COCO datasets using different network architectures (ResNet-18 and\nAlexNet for ImageNet, RetinaNet for MS COCO). Surprisingly, networks trained\nwith sign-symmetry can attain classification performance approaching that of\nBP-trained networks. These results complement the study by Bartunov et al.\n(2018), and establish a new benchmark for future biologically plausible\nlearning algorithms on more difficult datasets and more complex architectures.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 17:43:59 GMT"}, {"version": "v2", "created": "Sun, 25 Nov 2018 21:23:57 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 02:03:52 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Xiao", "Will", ""], ["Chen", "Honglin", ""], ["Liao", "Qianli", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1811.03568", "submitter": "Zhenyu Liao", "authors": "Yacine Chitour, Zhenyu Liao, Romain Couillet", "title": "A Geometric Approach of Gradient Descent Algorithms in Neural Networks", "comments": "Preprint. Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an original geometric framework to analyze the\nconvergence properties of gradient descent trajectories in the context of\nlinear neural networks. Built upon a key invariance property induced by the\nnetwork structure, we propose a conjecture called \\emph{overfitting conjecture}\nstating that, for almost every training data, the corresponding gradient\ndescent trajectory converges to a global minimum, for almost every initial\ncondition. This would imply that, for linear neural networks of an arbitrary\nnumber of hidden layers, the solution achieved by simple gradient descent\nalgorithm is equivalent to that of least square estimation. Our first result\nconsists in establishing, in the case of linear networks of arbitrary depth,\nconvergence of gradient descent trajectories to critical points of the loss\nfunction. Our second result is the proof of the \\emph{overfitting conjecture}\nin the case of single-hidden-layer linear networks with an argument based on\nthe notion of normal hyperbolicity and under a generic property on the training\ndata (i.e., holding for almost every training data).\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 17:45:19 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 17:05:02 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Chitour", "Yacine", ""], ["Liao", "Zhenyu", ""], ["Couillet", "Romain", ""]]}, {"id": "1811.03571", "submitter": "Luca Bortolussi", "authors": "Luca Bortolussi and Guido Sanguinetti", "title": "Intrinsic Geometric Vulnerability of High-Dimensional Artificial\n  Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of modern Artificial Intelligence (AI) technologies depends\ncritically on the ability to learn non-linear functional dependencies from\nlarge, high dimensional data sets. Despite recent high-profile successes,\nempirical evidence indicates that the high predictive performance is often\npaired with low robustness, making AI systems potentially vulnerable to\nadversarial attacks. In this report, we provide a simple intuitive argument\nsuggesting that high performance and vulnerability are intrinsically coupled,\nand largely dependent on the geometry of typical, high-dimensional data sets.\nOur work highlights a major potential pitfall of modern AI systems, and\nsuggests practical research directions to ameliorate the problem.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 17:51:27 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 14:13:58 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Bortolussi", "Luca", ""], ["Sanguinetti", "Guido", ""]]}, {"id": "1811.03575", "submitter": "Kashyap Chitta", "authors": "Kashyap Chitta, Jose M. Alvarez, Adam Lesnikowski", "title": "Large-Scale Visual Active Learning with Deep Probabilistic Ensembles", "comments": "arXiv admin note: text overlap with arXiv:1811.02640", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotating the right data for training deep neural networks is an important\nchallenge. Active learning using uncertainty estimates from Bayesian Neural\nNetworks (BNNs) could provide an effective solution to this. Despite being\ntheoretically principled, BNNs require approximations to be applied to\nlarge-scale problems, where both performance and uncertainty estimation are\ncrucial. In this paper, we introduce Deep Probabilistic Ensembles (DPEs), a\nscalable technique that uses a regularized ensemble to approximate a deep BNN.\nWe conduct a series of large-scale visual active learning experiments to\nevaluate DPEs on classification with the CIFAR-10, CIFAR-100 and ImageNet\ndatasets, and semantic segmentation with the BDD100k dataset. Our models\nrequire significantly less training data to achieve competitive performances,\nand steadily improve upon strong active learning baselines as the annotation\nbudget is increased.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 17:56:43 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 21:45:26 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2019 02:02:13 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Chitta", "Kashyap", ""], ["Alvarez", "Jose M.", ""], ["Lesnikowski", "Adam", ""]]}, {"id": "1811.03591", "submitter": "Ilya Razenshteyn", "authors": "Sepideh Mahabadi, Konstantin Makarychev, Yury Makarychev, Ilya\n  Razenshteyn", "title": "Nonlinear Dimension Reduction via Outer Bi-Lipschitz Extensions", "comments": "27 pages, 6 figures; an extended abstract appeared in the proceedings\n  of STOC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.LG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study the notion of an outer bi-Lipschitz extension of a map\nbetween Euclidean spaces. The notion is a natural analogue of the notion of a\nLipschitz extension of a Lipschitz map. We show that for every map $f$ there\nexists an outer bi-Lipschitz extension $f'$ whose distortion is greater than\nthat of $f$ by at most a constant factor. This result can be seen as a\ncounterpart of the classic Kirszbraun theorem for outer bi-Lipschitz\nextensions. We also study outer bi-Lipschitz extensions of near-isometric maps\nand show upper and lower bounds for them. Then, we present applications of our\nresults to prioritized and terminal dimension reduction problems.\n  * We prove a prioritized variant of the Johnson-Lindenstrauss lemma: given a\nset of points $X\\subset \\mathbb{R}^d$ of size $N$ and a permutation (\"priority\nranking\") of $X$, there exists an embedding $f$ of $X$ into $\\mathbb{R}^{O(\\log\nN)}$ with distortion $O(\\log \\log N)$ such that the point of rank $j$ has only\n$O(\\log^{3 + \\varepsilon} j)$ non-zero coordinates - more specifically, all but\nthe first $O(\\log^{3+\\varepsilon} j)$ coordinates are equal to $0$; the\ndistortion of $f$ restricted to the first $j$ points (according to the ranking)\nis at most $O(\\log\\log j)$. The result makes a progress towards answering an\nopen question by Elkin, Filtser, and Neiman about prioritized dimension\nreductions.\n  * We prove that given a set $X$ of $N$ points in $\\mathbb{R}^d$, there exists\na terminal dimension reduction embedding of $\\mathbb{R}^d$ into\n$\\mathbb{R}^{d'}$, where $d' = O\\left(\\frac{\\log N}{\\varepsilon^4}\\right)$,\nwhich preserves distances $\\|x-y\\|$ between points $x\\in X$ and $y \\in\n\\mathbb{R}^{d}$, up to a multiplicative factor of $1 \\pm \\varepsilon$. This\nimproves a recent result by Elkin, Filtser, and Neiman.\n  The dimension reductions that we obtain are nonlinear, and this nonlinearity\nis necessary.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 18:23:56 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Mahabadi", "Sepideh", ""], ["Makarychev", "Konstantin", ""], ["Makarychev", "Yury", ""], ["Razenshteyn", "Ilya", ""]]}, {"id": "1811.03600", "submitter": "Christopher Shallue", "authors": "Christopher J. Shallue and Jaehoon Lee and Joseph Antognini and Jascha\n  Sohl-Dickstein and Roy Frostig and George E. Dahl", "title": "Measuring the Effects of Data Parallelism on Neural Network Training", "comments": null, "journal-ref": "Journal of Machine Learning Research 20 (2019) 1-49", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent hardware developments have dramatically increased the scale of data\nparallelism available for neural network training. Among the simplest ways to\nharness next-generation hardware is to increase the batch size in standard\nmini-batch neural network training algorithms. In this work, we aim to\nexperimentally characterize the effects of increasing the batch size on\ntraining time, as measured by the number of steps necessary to reach a goal\nout-of-sample error. We study how this relationship varies with the training\nalgorithm, model, and data set, and find extremely large variation between\nworkloads. Along the way, we show that disagreements in the literature on how\nbatch size affects model quality can largely be explained by differences in\nmetaparameter tuning and compute budgets at different batch sizes. We find no\nevidence that larger batch sizes degrade out-of-sample performance. Finally, we\ndiscuss the implications of our results on efforts to train neural networks\nmuch faster in the future. Our experimental data is publicly available as a\ndatabase of 71,638,836 loss measurements taken over the course of training for\n168,160 individual models across 35 workloads.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 18:33:41 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 00:16:26 GMT"}, {"version": "v3", "created": "Fri, 19 Jul 2019 00:47:44 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Shallue", "Christopher J.", ""], ["Lee", "Jaehoon", ""], ["Antognini", "Joseph", ""], ["Sohl-Dickstein", "Jascha", ""], ["Frostig", "Roy", ""], ["Dahl", "George E.", ""]]}, {"id": "1811.03601", "submitter": "Jack Langerman", "authors": "Ziming Qiu, Jack Langerman, Nitin Nair, Orlando Aristizabal, Jonathan\n  Mamou, Daniel H. Turnbull, Jeffrey Ketterling, Yao Wang", "title": "Deep BV: A Fully Automated System for Brain Ventricle Localization and\n  Segmentation in 3D Ultrasound Images of Embryonic Mice", "comments": "IEEE Signal Processing in Medicine and Biology Symposium - 2018, 6\n  pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric analysis of brain ventricle (BV) structure is a key tool in the\nstudy of central nervous system development in embryonic mice. High-frequency\nultrasound (HFU) is the only non-invasive, real-time modality available for\nrapid volumetric imaging of embryos in utero. However, manual segmentation of\nthe BV from HFU volumes is tedious, time-consuming, and requires specialized\nexpertise. In this paper, we propose a novel deep learning based BV\nsegmentation system for whole-body HFU images of mouse embryos. Our fully\nautomated system consists of two modules: localization and segmentation. It\nfirst applies a volumetric convolutional neural network on a 3D sliding window\nover the entire volume to identify a 3D bounding box containing the entire BV.\nIt then employs a fully convolutional network to segment the detected bounding\nbox into BV and background. The system achieves a Dice Similarity Coefficient\n(DSC) of 0.8956 for BV segmentation on an unseen 111 HFU volume test set\nsurpassing the previous state-of-the-art method (DSC of 0.7119) by a margin of\n25%.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 20:07:53 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Qiu", "Ziming", ""], ["Langerman", "Jack", ""], ["Nair", "Nitin", ""], ["Aristizabal", "Orlando", ""], ["Mamou", "Jonathan", ""], ["Turnbull", "Daniel H.", ""], ["Ketterling", "Jeffrey", ""], ["Wang", "Yao", ""]]}, {"id": "1811.03617", "submitter": "Youjie Li", "authors": "Mingchao Yu, Zhifeng Lin, Krishna Narra, Songze Li, Youjie Li, Nam\n  Sung Kim, Alexander Schwing, Murali Annavaram, Salman Avestimehr", "title": "GradiVeQ: Vector Quantization for Bandwidth-Efficient Gradient\n  Aggregation in Distributed CNN Training", "comments": "Accepted at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data parallelism can boost the training speed of convolutional neural\nnetworks (CNN), but could suffer from significant communication costs caused by\ngradient aggregation. To alleviate this problem, several scalar quantization\ntechniques have been developed to compress the gradients. But these techniques\ncould perform poorly when used together with decentralized aggregation\nprotocols like ring all-reduce (RAR), mainly due to their inability to directly\naggregate compressed gradients. In this paper, we empirically demonstrate the\nstrong linear correlations between CNN gradients, and propose a gradient vector\nquantization technique, named GradiVeQ, to exploit these correlations through\nprincipal component analysis (PCA) for substantial gradient dimension\nreduction. GradiVeQ enables direct aggregation of compressed gradients, hence\nallows us to build a distributed learning system that parallelizes GradiVeQ\ngradient compression and RAR communications. Extensive experiments on popular\nCNNs demonstrate that applying GradiVeQ slashes the wall-clock gradient\naggregation time of the original RAR by more than 5X without noticeable\naccuracy loss, and reduces the end-to-end training time by almost 50%. The\nresults also show that GradiVeQ is compatible with scalar quantization\ntechniques such as QSGD (Quantized SGD), and achieves a much higher speed-up\ngain under the same compression ratio.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 18:59:50 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 06:01:28 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Yu", "Mingchao", ""], ["Lin", "Zhifeng", ""], ["Narra", "Krishna", ""], ["Li", "Songze", ""], ["Li", "Youjie", ""], ["Kim", "Nam Sung", ""], ["Schwing", "Alexander", ""], ["Annavaram", "Murali", ""], ["Avestimehr", "Salman", ""]]}, {"id": "1811.03619", "submitter": "Youjie Li", "authors": "Youjie Li, Mingchao Yu, Songze Li, Salman Avestimehr, Nam Sung Kim,\n  Alexander Schwing", "title": "Pipe-SGD: A Decentralized Pipelined SGD Framework for Distributed Deep\n  Net Training", "comments": "Accepted at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed training of deep nets is an important technique to address some\nof the present day computing challenges like memory consumption and\ncomputational demands. Classical distributed approaches, synchronous or\nasynchronous, are based on the parameter server architecture, i.e., worker\nnodes compute gradients which are communicated to the parameter server while\nupdated parameters are returned. Recently, distributed training with AllReduce\noperations gained popularity as well. While many of those operations seem\nappealing, little is reported about wall-clock training time improvements. In\nthis paper, we carefully analyze the AllReduce based setup, propose timing\nmodels which include network latency, bandwidth, cluster size and compute time,\nand demonstrate that a pipelined training with a width of two combines the best\nof both synchronous and asynchronous training. Specifically, for a setup\nconsisting of a four-node GPU cluster we show wall-clock time training\nimprovements of up to 5.4x compared to conventional approaches.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 18:59:55 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 04:54:32 GMT"}, {"version": "v3", "created": "Fri, 11 Jan 2019 08:38:49 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Li", "Youjie", ""], ["Yu", "Mingchao", ""], ["Li", "Songze", ""], ["Avestimehr", "Salman", ""], ["Kim", "Nam Sung", ""], ["Schwing", "Alexander", ""]]}, {"id": "1811.03621", "submitter": "Hang Qiu", "authors": "Hang Qiu, Krishna Chintalapudi, Ramesh Govindan", "title": "Satyam: Democratizing Groundtruth for Machine Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The democratization of machine learning (ML) has led to ML-based machine\nvision systems for autonomous driving, traffic monitoring, and video\nsurveillance. However, true democratization cannot be achieved without greatly\nsimplifying the process of collecting groundtruth for training and testing\nthese systems. This groundtruth collection is necessary to ensure good\nperformance under varying conditions. In this paper, we present the design and\nevaluation of Satyam, a first-of-its-kind system that enables a layperson to\nlaunch groundtruth collection tasks for machine vision with minimal effort.\nSatyam leverages a crowdtasking platform, Amazon Mechanical Turk, and automates\nseveral challenging aspects of groundtruth collection: creating and launching\nof custom web-UI tasks for obtaining the desired groundtruth, controlling\nresult quality in the face of spammers and untrained workers, adapting prices\nto match task complexity, filtering spammers and workers with poor performance,\nand processing worker payments. We validate Satyam using several popular\nbenchmark vision datasets, and demonstrate that groundtruth obtained by Satyam\nis comparable to that obtained from trained experts and provides matching ML\nperformance when used for training.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 01:35:47 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Qiu", "Hang", ""], ["Chintalapudi", "Krishna", ""], ["Govindan", "Ramesh", ""]]}, {"id": "1811.03659", "submitter": "Ulugbek Kamilov", "authors": "Yu Sun, Brendt Wohlberg, and Ulugbek S. Kamilov", "title": "Plug-In Stochastic Gradient Method", "comments": "To be presented at International Biomedical and Astronomical Signal\n  Processing (BASP) Frontiers workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plug-and-play priors (PnP) is a popular framework for regularized signal\nreconstruction by using advanced denoisers within an iterative algorithm. In\nthis paper, we discuss our recent online variant of PnP that uses only a subset\nof measurements at every iteration, which makes it scalable to very large\ndatasets. We additionally present novel convergence results for both batch and\nonline PnP algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 19:40:23 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Sun", "Yu", ""], ["Wohlberg", "Brendt", ""], ["Kamilov", "Ulugbek S.", ""]]}, {"id": "1811.03666", "submitter": "Daeyoung Choi", "authors": "Daeyoung Choi, Kyungeun Lee, Duhun Hwang, Wonjong Rhee", "title": "Statistical Characteristics of Deep Representations: An Empirical\n  Investigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, the effects of eight representation regularization methods are\ninvestigated, including two newly developed rank regularizers (RR). The\ninvestigation shows that the statistical characteristics of representations\nsuch as correlation, sparsity, and rank can be manipulated as intended, during\ntraining. Furthermore, it is possible to improve the baseline performance\nsimply by trying all the representation regularizers and fine-tuning the\nstrength of their effects. In contrast to performance improvement, no\nconsistent relationship between performance and statistical characteristics was\nobservable. The results indicate that manipulation of statistical\ncharacteristics can be helpful for improving performance, but only indirectly\nthrough its influence on learning dynamics or its tuning effects.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 20:17:50 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 11:39:32 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Choi", "Daeyoung", ""], ["Lee", "Kyungeun", ""], ["Hwang", "Duhun", ""], ["Rhee", "Wonjong", ""]]}, {"id": "1811.03679", "submitter": "Samuel Kessler", "authors": "Samuel Kessler, Arnold Salas, Vincent W. C. Tan, Stefan Zohren,\n  Stephen Roberts", "title": "Practical Bayesian Learning of Neural Networks via Adaptive Optimisation\n  Methods", "comments": "Presented at the ICML 2020 Workshop on Uncertainty and Robustness in\n  Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel framework for the estimation of the posterior\ndistribution over the weights of a neural network, based on a new probabilistic\ninterpretation of adaptive optimisation algorithms such as AdaGrad and Adam. We\ndemonstrate the effectiveness of our Bayesian Adam method, Badam, by\nexperimentally showing that the learnt uncertainties correctly relate to the\nweights' predictive capabilities by weight pruning. We also demonstrate the\nquality of the derived uncertainty measures by comparing the performance of\nBadam to standard methods in a Thompson sampling setting for multi-armed\nbandits, where good uncertainty measures are required for an agent to balance\nexploration and exploitation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 21:04:00 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 13:05:01 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 16:47:37 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kessler", "Samuel", ""], ["Salas", "Arnold", ""], ["Tan", "Vincent W. C.", ""], ["Zohren", "Stefan", ""], ["Roberts", "Stephen", ""]]}, {"id": "1811.03685", "submitter": "Ian Goodfellow", "authors": "Ian Goodfellow", "title": "New CleverHans Feature: Better Adversarial Robustness Evaluations with\n  Attack Bundling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report describes a new feature of the CleverHans library\ncalled \"attack bundling\". Many papers about adversarial examples present lists\nof error rates corresponding to different attack algorithms. A common approach\nis to take the maximum across this list and compare defenses against that error\nrate. We argue that a better approach is to use attack bundling: the max should\nbe taken across many examples at the level of individual examples, then the\nerror rate should be calculated by averaging after this maximization operation.\nReporting the bundled attacker error rate provides a lower bound on the true\nworst-case error rate. The traditional approach of reporting the maximum error\nrate across attacks can underestimate the true worst-case error rate by an\namount approaching 100\\% as the number of attacks approaches infinity. Attack\nbundling can be used with different prioritization schemes to optimize\nquantities such as error rate on adversarial examples, perturbation size needed\nto cause misclassification, or failure rate when using a specific confidence\nthreshold.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 21:30:57 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Goodfellow", "Ian", ""]]}, {"id": "1811.03692", "submitter": "Deepak Mishra", "authors": "Deepak Mishra, Prathosh A. P., Aravind Jayendran, Varun Srivastava,\n  Santanu Chaudhury", "title": "Mode matching in GANs through latent space learning and inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have shown remarkable success in\ngeneration of unstructured data, such as, natural images. However, discovery\nand separation of modes in the generated space, essential for several tasks\nbeyond naive data generation, is still a challenge. In this paper, we address\nthe problem of imposing desired modal properties on the generated space using a\nlatent distribution, engineered in accordance with the modal properties of the\ntrue data distribution. This is achieved by training a latent space inversion\nnetwork in tandem with the generative network using a divergence loss. The\nlatent space is made to follow a continuous multimodal distribution generated\nby reparameterization of a pair of continuous and discrete random variables. In\naddition, the modal priors of the latent distribution are learned to match with\nthe true data distribution using minimal-supervision with negligible increment\nin number of learnable parameters. We validate our method on multiple tasks\nsuch as mode separation, conditional generation, and attribute discovery on\nmultiple real world image datasets and demonstrate its efficacy over other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 22:08:12 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2018 06:44:46 GMT"}, {"version": "v3", "created": "Sun, 24 Mar 2019 07:02:37 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Mishra", "Deepak", ""], ["P.", "Prathosh A.", ""], ["Jayendran", "Aravind", ""], ["Srivastava", "Varun", ""], ["Chaudhury", "Santanu", ""]]}, {"id": "1811.03700", "submitter": "Chao Weng", "authors": "Chao Weng, Dong Yu", "title": "A Comparison of Lattice-free Discriminative Training Criteria for Purely\n  Sequence-Trained Neural Network Acoustic Models", "comments": "under review ICASSP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, three lattice-free (LF) discriminative training criteria for\npurely sequence-trained neural network acoustic models are compared on LVCSR\ntasks, namely maximum mutual information (MMI), boosted maximum mutual\ninformation (bMMI) and state-level minimum Bayes risk (sMBR). We demonstrate\nthat, analogous to LF-MMI, a neural network acoustic model can also be trained\nfrom scratch using LF-bMMI or LF-sMBR criteria respectively without the need of\ncross-entropy pre-training. Furthermore, experimental results on\nSwitchboard-300hrs and Switchboard+Fisher-2100hrs datasets show that models\ntrained with LF-bMMI consistently outperform those trained with plain LF-MMI\nand achieve a relative word error rate (WER) reduction of 5% over competitive\ntemporal convolution projected LSTM (TDNN-LSTMP) LF-MMI baselines.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 22:37:55 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 10:55:10 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Weng", "Chao", ""], ["Yu", "Dong", ""]]}, {"id": "1811.03707", "submitter": "Jakub Nalepa", "authors": "Jakub Nalepa, Michal Myller, Michal Kawulok", "title": "Validating Hyperspectral Image Segmentation", "comments": "Submitted to IEEE Geoscience and Remote Sensing Letters", "journal-ref": null, "doi": "10.1109/LGRS.2019.2895697", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral satellite imaging attracts enormous research attention in the\nremote sensing community, hence automated approaches for precise segmentation\nof such imagery are being rapidly developed. In this letter, we share our\nobservations on the strategy for validating hyperspectral image segmentation\nalgorithms currently followed in the literature, and show that it can lead to\nover-optimistic experimental insights. We introduce a new routine for\ngenerating segmentation benchmarks, and use it to elaborate ready-to-use\nhyperspectral training-test data partitions. They can be utilized for fair\nvalidation of new and existing algorithms without any training-test data\nleakage.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 22:59:40 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Nalepa", "Jakub", ""], ["Myller", "Michal", ""], ["Kawulok", "Michal", ""]]}, {"id": "1811.03711", "submitter": "Yuanyuan Liu", "authors": "Qiang Zhang, Rui Luo, Yaodong Yang, Yuanyuan Liu", "title": "Benchmarking Deep Sequential Models on Volatility Predictions for\n  Financial Time Series", "comments": "NIPS 2018, Workshop on Challenges and Opportunities for AI in\n  Financial Services", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volatility is a quantity of measurement for the price movements of stocks or\noptions which indicates the uncertainty within financial markets. As an\nindicator of the level of risk or the degree of variation, volatility is\nimportant to analyse the financial market, and it is taken into consideration\nin various decision-making processes in financial activities. On the other\nhand, recent advancement in deep learning techniques has shown strong\ncapabilities in modelling sequential data, such as speech and natural language.\nIn this paper, we empirically study the applicability of the latest deep\nstructures with respect to the volatility modelling problem, through which we\naim to provide an empirical guidance for the theoretical analysis of the\nmarriage between deep learning techniques and financial applications in the\nfuture. We examine both the traditional approaches and the deep sequential\nmodels on the task of volatility prediction, including the most recent variants\nof convolutional and recurrent networks, such as the dilated architecture.\nAccordingly, experiments with real-world stock price datasets are performed on\na set of 1314 daily stock series for 2018 days of transaction. The evaluation\nand comparison are based on the negative log likelihood (NLL) of real-world\nstock price time series. The result shows that the dilated neural models,\nincluding dilated CNN and Dilated RNN, produce most accurate estimation and\nprediction, outperforming various widely-used deterministic models in the GARCH\nfamily and several recently proposed stochastic models. In addition, the high\nflexibility and rich expressive power are validated in this study.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 23:11:38 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Zhang", "Qiang", ""], ["Luo", "Rui", ""], ["Yang", "Yaodong", ""], ["Liu", "Yuanyuan", ""]]}, {"id": "1811.03717", "submitter": "Micha{\\l} Derezi\\'nski", "authors": "Micha{\\l} Derezi\\'nski", "title": "Fast determinantal point processes via distortion-free intermediate\n  sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a fixed $n\\times d$ matrix $\\mathbf{X}$, where $n\\gg d$, we study the\ncomplexity of sampling from a distribution over all subsets of rows where the\nprobability of a subset is proportional to the squared volume of the\nparallelepiped spanned by the rows (a.k.a. a determinantal point process). In\nthis task, it is important to minimize the preprocessing cost of the procedure\n(performed once) as well as the sampling cost (performed repeatedly). To that\nend, we propose a new determinantal point process algorithm which has the\nfollowing two properties, both of which are novel: (1) a preprocessing step\nwhich runs in time $O(\\text{number-of-non-zeros}(\\mathbf{X})\\cdot\\log\nn)+\\text{poly}(d)$, and (2) a sampling step which runs in $\\text{poly}(d)$\ntime, independent of the number of rows $n$. We achieve this by introducing a\nnew regularized determinantal point process (R-DPP), which serves as an\nintermediate distribution in the sampling procedure by reducing the number of\nrows from $n$ to $\\text{poly}(d)$. Crucially, this intermediate distribution\ndoes not distort the probabilities of the target sample. Our key novelty in\ndefining the R-DPP is the use of a Poisson random variable for controlling the\nprobabilities of different subset sizes, leading to new determinantal formulas\nsuch as the normalization constant for this distribution. Our algorithm has\napplications in many diverse areas where determinantal point processes have\nbeen used, such as machine learning, stochastic optimization, data\nsummarization and low-rank matrix reconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 23:35:29 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 20:59:29 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Derezi\u0144ski", "Micha\u0142", ""]]}, {"id": "1811.03728", "submitter": "Bryant Chen", "authors": "Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig,\n  Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava", "title": "Detecting Backdoor Attacks on Deep Neural Networks by Activation\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While machine learning (ML) models are being increasingly trusted to make\ndecisions in different and varying areas, the safety of systems using such\nmodels has become an increasing concern. In particular, ML models are often\ntrained on data from potentially untrustworthy sources, providing adversaries\nwith the opportunity to manipulate them by inserting carefully crafted samples\ninto the training set. Recent work has shown that this type of attack, called a\npoisoning attack, allows adversaries to insert backdoors or trojans into the\nmodel, enabling malicious behavior with simple external backdoor triggers at\ninference time and only a blackbox perspective of the model itself. Detecting\nthis type of attack is challenging because the unexpected behavior occurs only\nwhen a backdoor trigger, which is known only to the adversary, is present.\nModel users, either direct users of training data or users of pre-trained model\nfrom a catalog, may not guarantee the safe operation of their ML-based system.\nIn this paper, we propose a novel approach to backdoor detection and removal\nfor neural networks. Through extensive experimental results, we demonstrate its\neffectiveness for neural networks classifying text and images. To the best of\nour knowledge, this is the first methodology capable of detecting poisonous\ndata crafted to insert backdoors and repairing the model that does not require\na verified and trusted dataset.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 01:08:00 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Chen", "Bryant", ""], ["Carvalho", "Wilka", ""], ["Baracaldo", "Nathalie", ""], ["Ludwig", "Heiko", ""], ["Edwards", "Benjamin", ""], ["Lee", "Taesung", ""], ["Molloy", "Ian", ""], ["Srivastava", "Biplav", ""]]}, {"id": "1811.03733", "submitter": "Bhavya Kailkhura", "authors": "Thomas A. Hogan and Bhavya Kailkhura", "title": "Universal Decision-Based Black-Box Perturbations: Breaking\n  Security-Through-Obscurity Defenses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding a universal (image-agnostic) perturbation to\nfool machine learning (ML) classifiers (e.g., neural nets, decision tress) in\nthe hard-label black-box setting. Recent work in adversarial ML in the\nwhite-box setting (model parameters are known) has shown that many\nstate-of-the-art image classifiers are vulnerable to universal adversarial\nperturbations: a fixed human-imperceptible perturbation that, when added to any\nimage, causes it to be misclassified with high probability Kurakin et al.\n[2016], Szegedy et al. [2013], Chen et al. [2017a], Carlini and Wagner [2017].\nThis paper considers a more practical and challenging problem of finding such\nuniversal perturbations in an obscure (or black-box) setting. More\nspecifically, we use zeroth order optimization algorithms to find such a\nuniversal adversarial perturbation when no model information is revealed-except\nthat the attacker can make queries to probe the classifier. We further relax\nthe assumption that the output of a query is continuous valued confidence\nscores for all the classes and consider the case where the output is a\nhard-label decision. Surprisingly, we found that even in these extremely\nobscure regimes, state-of-the-art ML classifiers can be fooled with a very high\nprobability just by adding a single human-imperceptible image perturbation to\nany natural image. The surprising existence of universal perturbations in a\nhard-label black-box setting raises serious security concerns with the\nexistence of a universal noise vector that adversaries can possibly exploit to\nbreak a classifier on most natural images.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 01:43:22 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 15:56:52 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Hogan", "Thomas A.", ""], ["Kailkhura", "Bhavya", ""]]}, {"id": "1811.03739", "submitter": "Sihong Xie", "authors": "Shuaijun Ge and Guixiang Ma and Sihong Xie and Philip S. Yu", "title": "Securing Behavior-based Opinion Spam Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reviews spams are prevalent in e-commerce to manipulate product ranking and\ncustomers decisions maliciously. While spams generated based on simple spamming\nstrategy can be detected effectively, hardened spammers can evade regular\ndetectors via more advanced spamming strategies. Previous work gave more\nattention to evasion against text and graph-based detectors, but evasions\nagainst behavior-based detectors are largely ignored, leading to\nvulnerabilities in spam detection systems. Since real evasion data are scarce,\nwe first propose EMERAL (Evasion via Maximum Entropy and Rating sAmpLing) to\ngenerate evasive spams to certain existing detectors. EMERAL can simulate\nspammers with different goals and levels of knowledge about the detectors,\ntargeting at different stages of the life cycle of target products. We show\nthat in the evasion-defense dynamic, only a few evasion types are meaningful to\nthe spammers, and any spammer will not be able to evade too many detection\nsignals at the same time. We reveal that some evasions are quite insidious and\ncan fail all detection signals. We then propose DETER (Defense via Evasion\ngeneraTion using EmeRal), based on model re-training on diverse evasive samples\ngenerated by EMERAL. Experiments confirm that DETER is more accurate in\ndetecting both suspicious time window and individual spamming reviews. In terms\nof security, DETER is versatile enough to be vaccinated against diverse and\nunexpected evasions, is agnostic about evasion strategy and can be released\nwithout privacy concern.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 02:09:31 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Ge", "Shuaijun", ""], ["Ma", "Guixiang", ""], ["Xie", "Sihong", ""], ["Yu", "Philip S.", ""]]}, {"id": "1811.03744", "submitter": "Anindya De", "authors": "Anindya De, Philip M. Long and Rocco A. Servedio", "title": "Density estimation for shift-invariant multidimensional distributions", "comments": "Appears in the Proceedings of ITCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study density estimation for classes of shift-invariant distributions over\n$\\mathbb{R}^d$. A multidimensional distribution is \"shift-invariant\" if,\nroughly speaking, it is close in total variation distance to a small shift of\nit in any direction. Shift-invariance relaxes smoothness assumptions commonly\nused in non-parametric density estimation to allow jump discontinuities. The\ndifferent classes of distributions that we consider correspond to different\nrates of tail decay.\n  For each such class we give an efficient algorithm that learns any\ndistribution in the class from independent samples with respect to total\nvariation distance. As a special case of our general result, we show that\n$d$-dimensional shift-invariant distributions which satisfy an exponential tail\nbound can be learned to total variation distance error $\\epsilon$ using\n$\\tilde{O}_d(1/ \\epsilon^{d+2})$ examples and $\\tilde{O}_d(1/ \\epsilon^{2d+2})$\ntime. This implies that, for constant $d$, multivariate log-concave\ndistributions can be learned in $\\tilde{O}_d(1/\\epsilon^{2d+2})$ time using\n$\\tilde{O}_d(1/\\epsilon^{d+2})$ samples, answering a question of [Diakonikolas,\nKane and Stewart, 2016] All of our results extend to a model of noise-tolerant\ndensity estimation using Huber's contamination model, in which the target\ndistribution to be learned is a $(1-\\epsilon,\\epsilon)$ mixture of some unknown\ndistribution in the class with some other arbitrary and unknown distribution,\nand the learning algorithm must output a hypothesis distribution with total\nvariation distance error $O(\\epsilon)$ from the target distribution. We show\nthat our general results are close to best possible by proving a simple\n$\\Omega\\left(1/\\epsilon^d\\right)$ information-theoretic lower bound on sample\ncomplexity even for learning bounded distributions that are shift-invariant.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 02:29:43 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["De", "Anindya", ""], ["Long", "Philip M.", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1811.03748", "submitter": "Umair Mohammad", "authors": "Umair Mohammad and Sameh Sorour", "title": "Adaptive Task Allocation for Mobile Edge Learning", "comments": "8 pages, 2 figures, submitted to IEEE WCNC Workshop 2019, Morocco", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to establish a new optimization paradigm for implementing\nrealistic distributed learning algorithms, with performance guarantees, on\nwireless edge nodes with heterogeneous computing and communication capacities.\nWe will refer to this new paradigm as `Mobile Edge Learning (MEL)'. The problem\nof dynamic task allocation for MEL is considered in this paper with the aim to\nmaximize the learning accuracy, while guaranteeing that the total times of data\ndistribution/aggregation over heterogeneous channels, and local computing\niterations at the heterogeneous nodes, are bounded by a preset duration. The\nproblem is first formulated as a quadratically-constrained integer linear\nproblem. Being an NP-hard problem, the paper relaxes it into a non-convex\nproblem over real variables. We thus proposed two solutions based on deriving\nanalytical upper bounds of the optimal solution of this relaxed problem using\nLagrangian analysis and KKT conditions, and the use of suggest-and-improve\nstarting from equal batch allocation, respectively. The merits of these\nproposed solutions are exhibited by comparing their performances to both\nnumerical approaches and the equal task allocation approach.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 02:49:53 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 21:53:59 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Mohammad", "Umair", ""], ["Sorour", "Sameh", ""]]}, {"id": "1811.03751", "submitter": "Sailik Sengupta", "authors": "Niharika Jain, Lydia Manikonda, Alberto Olmo Hernandez, Sailik\n  Sengupta and Subbarao Kambhampati", "title": "Imagining an Engineer: On GAN-Based Data Augmentation Perpetuating\n  Biases", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of synthetic data generated by Generative Adversarial Networks (GANs)\nhas become quite a popular method to do data augmentation for many\napplications. While practitioners celebrate this as an economical way to get\nmore synthetic data that can be used to train downstream classifiers, it is not\nclear that they recognize the inherent pitfalls of this technique. In this\npaper, we aim to exhort practitioners against deriving any false sense of\nsecurity against data biases based on data augmentation. To drive this point\nhome, we show that starting with a dataset consisting of head-shots of\nengineering researchers, GAN-based augmentation \"imagines\" synthetic engineers,\nmost of whom have masculine features and white skin color (inferred from a\nhuman subject study conducted on Amazon Mechanical Turk). This demonstrates how\nbiases inherent in the training data are reinforced, and sometimes even\namplified, by GAN-based data augmentation; it should serve as a cautionary tale\nfor the lay practitioners.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 03:02:35 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Jain", "Niharika", ""], ["Manikonda", "Lydia", ""], ["Hernandez", "Alberto Olmo", ""], ["Sengupta", "Sailik", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1811.03752", "submitter": "Naoto Sato", "authors": "Naoto Sato, Hironobu Kuruma, Masanori Kaneko, Yuichiroh Nakagawa,\n  Hideto Ogawa, Thai Son Hoang, Michael Butler", "title": "DeepSaucer: Unified Environment for Verifying Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a number of methods for verifying DNNs have been developed.\nBecause the approaches of the methods differ and have their own limitations, we\nthink that a number of verification methods should be applied to a developed\nDNN. To apply a number of methods to the DNN, it is necessary to translate\neither the implementation of the DNN or the verification method so that one\nruns in the same environment as the other. Since those translations are\ntime-consuming, a utility tool, named DeepSaucer, which helps to retain and\nreuse implementations of DNNs, verification methods, and their environments, is\nproposed. In DeepSaucer, code snippets of loading DNNs, running verification\nmethods, and creating their environments are retained and reused as software\nassets in order to reduce cost of verifying DNNs. The feasibility of DeepSaucer\nis confirmed by implementing it on the basis of Anaconda, which provides\nvirtual environment for loading a DNN and running a verification method. In\naddition, the effectiveness of DeepSaucer is demonstrated by usecase examples.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 03:09:28 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Sato", "Naoto", ""], ["Kuruma", "Hironobu", ""], ["Kaneko", "Masanori", ""], ["Nakagawa", "Yuichiroh", ""], ["Ogawa", "Hideto", ""], ["Hoang", "Thai Son", ""], ["Butler", "Michael", ""]]}, {"id": "1811.03760", "submitter": "Youru Li", "authors": "Youru Li, Zhenfeng Zhu, Deqiang Kong, Hua Han, Yao Zhao", "title": "EA-LSTM: Evolutionary Attention-based LSTM for Time Series Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series prediction with deep learning methods, especially long short-term\nmemory neural networks (LSTMs), have scored significant achievements in recent\nyears. Despite the fact that the LSTMs can help to capture long-term\ndependencies, its ability to pay different degree of attention on sub-window\nfeature within multiple time-steps is insufficient. To address this issue, an\nevolutionary attention-based LSTM training with competitive random search is\nproposed for multivariate time series prediction. By transferring shared\nparameters, an evolutionary attention learning approach is introduced to the\nLSTMs model. Thus, like that for biological evolution, the pattern for\nimportance-based attention sampling can be confirmed during temporal\nrelationship mining. To refrain from being trapped into partial optimization\nlike traditional gradient-based methods, an evolutionary computation inspired\ncompetitive random search method is proposed, which can well configure the\nparameters in the attention layer. Experimental results have illustrated that\nthe proposed model can achieve competetive prediction performance compared with\nother baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 03:42:36 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Li", "Youru", ""], ["Zhu", "Zhenfeng", ""], ["Kong", "Deqiang", ""], ["Han", "Hua", ""], ["Zhao", "Yao", ""]]}, {"id": "1811.03761", "submitter": "Liping Li", "authors": "Liping Li, Wei Xu, Tianyi Chen, Georgios B. Giannakis, and Qing Ling", "title": "RSA: Byzantine-Robust Stochastic Aggregation Methods for Distributed\n  Learning from Heterogeneous Datasets", "comments": "To appear in AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a class of robust stochastic subgradient methods\nfor distributed learning from heterogeneous datasets at presence of an unknown\nnumber of Byzantine workers. The Byzantine workers, during the learning\nprocess, may send arbitrary incorrect messages to the master due to data\ncorruptions, communication failures or malicious attacks, and consequently bias\nthe learned model. The key to the proposed methods is a regularization term\nincorporated with the objective function so as to robustify the learning task\nand mitigate the negative effects of Byzantine attacks. The resultant\nsubgradient-based algorithms are termed Byzantine-Robust Stochastic Aggregation\nmethods, justifying our acronym RSA used henceforth. In contrast to most of the\nexisting algorithms, RSA does not rely on the assumption that the data are\nindependent and identically distributed (i.i.d.) on the workers, and hence fits\nfor a wider class of applications. Theoretically, we show that: i) RSA\nconverges to a near-optimal solution with the learning error dependent on the\nnumber of Byzantine workers; ii) the convergence rate of RSA under Byzantine\nattacks is the same as that of the stochastic gradient descent method, which is\nfree of Byzantine attacks. Numerically, experiments on real dataset corroborate\nthe competitive performance of RSA and a complexity reduction compared to the\nstate-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 03:46:46 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 08:01:36 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Li", "Liping", ""], ["Xu", "Wei", ""], ["Chen", "Tianyi", ""], ["Giannakis", "Georgios B.", ""], ["Ling", "Qing", ""]]}, {"id": "1811.03762", "submitter": "Yonggyu Park", "authors": "Yonggyu Park, Junhyun Lee, Yookyung Koh, Inyeop Lee, Jinhyuk Lee,\n  Jaewoo Kang", "title": "Typeface Completion with Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mood of a text and the intention of the writer can be reflected in the\ntypeface. However, in designing a typeface, it is difficult to keep the style\nof various characters consistent, especially for languages with lots of\nmorphological variations such as Chinese. In this paper, we propose a Typeface\nCompletion Network (TCN) which takes one character as an input, and\nautomatically completes the entire set of characters in the same style as the\ninput characters. Unlike existing models proposed for image-to-image\ntranslation, TCN embeds a character image into two separate vectors\nrepresenting typeface and content. Combined with a reconstruction loss from the\nlatent space, and with other various losses, TCN overcomes the inherent\ndifficulty in designing a typeface. Also, compared to previous image-to-image\ntranslation models, TCN generates high quality character images of the same\ntypeface with a much smaller number of model parameters. We validate our\nproposed model on the Chinese and English character datasets, which is paired\ndata, and the CelebA dataset, which is unpaired data. In these datasets, TCN\noutperforms recently proposed state-of-the-art models for image-to-image\ntranslation. The source code of our model is available at\nhttps://github.com/yongqyu/TCN.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 03:49:12 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 13:01:12 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Park", "Yonggyu", ""], ["Lee", "Junhyun", ""], ["Koh", "Yookyung", ""], ["Lee", "Inyeop", ""], ["Lee", "Jinhyuk", ""], ["Kang", "Jaewoo", ""]]}, {"id": "1811.03804", "submitter": "Simon Du", "authors": "Simon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, Xiyu Zhai", "title": "Gradient Descent Finds Global Minima of Deep Neural Networks", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient descent finds a global minimum in training deep neural networks\ndespite the objective function being non-convex. The current paper proves\ngradient descent achieves zero training loss in polynomial time for a deep\nover-parameterized neural network with residual connections (ResNet). Our\nanalysis relies on the particular structure of the Gram matrix induced by the\nneural network architecture. This structure allows us to show the Gram matrix\nis stable throughout the training process and this stability implies the global\noptimality of the gradient descent algorithm. We further extend our analysis to\ndeep residual convolutional neural networks and obtain a similar convergence\nresult.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 07:39:59 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 05:31:53 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 03:35:26 GMT"}, {"version": "v4", "created": "Tue, 28 May 2019 19:01:22 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Du", "Simon S.", ""], ["Lee", "Jason D.", ""], ["Li", "Haochuan", ""], ["Wang", "Liwei", ""], ["Zhai", "Xiyu", ""]]}, {"id": "1811.03821", "submitter": "Mingxiao An", "authors": "Mingxiao An, Yongzhou Chen, Qi Liu, Chuanren Liu, Guangyi Lv, Fangzhao\n  Wu, Jianhui Ma", "title": "Skeptical Deep Learning with Distribution Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep neural networks have been successfully used for various\nclassification tasks, especially for problems with massive perfectly labeled\ntraining data. However, it is often costly to have large-scale credible labels\nin real-world applications. One solution is to make supervised learning robust\nwith imperfectly labeled input. In this paper, we develop a distribution\ncorrection approach that allows deep neural networks to avoid overfitting\nimperfect training data. Specifically, we treat the noisy input as samples from\nan incorrect distribution, which will be automatically corrected during our\ntraining process. We test our approach on several classification datasets with\nelaborately generated noisy labels. The results show significantly higher\nprediction and recovery accuracy with our approach compared to alternative\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 09:07:06 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2018 08:25:23 GMT"}, {"version": "v3", "created": "Sun, 13 Jan 2019 06:08:55 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["An", "Mingxiao", ""], ["Chen", "Yongzhou", ""], ["Liu", "Qi", ""], ["Liu", "Chuanren", ""], ["Lv", "Guangyi", ""], ["Wu", "Fangzhao", ""], ["Ma", "Jianhui", ""]]}, {"id": "1811.03850", "submitter": "Erwan Le Merrer", "authors": "Corentin Hardy, Erwan Le Merrer, Bruno Sericola", "title": "MD-GAN: Multi-Discriminator Generative Adversarial Networks for\n  Distributed Datasets", "comments": "To be published in IPDPS 2019: the 33rd IEEE International Parallel &\n  Distributed Processing Symposium", "journal-ref": "IEEE International Parallel and Distributed Processing Symposium\n  (IPDPS) 2019", "doi": "10.1109/IPDPS.2019.00095", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent technical breakthrough in the domain of machine learning is the\ndiscovery and the multiple applications of Generative Adversarial Networks\n(GANs). Those generative models are computationally demanding, as a GAN is\ncomposed of two deep neural networks, and because it trains on large datasets.\nA GAN is generally trained on a single server.\n  In this paper, we address the problem of distributing GANs so that they are\nable to train over datasets that are spread on multiple workers. MD-GAN is\nexposed as the first solution for this problem: we propose a novel learning\nprocedure for GANs so that they fit this distributed setup. We then compare the\nperformance of MD-GAN to an adapted version of Federated Learning to GANs,\nusing the MNIST and CIFAR10 datasets. MD-GAN exhibits a reduction by a factor\nof two of the learning complexity on each worker node, while providing better\nperformances than federated learning on both datasets. We finally discuss the\npractical implications of distributing GANs.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 10:24:35 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 15:41:49 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Hardy", "Corentin", ""], ["Merrer", "Erwan Le", ""], ["Sericola", "Bruno", ""]]}, {"id": "1811.03853", "submitter": "Qiming Zou", "authors": "Qiming Zou, Ling Wang, Ke Lu, Yu Li", "title": "Sample-Efficient Policy Learning based on Completely Behavior Cloning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct policy search is one of the most important algorithm of reinforcement\nlearning. However, learning from scratch needs a large amount of experience\ndata and can be easily prone to poor local optima. In addition to that, a\npartially trained policy tends to perform dangerous action to agent and\nenvironment. In order to overcome these challenges, this paper proposed a\npolicy initialization algorithm called Policy Learning based on Completely\nBehavior Cloning (PLCBC). PLCBC first transforms the Model Predictive Control\n(MPC) controller into a piecewise affine (PWA) function using multi-parametric\nprogramming, and uses a neural network to express this function. By this way,\nPLCBC can completely clone the MPC controller without any performance loss, and\nis totally training-free. The experiments show that this initialization\nstrategy can help agent learn at the high reward state region, and converge\nfaster and better.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 10:34:53 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Zou", "Qiming", ""], ["Wang", "Ling", ""], ["Lu", "Ke", ""], ["Li", "Yu", ""]]}, {"id": "1811.03862", "submitter": "David Gaudrie", "authors": "David Gaudrie, Rodolphe Le Riche, Victor Picheny, Benoit Enaux,\n  Vincent Herbert", "title": "Targeting Solutions in Bayesian Multi-Objective Optimization: Sequential\n  and Batch Versions", "comments": null, "journal-ref": "Annals of Mathematics and Artificial Intelligence volume 88, pages\n  187-212(2020)", "doi": "10.1007/s10472-019-09644-8", "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-objective optimization aims at finding trade-off solutions to\nconflicting objectives. These constitute the Pareto optimal set. In the context\nof expensive-to-evaluate functions, it is impossible and often non-informative\nto look for the entire set. As an end-user would typically prefer a certain\npart of the objective space, we modify the Bayesian multi-objective\noptimization algorithm which uses Gaussian Processes to maximize the Expected\nHypervolume Improvement, to focus the search in the preferred region. The\ncumulated effects of the Gaussian Processes and the targeting strategy lead to\na particularly efficient convergence to the desired part of the Pareto set. To\ntake advantage of parallel computing, a multi-point extension of the targeting\ncriterion is proposed and analyzed.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 11:03:54 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 09:00:00 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 14:14:34 GMT"}, {"version": "v4", "created": "Thu, 29 Aug 2019 14:04:02 GMT"}, {"version": "v5", "created": "Wed, 19 Feb 2020 08:02:44 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Gaudrie", "David", ""], ["Riche", "Rodolphe Le", ""], ["Picheny", "Victor", ""], ["Enaux", "Benoit", ""], ["Herbert", "Vincent", ""]]}, {"id": "1811.03875", "submitter": "Herman Kamper", "authors": "Ryan Eloff, Herman A. Engelbrecht, Herman Kamper", "title": "Multimodal One-Shot Learning of Speech and Images", "comments": "5 pages, 1 figure, 3 tables; accepted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagine a robot is shown new concepts visually together with spoken tags,\ne.g. \"milk\", \"eggs\", \"butter\". After seeing one paired audio-visual example per\nclass, it is shown a new set of unseen instances of these objects, and asked to\npick the \"milk\". Without receiving any hard labels, could it learn to match the\nnew continuous speech input to the correct visual instance? Although unimodal\none-shot learning has been studied, where one labelled example in a single\nmodality is given per class, this example motivates multimodal one-shot\nlearning. Our main contribution is to formally define this task, and to propose\nseveral baseline and advanced models. We use a dataset of paired spoken and\nvisual digits to specifically investigate recent advances in Siamese\nconvolutional neural networks. Our best Siamese model achieves twice the\naccuracy of a nearest neighbour model using pixel-distance over images and\ndynamic time warping over speech in 11-way cross-modal matching.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 12:14:20 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 15:08:03 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Eloff", "Ryan", ""], ["Engelbrecht", "Herman A.", ""], ["Kamper", "Herman", ""]]}, {"id": "1811.03883", "submitter": "Duo Zhang", "authors": "Duo Zhang, Geir Lindholm, Nicolas Martinez, Harsha Ratnaweera", "title": "Exploiting Capacity of Sewer System Using Unsupervised Learning\n  Algorithms Combined with Dimensionality Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting capacity of sewer system using decentralized control is a cost\neffective mean of minimizing the overflow. Given the size of the real sewer\nsystem, exploiting all the installed control structures in the sewer pipes can\nbe challenging. This paper presents a divide and conquer solution to implement\ndecentralized control measures based on unsupervised learning algorithms. A\nsewer system is first divided into a number of subcatchments. A series of\nnatural and built factors that have the impact on sewer system performance is\nthen collected. Clustering algorithms are then applied to grouping\nsubcatchments with similar hydraulic hydrologic characteristics. Following\nwhich, principal component analysis is performed to interpret the main features\nof sub-catchment groups and identify priority control locations. Overflows\nunder different control scenarios are compared based on the hydraulic model.\nSimulation results indicate that priority control applied to the most suitable\ncluster could bring the most profitable result.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 12:39:53 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Zhang", "Duo", ""], ["Lindholm", "Geir", ""], ["Martinez", "Nicolas", ""], ["Ratnaweera", "Harsha", ""]]}, {"id": "1811.03895", "submitter": "Sultan Javed Majeed", "authors": "Sultan Javed Majeed and Marcus Hutter", "title": "Performance Guarantees for Homomorphisms Beyond Markov Decision\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most real-world problems have huge state and/or action spaces. Therefore, a\nnaive application of existing tabular solution methods is not tractable on such\nproblems. Nonetheless, these solution methods are quite useful if an agent has\naccess to a relatively small state-action space homomorphism of the true\nenvironment and near-optimal performance is guaranteed by the map. A plethora\nof research is focused on the case when the homomorphism is a Markovian\nrepresentation of the underlying process. However, we show that near-optimal\nperformance is sometimes guaranteed even if the homomorphism is non-Markovian.\nMoreover, we can aggregate significantly more states by lifting the Markovian\nrequirement without compromising on performance. In this work, we expand\nExtreme State Aggregation (ESA) framework to joint state-action aggregations.\nWe also lift the policy uniformity condition for aggregation in ESA that allows\neven coarser modeling of the true environment.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 13:39:16 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Majeed", "Sultan Javed", ""], ["Hutter", "Marcus", ""]]}, {"id": "1811.03897", "submitter": "Patric Fulop", "authors": "Remus Pop, Patric Fulop", "title": "Deep Ensemble Bayesian Active Learning : Addressing the Mode Collapse\n  issue in Monte Carlo dropout via Ensembles", "comments": "ICLR under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image classification tasks, the ability of deep CNNs to deal with complex\nimage data has proven to be unrivalled. However, they require large amounts of\nlabeled training data to reach their full potential. In specialised domains\nsuch as healthcare, labeled data can be difficult and expensive to obtain.\nActive Learning aims to alleviate this problem, by reducing the amount of\nlabelled data needed for a specific task while delivering satisfactory\nperformance. We propose DEBAL, a new active learning strategy designed for deep\nneural networks. This method improves upon the current state-of-the-art deep\nBayesian active learning method, which suffers from the mode collapse problem.\nWe correct for this deficiency by making use of the expressive power and\nstatistical properties of model ensembles. Our proposed method manages to\ncapture superior data uncertainty, which translates into improved\nclassification performance. We demonstrate empirically that our ensemble method\nyields faster convergence of CNNs trained on the MNIST and CIFAR-10 datasets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 13:44:12 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Pop", "Remus", ""], ["Fulop", "Patric", ""]]}, {"id": "1811.03909", "submitter": "Athanasios Davvetas", "authors": "Athanasios Davvetas, Iraklis A. Klampanos and Vangelis Karkaletsis", "title": "Evidence Transfer for Improving Clustering Tasks Using External\n  Categorical Evidence", "comments": null, "journal-ref": null, "doi": "10.1109/IJCNN.2019.8852384", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce evidence transfer for clustering, a deep learning\nmethod that can incrementally manipulate the latent representations of an\nautoencoder, according to external categorical evidence, in order to improve a\nclustering outcome. By evidence transfer we define the process by which the\ncategorical outcome of an external, auxiliary task is exploited to improve a\nprimary task, in this case representation learning for clustering. Our proposed\nmethod makes no assumptions regarding the categorical evidence presented, nor\nthe structure of the latent space. We compare our method, against the baseline\nsolution by performing k-means clustering before and after its deployment.\nExperiments with three different kinds of evidence show that our method\neffectively manipulates the latent representations when introduced with real\ncorresponding evidence, while remaining robust when presented with low quality\nevidence.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 14:10:18 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 19:39:04 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Davvetas", "Athanasios", ""], ["Klampanos", "Iraklis A.", ""], ["Karkaletsis", "Vangelis", ""]]}, {"id": "1811.03962", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yuanzhi Li, Zhao Song", "title": "A Convergence Theory for Deep Learning via Over-Parameterization", "comments": "V2 adds citation and V3/V4/V5 polish writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have demonstrated dominating performance in many\nfields; since AlexNet, networks used in practice are going wider and deeper. On\nthe theoretical side, a long line of works has been focusing on training neural\nnetworks with one hidden layer. The theory of multi-layer networks remains\nlargely unsettled.\n  In this work, we prove why stochastic gradient descent (SGD) can find\n$\\textit{global minima}$ on the training objective of DNNs in\n$\\textit{polynomial time}$. We only make two assumptions: the inputs are\nnon-degenerate and the network is over-parameterized. The latter means the\nnetwork width is sufficiently large: $\\textit{polynomial}$ in $L$, the number\nof layers and in $n$, the number of samples.\n  Our key technique is to derive that, in a sufficiently large neighborhood of\nthe random initialization, the optimization landscape is almost-convex and\nsemi-smooth even with ReLU activations. This implies an equivalence between\nover-parameterized neural networks and neural tangent kernel (NTK) in the\nfinite (and polynomial) width setting.\n  As concrete examples, starting from randomly initialized weights, we prove\nthat SGD can attain 100% training accuracy in classification tasks, or minimize\nregression loss in linear convergence speed, with running time polynomial in\n$n,L$. Our theory applies to the widely-used but non-smooth ReLU activation,\nand to any smooth and possibly non-convex loss functions. In terms of network\narchitectures, our theory at least applies to fully-connected neural networks,\nconvolutional neural networks (CNN), and residual neural networks (ResNet).\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 15:16:13 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 18:54:20 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 11:44:07 GMT"}, {"version": "v4", "created": "Mon, 4 Feb 2019 03:57:59 GMT"}, {"version": "v5", "created": "Mon, 17 Jun 2019 06:39:04 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""], ["Song", "Zhao", ""]]}, {"id": "1811.03963", "submitter": "Cong Chen", "authors": "Ching-Yun Ko, Cong Chen, Yuke Zhang, Kim Batselier, Ngai Wong", "title": "Deep Compression of Sum-Product Networks on Tensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sum-product networks (SPNs) represent an emerging class of neural networks\nwith clear probabilistic semantics and superior inference speed over graphical\nmodels. This work reveals a strikingly intimate connection between SPNs and\ntensor networks, thus leading to a highly efficient representation that we call\ntensor SPNs (tSPNs). For the first time, through mapping an SPN onto a tSPN and\nemploying novel optimization techniques, we demonstrate remarkable parameter\ncompression with negligible loss in accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 15:16:55 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Ko", "Ching-Yun", ""], ["Chen", "Cong", ""], ["Zhang", "Yuke", ""], ["Batselier", "Kim", ""], ["Wong", "Ngai", ""]]}, {"id": "1811.03970", "submitter": "Iftitahu Ni'mah", "authors": "Wenting Xiong, Iftitahu Ni'mah, Juan M. G. Huesca, Werner van\n  Ipenburg, Jan Veldsink, and Mykola Pechenizkiy", "title": "Looking Deeper into Deep Learning Model: Attribution-based Explanations\n  of TextCNN", "comments": "NIPS 2018 Workshop on Challenges and Opportunities for AI in\n  Financial Services: the Impact of Fairness, Explainability, Accuracy, and\n  Privacy, Montr\\'eal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layer-wise Relevance Propagation (LRP) and saliency maps have been recently\nused to explain the predictions of Deep Learning models, specifically in the\ndomain of text classification. Given different attribution-based explanations\nto highlight relevant words for a predicted class label, experiments based on\nword deleting perturbation is a common evaluation method. This word removal\napproach, however, disregards any linguistic dependencies that may exist\nbetween words or phrases in a sentence, which could semantically guide a\nclassifier to a particular prediction. In this paper, we present a\nfeature-based evaluation framework for comparing the two attribution methods on\ncustomer reviews (public data sets) and Customer Due Diligence (CDD) extracted\nreports (corporate data set). Instead of removing words based on the relevance\nscore, we investigate perturbations based on embedded features removal from\nintermediate layers of Convolutional Neural Networks. Our experimental study is\ncarried out on embedded-word, embedded-document, and embedded-ngrams\nexplanations. Using the proposed framework, we provide a visualization tool to\nassist analysts in reasoning toward the model's final prediction.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 18:23:48 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 23:18:23 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Xiong", "Wenting", ""], ["Ni'mah", "Iftitahu", ""], ["Huesca", "Juan M. G.", ""], ["van Ipenburg", "Werner", ""], ["Veldsink", "Jan", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "1811.03980", "submitter": "Alberto Marchisio", "authors": "Alberto Marchisio, Muhammad Abdullah Hanif, Semeen Rehman, Maurizio\n  Martina, and Muhammad Shafique", "title": "A Methodology for Automatic Selection of Activation Functions to Design\n  Hybrid Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activation functions influence behavior and performance of DNNs. Nonlinear\nactivation functions, like Rectified Linear Units (ReLU), Exponential Linear\nUnits (ELU) and Scaled Exponential Linear Units (SELU), outperform the linear\ncounterparts. However, selecting an appropriate activation function is a\nchallenging problem, as it affects the accuracy and the complexity of the given\nDNN. In this paper, we propose a novel methodology to automatically select the\nbest-possible activation function for each layer of a given DNN, such that the\noverall DNN accuracy, compared to considering only one type of activation\nfunction for the whole DNN, is improved. However, an associated scientific\nchallenge in exploring all the different configurations of activation functions\nwould be time and resource-consuming. Towards this, our methodology identifies\nthe Evaluation Points during learning to evaluate the accuracy in an\nintermediate step of training and to perform early termination by checking the\naccuracy gradient of the learning curve. This helps in significantly reducing\nthe exploration time during training. Moreover, our methodology selects, for\neach layer, the dropout rate that optimizes the accuracy. Experiments show that\nwe are able to achieve on average 7% to 15% Relative Error Reduction on MNIST,\nCIFAR-10 and CIFAR-100 benchmarks, with limited performance and power penalty\non GPUs.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 14:30:58 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Marchisio", "Alberto", ""], ["Hanif", "Muhammad Abdullah", ""], ["Rehman", "Semeen", ""], ["Martina", "Maurizio", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1811.04006", "submitter": "Stanislav Fedorov", "authors": "Stanislav Fedorov, Antonio Candelieri", "title": "Reachability-based safe learning for optimal control problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we seek for an approach to integrate safety in the learning\nprocess that relies on a partly known state-space model of the system and\nregards the unknown dynamics as an additive bounded disturbance. We introduce a\nframework for safely learning a control strategy for a given system with an\nadditive disturbance. On the basis of the known part of the model, a safe set\nin which the system can learn safely, the algorithm can choose optimal actions\nfor pursuing the target set as long as the safety-preserving condition is\nsatisfied. After some learning episodes, the disturbance can be updated based\non real-world data. To this end, Gaussian Process regression is conducted on\nthe collected disturbance samples. Since the unstable nature of the law of the\nreal world, for example, change of friction or conductivity with the\ntemperature, we expect to have the more robust solution of optimal control\nproblem.\n  For evaluation of approach described above we choose an inverted pendulum as\na benchmark model. The proposed algorithm manages to learn a policy that does\nnot violate the pre-specified safety constraints. Observed performance is\nimproved when it was incorporated exploration set up to make sure that an\noptimal policy is learned everywhere in the safe set. Finally, we outline some\npromising directions for future research beyond the scope of this paper.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 16:37:59 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Fedorov", "Stanislav", ""], ["Candelieri", "Antonio", ""]]}, {"id": "1811.04017", "submitter": "Th\\'eo Ryffel", "authors": "Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso,\n  Daniel Rueckert and Jonathan Passerat-Palmbach", "title": "A generic framework for privacy preserving deep learning", "comments": "PPML 2018, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We detail a new framework for privacy preserving deep learning and discuss\nits assets. The framework puts a premium on ownership and secure processing of\ndata and introduces a valuable representation based on chains of commands and\ntensors. This abstraction allows one to implement complex privacy preserving\nconstructs such as Federated Learning, Secure Multiparty Computation, and\nDifferential Privacy while still exposing a familiar deep learning API to the\nend-user. We report early results on the Boston Housing and Pima Indian\nDiabetes datasets. While the privacy features apart from Differential Privacy\ndo not impact the prediction accuracy, the current implementation of the\nframework introduces a significant overhead in performance, which will be\naddressed at a later stage of the development. We believe this work is an\nimportant milestone introducing the first reliable, general framework for\nprivacy preserving deep learning.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 17:10:47 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 18:11:15 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Ryffel", "Theo", ""], ["Trask", "Andrew", ""], ["Dahl", "Morten", ""], ["Wagner", "Bobby", ""], ["Mancuso", "Jason", ""], ["Rueckert", "Daniel", ""], ["Passerat-Palmbach", "Jonathan", ""]]}, {"id": "1811.04022", "submitter": "Yiran Wang", "authors": "Gunther Uhlmann and Yiran Wang", "title": "Convolutional neural networks in phase space and inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study inverse problems consisting on determining medium properties using\nthe responses to probing waves from the machine learning point of view. Based\non the understanding of propagation of waves and their nonlinear interactions,\nwe construct a deep convolutional neural network in which the parameters are\nused to classify and reconstruct the coefficients of nonlinear wave equations\nthat model the medium properties. Furthermore, for given approximation\naccuracy, we obtain the depth and number of units of the network and their\nquantitative dependence on the complexity of the medium.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 17:17:07 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Uhlmann", "Gunther", ""], ["Wang", "Yiran", ""]]}, {"id": "1811.04026", "submitter": "Yibo Yang", "authors": "Yibo Yang, Paris Perdikaris", "title": "Adversarial Uncertainty Quantification in Physics-Informed Neural\n  Networks", "comments": "This paper has been submitted to Journal of Computational Physics. 33\n  pages, 7 figures", "journal-ref": null, "doi": "10.1016/j.jcp.2019.05.027", "report-no": null, "categories": "stat.ML cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning framework for quantifying and propagating\nuncertainty in systems governed by non-linear differential equations using\nphysics-informed neural networks. Specifically, we employ latent variable\nmodels to construct probabilistic representations for the system states, and\nput forth an adversarial inference procedure for training them on data, while\nconstraining their predictions to satisfy given physical laws expressed by\npartial differential equations. Such physics-informed constraints provide a\nregularization mechanism for effectively training deep generative models as\nsurrogates of physical systems in which the cost of data acquisition is high,\nand training data-sets are typically small. This provides a flexible framework\nfor characterizing uncertainty in the outputs of physical systems due to\nrandomness in their inputs or noise in their observations that entirely\nbypasses the need for repeatedly sampling expensive experiments or numerical\nsimulators. We demonstrate the effectiveness of our approach through a series\nof examples involving uncertainty propagation in non-linear conservation laws,\nand the discovery of constitutive laws for flow through porous media directly\nfrom noisy data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 17:20:31 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Yang", "Yibo", ""], ["Perdikaris", "Paris", ""]]}, {"id": "1811.04028", "submitter": "Philipp Blandfort", "authors": "Philipp Blandfort, J\\\"orn Hees, Desmond U. Patton", "title": "An Overview of Computational Approaches for Interpretation Analysis", "comments": "Preprint submitted to Digital Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is said that beauty is in the eye of the beholder. But how exactly can we\ncharacterize such discrepancies in interpretation? For example, are there any\nspecific features of an image that makes person A regard an image as beautiful\nwhile person B finds the same image displeasing? Such questions ultimately aim\nat explaining our individual ways of interpretation, an intention that has been\nof fundamental importance to the social sciences from the beginning. More\nrecently, advances in computer science brought up two related questions: First,\ncan computational tools be adopted for analyzing ways of interpretation?\nSecond, what if the \"beholder\" is a computer model, i.e., how can we explain a\ncomputer model's point of view? Numerous efforts have been made regarding both\nof these points, while many existing approaches focus on particular aspects and\nare still rather separate. With this paper, in order to connect these\napproaches we introduce a theoretical framework for analyzing interpretation,\nwhich is applicable to interpretation of both human beings and computer models.\nWe give an overview of relevant computational approaches from various fields,\nand discuss the most common and promising application areas. The focus of this\npaper lies on interpretation of text and image data, while many of the\npresented approaches are applicable to other types of data as well.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 17:25:25 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 09:14:59 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Blandfort", "Philipp", ""], ["Hees", "J\u00f6rn", ""], ["Patton", "Desmond U.", ""]]}, {"id": "1811.04047", "submitter": "Hongyang Jia", "authors": "Hongyang Jia, Yinqi Tang, Hossein Valavi, Jintao Zhang and Naveen\n  Verma", "title": "A Microprocessor implemented in 65nm CMOS with Configurable and\n  Bit-scalable Accelerator for Programmable In-memory Computing", "comments": null, "journal-ref": "IEEE Journal of Solid-State Circuits, vol. 55, no. 9, pp.\n  2609-2621, Sept. 2020", "doi": "10.1109/JSSC.2020.2987714", "report-no": null, "categories": "cs.AR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a programmable in-memory-computing processor,\ndemonstrated in a 65nm CMOS technology. For data-centric workloads, such as\ndeep neural networks, data movement often dominates when implemented with\ntoday's computing architectures. This has motivated spatial architectures,\nwhere the arrangement of data-storage and compute hardware is distributed and\nexplicitly aligned to the computation dataflow, most notably for matrix-vector\nmultiplication. In-memory computing is a spatial architecture where processing\nelements correspond to dense bit cells, providing local storage and compute,\ntypically employing analog operation. Though this raises the potential for high\nenergy efficiency and throughput, analog operation has significantly limited\nrobustness, scale, and programmability. This paper describes a 590kb\nin-memory-computing accelerator integrated in a programmable processor\narchitecture, by exploiting recent approaches to charge-domain in-memory\ncomputing. The architecture takes the approach of tight coupling with an\nembedded CPU, through accelerator interfaces enabling integration in the\nstandard processor memory space. Additionally, a near-memory-computing datapath\nboth enables diverse computations locally, to address operations required\nacross applications, and enables bit-precision scalability for\nmatrix/input-vector elements, through a bit-parallel/bit-serial (BP/BS) scheme.\nChip measurements show an energy efficiency of 152/297 1b-TOPS/W and throughput\nof 4.7/1.9 1b-TOPS (scaling linearly with the matrix/input-vector element\nprecisions) at VDD of 1.2/0.85V. Neural network demonstrations with 1-b/4-b\nweights and activations for CIFAR-10 classification consume 5.3/105.2\n$\\mu$J/image at 176/23 fps, with accuracy at the level of digital/software\nimplementation (89.3/92.4 $\\%$ accuracy).\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 18:03:14 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Jia", "Hongyang", ""], ["Tang", "Yinqi", ""], ["Valavi", "Hossein", ""], ["Zhang", "Jintao", ""], ["Verma", "Naveen", ""]]}, {"id": "1811.04060", "submitter": "Marcel Wever", "authors": "Marcel Wever and Felix Mohr and Eyke H\\\"ullermeier", "title": "Automated Multi-Label Classification based on ML-Plan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated machine learning (AutoML) has received increasing attention in the\nrecent past. While the main tools for AutoML, such as Auto-WEKA, TPOT, and\nauto-sklearn, mainly deal with single-label classification and regression,\nthere is very little work on other types of machine learning tasks. In\nparticular, there is almost no work on automating the engineering of machine\nlearning applications for multi-label classification. This paper makes two\ncontributions. First, it discusses the usefulness and feasibility of an AutoML\napproach for multi-label classification. Second, we show how the scope of\nML-Plan, an AutoML-tool for multi-class classification, can be extended towards\nmulti-label classification using MEKA, which is a multi-label extension of the\nwell-known Java library WEKA. The resulting approach recursively refines MEKA's\nmulti-label classifiers, which sometimes nest another multi-label classifier,\nup to the selection of a single-label base learner provided by WEKA. In our\nevaluation, we find that the proposed approach yields superb results and\nperforms significantly better than a set of baselines.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 18:40:35 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Wever", "Marcel", ""], ["Mohr", "Felix", ""], ["H\u00fcllermeier", "Eyke", ""]]}, {"id": "1811.04064", "submitter": "You Lu", "authors": "You Lu, Zhiyuan Liu, Bert Huang", "title": "Block Belief Propagation for Parameter Learning in Markov Random Fields", "comments": "Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional learning methods for training Markov random fields require doing\ninference over all variables to compute the likelihood gradient. The iteration\ncomplexity for those methods therefore scales with the size of the graphical\nmodels. In this paper, we propose \\emph{block belief propagation learning}\n(BBPL), which uses block-coordinate updates of approximate marginals to compute\napproximate gradients, removing the need to compute inference on the entire\ngraphical model. Thus, the iteration complexity of BBPL does not scale with the\nsize of the graphs. We prove that the method converges to the same solution as\nthat obtained by using full inference per iteration, despite these\napproximations, and we empirically demonstrate its scalability improvements\nover standard training methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 18:50:52 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Lu", "You", ""], ["Liu", "Zhiyuan", ""], ["Huang", "Bert", ""]]}, {"id": "1811.04076", "submitter": "Kou Tanaka", "authors": "Kou Tanaka, Hirokazu Kameoka, Takuhiro Kaneko, Nobukatsu Hojo", "title": "AttS2S-VC: Sequence-to-Sequence Voice Conversion with Attention and\n  Context Preservation Mechanisms", "comments": "Submitted to ICASSP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method based on a sequence-to-sequence learning\n(Seq2Seq) with attention and context preservation mechanism for voice\nconversion (VC) tasks. Seq2Seq has been outstanding at numerous tasks involving\nsequence modeling such as speech synthesis and recognition, machine\ntranslation, and image captioning. In contrast to current VC techniques, our\nmethod 1) stabilizes and accelerates the training procedure by considering\nguided attention and proposed context preservation losses, 2) allows not only\nspectral envelopes but also fundamental frequency contours and durations of\nspeech to be converted, 3) requires no context information such as phoneme\nlabels, and 4) requires no time-aligned source and target speech data in\nadvance. In our experiment, the proposed VC framework can be trained in only\none day, using only one GPU of an NVIDIA Tesla K80, while the quality of the\nsynthesized speech is higher than that of speech converted by Gaussian mixture\nmodel-based VC and is comparable to that of speech generated by recurrent\nneural network-based text-to-speech synthesis, which can be regarded as an\nupper limit on VC performance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 05:19:43 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Tanaka", "Kou", ""], ["Kameoka", "Hirokazu", ""], ["Kaneko", "Takuhiro", ""], ["Hojo", "Nobukatsu", ""]]}, {"id": "1811.04104", "submitter": "Vito Michele Leli", "authors": "Vito M. Leli, Saeed Osat, Timur Tlyachev, Dmitry V. Dylov, Jacob D.\n  Biamonte", "title": "Deep Learning Super-Diffusion in Multiplex Networks", "comments": "12 pages, 6 figures", "journal-ref": "Journal of Physics: Complexity 2(3), 035011 (2021)", "doi": "10.1088/2632-072X/abe6e9", "report-no": null, "categories": "physics.soc-ph cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex network theory has shown success in understanding the emergent and\ncollective behavior of complex systems [1]. Many real-world complex systems\nwere recently discovered to be more accurately modeled as multiplex networks\n[2-6]---in which each interaction type is mapped to its own network layer;\ne.g.~multi-layer transportation networks, coupled social networks, metabolic\nand regulatory networks, etc. A salient physical phenomena emerging from\nmultiplexity is super-diffusion: exhibited by an accelerated diffusion admitted\nby the multi-layer structure as compared to any single layer. Theoretically\nsuper-diffusion was only known to be predicted using the spectral gap of the\nfull Laplacian of a multiplex network and its interacting layers. Here we turn\nto machine learning which has developed techniques to recognize, classify, and\ncharacterize complex sets of data. We show that modern machine learning\narchitectures, such as fully connected and convolutional neural networks, can\nclassify and predict the presence of super-diffusion in multiplex networks with\n94.12\\% accuracy. Such predictions can be done {\\it in situ}, without the need\nto determine spectral properties of a network.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 19:18:07 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2020 14:10:27 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 08:29:18 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Leli", "Vito M.", ""], ["Osat", "Saeed", ""], ["Tlyachev", "Timur", ""], ["Dylov", "Dmitry V.", ""], ["Biamonte", "Jacob D.", ""]]}, {"id": "1811.04115", "submitter": "Soumyabrata Dev", "authors": "Murhaf Hossari, Soumyabrata Dev, Matthew Nicholson, Killian McCabe,\n  Atul Nautiyal, Clare Conran, Jian Tang, Wei Xu and Fran\\c{c}ois Piti\\'e", "title": "ADNet: A Deep Network for Detecting Adverts", "comments": "Published in Proc. 26th Irish Conference on Artificial Intelligence\n  and Cognitive Science (AICS 2018), First two authors contributed equally to\n  this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online video advertising gives content providers the ability to deliver\ncompelling content, reach a growing audience, and generate additional revenue\nfrom online media. Recently, advertising strategies are designed to look for\noriginal advert(s) in a video frame, and replacing them with new adverts. These\nstrategies, popularly known as product placement or embedded marketing, greatly\nhelp the marketing agencies to reach out to a wider audience. However, in the\nexisting literature, such detection of candidate frames in a video sequence for\nthe purpose of advert integration, is done manually. In this paper, we propose\na deep-learning architecture called ADNet, that automatically detects the\npresence of advertisements in video frames. Our approach is the first of its\nkind that automatically detects the presence of adverts in a video frame, and\nachieves state-of-the-art results on a public dataset.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 19:41:56 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Hossari", "Murhaf", ""], ["Dev", "Soumyabrata", ""], ["Nicholson", "Matthew", ""], ["McCabe", "Killian", ""], ["Nautiyal", "Atul", ""], ["Conran", "Clare", ""], ["Tang", "Jian", ""], ["Xu", "Wei", ""], ["Piti\u00e9", "Fran\u00e7ois", ""]]}, {"id": "1811.04127", "submitter": "Teodor Vanislavov Marinov", "authors": "Raman Arora, Michael Dinitz, Teodor V. Marinov, Mehryar Mohri", "title": "Policy Regret in Repeated Games", "comments": "Camera ready from NeurIPS 2018; 25 pages; Slightly updated results\n  and proofs for Section 3 and Section 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of \\emph{policy regret} in online learning is a well defined?\nperformance measure for the common scenario of adaptive adversaries, which more\ntraditional quantities such as external regret do not take into account. We\nrevisit the notion of policy regret and first show that there are online\nlearning settings in which policy regret and external regret are incompatible:\nany sequence of play that achieves a favorable regret with respect to one\ndefinition must do poorly with respect to the other. We then focus on the\ngame-theoretic setting where the adversary is a self-interested agent. In that\nsetting, we show that external regret and policy regret are not in conflict\nand, in fact, that a wide class of algorithms can ensure a favorable regret\nwith respect to both definitions, so long as the adversary is also using such\nan algorithm. We also show that the sequence of play of no-policy regret\nalgorithms converges to a \\emph{policy equilibrium}, a new notion of\nequilibrium that we introduce. Relating this back to external regret, we show\nthat coarse correlated equilibria, which no-external regret players converge\nto, are a strict subset of policy equilibria. Thus, in game-theoretic settings,\nevery sequence of play with no external regret also admits no policy regret,\nbut the converse does not hold.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 20:30:09 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 18:30:19 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Arora", "Raman", ""], ["Dinitz", "Michael", ""], ["Marinov", "Teodor V.", ""], ["Mohri", "Mehryar", ""]]}, {"id": "1811.04132", "submitter": "Monireh Ebrahimi", "authors": "Monireh Ebrahimi, Md Kamruzzaman Sarker, Federico Bianchi, Ning Xie,\n  Derek Doran, Pascal Hitzler", "title": "Reasoning over RDF Knowledge Bases using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Semantic Web knowledge representation standards, and in particular RDF and\nOWL, often come endowed with a formal semantics which is considered to be of\nfundamental importance for the field. Reasoning, i.e., the drawing of logical\ninferences from knowledge expressed in such standards, is traditionally based\non logical deductive methods and algorithms which can be proven to be sound and\ncomplete and terminating, i.e. correct in a very strong sense. For various\nreasons, though, in particular, the scalability issues arising from the\never-increasing amounts of Semantic Web data available and the inability of\ndeductive algorithms to deal with noise in the data, it has been argued that\nalternative means of reasoning should be investigated which bear high promise\nfor high scalability and better robustness. From this perspective, deductive\nalgorithms can be considered the gold standard regarding correctness against\nwhich alternative methods need to be tested. In this paper, we show that it is\npossible to train a Deep Learning system on RDF knowledge graphs, such that it\nis able to perform reasoning over new RDF knowledge graphs, with high precision\nand recall compared to the deductive gold standard.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 21:00:46 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Ebrahimi", "Monireh", ""], ["Sarker", "Md Kamruzzaman", ""], ["Bianchi", "Federico", ""], ["Xie", "Ning", ""], ["Doran", "Derek", ""], ["Hitzler", "Pascal", ""]]}, {"id": "1811.04133", "submitter": "Efthymios Tzinis", "authors": "Efthymios Tzinis, Georgios Paraskevopoulos, Christos Baziotis and\n  Alexandros Potamianos", "title": "Integrating Recurrence Dynamics for Speech Emotion Recognition", "comments": null, "journal-ref": "Proc. Interspeech 2018, pp. 927-931", "doi": "10.21437/Interspeech.2018-1377", "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the performance of features that can capture nonlinear\nrecurrence dynamics embedded in the speech signal for the task of Speech\nEmotion Recognition (SER). Reconstruction of the phase space of each speech\nframe and the computation of its respective Recurrence Plot (RP) reveals\ncomplex structures which can be measured by performing Recurrence\nQuantification Analysis (RQA). These measures are aggregated by using\nstatistical functionals over segment and utterance periods. We report SER\nresults for the proposed feature set on three databases using different\nclassification methods. When fusing the proposed features with traditional\nfeature sets, we show an improvement in unweighted accuracy of up to 5.7% and\n10.7% on Speaker-Dependent (SD) and Speaker-Independent (SI) SER tasks,\nrespectively, over the baseline. Following a segment-based approach we\ndemonstrate state-of-the-art performance on IEMOCAP using a Bidirectional\nRecurrent Neural Network.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 21:02:52 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Tzinis", "Efthymios", ""], ["Paraskevopoulos", "Georgios", ""], ["Baziotis", "Christos", ""], ["Potamianos", "Alexandros", ""]]}, {"id": "1811.04136", "submitter": "Wai Ming Tai", "authors": "Jeff M. Phillips, Wai Ming Tai", "title": "The GaussianSketch for Almost Relative Error Kernel Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two versions of a new sketch for approximately embedding the\nGaussian kernel into Euclidean inner product space. These work by truncating\ninfinite expansions of the Gaussian kernel, and carefully invoking the\nRecursiveTensorSketch [Ahle et al. SODA 2020]. After providing concentration\nand approximation properties of these sketches, we use them to approximate the\nkernel distance between points sets. These sketches yield almost\n$(1+\\varepsilon)$-relative error, but with a small additive $\\alpha$ term. In\nthe first variants the dependence on $1/\\alpha$ is poly-logarithmic, but has\nhigher degree of polynomial dependence on the original dimension $d$. In the\nsecond variant, the dependence on $1/\\alpha$ is still poly-logarithmic, but the\ndependence on $d$ is linear.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 21:12:32 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2019 09:26:19 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2020 06:43:27 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Phillips", "Jeff M.", ""], ["Tai", "Wai Ming", ""]]}, {"id": "1811.04142", "submitter": "Kehelwala Dewage Gayan Maduranga", "authors": "Kehelwala D. G. Maduranga, Kyle E. Helfrich, and Qiang Ye", "title": "Complex Unitary Recurrent Neural Networks using Scaled Cayley Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have been successfully used on a wide range\nof sequential data problems. A well known difficulty in using RNNs is the\n\\textit{vanishing or exploding gradient} problem. Recently, there have been\nseveral different RNN architectures that try to mitigate this issue by\nmaintaining an orthogonal or unitary recurrent weight matrix. One such\narchitecture is the scaled Cayley orthogonal recurrent neural network (scoRNN)\nwhich parameterizes the orthogonal recurrent weight matrix through a scaled\nCayley transform. This parametrization contains a diagonal scaling matrix\nconsisting of positive or negative one entries that can not be optimized by\ngradient descent. Thus the scaling matrix is fixed before training and a\nhyperparameter is introduced to tune the matrix for each particular task. In\nthis paper, we develop a unitary RNN architecture based on a complex scaled\nCayley transform. Unlike the real orthogonal case, the transformation uses a\ndiagonal scaling matrix consisting of entries on the complex unit circle which\ncan be optimized using gradient descent and no longer requires the tuning of a\nhyperparameter. We also provide an analysis of a potential issue of the modReLU\nactiviation function which is used in our work and several other unitary RNNs.\nIn the experiments conducted, the scaled Cayley unitary recurrent neural\nnetwork (scuRNN) achieves comparable or better results than scoRNN and other\nunitary RNNs without fixing the scaling matrix.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 21:37:36 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 16:06:38 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Maduranga", "Kehelwala D. G.", ""], ["Helfrich", "Kyle E.", ""], ["Ye", "Qiang", ""]]}, {"id": "1811.04151", "submitter": "Wei Zeng", "authors": "Wei Zeng, Azadeh Davoodi, Yu Hen Hu", "title": "Design Rule Violation Hotspot Prediction Based on Neural Network\n  Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design rule check is a critical step in the physical design of integrated\ncircuits to ensure manufacturability. However, it can be done only after a\ntime-consuming detailed routing procedure, which adds drastically to the time\nof design iterations. With advanced technology nodes, the outcomes of global\nrouting and detailed routing become less correlated, which adds to the\ndifficulty of predicting design rule violations from earlier stages. In this\npaper, a framework based on neural network ensembles is proposed to predict\ndesign rule violation hotspots using information from placement and global\nrouting. A soft voting structure and a PCA-based subset selection scheme are\ndeveloped on top of a baseline neural network from a recent work. Experimental\nresults show that the proposed architecture achieves significant improvement in\nmodel performance compared to the baseline case. For half of test cases, the\nperformance is even better than random forest, a commonly-used ensemble\nlearning model.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 22:18:26 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Zeng", "Wei", ""], ["Davoodi", "Azadeh", ""], ["Hu", "Yu Hen", ""]]}, {"id": "1811.04155", "submitter": "Dae Hoon Park", "authors": "Dae Hoon Park and Yi Chang", "title": "Adversarial Sampling and Training for Semi-Supervised Information\n  Retrieval", "comments": "Published in WWW 2019", "journal-ref": null, "doi": "10.1145/3308558.3313416", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ad-hoc retrieval models with implicit feedback often have problems, e.g., the\nimbalanced classes in the data set. Too few clicked documents may hurt\ngeneralization ability of the models, whereas too many non-clicked documents\nmay harm effectiveness of the models and efficiency of training. In addition,\nrecent neural network-based models are vulnerable to adversarial examples due\nto the linear nature in them. To solve the problems at the same time, we\npropose an adversarial sampling and training framework to learn ad-hoc\nretrieval models with implicit feedback. Our key idea is (i) to augment clicked\nexamples by adversarial training for better generalization and (ii) to obtain\nvery informational non-clicked examples by adversarial sampling and training.\nExperiments are performed on benchmark data sets for common ad-hoc retrieval\ntasks such as Web search, item recommendation, and question answering.\nExperimental results indicate that the proposed approaches significantly\noutperform strong baselines especially for high-ranked documents, and they\noutperform IRGAN in NDCG@5 using only 5% of labeled data for the Web search\ntask.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 22:57:18 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 01:18:34 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Park", "Dae Hoon", ""], ["Chang", "Yi", ""]]}, {"id": "1811.04177", "submitter": "Shunji Umetani", "authors": "Naoya Uematsu, Shunji Umetani and Yoshinobu Kawahara", "title": "An efficient branch-and-bound algorithm for submodular function\n  maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The submodular function maximization is an attractive optimization model that\nappears in many real applications. Although a variety of greedy algorithms\nquickly find good feasible solutions for many instances while guaranteeing\n(1-1/e)-approximation ratio, we still encounter many real applications that ask\noptimal or better feasible solutions within reasonable computation time. In\nthis paper, we present an efficient branch-and-bound algorithm for the\nnon-decreasing submodular function maximization problem based on its binary\ninteger programming (BIP) formulation with a huge number of constraints.\nNemhauser and Wolsey developed an exact algorithm called the constraint\ngeneration algorithm that starts from a reduced BIP problem with a small subset\nof constraints taken from the constraints and repeats solving a reduced BIP\nproblem while adding a new constraint at each iteration. However, their\nalgorithm is still computationally expensive due to many reduced BIP problems\nto be solved. To overcome this, we propose an improved constraint generation\nalgorithm to add a promising set of constraints at each iteration. We\nincorporate it into a branch-and-bound algorithm to attain good upper bounds\nwhile solving a smaller number of reduced BIP problems. According to\ncomputational results for well-known benchmark instances, our algorithm\nachieved better performance than the state-of-the-art exact algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 02:55:47 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Uematsu", "Naoya", ""], ["Umetani", "Shunji", ""], ["Kawahara", "Yoshinobu", ""]]}, {"id": "1811.04179", "submitter": "Valts Blukis", "authors": "Valts Blukis, Dipendra Misra, Ross A. Knepper, Yoav Artzi", "title": "Mapping Navigation Instructions to Continuous Control Actions with\n  Position-Visitation Prediction", "comments": "Appeared in Conference on Robot Learning 2018", "journal-ref": "In Conference on Robot Learning (pp. 505-518) (2018)", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for mapping natural language instructions and raw\nobservations to continuous control of a quadcopter drone. Our model predicts\ninterpretable position-visitation distributions indicating where the agent\nshould go during execution and where it should stop, and uses the predicted\ndistributions to select the actions to execute. This two-step model\ndecomposition allows for simple and efficient training using a combination of\nsupervised learning and imitation learning. We evaluate our approach with a\nrealistic drone simulator, and demonstrate absolute task-completion accuracy\nimprovements of 16.85% over two state-of-the-art instruction-following methods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 02:57:38 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 18:37:30 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Blukis", "Valts", ""], ["Misra", "Dipendra", ""], ["Knepper", "Ross A.", ""], ["Artzi", "Yoav", ""]]}, {"id": "1811.04184", "submitter": "Farshid Farhat", "authors": "Farshid Farhat, Mohammad Mahdi Kamani, James Z. Wang", "title": "CAPTAIN: Comprehensive Composition Assistance for Photo Taking", "comments": "30 pages, 21 figures, 4 tables, submitted to IJCV (International\n  Journal of Computer Vision)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many people are interested in taking astonishing photos and sharing with\nothers. Emerging hightech hardware and software facilitate ubiquitousness and\nfunctionality of digital photography. Because composition matters in\nphotography, researchers have leveraged some common composition techniques to\nassess the aesthetic quality of photos computationally. However, composition\ntechniques developed by professionals are far more diverse than well-documented\ntechniques can cover. We leverage the vast underexplored innovations in\nphotography for computational composition assistance. We propose a\ncomprehensive framework, named CAPTAIN (Composition Assistance for Photo\nTaking), containing integrated deep-learned semantic detectors, sub-genre\ncategorization, artistic pose clustering, personalized aesthetics-based image\nretrieval, and style set matching. The framework is backed by a large dataset\ncrawled from a photo-sharing Website with mostly photography enthusiasts and\nprofessionals. The work proposes a sequence of steps that have not been\nexplored in the past by researchers. The work addresses personal preferences\nfor composition through presenting a ranked-list of photographs to the user\nbased on user-specified weights in the similarity measure. The matching\nalgorithm recognizes the best shot among a sequence of shots with respect to\nthe user's preferred style set. We have conducted a number of experiments on\nthe newly proposed components and reported findings. A user study demonstrates\nthat the work is useful to those taking photos.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 03:43:05 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Farhat", "Farshid", ""], ["Kamani", "Mohammad Mahdi", ""], ["Wang", "James Z.", ""]]}, {"id": "1811.04194", "submitter": "Jingzhao Zhang", "authors": "Jingzhao Zhang, Hongyi Zhang, Suvrit Sra", "title": "R-SPIDER: A Fast Riemannian Stochastic Optimization Algorithm with\n  Curvature Independent Rate", "comments": "arXiv admin note: text overlap with arXiv:1605.07147", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study smooth stochastic optimization problems on Riemannian manifolds. Via\nadapting the recently proposed SPIDER algorithm \\citep{fang2018spider} (a\nvariance reduced stochastic method) to Riemannian manifold, we can achieve\nfaster rate than known algorithms in both the finite sum and stochastic\nsettings. Unlike previous works, by \\emph{not} resorting to bounding iterate\ndistances, our analysis yields curvature independent convergence rates for both\nthe nonconvex and strongly convex cases.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 04:48:41 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 00:58:54 GMT"}, {"version": "v3", "created": "Fri, 14 Dec 2018 17:23:30 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Zhang", "Jingzhao", ""], ["Zhang", "Hongyi", ""], ["Sra", "Suvrit", ""]]}, {"id": "1811.04201", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Yann LeCun", "title": "Adversarially-Trained Normalized Noisy-Feature Auto-Encoder for Text\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes Adversarially-Trained Normalized Noisy-Feature\nAuto-Encoder (ATNNFAE) for byte-level text generation. An ATNNFAE consists of\nan auto-encoder where the internal code is normalized on the unit sphere and\ncorrupted by additive noise. Simultaneously, a replica of the decoder (sharing\nthe same parameters as the AE decoder) is used as the generator and fed with\nrandom latent vectors. An adversarial discriminator is trained to distinguish\ntraining samples reconstructed from the AE from samples produced through the\nrandom-input generator, making the entire generator-discriminator path\ndifferentiable for discrete data like text. The combined effect of noise\ninjection in the code and shared weights between the decoder and the generator\ncan prevent the mode collapsing phenomenon commonly observed in GANs. Since\nperplexity cannot be applied to non-sequential text generation, we propose a\nnew evaluation method using the total variance distance between frequencies of\nhash-coded byte-level n-grams (NGTVD). NGTVD is a single benchmark that can\ncharacterize both the quality and the diversity of the generated texts.\nExperiments are offered in 6 large-scale datasets in Arabic, Chinese and\nEnglish, with comparisons against n-gram baselines and recurrent neural\nnetworks (RNNs). Ablation study on both the noise level and the discriminator\nis performed. We find that RNNs have trouble competing with the n-gram\nbaselines, and the ATNNFAE results are generally competitive.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 06:05:53 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Zhang", "Xiang", ""], ["LeCun", "Yann", ""]]}, {"id": "1811.04234", "submitter": "Felix Petersen", "authors": "Felix Petersen, Moritz Schubotz, Bela Gipp", "title": "Towards Formula Translation using Recursive Neural Networks", "comments": "11 pages, Work-in-Progress paper in CICM-WS 2018 Workshop Papers at\n  11th Conference on Intelligent Computer Mathematics CICM 2018", "journal-ref": "Conference on Intelligent Computer Mathematics (CICM) 2018,\n  CEUR-WS Vol-2307, WiP3", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While it has become common to perform automated translations on natural\nlanguage, performing translations between different representations of\nmathematical formulae has thus far not been possible. We implemented the first\ntranslator for mathematical formulae based on recursive neural networks. We\nchose recursive neural networks because mathematical formulae inherently\ninclude a structural encoding. In our implementation, we developed new\ntechniques and topologies for recursive tree-to-tree neural networks based on\nmulti-variate multi-valued Long Short-Term Memory cells. We propose a novel\napproach for mini-batch training that utilizes clustering and tree traversal.\nWe evaluate our translator and analyze the behavior of our proposed topologies\nand techniques based on a translation from generic LaTeX to the semantic LaTeX\nnotation. We use the semantic LaTeX notation from the Digital Library for\nMathematical Formulae and the Digital Repository for Mathematical Formulae at\nthe National Institute for Standards and Technology. We find that a simple\nheuristics-based clustering algorithm outperforms the conventional clustering\nalgorithms on the task of clustering binary trees of mathematical formulae with\nrespect to their topology. Furthermore, we find a mask for the loss function,\nwhich can prevent the neural network from finding a local minimum of the loss\nfunction. Given our preliminary results, a complete translation from formula to\nformula is not yet possible. However, we achieved a prediction accuracy of\n47.05% for predicting symbols at the correct position and an accuracy of 92.3%\nwhen ignoring the predicted position. Concluding, our work advances the field\nof recursive neural networks by improving the training speed and quality of\ntraining. In the future, we will work towards a complete translation allowing a\nmachine-interpretation of LaTeX formulae.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 11:20:18 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Petersen", "Felix", ""], ["Schubotz", "Moritz", ""], ["Gipp", "Bela", ""]]}, {"id": "1811.04246", "submitter": "Carlos Sarraute", "authors": "Martin Fixman, Ariel Berenstein, Jorge Brea, Martin Minnoni, Matias\n  Travizano, Carlos Sarraute", "title": "A Bayesian Approach to Income Inference in a Communication Network", "comments": "IEEE/ACM International Conference on Advances in Social Networks\n  Analysis and Mining (ASONAM 2016). August 18, 2016", "journal-ref": null, "doi": "10.1109/ASONAM.2016.7752294", "report-no": null, "categories": "cs.CY cs.LG cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The explosion of mobile phone communications in the last years occurs at a\nmoment where data processing power increases exponentially. Thanks to those two\nchanges in a global scale, the road has been opened to use mobile phone\ncommunications to generate inferences and characterizations of mobile phone\nusers. In this work, we use the communication network, enriched by a set of\nusers' attributes, to gain a better understanding of the demographic features\nof a population. Namely, we use call detail records and banking information to\ninfer the income of each person in the graph.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 12:44:36 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Fixman", "Martin", ""], ["Berenstein", "Ariel", ""], ["Brea", "Jorge", ""], ["Minnoni", "Martin", ""], ["Travizano", "Matias", ""], ["Sarraute", "Carlos", ""]]}, {"id": "1811.04251", "submitter": "David  McAllester", "authors": "David McAllester and Karl Stratos", "title": "Formal Limitations on the Measurement of Mutual Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring mutual information from finite data is difficult. Recent work has\nconsidered variational methods maximizing a lower bound. In this paper, we\nprove that serious statistical limitations are inherent to any method of\nmeasuring mutual information. More specifically, we show that any\ndistribution-free high-confidence lower bound on mutual information estimated\nfrom N samples cannot be larger than O(ln N ).\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 13:12:27 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 13:07:27 GMT"}, {"version": "v3", "created": "Wed, 2 Jan 2019 13:39:36 GMT"}, {"version": "v4", "created": "Wed, 20 May 2020 12:03:39 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["McAllester", "David", ""], ["Stratos", "Karl", ""]]}, {"id": "1811.04258", "submitter": "Mohamad Kazem Shirani Faradonbeh", "authors": "Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis", "title": "Input Perturbations for Adaptive Control and Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.RO cs.SY math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies adaptive algorithms for simultaneous regulation (i.e.,\ncontrol) and estimation (i.e., learning) of Multiple Input Multiple Output\n(MIMO) linear dynamical systems. It proposes practical, easy to implement\ncontrol policies based on perturbations of input signals. Such policies are\nshown to achieve a worst-case regret that scales as the square-root of the time\nhorizon, and holds uniformly over time. Further, it discusses specific settings\nwhere such greedy policies attain the information theoretic lower bound of\nlogarithmic regret. To establish the results, recent advances on\nself-normalized martingales together with a novel method of policy\ndecomposition are leveraged.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 14:20:15 GMT"}, {"version": "v2", "created": "Sat, 6 Jul 2019 17:51:48 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 00:02:42 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Faradonbeh", "Mohamad Kazem Shirani", ""], ["Tewari", "Ambuj", ""], ["Michailidis", "George", ""]]}, {"id": "1811.04272", "submitter": "Chao Yu", "authors": "Chao Yu, Tianpei Yang, Wenxuan Zhu, Dongxu wang, Guangliang Li", "title": "Learning Shaping Strategies in Human-in-the-loop Interactive\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing reinforcement learning agents with informationally rich human\nknowledge can dramatically improve various aspects of learning. Prior work has\ndeveloped different kinds of shaping methods that enable agents to learn\nefficiently in complex environments. All these methods, however, tailor human\nguidance to agents in specialized shaping procedures, thus embodying various\ncharacteristics and advantages in different domains. In this paper, we\ninvestigate the interplay between different shaping methods for more robust\nlearning performance. We propose an adaptive shaping algorithm which is capable\nof learning the most suitable shaping method in an on-line manner. Results in\ntwo classic domains verify its effectiveness from both simulated and real human\nstudies, shedding some light on the role and impact of human factors in\nhuman-robot collaborative learning.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 15:26:31 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Yu", "Chao", ""], ["Yang", "Tianpei", ""], ["Zhu", "Wenxuan", ""], ["wang", "Dongxu", ""], ["Li", "Guangliang", ""]]}, {"id": "1811.04277", "submitter": "Haitao Liu", "authors": "Haitao Liu, Randy C. Paffenroth, Jian Zou, Chong Zhou", "title": "Anomaly Detection via Graphical Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomalies and outliers are common in real-world data, and they can arise from\nmany sources, such as sensor faults. Accordingly, anomaly detection is\nimportant both for analyzing the anomalies themselves and for cleaning the data\nfor further analysis of its ambient structure. Nonetheless, a precise\ndefinition of anomalies is important for automated detection and herein we\napproach such problems from the perspective of detecting sparse latent effects\nembedded in large collections of noisy data. Standard Graphical Lasso-based\ntechniques can identify the conditional dependency structure of a collection of\nrandom variables based on their sample covariance matrix. However, classic\nGraphical Lasso is sensitive to outliers in the sample covariance matrix. In\nparticular, several outliers in a sample covariance matrix can destroy the\nsparsity of its inverse. Accordingly, we propose a novel optimization problem\nthat is similar in spirit to Robust Principal Component Analysis (RPCA) and\nsplits the sample covariance matrix $M$ into two parts, $M=F+S$, where $F$ is\nthe cleaned sample covariance whose inverse is sparse and computable by\nGraphical Lasso, and $S$ contains the outliers in $M$. We accomplish this\ndecomposition by adding an additional $ \\ell_1$ penalty to classic Graphical\nLasso, and name it \"Robust Graphical Lasso (Rglasso)\". Moreover, we propose an\nAlternating Direction Method of Multipliers (ADMM) solution to the optimization\nproblem which scales to large numbers of unknowns. We evaluate our algorithm on\nboth real and synthetic datasets, obtaining interpretable results and\noutperforming the standard robust Minimum Covariance Determinant (MCD) method\nand Robust Principal Component Analysis (RPCA) regarding both accuracy and\nspeed.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 16:15:04 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Liu", "Haitao", ""], ["Paffenroth", "Randy C.", ""], ["Zou", "Jian", ""], ["Zhou", "Chong", ""]]}, {"id": "1811.04288", "submitter": "Ovidiu Dan", "authors": "Ovidiu Dan, Vaibhav Parikh, Brian D. Davison", "title": "IP Geolocation through Reverse DNS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IP Geolocation databases are widely used in online services to map end user\nIP addresses to their geographical locations. However, they use proprietary\ngeolocation methods and in some cases they have poor accuracy. We propose a\nsystematic approach to use publicly accessible reverse DNS hostnames for\ngeolocating IP addresses. Our method is designed to be combined with other\ngeolocation data sources. We cast the task as a machine learning problem where\nfor a given hostname, we generate and rank a list of potential location\ncandidates. We evaluate our approach against three state of the art academic\nbaselines and two state of the art commercial IP geolocation databases. We show\nthat our work significantly outperforms the academic baselines, and is\ncomplementary and competitive with commercial databases. To aid\nreproducibility, we open source our entire approach.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 17:58:30 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Dan", "Ovidiu", ""], ["Parikh", "Vaibhav", ""], ["Davison", "Brian D.", ""]]}, {"id": "1811.04319", "submitter": "Ronen Tamari", "authors": "Ronen Tamari, Hiroyuki Shindo, Dafna Shahaf, Yuji Matsumoto", "title": "Playing by the Book: An Interactive Game Approach for Action Graph\n  Extraction from Text", "comments": "Accepted to NAACL 2019 ESSP workshop\n  (https://scientific-knowledge.github.io/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding procedural text requires tracking entities, actions and effects\nas the narrative unfolds. We focus on the challenging real-world problem of\naction-graph extraction from material science papers, where language is highly\nspecialized and data annotation is expensive and scarce. We propose a novel\napproach, Text2Quest, where procedural text is interpreted as instructions for\nan interactive game. A learning agent completes the game by executing the\nprocedure correctly in a text-based simulated lab environment. The framework\ncan complement existing approaches and enables richer forms of learning\ncompared to static texts. We discuss potential limitations and advantages of\nthe approach, and release a prototype proof-of-concept, hoping to encourage\nresearch in this direction.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 21:45:07 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2018 16:59:00 GMT"}, {"version": "v3", "created": "Sat, 6 Apr 2019 19:19:05 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Tamari", "Ronen", ""], ["Shindo", "Hiroyuki", ""], ["Shahaf", "Dafna", ""], ["Matsumoto", "Yuji", ""]]}, {"id": "1811.04324", "submitter": "Yuhang Song", "authors": "Yuhang Song, Jianyi Wang, Thomas Lukasiewicz, Zhenghua Xu, Mai Xu", "title": "Diversity-Driven Extensible Hierarchical Reinforcement Learning", "comments": "8 pages, 8 figures, In Proceedings of the 33rd National Conference on\n  Artificial Intelligence, AAAI 2019, Honolulu, Hawaii, USA, January 27, 2019.\n  Jianyi Wang is the co-first author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical reinforcement learning (HRL) has recently shown promising\nadvances on speeding up learning, improving the exploration, and discovering\nintertask transferable skills. Most recent works focus on HRL with two levels,\ni.e., a master policy manipulates subpolicies, which in turn manipulate\nprimitive actions. However, HRL with multiple levels is usually needed in many\nreal-world scenarios, whose ultimate goals are highly abstract, while their\nactions are very primitive. Therefore, in this paper, we propose a\ndiversity-driven extensible HRL (DEHRL), where an extensible and scalable\nframework is built and learned levelwise to realize HRL with multiple levels.\nDEHRL follows a popular assumption: diverse subpolicies are useful, i.e.,\nsubpolicies are believed to be more useful if they are more diverse. However,\nexisting implementations of this diversity assumption usually have their own\ndrawbacks, which makes them inapplicable to HRL with multiple levels.\nConsequently, we further propose a novel diversity-driven solution to achieve\nthis assumption in DEHRL. Experimental studies evaluate DEHRL with five\nbaselines from four perspectives in two domains; the results show that DEHRL\noutperforms the state-of-the-art baselines in all four aspects.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 23:35:34 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 10:26:58 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Song", "Yuhang", ""], ["Wang", "Jianyi", ""], ["Lukasiewicz", "Thomas", ""], ["Xu", "Zhenghua", ""], ["Xu", "Mai", ""]]}, {"id": "1811.04343", "submitter": "Rohitash Chandra", "authors": "Rohitash Chandra, Konark Jain, Ratneel V. Deo, Sally Cripps", "title": "Langevin-gradient parallel tempering for Bayesian neural learning", "comments": "In review. Software:\n  https://github.com/sydney-machine-learning/parallel-tempering-neural-net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian neural learning feature a rigorous approach to estimation and\nuncertainty quantification via the posterior distribution of weights that\nrepresent knowledge of the neural network. This not only provides point\nestimates of optimal set of weights but also the ability to quantify\nuncertainty in decision making using the posterior distribution. Markov chain\nMonte Carlo (MCMC) techniques are typically used to obtain sample-based\nestimates of the posterior distribution. However, these techniques face\nchallenges in convergence and scalability, particularly in settings with large\ndatasets and network architectures. This paper address these challenges in two\nways. First, parallel tempering is used used to explore multiple modes of the\nposterior distribution and implemented in multi-core computing architecture.\nSecond, we make within-chain sampling schemes more efficient by using Langevin\ngradient information in forming Metropolis-Hastings proposal distributions. We\ndemonstrate the techniques using time series prediction and pattern\nclassification applications. The results show that the method not only improves\nthe computational time, but provides better prediction or decision making\ncapabilities when compared to related methods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 03:53:54 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Chandra", "Rohitash", ""], ["Jain", "Konark", ""], ["Deo", "Ratneel V.", ""], ["Cripps", "Sally", ""]]}, {"id": "1811.04344", "submitter": "Abigail Jacobs", "authors": "Jen J. Gong, Abigail Z. Jacobs, Toby E. Stuart, Mathijs de Vaan", "title": "Discovering heterogeneous subpopulations for fine-grained analysis of\n  opioid use and opioid use disorders", "comments": "Withdrawn pending data use agreement clarification", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The opioid epidemic in the United States claims over 40,000 lives per year,\nand it is estimated that well over two million Americans have an opioid use\ndisorder. Over-prescription and misuse of prescription opioids play an\nimportant role in the epidemic. Individuals who are prescribed opioids, and who\nare diagnosed with opioid use disorder, have diverse underlying health states.\nPolicy interventions targeting prescription opioid use, opioid use disorder,\nand overdose often fail to account for this variation. To identify latent\nhealth states, or phenotypes, pertinent to opioid use and opioid use disorders,\nwe use probabilistic topic modeling with medical diagnosis histories from a\nstatewide population of individuals who were prescribed opioids. We demonstrate\nthat our learned phenotypes are predictive of future opioid use-related\noutcomes. In addition, we show how the learned phenotypes can provide important\ncontext for variability in opioid prescriptions. Understanding the\nheterogeneity in individual health states and in prescription opioid use can\nhelp identify policy interventions to address this public health crisis.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 04:00:32 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 04:52:42 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 23:25:41 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Gong", "Jen J.", ""], ["Jacobs", "Abigail Z.", ""], ["Stuart", "Toby E.", ""], ["de Vaan", "Mathijs", ""]]}, {"id": "1811.04345", "submitter": "Ishan Jindal", "authors": "Ishan Jindal, Zhiwei Qin, Xuewen Chen, Matthew Nokleby and Jieping Ye", "title": "Optimizing Taxi Carpool Policies via Reinforcement Learning and\n  Spatio-Temporal Mining", "comments": "Accepted at IEEE International Conference on Big Data 2018. arXiv\n  admin note: text overlap with arXiv:1710.04350", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a reinforcement learning (RL) based system to learn\nan effective policy for carpooling that maximizes transportation efficiency so\nthat fewer cars are required to fulfill the given amount of trip demand. For\nthis purpose, first, we develop a deep neural network model, called ST-NN\n(Spatio-Temporal Neural Network), to predict taxi trip time from the raw GPS\ntrip data. Secondly, we develop a carpooling simulation environment for RL\ntraining, with the output of ST-NN and using the NYC taxi trip dataset. In\norder to maximize transportation efficiency and minimize traffic congestion, we\nchoose the effective distance covered by the driver on a carpool trip as the\nreward. Therefore, the more effective distance a driver achieves over a trip\n(i.e. to satisfy more trip demand) the higher the efficiency and the less will\nbe the traffic congestion. We compared the performance of RL learned policy to\na fixed policy (which always accepts carpool) as a baseline and obtained\npromising results that are interpretable and demonstrate the advantage of our\nRL approach. We also compare the performance of ST-NN to that of\nstate-of-the-art travel time estimation methods and observe that ST-NN\nsignificantly improves the prediction performance and is more robust to\noutliers.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 04:13:31 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Jindal", "Ishan", ""], ["Qin", "Zhiwei", ""], ["Chen", "Xuewen", ""], ["Nokleby", "Matthew", ""], ["Ye", "Jieping", ""]]}, {"id": "1811.04350", "submitter": "John Yang", "authors": "John Yang, Gyujeong Lee, Minsung Hyun, Simyung Chang, Nojun Kwak", "title": "Towards Governing Agent's Efficacy: Action-Conditional $\\beta$-VAE for\n  Deep Transparent Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the blackbox issue of deep neural networks in the settings of\nreinforcement learning (RL) where neural agents learn towards maximizing reward\ngains in an uncontrollable way. Such learning approach is risky when the\ninteracting environment includes an expanse of state space because it is then\nalmost impossible to foresee all unwanted outcomes and penalize them with\nnegative rewards beforehand. Unlike reverse analysis of learned neural features\nfrom previous works, our proposed method \\nj{tackles the blackbox issue by\nencouraging} an RL policy network to learn interpretable latent features\nthrough an implementation of a disentangled representation learning method.\nToward this end, our method allows an RL agent to understand self-efficacy by\ndistinguishing its influences from uncontrollable environmental factors, which\nclosely resembles the way humans understand their scenes. Our experimental\nresults show that the learned latent factors not only are interpretable, but\nalso enable modeling the distribution of entire visited state space with a\nspecific action condition. We have experimented that this characteristic of the\nproposed structure can lead to ex post facto governance for desired behaviors\nof RL agents.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 04:48:15 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Yang", "John", ""], ["Lee", "Gyujeong", ""], ["Hyun", "Minsung", ""], ["Chang", "Simyung", ""], ["Kwak", "Nojun", ""]]}, {"id": "1811.04351", "submitter": "Min-Hsiu Hsieh", "authors": "Chao Zhang and Min-Hsiu Hsieh and Dacheng Tao", "title": "Generalization Bounds for Vicinal Risk Minimization Principle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vicinal risk minimization (VRM) principle, first proposed by\n\\citet{vapnik1999nature}, is an empirical risk minimization (ERM) variant that\nreplaces Dirac masses with vicinal functions. Although there is strong\nnumerical evidence showing that VRM outperforms ERM if appropriate vicinal\nfunctions are chosen, a comprehensive theoretical understanding of VRM is still\nlacking. In this paper, we study the generalization bounds for VRM. Our results\nsupport Vapnik's original arguments and additionally provide deeper insights\ninto VRM. First, we prove that the complexity of function classes convolving\nwith vicinal functions can be controlled by that of the original function\nclasses under the assumption that the function class is composed of\nLipschitz-continuous functions. Then, the resulting generalization bounds for\nVRM suggest that the generalization performance of VRM is also effected by the\nchoice of vicinity function and the quality of function classes. These findings\ncan be used to examine whether the choice of vicinal function is appropriate\nfor the VRM-based learning setting. Finally, we provide a theoretical\nexplanation for existing VRM models, e.g., uniform distribution-based models,\nGaussian distribution-based models, and mixup models.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 05:06:02 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Zhang", "Chao", ""], ["Hsieh", "Min-Hsiu", ""], ["Tao", "Dacheng", ""]]}, {"id": "1811.04364", "submitter": "Shehroz Khan", "authors": "Amir Ahmad, Shehroz S. Khan", "title": "Survey of state-of-the-art mixed data clustering algorithms", "comments": "20 Pages, 2 columns, 6 Tables, 209 References", "journal-ref": "IEEE Access, 2019", "doi": "10.1109/ACCESS.2019.2903568", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed data comprises both numeric and categorical features, and mixed\ndatasets occur frequently in many domains, such as health, finance, and\nmarketing. Clustering is often applied to mixed datasets to find structures and\nto group similar objects for further analysis. However, clustering mixed data\nis challenging because it is difficult to directly apply mathematical\noperations, such as summation or averaging, to the feature values of these\ndatasets. In this paper, we present a taxonomy for the study of mixed data\nclustering algorithms by identifying five major research themes. We then\npresent a state-of-the-art review of the research works within each research\ntheme. We analyze the strengths and weaknesses of these methods with pointers\nfor future research directions. Lastly, we present an in-depth analysis of the\noverall challenges in this field, highlight open research questions and discuss\nguidelines to make progress in the field.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 07:27:51 GMT"}, {"version": "v2", "created": "Sun, 25 Nov 2018 23:11:42 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 22:30:04 GMT"}, {"version": "v4", "created": "Tue, 29 Jan 2019 05:54:20 GMT"}, {"version": "v5", "created": "Sat, 9 Mar 2019 21:25:46 GMT"}, {"version": "v6", "created": "Mon, 18 Mar 2019 18:30:33 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Ahmad", "Amir", ""], ["Khan", "Shehroz S.", ""]]}, {"id": "1811.04369", "submitter": "Izzeddin Gur", "authors": "Izzeddin Gur, Dilek Hakkani-Tur, Gokhan Tur, Pararth Shah", "title": "User Modeling for Task Oriented Dialogues", "comments": "Accepted at SLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce end-to-end neural network based models for simulating users of\ntask-oriented dialogue systems. User simulation in dialogue systems is crucial\nfrom two different perspectives: (i) automatic evaluation of different dialogue\nmodels, and (ii) training task-oriented dialogue systems. We design a\nhierarchical sequence-to-sequence model that first encodes the initial user\ngoal and system turns into fixed length representations using Recurrent Neural\nNetworks (RNN). It then encodes the dialogue history using another RNN layer.\nAt each turn, user responses are decoded from the hidden representations of the\ndialogue level RNN. This hierarchical user simulator (HUS) approach allows the\nmodel to capture undiscovered parts of the user goal without the need of an\nexplicit dialogue state tracking. We further develop several variants by\nutilizing a latent variable model to inject random variations into user\nresponses to promote diversity in simulated user responses and a novel goal\nregularization mechanism to penalize divergence of user responses from the\ninitial user goal. We evaluate the proposed models on movie ticket booking\ndomain by systematically interacting each user simulator with various dialogue\nsystem policies trained with different objectives and users.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 08:21:55 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Gur", "Izzeddin", ""], ["Hakkani-Tur", "Dilek", ""], ["Tur", "Gokhan", ""], ["Shah", "Pararth", ""]]}, {"id": "1811.04370", "submitter": "Soham Saha", "authors": "Soham Saha, Girish Varma, C.V.Jawahar", "title": "Improved Visual Relocalization by Discovering Anchor Points", "comments": "10 Pages, 6 figures, BMVC 2018, Newcastle, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We address the visual relocalization problem of predicting the location and\ncamera orientation or pose (6DOF) of the given input scene. We propose a method\nbased on how humans determine their location using the visible landmarks. We\ndefine anchor points uniformly across the route map and propose a deep learning\narchitecture which predicts the most relevant anchor point present in the scene\nas well as the relative offsets with respect to it. The relevant anchor point\nneed not be the nearest anchor point to the ground truth location, as it might\nnot be visible due to the pose. Hence we propose a multi task loss function,\nwhich discovers the relevant anchor point, without needing the ground truth for\nit. We validate the effectiveness of our approach by experimenting on\nCambridgeLandmarks (large scale outdoor scenes) as well as 7 Scenes (indoor\nscenes) using variousCNN feature extractors. Our method improves the median\nerror in indoor as well as outdoor localization datasets compared to the\nprevious best deep learning model known as PoseNet (with geometric\nre-projection loss) using the same feature extractor. We improve the median\nerror in localization in the specific case of Street scene, by over 8m.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 08:39:57 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Saha", "Soham", ""], ["Varma", "Girish", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1811.04376", "submitter": "Tanmayee Narendra Ms", "authors": "Tanmayee Narendra, Anush Sankaran, Deepak Vijaykeerthy and Senthil\n  Mani", "title": "Explaining Deep Learning Models using Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning models have been successfully applied to a variety of\ntasks, due to the millions of parameters, they are becoming increasingly opaque\nand complex. In order to establish trust for their widespread commercial use,\nit is important to formalize a principled framework to reason over these\nmodels. In this work, we use ideas from causal inference to describe a general\nframework to reason over CNN models. Specifically, we build a Structural Causal\nModel (SCM) as an abstraction over a specific aspect of the CNN. We also\nformulate a method to quantitatively rank the filters of a convolution layer\naccording to their counterfactual importance. We illustrate our approach with\npopular CNN architectures such as LeNet5, VGG19, and ResNet32.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 09:26:55 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Narendra", "Tanmayee", ""], ["Sankaran", "Anush", ""], ["Vijaykeerthy", "Deepak", ""], ["Mani", "Senthil", ""]]}, {"id": "1811.04380", "submitter": "Iurii Kemaev", "authors": "Iurii Kemaev, Daniil Polykovskiy, Dmitry Vetrov", "title": "ReSet: Learning Recurrent Dynamic Routing in ResNet-like Neural Networks", "comments": "Published in Proceedings of The 10th Asian Conference on Machine\n  Learning, http://proceedings.mlr.press/v95/kemaev18a.html", "journal-ref": "Proceedings of The 10th Asian Conference on Machine Learning, PMLR\n  95:422-437, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Network is a powerful Machine Learning tool that shows outstanding\nperformance in Computer Vision, Natural Language Processing, and Artificial\nIntelligence. In particular, recently proposed ResNet architecture and its\nmodifications produce state-of-the-art results in image classification\nproblems. ResNet and most of the previously proposed architectures have a fixed\nstructure and apply the same transformation to all input images. In this work,\nwe develop a ResNet-based model that dynamically selects Computational Units\n(CU) for each input object from a learned set of transformations. Dynamic\nselection allows the network to learn a sequence of useful transformations and\napply only required units to predict the image label. We compare our model to\nResNet-38 architecture and achieve better results than the original ResNet on\nCIFAR-10.1 test set. While examining the produced paths, we discovered that the\nnetwork learned different routes for images from different classes and similar\nroutes for similar images.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 09:45:41 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Kemaev", "Iurii", ""], ["Polykovskiy", "Daniil", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "1811.04383", "submitter": "David Cortes", "authors": "David Cortes", "title": "Adapting multi-armed bandits policies to contextual bandits scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores adaptations of successful multi-armed bandits policies to\nthe online contextual bandits scenario with binary rewards using binary\nclassification algorithms such as logistic regression as black-box oracles.\nSome of these adaptations are achieved through bootstrapping or approximate\nbootstrapping, while others rely on other forms of randomness, resulting in\nmore scalable approaches than previous works, and the ability to work with any\ntype of classification algorithm. In particular, the Adaptive-Greedy algorithm\nshows a lot of promise, in many cases achieving better performance than upper\nconfidence bound and Thompson sampling strategies, at the expense of more\nhyperparameters to tune.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 09:56:11 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 07:40:14 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Cortes", "David", ""]]}, {"id": "1811.04393", "submitter": "Jiatao Jiang", "authors": "Jiatao Jiang, Zhen Cui, Chunyan Xu, Jian Yang", "title": "Gaussian-Induced Convolution for Graphs", "comments": "8 pages, 9 figures, AAAI-19 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning representation on graph plays a crucial role in numerous tasks of\npattern recognition. Different from grid-shaped images/videos, on which local\nconvolution kernels can be lattices, however, graphs are fully coordinate-free\non vertices and edges. In this work, we propose a Gaussian-induced convolution\n(GIC) framework to conduct local convolution filtering on irregular graphs.\nSpecifically, an edge-induced Gaussian mixture model is designed to encode\nvariations of subgraph region by integrating edge information into weighted\nGaussian models, each of which implicitly characterizes one component of\nsubgraph variations. In order to coarsen a graph, we derive a vertex-induced\nGaussian mixture model to cluster vertices dynamically according to the\nconnection of edges, which is approximately equivalent to the weighted graph\ncut. We conduct our multi-layer graph convolution network on several public\ndatasets of graph classification. The extensive experiments demonstrate that\nour GIC is effective and can achieve the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 11:21:18 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Jiang", "Jiatao", ""], ["Cui", "Zhen", ""], ["Xu", "Chunyan", ""], ["Yang", "Jian", ""]]}, {"id": "1811.04407", "submitter": "Liu Yuezhang", "authors": "Liu Yuezhang, Ruohan Zhang, Dana H. Ballard", "title": "An initial attempt of combining visual selective attention with deep\n  reinforcement learning", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention serves as a means of feature selection mechanism in the\nperceptual system. Motivated by Broadbent's leaky filter model of selective\nattention, we evaluate how such mechanism could be implemented and affect the\nlearning process of deep reinforcement learning. We visualize and analyze the\nfeature maps of DQN on a toy problem Catch, and propose an approach to combine\nvisual selective attention with deep reinforcement learning. We experiment with\noptical flow-based attention and A2C on Atari games. Experiment results show\nthat visual selective attention could lead to improvements in terms of sample\nefficiency on tested games. An intriguing relation between attention and batch\nnormalization is also discovered.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 12:22:44 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 07:14:00 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 17:48:44 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Yuezhang", "Liu", ""], ["Zhang", "Ruohan", ""], ["Ballard", "Dana H.", ""]]}, {"id": "1811.04411", "submitter": "Xiaoyu Du", "authors": "Xiangnan He, Jinhui Tang, Xiaoyu Du, Richang Hong, Tongwei Ren and\n  Tat-Seng Chua", "title": "Fast Matrix Factorization with Non-Uniform Weights on Missing Data", "comments": "IEEE Transactions on Neural Networks and Learning Systems (TNNLS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Matrix factorization (MF) has been widely used to discover the low-rank\nstructure and to predict the missing entries of data matrix. In many real-world\nlearning systems, the data matrix can be very high-dimensional but sparse. This\nposes an imbalanced learning problem, since the scale of missing entries is\nusually much larger than that of observed entries, but they cannot be ignored\ndue to the valuable negative signal. For efficiency concern, existing work\ntypically applies a uniform weight on missing entries to allow a fast learning\nalgorithm. However, this simplification will decrease modeling fidelity,\nresulting in suboptimal performance for downstream applications.\n  In this work, we weight the missing data non-uniformly, and more generically,\nwe allow any weighting strategy on the missing data. To address the efficiency\nchallenge, we propose a fast learning method, for which the time complexity is\ndetermined by the number of observed entries in the data matrix, rather than\nthe matrix size. The key idea is two-fold: 1) we apply truncated SVD on the\nweight matrix to get a more compact representation of the weights, and 2) we\nlearn MF parameters with element-wise alternating least squares (eALS) and\nmemorize the key intermediate variables to avoid repeating computations that\nare unnecessary. We conduct extensive experiments on two recommendation\nbenchmarks, demonstrating the correctness, efficiency, and effectiveness of our\nfast eALS method.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 13:17:42 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 07:07:27 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["He", "Xiangnan", ""], ["Tang", "Jinhui", ""], ["Du", "Xiaoyu", ""], ["Hong", "Richang", ""], ["Ren", "Tongwei", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1811.04422", "submitter": "Xiaojin Zhu", "authors": "Xiaojin Zhu", "title": "An Optimal Control View of Adversarial Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I describe an optimal control view of adversarial machine learning, where the\ndynamical system is the machine learner, the input are adversarial actions, and\nthe control costs are defined by the adversary's goals to do harm and be hard\nto detect. This view encompasses many types of adversarial machine learning,\nincluding test-item attacks, training-data poisoning, and adversarial reward\nshaping. The view encourages adversarial machine learning researcher to utilize\nadvances in control theory and reinforcement learning.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 14:28:34 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Zhu", "Xiaojin", ""]]}, {"id": "1811.04451", "submitter": "Richard Kurle", "authors": "Richard Kurle and Stephan G\\\"unnemann and Patrick van der Smagt", "title": "Multi-Source Neural Variational Inference", "comments": "AAAI 2019, Association for the Advancement of Artificial Intelligence\n  (AAAI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from multiple sources of information is an important problem in\nmachine-learning research. The key challenges are learning representations and\nformulating inference methods that take into account the complementarity and\nredundancy of various information sources. In this paper we formulate a\nvariational autoencoder based multi-source learning framework in which each\nencoder is conditioned on a different information source. This allows us to\nrelate the sources via the shared latent variables by computing divergence\nmeasures between individual source's posterior approximations. We explore a\nvariety of options to learn these encoders and to integrate the beliefs they\ncompute into a consistent posterior approximation. We visualise learned beliefs\non a toy dataset and evaluate our methods for learning shared representations\nand structured output prediction, showing trade-offs of learning separate\nencoders for each information source. Furthermore, we demonstrate how conflict\ndetection and redundancy can increase robustness of inference in a multi-source\nsetting.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 18:59:21 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 13:46:04 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Kurle", "Richard", ""], ["G\u00fcnnemann", "Stephan", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1811.04454", "submitter": "Milan Aggarwal", "authors": "Milan Aggarwal, Nupur Kumari, Ayush Bansal, Balaji Krishnamurthy", "title": "ReDecode Framework for Iterative Improvement in Paraphrase Generation", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating paraphrases, that is, different variations of a sentence conveying\nthe same meaning, is an important yet challenging task in NLP. Automatically\ngenerating paraphrases has its utility in many NLP tasks like question\nanswering, information retrieval, conversational systems to name a few. In this\npaper, we introduce iterative refinement of generated paraphrases within VAE\nbased generation framework. Current sequence generation models lack the\ncapability to (1) make improvements once the sentence is generated; (2) rectify\nerrors made while decoding. We propose a technique to iteratively refine the\noutput using multiple decoders, each one attending on the output sentence\ngenerated by the previous decoder. We improve current state of the art results\nsignificantly - with over 9% and 28% absolute increase in METEOR scores on\nQuora question pairs and MSCOCO datasets respectively. We also show\nqualitatively through examples that our re-decoding approach generates better\nparaphrases compared to a single decoder by rectifying errors and making\nimprovements in paraphrase structure, inducing variations and introducing new\nbut semantically coherent information.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 19:02:50 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Aggarwal", "Milan", ""], ["Kumari", "Nupur", ""], ["Bansal", "Ayush", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "1811.04455", "submitter": "Anthony Nouy", "authors": "Erwan Grelier and Anthony Nouy and Mathilde Chevreuil", "title": "Learning with tree-based tensor formats", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the approximation of high-dimensional functions\nin a statistical learning setting, by empirical risk minimization over model\nclasses of functions in tree-based tensor format. These are particular classes\nof rank-structured functions that can be seen as deep neural networks with a\nsparse architecture related to the tree and multilinear activation functions.\nFor learning in a given model class, we exploit the fact that tree-based tensor\nformats are multilinear models and recast the problem of risk minimization over\na nonlinear set into a succession of learning problems with linear models.\nSuitable changes of representation yield numerically stable learning problems\nand allow to exploit sparsity. For high-dimensional problems or when only a\nsmall data set is available, the selection of a good model class is a critical\nissue. For a given tree, the selection of the tuple of tree-based ranks that\nminimize the risk is a combinatorial problem. Here, we propose a rank\nadaptation strategy which provides in practice a good convergence of the risk\nas a function of the model class complexity. Finding a good tree is also a\ncombinatorial problem, which can be related to the choice of a particular\nsparse architecture for deep neural networks. Here, we propose a stochastic\nalgorithm for minimizing the complexity of the representation of a given\nfunction over a class of trees with a given arity, allowing changes in the\ntopology of the tree. This tree optimization algorithm is then included in a\nlearning scheme that successively adapts the tree and the corresponding\ntree-based ranks. Contrary to classical learning algorithms for nonlinear model\nclasses, the proposed algorithms are numerically stable, reliable, and require\nonly a low level expertise of the user.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 19:03:31 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 12:33:34 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Grelier", "Erwan", ""], ["Nouy", "Anthony", ""], ["Chevreuil", "Mathilde", ""]]}, {"id": "1811.04463", "submitter": "Fayyaz Minhas", "authors": "Kanza Hamid, Amina Asif, Wajid Abbasi, Durre Sabih and Fayyaz Minhas", "title": "Machine Learning with Abstention for Automated Liver Disease Diagnosis", "comments": "Preprint version before submission for publication. complete version\n  published in proc. 15th International Conference on Frontiers of Information\n  Technology (FIT 2017), December 18-20, 2017, Islamabad, Pakistan.\n  http://ieeexplore.ieee.org/document/8261064/", "journal-ref": "15th IEEE International Conference on Frontiers of Information\n  Technology (FIT 2017), December 18-20, 2017, Islamabad, Pakistan", "doi": "10.1109/FIT.2017.00070", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for detection of liver abnormalities in\nan automated manner using ultrasound images. For this purpose, we have\nimplemented a machine learning model that can not only generate labels (normal\nand abnormal) for a given ultrasound image but it can also detect when its\nprediction is likely to be incorrect. The proposed model abstains from\ngenerating the label of a test example if it is not confident about its\nprediction. Such behavior is commonly practiced by medical doctors who, when\ngiven insufficient information or a difficult case, can chose to carry out\nfurther clinical or diagnostic tests before generating a diagnosis. However,\nexisting machine learning models are designed in a way to always generate a\nlabel for a given example even when the confidence of their prediction is low.\nWe have proposed a novel stochastic gradient based solver for the learning with\nabstention paradigm and use it to make a practical, state of the art method for\nliver disease classification. The proposed method has been benchmarked on a\ndata set of approximately 100 patients from MINAR, Multan, Pakistan and our\nresults show that the proposed scheme offers state of the art classification\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 19:37:40 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Hamid", "Kanza", ""], ["Asif", "Amina", ""], ["Abbasi", "Wajid", ""], ["Sabih", "Durre", ""], ["Minhas", "Fayyaz", ""]]}, {"id": "1811.04471", "submitter": "Zhen Li", "authors": "Zhen Li, Nicholas J. Meyer, Eric B. Laber, Robert Brigantic", "title": "Thompson Sampling for Pursuit-Evasion Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pursuit-evasion is a multi-agent sequential decision problem wherein a group\nof agents known as pursuers coordinate their traversal of a spatial domain to\nlocate an agent trying to evade them. Pursuit evasion problems arise in a\nnumber of import application domains including defense and route planning.\nLearning to optimally coordinate pursuer behaviors so as to minimize time to\ncapture of the evader is challenging because of a large action space and sparse\nnoisy state information; consequently, previous approaches have relied\nprimarily on heuristics. We propose a variant of Thompson Sampling for\npursuit-evasion that allows for the application of existing model-based\nplanning algorithms. This approach is general in that it allows for an\narbitrary number of pursuers, a general spatial domain, and the integration of\nauxiliary information provided by informants. In a suite of simulation\nexperiments, Thompson Sampling for pursuit evasion significantly reduces\ntime-to-capture relative to competing algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 20:17:01 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Li", "Zhen", ""], ["Meyer", "Nicholas J.", ""], ["Laber", "Eric B.", ""], ["Brigantic", "Robert", ""]]}, {"id": "1811.04475", "submitter": "Shaunak Mishra", "authors": "Anit Kumar Sahu, Shaunak Mishra, Narayan Bhamidipati", "title": "Managing App Install Ad Campaigns in RTB: A Q-Learning Approach", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real time bidding (RTB) enables demand side platforms (bidders) to scale ad\ncampaigns across multiple publishers affiliated to an RTB ad exchange. While\ndriving multiple campaigns for mobile app install ads via RTB, the bidder\ntypically has to: (i) maintain each campaign's efficiency (i.e., meet\nadvertiser's target cost-per-install), (ii) be sensitive to advertiser's\nbudget, and (iii) make profit after payouts to the ad exchange. In this\nprocess, there is a sense of delayed rewards for the bidder's actions; the\nexchange charges the bidder right after the ad is shown, but the bidder gets to\nknow about resultant installs after considerable delay. This makes it\nchallenging for the bidder to decide beforehand the bid (and corresponding cost\ncharged to advertiser) for each ad display opportunity. To jointly handle the\nobjectives mentioned above, we propose a state space based policy which decides\nthe exchange bid and advertiser cost for each opportunity. The state space\ncaptures the current efficiency, budget utilization and profit. The policy\nbased on this state space is trained on past decisions and outcomes via a novel\nQ-learning algorithm which accounts for the delay in install notifications. In\nour experiments based on data from app install campaigns managed by Yahoo's\nGemini advertising platform, the Q-learning based policy led to a significant\nincrease in the profit and number of efficient campaigns.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 20:42:09 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Sahu", "Anit Kumar", ""], ["Mishra", "Shaunak", ""], ["Bhamidipati", "Narayan", ""]]}, {"id": "1811.04477", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Unifying Gaussian LWF and AMP Chain Graphs to Model Interference", "comments": "v2: Section 6 has been added. v3: Sections 7 and 8 have been added.\n  v4: Major reorganization. v5: Major reorganization. v6-v7: Minor changes. v8:\n  Addition of Appendix B. v9: Section 7 has been rewritten", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An intervention may have an effect on units other than those to which it was\nadministered. This phenomenon is called interference and it usually goes\nunmodeled. In this paper, we propose to combine Lauritzen-Wermuth-Frydenberg\nand Andersson-Madigan-Perlman chain graphs to create a new class of causal\nmodels that can represent both interference and non-interference relationships\nfor Gaussian distributions. Specifically, we define the new class of models,\nintroduce global and local and pairwise Markov properties for them, and prove\ntheir equivalence. We also propose an algorithm for maximum likelihood\nparameter estimation for the new models, and report experimental results.\nFinally, we show how to compute the effects of interventions in the new models.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 20:43:19 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 15:07:45 GMT"}, {"version": "v3", "created": "Wed, 23 Jan 2019 22:12:47 GMT"}, {"version": "v4", "created": "Mon, 1 Apr 2019 20:43:27 GMT"}, {"version": "v5", "created": "Mon, 29 Apr 2019 09:57:12 GMT"}, {"version": "v6", "created": "Tue, 30 Apr 2019 14:19:18 GMT"}, {"version": "v7", "created": "Tue, 4 Jun 2019 11:52:31 GMT"}, {"version": "v8", "created": "Sun, 16 Jun 2019 19:34:21 GMT"}, {"version": "v9", "created": "Fri, 5 Jul 2019 10:17:54 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1811.04480", "submitter": "Vahid Noroozi", "authors": "Vahid Noroozi, Sara Bahaadini, Lei Zheng, Sihong Xie, Weixiang Shao,\n  Philip S. Yu", "title": "Semi-supervised Deep Representation Learning for Multi-View Problems", "comments": "Accepted to IEEE Big Data 2018. 9 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While neural networks for learning representation of multi-view data have\nbeen previously proposed as one of the state-of-the-art multi-view dimension\nreduction techniques, how to make the representation discriminative with only a\nsmall amount of labeled data is not well-studied. We introduce a\nsemi-supervised neural network model, named Multi-view Discriminative Neural\nNetwork (MDNN), for multi-view problems. MDNN finds nonlinear view-specific\nmappings by projecting samples to a common feature space using multiple coupled\ndeep networks. It is capable of leveraging both labeled and unlabeled data to\nproject multi-view data so that samples from different classes are separated\nand those from the same class are clustered together. It also uses the\ninter-view correlation between views to exploit the available information in\nboth the labeled and unlabeled data. Extensive experiments conducted on four\ndatasets demonstrate the effectiveness of the proposed algorithm for multi-view\nsemi-supervised learning.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 20:53:50 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Noroozi", "Vahid", ""], ["Bahaadini", "Sara", ""], ["Zheng", "Lei", ""], ["Xie", "Sihong", ""], ["Shao", "Weixiang", ""], ["Yu", "Philip S.", ""]]}, {"id": "1811.04504", "submitter": "Aaron Mishkin", "authors": "Aaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark Schmidt and\n  Mohammad Emtiyaz Khan", "title": "SLANG: Fast Structured Covariance Approximations for Bayesian Deep\n  Learning with Natural Gradient", "comments": "NeurIPS 2018 final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty estimation in large deep-learning models is a computationally\nchallenging task, where it is difficult to form even a Gaussian approximation\nto the posterior distribution. In such situations, existing methods usually\nresort to a diagonal approximation of the covariance matrix despite, the fact\nthat these matrices are known to result in poor uncertainty estimates. To\naddress this issue, we propose a new stochastic, low-rank, approximate\nnatural-gradient (SLANG) method for variational inference in large, deep\nmodels. Our method estimates a \"diagonal plus low-rank\" structure based solely\non back-propagated gradients of the network log-likelihood. This requires\nstrictly less gradient computations than methods that compute the gradient of\nthe whole variational objective. Empirical evaluations on standard benchmarks\nconfirm that SLANG enables faster and more accurate estimation of uncertainty\nthan mean-field methods, and performs comparably to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 23:18:27 GMT"}, {"version": "v2", "created": "Sat, 12 Jan 2019 01:01:06 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Mishkin", "Aaron", ""], ["Kunstner", "Frederik", ""], ["Nielsen", "Didrik", ""], ["Schmidt", "Mark", ""], ["Khan", "Mohammad Emtiyaz", ""]]}, {"id": "1811.04516", "submitter": "Oscar Chang", "authors": "Oscar Chang, Robert Kwiatkowski, Siyuan Chen, Hod Lipson", "title": "Agent Embeddings: A Latent Representation for Pole-Balancing Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that it is possible to reduce a high-dimensional object like a neural\nnetwork agent into a low-dimensional vector representation with semantic\nmeaning that we call agent embeddings, akin to word or face embeddings. This\ncan be done by collecting examples of existing networks, vectorizing their\nweights, and then learning a generative model over the weight space in a\nsupervised fashion. We investigate a pole-balancing task, Cart-Pole, as a case\nstudy and show that multiple new pole-balancing networks can be generated from\ntheir agent embeddings without direct access to training data from the\nCart-Pole simulator. In general, the learned embedding space is helpful for\nmapping out the space of solutions for a given task. We observe in the case of\nCart-Pole the surprising finding that good agents make different decisions\ndespite learning similar representations, whereas bad agents make similar (bad)\ndecisions while learning dissimilar representations. Linearly interpolating\nbetween the latent embeddings for a good agent and a bad agent yields an agent\nembedding that generates a network with intermediate performance, where the\nperformance can be tuned according to the coefficient of interpolation. Linear\nextrapolation in the latent space also results in performance boosts, up to a\npoint.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 00:31:59 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 20:35:01 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2019 22:31:49 GMT"}, {"version": "v4", "created": "Tue, 19 Mar 2019 03:12:53 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Chang", "Oscar", ""], ["Kwiatkowski", "Robert", ""], ["Chen", "Siyuan", ""], ["Lipson", "Hod", ""]]}, {"id": "1811.04539", "submitter": "Apoorva Nandini Saridena", "authors": "Naman Patel, Apoorva Nandini Saridena, Anna Choromanska, Prashanth\n  Krishnamurthy, Farshad Khorrami", "title": "Adversarial Learning-Based On-Line Anomaly Monitoring for Assured\n  Autonomy", "comments": "Proceedings of the 2018 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes an on-line monitoring framework for continuous real-time\nsafety/security in learning-based control systems (specifically application to\na unmanned ground vehicle). We monitor validity of mappings from sensor inputs\nto actuator commands, controller-focused anomaly detection (CFAM), and from\nactuator commands to sensor inputs, system-focused anomaly detection (SFAM).\nCFAM is an image conditioned energy based generative adversarial network\n(EBGAN) in which the energy based discriminator distinguishes between proper\nand anomalous actuator commands. SFAM is based on an action condition video\nprediction framework to detect anomalies between predicted and observed\ntemporal evolution of sensor data. We demonstrate the effectiveness of the\napproach on our autonomous ground vehicle for indoor environments and on\nUdacity dataset for outdoor environments.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 03:33:45 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Patel", "Naman", ""], ["Saridena", "Apoorva Nandini", ""], ["Choromanska", "Anna", ""], ["Krishnamurthy", "Prashanth", ""], ["Khorrami", "Farshad", ""]]}, {"id": "1811.04548", "submitter": "Liu Jiang", "authors": "Liu Jiang, Shixia Liu, Changjian Chen", "title": "Recent Research Advances on Interactive Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive Machine Learning (IML) is an iterative learning process that\ntightly couples a human with a machine learner, which is widely used by\nresearchers and practitioners to effectively solve a wide variety of real-world\napplication problems. Although recent years have witnessed the proliferation of\nIML in the field of visual analytics, most recent surveys either focus on a\nspecific area of IML or aim to summarize a visualization field that is too\ngeneric for IML. In this paper, we systematically review the recent literature\non IML and classify them into a task-oriented taxonomy built by us. We conclude\nthe survey with a discussion of open challenges and research opportunities that\nwe believe are inspiring for future work in IML.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 04:07:46 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Jiang", "Liu", ""], ["Liu", "Shixia", ""], ["Chen", "Changjian", ""]]}, {"id": "1811.04551", "submitter": "Danijar Hafner", "authors": "Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David\n  Ha, Honglak Lee, James Davidson", "title": "Learning Latent Dynamics for Planning from Pixels", "comments": "20 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning has been very successful for control tasks with known environment\ndynamics. To leverage planning in unknown environments, the agent needs to\nlearn the dynamics from interactions with the world. However, learning dynamics\nmodels that are accurate enough for planning has been a long-standing\nchallenge, especially in image-based domains. We propose the Deep Planning\nNetwork (PlaNet), a purely model-based agent that learns the environment\ndynamics from images and chooses actions through fast online planning in latent\nspace. To achieve high performance, the dynamics model must accurately predict\nthe rewards ahead for multiple time steps. We approach this using a latent\ndynamics model with both deterministic and stochastic transition components.\nMoreover, we propose a multi-step variational inference objective that we name\nlatent overshooting. Using only pixel observations, our agent solves continuous\ncontrol tasks with contact dynamics, partial observability, and sparse rewards,\nwhich exceed the difficulty of tasks that were previously solved by planning\nwith learned models. PlaNet uses substantially fewer episodes and reaches final\nperformance close to and sometimes higher than strong model-free algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 04:30:10 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 22:21:00 GMT"}, {"version": "v3", "created": "Thu, 14 Feb 2019 19:12:41 GMT"}, {"version": "v4", "created": "Wed, 15 May 2019 18:28:53 GMT"}, {"version": "v5", "created": "Tue, 4 Jun 2019 18:13:09 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Hafner", "Danijar", ""], ["Lillicrap", "Timothy", ""], ["Fischer", "Ian", ""], ["Villegas", "Ruben", ""], ["Ha", "David", ""], ["Lee", "Honglak", ""], ["Davidson", "James", ""]]}, {"id": "1811.04576", "submitter": "Yasuhiro Ikeda", "authors": "Yasuhiro Ikeda, Kengo Tajiri, Yuusuke Nakano, Keishiro Watanabe,\n  Keisuke Ishibashi", "title": "Estimation of Dimensions Contributing to Detected Anomalies with\n  Variational Autoencoders", "comments": null, "journal-ref": "AAAI-19 Workshop on Network Interpretability for Deep Learning,\n  2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection using dimensionality reduction has been an essential\ntechnique for monitoring multidimensional data. Although deep learning-based\nmethods have been well studied for their remarkable detection performance,\ntheir interpretability is still a problem. In this paper, we propose a novel\nalgorithm for estimating the dimensions contributing to the detected anomalies\nby using variational autoencoders (VAEs). Our algorithm is based on an\napproximative probabilistic model that considers the existence of anomalies in\nthe data, and by maximizing the log-likelihood, we estimate which dimensions\ncontribute to determining data as an anomaly. The experiments results with\nbenchmark datasets show that our algorithm extracts the contributing dimensions\nmore accurately than baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 06:36:52 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 02:32:05 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Ikeda", "Yasuhiro", ""], ["Tajiri", "Kengo", ""], ["Nakano", "Yuusuke", ""], ["Watanabe", "Keishiro", ""], ["Ishibashi", "Keisuke", ""]]}, {"id": "1811.04624", "submitter": "V\\'ictor Campos", "authors": "V\\'ictor Campos, Xavier Giro-i-Nieto, Jordi Torres", "title": "Importance Weighted Evolution Strategies", "comments": "NIPS Deep Reinforcement Learning Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolution Strategies (ES) emerged as a scalable alternative to popular\nReinforcement Learning (RL) techniques, providing an almost perfect speedup\nwhen distributed across hundreds of CPU cores thanks to a reduced communication\noverhead. Despite providing large improvements in wall-clock time, ES is data\ninefficient when compared to competing RL methods. One of the main causes of\nsuch inefficiency is the collection of large batches of experience, which are\ndiscarded after each policy update. In this work, we study how to perform more\nthan one update per batch of experience by means of Importance Sampling while\npreserving the scalability of the original method. The proposed method,\nImportance Weighted Evolution Strategies (IW-ES), shows promising results and\nis a first step towards designing efficient ES algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 09:44:50 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Campos", "V\u00edctor", ""], ["Giro-i-Nieto", "Xavier", ""], ["Torres", "Jordi", ""]]}, {"id": "1811.04631", "submitter": "Christoph Anderson", "authors": "Judith S. Heinisch, Christoph Anderson, Klaus David", "title": "Angry or Climbing Stairs? Towards Physiological Emotion Recognition in\n  the Wild", "comments": "6 pages, 5 figures, submitted to the 2018 IEEE International\n  Conference on Pervasive Computing and Communications Workshops (PerCom\n  Workshops), EmotionAware", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring emotions from physiological signals has gained much traction in the\nlast years. Physiological responses to emotions, however, are commonly\ninterfered and overlapped by physical activities, posing a challenge towards\nemotion recognition in the wild. In this paper, we address this challenge by\ninvestigating new features and machine-learning models for emotion recognition,\nnon-sensitive to physical-based interferences. We recorded physiological\nsignals from 18 participants that were exposed to emotions before and while\nperforming physical activities to assess the performance of non-sensitive\nemotion recognition models. We trained models with the least exhaustive\nphysical activity (sitting) and tested with the remaining, more exhausting\nactivities. For three different emotion categories, we achieve classification\naccuracies ranging from 47.88% - 73.35% for selected feature sets and per\nparticipant. Furthermore, we investigate the performance across all\nparticipants and of each activity individually. In this regard, we achieve\nsimilar results, between 55.17% and 67.41%, indicating the viability of emotion\nrecognition models not being influenced by single physical activities.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 09:51:52 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Heinisch", "Judith S.", ""], ["Anderson", "Christoph", ""], ["David", "Klaus", ""]]}, {"id": "1811.04634", "submitter": "Firat Ozdemir", "authors": "Firat Ozdemir, Orcun Goksel", "title": "Extending Pretrained Segmentation Networks with Additional Anatomical\n  Structures", "comments": "Published in IJCARS. 8 pages, 4 figures, contains supplementary\n  material", "journal-ref": "International Journal of Computer Assisted Radiology and Surgery,\n  2 May 2019, issn 1861-6429", "doi": "10.1007/s11548-019-01984-4", "report-no": null, "categories": "eess.IV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comprehensive surgical planning require complex patient-specific anatomical\nmodels. For instance, functional muskuloskeletal simulations necessitate all\nrelevant structures to be segmented, which could be performed in real-time\nusing deep neural networks given sufficient annotated samples. Such large\ndatasets of multiple structure annotations are costly to procure and are often\nunavailable in practice. Nevertheless, annotations from different studies and\ncenters can be readily available, or become available in the future in an\nincremental fashion. We propose a class-incremental segmentation framework for\nextending a deep network trained for some anatomical structure to yet another\nstructure using a small incremental annotation set. Through distilling\nknowledge from the current state of the framework, we bypass the need for a\nfull retraining. This is a meta-method to extend any choice of desired deep\nsegmentation network with only a minor addition per structure, which makes it\nsuitable for lifelong class-incremental learning and applicable also for future\ndeep neural network architectures. We evaluated our methods on a public knee\ndataset of 100 MR volumes. Through varying amount of incremental annotation\nratios, we show how our proposed method can retain the previous anatomical\nstructure segmentation performance superior to the conventional finetuning\napproach. In addition, our framework inherently exploits transferable knowledge\nfrom previously trained structures to incremental tasks, demonstrated by\nsuperior results compared to non-incremental training. With the presented\nmethod, new anatomical structures can be learned without catastrophic\nforgetting of older structures and without extensive increase of memory and\ncomplexity.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 09:58:07 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 12:31:38 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Ozdemir", "Firat", ""], ["Goksel", "Orcun", ""]]}, {"id": "1811.04646", "submitter": "Adrien Spagnol", "authors": "Adrien Spagnol, Rodolphe Le Riche and Sebastien Da Veiga", "title": "Global sensitivity analysis for optimization with variable selection", "comments": null, "journal-ref": "SIAM/ASA Journal on Uncertainty Quantification, ASA, American\n  Statistical Association, 2019", "doi": "10.1137/18M1167978", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimization of high dimensional functions is a key issue in engineering\nproblems but it frequently comes at a cost that is not acceptable since it\nusually involves a complex and expensive computer code. Engineers often\novercome this limitation by first identifying which parameters drive the most\nthe function variations: non-influential variables are set to a fixed value and\nthe optimization procedure is carried out with the remaining influential\nvariables. Such variable selection is performed through influence measures that\nare meaningful for regression problems. However it does not account for the\nspecific structure of optimization problems where we would like to identify\nwhich variables most lead to constraints satisfaction and low values of the\nobjective function. In this paper, we propose a new sensitivity analysis that\naccounts for the specific aspects of optimization problems. In particular, we\nintroduce an influence measure based on the Hilbert-Schmidt Independence\nCriterion to characterize whether a design variable matters to reach low values\nof the objective function and to satisfy the constraints. This sensitivity\nmeasure makes it possible to sort the inputs and reduce the problem dimension.\nWe compare a random and a greedy strategies to set the values of the\nnon-influential variables before conducting a local optimization. Applications\nto several test-cases show that this variable selection and the greedy strategy\nsignificantly reduce the number of function evaluations at a limited cost in\nterms of solution performance.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 10:41:20 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Spagnol", "Adrien", ""], ["Riche", "Rodolphe Le", ""], ["Da Veiga", "Sebastien", ""]]}, {"id": "1811.04661", "submitter": "Namita Jain Mrs", "authors": "Namita Jain, Susmita Ghosh, C. A. Murthy", "title": "RelDenClu: A Relative Density based Biclustering Method for identifying\n  non-linear feature relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing biclustering algorithms for finding feature relation based\nbiclusters often depend on assumptions like monotonicity or linearity. Though a\nfew algorithms overcome this problem by using density-based methods, they tend\nto miss out many biclusters because they use global criteria for identifying\ndense regions. The proposed method, RelDenClu uses the local variations in\nmarginal and joint densities for each pair of features to find the subset of\nobservations, which forms the bases of the relation between them. It then finds\nthe set of features connected by a common set of observations, resulting in a\nbicluster.\n  To show the effectiveness of the proposed methodology, experimentation has\nbeen carried out on fifteen types of simulated datasets. Further, it has been\napplied to six real-life datasets. For three of these real-life datasets, the\nproposed method is used for unsupervised learning, while for other three\nreal-life datasets it is used as an aid to supervised learning. For all the\ndatasets the performance of the proposed method is compared with that of seven\ndifferent state-of-the-art algorithms and the proposed algorithm is seen to\nproduce better results. The efficacy of proposed algorithm is also seen by its\nuse on COVID-19 dataset for identifying some features (genetic, demographics\nand others) that are likely to affect the spread of COVID-19.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 11:11:26 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 10:26:25 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 17:39:50 GMT"}, {"version": "v4", "created": "Thu, 28 May 2020 09:54:59 GMT"}, {"version": "v5", "created": "Tue, 11 May 2021 11:32:37 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Jain", "Namita", ""], ["Ghosh", "Susmita", ""], ["Murthy", "C. A.", ""]]}, {"id": "1811.04662", "submitter": "Navin Cooray", "authors": "Navin Cooray (1), Fernando Andreotti (1), Christine Lo (2), Mkael\n  Symmonds (3), Michele T.M. Hu (2) and Maarten De Vos (1) ((1) University of\n  Oxford, Institute of Biomedical Engineering, Dept. Engineering Sciences,\n  Oxford, UK, (2) Nuffield Department of Clinical Neurosciences, Oxford\n  Parkinson's Disease Centre (OPDC), University of Oxford, UK, (3) Department\n  of Clinical Neurophysiology, Oxford University Hospitals, John Radcliffe\n  Hospital, University of Oxford, UK)", "title": "Detection of REM Sleep Behaviour Disorder by Automated Polysomnography\n  Analysis", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Evidence suggests Rapid-Eye-Movement (REM) Sleep Behaviour Disorder (RBD) is\nan early predictor of Parkinson's disease. This study proposes a\nfully-automated framework for RBD detection consisting of automated sleep\nstaging followed by RBD identification. Analysis was assessed using a limited\npolysomnography montage from 53 participants with RBD and 53 age-matched\nhealthy controls. Sleep stage classification was achieved using a Random Forest\n(RF) classifier and 156 features extracted from electroencephalogram (EEG),\nelectrooculogram (EOG) and electromyogram (EMG) channels. For RBD detection, a\nRF classifier was trained combining established techniques to quantify muscle\natonia with additional features that incorporate sleep architecture and the EMG\nfractal exponent. Automated multi-state sleep staging achieved a 0.62 Cohen's\nKappa score. RBD detection accuracy improved by 10% to 96% (compared to\nindividual established metrics) when using manually annotated sleep staging.\nAccuracy remained high (92%) when using automated sleep staging. This study\noutperforms established metrics and demonstrates that incorporating sleep\narchitecture and sleep stage transitions can benefit RBD detection. This study\nalso achieved automated sleep staging with a level of accuracy comparable to\nmanual annotation. This study validates a tractable, fully-automated, and\nsensitive pipeline for RBD identification that could be translated to wearable\ntake-home technology.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 11:13:51 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Cooray", "Navin", ""], ["Andreotti", "Fernando", ""], ["Lo", "Christine", ""], ["Symmonds", "Mkael", ""], ["Hu", "Michele T. M.", ""], ["De Vos", "Maarten", ""]]}, {"id": "1811.04682", "submitter": "Songmin Dai", "authors": "Songmin Dai, Xiaoqiang Li, Lu Wang, Pin Wu, Weiqin Tong, Yimin Chen", "title": "Learning Segmentation Masks with the Independence Prior", "comments": "7+5 pages, 13 figures, Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An instance with a bad mask might make a composite image that uses it look\nfake. This encourages us to learn segmentation by generating realistic\ncomposite images. To achieve this, we propose a novel framework that exploits a\nnew proposed prior called the independence prior based on Generative\nAdversarial Networks (GANs). The generator produces an image with multiple\ncategory-specific instance providers, a layout module and a composition module.\nFirstly, each provider independently outputs a category-specific instance image\nwith a soft mask. Then the provided instances' poses are corrected by the\nlayout module. Lastly, the composition module combines these instances into a\nfinal image. Training with adversarial loss and penalty for mask area, each\nprovider learns a mask that is as small as possible but enough to cover a\ncomplete category-specific instance. Weakly supervised semantic segmentation\nmethods widely use grouping cues modeling the association between image parts,\nwhich are either artificially designed or learned with costly segmentation\nlabels or only modeled on local pairs. Unlike them, our method automatically\nmodels the dependence between any parts and learns instance segmentation. We\napply our framework in two cases: (1) Foreground segmentation on\ncategory-specific images with box-level annotation. (2) Unsupervised learning\nof instance appearances and masks with only one image of homogeneous object\ncluster (HOC). We get appealing results in both tasks, which shows the\nindependence prior is useful for instance segmentation and it is possible to\nunsupervisedly learn instance masks with only one image.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 12:06:30 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 11:40:15 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Dai", "Songmin", ""], ["Li", "Xiaoqiang", ""], ["Wang", "Lu", ""], ["Wu", "Pin", ""], ["Tong", "Weiqin", ""], ["Chen", "Yimin", ""]]}, {"id": "1811.04689", "submitter": "Che-Ping Tsai", "authors": "Che-Ping Tsai, Hung-Yi Lee", "title": "Adversarial Learning of Label Dependency: A Novel Framework for\n  Multi-class Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that exploiting relations between labels improves the\nperformance of multi-label classification. We propose a novel framework based\non generative adversarial networks (GANs) to model label dependency. The\ndiscriminator learns to model label dependency by discriminating real and\ngenerated label sets. To fool the discriminator, the classifier, or generator,\nlearns to generate label sets with dependencies close to real data. Extensive\nexperiments and comparisons on two large-scale image classification benchmark\ndatasets (MS-COCO and NUS-WIDE) show that the discriminator improves\ngeneralization ability for different kinds of models\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 12:29:17 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Tsai", "Che-Ping", ""], ["Lee", "Hung-Yi", ""]]}, {"id": "1811.04713", "submitter": "Michael Chertkov", "authors": "Michael Chertkov, Vladimir Chernyak and Yury Maximov", "title": "Gauges, Loops, and Polynomials for Partition Functions of Graphical\n  Models", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": "LA-UR-18-30593", "categories": "cs.LG math-ph math.MP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models represent multivariate and generally not normalized\nprobability distributions. Computing the normalization factor, called the\npartition function, is the main inference challenge relevant to multiple\nstatistical and optimization applications. The problem is of an exponential\ncomplexity with respect to the number of variables. In this manuscript, aimed\nat approximating the PF, we consider Multi-Graph Models where binary variables\nand multivariable factors are associated with edges and nodes, respectively, of\nan undirected multi-graph. We suggest a new methodology for analysis and\ncomputations that combines the Gauge Function technique with the technique from\nthe field of real stable polynomials. We show that the Gauge Function has a\nnatural polynomial representation in terms of gauges/variables associated with\nedges of the multi-graph. Moreover, it can be used to recover the Partition\nFunction through a sequence of transformations allowing appealing algebraic and\ngraphical interpretations. Algebraically, one step in the sequence consists in\napplication of a differential operator over gauges associated with an edge.\nGraphically, the sequence is interpreted as a repetitive elimination of edges\nresulting in a sequence of models on decreasing in size graphs with the same\nPartition Function. Even though complexity of computing factors in the sequence\nmodels grow exponentially with the number of eliminated edges, polynomials\nassociated with the new factors remain bi-stable if the original factors have\nthis property. Moreover, we show that Belief Propagation estimations in the\nsequence do not decrease, each low-bounding the Partition Function.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 13:27:42 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 18:07:08 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2018 13:15:04 GMT"}, {"version": "v4", "created": "Sun, 6 Jan 2019 06:41:58 GMT"}, {"version": "v5", "created": "Mon, 1 Apr 2019 02:24:32 GMT"}, {"version": "v6", "created": "Fri, 28 Aug 2020 20:52:03 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Chertkov", "Michael", ""], ["Chernyak", "Vladimir", ""], ["Maximov", "Yury", ""]]}, {"id": "1811.04727", "submitter": "Robert Walecki Mr", "authors": "Robert Walecki, Albert Buchard, Kostis Gourgoulias, Chris Hart, Maria\n  Lomeli, A. K. W. Navarro, Max Zwiessele, Yura Perov, Saurabh Johri", "title": "Universal Marginalizer for Amortised Inference and Embedding of\n  Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic graphical models are powerful tools which allow us to formalise\nour knowledge about the world and reason about its inherent uncertainty. There\nexist a considerable number of methods for performing inference in\nprobabilistic graphical models; however, they can be computationally costly due\nto significant time burden and/or storage requirements; or they lack\ntheoretical guarantees of convergence and accuracy when applied to large scale\ngraphical models. To this end, we propose the Universal Marginaliser Importance\nSampler (UM-IS) -- a hybrid inference scheme that combines the flexibility of a\ndeep neural network trained on samples from the model and inherits the\nasymptotic guarantees of importance sampling. We show how combining samples\ndrawn from the graphical model with an appropriate masking function allows us\nto train a single neural network to approximate any of the corresponding\nconditional marginal distributions, and thus amortise the cost of inference. We\nalso show that the graph embeddings can be applied for tasks such as:\nclustering, classification and interpretation of relationships between the\nnodes. Finally, we benchmark the method on a large graph (>1000 nodes), showing\nthat UM-IS outperforms sampling-based methods by a large margin while being\ncomputationally efficient.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 13:55:15 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Walecki", "Robert", ""], ["Buchard", "Albert", ""], ["Gourgoulias", "Kostis", ""], ["Hart", "Chris", ""], ["Lomeli", "Maria", ""], ["Navarro", "A. K. W.", ""], ["Zwiessele", "Max", ""], ["Perov", "Yura", ""], ["Johri", "Saurabh", ""]]}, {"id": "1811.04745", "submitter": "Xiaolei Ma", "authors": "Xiaolei Ma, Yi Li, Zhiyong Cui, Yinhai Wang", "title": "Forecasting Transportation Network Speed Using Deep Capsule Networks\n  with Nested LSTM Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and reliable traffic forecasting for complicated transportation\nnetworks is of vital importance to modern transportation management. The\ncomplicated spatial dependencies of roadway links and the dynamic temporal\npatterns of traffic states make it particularly challenging. To address these\nchallenges, we propose a new capsule network (CapsNet) to extract the spatial\nfeatures of traffic networks and utilize a nested LSTM (NLSTM) structure to\ncapture the hierarchical temporal dependencies in traffic sequence data. A\nframework for network-level traffic forecasting is also proposed by\nsequentially connecting CapsNet and NLSTM. On the basis of literature review,\nour study is the first to adopt CapsNet and NLSTM in the field of traffic\nforecasting. An experiment on a Beijing transportation network with 278 links\nshows that the proposed framework with the capability of capturing complicated\nspatiotemporal traffic patterns outperforms multiple state-of-the-art traffic\nforecasting baseline models. The superiority and feasibility of CapsNet and\nNLSTM are also demonstrated, respectively, by visualizing and quantitatively\nevaluating the experimental results.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 01:13:32 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Ma", "Xiaolei", ""], ["Li", "Yi", ""], ["Cui", "Zhiyong", ""], ["Wang", "Yinhai", ""]]}, {"id": "1811.04751", "submitter": "Jarek Duda dr", "authors": "Jarek Duda", "title": "Gaussian AutoEncoder", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative AutoEncoders require a chosen probability distribution in latent\nspace, usually multivariate Gaussian. The original Variational AutoEncoder\n(VAE) uses randomness in encoder - causing problematic distortion, and overlaps\nin latent space for distinct inputs. It turned out unnecessary: we can instead\nuse deterministic encoder with additional regularizer to ensure that sample\ndistribution in latent space is close to the required. The original approach\n(WAE) uses Wasserstein metric, what required comparing with random sample and\nusing an arbitrarily chosen kernel. Later CWAE finally derived a non-random\nanalytic formula by averaging $L_2$ distance of Gaussian-smoothened sample over\nall 1D projections. However, these arbitrarily chosen regularizers do not lead\nto Gaussian distribution.\n  This article proposes approach for regularizers directly optimizing agreement\nbetween empirical distribution function and its desired CDF for chosen\nproperties, for example radii and distances for Gaussian distribution, or\ncoordinate-wise, to directly attract this distribution in latent space of\nAutoEncoder. We can also attract different distributions with this general\napproach, for example latent space uniform distribution on $[0,1]^D$ hypercube\nor torus would allow for data compression without entropy coding, increased\ndensity near codewords would optimize for the required quantization.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 14:49:19 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 16:25:12 GMT"}, {"version": "v3", "created": "Wed, 26 Dec 2018 14:39:33 GMT"}, {"version": "v4", "created": "Mon, 14 Jan 2019 13:14:21 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Duda", "Jarek", ""]]}, {"id": "1811.04752", "submitter": "Brandon Malone", "authors": "Brandon Malone, Alberto Garcia-Duran, and Mathias Niepert", "title": "Learning Representations of Missing Data for Predicting Patient Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting actionable insight from Electronic Health Records (EHRs) poses\nseveral challenges for traditional machine learning approaches. Patients are\noften missing data relative to each other; the data comes in a variety of\nmodalities, such as multivariate time series, free text, and categorical\ndemographic information; important relationships among patients can be\ndifficult to detect; and many others. In this work, we propose a novel approach\nto address these first three challenges using a representation learning scheme\nbased on message passing. We show that our proposed approach is competitive\nwith or outperforms the state of the art for predicting in-hospital mortality\n(binary classification), the length of hospital visits (regression) and the\ndischarge destination (multiclass classification).\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 14:51:41 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Malone", "Brandon", ""], ["Garcia-Duran", "Alberto", ""], ["Niepert", "Mathias", ""]]}, {"id": "1811.04759", "submitter": "Gherardo Varando", "authors": "Gherardo Varando and Concha Bielza and Pedro Larra\\~naga and Eva\n  Riccomagno", "title": "Markov Property in Generative Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that, for generative classifiers, conditional independence\ncorresponds to linear constraints for the induced discrimination functions.\nDiscrimination functions of undirected Markov network classifiers can thus be\ncharacterized by sets of linear constraints. These constraints are represented\nby a second order finite difference operator over functions of categorical\nvariables. As an application we study the expressive power of generative\nclassifiers under the undirected Markov property and we present a general\nmethod to combine discriminative and generative classifiers.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 15:02:49 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Varando", "Gherardo", ""], ["Bielza", "Concha", ""], ["Larra\u00f1aga", "Pedro", ""], ["Riccomagno", "Eva", ""]]}, {"id": "1811.04768", "submitter": "Mingyang Geng", "authors": "Mingyang Geng, Kele Xu, Bo Ding, Huaimin Wang, Lei Zhang", "title": "Learning data augmentation policies using augmented random search", "comments": "Submitted to ICASSP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous attempts for data augmentation are designed manually, and the\naugmentation policies are dataset-specific. Recently, an automatic data\naugmentation approach, named AutoAugment, is proposed using reinforcement\nlearning. AutoAugment searches for the augmentation polices in the discrete\nsearch space, which may lead to a sub-optimal solution. In this paper, we\nemploy the Augmented Random Search method (ARS) to improve the performance of\nAutoAugment. Our key contribution is to change the discrete search space to\ncontinuous space, which will improve the searching performance and maintain the\ndiversities between sub-policies. With the proposed method, state-of-the-art\naccuracies are achieved on CIFAR-10, CIFAR-100, and ImageNet (without\nadditional data). Our code is available at https://github.com/gmy2013/ARS-Aug.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 15:14:18 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Geng", "Mingyang", ""], ["Xu", "Kele", ""], ["Ding", "Bo", ""], ["Wang", "Huaimin", ""], ["Zhang", "Lei", ""]]}, {"id": "1811.04769", "submitter": "Eunwoo Song", "authors": "Eunwoo Song, Kyungguen Byun, Hong-Goo Kang", "title": "ExcitNet vocoder: A neural excitation model for parametric speech\n  synthesis systems", "comments": "Accepted to the conference of EUSIPCO 2019. arXiv admin note: text\n  overlap with arXiv:1811.03311", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a WaveNet-based neural excitation model (ExcitNet) for\nstatistical parametric speech synthesis systems. Conventional WaveNet-based\nneural vocoding systems significantly improve the perceptual quality of\nsynthesized speech by statistically generating a time sequence of speech\nwaveforms through an auto-regressive framework. However, they often suffer from\nnoisy outputs because of the difficulties in capturing the complicated\ntime-varying nature of speech signals. To improve modeling efficiency, the\nproposed ExcitNet vocoder employs an adaptive inverse filter to decouple\nspectral components from the speech signal. The residual component, i.e.\nexcitation signal, is then trained and generated within the WaveNet framework.\nIn this way, the quality of the synthesized speech signal can be further\nimproved since the spectral component is well represented by a deep learning\nframework and, moreover, the residual component is efficiently generated by the\nWaveNet framework. Experimental results show that the proposed ExcitNet\nvocoder, trained both speaker-dependently and speaker-independently,\noutperforms traditional linear prediction vocoders and similarly configured\nconventional WaveNet vocoders.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 09:02:55 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 06:07:49 GMT"}, {"version": "v3", "created": "Wed, 21 Aug 2019 09:56:18 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Song", "Eunwoo", ""], ["Byun", "Kyungguen", ""], ["Kang", "Hong-Goo", ""]]}, {"id": "1811.04770", "submitter": "Bradley McDanel", "authors": "H. T. Kung and Bradley McDanel and Sai Qian Zhang", "title": "Packing Sparse Convolutional Neural Networks for Efficient Systolic\n  Array Implementations: Column Combining Under Joint Optimization", "comments": "To appear in ASPLOS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel approach of packing sparse convolutional neural\nnetworks for their efficient systolic array implementations. By combining\nsubsets of columns in the original filter matrix associated with a\nconvolutional layer, we increase the utilization efficiency of the systolic\narray substantially (e.g., ~4x) due to the increased density of nonzeros in the\nresulting packed filter matrix. In combining columns, for each row, all filter\nweights but one with the largest magnitude are pruned. We retrain the remaining\nweights to preserve high accuracy. We demonstrate that in mitigating data\nprivacy concerns the retraining can be accomplished with only fractions of the\noriginal dataset (e.g., 10\\% for CIFAR-10). We study the effectiveness of this\njoint optimization for both high utilization and classification accuracy with\nASIC and FPGA designs based on efficient bit-serial implementations of\nmultiplier-accumulators. We present analysis and empirical evidence on the\nsuperior performance of our column combining approach against prior arts under\nmetrics such as energy efficiency (3x) and inference latency (12x).\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 23:09:31 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Kung", "H. T.", ""], ["McDanel", "Bradley", ""], ["Zhang", "Sai Qian", ""]]}, {"id": "1811.04784", "submitter": "Tim Verbelen", "authors": "Xander Steenbrugge, Sam Leroux, Tim Verbelen, Bart Dhoedt", "title": "Improving Generalization for Abstract Reasoning Tasks Using Disentangled\n  Feature Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore the generalization characteristics of unsupervised\nrepresentation learning by leveraging disentangled VAE's to learn a useful\nlatent space on a set of relational reasoning problems derived from Raven\nProgressive Matrices. We show that the latent representations, learned by\nunsupervised training using the right objective function, significantly\noutperform the same architectures trained with purely supervised learning,\nespecially when it comes to generalization.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 15:23:26 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Steenbrugge", "Xander", ""], ["Leroux", "Sam", ""], ["Verbelen", "Tim", ""], ["Dhoedt", "Bart", ""]]}, {"id": "1811.04788", "submitter": "Sourish Das", "authors": "Rajiv Sambasivan and Sourish Das and Sujit K Sahu", "title": "A Bayesian Perspective of Statistical Machine Learning for Big Data", "comments": "26 pages, 3 figures, Review paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical Machine Learning (SML) refers to a body of algorithms and methods\nby which computers are allowed to discover important features of input data\nsets which are often very large in size. The very task of feature discovery\nfrom data is essentially the meaning of the keyword `learning' in SML.\nTheoretical justifications for the effectiveness of the SML algorithms are\nunderpinned by sound principles from different disciplines, such as Computer\nScience and Statistics. The theoretical underpinnings particularly justified by\nstatistical inference methods are together termed as statistical learning\ntheory.\n  This paper provides a review of SML from a Bayesian decision theoretic point\nof view -- where we argue that many SML techniques are closely connected to\nmaking inference by using the so called Bayesian paradigm. We discuss many\nimportant SML techniques such as supervised and unsupervised learning, deep\nlearning, online learning and Gaussian processes especially in the context of\nvery large data sets where these are often employed. We present a dictionary\nwhich maps the key concepts of SML from Computer Science and Statistics. We\nillustrate the SML techniques with three moderately large data sets where we\nalso discuss many practical implementation issues. Thus the review is\nespecially targeted at statisticians and computer scientists who are aspiring\nto understand and apply SML for moderately large to big data sets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 14:26:55 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 01:43:53 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Sambasivan", "Rajiv", ""], ["Das", "Sourish", ""], ["Sahu", "Sujit K", ""]]}, {"id": "1811.04803", "submitter": "Mark Chilenski", "authors": "Mark Chilenski, George Cybenko, Isaac Dekine, Piyush Kumar, Gil Raz", "title": "Observability Properties of Colored Graphs", "comments": "13 pages, 17 figures", "journal-ref": null, "doi": "10.1109/TNSE.2019.2948474", "report-no": null, "categories": "cs.LG math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A colored graph is a directed graph in which nodes or edges have been\nassigned colors that are not necessarily unique. Observability problems in such\ngraphs consider whether an agent observing the colors of edges or nodes\ntraversed on a path in the graph can determine which node they are at currently\nor which nodes were visited earlier in the traversal. Previous research efforts\nhave identified several different notions of observability as well as the\nassociated properties of graphs for which those observability properties hold.\nThis paper unifies the prior work into a common framework with several new\nresults about relationships between those notions and associated graph\nproperties. The new framework provides an intuitive way to reason about the\nattainable accuracy as a function of lag and time spent observing, and\nidentifies simple modifications to improve the observability of a given graph.\nWe show that one form of the graph modification problem is in NP-Complete. The\nintuition of the new framework is borne out with numerical experiments. This\nwork has implications for problems that can be described in terms of an agent\ntraversing a colored graph, including the reconstruction of hidden states in a\nhidden Markov model (HMM).\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 15:12:24 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 21:31:06 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Chilenski", "Mark", ""], ["Cybenko", "George", ""], ["Dekine", "Isaac", ""], ["Kumar", "Piyush", ""], ["Raz", "Gil", ""]]}, {"id": "1811.04817", "submitter": "Pedram Hassanzadeh", "authors": "Ashesh Chattopadhyay and Pedram Hassanzadeh and Saba Pasha", "title": "A test case for application of convolutional neural networks to\n  spatio-temporal climate data: Re-identifying clustered weather patterns", "comments": null, "journal-ref": "Scientific Reports, 2020", "doi": "10.1038/s41598-020-57897-9", "report-no": null, "categories": "physics.ao-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) can potentially provide powerful tools\nfor classifying and identifying patterns in climate and environmental data.\nHowever, because of the inherent complexities of such data, which are often\nspatio-temporal, chaotic, and non-stationary, the CNN algorithms must be\ndesigned/evaluated for each specific dataset and application. Yet to start,\nCNN, a supervised technique, requires a large labeled dataset. Labeling demands\n(human) expert time, which combined with the limited number of relevant\nexamples in this area, can discourage using CNNs for new problems. To address\nthese challenges, here we (1) Propose an effective auto-labeling strategy based\non using an unsupervised clustering algorithm and evaluating the performance of\nCNNs in re-identifying these clusters; (2) Use this approach to label thousands\nof daily large-scale weather patterns over North America in the outputs of a\nfully-coupled climate model and show the capabilities of CNNs in re-identifying\nthe 4 clustered regimes. The deep CNN trained with $1000$ samples or more per\ncluster has an accuracy of $90\\%$ or better. Accuracy scales monotonically but\nnonlinearly with the size of the training set, e.g. reaching $94\\%$ with $3000$\ntraining samples per cluster. Effects of architecture and hyperparameters on\nthe performance of CNNs are examined and discussed.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 15:56:30 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chattopadhyay", "Ashesh", ""], ["Hassanzadeh", "Pedram", ""], ["Pasha", "Saba", ""]]}, {"id": "1811.04820", "submitter": "Jessa Bekker", "authors": "Jessa Bekker and Jesse Davis", "title": "Learning from positive and unlabeled data: a survey", "comments": "There was a typo in section 2.4. The fraction of labeled examples in\n  the single-training-set scenario should be \\alpha c, and not \\alpha e(x) as\n  was written in the previous version", "journal-ref": "Machine Learning (2020) 1-42", "doi": "10.1007/s10994-020-05877-5", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from positive and unlabeled data or PU learning is the setting where\na learner only has access to positive examples and unlabeled data. The\nassumption is that the unlabeled data can contain both positive and negative\nexamples. This setting has attracted increasing interest within the machine\nlearning literature as this type of data naturally arises in applications such\nas medical diagnosis and knowledge base completion. This article provides a\nsurvey of the current state of the art in PU learning. It proposes seven key\nresearch questions that commonly arise in this field and provides a broad\noverview of how the field has tried to address them.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 16:00:36 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 11:31:44 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 10:33:23 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Bekker", "Jessa", ""], ["Davis", "Jesse", ""]]}, {"id": "1811.04852", "submitter": "Chunhao Wang", "authors": "Nai-Hui Chia, Han-Hsuan Lin, Chunhao Wang", "title": "Quantum-inspired sublinear classical algorithms for solving low-rank\n  linear systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present classical sublinear-time algorithms for solving low-rank linear\nsystems of equations. Our algorithms are inspired by the HHL quantum algorithm\nfor solving linear systems and the recent breakthrough by Tang of dequantizing\nthe quantum algorithm for recommendation systems. Let $A \\in \\mathbb{C}^{m\n\\times n}$ be a rank-$k$ matrix, and $b \\in \\mathbb{C}^m$ be a vector. We\npresent two algorithms: a \"sampling\" algorithm that provides a sample from\n$A^{-1}b$ and a \"query\" algorithm that outputs an estimate of an entry of\n$A^{-1}b$, where $A^{-1}$ denotes the Moore-Penrose pseudo-inverse. Both of our\nalgorithms have query and time complexity $O(\\mathrm{poly}(k, \\kappa, \\|A\\|_F,\n1/\\epsilon)\\,\\mathrm{polylog}(m, n))$, where $\\kappa$ is the condition number\nof $A$ and $\\epsilon$ is the precision parameter. Note that the algorithms we\nconsider are sublinear time, so they cannot write and read the whole matrix or\nvectors. In this paper, we assume that $A$ and $b$ come with well-known\nlow-overhead data structures such that entries of $A$ and $b$ can be sampled\naccording to some natural probability distributions. Alternatively, when $A$ is\npositive semidefinite, our algorithms can be adapted so that the sampling\nassumption on $b$ is not required.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 16:57:33 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Chia", "Nai-Hui", ""], ["Lin", "Han-Hsuan", ""], ["Wang", "Chunhao", ""]]}, {"id": "1811.04871", "submitter": "Rama Akkiraju Ms", "authors": "Rama Akkiraju, Vibha Sinha, Anbang Xu, Jalal Mahmud, Pritam Gundecha,\n  Zhe Liu, Xiaotong Liu, John Schumacher", "title": "Characterizing machine learning process: A maturity framework", "comments": "10 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Academic literature on machine learning modeling fails to address how to make\nmachine learning models work for enterprises. For example, existing machine\nlearning processes cannot address how to define business use cases for an AI\napplication, how to convert business requirements from offering managers into\ndata requirements for data scientists, and how to continuously improve AI\napplications in term of accuracy and fairness, and how to customize general\npurpose machine learning models with industry, domain, and use case specific\ndata to make them more accurate for specific situations etc. Making AI work for\nenterprises requires special considerations, tools, methods and processes. In\nthis paper we present a maturity framework for machine learning model lifecycle\nmanagement for enterprises. Our framework is a re-interpretation of the\nsoftware Capability Maturity Model (CMM) for machine learning model development\nprocess. We present a set of best practices from our personal experience of\nbuilding large scale real-world machine learning models to help organizations\nachieve higher levels of maturity independent of their starting point.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 17:32:24 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Akkiraju", "Rama", ""], ["Sinha", "Vibha", ""], ["Xu", "Anbang", ""], ["Mahmud", "Jalal", ""], ["Gundecha", "Pritam", ""], ["Liu", "Zhe", ""], ["Liu", "Xiaotong", ""], ["Schumacher", "John", ""]]}, {"id": "1811.04890", "submitter": "Zhao Wang", "authors": "Zhao Wang and Aron Culotta", "title": "When do Words Matter? Understanding the Impact of Lexical Choice on\n  Audience Perception using Individual Treatment Effect Estimation", "comments": "AAAI_2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies across many disciplines have shown that lexical choice can affect\naudience perception. For example, how users describe themselves in a social\nmedia profile can affect their perceived socio-economic status. However, we\nlack general methods for estimating the causal effect of lexical choice on the\nperception of a specific sentence. While randomized controlled trials may\nprovide good estimates, they do not scale to the potentially millions of\ncomparisons necessary to consider all lexical choices. Instead, in this paper,\nwe first offer two classes of methods to estimate the effect on perception of\nchanging one word to another in a given sentence. The first class of algorithms\nbuilds upon quasi-experimental designs to estimate individual treatment effects\nfrom observational data. The second class treats treatment effect estimation as\na classification problem. We conduct experiments with three data sources (Yelp,\nTwitter, and Airbnb), finding that the algorithmic estimates align well with\nthose produced by randomized-control trials. Additionally, we find that it is\npossible to transfer treatment effect classifiers across domains and still\nmaintain high accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 18:13:40 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 02:53:17 GMT"}, {"version": "v3", "created": "Wed, 14 Nov 2018 01:25:30 GMT"}, {"version": "v4", "created": "Thu, 15 Nov 2018 03:49:36 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Wang", "Zhao", ""], ["Culotta", "Aron", ""]]}, {"id": "1811.04907", "submitter": "Yannick Suter", "authors": "Yannick Suter, Alain Jungo, Michael Rebsamen, Urspeter Knecht, Evelyn\n  Herrmann, Roland Wiest, Mauricio Reyes", "title": "Deep Learning versus Classical Regression for Brain Tumor Patient\n  Survival Prediction", "comments": "Contribution to The International Multimodal Brain Tumor Segmentation\n  (BraTS) Challenge 2018, survival prediction task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning for regression tasks on medical imaging data has shown\npromising results. However, compared to other approaches, their power is\nstrongly linked to the dataset size. In this study, we evaluate\n3D-convolutional neural networks (CNNs) and classical regression methods with\nhand-crafted features for survival time regression of patients with high grade\nbrain tumors. The tested CNNs for regression showed promising but unstable\nresults. The best performing deep learning approach reached an accuracy of\n51.5% on held-out samples of the training set. All tested deep learning\nexperiments were outperformed by a Support Vector Classifier (SVC) using 30\nradiomic features. The investigated features included intensity, shape,\nlocation and deep features. The submitted method to the BraTS 2018 survival\nprediction challenge is an ensemble of SVCs, which reached a cross-validated\naccuracy of 72.2% on the BraTS 2018 training set, 57.1% on the validation set,\nand 42.9% on the testing set. The results suggest that more training data is\nnecessary for a stable performance of a CNN model for direct regression from\nmagnetic resonance images, and that non-imaging clinical patient information is\ncrucial along with imaging information.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 18:45:08 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Suter", "Yannick", ""], ["Jungo", "Alain", ""], ["Rebsamen", "Michael", ""], ["Knecht", "Urspeter", ""], ["Herrmann", "Evelyn", ""], ["Wiest", "Roland", ""], ["Reyes", "Mauricio", ""]]}, {"id": "1811.04911", "submitter": "Chang Liu", "authors": "Sophia Collet, Robert Dadashi, Zahi N. Karam, Chang Liu, Parinaz\n  Sobhani, Yevgeniy Vahlis, Ji Chao Zhang", "title": "Boosting Model Performance through Differentially Private Model\n  Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key factor in developing high performing machine learning models is the\navailability of sufficiently large datasets. This work is motivated by\napplications arising in Software as a Service (SaaS) companies where there\nexist numerous similar yet disjoint datasets from multiple client companies. To\novercome the challenges of insufficient data without explicitly aggregating the\nclients' datasets due to privacy concerns, one solution is to collect more data\nfor each individual client, another is to privately aggregate information from\nmodels trained on each client's data. In this work, two approaches for private\nmodel aggregation are proposed that enable the transfer of knowledge from\nexisting models trained on other companies' datasets to a new company with\nlimited labeled data while protecting each client company's underlying\nindividual sensitive information. The two proposed approaches are based on\nstate-of-the-art private learning algorithms: Differentially Private\nPermutation-based Stochastic Gradient Descent and Approximate Minima\nPerturbation. We empirically show that by leveraging differentially private\ntechniques, we can enable private model aggregation and augment data utility\nwhile providing provable mathematical guarantees on privacy. The proposed\nmethods thus provide significant business value for SaaS companies and their\nclients, specifically as a solution for the cold-start problem.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 18:50:17 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 17:48:03 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Collet", "Sophia", ""], ["Dadashi", "Robert", ""], ["Karam", "Zahi N.", ""], ["Liu", "Chang", ""], ["Sobhani", "Parinaz", ""], ["Vahlis", "Yevgeniy", ""], ["Zhang", "Ji Chao", ""]]}, {"id": "1811.04918", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu and Yuanzhi Li and Yingyu Liang", "title": "Learning and Generalization in Overparameterized Neural Networks, Going\n  Beyond Two Layers", "comments": "V1/V2/V3/V4 polish writing, V5 adds experiments, V6 reflects our\n  camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental learning theory behind neural networks remains largely open.\nWhat classes of functions can neural networks actually learn? Why doesn't the\ntrained network overfit when it is overparameterized?\n  In this work, we prove that overparameterized neural networks can learn some\nnotable concept classes, including two and three-layer networks with fewer\nparameters and smooth activations. Moreover, the learning can be simply done by\nSGD (stochastic gradient descent) or its variants in polynomial time using\npolynomially many samples. The sample complexity can also be almost independent\nof the number of parameters in the network.\n  On the technique side, our analysis goes beyond the so-called NTK (neural\ntangent kernel) linearization of neural networks in prior works. We establish a\nnew notion of quadratic approximation of the neural network (that can be viewed\nas a second-order variant of NTK), and connect it to the SGD theory of escaping\nsaddle points.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 18:57:02 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 15:56:01 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 04:10:51 GMT"}, {"version": "v4", "created": "Fri, 29 Mar 2019 17:09:46 GMT"}, {"version": "v5", "created": "Tue, 28 May 2019 10:25:09 GMT"}, {"version": "v6", "created": "Mon, 1 Jun 2020 17:11:51 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""], ["Liang", "Yingyu", ""]]}, {"id": "1811.04968", "submitter": "Joshua Izaac", "authors": "Ville Bergholm, Josh Izaac, Maria Schuld, Christian Gogolin, M. Sohaib\n  Alam, Shahnawaz Ahmed, Juan Miguel Arrazola, Carsten Blank, Alain Delgado,\n  Soran Jahangiri, Keri McKiernan, Johannes Jakob Meyer, Zeyue Niu, Antal\n  Sz\\'ava, Nathan Killoran", "title": "PennyLane: Automatic differentiation of hybrid quantum-classical\n  computations", "comments": "Code available at https://github.com/XanaduAI/pennylane/ .\n  Significant contributions to the code (new features, new plugins, etc.) will\n  be recognized by the opportunity to be a co-author on this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.ET cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PennyLane is a Python 3 software framework for optimization and machine\nlearning of quantum and hybrid quantum-classical computations. The library\nprovides a unified architecture for near-term quantum computing devices,\nsupporting both qubit and continuous-variable paradigms. PennyLane's core\nfeature is the ability to compute gradients of variational quantum circuits in\na way that is compatible with classical techniques such as backpropagation.\nPennyLane thus extends the automatic differentiation algorithms common in\noptimization and machine learning to include quantum and hybrid computations. A\nplugin system makes the framework compatible with any gate-based quantum\nsimulator or hardware. We provide plugins for Strawberry Fields, Rigetti\nForest, Qiskit, Cirq, and ProjectQ, allowing PennyLane optimizations to be run\non publicly accessible quantum devices provided by Rigetti and IBM Q. On the\nclassical front, PennyLane interfaces with accelerated machine learning\nlibraries such as TensorFlow, PyTorch, and autograd. PennyLane can be used for\nthe optimization of variational quantum eigensolvers, quantum approximate\noptimization, quantum machine learning models, and many other applications.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 19:18:57 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 15:37:17 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 00:18:42 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Bergholm", "Ville", ""], ["Izaac", "Josh", ""], ["Schuld", "Maria", ""], ["Gogolin", "Christian", ""], ["Alam", "M. Sohaib", ""], ["Ahmed", "Shahnawaz", ""], ["Arrazola", "Juan Miguel", ""], ["Blank", "Carsten", ""], ["Delgado", "Alain", ""], ["Jahangiri", "Soran", ""], ["McKiernan", "Keri", ""], ["Meyer", "Johannes Jakob", ""], ["Niu", "Zeyue", ""], ["Sz\u00e1va", "Antal", ""], ["Killoran", "Nathan", ""]]}, {"id": "1811.04973", "submitter": "Ehsan Kazemi", "authors": "Soheil Ghili and Ehsan Kazemi and Amin Karbasi", "title": "Eliminating Latent Discrimination: Train Then Mask", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we control for latent discrimination in predictive models? How can we\nprovably remove it? Such questions are at the heart of algorithmic fairness and\nits impacts on society. In this paper, we define a new operational fairness\ncriteria, inspired by the well-understood notion of omitted variable-bias in\nstatistics and econometrics. Our notion of fairness effectively controls for\nsensitive features and provides diagnostics for deviations from fair decision\nmaking. We then establish analytical and algorithmic results about the\nexistence of a fair classifier in the context of supervised learning. Our\nresults readily imply a simple, but rather counter-intuitive, strategy for\neliminating latent discrimination. In order to prevent other features proxying\nfor sensitive features, we need to include sensitive features in the training\nphase, but exclude them in the test/evaluation phase while controlling for\ntheir effects. We evaluate the performance of our algorithm on several\nreal-world datasets and show how fairness for these datasets can be improved\nwith a very small loss in accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 19:25:05 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 22:25:55 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Ghili", "Soheil", ""], ["Kazemi", "Ehsan", ""], ["Karbasi", "Amin", ""]]}, {"id": "1811.04983", "submitter": "Dimitri Kartsaklis", "authors": "Victor Prokhorov, Mohammad Taher Pilehvar, Dimitri Kartsaklis, Pietro\n  Lio, Nigel Collier", "title": "Unseen Word Representation by Aligning Heterogeneous Lexical Semantic\n  Spaces", "comments": "Accepted for presentation at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding techniques heavily rely on the abundance of training data for\nindividual words. Given the Zipfian distribution of words in natural language\ntexts, a large number of words do not usually appear frequently or at all in\nthe training data. In this paper we put forward a technique that exploits the\nknowledge encoded in lexical resources, such as WordNet, to induce embeddings\nfor unseen words. Our approach adapts graph embedding and cross-lingual vector\nspace transformation techniques in order to merge lexical knowledge encoded in\nontologies with that derived from corpus statistics. We show that the approach\ncan provide consistent performance improvements across multiple evaluation\nbenchmarks: in-vitro, on multiple rare word similarity datasets, and in-vivo,\nin two downstream text classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 20:02:00 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Prokhorov", "Victor", ""], ["Pilehvar", "Mohammad Taher", ""], ["Kartsaklis", "Dimitri", ""], ["Lio", "Pietro", ""], ["Collier", "Nigel", ""]]}, {"id": "1811.04985", "submitter": "Aswin Raghavan", "authors": "Samyak Parajuli, Aswin Raghavan, Sek Chai", "title": "Generalized Ternary Connect: End-to-End Learning and Compression of\n  Multiplication-Free Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of deep neural networks in edge computing devices hinges on the\nbalance between accuracy and complexity of computations. Ternary Connect (TC)\n\\cite{lin2015neural} addresses this issue by restricting the parameters to\nthree levels $-1, 0$, and $+1$, thus eliminating multiplications in the forward\npass of the network during prediction. We propose Generalized Ternary Connect\n(GTC), which allows an arbitrary number of levels while at the same time\neliminating multiplications by restricting the parameters to integer powers of\ntwo. The primary contribution is that GTC learns the number of levels and their\nvalues for each layer, jointly with the weights of the network in an end-to-end\nfashion. Experiments on MNIST and CIFAR-10 show that GTC naturally converges to\nan `almost binary' network for deep classification networks (e.g. VGG-16) and\ndeep variational auto-encoders, with negligible loss of classification accuracy\nand comparable visual quality of generated samples respectively. We demonstrate\nsuperior compression and similar accuracy of GTC in comparison to several\nstate-of-the-art methods for neural network compression. We conclude with\nsimulations showing the potential benefits of GTC in hardware.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 20:08:00 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Parajuli", "Samyak", ""], ["Raghavan", "Aswin", ""], ["Chai", "Sek", ""]]}, {"id": "1811.05010", "submitter": "Long Nguyen Msc", "authors": "Long Nguyen, Zhou Yang, Jiazhen Zhu, Jia Li, Fang Jin", "title": "Coordinating Disaster Emergency Response with Heuristic Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crucial and time-sensitive task when any disaster occurs is to rescue\nvictims and distribute resources to the right groups and locations. This task\nis challenging in populated urban areas, due to the huge burst of help requests\ngenerated in a very short period. To improve the efficiency of the emergency\nresponse in the immediate aftermath of a disaster, we propose a heuristic\nmulti-agent reinforcement learning scheduling algorithm, named as ResQ, which\ncan effectively schedule the rapid deployment of volunteers to rescue victims\nin dynamic settings. The core concept is to quickly identify victims and\nvolunteers from social network data and then schedule rescue parties with an\nadaptive learning algorithm. This framework performs two key functions: 1)\nidentify trapped victims and rescue volunteers, and 2) optimize the volunteers'\nrescue strategy in a complex time-sensitive environment. The proposed ResQ\nalgorithm can speed up the training processes through a heuristic function\nwhich reduces the state-action space by identifying the set of particular\nactions over others. Experimental results showed that the proposed heuristic\nmulti-agent reinforcement learning based scheduling outperforms several\nstate-of-art methods, in terms of both reward rate and response times.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 21:39:07 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Nguyen", "Long", ""], ["Yang", "Zhou", ""], ["Zhu", "Jiazhen", ""], ["Li", "Jia", ""], ["Jin", "Fang", ""]]}, {"id": "1811.05013", "submitter": "Ankesh Anand", "authors": "Ankesh Anand, Eugene Belilovsky, Kyle Kastner, Hugo Larochelle, Aaron\n  Courville", "title": "Blindfold Baselines for Embodied QA", "comments": "NIPS 2018 Visually-Grounded Interaction and Language (ViGilL)\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore blindfold (question-only) baselines for Embodied Question\nAnswering. The EmbodiedQA task requires an agent to answer a question by\nintelligently navigating in a simulated environment, gathering necessary visual\ninformation only through first-person vision before finally answering.\nConsequently, a blindfold baseline which ignores the environment and visual\ninformation is a degenerate solution, yet we show through our experiments on\nthe EQAv1 dataset that a simple question-only baseline achieves\nstate-of-the-art results on the EmbodiedQA task in all cases except when the\nagent is spawned extremely close to the object.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 21:45:41 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Anand", "Ankesh", ""], ["Belilovsky", "Eugene", ""], ["Kastner", "Kyle", ""], ["Larochelle", "Hugo", ""], ["Courville", "Aaron", ""]]}, {"id": "1811.05016", "submitter": "Shuang Li", "authors": "Shuang Li, Shuai Xiao, Shixiang Zhu, Nan Du, Yao Xie, Le Song", "title": "Learning Temporal Point Processes via Reinforcement Learning", "comments": "Add code link", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social goods, such as healthcare, smart city, and information networks, often\nproduce ordered event data in continuous time. The generative processes of\nthese event data can be very complex, requiring flexible models to capture\ntheir dynamics. Temporal point processes offer an elegant framework for\nmodeling event data without discretizing the time. However, the existing\nmaximum-likelihood-estimation (MLE) learning paradigm requires hand-crafting\nthe intensity function beforehand and cannot directly monitor the\ngoodness-of-fit of the estimated model in the process of training. To alleviate\nthe risk of model-misspecification in MLE, we propose to generate samples from\nthe generative model and monitor the quality of the samples in the process of\ntraining until the samples and the real data are indistinguishable. We take\ninspiration from reinforcement learning (RL) and treat the generation of each\nevent as the action taken by a stochastic policy. We parameterize the policy as\na flexible recurrent neural network and gradually improve the policy to mimic\nthe observed event distribution. Since the reward function is unknown in this\nsetting, we uncover an analytic and nonparametric form of the reward function\nusing an inverse reinforcement learning formulation. This new RL framework\nallows us to derive an efficient policy gradient algorithm for learning\nflexible point process models, and we show that it performs well in both\nsynthetic and real data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 21:56:27 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2020 21:53:06 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Li", "Shuang", ""], ["Xiao", "Shuai", ""], ["Zhu", "Shixiang", ""], ["Du", "Nan", ""], ["Xie", "Yao", ""], ["Song", "Le", ""]]}, {"id": "1811.05027", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Shiyang Cheng and Evangelos Ververas and Irene\n  Kotsia and Stefanos Zafeiriou", "title": "Deep Neural Network Augmentation: Generating Faces for Affect Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for synthesizing facial affect; either\nin terms of the six basic expressions (i.e., anger, disgust, fear, joy, sadness\nand surprise), or in terms of valence (i.e., how positive or negative is an\nemotion) and arousal (i.e., power of the emotion activation). The proposed\napproach accepts the following inputs: i) a neutral 2D image of a person; ii) a\nbasic facial expression or a pair of valence-arousal (VA) emotional state\ndescriptors to be generated, or a path of affect in the 2D VA Space to be\ngenerated as an image sequence. In order to synthesize affect in terms of VA,\nfor this person, $600,000$ frames from the 4DFAB database were annotated. The\naffect synthesis is implemented by fitting a 3D Morphable Model on the neutral\nimage, then deforming the reconstructed face and adding the inputted affect,\nand blending the new face with the given affect into the original image.\nQualitative experiments illustrate the generation of realistic images, when the\nneutral image is sampled from thirteen well known lab-controlled or in-the-wild\ndatabases, including Aff-Wild, AffectNet, RAF-DB; comparisons with Generative\nAdversarial Networks (GANs) show the higher quality achieved by the proposed\napproach. Then, quantitative experiments are conducted, in which the\nsynthesized images are used for data augmentation in training Deep Neural\nNetworks to perform affect recognition over all databases; greatly improved\nperformances are achieved when compared with state-of-the-art methods, as well\nas with GAN-based data augmentation, in all cases.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 22:42:40 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 21:33:58 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Cheng", "Shiyang", ""], ["Ververas", "Evangelos", ""], ["Kotsia", "Irene", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1811.05039", "submitter": "Zhenyu A. Liao", "authors": "Zhenyu A. Liao, Charupriya Sharma, James Cussens and Peter van Beek", "title": "Finding All Bayesian Network Structures within a Factor of Optimal", "comments": "11 pages with supplemental material, to appear at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian network is a widely used probabilistic graphical model with\napplications in knowledge discovery and prediction. Learning a Bayesian network\n(BN) from data can be cast as an optimization problem using the well-known\nscore-and-search approach. However, selecting a single model (i.e., the best\nscoring BN) can be misleading or may not achieve the best possible accuracy. An\nalternative to committing to a single model is to perform some form of Bayesian\nor frequentist model averaging, where the space of possible BNs is sampled or\nenumerated in some fashion. Unfortunately, existing approaches for model\naveraging either severely restrict the structure of the Bayesian network or\nhave only been shown to scale to networks with fewer than 30 random variables.\nIn this paper, we propose a novel approach to model averaging inspired by\nperformance guarantees in approximation algorithms. Our approach has two\nprimary advantages. First, our approach only considers credible models in that\nthey are optimal or near-optimal in score. Second, our approach is more\nefficient and scales to significantly larger Bayesian networks than existing\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 23:19:51 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Liao", "Zhenyu A.", ""], ["Sharma", "Charupriya", ""], ["Cussens", "James", ""], ["van Beek", "Peter", ""]]}, {"id": "1811.05042", "submitter": "Jun Wen", "authors": "Jun Wen, Risheng Liu, Nenggan Zheng, Qian Zheng, Zhefeng Gong, Junsong\n  Yuan", "title": "Exploiting Local Feature Patterns for Unsupervised Domain Adaptation", "comments": "AAAI-2019 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation methods aim to alleviate performance\ndegradation caused by domain-shift by learning domain-invariant\nrepresentations. Existing deep domain adaptation methods focus on holistic\nfeature alignment by matching source and target holistic feature distributions,\nwithout considering local features and their multi-mode statistics. We show\nthat the learned local feature patterns are more generic and transferable and a\nfurther local feature distribution matching enables fine-grained feature\nalignment. In this paper, we present a method for learning domain-invariant\nlocal feature patterns and jointly aligning holistic and local feature\nstatistics. Comparisons to the state-of-the-art unsupervised domain adaptation\nmethods on two popular benchmark datasets demonstrate the superiority of our\napproach and its effectiveness on alleviating negative transfer.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 23:23:23 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 20:27:16 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Wen", "Jun", ""], ["Liu", "Risheng", ""], ["Zheng", "Nenggan", ""], ["Zheng", "Qian", ""], ["Gong", "Zhefeng", ""], ["Yuan", "Junsong", ""]]}, {"id": "1811.05062", "submitter": "Chandrasekaran Anirudh Bhardwaj", "authors": "Chandrasekaran Anirudh Bhardwaj, Megha Mishra, Kalyani Desikan", "title": "Dynamic Feature Scaling for K-Nearest Neighbor Algorithm", "comments": "Presented in International Conference on Mathematical Computer\n  Engineering 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Nearest Neighbors Algorithm is a Lazy Learning Algorithm, in which the\nalgorithm tries to approximate the predictions with the help of similar\nexisting vectors in the training dataset. The predictions made by the K-Nearest\nNeighbors algorithm is based on averaging the target values of the spatial\nneighbors. The selection process for neighbors in the Hermitian space is done\nwith the help of distance metrics such as Euclidean distance, Minkowski\ndistance, Mahalanobis distance etc. A majority of the metrics such as Euclidean\ndistance are scale variant, meaning that the results could vary for different\nrange of values used for the features. Standard techniques used for the\nnormalization of scaling factors are feature scaling method such as Z-score\nnormalization technique, Min-Max scaling etc. Scaling methods uniformly assign\nequal weights to all the features, which might result in a non-ideal situation.\nThis paper proposes a novel method to assign weights to individual feature with\nthe help of out of bag errors obtained from constructing multiple decision tree\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 01:44:55 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Bhardwaj", "Chandrasekaran Anirudh", ""], ["Mishra", "Megha", ""], ["Desikan", "Kalyani", ""]]}, {"id": "1811.05072", "submitter": "Ji Wang", "authors": "Ji Wang and Weidong Bao and Lichao Sun and Xiaomin Zhu and Bokai Cao\n  and Philip S. Yu", "title": "Private Model Compression via Knowledge Distillation", "comments": "Conference version accepted by AAAI'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The soaring demand for intelligent mobile applications calls for deploying\npowerful deep neural networks (DNNs) on mobile devices. However, the\noutstanding performance of DNNs notoriously relies on increasingly complex\nmodels, which in turn is associated with an increase in computational expense\nfar surpassing mobile devices' capacity. What is worse, app service providers\nneed to collect and utilize a large volume of users' data, which contain\nsensitive information, to build the sophisticated DNN models. Directly\ndeploying these models on public mobile devices presents prohibitive privacy\nrisk. To benefit from the on-device deep learning without the capacity and\nprivacy concerns, we design a private model compression framework RONA.\nFollowing the knowledge distillation paradigm, we jointly use hint learning,\ndistillation learning, and self learning to train a compact and fast neural\nnetwork. The knowledge distilled from the cumbersome model is adaptively\nbounded and carefully perturbed to enforce differential privacy. We further\npropose an elegant query sample selection method to reduce the number of\nqueries and control the privacy loss. A series of empirical evaluations as well\nas the implementation on an Android mobile device show that RONA can not only\ncompress cumbersome models efficiently but also provide a strong privacy\nguarantee. For example, on SVHN, when a meaningful\n$(9.83,10^{-6})$-differential privacy is guaranteed, the compact model trained\nby RONA can obtain 20$\\times$ compression ratio and 19$\\times$ speed-up with\nmerely 0.97% accuracy loss.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 02:21:57 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Wang", "Ji", ""], ["Bao", "Weidong", ""], ["Sun", "Lichao", ""], ["Zhu", "Xiaomin", ""], ["Cao", "Bokai", ""], ["Yu", "Philip S.", ""]]}, {"id": "1811.05076", "submitter": "Miaoyan Wang", "authors": "Miaoyan Wang and Lexin Li", "title": "Learning from Binary Multiway Data: Probabilistic Tensor Decomposition\n  and its Statistical Optimality", "comments": "35 pages, 7 figures, 4 tables", "journal-ref": "Journal of Machine Learning Research, 21(154): 1-38, 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of decomposing a higher-order tensor with binary\nentries. Such data problems arise frequently in applications such as\nneuroimaging, recommendation system, topic modeling, and sensor network\nlocalization. We propose a multilinear Bernoulli model, develop a\nrank-constrained likelihood-based estimation method, and obtain the theoretical\naccuracy guarantees. In contrast to continuous-valued problems, the binary\ntensor problem exhibits an interesting phase transition phenomenon according to\nthe signal-to-noise ratio. The error bound for the parameter tensor estimation\nis established, and we show that the obtained rate is minimax optimal under the\nconsidered model. Furthermore, we develop an alternating optimization algorithm\nwith convergence guarantees. The efficacy of our approach is demonstrated\nthrough both simulations and analyses of multiple data sets on the tasks of\ntensor completion and clustering.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 02:49:17 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 04:48:47 GMT"}, {"version": "v3", "created": "Sun, 20 Sep 2020 04:05:02 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Wang", "Miaoyan", ""], ["Li", "Lexin", ""]]}, {"id": "1811.05090", "submitter": "Joseph Marino", "authors": "Joseph Marino, Milan Cvitkovic, Yisong Yue", "title": "A General Method for Amortizing Variational Filtering", "comments": "Advances in Neural Information Processing Systems (NIPS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the variational filtering EM algorithm, a simple,\ngeneral-purpose method for performing variational inference in dynamical latent\nvariable models using information from only past and present variables, i.e.\nfiltering. The algorithm is derived from the variational objective in the\nfiltering setting and consists of an optimization procedure at each time step.\nBy performing each inference optimization procedure with an iterative amortized\ninference model, we obtain a computationally efficient implementation of the\nalgorithm, which we call amortized variational filtering. We present\nexperiments demonstrating that this general-purpose method improves performance\nacross several deep dynamical latent variable models.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 03:39:27 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Marino", "Joseph", ""], ["Cvitkovic", "Milan", ""], ["Yue", "Yisong", ""]]}, {"id": "1811.05095", "submitter": "Sergul Aydore", "authors": "Sergul Aydore, Lee Dicker, Dean Foster", "title": "A Local Regret in Nonconvex Online Learning", "comments": "Continual Workshop at NIPS 2018, 2 figures, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an online learning process to forecast a sequence of outcomes for\nnonconvex models. A typical measure to evaluate online learning algorithms is\nregret but such standard definition of regret is intractable for nonconvex\nmodels even in offline settings. Hence, gradient based definition of regrets\nare common for both offline and online nonconvex problems. Recently, a notion\nof local gradient based regret was introduced. Inspired by the concept of\ncalibration and a local gradient based regret, we introduce another definition\nof regret and we discuss why our definition is more interpretable for\nforecasting problems. We also provide bound analysis for our regret under\ncertain assumptions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 04:19:19 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 22:14:41 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Aydore", "Sergul", ""], ["Dicker", "Lee", ""], ["Foster", "Dean", ""]]}, {"id": "1811.05097", "submitter": "Pan Zhou", "authors": "Senmao Wang, Pan Zhou, Wei Chen, Jia Jia, Lei Xie", "title": "Exploring RNN-Transducer for Chinese Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end approaches have drawn much attention recently for significantly\nsimplifying the construction of an automatic speech recognition (ASR) system.\nRNN transducer (RNN-T) is one of the popular end-to-end methods. Previous\nstudies have shown that RNN-T is difficult to train and a very complex training\nprocess is needed for a reasonable performance. In this paper, we explore RNN-T\nfor a Chinese large vocabulary continuous speech recognition (LVCSR) task and\naim to simplify the training process while maintaining performance. First, a\nnew strategy of learning rate decay is proposed to accelerate the model\nconvergence. Second, we find that adding convolutional layers at the beginning\nof the network and using ordered data can discard the pre-training process of\nthe encoder without loss of performance. Besides, we design experiments to find\na balance among the usage of GPU memory, training circle and model performance.\nFinally, we achieve 16.9% character error rate (CER) on our test set which is\n2% absolute improvement from a strong BLSTM CE system with language model\ntrained on the same text corpus.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 04:37:11 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 03:39:24 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Wang", "Senmao", ""], ["Zhou", "Pan", ""], ["Chen", "Wei", ""], ["Jia", "Jia", ""], ["Xie", "Lei", ""]]}, {"id": "1811.05105", "submitter": "Arjun Punjabi", "authors": "Arjun Punjabi, Adam Martersteck, Yanran Wang, Todd B. Parrish, Aggelos\n  K. Katsaggelos, and the Alzheimer's Disease Neuroimaging Initiative", "title": "Neuroimaging Modality Fusion in Alzheimer's Classification Using\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0225759", "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated methods for Alzheimer's disease (AD) classification have the\npotential for great clinical benefits and may provide insight for combating the\ndisease. Machine learning, and more specifically deep neural networks, have\nbeen shown to have great efficacy in this domain. These algorithms often use\nneurological imaging data such as MRI and PET, but a comprehensive and balanced\ncomparison of these modalities has not been performed. In order to accurately\ndetermine the relative strength of each imaging variant, this work performs a\ncomparison study in the context of Alzheimer's dementia classification using\nthe Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Furthermore,\nthis work analyzes the benefits of using both modalities in a fusion setting\nand discusses how these data types may be leveraged in future AD studies using\ndeep learning.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 04:53:54 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Punjabi", "Arjun", ""], ["Martersteck", "Adam", ""], ["Wang", "Yanran", ""], ["Parrish", "Todd B.", ""], ["Katsaggelos", "Aggelos K.", ""], ["Initiative", "the Alzheimer's Disease Neuroimaging", ""]]}, {"id": "1811.05121", "submitter": "Chang Xu", "authors": "Chang Xu, Weiran Huang, Hongwei Wang, Gang Wang and Tie-Yan Liu", "title": "Modeling Local Dependence in Natural Language with Multi-channel\n  Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) have been widely used in processing natural\nlanguage tasks and achieve huge success. Traditional RNNs usually treat each\ntoken in a sentence uniformly and equally. However, this may miss the rich\nsemantic structure information of a sentence, which is useful for understanding\nnatural languages. Since semantic structures such as word dependence patterns\nare not parameterized, it is a challenge to capture and leverage structure\ninformation. In this paper, we propose an improved variant of RNN,\nMulti-Channel RNN (MC-RNN), to dynamically capture and leverage local semantic\nstructure information. Concretely, MC-RNN contains multiple channels, each of\nwhich represents a local dependence pattern at a time. An attention mechanism\nis introduced to combine these patterns at each step, according to the semantic\ninformation. Then we parameterize structure information by adaptively selecting\nthe most appropriate connection structures among channels. In this way, diverse\nlocal structures and dependence patterns in sentences can be well captured by\nMC-RNN. To verify the effectiveness of MC-RNN, we conduct extensive experiments\non typical natural language processing tasks, including neural machine\ntranslation, abstractive summarization, and language modeling. Experimental\nresults on these tasks all show significant improvements of MC-RNN over current\ntop systems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 06:22:02 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Xu", "Chang", ""], ["Huang", "Weiran", ""], ["Wang", "Hongwei", ""], ["Wang", "Gang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1811.05134", "submitter": "Xiaowei Chen", "authors": "Xiaowei Chen, Weiran Huang, Wei Chen, John C.S. Lui", "title": "Community Exploration: From Offline Optimization to Online Learning", "comments": "full version of the nips'18 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the community exploration problem that has many real-world\napplications such as online advertising. In the problem, an explorer allocates\nlimited budget to explore communities so as to maximize the number of members\nhe could meet. We provide a systematic study of the community exploration\nproblem, from offline optimization to online learning. For the offline setting\nwhere the sizes of communities are known, we prove that the greedy methods for\nboth of non-adaptive exploration and adaptive exploration are optimal. For the\nonline setting where the sizes of communities are not known and need to be\nlearned from the multi-round explorations, we propose an `upper confidence'\nlike algorithm that achieves the logarithmic regret bounds. By combining the\nfeedback from different rounds, we can achieve a constant regret bound.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 07:18:38 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 07:01:47 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Chen", "Xiaowei", ""], ["Huang", "Weiran", ""], ["Chen", "Wei", ""], ["Lui", "John C. S.", ""]]}, {"id": "1811.05141", "submitter": "Zhuoyi Wang", "authors": "Zhuoyi Wang, Zelun Kong, Hemeng Tao, Swarup Chandra, Latifur Khan", "title": "Co-Representation Learning For Classification and Novel Class Detection\n  via Deep Networks", "comments": "The paper absence of relative theoretical prove, some supplement\n  experiment should also be add", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges of performing label prediction over a data stream\nconcerns with the emergence of instances belonging to unobserved class labels\nover time. Previously, this problem has been addressed by detecting such\ninstances and using them for appropriate classifier adaptation. The fundamental\naspect of a novel-class detection strategy relies on the ability of comparison\namong observed instances to discriminate them into known and unknown classes.\nTherefore, studies in the past have proposed various metrics suitable for\ncomparison over the observed feature space. Unfortunately, these similarity\nmeasures fail to reliably identify distinct regions in observed feature spaces\nuseful for class discrimination and novel-class detection, especially in\nstreams containing high-dimensional data instances such as images and texts. In\nthis paper, we address this key challenge by proposing a semi-supervised\nmulti-task learning framework called \\sysname{} which aims to intrinsically\nsearch for a latent space suitable for detecting labels of instances from both\nknown and unknown classes. We empirically measure the performance of \\sysname{}\nover multiple real-world image and text datasets and demonstrate its\nsuperiority by comparing its performance with existing semi-supervised methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 07:39:37 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 03:11:33 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Wang", "Zhuoyi", ""], ["Kong", "Zelun", ""], ["Tao", "Hemeng", ""], ["Chandra", "Swarup", ""], ["Khan", "Latifur", ""]]}, {"id": "1811.05154", "submitter": "Branislav Kveton", "authors": "Branislav Kveton, Csaba Szepesvari, Sharan Vaswani, Zheng Wen,\n  Mohammad Ghavamzadeh, and Tor Lattimore", "title": "Garbage In, Reward Out: Bootstrapping Exploration in Multi-Armed Bandits", "comments": "Proceedings of the 36th International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a bandit algorithm that explores by randomizing its history of\nrewards. Specifically, it pulls the arm with the highest mean reward in a\nnon-parametric bootstrap sample of its history with pseudo rewards. We design\nthe pseudo rewards such that the bootstrap mean is optimistic with a\nsufficiently high probability. We call our algorithm Giro, which stands for\ngarbage in, reward out. We analyze Giro in a Bernoulli bandit and derive a $O(K\n\\Delta^{-1} \\log n)$ bound on its $n$-round regret, where $\\Delta$ is the\ndifference in the expected rewards of the optimal and the best suboptimal arms,\nand $K$ is the number of arms. The main advantage of our exploration design is\nthat it easily generalizes to structured problems. To show this, we propose\ncontextual Giro with an arbitrary reward generalization model. We evaluate Giro\nand its contextual variant on multiple synthetic and real-world problems, and\nobserve that it performs well.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 08:15:39 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 04:53:11 GMT"}, {"version": "v3", "created": "Fri, 21 Jun 2019 03:53:30 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Kveton", "Branislav", ""], ["Szepesvari", "Csaba", ""], ["Vaswani", "Sharan", ""], ["Wen", "Zheng", ""], ["Ghavamzadeh", "Mohammad", ""], ["Lattimore", "Tor", ""]]}, {"id": "1811.05157", "submitter": "Bin Yang", "authors": "Jilin Hu, Chenjuan Guo, Bin Yang, Christian S. Jensen, Lu Chen", "title": "Recurrent Multi-Graph Neural Networks for Travel Cost Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Origin-destination (OD) matrices are often used in urban planning, where a\ncity is partitioned into regions and an element (i, j) in an OD matrix records\nthe cost (e.g., travel time, fuel consumption, or travel speed) from region i\nto region j. In this paper, we partition a day into multiple intervals, e.g.,\n96 15-min intervals and each interval is associated with an OD matrix which\nrepresents the costs in the interval; and we consider sparse and stochastic OD\nmatrices, where the elements represent stochastic but not deterministic costs\nand some elements are missing due to lack of data between two regions. We solve\nthe sparse, stochastic OD matrix forecasting problem. Given a sequence of\nhistorical OD matrices that are sparse, we aim at predicting future OD matrices\nwith no empty elements. We propose a generic learning framework to solve the\nproblem by dealing with sparse matrices via matrix factorization and two graph\nconvolutional neural networks and capturing temporal dynamics via recurrent\nneural network. Empirical studies using two taxi datasets from different\ncountries verify the effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 08:19:41 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Hu", "Jilin", ""], ["Guo", "Chenjuan", ""], ["Yang", "Bin", ""], ["Jensen", "Christian S.", ""], ["Chen", "Lu", ""]]}, {"id": "1811.05187", "submitter": "Lei Ma", "authors": "Qianyu Guo, Xiaofei Xie, Lei Ma, Qiang Hu, Ruitao Feng, Li Li, Yang\n  Liu, Jianjun Zhao, Xiaohong Li", "title": "An Orchestrated Empirical Study on Deep Learning Frameworks and\n  Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) has recently achieved tremendous success in a variety of\ncutting-edge applications, e.g., image recognition, speech and natural language\nprocessing, and autonomous driving. Besides the available big data and hardware\nevolution, DL frameworks and platforms play a key role to catalyze the\nresearch, development, and deployment of DL intelligent solutions. However, the\ndifference in computation paradigm, architecture design and implementation of\nexisting DL frameworks and platforms brings challenges for DL software\ndevelopment, deployment, maintenance, and migration. Up to the present, it\nstill lacks a comprehensive study on how current diverse DL frameworks and\nplatforms influence the DL software development process.\n  In this paper, we initiate the first step towards the investigation on how\nexisting state-of-the-art DL frameworks (i.e., TensorFlow, Theano, and Torch)\nand platforms (i.e., server/desktop, web, and mobile) support the DL software\ndevelopment activities. We perform an in-depth and comparative evaluation on\nmetrics such as learning accuracy, DL model size, robustness, and performance,\non state-of-the-art DL frameworks across platforms using two popular datasets\nMNIST and CIFAR-10. Our study reveals that existing DL frameworks still suffer\nfrom compatibility issues, which becomes even more severe when it comes to\ndifferent platforms. We pinpoint the current challenges and opportunities\ntowards developing high quality and compatible DL systems. To ignite further\ninvestigation along this direction to address urgent industrial demands of\nintelligent solutions, we make all of our assembled feasible toolchain and\ndataset publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 09:51:57 GMT"}], "update_date": "2018-11-18", "authors_parsed": [["Guo", "Qianyu", ""], ["Xie", "Xiaofei", ""], ["Ma", "Lei", ""], ["Hu", "Qiang", ""], ["Feng", "Ruitao", ""], ["Li", "Li", ""], ["Liu", "Yang", ""], ["Zhao", "Jianjun", ""], ["Li", "Xiaohong", ""]]}, {"id": "1811.05232", "submitter": "Zhuozhuo Tu", "authors": "Zhuozhuo Tu, Jingwei Zhang, Dacheng Tao", "title": "Theoretical Analysis of Adversarial Learning: A Minimax Approach", "comments": "27 pages, add some references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Here we propose a general theoretical method for analyzing the risk bound in\nthe presence of adversaries. Specifically, we try to fit the adversarial\nlearning problem into the minimax framework. We first show that the original\nadversarial learning problem can be reduced to a minimax statistical learning\nproblem by introducing a transport map between distributions. Then, we prove a\nnew risk bound for this minimax problem in terms of covering numbers under a\nweak version of Lipschitz condition. Our method can be applied to multi-class\nclassification problems and commonly used loss functions such as the hinge and\nramp losses. As some illustrative examples, we derive the adversarial risk\nbounds for SVMs, deep neural networks, and PCA, and our bounds have two\ndata-dependent terms, which can be optimized for achieving adversarial\nrobustness.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 11:48:43 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 00:24:09 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Tu", "Zhuozhuo", ""], ["Zhang", "Jingwei", ""], ["Tao", "Dacheng", ""]]}, {"id": "1811.05233", "submitter": "Yuichi Kageyama", "authors": "Hiroaki Mikami, Hisahiro Suganuma, Pongsakorn U-chupala, Yoshiki\n  Tanaka and Yuichi Kageyama", "title": "Massively Distributed SGD: ImageNet/ResNet-50 Training in a Flash", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling the distributed deep learning to a massive GPU cluster level is\nchallenging due to the instability of the large mini-batch training and the\noverhead of the gradient synchronization. We address the instability of the\nlarge mini-batch training with batch-size control and label smoothing. We\naddress the overhead of the gradient synchronization with 2D-Torus all-reduce.\nSpecifically, 2D-Torus all-reduce arranges GPUs in a logical 2D grid and\nperforms a series of collective operation in different orientations. These two\ntechniques are implemented with Neural Network Libraries (NNL). We have\nsuccessfully trained ImageNet/ResNet-50 in 122 seconds without significant\naccuracy loss on ABCI cluster.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 11:52:04 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 09:18:09 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Mikami", "Hiroaki", ""], ["Suganuma", "Hisahiro", ""], ["U-chupala", "Pongsakorn", ""], ["Tanaka", "Yoshiki", ""], ["Kageyama", "Yuichi", ""]]}, {"id": "1811.05242", "submitter": "Martino Mensio", "authors": "Martino Mensio, Emanuele Bastianelli, Ilaria Tiddi, Giuseppe Rizzo", "title": "A Multi-layer LSTM-based Approach for Robot Command Interaction Modeling", "comments": "Workshop on Language and Robotics, IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the first robotic platforms slowly approach our everyday life, we can\nimagine a near future where service robots will be easily accessible by\nnon-expert users through vocal interfaces. The capability of managing natural\nlanguage would indeed speed up the process of integrating such platform in the\nordinary life. Semantic parsing is a fundamental task of the Natural Language\nUnderstanding process, as it allows extracting the meaning of a user utterance\nto be used by a machine. In this paper, we present a preliminary study to\nsemantically parse user vocal commands for a House Service robot, using a\nmulti-layer Long-Short Term Memory neural network with attention mechanism. The\nsystem is trained on the Human Robot Interaction Corpus, and it is\npreliminarily compared with previous approaches.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 12:10:29 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Mensio", "Martino", ""], ["Bastianelli", "Emanuele", ""], ["Tiddi", "Ilaria", ""], ["Rizzo", "Giuseppe", ""]]}, {"id": "1811.05247", "submitter": "Pan Zhou", "authors": "Ruchao Fan, Pan Zhou, Wei Chen, Jia Jia, Gang Liu", "title": "An Online Attention-based Model for Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based end-to-end models such as Listen, Attend and Spell (LAS),\nsimplify the whole pipeline of traditional automatic speech recognition (ASR)\nsystems and become popular in the field of speech recognition. In previous\nwork, researchers have shown that such architectures can acquire comparable\nresults to state-of-the-art ASR systems, especially when using a bidirectional\nencoder and global soft attention (GSA) mechanism. However, bidirectional\nencoder and GSA are two obstacles for real-time speech recognition. In this\nwork, we aim to stream LAS baseline by removing the above two obstacles. On the\nencoder side, we use a latency-controlled (LC) bidirectional structure to\nreduce the delay of forward computation. Meanwhile, an adaptive monotonic\nchunk-wise attention (AMoChA) mechanism is proposed to replace GSA for the\ncalculation of attention weight distribution. Furthermore, we propose two\nmethods to alleviate the huge performance degradation when combining LC and\nAMoChA. Finally, we successfully acquire an online LAS model, LC-AMoChA, which\nhas only 3.5% relative performance reduction to LAS baseline on our internal\nMandarin corpus.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 12:23:37 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 09:13:17 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Fan", "Ruchao", ""], ["Zhou", "Pan", ""], ["Chen", "Wei", ""], ["Jia", "Jia", ""], ["Liu", "Gang", ""]]}, {"id": "1811.05249", "submitter": "Louis Kirsch", "authors": "Louis Kirsch, Julius Kunze, David Barber", "title": "Modular Networks: Learning to Decompose Neural Computation", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling model capacity has been vital in the success of deep learning. For a\ntypical network, necessary compute resources and training time grow\ndramatically with model size. Conditional computation is a promising way to\nincrease the number of parameters with a relatively small increase in\nresources. We propose a training algorithm that flexibly chooses neural modules\nbased on the data to be processed. Both the decomposition and modules are\nlearned end-to-end. In contrast to existing approaches, training does not rely\non regularization to enforce diversity in module use. We apply modular networks\nboth to image recognition and language modeling tasks, where we achieve\nsuperior performance compared to several baselines. Introspection reveals that\nmodules specialize in interpretable contexts.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 12:24:23 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Kirsch", "Louis", ""], ["Kunze", "Julius", ""], ["Barber", "David", ""]]}, {"id": "1811.05259", "submitter": "Manaar Alam", "authors": "Manaar Alam and Debdeep Mukhopadhyay", "title": "How Secure are Deep Learning Algorithms from Side-Channel based Reverse\n  Engineering?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning algorithms have recently become the de-facto paradigm for\nvarious prediction problems, which include many privacy-preserving applications\nlike online medical image analysis. Presumably, the privacy of data in a deep\nlearning system is a serious concern. There have been several efforts to\nanalyze and exploit the information leakages from deep learning architectures\nto compromise data privacy. In this paper, however, we attempt to provide an\nevaluation strategy for such information leakages through deep neural network\narchitectures by considering a case study on Convolutional Neural Network (CNN)\nbased image classifier. The approach takes the aid of low-level hardware\ninformation, provided by Hardware Performance Counters (HPCs), during the\nexecution of a CNN classifier and a simple hypothesis testing in order to\nproduce an alarm if there exists any information leakage on the actual input.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 12:42:24 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Alam", "Manaar", ""], ["Mukhopadhyay", "Debdeep", ""]]}, {"id": "1811.05266", "submitter": "Jean-Marc Andreoli", "authors": "Jean-Marc Andreoli", "title": "A conjugate prior for the Dirichlet distribution", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note investigates a conjugate class for the Dirichlet distribution class\nin the exponential family.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 13:02:55 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Andreoli", "Jean-Marc", ""]]}, {"id": "1811.05269", "submitter": "Andrea Borghesi", "authors": "Andrea Borghesi, Andrea Bartolini, Michele Lombardi, Michela Milano,\n  Luca Benini", "title": "Anomaly Detection using Autoencoders in High Performance Computing\n  Systems", "comments": "9 pages, 3 figures", "journal-ref": "Proceedings of the AAAI Conference on Artificial Intelligence,\n  Vol. 33, pages 9428-9433, 2019", "doi": "10.1609/aaai.v33i01.33019428", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in supercomputers is a very difficult problem due to the\nbig scale of the systems and the high number of components. The current state\nof the art for automated anomaly detection employs Machine Learning methods or\nstatistical regression models in a supervised fashion, meaning that the\ndetection tool is trained to distinguish among a fixed set of behaviour classes\n(healthy and unhealthy states).\n  We propose a novel approach for anomaly detection in High Performance\nComputing systems based on a Machine (Deep) Learning technique, namely a type\nof neural network called autoencoder. The key idea is to train a set of\nautoencoders to learn the normal (healthy) behaviour of the supercomputer nodes\nand, after training, use them to identify abnormal conditions. This is\ndifferent from previous approaches which where based on learning the abnormal\ncondition, for which there are much smaller datasets (since it is very hard to\nidentify them to begin with).\n  We test our approach on a real supercomputer equipped with a fine-grained,\nscalable monitoring infrastructure that can provide large amount of data to\ncharacterize the system behaviour. The results are extremely promising: after\nthe training phase to learn the normal system behaviour, our method is capable\nof detecting anomalies that have never been seen before with a very good\naccuracy (values ranging between 88% and 96%).\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 13:08:47 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Borghesi", "Andrea", ""], ["Bartolini", "Andrea", ""], ["Lombardi", "Michele", ""], ["Milano", "Michela", ""], ["Benini", "Luca", ""]]}, {"id": "1811.05296", "submitter": "Luca Massarelli", "authors": "Luca Massarelli, Giuseppe Antonio Di Luna, Fabio Petroni, Leonardo\n  Querzoni and Roberto Baldoni", "title": "SAFE: Self-Attentive Function Embeddings for Binary Similarity", "comments": "Published in International Conference on Detection of Intrusions and\n  Malware, and Vulnerability Assessment (DIMVA) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The binary similarity problem consists in determining if two functions are\nsimilar by only considering their compiled form. Advanced techniques for binary\nsimilarity recently gained momentum as they can be applied in several fields,\nsuch as copyright disputes, malware analysis, vulnerability detection, etc.,\nand thus have an immediate practical impact. Current solutions compare\nfunctions by first transforming their binary code in multi-dimensional vector\nrepresentations (embeddings), and then comparing vectors through simple and\nefficient geometric operations. However, embeddings are usually derived from\nbinary code using manual feature extraction, that may fail in considering\nimportant function characteristics, or may consider features that are not\nimportant for the binary similarity problem. In this paper we propose SAFE, a\nnovel architecture for the embedding of functions based on a self-attentive\nneural network. SAFE works directly on disassembled binary functions, does not\nrequire manual feature extraction, is computationally more efficient than\nexisting solutions (i.e., it does not incur in the computational overhead of\nbuilding or manipulating control flow graphs), and is more general as it works\non stripped binaries and on multiple architectures. We report the results from\na quantitative and qualitative analysis that show how SAFE provides a\nnoticeable performance improvement with respect to previous solutions.\nFurthermore, we show how clusters of our embedding vectors are closely related\nto the semantic of the implemented algorithms, paving the way for further\ninteresting applications (e.g. semantic-based binary function search).\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 14:01:16 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 20:59:31 GMT"}, {"version": "v3", "created": "Wed, 30 Jan 2019 17:26:46 GMT"}, {"version": "v4", "created": "Thu, 19 Dec 2019 10:06:09 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Massarelli", "Luca", ""], ["Di Luna", "Giuseppe Antonio", ""], ["Petroni", "Fabio", ""], ["Querzoni", "Leonardo", ""], ["Baldoni", "Roberto", ""]]}, {"id": "1811.05320", "submitter": "Haifeng Li", "authors": "Ling Zhao, Yujiao Song, Chao Zhang, Yu Liu, Pu Wang, Tao Lin, Min Deng\n  and Haifeng Li", "title": "T-GCN: A Temporal Graph ConvolutionalNetwork for Traffic Prediction", "comments": "10 pages, 14 figures", "journal-ref": "IEEE Transactions on Intelligent Transportation Systems-2019", "doi": "10.1109/TITS.2019.2935152", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and real-time traffic forecasting plays an important role in the\nIntelligent Traffic System and is of great significance for urban traffic\nplanning, traffic management, and traffic control. However, traffic forecasting\nhas always been considered an open scientific issue, owing to the constraints\nof urban road network topological structure and the law of dynamic change with\ntime, namely, spatial dependence and temporal dependence. To capture the\nspatial and temporal dependence simultaneously, we propose a novel neural\nnetwork-based traffic forecasting method, the temporal graph convolutional\nnetwork (T-GCN) model, which is in combination with the graph convolutional\nnetwork (GCN) and gated recurrent unit (GRU). Specifically, the GCN is used to\nlearn complex topological structures to capture spatial dependence and the\ngated recurrent unit is used to learn dynamic changes of traffic data to\ncapture temporal dependence. Then, the T-GCN model is employed to traffic\nforecasting based on the urban road network. Experiments demonstrate that our\nT-GCN model can obtain the spatio-temporal correlation from traffic data and\nthe predictions outperform state-of-art baselines on real-world traffic\ndatasets. Our tensorflow implementation of the T-GCN is available at\nhttps://github.com/lehaifeng/T-GCN.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 03:30:03 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 15:40:57 GMT"}, {"version": "v3", "created": "Mon, 31 Dec 2018 05:32:23 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Zhao", "Ling", ""], ["Song", "Yujiao", ""], ["Zhang", "Chao", ""], ["Liu", "Yu", ""], ["Wang", "Pu", ""], ["Lin", "Tao", ""], ["Deng", "Min", ""], ["Li", "Haifeng", ""]]}, {"id": "1811.05321", "submitter": "Alexander Gorban", "authors": "A.N. Gorban, A. Golubkov, B. Grechuk, E.M. Mirkes, I.Y. Tyukin", "title": "Correction of AI systems by linear discriminants: Probabilistic\n  foundations", "comments": "arXiv admin note: text overlap with arXiv:1809.07656 and\n  arXiv:1802.02172", "journal-ref": "Information Sciences 466 (2018), 303-322", "doi": "10.1016/j.ins.2018.07.040", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial Intelligence (AI) systems sometimes make errors and will make\nerrors in the future, from time to time. These errors are usually unexpected,\nand can lead to dramatic consequences. Intensive development of AI and its\npractical applications makes the problem of errors more important. Total\nre-engineering of the systems can create new errors and is not always possible\ndue to the resources involved. The important challenge is to develop fast\nmethods to correct errors without damaging existing skills. We formulated the\ntechnical requirements to the 'ideal' correctors. Such correctors include\nbinary classifiers, which separate the situations with high risk of errors from\nthe situations where the AI systems work properly. Surprisingly, for\nessentially high-dimensional data such methods are possible: simple linear\nFisher discriminant can separate the situations with errors from correctly\nsolved tasks even for exponentially large samples. The paper presents the\nprobabilistic basis for fast non-destructive correction of AI systems. A series\nof new stochastic separation theorems is proven. These theorems provide new\ninstruments for fast non-iterative correction of errors of legacy AI systems.\nThe new approaches become efficient in high-dimensions, for correction of\nhigh-dimensional systems in high-dimensional world (i.e. for processing of\nessentially high-dimensional data by large systems).\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 13:11:13 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Gorban", "A. N.", ""], ["Golubkov", "A.", ""], ["Grechuk", "B.", ""], ["Mirkes", "E. M.", ""], ["Tyukin", "I. Y.", ""]]}, {"id": "1811.05336", "submitter": "Xingwei Hu Dr", "authors": "Xingwei Hu", "title": "On Asymptotic Covariances of A Few Unrotated Factor Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide explicit formulas, in terms of the covariances of\nsample covariances or sample correlations, for the asymptotic covariances of\nunrotated factor loading estimates and unique variance estimates. These\nestimates are extracted from least square, principal, iterative principal\ncomponent, alpha or image factor analysis. If the sample is taken from a\nmultivariate normal population, these formulas, together with the delta\nmethods, will produce the standard errors for the rotated loading estimates. A\nsimulation study shows that the formulas provide reasonable results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 05:10:11 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Hu", "Xingwei", ""]]}, {"id": "1811.05355", "submitter": "Fabien Cardinaux", "authors": "Fabien Cardinaux and Stefan Uhlich and Kazuki Yoshiyama and Javier\n  Alonso Garc\\'ia and Stephen Tiedemann and Thomas Kemp and Akira Nakamura", "title": "Iteratively Training Look-Up Tables for Network Quantization", "comments": "NIPS 2018 workshop on Compact Deep Neural Networks with industrial\n  applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Operating deep neural networks on devices with limited resources requires the\nreduction of their memory footprints and computational requirements. In this\npaper we introduce a training method, called look-up table quantization, LUT-Q,\nwhich learns a dictionary and assigns each weight to one of the dictionary's\nvalues. We show that this method is very flexible and that many other\ntechniques can be seen as special cases of LUT-Q. For example, we can constrain\nthe dictionary trained with LUT-Q to generate networks with pruned weight\nmatrices or restrict the dictionary to powers-of-two to avoid the need for\nmultiplications. In order to obtain fully multiplier-less networks, we also\nintroduce a multiplier-less version of batch normalization. Extensive\nexperiments on image recognition and object detection tasks show that LUT-Q\nconsistently achieves better performance than other methods with the same\nquantization bitwidth.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 15:03:49 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Cardinaux", "Fabien", ""], ["Uhlich", "Stefan", ""], ["Yoshiyama", "Kazuki", ""], ["Garc\u00eda", "Javier Alonso", ""], ["Tiedemann", "Stephen", ""], ["Kemp", "Thomas", ""], ["Nakamura", "Akira", ""]]}, {"id": "1811.05370", "submitter": "Aditya Siddhant", "authors": "Aditya Siddhant, Anuj Goyal, Angeliki Metallinou", "title": "Unsupervised Transfer Learning for Spoken Language Understanding in\n  Intelligent Agents", "comments": "To appear at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User interaction with voice-powered agents generates large amounts of\nunlabeled utterances. In this paper, we explore techniques to efficiently\ntransfer the knowledge from these unlabeled utterances to improve model\nperformance on Spoken Language Understanding (SLU) tasks. We use Embeddings\nfrom Language Model (ELMo) to take advantage of unlabeled data by learning\ncontextualized word representations. Additionally, we propose ELMo-Light\n(ELMoL), a faster and simpler unsupervised pre-training method for SLU. Our\nfindings suggest unsupervised pre-training on a large corpora of unlabeled\nutterances leads to significantly better SLU performance compared to training\nfrom scratch and it can even outperform conventional supervised transfer.\nAdditionally, we show that the gains from unsupervised transfer techniques can\nbe further improved by supervised transfer. The improvements are more\npronounced in low resource settings and when using only 1000 labeled in-domain\nsamples, our techniques match the performance of training from scratch on\n10-15x more labeled in-domain data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 15:44:31 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Siddhant", "Aditya", ""], ["Goyal", "Anuj", ""], ["Metallinou", "Angeliki", ""]]}, {"id": "1811.05372", "submitter": "Abhishek Divekar", "authors": "Abhishek Divekar (1 and 5), Meet Parekh (2 and 5), Vaibhav Savla (3\n  and 5), Rudra Mishra (4 and 5), Mahesh Shirole (5) ((1) Amazon, (2) New York\n  University, (3) Infosys, (4) Samsung, (5) Veermata Jijabai Technological\n  Institute)", "title": "Benchmarking datasets for Anomaly-based Network Intrusion Detection: KDD\n  CUP 99 alternatives", "comments": "Paper accepted into Proceedings of IEEE International Conference on\n  Computing, Communication and Security 2018 (ICCCS-2018) Statistics: 8 pages,\n  7 tables, 3 figures, 34 references", "journal-ref": "2018 3rd IEEE International Conference on Computing, Communication\n  and Security (ICCCS)", "doi": "10.1109/CCCS.2018.8586840", "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning has been steadily gaining traction for its use in\nAnomaly-based Network Intrusion Detection Systems (A-NIDS). Research into this\ndomain is frequently performed using the KDD~CUP~99 dataset as a benchmark.\nSeveral studies question its usability while constructing a contemporary NIDS,\ndue to the skewed response distribution, non-stationarity, and failure to\nincorporate modern attacks. In this paper, we compare the performance for\nKDD-99 alternatives when trained using classification models commonly found in\nliterature: Neural Network, Support Vector Machine, Decision Tree, Random\nForest, Naive Bayes and K-Means. Applying the SMOTE oversampling technique and\nrandom undersampling, we create a balanced version of NSL-KDD and prove that\nskewed target classes in KDD-99 and NSL-KDD hamper the efficacy of classifiers\non minority classes (U2R and R2L), leading to possible security risks. We\nexplore UNSW-NB15, a modern substitute to KDD-99 with greater uniformity of\npattern distribution. We benchmark this dataset before and after SMOTE\noversampling to observe the effect on minority performance. Our results\nindicate that classifiers trained on UNSW-NB15 match or better the Weighted\nF1-Score of those trained on NSL-KDD and KDD-99 in the binary case, thus\nadvocating UNSW-NB15 as a modern substitute to these datasets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 15:49:54 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Divekar", "Abhishek", "", "1 and 5"], ["Parekh", "Meet", "", "2 and 5"], ["Savla", "Vaibhav", "", "3\n  and 5"], ["Mishra", "Rudra", "", "4 and 5"], ["Shirole", "Mahesh", ""]]}, {"id": "1811.05375", "submitter": "Carlos Sarraute", "authors": "Martin Fixman, Martin Minnoni, Carlos Sarraute", "title": "Comparison of Feature Extraction Methods and Predictors for Income\n  Inference", "comments": "Argentine Symposium on Big Data (AGRANDA), September 5, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Patterns of mobile phone communications, coupled with the information of the\nsocial network graph and financial behavior, allow us to make inferences of\nusers' socio-economic attributes such as their income level. We present here\nseveral methods to extract features from mobile phone usage (calls and\nmessages), and compare different combinations of supervised machine learning\ntechniques and sets of features used as input for the inference of users'\nincome. Our experimental results show that the Bayesian method based on the\ncommunication graph outperforms standard machine learning algorithms using\nnode-based features.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 15:53:22 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Fixman", "Martin", ""], ["Minnoni", "Martin", ""], ["Sarraute", "Carlos", ""]]}, {"id": "1811.05381", "submitter": "James Lucas", "authors": "Cem Anil, James Lucas, Roger Grosse", "title": "Sorting out Lipschitz function approximation", "comments": "8 main pages, 21 pages total, 17 figures. Accepted at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural networks under a strict Lipschitz constraint is useful for\nprovable adversarial robustness, generalization bounds, interpretable\ngradients, and Wasserstein distance estimation. By the composition property of\nLipschitz functions, it suffices to ensure that each individual affine\ntransformation or nonlinear activation is 1-Lipschitz. The challenge is to do\nthis while maintaining the expressive power. We identify a necessary property\nfor such an architecture: each of the layers must preserve the gradient norm\nduring backpropagation. Based on this, we propose to combine a gradient norm\npreserving activation function, GroupSort, with norm-constrained weight\nmatrices. We show that norm-constrained GroupSort architectures are universal\nLipschitz function approximators. Empirically, we show that norm-constrained\nGroupSort networks achieve tighter estimates of Wasserstein distance than their\nReLU counterparts and can achieve provable adversarial robustness guarantees\nwith little cost to accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 16:15:22 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 14:34:43 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Anil", "Cem", ""], ["Lucas", "James", ""], ["Grosse", "Roger", ""]]}, {"id": "1811.05443", "submitter": "Prasanna Sattigeri", "authors": "Abhishek Kumar and Prasanna Sattigeri and Kahini Wadhawan and Leonid\n  Karlinsky and Rogerio Feris and William T. Freeman and Gregory Wornell", "title": "Co-regularized Alignment for Unsupervised Domain Adaptation", "comments": "NIPS 2018 accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks, trained with large amount of labeled data, can fail to\ngeneralize well when tested with examples from a \\emph{target domain} whose\ndistribution differs from the training data distribution, referred as the\n\\emph{source domain}. It can be expensive or even infeasible to obtain required\namount of labeled data in all possible domains. Unsupervised domain adaptation\nsets out to address this problem, aiming to learn a good predictive model for\nthe target domain using labeled examples from the source domain but only\nunlabeled examples from the target domain. Domain alignment approaches this\nproblem by matching the source and target feature distributions, and has been\nused as a key component in many state-of-the-art domain adaptation methods.\nHowever, matching the marginal feature distributions does not guarantee that\nthe corresponding class conditional distributions will be aligned across the\ntwo domains. We propose co-regularized domain alignment for unsupervised domain\nadaptation, which constructs multiple diverse feature spaces and aligns source\nand target distributions in each of them individually, while encouraging that\nalignments agree with each other with regard to the class predictions on the\nunlabeled target examples. The proposed method is generic and can be used to\nimprove any domain adaptation method which uses domain alignment. We\ninstantiate it in the context of a recent state-of-the-art method and observe\nthat it provides significant performance improvements on several domain\nadaptation benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 18:22:15 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Kumar", "Abhishek", ""], ["Sattigeri", "Prasanna", ""], ["Wadhawan", "Kahini", ""], ["Karlinsky", "Leonid", ""], ["Feris", "Rogerio", ""], ["Freeman", "William T.", ""], ["Wornell", "Gregory", ""]]}, {"id": "1811.05467", "submitter": "Laura Martinus", "authors": "Jade Z. Abbott and Laura Martinus", "title": "Towards Neural Machine Translation for African Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given that South African education is in crisis, strategies for improvement\nand sustainability of high-quality, up-to-date education must be explored. In\nthe migration of education online, inclusion of machine translation for\nlow-resourced local languages becomes necessary. This paper aims to spur the\nuse of current neural machine translation (NMT) techniques for low-resourced\nlocal languages. The paper demonstrates state-of-the-art performance on\nEnglish-to-Setswana translation using the Autshumato dataset. The use of the\nTransformer architecture beat previous techniques by 5.33 BLEU points. This\ndemonstrates the promise of using current NMT techniques for African languages.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 06:49:08 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Abbott", "Jade Z.", ""], ["Martinus", "Laura", ""]]}, {"id": "1811.05468", "submitter": "Maximilian Hofer", "authors": "Maximilian Hofer, Andrey Kormilitzin, Paul Goldberg, Alejo\n  Nevado-Holgado", "title": "Few-shot Learning for Named Entity Recognition in Medical Text", "comments": "10 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network models have recently achieved state-of-the-art\nperformance gains in a variety of natural language processing (NLP) tasks\n(Young, Hazarika, Poria, & Cambria, 2017). However, these gains rely on the\navailability of large amounts of annotated examples, without which\nstate-of-the-art performance is rarely achievable. This is especially\ninconvenient for the many NLP fields where annotated examples are scarce, such\nas medical text. To improve NLP models in this situation, we evaluate five\nimprovements on named entity recognition (NER) tasks when only ten annotated\nexamples are available: (1) layer-wise initialization with pre-trained weights,\n(2) hyperparameter tuning, (3) combining pre-training data, (4) custom word\nembeddings, and (5) optimizing out-of-vocabulary (OOV) words. Experimental\nresults show that the F1 score of 69.3% achievable by state-of-the-art models\ncan be improved to 78.87%.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 13:12:02 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Hofer", "Maximilian", ""], ["Kormilitzin", "Andrey", ""], ["Goldberg", "Paul", ""], ["Nevado-Holgado", "Alejo", ""]]}, {"id": "1811.05475", "submitter": "Jingcheng Du", "authors": "Jingcheng Du, Qingyu Chen, Yifan Peng, Yang Xiang, Cui Tao, Zhiyong Lu", "title": "ML-Net: multi-label classification of biomedical texts with deep neural\n  networks", "comments": null, "journal-ref": null, "doi": "10.1093/jamia/ocz085", "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-label text classification, each textual document can be assigned\nwith one or more labels. Due to this nature, the multi-label text\nclassification task is often considered to be more challenging compared to the\nbinary or multi-class text classification problems. As an important task with\nbroad applications in biomedicine such as assigning diagnosis codes, a number\nof different computational methods (e.g. training and combining binary\nclassifiers for each label) have been proposed in recent years. However, many\nsuffered from modest accuracy and efficiency, with only limited success in\npractical use. We propose ML-Net, a novel deep learning framework, for\nmulti-label classification of biomedical texts. As an end-to-end system, ML-Net\ncombines a label prediction network with an automated label count prediction\nmechanism to output an optimal set of labels by leveraging both predicted\nconfidence score of each label and the contextual information in the target\ndocument. We evaluate ML-Net on three independent, publicly-available corpora\nin two kinds of text genres: biomedical literature and clinical notes. For\nevaluation, example-based measures such as precision, recall and f-measure are\nused. ML-Net is compared with several competitive machine learning baseline\nmodels. Our benchmarking results show that ML-Net compares favorably to the\nstate-of-the-art methods in multi-label classification of biomedical texts.\nML-NET is also shown to be robust when evaluated on different text genres in\nbiomedicine. Unlike traditional machine learning methods, ML-Net does not\nrequire human efforts in feature engineering and is highly efficient and\nscalable approach to tasks with a large set of labels (no need to build\nindividual classifiers for each separate label). Finally, ML-NET is able to\ndynamically estimate the label count based on the document context in a more\nsystematic and accurate manner.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 17:31:49 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 16:02:52 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Du", "Jingcheng", ""], ["Chen", "Qingyu", ""], ["Peng", "Yifan", ""], ["Xiang", "Yang", ""], ["Tao", "Cui", ""], ["Lu", "Zhiyong", ""]]}, {"id": "1811.05512", "submitter": "Paulina Grnarova", "authors": "Paulina Grnarova, Kfir Y Levy, Aurelien Lucchi, Nathanael Perraudin,\n  Ian Goodfellow, Thomas Hofmann and Andreas Krause", "title": "A domain agnostic measure for monitoring and evaluating GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have shown remarkable results in\nmodeling complex distributions, but their evaluation remains an unsettled\nissue. Evaluations are essential for: (i) relative assessment of different\nmodels and (ii) monitoring the progress of a single model throughout training.\nThe latter cannot be determined by simply inspecting the generator and\ndiscriminator loss curves as they behave non-intuitively. We leverage the\nnotion of duality gap from game theory to propose a measure that addresses both\n(i) and (ii) at a low computational cost. Extensive experiments show the\neffectiveness of this measure to rank different GAN models and capture the\ntypical GAN failure scenarios, including mode collapse and non-convergent\nbehaviours. This evaluation metric also provides meaningful monitoring on the\nprogression of the loss during training. It highly correlates with FID on\nnatural image datasets, and with domain specific scores for text, sound and\ncosmology data where FID is not directly suitable. In particular, our proposed\nmetric requires no labels or a pretrained classifier, making it domain\nagnostic.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 19:49:57 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 09:44:56 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Grnarova", "Paulina", ""], ["Levy", "Kfir Y", ""], ["Lucchi", "Aurelien", ""], ["Perraudin", "Nathanael", ""], ["Goodfellow", "Ian", ""], ["Hofmann", "Thomas", ""], ["Krause", "Andreas", ""]]}, {"id": "1811.05521", "submitter": "Mandar Kulkarni Mr.", "authors": "Mandar Kulkarni", "title": "Deep Q learning for fooling neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are vulnerable to external attacks. In this paper, we\npropose a Reinforcement Learning (RL) based approach to generate adversarial\nexamples for the pre-trained (target) models. We assume a semi black-box\nsetting where the only access an adversary has to the target model is the class\nprobabilities obtained for the input queries. We train a Deep Q Network (DQN)\nagent which, with experience, learns to attack only a small portion of image\npixels to generate non-targeted adversarial images. Initially, an agent\nexplores an environment by sequentially modifying random sets of image pixels\nand observes its effect on the class probabilities. At the end of an episode,\nit receives a positive (negative) reward if it succeeds (fails) to alter the\nlabel of the image. Experimental results with MNIST, CIFAR-10 and Imagenet\ndatasets demonstrate that our RL framework is able to learn an effective attack\npolicy.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 20:23:37 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Kulkarni", "Mandar", ""]]}, {"id": "1811.05527", "submitter": "Gabriel Peyr\\'e", "authors": "Marco Cuturi and Gabriel Peyr\\'e", "title": "Semi-dual Regularized Optimal Transport", "comments": null, "journal-ref": "SIAM Review, 60(4), 941-965, 2018", "doi": "10.1137/18M1208654", "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational problems that involve Wasserstein distances and more generally\noptimal transport (OT) theory are playing an increasingly important role in\ndata sciences. Such problems can be used to form an examplar measure out of\nvarious probability measures, as in the Wasserstein barycenter problem, or to\ncarry out parametric inference and density fitting, where the loss is measured\nin terms of an optimal transport cost to the measure of observations. Despite\nbeing conceptually simple, such problems are computationally challenging\nbecause they involve minimizing over quantities (Wasserstein distances) that\nare themselves hard to compute. Entropic regularization has recently emerged as\nan efficient tool to approximate the solution of such variational Wasserstein\nproblems. In this paper, we give a thorough duality tour of these\nregularization techniques. In particular, we show how important concepts from\nclassical OT such as c-transforms and semi-discrete approaches translate into\nsimilar ideas in a regularized setting. These dual formulations lead to smooth\nvariational problems, which can be solved using smooth, differentiable and\nconvex optimization problems that are simpler to implement and numerically more\nstable that their un-regularized counterparts. We illustrate the versatility of\nthis approach by applying it to the computation of Wasserstein barycenters and\ngradient flows of spatial regularization functionals.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 20:56:14 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Cuturi", "Marco", ""], ["Peyr\u00e9", "Gabriel", ""]]}, {"id": "1811.05537", "submitter": "Tong Qin", "authors": "Tong Qin, Kailiang Wu, Dongbin Xiu", "title": "Data Driven Governing Equations Approximation Using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2019.06.042", "report-no": null, "categories": "math.NA cs.LG cs.NE math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a numerical framework for approximating unknown governing\nequations using observation data and deep neural networks (DNN). In particular,\nwe propose to use residual network (ResNet) as the basic building block for\nequation approximation. We demonstrate that the ResNet block can be considered\nas a one-step method that is exact in temporal integration. We then present two\nmulti-step methods, recurrent ResNet (RT-ResNet) method and recursive ReNet\n(RS-ResNet) method. The RT-ResNet is a multi-step method on uniform time steps,\nwhereas the RS-ResNet is an adaptive multi-step method using variable time\nsteps. All three methods presented here are based on integral form of the\nunderlying dynamical system. As a result, they do not require time derivative\ndata for equation recovery and can cope with relatively coarsely distributed\ntrajectory data. Several numerical examples are presented to demonstrate the\nperformance of the methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 21:47:27 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Qin", "Tong", ""], ["Wu", "Kailiang", ""], ["Xiu", "Dongbin", ""]]}, {"id": "1811.05540", "submitter": "Ahmed Nazim Uddin", "authors": "Ahmed Nazim Uddin, Md Ashequr Rahman, Md. Rafidul Islam, Mohammad\n  Ariful Haque", "title": "Native Language Identification using i-vector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of determining a speaker's native language based only on his\nspeeches in a second language is known as Native Language Identification or\nNLI. Due to its increasing applications in various domains of speech signal\nprocessing, this has emerged as an important research area in recent times. In\nthis paper we have proposed an i-vector based approach to develop an automatic\nNLI system using MFCC and GFCC features. For evaluation of our approach, we\nhave tested our framework on the 2016 ComParE Native language sub-challenge\ndataset which has English language speakers from 11 different native language\nbackgrounds. Our proposed method outperforms the baseline system with an\nimprovement in accuracy by 21.95% for the MFCC feature based i-vector framework\nand 22.81% for the GFCC feature based i-vector framework.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 17:12:47 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Uddin", "Ahmed Nazim", ""], ["Rahman", "Md Ashequr", ""], ["Islam", "Md. Rafidul", ""], ["Haque", "Mohammad Ariful", ""]]}, {"id": "1811.05542", "submitter": "Aran Komatsuzaki", "authors": "Aran Komatsuzaki", "title": "Extractive Summary as Discrete Latent Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we compare various methods to compress a text using a neural\nmodel. We find that extracting tokens as latent variables significantly\noutperforms the state-of-the-art discrete latent variable models such as\nVQ-VAE. Furthermore, we compare various extractive compression schemes. There\nare two best-performing methods that perform equally. One method is to simply\nchoose the tokens with the highest tf-idf scores. Another is to train a\nbidirectional language model similar to ELMo and choose the tokens with the\nhighest loss. If we consider any subsequence of a text to be a text in a\nbroader sense, we conclude that language is a strong compression code of\nitself. Our finding justifies the high quality of generation achieved with\nhierarchical method, as their latent variables are nothing but natural language\nsummary. We also conclude that there is a hierarchy in language such that an\nentire text can be predicted much more easily based on a sequence of a small\nnumber of keywords, which can be easily found by classical methods as tf-idf.\nWe speculate that this extraction process may be useful for unsupervised\nhierarchical text generation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 02:02:18 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 22:56:42 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Komatsuzaki", "Aran", ""]]}, {"id": "1811.05544", "submitter": "Dichao Hu", "authors": "Dichao Hu", "title": "An Introductory Survey on Attention Mechanisms in NLP Problems", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First derived from human intuition, later adapted to machine translation for\nautomatic token alignment, attention mechanism, a simple method that can be\nused for encoding sequence data based on the importance score each element is\nassigned, has been widely applied to and attained significant improvement in\nvarious tasks in natural language processing, including sentiment\nclassification, text summarization, question answering, dependency parsing,\netc. In this paper, we survey through recent works and conduct an introductory\nsummary of the attention mechanism in different NLP problems, aiming to provide\nour readers with basic knowledge on this widely used method, discuss its\ndifferent variants for different tasks, explore its association with other\ntechniques in machine learning, and examine methods for evaluating its\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 16:19:22 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Hu", "Dichao", ""]]}, {"id": "1811.05550", "submitter": "Li-Chia Yang", "authors": "Lamtharn Hantrakul, Li-Chia Yang", "title": "Neural Wavetable: a playable wavetable synthesizer using neural networks", "comments": "2 pages, Accepted by Conference on Neural Information Processing\n  Systems (NIPS), Workshop on Machine Learning for Creativity and Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Neural Wavetable, a proof-of-concept wavetable synthesizer that\nuses neural networks to generate playable wavetables. The system can produce\nnew, distinct waveforms through the interpolation of traditional wavetables in\nan autoencoder's latent space. It is available as a VST/AU plugin for use in a\nDigital Audio Workstation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 22:27:17 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 19:54:20 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Hantrakul", "Lamtharn", ""], ["Yang", "Li-Chia", ""]]}, {"id": "1811.05561", "submitter": "Deovrat Kakde", "authors": "Deovrat Kakde, Arin Chaudhuri and Diana Shaw", "title": "A New SVDD-Based Multivariate Non-parametric Process Capability Index", "comments": null, "journal-ref": null, "doi": "10.1109/ICPHM.2018.8448517", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process capability index (PCI) is a commonly used statistic to measure\nability of a process to operate within the given specifications or to produce\nproducts which meet the required quality specifications. PCI can be univariate\nor multivariate depending upon the number of process specifications or quality\ncharacteristics of interest. Most PCIs make distributional assumptions which\nare often unrealistic in practice.\n  This paper proposes a new multivariate non-parametric process capability\nindex. This index can be used when distribution of the process or quality\nparameters is either unknown or does not follow commonly used distributions\nsuch as multivariate normal.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 23:05:38 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Kakde", "Deovrat", ""], ["Chaudhuri", "Arin", ""], ["Shaw", "Diana", ""]]}, {"id": "1811.05577", "submitter": "Pedro Saleiro", "authors": "Pedro Saleiro, Benedict Kuester, Loren Hinkson, Jesse London, Abby\n  Stevens, Ari Anisfeld, Kit T. Rodolfa, Rayid Ghani", "title": "Aequitas: A Bias and Fairness Audit Toolkit", "comments": "Aequitas website: http://dsapp.uchicago.edu/aequitas", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has raised concerns on the risk of unintended bias in AI systems\nbeing used nowadays that can affect individuals unfairly based on race, gender\nor religion, among other possible characteristics. While a lot of bias metrics\nand fairness definitions have been proposed in recent years, there is no\nconsensus on which metric/definition should be used and there are very few\navailable resources to operationalize them. Therefore, despite recent\nawareness, auditing for bias and fairness when developing and deploying AI\nsystems is not yet a standard practice. We present Aequitas, an open source\nbias and fairness audit toolkit that is an intuitive and easy to use addition\nto the machine learning workflow, enabling users to seamlessly test models for\nseveral bias and fairness metrics in relation to multiple population\nsub-groups. Aequitas facilitates informed and equitable decisions around\ndeveloping and deploying algorithmic decision making systems for both data\nscientists, machine learning researchers and policymakers.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 00:34:01 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 16:28:23 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Saleiro", "Pedro", ""], ["Kuester", "Benedict", ""], ["Hinkson", "Loren", ""], ["London", "Jesse", ""], ["Stevens", "Abby", ""], ["Anisfeld", "Ari", ""], ["Rodolfa", "Kit T.", ""], ["Ghani", "Rayid", ""]]}, {"id": "1811.05590", "submitter": "Vahid Behzadan", "authors": "Vahid Behzadan, Roman V. Yampolskiy and Arslan Munir", "title": "Emergence of Addictive Behaviors in Reinforcement Learning Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to the technical analysis of wireheading\nin intelligent agents. Inspired by the natural analogues of wireheading and\ntheir prevalent manifestations, we propose the modeling of such phenomenon in\nReinforcement Learning (RL) agents as psychological disorders. In a preliminary\nstep towards evaluating this proposal, we study the feasibility and dynamics of\nemergent addictive policies in Q-learning agents in the tractable environment\nof the game of Snake. We consider a slightly modified settings for this game,\nin which the environment provides a \"drug\" seed alongside the original\n\"healthy\" seed for the consumption of the snake. We adopt and extend an\nRL-based model of natural addiction to Q-learning agents in this settings, and\nderive sufficient parametric conditions for the emergence of addictive\nbehaviors in such agents. Furthermore, we evaluate our theoretical analysis\nwith three sets of simulation-based experiments. The results demonstrate the\nfeasibility of addictive wireheading in RL agents, and provide promising venues\nof further research on the psychopathological modeling of complex AI safety\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 01:30:00 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Behzadan", "Vahid", ""], ["Yampolskiy", "Roman V.", ""], ["Munir", "Arslan", ""]]}, {"id": "1811.05592", "submitter": "C. H. Huck Yang", "authors": "Rise Ooi, Chao-Han Huck Yang, Pin-Yu Chen, V\\`ictor Egu\\`iluz, Narsis\n  Kiani, Hector Zenil, David Gomez-Cabrero, Jesper Tegn\\`er", "title": "Controllability, Multiplexing, and Transfer Learning in Networks using\n  Evolutionary Learning", "comments": "A revised version. (word source code to pdf; owing to the algo\n  package conflicts)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG q-bio.MN", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Networks are fundamental building blocks for representing data, and\ncomputations. Remarkable progress in learning in structurally defined (shallow\nor deep) networks has recently been achieved. Here we introduce evolutionary\nexploratory search and learning method of topologically flexible networks under\nthe constraint of producing elementary computational steady-state input-output\noperations.\n  Our results include; (1) the identification of networks, over four orders of\nmagnitude, implementing computation of steady-state input-output functions,\nsuch as a band-pass filter, a threshold function, and an inverse band-pass\nfunction. Next, (2) the learned networks are technically controllable as only a\nsmall number of driver nodes are required to move the system to a new state.\nFurthermore, we find that the fraction of required driver nodes is constant\nduring evolutionary learning, suggesting a stable system design. (3), our\nframework allows multiplexing of different computations using the same network.\nFor example, using a binary representation of the inputs, the network can\nreadily compute three different input-output functions. Finally, (4) the\nproposed evolutionary learning demonstrates transfer learning. If the system\nlearns one function A, then learning B requires on average less number of steps\nas compared to learning B from tabula rasa.\n  We conclude that the constrained evolutionary learning produces large robust\ncontrollable circuits, capable of multiplexing and transfer learning. Our study\nsuggests that network-based computations of steady-state functions,\nrepresenting either cellular modules of cell-to-cell communication networks or\ninternal molecular circuits communicating within a cell, could be a powerful\nmodel for biologically inspired computing. This complements conceptualizations\nsuch as attractor based models, or reservoir computing.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 01:36:52 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 02:51:52 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Ooi", "Rise", ""], ["Yang", "Chao-Han Huck", ""], ["Chen", "Pin-Yu", ""], ["Egu\u00ecluz", "V\u00ecctor", ""], ["Kiani", "Narsis", ""], ["Zenil", "Hector", ""], ["Gomez-Cabrero", "David", ""], ["Tegn\u00e8r", "Jesper", ""]]}, {"id": "1811.05614", "submitter": "Ziyao Li", "authors": "Ziyao Li and Liang Zhang and Guojie Song", "title": "SepNE: Bringing Separability to Network Embedding", "comments": "8 pages, 4 figures, accepted in the Proceedings of the 33rd AAAI's\n  Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many successful methods have been proposed for learning low dimensional\nrepresentations on large-scale networks, while almost all existing methods are\ndesigned in inseparable processes, learning embeddings for entire networks even\nwhen only a small proportion of nodes are of interest. This leads to great\ninconvenience, especially on super-large or dynamic networks, where these\nmethods become almost impossible to implement. In this paper, we formalize the\nproblem of separated matrix factorization, based on which we elaborate a novel\nobjective function that preserves both local and global information. We further\npropose SepNE, a simple and flexible network embedding algorithm which\nindependently learns representations for different subsets of nodes in\nseparated processes. By implementing separability, our algorithm reduces the\nredundant efforts to embed irrelevant nodes, yielding scalability to\nsuper-large networks, automatic implementation in distributed learning and\nfurther adaptations. We demonstrate the effectiveness of this approach on\nseveral real-world networks with different scales and subjects. With comparable\naccuracy, our approach significantly outperforms state-of-the-art baselines in\nrunning times on large networks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 02:47:28 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 09:53:03 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Li", "Ziyao", ""], ["Zhang", "Liang", ""], ["Song", "Guojie", ""]]}, {"id": "1811.05642", "submitter": "Xiao Li", "authors": "Zhihui Zhu, Xiao Li, Kai Liu, and Qiuwei Li", "title": "Dropping Symmetry for Fast Symmetric Nonnegative Matrix Factorization", "comments": "Accepted in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric nonnegative matrix factorization (NMF), a special but important\nclass of the general NMF, is demonstrated to be useful for data analysis and in\nparticular for various clustering tasks. Unfortunately, designing fast\nalgorithms for Symmetric NMF is not as easy as for the nonsymmetric\ncounterpart, the latter admitting the splitting property that allows efficient\nalternating-type algorithms. To overcome this issue, we transfer the symmetric\nNMF to a nonsymmetric one, then we can adopt the idea from the state-of-the-art\nalgorithms for nonsymmetric NMF to design fast algorithms solving symmetric\nNMF. We rigorously establish that solving nonsymmetric reformulation returns a\nsolution for symmetric NMF and then apply fast alternating based algorithms for\nthe corresponding reformulated problem. Furthermore, we show these fast\nalgorithms admit strong convergence guarantee in the sense that the generated\nsequence is convergent at least at a sublinear rate and it converges globally\nto a critical point of the symmetric NMF. We conduct experiments on both\nsynthetic data and image clustering to support our result.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 05:09:33 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Zhu", "Zhihui", ""], ["Li", "Xiao", ""], ["Liu", "Kai", ""], ["Li", "Qiuwei", ""]]}, {"id": "1811.05646", "submitter": "Yizheng Liao", "authors": "Yizheng Liao, Yang Weng, Chin-Woo Tan, Ram Rajagopal", "title": "Fast Distribution Grid Line Outage Identification with $\\mu$PMU", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing integration of distributed energy resources (DERs) in urban\ndistribution grids raises various reliability issues due to DER's uncertain and\ncomplex behaviors. With a large-scale DER penetration, traditional outage\ndetection methods, which rely on customers making phone calls and smart meters'\n\"last gasp\" signals, will have limited performance, because the renewable\ngenerators can supply powers after line outages and many urban grids are mesh\nso line outages do not affect power supply. To address these drawbacks, we\npropose a data-driven outage monitoring approach based on the stochastic time\nseries analysis from micro phasor measurement unit ($\\mu$PMU). Specifically, we\nprove via power flow analysis that the dependency of time-series voltage\nmeasurements exhibits significant statistical changes after line outages. This\nmakes the theory on optimal change-point detection suitable to identify line\noutages via $\\mu$PMUs with fast and accurate sampling. However, existing change\npoint detection methods require post-outage voltage distribution unknown in\ndistribution systems. Therefore, we design a maximum likelihood-based method to\ndirectly learn the distribution parameters from $\\mu$PMU data. We prove that\nthe estimated parameters-based detection still achieves the optimal\nperformance, making it extremely useful for distribution grid outage\nidentifications. Simulation results show highly accurate outage identification\nin eight distribution grids with 14 configurations with and without DERs using\n$\\mu$PMU data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 05:19:31 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Liao", "Yizheng", ""], ["Weng", "Yang", ""], ["Tan", "Chin-Woo", ""], ["Rajagopal", "Ram", ""]]}, {"id": "1811.05654", "submitter": "Sandeep Juneja", "authors": "Sandeep Juneja and Subhashini Krishnasamy", "title": "Sample complexity of partition identification using multi-armed bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a vector of probability distributions, or arms, each of which can be\nsampled independently, we consider the problem of identifying the partition to\nwhich this vector belongs from a finitely partitioned universe of such vector\nof distributions. We study this as a pure exploration problem in multi armed\nbandit settings and develop sample complexity bounds on the total mean number\nof samples required for identifying the correct partition with high\nprobability. This framework subsumes well studied problems such as finding the\nbest arm or the best few arms. We consider distributions belonging to the\nsingle parameter exponential family and primarily consider partitions where the\nvector of means of arms lie either in a given set or its complement. The sets\nconsidered correspond to distributions where there exists a mean above a\nspecified threshold, where the set is a half space and where either the set or\nits complement is a polytope, or more generally, a convex set. In these\nsettings, we characterize the lower bounds on mean number of samples for each\narm highlighting their dependence on the problem geometry. Further, inspired by\nthe lower bounds, we propose algorithms that can match these bounds\nasymptotically with decreasing probability of error. Applications of this\nframework may be diverse. We briefly discuss one associated with finance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 05:41:08 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 12:09:25 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Juneja", "Sandeep", ""], ["Krishnasamy", "Subhashini", ""]]}, {"id": "1811.05660", "submitter": "Soumya Sanyal", "authors": "Soumya Sanyal, Janakiraman Balachandran, Naganand Yadati, Abhishek\n  Kumar, Padmini Rajagopalan, Suchismita Sanyal, Partha Talukdar", "title": "MT-CGCNN: Integrating Crystal Graph Convolutional Neural Network with\n  Multitask Learning for Material Property Prediction", "comments": "NIPS Workshop on Machine Learning for Molecules and Materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.mtrl-sci stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing accurate, transferable and computationally inexpensive machine\nlearning models can rapidly accelerate the discovery and development of new\nmaterials. Some of the major challenges involved in developing such models are,\n(i) limited availability of materials data as compared to other fields, (ii)\nlack of universal descriptor of materials to predict its various properties.\nThe limited availability of materials data can be addressed through transfer\nlearning, while the generic representation was recently addressed by Xie and\nGrossman [1], where they developed a crystal graph convolutional neural network\n(CGCNN) that provides a unified representation of crystals. In this work, we\ndevelop a new model (MT-CGCNN) by integrating CGCNN with transfer learning\nbased on multi-task (MT) learning. We demonstrate the effectiveness of MT-CGCNN\nby simultaneous prediction of various material properties such as Formation\nEnergy, Band Gap and Fermi Energy for a wide range of inorganic crystals (46774\nmaterials). MT-CGCNN is able to reduce the test error when employed on\ncorrelated properties by upto 8%. The model prediction has lower test error\ncompared to CGCNN, even when the training data is reduced by 10%. We also\ndemonstrate our model's better performance through prediction of end user\nscenario related to metal/non-metal classification. These results encourage\nfurther development of machine learning approaches which leverage multi-task\nlearning to address the aforementioned challenges in the discovery of new\nmaterials. We make MT-CGCNN's source code available to encourage reproducible\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 06:13:29 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Sanyal", "Soumya", ""], ["Balachandran", "Janakiraman", ""], ["Yadati", "Naganand", ""], ["Kumar", "Abhishek", ""], ["Rajagopalan", "Padmini", ""], ["Sanyal", "Suchismita", ""], ["Talukdar", "Partha", ""]]}, {"id": "1811.05688", "submitter": "Yixing Guan", "authors": "Yixing Guan, Jinyu Zhao, Yiqin Qiu, Zheng Zhang, Gus Xia", "title": "Melodic Phrase Segmentation By Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated melodic phrase detection and segmentation is a classical task in\ncontent-based music information retrieval and also the key towards automated\nmusic structure analysis. However, traditional methods still cannot satisfy\npractical requirements. In this paper, we explore and adapt various neural\nnetwork architectures to see if they can be generalized to work with the\nsymbolic representation of music and produce satisfactory melodic phrase\nsegmentation. The main issue of applying deep-learning methods to phrase\ndetection is the sparse labeling problem of training sets. We proposed two\ntailored label engineering with corresponding training techniques for different\nneural networks in order to make decisions at a sequential level. Experiment\nresults show that the CNN-CRF architecture performs the best, being able to\noffer finer segmentation and faster to train, while CNN, Bi-LSTM-CNN and\nBi-LSTM-CRF are acceptable alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 08:42:06 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Guan", "Yixing", ""], ["Zhao", "Jinyu", ""], ["Qiu", "Yiqin", ""], ["Zhang", "Zheng", ""], ["Xia", "Gus", ""]]}, {"id": "1811.05695", "submitter": "Xiao He", "authors": "Xiao He, Francesco Alesiani and Ammar Shaker", "title": "Efficient and Scalable Multi-task Regression on Massive Number of Tasks", "comments": "Accepted at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world large-scale regression problems can be formulated as\nMulti-task Learning (MTL) problems with a massive number of tasks, as in retail\nand transportation domains. However, existing MTL methods still fail to offer\nboth the generalization performance and the scalability for such problems.\nScaling up MTL methods to problems with a tremendous number of tasks is a big\nchallenge. Here, we propose a novel algorithm, named Convex Clustering\nMulti-Task regression Learning (CCMTL), which integrates with convex clustering\non the k-nearest neighbor graph of the prediction models. Further, CCMTL\nefficiently solves the underlying convex problem with a newly proposed\noptimization method. CCMTL is accurate, efficient to train, and empirically\nscales linearly in the number of tasks. On both synthetic and real-world\ndatasets, the proposed CCMTL outperforms seven state-of-the-art (SoA)\nmulti-task learning methods in terms of prediction accuracy as well as\ncomputational efficiency. On a real-world retail dataset with 23,812 tasks,\nCCMTL requires only around 30 seconds to train on a single thread, while the\nSoA methods need up to hours or even days.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 09:19:39 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["He", "Xiao", ""], ["Alesiani", "Francesco", ""], ["Shaker", "Ammar", ""]]}, {"id": "1811.05699", "submitter": "Ezequiel De La Rosa", "authors": "Ezequiel de la Rosa, Diana M. Sima, Thijs Vande Vyvere, Jan S.\n  Kirschke, Bjoern Menze", "title": "A Radiomics Approach to Traumatic Brain Injury Prediction in CT Scans", "comments": "Submitted to ISBI 2019", "journal-ref": "2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI\n  2019) (pp. 732-735). IEEE", "doi": "10.1109/ISBI.2019.8759229", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer Tomography (CT) is the gold standard technique for brain damage\nevaluation after acute Traumatic Brain Injury (TBI). It allows identification\nof most lesion types and determines the need of surgical or alternative\ntherapeutic procedures. However, the traditional approach for lesion\nclassification is restricted to visual image inspection. In this work, we\ncharacterize and predict TBI lesions by using CT-derived radiomics descriptors.\nRelevant shape, intensity and texture biomarkers characterizing the different\nlesions are isolated and a lesion predictive model is built by using Partial\nLeast Squares. On a dataset containing 155 scans (105 train, 50 test) the\nmethodology achieved 89.7 % accuracy over the unseen data. When a model was\nbuild using only texture features, a 88.2 % accuracy was obtained. Our results\nsuggest that selected radiomics descriptors could play a key role in brain\ninjury prediction. Besides, the proposed methodology is close to reproduce\nradiologists decision making. These results open new possibilities for\nradiomics-inspired brain lesion detection, segmentation and prediction.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 09:29:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["de la Rosa", "Ezequiel", ""], ["Sima", "Diana M.", ""], ["Vyvere", "Thijs Vande", ""], ["Kirschke", "Jan S.", ""], ["Menze", "Bjoern", ""]]}, {"id": "1811.05711", "submitter": "Muhammed Tarik Altuncu", "authors": "M. Tarik Altuncu, Erik Mayer, Sophia N. Yaliraki, Mauricio Barahona", "title": "From Free Text to Clusters of Content in Health Records: An Unsupervised\n  Graph Partitioning Approach", "comments": "25 pages, 2 tables, 8 figures and 5 supplementary figures", "journal-ref": null, "doi": "10.1007/s41109-018-0109-9", "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic Healthcare records contain large volumes of unstructured data in\ndifferent forms. Free text constitutes a large portion of such data, yet this\nsource of richly detailed information often remains under-used in practice\nbecause of a lack of suitable methodologies to extract interpretable content in\na timely manner. Here we apply network-theoretical tools to the analysis of\nfree text in Hospital Patient Incident reports in the English National Health\nService, to find clusters of reports in an unsupervised manner and at different\nlevels of resolution based directly on the free text descriptions contained\nwithin them. To do so, we combine recently developed deep neural network\ntext-embedding methodologies based on paragraph vectors with multi-scale Markov\nStability community detection applied to a similarity graph of documents\nobtained from sparsified text vector similarities. We showcase the approach\nwith the analysis of incident reports submitted in Imperial College Healthcare\nNHS Trust, London. The multiscale community structure reveals levels of meaning\nwith different resolution in the topics of the dataset, as shown by relevant\ndescriptive terms extracted from the groups of records, as well as by comparing\na posteriori against hand-coded categories assigned by healthcare personnel.\nOur content communities exhibit good correspondence with well-defined\nhand-coded categories, yet our results also provide further medical detail in\ncertain areas as well as revealing complementary descriptors of incidents\nbeyond the external classification. We also discuss how the method can be used\nto monitor reports over time and across different healthcare providers, and to\ndetect emerging trends that fall outside of pre-existing categories.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 10:08:19 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Altuncu", "M. Tarik", ""], ["Mayer", "Erik", ""], ["Yaliraki", "Sophia N.", ""], ["Barahona", "Mauricio", ""]]}, {"id": "1811.05768", "submitter": "Shaun D'Souza", "authors": "Shaun D'Souza", "title": "Parser Extraction of Triples in Unstructured Text", "comments": null, "journal-ref": "IAES International Journal of Artificial Intelligence (IJ-AI),\n  5(4):143-148, 2017", "doi": "10.11591/ij-ai.v5.i4.pp143-148", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The web contains vast repositories of unstructured text. We investigate the\nopportunity for building a knowledge graph from these text sources. We generate\na set of triples which can be used in knowledge gathering and integration. We\ndefine the architecture of a language compiler for processing\nsubject-predicate-object triples using the OpenNLP parser. We implement a\ndepth-first search traversal on the POS tagged syntactic tree appending\npredicate and object information. A parser enables higher precision and higher\nrecall extractions of syntactic relationships across conjunction boundaries. We\nare able to extract 2-2.5 times the correct extractions of ReVerb. The\nextractions are used in a variety of semantic web applications and question\nanswering. We verify extraction of 50,000 triples on the ClueWeb dataset.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 16:12:00 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["D'Souza", "Shaun", ""]]}, {"id": "1811.05785", "submitter": "Nelson Fernandez Pinto", "authors": "Nelson Fernandez", "title": "Two-stream convolutional networks for end-to-end learning of\n  self-driving cars", "comments": null, "journal-ref": "NeurIPS 2018 Workshop on modeling and decision-making in the\n  spatiotemporal domain, Montreal, Canada", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a methodology to extend the concept of Two-Stream Convolutional\nNetworks to perform end-to-end learning for self-driving cars with temporal\ncues. The system has the ability to learn spatiotemporal features by\nsimultaneously mapping raw images and pre-calculated optical flows directly to\nsteering commands. Although optical flows encode temporal-rich information, we\nfound that 2D-CNNs are prone to capturing features only as spatial\nrepresentations. We show how the use of Multitask Learning favors the learning\nof temporal features via inductive transfer from a shared spatiotemporal\nrepresentation. Preliminary results demonstrate a competitive improvement of\n30% in prediction accuracy and stability compared to widely used regression\nmethods trained on the Comma.ai dataset.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 12:34:42 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 15:16:02 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Fernandez", "Nelson", ""]]}, {"id": "1811.05788", "submitter": "Robin Spiess", "authors": "Robin Spiess, Felix Berkenkamp, Jan Poland, Andreas Krause", "title": "Learning to Compensate Photovoltaic Power Fluctuations from Images of\n  the Sky by Imitating an Optimal Policy", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The energy output of photovoltaic (PV) power plants depends on the\nenvironment and thus fluctuates over time. As a result, PV power can cause\ninstability in the power grid, in particular when increasingly used. Limiting\nthe rate of change of the power output is a common way to mitigate these\nfluctuations, often with the help of large batteries. A reactive controller\nthat uses these batteries to compensate ramps works in practice, but causes\nstress on the battery due to a high energy throughput. In this paper, we\npresent a deep learning approach that uses images of the sky to compensate\npower fluctuations predictively and reduces battery stress. In particular, we\nshow that the optimal control policy can be computed using information that is\nonly available in hindsight. Based on this, we use imitation learning to train\na neural network that approximates this hindsight-optimal policy, but uses only\ncurrently available sky images and sensor data. We evaluate our method on a\nlarge dataset of measurements and images from a real power plant and show that\nthe trained policy reduces stress on the battery.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 09:39:53 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Spiess", "Robin", ""], ["Berkenkamp", "Felix", ""], ["Poland", "Jan", ""], ["Krause", "Andreas", ""]]}, {"id": "1811.05826", "submitter": "Shubham Agarwal", "authors": "Shubham Agarwal, Marc Dymetman and Eric Gaussier", "title": "Char2char Generation with Reranking for the E2E NLG Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our submission to the E2E NLG Challenge. Recently,\nneural seq2seq approaches have become mainstream in NLG, often resorting to\npre- (respectively post-) processing delexicalization (relexicalization) steps\nat the word-level to handle rare words. By contrast, we train a simple\ncharacter level seq2seq model, which requires no pre/post-processing\n(delexicalization, tokenization or even lowercasing), with surprisingly good\nresults. For further improvement, we explore two re-ranking approaches for\nscoring candidates. We also introduce a synthetic dataset creation procedure,\nwhich opens up a new way of creating artificial datasets for Natural Language\nGeneration.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 22:56:50 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Agarwal", "Shubham", ""], ["Dymetman", "Marc", ""], ["Gaussier", "Eric", ""]]}, {"id": "1811.05831", "submitter": "Jarrid Rector-Brooks", "authors": "Jarrid Rector-Brooks, Jun-Kun Wang, Barzan Mozafari", "title": "Revisiting Projection-Free Optimization for Strongly Convex Constraint\n  Sets", "comments": "Extended version of paper accepted at AAAI-19, 19 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the Frank-Wolfe (FW) optimization under strongly convex constraint\nsets. We provide a faster convergence rate for FW without line search, showing\nthat a previously overlooked variant of FW is indeed faster than the standard\nvariant. With line search, we show that FW can converge to the global optimum,\neven for smooth functions that are not convex, but are quasi-convex and\nlocally-Lipschitz. We also show that, for the general case of (smooth)\nnon-convex functions, FW with line search converges with high probability to a\nstationary point at a rate of $O\\left(\\frac{1}{t}\\right)$, as long as the\nconstraint set is strongly convex -- one of the fastest convergence rates in\nnon-convex optimization.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 15:09:39 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 01:59:07 GMT"}, {"version": "v3", "created": "Thu, 31 Jan 2019 09:24:15 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Rector-Brooks", "Jarrid", ""], ["Wang", "Jun-Kun", ""], ["Mozafari", "Barzan", ""]]}, {"id": "1811.05844", "submitter": "Gautier Izacard", "authors": "Gautier Izacard, Brett Bernstein and Carlos Fernandez-Granda", "title": "A Learning-Based Framework for Line-Spectra Super-resolution", "comments": "Accepted at ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning-based approach for estimating the spectrum of a\nmultisinusoidal signal from a finite number of samples. A neural-network is\ntrained to approximate the spectra of such signals on simulated data. The\nproposed methodology is very flexible: adapting to different signal and noise\nmodels only requires modifying the training data accordingly. Numerical\nexperiments show that the approach performs competitively with classical\nmethods designed for additive Gaussian noise at a range of noise levels, and is\nalso effective in the presence of impulsive noise.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 15:20:29 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 20:33:33 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Izacard", "Gautier", ""], ["Bernstein", "Brett", ""], ["Fernandez-Granda", "Carlos", ""]]}, {"id": "1811.05850", "submitter": "Senwei Liang", "authors": "Senwei Liang, Yuehaw Khoo, Haizhao Yang", "title": "Drop-Activation: Implicit Parameter Reduction and Harmonic\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overfitting frequently occurs in deep learning. In this paper, we propose a\nnovel regularization method called Drop-Activation to reduce overfitting and\nimprove generalization. The key idea is to drop nonlinear activation functions\nby setting them to be identity functions randomly during training time. During\ntesting, we use a deterministic network with a new activation function to\nencode the average effect of dropping activations randomly. Our theoretical\nanalyses support the regularization effect of Drop-Activation as implicit\nparameter reduction and verify its capability to be used together with Batch\nNormalization (Ioffe and Szegedy 2015). The experimental results on CIFAR-10,\nCIFAR-100, SVHN, EMNIST, and ImageNet show that Drop-Activation generally\nimproves the performance of popular neural network architectures for the image\nclassification task. Furthermore, as a regularizer Drop-Activation can be used\nin harmony with standard training and regularization techniques such as Batch\nNormalization and Auto Augment (Cubuk et al. 2019). The code is available at\n\\url{https://github.com/LeungSamWai/Drop-Activation}.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 15:27:56 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 04:00:30 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 01:10:35 GMT"}, {"version": "v4", "created": "Mon, 3 Jun 2019 11:05:48 GMT"}, {"version": "v5", "created": "Sat, 28 Mar 2020 19:08:30 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Liang", "Senwei", ""], ["Khoo", "Yuehaw", ""], ["Yang", "Haizhao", ""]]}, {"id": "1811.05852", "submitter": "Kelli Humbird", "authors": "K. D. Humbird, J. L. Peterson, R. G. McClarren", "title": "Predicting the time-evolution of multi-physics systems with\n  sequence-to-sequence models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, sequence-to-sequence (seq2seq) models, originally developed for\nlanguage translation, are used to predict the temporal evolution of complex,\nmulti-physics computer simulations. The predictive performance of seq2seq\nmodels is compared to state transition models for datasets generated with\nmulti-physics codes with varying levels of complexity - from simple 1D\ndiffusion calculations to simulations of inertial confinement fusion\nimplosions. Seq2seq models demonstrate the ability to accurately emulate\ncomplex systems, enabling the rapid estimation of the evolution of quantities\nof interest in computationally expensive simulations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 15:36:46 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Humbird", "K. D.", ""], ["Peterson", "J. L.", ""], ["McClarren", "R. G.", ""]]}, {"id": "1811.05868", "submitter": "Oleksandr Shchur", "authors": "Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, Stephan\n  G\\\"unnemann", "title": "Pitfalls of Graph Neural Network Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised node classification in graphs is a fundamental problem in\ngraph mining, and the recently proposed graph neural networks (GNNs) have\nachieved unparalleled results on this task. Due to their massive success, GNNs\nhave attracted a lot of attention, and many novel architectures have been put\nforward. In this paper we show that existing evaluation strategies for GNN\nmodels have serious shortcomings. We show that using the same\ntrain/validation/test splits of the same datasets, as well as making\nsignificant changes to the training procedure (e.g. early stopping criteria)\nprecludes a fair comparison of different architectures. We perform a thorough\nempirical evaluation of four prominent GNN models and show that considering\ndifferent splits of the data leads to dramatically different rankings of\nmodels. Even more importantly, our findings suggest that simpler GNN\narchitectures are able to outperform the more sophisticated ones if the\nhyperparameters and the training procedure are tuned fairly for all models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 15:53:19 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 13:15:39 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Shchur", "Oleksandr", ""], ["Mumme", "Maximilian", ""], ["Bojchevski", "Aleksandar", ""], ["G\u00fcnnemann", "Stephan", ""]]}, {"id": "1811.05869", "submitter": "Haokun Chen", "authors": "Haokun Chen, Xinyi Dai, Han Cai, Weinan Zhang, Xuejian Wang, Ruiming\n  Tang, Yuzhou Zhang, Yong Yu", "title": "Large-scale Interactive Recommendation with Tree-structured Policy\n  Gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) has recently been introduced to interactive\nrecommender systems (IRS) because of its nature of learning from dynamic\ninteractions and planning for long-run performance. As IRS is always with\nthousands of items to recommend (i.e., thousands of actions), most existing\nRL-based methods, however, fail to handle such a large discrete action space\nproblem and thus become inefficient. The existing work that tries to deal with\nthe large discrete action space problem by utilizing the deep deterministic\npolicy gradient framework suffers from the inconsistency between the continuous\naction representation (the output of the actor network) and the real discrete\naction. To avoid such inconsistency and achieve high efficiency and\nrecommendation effectiveness, in this paper, we propose a Tree-structured\nPolicy Gradient Recommendation (TPGR) framework, where a balanced hierarchical\nclustering tree is built over the items and picking an item is formulated as\nseeking a path from the root to a certain leaf of the tree. Extensive\nexperiments on carefully-designed environments based on two real-world datasets\ndemonstrate that our model provides superior recommendation performance and\nsignificant efficiency improvement over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 15:53:25 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Chen", "Haokun", ""], ["Dai", "Xinyi", ""], ["Cai", "Han", ""], ["Zhang", "Weinan", ""], ["Wang", "Xuejian", ""], ["Tang", "Ruiming", ""], ["Zhang", "Yuzhou", ""], ["Yu", "Yong", ""]]}, {"id": "1811.05896", "submitter": "Miguel De Prado", "authors": "Miguel de Prado, Maurizio Denna, Luca Benini and Nuria Pazos", "title": "QUENN: QUantization Engine for low-power Neural Networks", "comments": "Computing Frontiers 2018", "journal-ref": null, "doi": "10.1145/3203217.3203282", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning is moving to edge devices, ushering in a new age of distributed\nArtificial Intelligence (AI). The high demand of computational resources\nrequired by deep neural networks may be alleviated by approximate computing\ntechniques, and most notably reduced-precision arithmetic with coarsely\nquantized numerical representations. In this context, Bonseyes comes in as an\ninitiative to enable stakeholders to bring AI to low-power and autonomous\nenvironments such as: Automotive, Medical Healthcare and Consumer Electronics.\nTo achieve this, we introduce LPDNN, a framework for optimized deployment of\nDeep Neural Networks on heterogeneous embedded devices. In this work, we detail\nthe quantization engine that is integrated in LPDNN. The engine depends on a\nfine-grained workflow which enables a Neural Network Design Exploration and a\nsensitivity analysis of each layer for quantization. We demonstrate the engine\nwith a case study on Alexnet and VGG16 for three different techniques for\ndirect quantization: standard fixed-point, dynamic fixed-point and k-means\nclustering, and demonstrate the potential of the latter. We argue that using a\nGaussian quantizer with k-means clustering can achieve better performance than\nlinear quantizers. Without retraining, we achieve over 55.64\\% saving for\nweights' storage and 69.17\\% for run-time memory accesses with less than 1\\%\ndrop in top5 accuracy in Imagenet.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 16:38:42 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["de Prado", "Miguel", ""], ["Denna", "Maurizio", ""], ["Benini", "Luca", ""], ["Pazos", "Nuria", ""]]}, {"id": "1811.05910", "submitter": "Jonas Adler", "authors": "Jonas Adler, Ozan \\\"Oktem", "title": "Deep Bayesian Inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing statistical properties of solutions of inverse problems is\nessential for decision making. Bayesian inversion offers a tractable framework\nfor this purpose, but current approaches are computationally unfeasible for\nmost realistic imaging applications in the clinic. We introduce two novel deep\nlearning based methods for solving large-scale inverse problems using Bayesian\ninversion: a sampling based method using a WGAN with a novel mini-discriminator\nand a direct approach that trains a neural network using a novel loss function.\nThe performance of both methods is demonstrated on image reconstruction in\nultra low dose 3D helical CT. We compute the posterior mean and standard\ndeviation of the 3D images followed by a hypothesis test to assess whether a\n\"dark spot\" in the liver of a cancer stricken patient is present. Both methods\nare computationally efficient and our evaluation shows very promising\nperformance that clearly supports the claim that Bayesian inversion is usable\nfor 3D imaging in time critical applications.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 17:06:56 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Adler", "Jonas", ""], ["\u00d6ktem", "Ozan", ""]]}, {"id": "1811.05922", "submitter": "Assaf Eisenman", "authors": "Assaf Eisenman, Maxim Naumov, Darryl Gardner, Misha Smelyanskiy,\n  Sergey Pupyrev, Kim Hazelwood, Asaf Cidon, Sachin Katti", "title": "Bandana: Using Non-volatile Memory for Storing Deep Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical large-scale recommender systems use deep learning models that are\nstored on a large amount of DRAM. These models often rely on embeddings, which\nconsume most of the required memory. We present Bandana, a storage system that\nreduces the DRAM footprint of embeddings, by using Non-volatile Memory (NVM) as\nthe primary storage medium, with a small amount of DRAM as cache. The main\nchallenge in storing embeddings on NVM is its limited read bandwidth compared\nto DRAM. Bandana uses two primary techniques to address this limitation: first,\nit stores embedding vectors that are likely to be read together in the same\nphysical location, using hypergraph partitioning, and second, it decides the\nnumber of embedding vectors to cache in DRAM by simulating dozens of small\ncaches. These techniques allow Bandana to increase the effective read bandwidth\nof NVM by 2-3x and thereby significantly reduce the total cost of ownership.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 17:47:33 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 01:48:26 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Eisenman", "Assaf", ""], ["Naumov", "Maxim", ""], ["Gardner", "Darryl", ""], ["Smelyanskiy", "Misha", ""], ["Pupyrev", "Sergey", ""], ["Hazelwood", "Kim", ""], ["Cidon", "Asaf", ""], ["Katti", "Sachin", ""]]}, {"id": "1811.05927", "submitter": "Zheng Tracy Ke", "authors": "Jiashun Jin, Zheng Tracy Ke, Shengming Luo", "title": "SCORE+ for Network Community Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SCORE is a recent approach to network community detection proposed by Jin\n(2015). In this note, we propose a simple improvement of SCORE, called SCORE+,\nand compare its performance with several other methods, using 10 different\nnetwork data sets. For 7 of these data sets, the performances of SCORE and\nSCORE+ are similar, but for the other 3 data sets (Polbooks, Simmons, Caltech),\nSCORE+ provides a significant improvement.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 17:53:33 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Jin", "Jiashun", ""], ["Ke", "Zheng Tracy", ""], ["Luo", "Shengming", ""]]}, {"id": "1811.05932", "submitter": "Xi Liu", "authors": "Xi Liu, Ping-Chun Hsieh, Nick Duffield, Rui Chen, Muhe Xie, Xidao Wen", "title": "Streaming Network Embedding through Local Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, considerable research attention has been paid to network embedding,\na popular approach to construct feature vectors of vertices. Due to the curse\nof dimensionality and sparsity in graphical datasets, this approach has become\nindispensable for machine learning tasks over large networks. The majority of\nexisting literature has considered this technique under the assumption that the\nnetwork is static. However, networks in many applications, nodes and edges\naccrue to a growing network as a streaming. A small number of very recent\nresults have addressed the problem of embedding for dynamic networks. However,\nthey either rely on knowledge of vertex attributes, suffer high-time complexity\nor need to be re-trained without closed-form expression. Thus the approach of\nadapting the existing methods to the streaming environment faces non-trivial\ntechnical challenges.\n  These challenges motivate developing new approaches to the problems of\nstreaming network embedding. In this paper, We propose a new framework that is\nable to generate latent features for new vertices with high efficiency and low\ncomplexity under specified iteration rounds. We formulate a constrained\noptimization problem for the modification of the representation resulting from\na stream arrival. We show this problem has no closed-form solution and instead\ndevelop an online approximation solution. Our solution follows three steps: (1)\nidentify vertices affected by new vertices, (2) generate latent features for\nnew vertices, and (3) update the latent features of the most affected vertices.\nThe generated representations are provably feasible and not far from the\noptimal ones in terms of expectation. Multi-class classification and clustering\non five real-world networks demonstrate that our model can efficiently update\nvertex representations and simultaneously achieve comparable or even better\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 18:02:29 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Liu", "Xi", ""], ["Hsieh", "Ping-Chun", ""], ["Duffield", "Nick", ""], ["Chen", "Rui", ""], ["Xie", "Muhe", ""], ["Wen", "Xidao", ""]]}, {"id": "1811.05933", "submitter": "Arash Mehrjou", "authors": "Arash Mehrjou, Bernhard Sch\\\"olkopf", "title": "Deep Nonlinear Non-Gaussian Filtering for Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filtering is a general name for inferring the states of a dynamical system\ngiven observations. The most common filtering approach is Gaussian Filtering\n(GF) where the distribution of the inferred states is a Gaussian whose mean is\nan affine function of the observations. There are two restrictions in this\nmodel: Gaussianity and Affinity. We propose a model to relax both these\nassumptions based on recent advances in implicit generative models. Empirical\nresults show that the proposed method gives a significant advantage over GF and\nnonlinear methods based on fixed nonlinear kernels.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 18:02:58 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Mehrjou", "Arash", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1811.05949", "submitter": "Marek Rei", "authors": "Marek Rei, Anders S{\\o}gaard", "title": "Jointly Learning to Label Sentences and Tokens", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to construct text representations in end-to-end systems can be\ndifficult, as natural languages are highly compositional and task-specific\nannotated datasets are often limited in size. Methods for directly supervising\nlanguage composition can allow us to guide the models based on existing\nknowledge, regularizing them towards more robust and interpretable\nrepresentations. In this paper, we investigate how objectives at different\ngranularities can be used to learn better language representations and we\npropose an architecture for jointly learning to label sentences and tokens. The\npredictions at each level are combined together using an attention mechanism,\nwith token-level labels also acting as explicit supervision for composing\nsentence-level representations. Our experiments show that by learning to\nperform these tasks jointly on multiple levels, the model achieves substantial\nimprovements for both sentence classification and sequence labeling.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 18:32:18 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Rei", "Marek", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1811.05965", "submitter": "Eli Sennesh", "authors": "Eli Sennesh, Adam \\'Scibior, Hao Wu, Jan-Willem van de Meent", "title": "Composing Modeling and Inference Operations with Probabilistic Program\n  Combinators", "comments": "Published at the NeurIPS workshop \"All of Bayesian Nonparametrics\n  (Especially the Useful Bits)\" 2018\n  (https://sites.google.com/view/nipsbnp2018/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic programs with dynamic computation graphs can define measures\nover sample spaces with unbounded dimensionality, which constitute programmatic\nanalogues to Bayesian nonparametrics. Owing to the generality of this model\nclass, inference relies on `black-box' Monte Carlo methods that are often not\nable to take advantage of conditional independence and exchangeability, which\nhave historically been the cornerstones of efficient inference. We here seek to\ndevelop a `middle ground' between probabilistic models with fully dynamic and\nfully static computation graphs. To this end, we introduce a combinator library\nfor the Probabilistic Torch framework. Combinators are functions that accept\nmodels and return transformed models. We assume that models are dynamic, but\nthat model composition is static, in the sense that combinator application\ntakes place prior to evaluating the model on data. Combinators provide\nprimitives for both model and inference composition. Model combinators take the\nform of classic functional programming constructs such as map and reduce. These\nconstructs define a computation graph at a coarsened level of representation,\nin which nodes correspond to models, rather than individual variables.\nInference combinators implement operations such as importance resampling and\napplication of a transition kernel, which alter the evaluation strategy for a\nmodel whilst preserving proper weighting. Owing to this property, models\ndefined using combinators can be trained using stochastic methods that optimize\neither variational or wake-sleep style objectives. As a validation of this\nprinciple, we use combinators to implement black box inference for hidden\nMarkov models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 18:53:28 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 14:16:04 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 01:05:58 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Sennesh", "Eli", ""], ["\u015acibior", "Adam", ""], ["Wu", "Hao", ""], ["van de Meent", "Jan-Willem", ""]]}, {"id": "1811.05975", "submitter": "Fredrik D. Johansson", "authors": "Fredrik D. Johansson", "title": "Machine Learning Analysis of Heterogeneity in the Effect of Student\n  Mindset Interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study heterogeneity in the effect of a mindset intervention on\nstudent-level performance through an observational dataset from the National\nStudy of Learning Mindsets (NSLM). Our analysis uses machine learning (ML) to\naddress the following associated problems: assessing treatment group overlap\nand covariate balance, imputing conditional average treatment effects, and\ninterpreting imputed effects. By comparing several different model families we\nillustrate the flexibility of both off-the-shelf and purpose-built estimators.\nWe find that the mindset intervention has a positive average effect of 0.26,\n95%-CI [0.22, 0.30], and that heterogeneity in the range of [0.1, 0.4] is\nmoderated by school-level achievement level, poverty concentration, urbanicity,\nand student prior expectations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 13:43:39 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Johansson", "Fredrik D.", ""]]}, {"id": "1811.05994", "submitter": "Itamar Reis", "authors": "Itamar Reis, Dalya Baron, and Sahar Shahaf", "title": "Probabilistic Random Forest: A machine learning algorithm for noisy\n  datasets", "comments": "Accepted by AJ, comments are welcome! Code is available at\n  https://github.com/ireis/PRF", "journal-ref": null, "doi": "10.3847/1538-3881/aaf101", "report-no": null, "categories": "astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) algorithms become increasingly important in the\nanalysis of astronomical data. However, since most ML algorithms are not\ndesigned to take data uncertainties into account, ML based studies are mostly\nrestricted to data with high signal-to-noise ratio. Astronomical datasets of\nsuch high-quality are uncommon. In this work we modify the long-established\nRandom Forest (RF) algorithm to take into account uncertainties in the\nmeasurements (i.e., features) as well as in the assigned classes (i.e.,\nlabels). To do so, the Probabilistic Random Forest (PRF) algorithm treats the\nfeatures and labels as probability distribution functions, rather than\ndeterministic quantities. We perform a variety of experiments where we inject\ndifferent types of noise to a dataset, and compare the accuracy of the PRF to\nthat of RF. The PRF outperforms RF in all cases, with a moderate increase in\nrunning time. We find an improvement in classification accuracy of up to 10% in\nthe case of noisy features, and up to 30% in the case of noisy labels. The PRF\naccuracy decreased by less then 5% for a dataset with as many as 45%\nmisclassified objects, compared to a clean dataset. Apart from improving the\nprediction accuracy in noisy datasets, the PRF naturally copes with missing\nvalues in the data, and outperforms RF when applied to a dataset with different\nnoise characteristics in the training and test sets, suggesting that it can be\nused for Transfer Learning.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 19:01:14 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Reis", "Itamar", ""], ["Baron", "Dalya", ""], ["Shahaf", "Sahar", ""]]}, {"id": "1811.06002", "submitter": "Gennady Ososkov Alexeevich", "authors": "Dmitriy Baranov, Gennady Ososkov, Pavel Goncharov, Andrei Tsytrinov", "title": "Catch and Prolong: recurrent neural network for seeking track-candidates", "comments": "5 pages, 1 figure, XXII International Scientific Conference of Young\n  Scientists and Specialists (AYSS-2018), April 23-27, 2018,\n  http://ayss-2018.jinr.ru", "journal-ref": null, "doi": "10.1051/epjconf/201920105001", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the most important problems of data processing in high energy and\nnuclear physics is the event reconstruction. Its main part is the track\nreconstruction procedure which consists in looking for all tracks that\nelementary particles leave when they pass through a detector among a huge\nnumber of points, so-called hits, produced when flying particles fire detector\ncoordinate planes. Unfortunately, the tracking is seriously impeded by the\nfamous shortcoming of multiwired, strip and GEM detectors due to appearance in\nthem a lot of fake hits caused by extra spurious crossings of fired strips.\nSince the number of those fakes is several orders of magnitude greater than for\ntrue hits, one faces with the quite serious difficulty to unravel possible\ntrack-candidates via true hits ignoring fakes. We introduce a renewed method\nthat is a significant improvement of our previous two-stage approach based on\nhit preprocessing using directed K-d tree search followed a deep neural\nclassifier. We combine these two stages in one by applying recurrent neural\nnetwork that simultaneously determines whether a set of points belongs to a\ntrue track or not and predicts where to look for the next point of track on the\nnext coordinate plane of the detector. We show that proposed deep network is\nmore accurate, faster and does not require any special preprocessing stage.\nPreliminary results of our approach for simulated events of the BM@N GEM\ndetector are presented.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 19:03:26 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Baranov", "Dmitriy", ""], ["Ososkov", "Gennady", ""], ["Goncharov", "Pavel", ""], ["Tsytrinov", "Andrei", ""]]}, {"id": "1811.06017", "submitter": "Cunxi Yu", "authors": "Cunxi Yu and Wang Zhou", "title": "Performance Estimation of Synthesis Flows cross Technologies using LSTMs\n  and Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing complexity of Integrated Circuits (ICs) and\nSystem-on-Chip (SoC), developing high-quality synthesis flows within a short\nmarket time becomes more challenging. We propose a general approach that\nprecisely estimates the Quality-of-Result (QoR), such as delay and area, of\nunseen synthesis flows for specific designs. The main idea is training a\nRecurrent Neural Network (RNN) regressor, where the flows are inputs and QoRs\nare ground truth. The RNN regressor is constructed with Long Short-Term Memory\n(LSTM) and fully-connected layers. This approach is demonstrated with 1.2\nmillion data points collected using 14nm, 7nm regular-voltage (RVT), and 7nm\nlow-voltage (LVT) FinFET technologies with twelve IC designs. The accuracy of\npredicting the QoRs (delay and area) within one technology is\n$\\boldsymbol{\\geq}$\\textbf{98.0}\\% over $\\sim$240,000 test points. To enable\naccurate predictions cross different technologies and different IC designs, we\npropose a transfer-learning approach that utilizes the model pre-trained with\n14nm datasets. Our transfer learning approach obtains estimation accuracy\n$\\geq$96.3\\% over $\\sim$960,000 test points, using only 100 data points for\ntraining.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 19:17:14 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Yu", "Cunxi", ""], ["Zhou", "Wang", ""]]}, {"id": "1811.06026", "submitter": "Jieming Mao", "authors": "Nicole Immorlica, Jieming Mao, Aleksandrs Slivkins, Zhiwei Steven Wu", "title": "Incentivizing Exploration with Selective Data Disclosure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the design of rating systems that incentivize efficient social\nlearning. Agents arrive sequentially and choose actions, each of which yields a\nreward drawn from an unknown distribution. A policy maps the rewards of\npreviously-chosen actions to messages for arriving agents. The regret of a\npolicy is the difference, over all rounds, between the expected reward of the\nbest action and the reward induced by the policy. Prior work proposes policies\nthat recommend a single action to each agent, obtaining optimal regret under\nstandard rationality assumptions. We instead assume a frequentist behavioral\nmodel and, accordingly, restrict attention to disclosure policies that use\nmessages consisting of the actions and rewards from a subsequence of past\nagents, chosen ex ante. We design a policy with optimal regret in the worst\ncase over reward distributions. Our research suggests three components of\neffective polices: independent focus groups, group aggregators, and interlaced\ninformation structures.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 19:29:16 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 19:26:09 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 15:58:06 GMT"}, {"version": "v4", "created": "Tue, 29 Dec 2020 16:41:07 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Immorlica", "Nicole", ""], ["Mao", "Jieming", ""], ["Slivkins", "Aleksandrs", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1811.06029", "submitter": "Qinglong Wang", "authors": "Qinglong Wang, Kaixuan Zhang, Xue Liu, C. Lee Giles", "title": "Verification of Recurrent Neural Networks Through Rule Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The verification problem for neural networks is verifying whether a neural\nnetwork will suffer from adversarial samples, or approximating the maximal\nallowed scale of adversarial perturbation that can be endured. While most prior\nwork contributes to verifying feed-forward networks, little has been explored\nfor verifying recurrent networks. This is due to the existence of a more\nrigorous constraint on the perturbation space for sequential data, and the lack\nof a proper metric for measuring the perturbation. In this work, we address\nthese challenges by proposing a metric which measures the distance between\nstrings, and use deterministic finite automata (DFA) to represent a rigorous\noracle which examines if the generated adversarial samples violate certain\nconstraints on a perturbation. More specifically, we empirically show that\ncertain recurrent networks allow relatively stable DFA extraction. As such,\nDFAs extracted from these recurrent networks can serve as a surrogate oracle\nfor when the ground truth DFA is unknown. We apply our verification mechanism\nto several widely used recurrent networks on a set of the Tomita grammars. The\nresults demonstrate that only a few models remain robust against adversarial\nsamples. In addition, we show that for grammars with different levels of\ncomplexity, there is also a difference in the difficulty of robust learning of\nthese grammars.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 19:40:30 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Wang", "Qinglong", ""], ["Zhang", "Kaixuan", ""], ["Liu", "Xue", ""], ["Giles", "C. Lee", ""]]}, {"id": "1811.06032", "submitter": "Amy Zhang", "authors": "Amy Zhang, Yuxin Wu, Joelle Pineau", "title": "Natural Environment Benchmarks for Reinforcement Learning", "comments": "12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While current benchmark reinforcement learning (RL) tasks have been useful to\ndrive progress in the field, they are in many ways poor substitutes for\nlearning with real-world data. By testing increasingly complex RL algorithms on\nlow-complexity simulation environments, we often end up with brittle RL\npolicies that generalize poorly beyond the very specific domain. To combat\nthis, we propose three new families of benchmark RL domains that contain some\nof the complexity of the natural world, while still supporting fast and\nextensive data acquisition. The proposed domains also permit a characterization\nof generalization through fair train/test separation, and easy comparison and\nreplication of results. Through this work, we challenge the RL research\ncommunity to develop more robust algorithms that meet high standards of\nevaluation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 19:50:54 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Zhang", "Amy", ""], ["Wu", "Yuxin", ""], ["Pineau", "Joelle", ""]]}, {"id": "1811.06052", "submitter": "Georgios Mastorakis", "authors": "Georgios Mastorakis", "title": "Human-like machine learning: limitations and suggestions", "comments": "Preprint, 24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper attempts to address the issues of machine learning in its current\nimplementation. It is known that machine learning algorithms require a\nsignificant amount of data for training purposes, whereas recent developments\nin deep learning have increased this requirement dramatically. The performance\nof an algorithm depends on the quality of data and hence, algorithms are as\ngood as the data they are trained on. Supervised learning is developed based on\nhuman learning processes by analysing named (i.e. annotated) objects, scenes\nand actions. Whether training on large quantities of data (i.e. big data) is\nthe right or the wrong approach, is debatable. The fact is, that training\nalgorithms the same way we learn ourselves, comes with limitations. This paper\ndiscusses the issues around applying a human-like approach to train algorithms\nand the implications of this approach when using limited data. Several current\nstudies involving non-data-driven algorithms and natural examples are also\ndiscussed and certain alternative approaches are suggested.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 20:44:50 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Mastorakis", "Georgios", ""]]}, {"id": "1811.06060", "submitter": "Truyen Tran", "authors": "Phuoc Nguyen, Truyen Tran, Sunil Gupta, Santu Rana and Svetha\n  Venkatesh", "title": "Hybrid Generative-Discriminative Models for Inverse Materials Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.mtrl-sci cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering new physical products and processes often demands enormous\nexperimentation and expensive simulation. To design a new product with certain\ntarget characteristics, an extensive search is performed in the design space by\ntrying out a large number of design combinations before reaching to the target\ncharacteristics. However, forward searching for the target design becomes\nprohibitive when the target is itself moving or only partially understood. To\naddress this bottleneck, we propose to use backward prediction by leveraging\nthe rich data generated during earlier exploration and construct a machine\nlearning framework to predict the design parameters for any target in a single\nstep. This poses two technical challenges: the first caused due to one-to-many\nmapping when learning the inverse problem and the second caused due to an user\nspecifying the target specifications only partially. To overcome the\nchallenges, we formulate this problem as conditional density estimation under\nhigh-dimensional setting with incomplete input and multimodal output. We solve\nthe problem through a deep hybrid generative-discriminative model, which is\ntrained end-to-end to predict the optimum design.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 03:04:41 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Nguyen", "Phuoc", ""], ["Tran", "Truyen", ""], ["Gupta", "Sunil", ""], ["Rana", "Santu", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1811.06067", "submitter": "Sambuddha Ghosal", "authors": "Balaji Sesha Sarath Pokuri, Sambuddha Ghosal, Apurva Kokate, Baskar\n  Ganapathysubramanian and Soumik Sarkar", "title": "Interpretable deep learning for guided structure-property explorations\n  in photovoltaics", "comments": "Workshop on Machine Learning for Molecules and Materials (MLMM),\n  Neural Information Processing Systems (NeurIPS) 2018, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of an organic photovoltaic device is intricately connected to\nits active layer morphology. This connection between the active layer and\ndevice performance is very expensive to evaluate, either experimentally or\ncomputationally. Hence, designing morphologies to achieve higher performances\nis non-trivial and often intractable. To solve this, we first introduce a deep\nconvolutional neural network (CNN) architecture that can serve as a fast and\nrobust surrogate for the complex structure-property map. Several tests were\nperformed to gain trust in this trained model. Then, we utilize this fast\nframework to perform robust microstructural design to enhance device\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 21:08:08 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 19:59:56 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2018 02:14:14 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Pokuri", "Balaji Sesha Sarath", ""], ["Ghosal", "Sambuddha", ""], ["Kokate", "Apurva", ""], ["Ganapathysubramanian", "Baskar", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1811.06083", "submitter": "Ruidi Chen", "authors": "Ruidi Chen, and Ioannis Paschalidis", "title": "Learning Optimal Personalized Treatment Rules Using Robust Regression\n  Informed K-NN", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a prediction-based prescriptive model for learning optimal\npersonalized treatments for patients based on their Electronic Health Records\n(EHRs). Our approach consists of: (i) predicting future outcomes under each\npossible therapy using a robustified nonlinear model, and (ii) adopting a\nrandomized prescriptive policy determined by the predicted outcomes. We show\ntheoretical results that guarantee the out-of-sample predictive power of the\nmodel, and prove the optimality of the randomized strategy in terms of the\nexpected true future outcome. We apply the proposed methodology to develop\noptimal therapies for patients with type 2 diabetes or hypertension using EHRs\nfrom a major safety-net hospital in New England, and show that our algorithm\nleads to a larger reduction of the HbA1c, for diabetics, or systolic blood\npressure, for patients with hypertension, compared to the alternatives. We\ndemonstrate that our approach outperforms the standard of care under the\nrobustified nonlinear predictive model.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 21:46:56 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 18:14:23 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2018 18:25:30 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Chen", "Ruidi", ""], ["Paschalidis", "Ioannis", ""]]}, {"id": "1811.06086", "submitter": "Amin Hosseininasab", "authors": "Amin Hosseininasab, Willem-Jan van Hoeve, Andre A. Cire", "title": "Constraint-based Sequential Pattern Mining with Decision Diagrams", "comments": "AAAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained sequential pattern mining aims at identifying frequent patterns\non a sequential database of items while observing constraints defined over the\nitem attributes. We introduce novel techniques for constraint-based sequential\npattern mining that rely on a multi-valued decision diagram representation of\nthe database. Specifically, our representation can accommodate multiple item\nattributes and various constraint types, including a number of non-monotone\nconstraints. To evaluate the applicability of our approach, we develop an\nMDD-based prefix-projection algorithm and compare its performance against a\ntypical generate-and-check variant, as well as a state-of-the-art\nconstraint-based sequential pattern mining algorithm. Results show that our\napproach is competitive with or superior to these other methods in terms of\nscalability and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 21:54:58 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Hosseininasab", "Amin", ""], ["van Hoeve", "Willem-Jan", ""], ["Cire", "Andre A.", ""]]}, {"id": "1811.06094", "submitter": "Soumya Ghosh", "authors": "Kristen Severson, Soumya Ghosh, Kenney Ng", "title": "Unsupervised learning with contrastive latent variable models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In unsupervised learning, dimensionality reduction is an important tool for\ndata exploration and visualization. Because these aims are typically\nopen-ended, it can be useful to frame the problem as looking for patterns that\nare enriched in one dataset relative to another. These pairs of datasets occur\ncommonly, for instance a population of interest vs. control or signal vs.\nsignal free recordings.However, there are few methods that work on sets of data\nas opposed to data points or sequences. Here, we present a probabilistic model\nfor dimensionality reduction to discover signal that is enriched in the target\ndataset relative to the background dataset. The data in these sets do not need\nto be paired or grouped beyond set membership. By using a probabilistic model\nwhere some structure is shared amongst the two datasets and some is unique to\nthe target dataset, we are able to recover interesting structure in the latent\nspace of the target dataset. The method also has the advantages of a\nprobabilistic model, namely that it allows for the incorporation of prior\ninformation, handles missing data, and can be generalized to different\ndistributional assumptions. We describe several possible variations of the\nmodel and demonstrate the application of the technique to de-noising, feature\nselection, and subgroup discovery settings.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 22:12:56 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Severson", "Kristen", ""], ["Ghosh", "Soumya", ""], ["Ng", "Kenney", ""]]}, {"id": "1811.06100", "submitter": "Kent Loong Tan", "authors": "Chien-Chih Wang, Kent Loong Tan, Chih-Jen Lin", "title": "Newton Methods for Convolutional Neural Networks", "comments": "Supplementary materials, experimental code and an efficient MATLAB\n  implementation are available at https://www.csie.ntu.edu.tw/~cjlin/cnn/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning involves a difficult non-convex optimization problem, which is\noften solved by stochastic gradient (SG) methods. While SG is usually\neffective, it may not be robust in some situations. Recently, Newton methods\nhave been investigated as an alternative optimization technique, but nearly all\nexisting studies consider only fully-connected feedforward neural networks.\nThey do not investigate other types of networks such as Convolutional Neural\nNetworks (CNN), which are more commonly used in deep-learning applications. One\nreason is that Newton methods for CNN involve complicated operations, and so\nfar no works have conducted a thorough investigation. In this work, we give\ndetails of all building blocks including function, gradient, and Jacobian\nevaluation, and Gauss-Newton matrix-vector products. These basic components are\nvery important because with them further developments of Newton methods for CNN\nbecome possible. We show that an efficient MATLAB implementation can be done in\njust several hundred lines of code and demonstrate that the Newton method gives\ncompetitive test accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 22:29:37 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Wang", "Chien-Chih", ""], ["Tan", "Kent Loong", ""], ["Lin", "Chih-Jen", ""]]}, {"id": "1811.06103", "submitter": "S. Asim Ahmed", "authors": "S. Asim Ahmed, Subhashish Chakravarty and Michael Newhouse", "title": "Deep Neural Networks based Modrec: Some Results with Inter-Symbol\n  Interference and Adversarial Examples", "comments": "4 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent successes and advances in Deep Neural Networks (DNN) in machine vision\nand Natural Language Processing (NLP) have motivated their use in traditional\nsignal processing and communications systems. In this paper, we present results\nof such applications to the problem of automatic modulation recognition.\nVariations in wireless communication channels are represented by statistical\nchannel models and their parameterization will increase with the advent of 5G.\nIn this paper, we report effect of simple two path channel model on our naive\ndeep neural network based implementation. We also report impact of adversarial\nperturbation to the input signal.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 22:36:47 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Ahmed", "S. Asim", ""], ["Chakravarty", "Subhashish", ""], ["Newhouse", "Michael", ""]]}, {"id": "1811.06109", "submitter": "Junxuan Li", "authors": "Junxuan Li, Yung-wen Liu, Yuting Jia, Yifei Ren, Jay Nanduri", "title": "Predictive Modeling with Delayed Information: a Case Study in E-commerce\n  Transaction Fraud Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Business Intelligence, accurate predictive modeling is the key for\nproviding adaptive decisions. We studied predictive modeling problems in this\nresearch which was motivated by real-world cases that Microsoft data scientists\nencountered while dealing with e-commerce transaction fraud control decisions\nusing transaction streaming data in an uncertain probabilistic decision\nenvironment. The values of most online transactions related features can return\ninstantly, while the true fraud labels only return after a stochastic delay.\nUsing partially mature data directly for predictive modeling in an uncertain\nprobabilistic decision environment would lead to significant inaccuracy on risk\ndecision-making. To improve accurate estimation of the probabilistic prediction\nenvironment, which leads to more accurate predictive modeling, two frameworks,\nCurrent Environment Inference (CEI) and Future Environment Inference (FEI), are\nproposed. These frameworks generated decision environment related features\nusing long-term fully mature and short-term partially mature data, and the\nvalues of those features were estimated using varies of learning methods,\nincluding linear regression, random forest, gradient boosted tree, artificial\nneural network, and recurrent neural network. Performance tests were conducted\nusing some e-commerce transaction data from Microsoft. Testing results\nsuggested that proposed frameworks significantly improved the accuracy of\ndecision environment estimation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 23:08:14 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Li", "Junxuan", ""], ["Liu", "Yung-wen", ""], ["Jia", "Yuting", ""], ["Ren", "Yifei", ""], ["Nanduri", "Jay", ""]]}, {"id": "1811.06128", "submitter": "Antoine Prouvost", "authors": "Yoshua Bengio and Andrea Lodi and Antoine Prouvost", "title": "Machine Learning for Combinatorial Optimization: a Methodological Tour\n  d'Horizon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper surveys the recent attempts, both from the machine learning and\noperations research communities, at leveraging machine learning to solve\ncombinatorial optimization problems. Given the hard nature of these problems,\nstate-of-the-art algorithms rely on handcrafted heuristics for making decisions\nthat are otherwise too expensive to compute or mathematically not well defined.\nThus, machine learning looks like a natural candidate to make such decisions in\na more principled and optimized way. We advocate for pushing further the\nintegration of machine learning and combinatorial optimization and detail a\nmethodology to do so. A main point of the paper is seeing generic optimization\nproblems as data points and inquiring what is the relevant distribution of\nproblems to use for learning on a given task.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 00:40:32 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 18:53:21 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Bengio", "Yoshua", ""], ["Lodi", "Andrea", ""], ["Prouvost", "Antoine", ""]]}, {"id": "1811.06145", "submitter": "Jing Shi", "authors": "Jing Shi, Jiaming Xu, Yiqun Yao, Bo Xu", "title": "Concept Learning through Deep Reinforcement Learning with\n  Memory-Augmented Neural Networks", "comments": "27 pages, 2 figures", "journal-ref": null, "doi": "10.1016/j.neunet.2018.10.018", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown superior performance in many regimes to\nremember familiar patterns with large amounts of data. However, the standard\nsupervised deep learning paradigm is still limited when facing the need to\nlearn new concepts efficiently from scarce data. In this paper, we present a\nmemory-augmented neural network which is motivated by the process of human\nconcept learning. The training procedure, imitating the concept formation\ncourse of human, learns how to distinguish samples from different classes and\naggregate samples of the same kind. In order to better utilize the advantages\noriginated from the human behavior, we propose a sequential process, during\nwhich the network should decide how to remember each sample at every step. In\nthis sequential process, a stable and interactive memory serves as an important\nmodule. We validate our model in some typical one-shot learning tasks and also\nan exploratory outlier detection problem. In all the experiments, our model\ngets highly competitive to reach or outperform those strong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 02:38:57 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Shi", "Jing", ""], ["Xu", "Jiaming", ""], ["Yao", "Yiqun", ""], ["Xu", "Bo", ""]]}, {"id": "1811.06146", "submitter": "Liang Zhang", "authors": "Liang Zhang, Gang Wang, Georgios B. Giannakis", "title": "Real-time Power System State Estimation and Forecasting via Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2926023", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary power grids are being challenged by rapid voltage fluctuations\nthat are caused by large-scale deployment of renewable generation, electric\nvehicles, and demand response programs. In this context, monitoring the grid's\noperating conditions in real time becomes increasingly critical. With the\nemergent large scale and nonconvexity however, the existing power system state\nestimation (PSSE) schemes become computationally expensive or yield suboptimal\nperformance. To bypass these hurdles, this paper advocates deep neural networks\n(DNNs) for real-time power system monitoring. By unrolling an iterative\nphysics-based prox-linear solver, a novel model-specific DNN is developed for\nreal-time PSSE with affordable training and minimal tuning effort. To further\nenable system awareness even ahead of the time horizon, as well as to endow the\nDNN-based estimator with resilience, deep recurrent neural networks (RNNs) are\nalso pursued for power system state forecasting. Deep RNNs leverage the\nlong-term nonlinear dependencies present in the historical voltage time series\nto enable forecasting, and they are easy to implement. Numerical tests showcase\nimproved performance of the proposed DNN-based estimation and forecasting\napproaches compared with existing alternatives. In real load data experiments\non the IEEE 118-bus benchmark system, the novel model-specific DNN-based PSSE\nscheme outperforms nearly by an order-of-magnitude the competing alternatives,\nincluding the widely adopted Gauss-Newton PSSE solver.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 02:41:01 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 03:02:40 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhang", "Liang", ""], ["Wang", "Gang", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1811.06149", "submitter": "Maryam Aziz", "authors": "Maryam Aziz and Kevin Jamieson and Javed Aslam", "title": "Pure-Exploration for Infinite-Armed Bandits with General Arm Reservoirs", "comments": "We found an irrecoverable error in one of the proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a multi-armed bandit game where the number of arms is\nmuch larger than the maximum budget and is effectively infinite. We\ncharacterize necessary and sufficient conditions on the total budget for an\nalgorithm to return an {\\epsilon}-good arm with probability at least 1 -\n{\\delta}. In such situations, the sample complexity depends on {\\epsilon},\n{\\delta} and the so-called reservoir distribution {\\nu} from which the means of\nthe arms are drawn iid. While a substantial literature has developed around\nanalyzing specific cases of {\\nu} such as the beta distribution, our analysis\nmakes no assumption about the form of {\\nu}. Our algorithm is based on\nsuccessive halving with the surprising exception that arms start to be\ndiscarded after just a single pull, requiring an analysis that goes beyond\nconcentration alone. The provable correctness of this algorithm also provides\nan explanation for the empirical observation that the most aggressive bracket\nof the Hyperband algorithm of Li et al. (2017) for hyperparameter tuning is\nalmost always best.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 02:51:04 GMT"}, {"version": "v2", "created": "Sun, 13 Jan 2019 00:04:15 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Aziz", "Maryam", ""], ["Jamieson", "Kevin", ""], ["Aslam", "Javed", ""]]}, {"id": "1811.06150", "submitter": "David Moore", "authors": "Dave Moore and Maria I. Gorinova", "title": "Effect Handling for Composable Program Transformations in Edward2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algebraic effects and handlers have emerged in the programming languages\ncommunity as a convenient, modular abstraction for controlling computational\neffects. They have found several applications including concurrent programming,\nmeta programming, and more recently, probabilistic programming, as part of\nPyro's Poutines library. We investigate the use of effect handlers as a\nlightweight abstraction for implementing probabilistic programming languages\n(PPLs). We interpret the existing design of Edward2 as an accidental\nimplementation of an effect-handling mechanism, and extend that design to\nsupport nested, composable transformations. We demonstrate that this enables\nstraightforward implementation of sophisticated model transformations and\ninference algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 02:51:29 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Moore", "Dave", ""], ["Gorinova", "Maria I.", ""]]}, {"id": "1811.06156", "submitter": "Yu Hao", "authors": "Yu Hao, Xien Liu, Ji Wu, Ping Lv", "title": "Exploiting Sentence Embedding for Medical Question Answering", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great success of word embedding, sentence embedding remains a\nnot-well-solved problem. In this paper, we present a supervised learning\nframework to exploit sentence embedding for the medical question answering\ntask. The learning framework consists of two main parts: 1) a sentence\nembedding producing module, and 2) a scoring module. The former is developed\nwith contextual self-attention and multi-scale techniques to encode a sentence\ninto an embedding tensor. This module is shortly called Contextual\nself-Attention Multi-scale Sentence Embedding (CAMSE). The latter employs two\nscoring strategies: Semantic Matching Scoring (SMS) and Semantic Association\nScoring (SAS). SMS measures similarity while SAS captures association between\nsentence pairs: a medical question concatenated with a candidate choice, and a\npiece of corresponding supportive evidence. The proposed framework is examined\nby two Medical Question Answering(MedicalQA) datasets which are collected from\nreal-world applications: medical exam and clinical diagnosis based on\nelectronic medical records (EMR). The comparison results show that our proposed\nframework achieved significant improvements compared to competitive baseline\napproaches. Additionally, a series of controlled experiments are also conducted\nto illustrate that the multi-scale strategy and the contextual self-attention\nlayer play important roles for producing effective sentence embedding, and the\ntwo kinds of scoring strategies are highly complementary to each other for\nquestion answering problems.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 03:38:20 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Hao", "Yu", ""], ["Liu", "Xien", ""], ["Wu", "Ji", ""], ["Lv", "Ping", ""]]}, {"id": "1811.06173", "submitter": "Huicheng Liu", "authors": "Huicheng Liu", "title": "Leveraging Financial News for Stock Trend Prediction with\n  Attention-Based Recurrent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stock market prediction is one of the most attractive research topic since\nthe successful prediction on the market's future movement leads to significant\nprofit. Traditional short term stock market predictions are usually based on\nthe analysis of historical market data, such as stock prices, moving averages\nor daily returns. However, financial news also contains useful information on\npublic companies and the market. Existing methods in finance literature exploit\nsentiment signal features, which are limited by not considering factors such as\nevents and the news context. We address this issue by leveraging deep neural\nmodels to extract rich semantic features from news text. In particular, a\nBidirectional-LSTM are used to encode the news text and capture the context\ninformation, self attention mechanism are applied to distribute attention on\nmost relative words, news and days. In terms of predicting directional changes\nin both Standard & Poor's 500 index and individual companies stock price, we\nshow that this technique is competitive with other state of the art approaches,\ndemonstrating the effectiveness of recent NLP technology advances for\ncomputational finance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 04:49:21 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Liu", "Huicheng", ""]]}, {"id": "1811.06199", "submitter": "Trung Le", "authors": "Trung Le, Khanh Nguyen, Nhat Ho, Hung Bui, Dinh Phung", "title": "On Deep Domain Adaptation: Some Theoretical Understandings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with shallow domain adaptation, recent progress in deep domain\nadaptation has shown that it can achieve higher predictive performance and\nstronger capacity to tackle structural data (e.g., image and sequential data).\nThe underlying idea of deep domain adaptation is to bridge the gap between\nsource and target domains in a joint space so that a supervised classifier\ntrained on labeled source data can be nicely transferred to the target domain.\nThis idea is certainly intuitive and powerful, however, limited theoretical\nunderstandings have been developed to support its underpinning principle. In\nthis paper, we have provided a rigorous framework to explain why it is possible\nto close the gap of the target and source domains in the joint space. More\nspecifically, we first study the loss incurred when performing transfer\nlearning from the source to the target domain. This provides a theory that\nexplains and generalizes existing work in deep domain adaptation which was\nmainly empirical. This enables us to further explain why closing the gap in the\njoint space can directly minimize the loss incurred for transfer learning\nbetween the two domains. To our knowledge, this offers the first theoretical\nresult that characterizes a direct bound on the joint space and the gain of\ntransfer learning via deep domain adaptation\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 06:27:15 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 00:48:19 GMT"}, {"version": "v3", "created": "Wed, 19 Jun 2019 23:41:40 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Le", "Trung", ""], ["Nguyen", "Khanh", ""], ["Ho", "Nhat", ""], ["Bui", "Hung", ""], ["Phung", "Dinh", ""]]}, {"id": "1811.06210", "submitter": "Yu Nishiyama", "authors": "Shunsuke Tsuzuki and Yu Nishiyama", "title": "Short-Term Wind-Speed Forecasting Using Kernel Spectral Hidden Markov\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, a nonparametric forecasting algorithm for time series\ndata has been proposed, called the kernel spectral hidden Markov model (KSHMM).\nIn this paper, we propose a technique for short-term wind-speed prediction\nbased on KSHMM. We numerically compared the performance of our KSHMM-based\nforecasting technique to other techniques with machine learning, using\nwind-speed data offered by the National Renewable Energy Laboratory. Our\nresults demonstrate that, compared to these methods, the proposed technique\noffers comparable or better performance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 07:26:19 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Tsuzuki", "Shunsuke", ""], ["Nishiyama", "Yu", ""]]}, {"id": "1811.06219", "submitter": "Leo Laugier", "authors": "Leo Laugier, Daniil Bash, Jose Recatala, Hong Kuan Ng, Savitha\n  Ramasamy, Chuan-Sheng Foo, Vijay R Chandrasekhar, Kedar Hippalgaonkar", "title": "Predicting thermoelectric properties from crystal graphs and material\n  descriptors - first application for functional materials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cond-mat.mtrl-sci cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the use of Crystal Graph Convolutional Neural Networks (CGCNN),\nFully Connected Neural Networks (FCNN) and XGBoost to predict thermoelectric\nproperties. The dataset for the CGCNN is independent of Density Functional\nTheory (DFT) and only relies on the crystal and atomic information, while that\nfor the FCNN is based on a rich attribute list mined from Materialsproject.org.\nThe results show that the optimized FCNN is three layer deep and is able to\npredict the scattering-time independent thermoelectric powerfactor much better\nthan the CGCNN (or XGBoost), suggesting that bonding and density of states\ndescriptors informed from materials science knowledge obtained partially from\nDFT are vital to predict functional properties.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 07:57:47 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Laugier", "Leo", ""], ["Bash", "Daniil", ""], ["Recatala", "Jose", ""], ["Ng", "Hong Kuan", ""], ["Ramasamy", "Savitha", ""], ["Foo", "Chuan-Sheng", ""], ["Chandrasekhar", "Vijay R", ""], ["Hippalgaonkar", "Kedar", ""]]}, {"id": "1811.06224", "submitter": "Moritz Kulessa", "authors": "Moritz Kulessa, Alejandro Molina, Carsten Binnig, Benjamin Hilprecht,\n  Kristian Kersting", "title": "Model-based Approximate Query Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive visualizations are arguably the most important tool to explore,\nunderstand and convey facts about data. In the past years, the database\ncommunity has been working on different techniques for Approximate Query\nProcessing (AQP) that aim to deliver an approximate query result given a fixed\ntime bound to support interactive visualizations better. However, classical AQP\napproaches suffer from various problems that limit the applicability to support\nthe ad-hoc exploration of a new data set: (1) Classical AQP approaches that\nperform online sampling can support ad-hoc exploration queries but yield low\nquality if executed over rare subpopulations. (2) Classical AQP approaches that\nrely on offline sampling can use some form of biased sampling to mitigate these\nproblems but require a priori knowledge of the workload, which is often not\nrealistic if users want to explore a new database. In this paper, we present a\nnew approach to AQP called Model-based AQP that leverages generative models\nlearned over the complete database to answer SQL queries at interactive speeds.\nDifferent from classical AQP approaches, generative models allow us to compute\nresponses to ad-hoc queries and deliver high-quality estimates also over rare\nsubpopulations at the same time. In our experiments with real and synthetic\ndata sets, we show that Model-based AQP can in many scenarios return more\naccurate results in a shorter runtime. Furthermore, we think that our\ntechniques of using generative models presented in this paper can not only be\nused for AQP in databases but also has applications for other database problems\nincluding Query Optimization as well as Data Cleaning.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 08:14:53 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Kulessa", "Moritz", ""], ["Molina", "Alejandro", ""], ["Binnig", "Carsten", ""], ["Hilprecht", "Benjamin", ""], ["Kersting", "Kristian", ""]]}, {"id": "1811.06225", "submitter": "Sergey Pankov", "authors": "Sergey Pankov", "title": "Reward-estimation variance elimination in sequential decision processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy gradient methods are very attractive in reinforcement learning due to\ntheir model-free nature and convergence guarantees. These methods, however,\nsuffer from high variance in gradient estimation, resulting in poor sample\nefficiency. To mitigate this issue, a number of variance-reduction approaches\nhave been proposed. Unfortunately, in the challenging problems with delayed\nrewards, these approaches either bring a relatively modest improvement or do\nreduce variance at expense of introducing a bias and undermining convergence.\nThe unbiased methods of gradient estimation, in general, only partially reduce\nvariance, without eliminating it completely even in the limit of exact\nknowledge of the value functions and problem dynamics, as one might have\nwished. In this work we propose an unbiased method that does completely\neliminate variance under some, commonly encountered, conditions. Of practical\ninterest is the limit of deterministic dynamics and small policy stochasticity.\nIn the case of a quadratic value function, as in linear quadratic Gaussian\nmodels, the policy randomness need not be small. We use such a model to analyze\nperformance of the proposed variance-elimination approach and compare it with\nstandard variance-reduction methods. The core idea behind the approach is to\nuse control variates at all future times down the trajectory. We present both a\nmodel-based and model-free formulations.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 08:18:33 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Pankov", "Sergey", ""]]}, {"id": "1811.06231", "submitter": "Minggang Zeng Mr", "authors": "Minggang Zeng, Jatin Nitin Kumar, Zeng Zeng, Ramasamy Savitha, Vijay\n  Ramaseshan Chandrasekhar, Kedar Hippalgaonkar", "title": "Graph Convolutional Neural Networks for Polymers Property Prediction", "comments": "Accepted for NIPS 2018 Workshop on Machine Learning for Molecules and\n  Materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fast and accurate predictive tool for polymer properties is demanding and\nwill pave the way to iterative inverse design. In this work, we apply graph\nconvolutional neural networks (GCNN) to predict the dielectric constant and\nenergy bandgap of polymers. Using density functional theory (DFT) calculated\nproperties as the ground truth, GCNN can achieve remarkable agreement with DFT\nresults. Moreover, we show that GCNN outperforms other machine learning\nalgorithms. Our work proves that GCNN relies only on morphological data of\npolymers and removes the requirement for complicated hand-crafted descriptors,\nwhile still offering accuracy in fast predictions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 08:31:55 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Zeng", "Minggang", ""], ["Kumar", "Jatin Nitin", ""], ["Zeng", "Zeng", ""], ["Savitha", "Ramasamy", ""], ["Chandrasekhar", "Vijay Ramaseshan", ""], ["Hippalgaonkar", "Kedar", ""]]}, {"id": "1811.06234", "submitter": "Daniel Michelsanti", "authors": "Daniel Michelsanti, Zheng-Hua Tan, Sigurdur Sigurdsson, Jesper Jensen", "title": "On Training Targets and Objective Functions for Deep-Learning-Based\n  Audio-Visual Speech Enhancement", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2019.8682790", "report-no": null, "categories": "eess.AS cs.LG cs.SD eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio-visual speech enhancement (AV-SE) is the task of improving speech\nquality and intelligibility in a noisy environment using audio and visual\ninformation from a talker. Recently, deep learning techniques have been adopted\nto solve the AV-SE task in a supervised manner. In this context, the choice of\nthe target, i.e. the quantity to be estimated, and the objective function,\nwhich quantifies the quality of this estimate, to be used for training is\ncritical for the performance. This work is the first that presents an\nexperimental study of a range of different targets and objective functions used\nto train a deep-learning-based AV-SE system. The results show that the\napproaches that directly estimate a mask perform the best overall in terms of\nestimated speech quality and intelligibility, although the model that directly\nestimates the log magnitude spectrum performs as good in terms of estimated\nspeech quality.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 08:39:04 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Michelsanti", "Daniel", ""], ["Tan", "Zheng-Hua", ""], ["Sigurdsson", "Sigurdur", ""], ["Jensen", "Jesper", ""]]}, {"id": "1811.06237", "submitter": "Anton Tsitsulin", "authors": "Anton Tsitsulin, Davide Mottin, Panagiotis Karras, Alex Bronstein,\n  Emmanuel M\\\"uller", "title": "SGR: Self-Supervised Spectral Graph Representation Learning", "comments": "As appeared in KDD Deep Learning Day workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing a graph as a vector is a challenging task; ideally, the\nrepresentation should be easily computable and conducive to efficient\ncomparisons among graphs, tailored to the particular data and analytical task\nat hand. Unfortunately, a \"one-size-fits-all\" solution is unattainable, as\ndifferent analytical tasks may require different attention to global or local\ngraph features. We develop SGR, the first, to our knowledge, method for\nlearning graph representations in a self-supervised manner. Grounded on\nspectral graph analysis, SGR seamlessly combines all aforementioned desirable\nproperties. In extensive experiments, we show how our approach works on large\ngraph collections, facilitates self-supervised representation learning across a\nvariety of application domains, and performs competitively to state-of-the-art\nmethods without re-training.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 08:50:34 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Tsitsulin", "Anton", ""], ["Mottin", "Davide", ""], ["Karras", "Panagiotis", ""], ["Bronstein", "Alex", ""], ["M\u00fcller", "Emmanuel", ""]]}, {"id": "1811.06250", "submitter": "Daniel Michelsanti", "authors": "Daniel Michelsanti, Zheng-Hua Tan, Sigurdur Sigurdsson, Jesper Jensen", "title": "Effects of Lombard Reflex on the Performance of Deep-Learning-Based\n  Audio-Visual Speech Enhancement Systems", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2019.8682713", "report-no": null, "categories": "eess.AS cs.LG cs.SD eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans tend to change their way of speaking when they are immersed in a noisy\nenvironment, a reflex known as Lombard effect. Current speech enhancement\nsystems based on deep learning do not usually take into account this change in\nthe speaking style, because they are trained with neutral (non-Lombard) speech\nutterances recorded under quiet conditions to which noise is artificially\nadded. In this paper, we investigate the effects that the Lombard reflex has on\nthe performance of audio-visual speech enhancement systems based on deep\nlearning. The results show that a gap in the performance of as much as\napproximately 5 dB between the systems trained on neutral speech and the ones\ntrained on Lombard speech exists. This indicates the benefit of taking into\naccount the mismatch between neutral and Lombard speech in the design of\naudio-visual speech enhancement systems.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 09:29:14 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Michelsanti", "Daniel", ""], ["Tan", "Zheng-Hua", ""], ["Sigurdsson", "Sigurdur", ""], ["Jensen", "Jesper", ""]]}, {"id": "1811.06272", "submitter": "Lars Buesing", "authors": "Lars Buesing, Theophane Weber, Yori Zwols, Sebastien Racaniere, Arthur\n  Guez, Jean-Baptiste Lespiau, Nicolas Heess", "title": "Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning policies on data synthesized by models can in principle quench the\nthirst of reinforcement learning algorithms for large amounts of real\nexperience, which is often costly to acquire. However, simulating plausible\nexperience de novo is a hard problem for many complex environments, often\nresulting in biases for model-based policy evaluation and search. Instead of de\nnovo synthesis of data, here we assume logged, real experience and model\nalternative outcomes of this experience under counterfactual actions, actions\nthat were not actually taken. Based on this, we propose the\nCounterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies\nin POMDPs from off-policy experience. It leverages structural causal models for\ncounterfactual evaluation of arbitrary policies on individual off-policy\nepisodes. CF-GPS can improve on vanilla model-based RL algorithms by making use\nof available logged data to de-bias model predictions. In contrast to\noff-policy algorithms based on Importance Sampling which re-weight data, CF-GPS\nleverages a model to explicitly consider alternative outcomes, allowing the\nalgorithm to make better use of experience data. We find empirically that these\nadvantages translate into improved policy evaluation and search results on a\nnon-trivial grid-world task. Finally, we show that CF-GPS generalizes the\npreviously proposed Guided Policy Search and that reparameterization-based\nalgorithms such Stochastic Value Gradient can be interpreted as counterfactual\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 10:08:58 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Buesing", "Lars", ""], ["Weber", "Theophane", ""], ["Zwols", "Yori", ""], ["Racaniere", "Sebastien", ""], ["Guez", "Arthur", ""], ["Lespiau", "Jean-Baptiste", ""], ["Heess", "Nicolas", ""]]}, {"id": "1811.06284", "submitter": "Guansong Lu", "authors": "Guansong Lu, Zhiming Zhou, Yuxuan Song, Kan Ren, Yong Yu", "title": "Guiding the One-to-one Mapping in CycleGAN via Optimal Transport", "comments": "The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CycleGAN is capable of learning a one-to-one mapping between two data\ndistributions without paired examples, achieving the task of unsupervised data\ntranslation. However, there is no theoretical guarantee on the property of the\nlearned one-to-one mapping in CycleGAN. In this paper, we experimentally find\nthat, under some circumstances, the one-to-one mapping learned by CycleGAN is\njust a random one within the large feasible solution space. Based on this\nobservation, we explore to add extra constraints such that the one-to-one\nmapping is controllable and satisfies more properties related to specific\ntasks. We propose to solve an optimal transport mapping restrained by a\ntask-specific cost function that reflects the desired properties, and use the\nbarycenters of optimal transport mapping to serve as references for CycleGAN.\nOur experiments indicate that the proposed algorithm is capable of learning a\none-to-one mapping with the desired properties.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 10:34:33 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Lu", "Guansong", ""], ["Zhou", "Zhiming", ""], ["Song", "Yuxuan", ""], ["Ren", "Kan", ""], ["Yu", "Yong", ""]]}, {"id": "1811.06341", "submitter": "Zahra Karevan", "authors": "Zahra Karevan and Johan A. K. Suykens", "title": "Spatio-temporal Stacked LSTM for Temperature Prediction in Weather\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory (LSTM) is a well-known method used widely on sequence\nlearning and time series prediction. In this paper we deployed stacked LSTM\nmodel in an application of weather forecasting. We propose a 2-layer\nspatio-temporal stacked LSTM model which consists of independent LSTM models\nper location in the first LSTM layer. Subsequently, the input of the second\nLSTM layer is formed based on the combination of the hidden states of the first\nlayer LSTM models. The experiments show that by utilizing the spatial\ninformation the prediction performance of the stacked LSTM model improves in\nmost of the cases.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 13:42:04 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Karevan", "Zahra", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1811.06349", "submitter": "Benjamin Epstein", "authors": "Benjamin Epstein (1) and Roy H. Olsson III (2) ((1) ECS Federal, (2)\n  Defense Advanced Research Projects Agency)", "title": "Physical Signal Classification Via Deep Neural Networks", "comments": "4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Deep Neural Network is applied to classify physical signatures obtained\nfrom physical sensor measurements of running gasoline and diesel-powered\nvehicles and other devices. The classification provides information on the\ntarget identities as to vehicle type and even vehicle model. The physical\nmeasurements include acoustic, acceleration (vibration), geophonic, and\nmagnetic.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 14:03:29 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Epstein", "Benjamin", ""], ["Olsson", "Roy H.", "III"]]}, {"id": "1811.06366", "submitter": "Samuel Bruno Da Silva Sousa", "authors": "Samuel Bruno da Silva Sousa, Ronaldo de Castro Del-Fiaco, and Lilian\n  Berton", "title": "Cluster analysis of homicide rates in the Brazilian state of Goias from\n  2002 to 2014", "comments": null, "journal-ref": "Proceedings of the 44th Latin American Computing Conference - Clei\n  2018. S\\~ao Paulo, Brazil (2018)", "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Homicide mortality is a worldwide concern and has occupied the agenda of\nresearchers and public managers. In Brazil, homicide is the third leading cause\nof death in the general population and the first in the 15-39 age group. In\nSouth America, Brazil has the third highest homicide mortality, behind\nVenezuela and Colombia. To measure the impacts of violence it is important to\nassess health systems and criminal justice, as well as other areas. In this\npaper, we analyze the spatial distribution of homicide mortality in the state\nof Goias, Center-West of Brazil, since the homicide rate increased from 24.5\nper 100,000 in 2002 to 42.6 per 100,000 in 2014 in this location. Moreover,\nthis state had the fifth position of homicides in Brazil in 2014. We considered\nsocio-demographic variables for the state, performed analysis about correlation\nand employed three clustering algorithms: K-means, Density-based and\nHierarchical. The results indicate the homicide rates are higher in cities\nneighbors of large urban centers, although these cities have the best\nsocioeconomic indicators.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 02:10:18 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 10:05:04 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Sousa", "Samuel Bruno da Silva", ""], ["Del-Fiaco", "Ronaldo de Castro", ""], ["Berton", "Lilian", ""]]}, {"id": "1811.06367", "submitter": "Duo Zhang", "authors": "Duo Zhang, Erlend Skullestad Holland, Geir Lindholm, Harsha Ratnaweera", "title": "Enhancing Operation of a Sewage Pumping Station for Inter Catchment\n  Wastewater Transfer by Using Deep Learning and Hydraulic Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel Inter Catchment Wastewater Transfer (ICWT) method\nfor mitigating sewer overflow. The ICWT aims at balancing the spatial mismatch\nof sewer flow and treatment capacity of Wastewater Treatment Plant (WWTP),\nthrough collaborative operation of sewer system facilities. Using a hydraulic\nmodel, the effectiveness of ICWT is investigated in a sewer system in Drammen,\nNorway. Concerning the whole system performance, we found that the S{\\o}ren\nLemmich pump station plays a vital role in the ICWT framework. To enhance the\noperation of this pump station, it is imperative to construct a multi-step\nahead water level prediction model. Hence, one of the most promising artificial\nintelligence techniques, Long Short Term Memory (LSTM), is employed to\nundertake this task. Experiments demonstrated that LSTM is superior to Gated\nRecurrent Unit (GRU), Recurrent Neural Network (RNN), Feed-forward Neural\nNetwork (FFNN) and Support Vector Regression (SVR).\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 12:28:53 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Zhang", "Duo", ""], ["Holland", "Erlend Skullestad", ""], ["Lindholm", "Geir", ""], ["Ratnaweera", "Harsha", ""]]}, {"id": "1811.06368", "submitter": "Duo Zhang", "authors": "Duo Zhang, Geir Lindholm, Harsha Ratnaweera", "title": "DeepCSO: Forecasting of Combined Sewer Overflow at a Citywide Level\n  using Multi-task Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combined Sewer Overflow (CSO) is a major problem to be addressed by many\ncities. Understanding the behavior of sewer system through proper urban\nhydrological models is an effective method of enhancing sewer system\nmanagement. Conventional deterministic methods, which heavily rely on physical\nprinciples, is inappropriate for real-time purpose due to their expensive\ncomputation. On the other hand, data-driven methods have gained huge interests,\nbut most studies only focus on modeling a single component of the sewer system\nand supply information at a very abstract level. In this paper, we proposed the\nDeepCSO model, which aims at forecasting CSO events from multiple CSO\nstructures simultaneously in near real time at a citywide level. The proposed\nmodel provided an intermediate methodology that combines the flexibility of\ndata-driven methods and the rich information contained in deterministic methods\nwhile avoiding the drawbacks of these two methods. A comparison of the results\ndemonstrated that the deep learning based multi-task model is superior to the\ntraditional methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 12:27:28 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Zhang", "Duo", ""], ["Lindholm", "Geir", ""], ["Ratnaweera", "Harsha", ""]]}, {"id": "1811.06369", "submitter": "Drahomira Herrmannova", "authors": "Martin Hlosta, Drahomira Herrmannova, Lucie Vachova, Jakub Kuzilek,\n  Zdenek Zdrahal, Annika Wolff", "title": "Modelling student online behaviour in a virtual learning environment", "comments": "In Proceedings of the 2014 Workshop on Learning Analytics and Machine\n  Learning at the 2014 International Conference on Learning Analytics and\n  Knowledge (LAK 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, distance education has enjoyed a major boom. Much work at\nThe Open University (OU) has focused on improving retention rates in these\nmodules by providing timely support to students who are at risk of failing the\nmodule. In this paper we explore methods for analysing student activity in\nonline virtual learning environment (VLE) -- General Unary Hypotheses Automaton\n(GUHA) and Markov chain-based analysis -- and we explain how this analysis can\nbe relevant for module tutors and other student support staff. We show that\nboth methods are a valid approach to modelling student activities. An advantage\nof the Markov chain-based approach is in its graphical output and in the\npossibility to model time dependencies of the student activities.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 16:31:04 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Hlosta", "Martin", ""], ["Herrmannova", "Drahomira", ""], ["Vachova", "Lucie", ""], ["Kuzilek", "Jakub", ""], ["Zdrahal", "Zdenek", ""], ["Wolff", "Annika", ""]]}, {"id": "1811.06396", "submitter": "Shuheng Shen", "authors": "Shuheng Shen, Linli Xu, Jingchang Liu, Junliang Guo and Qing Ling", "title": "Asynchronous Stochastic Composition Optimization with Variance Reduction", "comments": "30 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composition optimization has drawn a lot of attention in a wide variety of\nmachine learning domains from risk management to reinforcement learning.\nExisting methods solving the composition optimization problem often work in a\nsequential and single-machine manner, which limits their applications in\nlarge-scale problems. To address this issue, this paper proposes two\nasynchronous parallel variance reduced stochastic compositional gradient\n(AsyVRSC) algorithms that are suitable to handle large-scale data sets. The two\nalgorithms are AsyVRSC-Shared for the shared-memory architecture and\nAsyVRSC-Distributed for the master-worker architecture. The embedded variance\nreduction techniques enable the algorithms to achieve linear convergence rates.\nFurthermore, AsyVRSC-Shared and AsyVRSC-Distributed enjoy provable linear\nspeedup, when the time delays are bounded by the data dimensionality or the\nsparsity ratio of the partial gradients, respectively. Extensive experiments\nare conducted to verify the effectiveness of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 14:34:32 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Shen", "Shuheng", ""], ["Xu", "Linli", ""], ["Liu", "Jingchang", ""], ["Guo", "Junliang", ""], ["Ling", "Qing", ""]]}, {"id": "1811.06407", "submitter": "Bilal Piot", "authors": "Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Bernardo A.\n  Pires and R\\'emi Munos", "title": "Neural Predictive Belief Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised representation learning has succeeded with excellent results in\nmany applications. It is an especially powerful tool to learn a good\nrepresentation of environments with partial or noisy observations. In partially\nobservable domains it is important for the representation to encode a belief\nstate, a sufficient statistic of the observations seen so far. In this paper,\nwe investigate whether it is possible to learn such a belief representation\nusing modern neural architectures. Specifically, we focus on one-step frame\nprediction and two variants of contrastive predictive coding (CPC) as the\nobjective functions to learn the representations. To evaluate these learned\nrepresentations, we test how well they can predict various pieces of\ninformation about the underlying state of the environment, e.g., position of\nthe agent in a 3D maze. We show that all three methods are able to learn belief\nrepresentations of the environment, they encode not only the state information,\nbut also its uncertainty, a crucial aspect of belief states. We also find that\nfor CPC multi-step predictions and action-conditioning are critical for\naccurate belief representations in visually complex environments. The ability\nof neural representations to capture the belief information has the potential\nto spur new advances for learning and planning in partially observable domains,\nwhere leveraging uncertainty is essential for optimal decision making.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 14:51:12 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 15:56:57 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Guo", "Zhaohan Daniel", ""], ["Azar", "Mohammad Gheshlaghi", ""], ["Piot", "Bilal", ""], ["Pires", "Bernardo A.", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1811.06418", "submitter": "Ilya Razenshteyn", "authors": "S\\'ebastien Bubeck, Yin Tat Lee, Eric Price, Ilya Razenshteyn", "title": "Adversarial Examples from Cryptographic Pseudo-Random Generators", "comments": "4 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our recent work (Bubeck, Price, Razenshteyn, arXiv:1805.10204) we argued\nthat adversarial examples in machine learning might be due to an inherent\ncomputational hardness of the problem. More precisely, we constructed a binary\nclassification task for which (i) a robust classifier exists; yet no\nnon-trivial accuracy can be obtained with an efficient algorithm in (ii) the\nstatistical query model. In the present paper we significantly strengthen both\n(i) and (ii): we now construct a task which admits (i') a maximally robust\nclassifier (that is it can tolerate perturbations of size comparable to the\nsize of the examples themselves); and moreover we prove computational hardness\nof learning this task under (ii') a standard cryptographic assumption.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 15:08:12 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Lee", "Yin Tat", ""], ["Price", "Eric", ""], ["Razenshteyn", "Ilya", ""]]}, {"id": "1811.06419", "submitter": "Salimeh Yasaei Sekeh", "authors": "Salimeh Yasaei Sekeh, Brandon Oselio, Alfred O. Hero", "title": "Learning to Bound the Multi-class Bayes Error", "comments": "15 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of supervised learning, meta learning uses features, metadata\nand other information to learn about the difficulty, behavior, or composition\nof the problem. Using this knowledge can be useful to contextualize classifier\nresults or allow for targeted decisions about future data sampling. In this\npaper, we are specifically interested in learning the Bayes error rate (BER)\nbased on a labeled data sample. Providing a tight bound on the BER that is also\nfeasible to estimate has been a challenge. Previous work[1] has shown that a\npairwise bound based on the sum of Henze-Penrose (HP) divergence over label\npairs can be directly estimated using a sum of Friedman-Rafsky (FR)\nmultivariate run test statistics. However, in situations in which the dataset\nand number of classes are large, this bound is computationally infeasible to\ncalculate and may not be tight. Other multi-class bounds also suffer from\ncomputationally complex estimation procedures. In this paper, we present a\ngeneralized HP divergence measure that allows us to estimate the Bayes error\nrate with log-linear computation. We prove that the proposed bound is tighter\nthan both the pairwise method and a bound proposed by Lin [2]. We also\nempirically show that these bounds are close to the BER. We illustrate the\nproposed method on the MNIST dataset, and show its utility for the evaluation\nof feature reduction strategies. We further demonstrate an approach for\nevaluation of deep learning architectures using the proposed bounds.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 15:08:26 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 19:44:46 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 15:43:25 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Sekeh", "Salimeh Yasaei", ""], ["Oselio", "Brandon", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1811.06437", "submitter": "Pranshu Malviya", "authors": "Yash Pratyush Sinha, Pranshu Malviya, Minerva Panda, Syed Mohd Ali", "title": "Contextual Care Protocol using Neural Networks and Decision Trees", "comments": null, "journal-ref": "2018 Second International Conference on Advances in Electronics,\n  Computers and Communications (ICAECC)", "doi": "10.1109/ICAECC.2018.8479433", "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A contextual care protocol is used by a medical practitioner for patient\nhealthcare, given the context or situation that the specified patient is in.\nThis paper proposes a method to build an automated self-adapting protocol which\ncan help make relevant, early decisions for effective healthcare delivery. The\nhybrid model leverages neural networks and decision trees. The neural network\nestimates the chances of each disease and each tree in the decision trees\nrepresents care protocol for a disease. These trees are subject to change in\ncase of aberrations found by the diagnosticians. These corrections or\nprediction errors are clustered into similar groups for scalability and review\nby the experts. The corrections as suggested by the experts are incorporated\ninto the model.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 15:46:02 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Sinha", "Yash Pratyush", ""], ["Malviya", "Pranshu", ""], ["Panda", "Minerva", ""], ["Ali", "Syed Mohd", ""]]}, {"id": "1811.06471", "submitter": "Mark Ibrahim", "authors": "Ceena Modarres, Mark Ibrahim, Melissa Louie, John Paisley", "title": "Towards Explainable Deep Learning for Credit Lending: A Case Study", "comments": "Accepted NIPS 2018 FEAP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning adoption in the financial services industry has been limited\ndue to a lack of model interpretability. However, several techniques have been\nproposed to explain predictions made by a neural network. We provide an initial\ninvestigation into these techniques for the assessment of credit risk with\nneural networks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 17:03:59 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 21:16:03 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Modarres", "Ceena", ""], ["Ibrahim", "Mark", ""], ["Louie", "Melissa", ""], ["Paisley", "John", ""]]}, {"id": "1811.06477", "submitter": "Thomas Cherian", "authors": "Thomas Cherian, Akshay Badola and Vineet Padmanabhan", "title": "Multi-cell LSTM Based Neural Language Model", "comments": "7 pages including 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language models, being at the heart of many NLP problems, are always of great\ninterest to researchers. Neural language models come with the advantage of\ndistributed representations and long range contexts. With its particular\ndynamics that allow the cycling of information within the network, `Recurrent\nneural network' (RNN) becomes an ideal paradigm for neural language modeling.\nLong Short-Term Memory (LSTM) architecture solves the inadequacies of the\nstandard RNN in modeling long-range contexts. In spite of a plethora of RNN\nvariants, possibility to add multiple memory cells in LSTM nodes was seldom\nexplored. Here we propose a multi-cell node architecture for LSTMs and study\nits applicability for neural language modeling. The proposed multi-cell LSTM\nlanguage models outperform the state-of-the-art results on well-known Penn\nTreebank (PTB) setup.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 17:09:53 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Cherian", "Thomas", ""], ["Badola", "Akshay", ""], ["Padmanabhan", "Vineet", ""]]}, {"id": "1811.06488", "submitter": "Ezra Webb", "authors": "Ezra Webb, Cheng Lei, Chun-Jung Huang, Hirofumi Kobayashi, Hideharu\n  Mikami, Keisuke Goda", "title": "Exploring the Deep Feature Space of a Cell Classification Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present contemporary techniques for visualising the feature\nspace of a deep learning image classification neural network. These techniques\nare viewed in the context of a feed-forward network trained to classify low\nresolution fluorescence images of white blood cells captured using optofluidic\nimaging. The model has two output classes corresponding to two different cell\ntypes, which are often difficult to distinguish by eye. This paper has two\nmajor sections. The first looks to develop the information space presented by\ndimension reduction techniques, such as t-SNE, used to embed high-dimensional\npre-softmax layer activations into a two-dimensional plane. The second section\nlooks at feature visualisation by optimisation to generate feature images\nrepresenting the learned features of the network. Using and developing these\ntechniques we visualise class separation and structures within the dataset at\nvarious depths using clustering algorithms and feature images; track the\ndevelopment of feature complexity as we ascend the network; and begin to\nextract the features the network has learnt by modulating single-channel\nfeature images with up-scaled neuron activation maps to distinguish their most\nsalient parts.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 17:26:17 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Webb", "Ezra", ""], ["Lei", "Cheng", ""], ["Huang", "Chun-Jung", ""], ["Kobayashi", "Hirofumi", ""], ["Mikami", "Hideharu", ""], ["Goda", "Keisuke", ""]]}, {"id": "1811.06492", "submitter": "Zehao Dou", "authors": "Zehao Dou, Stanley J. Osher, and Bao Wang", "title": "Mathematical Analysis of Adversarial Attacks", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze efficacy of the fast gradient sign method (FGSM)\nand the Carlini-Wagner's L2 (CW-L2) attack. We prove that, within a certain\nregime, the untargeted FGSM can fool any convolutional neural nets (CNNs) with\nReLU activation; the targeted FGSM can mislead any CNNs with ReLU activation to\nclassify any given image into any prescribed class. For a special two-layer\nneural network: a linear layer followed by the softmax output activation, we\nshow that the CW-L2 attack increases the ratio of the classification\nprobability between the target and ground truth classes. Moreover, we provide\nnumerical results to verify all our theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 17:38:59 GMT"}, {"version": "v2", "created": "Sun, 25 Nov 2018 15:56:12 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Dou", "Zehao", ""], ["Osher", "Stanley J.", ""], ["Wang", "Bao", ""]]}, {"id": "1811.06497", "submitter": "Kunal Nagpal", "authors": "Kunal Nagpal, Davis Foote, Yun Liu, Po-Hsuan (Cameron) Chen, Ellery\n  Wulczyn, Fraser Tan, Niels Olson, Jenny L. Smith, Arash Mohtashamian, James\n  H. Wren, Greg S. Corrado, Robert MacDonald, Lily H. Peng, Mahul B. Amin,\n  Andrew J. Evans, Ankur R. Sangoi, Craig H. Mermel, Jason D. Hipp, Martin C.\n  Stumpe", "title": "Development and Validation of a Deep Learning Algorithm for Improving\n  Gleason Scoring of Prostate Cancer", "comments": null, "journal-ref": "Nature Partner Journal Digital Medicine (2019)", "doi": "10.1038/s41746-019-0112-2", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For prostate cancer patients, the Gleason score is one of the most important\nprognostic factors, potentially determining treatment independent of the stage.\nHowever, Gleason scoring is based on subjective microscopic examination of\ntumor morphology and suffers from poor reproducibility. Here we present a deep\nlearning system (DLS) for Gleason scoring whole-slide images of\nprostatectomies. Our system was developed using 112 million\npathologist-annotated image patches from 1,226 slides, and evaluated on an\nindependent validation dataset of 331 slides, where the reference standard was\nestablished by genitourinary specialist pathologists. On the validation\ndataset, the mean accuracy among 29 general pathologists was 0.61. The DLS\nachieved a significantly higher diagnostic accuracy of 0.70 (p=0.002) and\ntrended towards better patient risk stratification in correlations to clinical\nfollow-up data. Our approach could improve the accuracy of Gleason scoring and\nsubsequent therapy decisions, particularly where specialist expertise is\nunavailable. The DLS also goes beyond the current Gleason system to more finely\ncharacterize and quantitate tumor morphology, providing opportunities for\nrefinement of the Gleason system itself.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 17:49:50 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Nagpal", "Kunal", "", "Cameron"], ["Foote", "Davis", "", "Cameron"], ["Liu", "Yun", "", "Cameron"], ["Po-Hsuan", "", "", "Cameron"], ["Chen", "", ""], ["Wulczyn", "Ellery", ""], ["Tan", "Fraser", ""], ["Olson", "Niels", ""], ["Smith", "Jenny L.", ""], ["Mohtashamian", "Arash", ""], ["Wren", "James H.", ""], ["Corrado", "Greg S.", ""], ["MacDonald", "Robert", ""], ["Peng", "Lily H.", ""], ["Amin", "Mahul B.", ""], ["Evans", "Andrew J.", ""], ["Sangoi", "Ankur R.", ""], ["Mermel", "Craig H.", ""], ["Hipp", "Jason D.", ""], ["Stumpe", "Martin C.", ""]]}, {"id": "1811.06512", "submitter": "Reazul Hasan Russel", "authors": "Reazul Hasan Russel and Marek Petrik", "title": "Tight Bayesian Ambiguity Sets for Robust MDPs", "comments": "5 pages. Accepted at Infer to Control Workshop at Neural Information\n  Processing Systems (NIPS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness is important for sequential decision making in a stochastic\ndynamic environment with uncertain probabilistic parameters. We address the\nproblem of using robust MDPs (RMDPs) to compute policies with provable\nworst-case guarantees in reinforcement learning. The quality and robustness of\nan RMDP solution is determined by its ambiguity set. Existing methods construct\nambiguity sets that lead to impractically conservative solutions. In this\npaper, we propose RSVF, which achieves less conservative solutions with the\nsame worst-case guarantees by 1) leveraging a Bayesian prior, 2) optimizing the\nsize and location of the ambiguity set, and, most importantly, 3) relaxing the\nrequirement that the set is a confidence interval. Our theoretical analysis\nshows the safety of RSVF, and the empirical results demonstrate its practical\npromise.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 18:18:39 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Russel", "Reazul Hasan", ""], ["Petrik", "Marek", ""]]}, {"id": "1811.06521", "submitter": "Jan Leike", "authors": "Borja Ibarz and Jan Leike and Tobias Pohlen and Geoffrey Irving and\n  Shane Legg and Dario Amodei", "title": "Reward learning from human preferences and demonstrations in Atari", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To solve complex real-world problems with reinforcement learning, we cannot\nrely on manually specified reward functions. Instead, we can have humans\ncommunicate an objective to the agent directly. In this work, we combine two\napproaches to learning from human feedback: expert demonstrations and\ntrajectory preferences. We train a deep neural network to model the reward\nfunction and use its predicted reward to train an DQN-based deep reinforcement\nlearning agent on 9 Atari games. Our approach beats the imitation learning\nbaseline in 7 games and achieves strictly superhuman performance on 2 games\nwithout using game rewards. Additionally, we investigate the goodness of fit of\nthe reward model, present some reward hacking problems, and study the effects\nof noise in the human labels.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 18:33:43 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Ibarz", "Borja", ""], ["Leike", "Jan", ""], ["Pohlen", "Tobias", ""], ["Irving", "Geoffrey", ""], ["Legg", "Shane", ""], ["Amodei", "Dario", ""]]}, {"id": "1811.06524", "submitter": "Matthew Klawonn", "authors": "Matthew Klawonn, Eric Heim, James Hendler", "title": "Exploiting Class Learnability in Noisy Data", "comments": "Accepted to AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many domains, collecting sufficient labeled training data for supervised\nmachine learning requires easily accessible but noisy sources, such as\ncrowdsourcing services or tagged Web data. Noisy labels occur frequently in\ndata sets harvested via these means, sometimes resulting in entire classes of\ndata on which learned classifiers generalize poorly. For real world\napplications, we argue that it can be beneficial to avoid training on such\nclasses entirely. In this work, we aim to explore the classes in a given data\nset, and guide supervised training to spend time on a class proportional to its\nlearnability. By focusing the training process, we aim to improve model\ngeneralization on classes with a strong signal. To that end, we develop an\nonline algorithm that works in conjunction with classifier and training\nalgorithm, iteratively selecting training data for the classifier based on how\nwell it appears to generalize on each class. Testing our approach on a variety\nof data sets, we show our algorithm learns to focus on classes for which the\nmodel has low generalization error relative to strong baselines, yielding a\nclassifier with good performance on learnable classes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 18:42:30 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Klawonn", "Matthew", ""], ["Heim", "Eric", ""], ["Hendler", "James", ""]]}, {"id": "1811.06529", "submitter": "Tomasz Kornuta", "authors": "Vincent Marois and T.S. Jayram and Vincent Albouy and Tomasz Kornuta\n  and Younes Bouhadjar and Ahmet S. Ozcan", "title": "On transfer learning using a MAC model variant", "comments": "Paper accepted for Visually Grounded Interaction and Language (ViGIL)\n  Workshop, NIPS 2018, Montreeal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a variant of the MAC model (Hudson and Manning, ICLR 2018) with\na simplified set of equations that achieves comparable accuracy, while training\nfaster. We evaluate both models on CLEVR and CoGenT, and show that, transfer\nlearning with fine-tuning results in a 15 point increase in accuracy, matching\nthe state of the art. Finally, in contrast, we demonstrate that improper\nfine-tuning can actually reduce a model's accuracy as well.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 18:52:06 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 23:37:30 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Marois", "Vincent", ""], ["Jayram", "T. S.", ""], ["Albouy", "Vincent", ""], ["Kornuta", "Tomasz", ""], ["Bouhadjar", "Younes", ""], ["Ozcan", "Ahmet S.", ""]]}, {"id": "1811.06533", "submitter": "Siyu He", "authors": "Siyu He, Yin Li, Yu Feng, Shirley Ho, Siamak Ravanbakhsh, Wei Chen,\n  and Barnab\\'as P\\'oczos", "title": "Learning to Predict the Cosmological Structure Formation", "comments": "8 pages, 5 figures, 1 table", "journal-ref": "PNAS July 9, 2019 116 (28) 13825-13832", "doi": "10.1073/pnas.1821458116", "report-no": null, "categories": "astro-ph.CO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matter evolved under influence of gravity from minuscule density\nfluctuations. Non-perturbative structure formed hierarchically over all scales,\nand developed non-Gaussian features in the Universe, known as the Cosmic Web.\nTo fully understand the structure formation of the Universe is one of the holy\ngrails of modern astrophysics. Astrophysicists survey large volumes of the\nUniverse and employ a large ensemble of computer simulations to compare with\nthe observed data in order to extract the full information of our own Universe.\nHowever, to evolve trillions of galaxies over billions of years even with the\nsimplest physics is a daunting task. We build a deep neural network, the Deep\nDensity Displacement Model (hereafter D$^3$M), to predict the non-linear\nstructure formation of the Universe from simple linear perturbation theory. Our\nextensive analysis, demonstrates that D$^3$M outperforms the second order\nperturbation theory (hereafter 2LPT), the commonly used fast approximate\nsimulation method, in point-wise comparison, 2-point correlation, and 3-point\ncorrelation. We also show that D$^3$M is able to accurately extrapolate far\nbeyond its training data, and predict structure formation for significantly\ndifferent cosmological parameters. Our study proves, for the first time, that\ndeep learning is a practical and accurate alternative to approximate\nsimulations of the gravitational structure formation of the Universe.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 18:56:58 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 05:47:15 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["He", "Siyu", ""], ["Li", "Yin", ""], ["Feng", "Yu", ""], ["Ho", "Shirley", ""], ["Ravanbakhsh", "Siamak", ""], ["Chen", "Wei", ""], ["P\u00f3czos", "Barnab\u00e1s", ""]]}, {"id": "1811.06539", "submitter": "Jamie Hayes", "authors": "Jamie Hayes", "title": "A note on hyperparameters in black-box adversarial examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since Biggio et al. (2013) and Szegedy et al. (2013) first drew attention to\nadversarial examples, there has been a flood of research into defending and\nattacking machine learning models. However, almost all proposed attacks assume\nwhite-box access to a model. In other words, the attacker is assumed to have\nperfect knowledge of the models weights and architecture. With this insider\nknowledge, a white-box attack can leverage gradient information to craft\nadversarial examples. Black-box attacks assume no knowledge of the model\nweights or architecture. These attacks craft adversarial examples using\ninformation only contained in the logits or hard classification label. Here, we\nassume the attacker can use the logits in order to find an adversarial example.\nEmpirically, we show that 2-sided stochastic gradient estimation techniques are\nnot sensitive to scaling parameters, and can be used to mount powerful\nblack-box attacks requiring relatively few model queries.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 23:45:20 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Hayes", "Jamie", ""]]}, {"id": "1811.06569", "submitter": "Elizabeth Newman", "authors": "Elizabeth Newman, Lior Horesh, Haim Avron, Misha Kilmer", "title": "Stable Tensor Neural Networks for Rapid Deep Learning", "comments": "20 pages, 6 figures, submitted to SIMODS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a tensor neural network ($t$-NN) framework that offers an exciting\nnew paradigm for designing neural networks with multidimensional (tensor) data.\nOur network architecture is based on the $t$-product (Kilmer and Martin, 2011),\nan algebraic formulation to multiply tensors via circulant convolution. In this\n$t$-product algebra, we interpret tensors as $t$-linear operators analogous to\nmatrices as linear operators, and hence our framework inherits mimetic matrix\nproperties. To exemplify the elegant, matrix-mimetic algebraic structure of our\n$t$-NNs, we expand on recent work (Haber and Ruthotto, 2017) which interprets\ndeep neural networks as discretizations of non-linear differential equations\nand introduces stable neural networks which promote superior generalization.\nMotivated by this dynamic framework, we introduce a stable $t$-NN which\nfacilitates more rapid learning because of its reduced, more powerful\nparameterization. Through our high-dimensional design, we create a more compact\nparameter space and extract multidimensional correlations otherwise latent in\ntraditional algorithms. We further generalize our $t$-NN framework to a family\nof tensor-tensor products (Kernfeld, Kilmer, and Aeron, 2015) which still\ninduce a matrix-mimetic algebraic structure. Through numerical experiments on\nthe MNIST and CIFAR-10 datasets, we demonstrate the more powerful\nparameterizations and improved generalizability of stable $t$-NNs.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 19:37:24 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Newman", "Elizabeth", ""], ["Horesh", "Lior", ""], ["Avron", "Haim", ""], ["Kilmer", "Misha", ""]]}, {"id": "1811.06580", "submitter": "Weiwei Li", "authors": "Weiwei Li, Jan Hannig, Sayan Mukherjee", "title": "Subspace Clustering through Sub-Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of dimension reduction is of increasing importance in modern data\nanalysis. In this paper, we consider modeling the collection of points in a\nhigh dimensional space as a union of low dimensional subspaces. In particular\nwe propose a highly scalable sampling based algorithm that clusters the entire\ndata via first spectral clustering of a small random sample followed by\nclassifying or labeling the remaining out of sample points. The key idea is\nthat this random subset borrows information across the entire data set and that\nthe problem of clustering points can be replaced with the more efficient and\nrobust problem of \"clustering sub-clusters\". We provide theoretical guarantees\nfor our procedure. The numerical results indicate we outperform other\nstate-of-the-art subspace clustering algorithms with respect to accuracy and\nspeed.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 20:15:53 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 15:18:06 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Li", "Weiwei", ""], ["Hannig", "Jan", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1811.06588", "submitter": "Arno Solin", "authors": "Arno Solin, James Hensman, Richard E. Turner", "title": "Infinite-Horizon Gaussian Processes", "comments": "To appear in Advances in Neural Information Processing Systems (NIPS\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes provide a flexible framework for forecasting, removing\nnoise, and interpreting long temporal datasets. State space modelling (Kalman\nfiltering) enables these non-parametric models to be deployed on long datasets\nby reducing the complexity to linear in the number of data points. The\ncomplexity is still cubic in the state dimension $m$ which is an impediment to\npractical application. In certain special cases (Gaussian likelihood, regular\nspacing) the GP posterior will reach a steady posterior state when the data are\nvery long. We leverage this and formulate an inference scheme for GPs with\ngeneral likelihoods, where inference is based on single-sweep EP (assumed\ndensity filtering). The infinite-horizon model tackles the cubic cost in the\nstate dimensionality and reduces the cost in the state dimension $m$ to\n$\\mathcal{O}(m^2)$ per data point. The model is extended to online-learning of\nhyperparameters. We show examples for large finite-length modelling problems,\nand present how the method runs in real-time on a smartphone on a continuous\ndata stream updated at 100~Hz.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 20:52:40 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Solin", "Arno", ""], ["Hensman", "James", ""], ["Turner", "Richard E.", ""]]}, {"id": "1811.06603", "submitter": "Lin Chen", "authors": "Lin Chen, Moran Feldman, Amin Karbasi", "title": "Unconstrained Submodular Maximization with Constant Adaptive Complexity", "comments": "Authors are listed in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the unconstrained submodular maximization problem.\nWe propose the first algorithm for this problem that achieves a tight\n$(1/2-\\varepsilon)$-approximation guarantee using $\\tilde{O}(\\varepsilon^{-1})$\nadaptive rounds and a linear number of function evaluations. No previously\nknown algorithm for this problem achieves an approximation ratio better than\n$1/3$ using less than $\\Omega(n)$ rounds of adaptivity, where $n$ is the size\nof the ground set. Moreover, our algorithm easily extends to the maximization\nof a non-negative continuous DR-submodular function subject to a box constraint\nand achieves a tight $(1/2-\\varepsilon)$-approximation guarantee for this\nproblem while keeping the same adaptive and query complexities.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 21:56:33 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 01:39:47 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Chen", "Lin", ""], ["Feldman", "Moran", ""], ["Karbasi", "Amin", ""]]}, {"id": "1811.06609", "submitter": "Shivam Garg", "authors": "Shivam Garg, Vatsal Sharan, Brian Hu Zhang, Gregory Valiant", "title": "A Spectral View of Adversarially Robust Features", "comments": "To appear at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the apparent difficulty of learning models that are robust to\nadversarial perturbations, we propose tackling the simpler problem of\ndeveloping adversarially robust features. Specifically, given a dataset and\nmetric of interest, the goal is to return a function (or multiple functions)\nthat 1) is robust to adversarial perturbations, and 2) has significant\nvariation across the datapoints. We establish strong connections between\nadversarially robust features and a natural spectral property of the geometry\nof the dataset and metric of interest. This connection can be leveraged to\nprovide both robust features, and a lower bound on the robustness of any\nfunction that has significant variance across the dataset. Finally, we provide\nempirical evidence that the adversarially robust features given by this\nspectral approach can be fruitfully leveraged to learn a robust (and accurate)\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 22:09:28 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Garg", "Shivam", ""], ["Sharan", "Vatsal", ""], ["Zhang", "Brian Hu", ""], ["Valiant", "Gregory", ""]]}, {"id": "1811.06622", "submitter": "Daniel T Chang", "authors": "Daniel T. Chang", "title": "Concept-Oriented Deep Learning: Generative Concept Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative concept representations have three major advantages over\ndiscriminative ones: they can represent uncertainty, they support integration\nof learning and reasoning, and they are good for unsupervised and\nsemi-supervised learning. We discuss probabilistic and generative deep\nlearning, which generative concept representations are based on, and the use of\nvariational autoencoders and generative adversarial networks for learning\ngenerative concept representations, particularly for concepts whose data are\nsequences, structured data or graphs.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 23:13:26 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Chang", "Daniel T.", ""]]}, {"id": "1811.06626", "submitter": "Raksha Kumaraswamy", "authors": "Vincent Liu, Raksha Kumaraswamy, Lei Le, Martha White", "title": "The Utility of Sparse Representations for Control in Reinforcement\n  Learning", "comments": "Association for the Advancement of Artificial Intelligence 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate sparse representations for control in reinforcement learning.\nWhile these representations are widely used in computer vision, their\nprevalence in reinforcement learning is limited to sparse coding where\nextracting representations for new data can be computationally intensive. Here,\nwe begin by demonstrating that learning a control policy incrementally with a\nrepresentation from a standard neural network fails in classic control domains,\nwhereas learning with a representation obtained from a neural network that has\nsparsity properties enforced is effective. We provide evidence that the reason\nfor this is that the sparse representation provides locality, and so avoids\ncatastrophic interference, and particularly keeps consistent, stable values for\nbootstrapping. We then discuss how to learn such sparse representations. We\nexplore the idea of Distributional Regularizers, where the activation of hidden\nnodes is encouraged to match a particular distribution that results in sparse\nactivation across time. We identify a simple but effective way to obtain sparse\nrepresentations, not afforded by previously proposed strategies, making it more\npractical for further investigation into sparse representations for\nreinforcement learning.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 23:23:36 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Liu", "Vincent", ""], ["Kumaraswamy", "Raksha", ""], ["Le", "Lei", ""], ["White", "Martha", ""]]}, {"id": "1811.06629", "submitter": "Raksha Kumaraswamy", "authors": "Raksha Kumaraswamy, Matthew Schlegel, Adam White, Martha White", "title": "Context-Dependent Upper-Confidence Bounds for Directed Exploration", "comments": "Neural Information Processing Systems 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed exploration strategies for reinforcement learning are critical for\nlearning an optimal policy in a minimal number of interactions with the\nenvironment. Many algorithms use optimism to direct exploration, either through\nvisitation estimates or upper confidence bounds, as opposed to data-inefficient\nstrategies like \\epsilon-greedy that use random, undirected exploration. Most\ndata-efficient exploration methods require significant computation, typically\nrelying on a learned model to guide exploration. Least-squares methods have the\npotential to provide some of the data-efficiency benefits of model-based\napproaches -- because they summarize past interactions -- with the computation\ncloser to that of model-free approaches. In this work, we provide a novel,\ncomputationally efficient, incremental exploration strategy, leveraging this\nproperty of least-squares temporal difference learning (LSTD). We derive upper\nconfidence bounds on the action-values learned by LSTD, with context-dependent\n(or state-dependent) noise variance. Such context-dependent noise focuses\nexploration on a subset of variable states, and allows for reduced exploration\nin other states. We empirically demonstrate that our algorithm can converge\nmore quickly than other incremental exploration strategies using confidence\nestimates on action-values.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 23:43:56 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 16:42:14 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kumaraswamy", "Raksha", ""], ["Schlegel", "Matthew", ""], ["White", "Adam", ""], ["White", "Martha", ""]]}, {"id": "1811.06642", "submitter": "Thomas Beckers", "authors": "Thomas Beckers, Jonas Umlauft, Sandra Hirche", "title": "Mean Square Prediction Error of Misspecified Gaussian Process Models", "comments": "Please cite the conference paper (to be published in 2018 IEEE 57th\n  Annual Conference on Decision and Control (CDC))", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric modeling approaches show very promising results in the area of\nsystem identification and control. A naturally provided model confidence is\nhighly relevant for system-theoretical considerations to provide guarantees for\napplication scenarios. Gaussian process regression represents one approach\nwhich provides such an indicator for the model confidence. However, this\nmeasure is only valid if the covariance function and its hyperparameters fit\nthe underlying data generating process. In this paper, we derive an upper bound\nfor the mean square prediction error of misspecified Gaussian process models\nbased on a pseudo-concave optimization problem. We present application\nscenarios and a simulation to compare the derived upper bound with the true\nmean square error.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 01:24:01 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Beckers", "Thomas", ""], ["Umlauft", "Jonas", ""], ["Hirche", "Sandra", ""]]}, {"id": "1811.06665", "submitter": "Long Nguyen Msc", "authors": "Long Nguyen, Jia Zhen, Zhe Lin, Hanxiang Du, Zhou Yang, Wenxuan Guo,\n  Fang Jin", "title": "Spatial-temporal Multi-Task Learning for Within-field Cotton Yield\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and accurately predicting within-field spatial variability of\ncrop yield play a key role in site-specific management of crop inputs such as\nirrigation water and fertilizer for optimized crop production. However, such a\ntask is challenged by the complex interaction between crop growth and\nenvironmental and managerial factors, such as climate, soil conditions,\ntillage, and irrigation. In this paper, we present a novel Spatial-temporal\nMulti-Task Learning algorithms for within-field crop yield prediction in west\nTexas from 2001 to 2003. This algorithm integrates multiple heterogeneous data\nsources to learn different features simultaneously, and to aggregate\nspatial-temporal features by introducing a weighted regularizer to the loss\nfunctions. Our comprehensive experimental results consistently outperform the\nresults of other conventional methods, and suggest a promising approach, which\nimproves the landscape of crop prediction research fields.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 03:20:49 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Nguyen", "Long", ""], ["Zhen", "Jia", ""], ["Lin", "Zhe", ""], ["Du", "Hanxiang", ""], ["Yang", "Zhou", ""], ["Guo", "Wenxuan", ""], ["Jin", "Fang", ""]]}, {"id": "1811.06668", "submitter": "You Qiaoben", "authors": "You Qiaoben, Zheng Wang, Jianguo Li, Yinpeng Dong, Yu-Gang Jiang, Jun\n  Zhu", "title": "Composite Binary Decomposition Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary neural networks have great resource and computing efficiency, while\nsuffer from long training procedure and non-negligible accuracy drops, when\ncomparing to the full-precision counterparts. In this paper, we propose the\ncomposite binary decomposition networks (CBDNet), which first compose\nreal-valued tensor of each layer with a limited number of binary tensors, and\nthen decompose some conditioned binary tensors into two low-rank binary\ntensors, so that the number of parameters and operations are greatly reduced\ncomparing to the original ones. Experiments demonstrate the effectiveness of\nthe proposed method, as CBDNet can approximate image classification network\nResNet-18 using 5.25 bits, VGG-16 using 5.47 bits, DenseNet-121 using 5.72\nbits, object detection networks SSD300 using 4.38 bits, and semantic\nsegmentation networks SegNet using 5.18 bits, all with minor accuracy drops.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 03:29:34 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Qiaoben", "You", ""], ["Wang", "Zheng", ""], ["Li", "Jianguo", ""], ["Dong", "Yinpeng", ""], ["Jiang", "Yu-Gang", ""], ["Zhu", "Jun", ""]]}, {"id": "1811.06669", "submitter": "Jonathan Huang J", "authors": "Jonathan J Huang, Juan Jose Alvarado Leanos", "title": "AclNet: efficient end-to-end audio classification CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient end-to-end convolutional neural network architecture,\nAclNet, for audio classification. When trained with our data augmentation and\nregularization, we achieved state-of-the-art performance on the ESC-50 corpus\nwith 85:65% accuracy. Our network allows configurations such that memory and\ncompute requirements are drastically reduced, and a tradeoff analysis of\naccuracy and complexity is presented. The analysis shows high accuracy at\nsignificantly reduced computational complexity compared to existing solutions.\nFor example, a configuration with only 155k parameters and 49:3 million\nmultiply-adds per second is 81:75%, exceeding human accuracy of 81:3%. This\nimproved efficiency can enable always-on inference in energy-efficient\nplatforms.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 03:31:35 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Huang", "Jonathan J", ""], ["Leanos", "Juan Jose Alvarado", ""]]}, {"id": "1811.06672", "submitter": "Haruna Isah", "authors": "Sazia Mahfuz, Haruna Isah, Farhana Zulkernine, Peter Nicholls", "title": "Detecting Irregular Patterns in IoT Streaming Data for Fall Detection", "comments": "7 pages", "journal-ref": null, "doi": "10.1109/IEMCON.2018.8614822", "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting patterns in real time streaming data has been an interesting and\nchallenging data analytics problem. With the proliferation of a variety of\nsensor devices, real-time analytics of data from the Internet of Things (IoT)\nto learn regular and irregular patterns has become an important machine\nlearning problem to enable predictive analytics for automated notification and\ndecision support. In this work, we address the problem of learning an irregular\nhuman activity pattern, fall, from streaming IoT data from wearable sensors. We\npresent a deep neural network model for detecting fall based on accelerometer\ndata giving 98.75 percent accuracy using an online physical activity monitoring\ndataset called \"MobiAct\", which was published by Vavoulas et al. The initial\nmodel was developed using IBM Watson studio and then later transferred and\ndeployed on IBM Cloud with the streaming analytics service supported by IBM\nStreams for monitoring real-time IoT data. We also present the systems\narchitecture of the real-time fall detection framework that we intend to use\nwith mbientlabs wearable health monitoring sensors for real time patient\nmonitoring at retirement homes or rehabilitation clinics.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 03:59:22 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Mahfuz", "Sazia", ""], ["Isah", "Haruna", ""], ["Zulkernine", "Farhana", ""], ["Nicholls", "Peter", ""]]}, {"id": "1811.06692", "submitter": "Changho Shin", "authors": "Changho Shin, Sunghwan Joo, Jaeryun Yim, Hyoseop Lee, Taesup Moon,\n  Wonjong Rhee", "title": "Subtask Gated Networks for Non-Intrusive Load Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-intrusive load monitoring (NILM), also known as energy disaggregation, is\na blind source separation problem where a household's aggregate electricity\nconsumption is broken down into electricity usages of individual appliances. In\nthis way, the cost and trouble of installing many measurement devices over\nnumerous household appliances can be avoided, and only one device needs to be\ninstalled. The problem has been well-known since Hart's seminal paper in 1992,\nand recently significant performance improvements have been achieved by\nadopting deep networks. In this work, we focus on the idea that appliances have\non/off states, and develop a deep network for further performance improvements.\nSpecifically, we propose a subtask gated network that combines the main\nregression network with an on/off classification subtask network. Unlike\ntypical multitask learning algorithms where multiple tasks simply share the\nnetwork parameters to take advantage of the relevance among tasks, the subtask\ngated network multiply the main network's regression output with the subtask's\nclassification probability. When standby-power is additionally learned, the\nproposed solution surpasses the state-of-the-art performance for most of the\nbenchmark cases. The subtask gated network can be very effective for any\nproblem that inherently has on/off states.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 07:38:48 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Shin", "Changho", ""], ["Joo", "Sunghwan", ""], ["Yim", "Jaeryun", ""], ["Lee", "Hyoseop", ""], ["Moon", "Taesup", ""], ["Rhee", "Wonjong", ""]]}, {"id": "1811.06711", "submitter": "Takayuki Osa", "authors": "Takayuki Osa and Joni Pajarinen and Gerhard Neumann and J. Andrew\n  Bagnell and Pieter Abbeel and Jan Peters", "title": "An Algorithmic Perspective on Imitation Learning", "comments": "187 pages. Published in Foundations and Trends in Robotics", "journal-ref": null, "doi": "10.1561/2300000053", "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As robots and other intelligent agents move from simple environments and\nproblems to more complex, unstructured settings, manually programming their\nbehavior has become increasingly challenging and expensive. Often, it is easier\nfor a teacher to demonstrate a desired behavior rather than attempt to manually\nengineer it. This process of learning from demonstrations, and the study of\nalgorithms to do so, is called imitation learning. This work provides an\nintroduction to imitation learning. It covers the underlying assumptions,\napproaches, and how they relate; the rich set of algorithms developed to tackle\nthe problem; and advice on effective tools and implementation.\n  We intend this paper to serve two audiences. First, we want to familiarize\nmachine learning experts with the challenges of imitation learning,\nparticularly those arising in robotics, and the interesting theoretical and\npractical distinctions between it and more familiar frameworks like statistical\nsupervised learning theory and reinforcement learning. Second, we want to give\nroboticists and experts in applied artificial intelligence a broader\nappreciation for the frameworks and tools available for imitation learning.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 09:06:54 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Osa", "Takayuki", ""], ["Pajarinen", "Joni", ""], ["Neumann", "Gerhard", ""], ["Bagnell", "J. Andrew", ""], ["Abbeel", "Pieter", ""], ["Peters", "Jan", ""]]}, {"id": "1811.06736", "submitter": "Alon Cohen", "authors": "Alon Cohen, Moran Koren, Argyrios Deligkas", "title": "Incentivizing the Dynamic Workforce: Learning Contracts in the\n  Gig-Economy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG econ.TH stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In principal-agent models, a principal offers a contract to an agent to\nperform a certain task. The agent exerts a level of effort that maximizes her\nutility. The principal is oblivious to the agent's chosen level of effort, and\nconditions her wage only on possible outcomes. In this work, we consider a\nmodel in which the principal is unaware of the agent's utility and action\nspace. She sequentially offers contracts to identical agents, and observes the\nresulting outcomes. We present an algorithm for learning the optimal contract\nunder mild assumptions. We bound the number of samples needed for the principal\nobtain a contract that is within $\\epsilon$ of her optimal net profit for every\n$\\epsilon>0$.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 10:05:42 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Cohen", "Alon", ""], ["Koren", "Moran", ""], ["Deligkas", "Argyrios", ""]]}, {"id": "1811.06746", "submitter": "Chih-Hong Cheng", "authors": "Chih-Hong Cheng, Chung-Hao Huang, Georg N\\\"uhrenberg", "title": "nn-dependability-kit: Engineering Neural Networks for Safety-Critical\n  Autonomous Driving Systems", "comments": "Tool available at\n  https://github.com/dependable-ai/nn-dependability-kit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can engineering neural networks be approached in a disciplined way similar to\nhow engineers build software for civil aircraft? We present\nnn-dependability-kit, an open-source toolbox to support safety engineering of\nneural networks for autonomous driving systems. The rationale behind\nnn-dependability-kit is to consider a structured approach (via Goal Structuring\nNotation) to argue the quality of neural networks. In particular, the tool\nrealizes recent scientific results including (a) novel dependability metrics\nfor indicating sufficient elimination of uncertainties in the product life\ncycle, (b) formal reasoning engine for ensuring that the generalization does\nnot lead to undesired behaviors, and (c) runtime monitoring for reasoning\nwhether a decision of a neural network in operation is supported by prior\nsimilarities in the training data. A proprietary version of\nnn-dependability-kit has been used to improve the quality of a level-3\nautonomous driving component developed by Audi for highway maneuvers.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 10:48:07 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 20:02:25 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Cheng", "Chih-Hong", ""], ["Huang", "Chung-Hao", ""], ["N\u00fchrenberg", "Georg", ""]]}, {"id": "1811.06753", "submitter": "Tom Veniat", "authors": "Tom V\\'eniat, Olivier Schwander, Ludovic Denoyer", "title": "Stochastic Adaptive Neural Architecture Search for Keyword Spotting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of keyword spotting i.e. identifying keywords in a real-time\naudio stream is mainly solved by applying a neural network over successive\nsliding windows. Due to the difficulty of the task, baseline models are usually\nlarge, resulting in a high computational cost and energy consumption level. We\npropose a new method called SANAS (Stochastic Adaptive Neural Architecture\nSearch) which is able to adapt the architecture of the neural network\non-the-fly at inference time such that small architectures will be used when\nthe stream is easy to process (silence, low noise, ...) and bigger networks\nwill be used when the task becomes more difficult. We show that this adaptive\nmodel can be learned end-to-end by optimizing a trade-off between the\nprediction performance and the average computational cost per unit of time.\nExperiments on the Speech Commands dataset show that this approach leads to a\nhigh recognition level while being much faster (and/or energy saving) than\nclassical approaches where the network architecture is static.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 11:08:26 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["V\u00e9niat", "Tom", ""], ["Schwander", "Olivier", ""], ["Denoyer", "Ludovic", ""]]}, {"id": "1811.06763", "submitter": "Dong Liu", "authors": "Dong Liu, Minh Th\\`anh Vu, Saikat Chatterjee, and Lars K. Rasmussen", "title": "Entropy-regularized Optimal Transport Generative Models", "comments": null, "journal-ref": "ICASSP 2019", "doi": "10.1109/ICASSP.2019.8682721", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of entropy-regularized optimal transport (EOT) cost in\ndeveloping generative models to learn implicit distributions. Two generative\nmodels are proposed. One uses EOT cost directly in an one-shot optimization\nproblem and the other uses EOT cost iteratively in an adversarial game. The\nproposed generative models show improved performance over contemporary models\nfor image generation on MNSIT.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 11:33:12 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Liu", "Dong", ""], ["Vu", "Minh Th\u00e0nh", ""], ["Chatterjee", "Saikat", ""], ["Rasmussen", "Lars K.", ""]]}, {"id": "1811.06773", "submitter": "Ashkan Esmaeili", "authors": "Ashkan Esmaeili and Farokh Marvasti", "title": "A Novel Approach to Sparse Inverse Covariance Estimation Using Transform\n  Domain Updates and Exponentially Adaptive Thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Inverse Covariance Estimation (SICE) is useful in many practical data\nanalyses. Recovering the connectivity, non-connectivity graph of covariates is\nclassified amongst the most important data mining and learning problems. In\nthis paper, we introduce a novel SICE approach using adaptive thresholding. Our\nmethod is based on updates in a transformed domain of the desired matrix and\nexponentially decaying adaptive thresholding in the main domain (Inverse\nCovariance matrix domain). In addition to the proposed algorithm, the\nconvergence analysis is also provided. In the Numerical Experiments Section, we\nshow that the proposed method outperforms state-of-the-art methods in terms of\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 12:03:46 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 00:29:49 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Esmaeili", "Ashkan", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1811.06783", "submitter": "Hengyue Pan", "authors": "Hengyue Pan, Hui Jiang, Xin Niu and Yong Dou", "title": "DropFilter: A Novel Regularization Method for Learning Convolutional\n  Neural Networks", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have witnessed the fast development of different\nregularization methods for deep learning models such as fully-connected deep\nneural networks (DNNs) and Convolutional Neural Networks (CNNs). Most of\nprevious methods mainly consider to drop features from input data and hidden\nlayers, such as Dropout, Cutout and DropBlocks. DropConnect select to drop\nconnections between fully-connected layers. By randomly discard some features\nor connections, the above mentioned methods control the overfitting problem and\nimprove the performance of neural networks. In this paper, we proposed two\nnovel regularization methods, namely DropFilter and DropFilter-PLUS, for the\nlearning of CNNs. Different from the previous methods, DropFilter and\nDropFilter-PLUS selects to modify the convolution filters. For DropFilter-PLUS,\nwe find a suitable way to accelerate the learning process based on theoretical\nanalysis. Experimental results on MNIST show that using DropFilter and\nDropFilter-PLUS may improve performance on image classification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 12:40:39 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 01:28:42 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Pan", "Hengyue", ""], ["Jiang", "Hui", ""], ["Niu", "Xin", ""], ["Dou", "Yong", ""]]}, {"id": "1811.06802", "submitter": "Matteo Manica", "authors": "Ali Oskooei, Jannis Born, Matteo Manica, Vigneshwari Subramanian,\n  Julio S\\'aez-Rodr\\'iguez, Mar\\'ia Rodr\\'iguez Mart\\'inez", "title": "PaccMann: Prediction of anticancer compound sensitivity with multi-modal\n  attention-based neural networks", "comments": "10 pages, 5 figures, 2 tables. NIPS MLMM 2018", "journal-ref": "NeurIPS 2018 Workshop on Machine Learning for Molecules &\n  Materials", "doi": null, "report-no": null, "categories": "cs.LG q-bio.MN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for the prediction of anticancer compound\nsensitivity by means of multi-modal attention-based neural networks (PaccMann).\nIn our approach, we integrate three key pillars of drug sensitivity, namely,\nthe molecular structure of compounds, transcriptomic profiles of cancer cells\nas well as prior knowledge about interactions among proteins within cells. Our\nmodels ingest a drug-cell pair consisting of SMILES encoding of a compound and\nthe gene expression profile of a cancer cell and predicts an IC50 sensitivity\nvalue. Gene expression profiles are encoded using an attention-based encoding\nmechanism that assigns high weights to the most informative genes. We present\nand study three encoders for SMILES string of compounds: 1) bidirectional\nrecurrent 2) convolutional 3) attention-based encoders. We compare our devised\nmodels against a baseline model that ingests engineered fingerprints to\nrepresent the molecular structure. We demonstrate that using our\nattention-based encoders, we can surpass the baseline model. The use of\nattention-based encoders enhance interpretability and enable us to identify\ngenes, bonds and atoms that were used by the network to make a prediction.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 13:47:46 GMT"}, {"version": "v2", "created": "Sun, 14 Jul 2019 14:05:30 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Oskooei", "Ali", ""], ["Born", "Jannis", ""], ["Manica", "Matteo", ""], ["Subramanian", "Vigneshwari", ""], ["S\u00e1ez-Rodr\u00edguez", "Julio", ""], ["Mart\u00ednez", "Mar\u00eda Rodr\u00edguez", ""]]}, {"id": "1811.06805", "submitter": "Szymon Drgas", "authors": "Tomasz Grzywalski and Szymon Drgas", "title": "Using recurrences in time and frequency within U-net architecture for\n  speech enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When designing fully-convolutional neural network, there is a trade-off\nbetween receptive field size, number of parameters and spatial resolution of\nfeatures in deeper layers of the network. In this work we present a novel\nnetwork design based on combination of many convolutional and recurrent layers\nthat solves these dilemmas. We compare our solution with U-nets based models\nknown from the literature and other baseline models on speech enhancement task.\nWe test our solution on TIMIT speech utterances combined with noise segments\nextracted from NOISEX-92 database and show clear advantage of proposed solution\nin terms of SDR (signal-to-distortion ratio), SIR (signal-to-interference\nratio) and STOI (spectro-temporal objective intelligibility) metrics compared\nto the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 13:58:42 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Grzywalski", "Tomasz", ""], ["Drgas", "Szymon", ""]]}, {"id": "1811.06817", "submitter": "Rhiannon Michelmore", "authors": "Rhiannon Michelmore, Marta Kwiatkowska, Yarin Gal", "title": "Evaluating Uncertainty Quantification in End-to-End Autonomous Driving\n  Control", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rise in popularity of Deep Neural Networks (DNNs), attributed to more\npowerful GPUs and widely available datasets, has seen them being increasingly\nused within safety-critical domains. One such domain, self-driving, has\nbenefited from significant performance improvements, with millions of miles\nhaving been driven with no human intervention. Despite this, crashes and\nerroneous behaviours still occur, in part due to the complexity of verifying\nthe correctness of DNNs and a lack of safety guarantees.\n  In this paper, we demonstrate how quantitative measures of uncertainty can be\nextracted in real-time, and their quality evaluated in end-to-end controllers\nfor self-driving cars. To this end we utilise a recent method for gathering\napproximate uncertainty information from DNNs without changing the network's\narchitecture. We propose evaluation techniques for the uncertainty on two\nseparate architectures which use the uncertainty to predict crashes up to five\nseconds in advance. We find that mutual information, a measure of uncertainty\nin classification networks, is a promising indicator of forthcoming crashes.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 14:30:30 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Michelmore", "Rhiannon", ""], ["Kwiatkowska", "Marta", ""], ["Gal", "Yarin", ""]]}, {"id": "1811.06837", "submitter": "Zeyu Sun", "authors": "Zeyu Sun, Qihao Zhu, Lili Mou, Yingfei Xiong, Ge Li, Lu Zhang", "title": "A Grammar-Based Structural CNN Decoder for Code Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Code generation maps a program description to executable source code in a\nprogramming language. Existing approaches mainly rely on a recurrent neural\nnetwork (RNN) as the decoder. However, we find that a program contains\nsignificantly more tokens than a natural language sentence, and thus it may be\ninappropriate for RNN to capture such a long sequence. In this paper, we\npropose a grammar-based structural convolutional neural network (CNN) for code\ngeneration. Our model generates a program by predicting the grammar rules of\nthe programming language; we design several CNN modules, including the\ntree-based convolution and pre-order convolution, whose information is further\naggregated by dedicated attentive pooling layers. Experimental results on the\nHearthStone benchmark dataset show that our CNN code generator significantly\noutperforms the previous state-of-the-art method by 5 percentage points;\nadditional experiments on several semantic parsing tasks demonstrate the\nrobustness of our model. We also conduct in-depth ablation test to better\nunderstand each component of our model.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 14:45:35 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Sun", "Zeyu", ""], ["Zhu", "Qihao", ""], ["Mou", "Lili", ""], ["Xiong", "Yingfei", ""], ["Li", "Ge", ""], ["Zhang", "Lu", ""]]}, {"id": "1811.06838", "submitter": "Arin Chaudhuri", "authors": "Arin Chaudhuri, Carol Sadek, Deovrat Kakde, Wenhao Hu, Hansi Jiang,\n  Seunghyun Kong, Yuewei Liao, Sergiy Peredriy and Haoyu Wang", "title": "The Trace Criterion for Kernel Bandwidth Selection for Support Vector\n  Data Description", "comments": "note: some text overlap with arXiv:1708.05106 because common\n  background material is covered in both papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector data description (SVDD) is a popular anomaly detection\ntechnique. The SVDD classifier partitions the whole data space into an inlier\nregion, which consists of the region near the training data, and an outlier\nregion, which consists of points away from the training data. The computation\nof the SVDD classifier requires a kernel function, for which the Gaussian\nkernel is a common choice. The Gaussian kernel has a bandwidth parameter, and\nit is important to set the value of this parameter correctly for good results.\nA small bandwidth leads to overfitting such that the resulting SVDD classifier\noverestimates the number of anomalies, whereas a large bandwidth leads to\nunderfitting and an inability to detect many anomalies. In this paper, we\npresent a new unsupervised method for selecting the Gaussian kernel bandwidth.\nOur method exploits a low-rank representation of the kernel matrix to suggest a\nkernel bandwidth value. Our new technique is competitive with the current state\nof the art for low-dimensional data and performs extremely well for many\nclasses of high-dimensional data. Because the mathematical formulation of SVDD\nis identical with the mathematical formulation of one-class support vector\nmachines (OCSVM) when the Gaussian kernel is used, our method is equally\napplicable to Gaussian kernel bandwidth tuning for OCSVM.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 14:16:21 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 00:28:44 GMT"}, {"version": "v3", "created": "Wed, 5 Feb 2020 20:43:09 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Chaudhuri", "Arin", ""], ["Sadek", "Carol", ""], ["Kakde", "Deovrat", ""], ["Hu", "Wenhao", ""], ["Jiang", "Hansi", ""], ["Kong", "Seunghyun", ""], ["Liao", "Yuewei", ""], ["Peredriy", "Sergiy", ""], ["Wang", "Haoyu", ""]]}, {"id": "1811.06841", "submitter": "Lu Hang", "authors": "Hang Lu and Xin Wei and Ning Lin and Guihai Yan and and Xiaowei Li", "title": "Tetris: Re-architecting Convolutional Neural Network Computation for\n  Machine Learning Accelerators", "comments": null, "journal-ref": "ICCAD 2018 paper", "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference efficiency is the predominant consideration in designing deep\nlearning accelerators. Previous work mainly focuses on skipping zero values to\ndeal with remarkable ineffectual computation, while zero bits in non-zero\nvalues, as another major source of ineffectual computation, is often ignored.\nThe reason lies on the difficulty of extracting essential bits during operating\nmultiply-and-accumulate (MAC) in the processing element. Based on the fact that\nzero bits occupy as high as 68.9% fraction in the overall weights of modern\ndeep convolutional neural network models, this paper firstly proposes a weight\nkneading technique that could eliminate ineffectual computation caused by\neither zero value weights or zero bits in non-zero weights, simultaneously.\nBesides, a split-and-accumulate (SAC) computing pattern in replacement of\nconventional MAC, as well as the corresponding hardware accelerator design\ncalled Tetris are proposed to support weight kneading at the hardware level.\nExperimental results prove that Tetris could speed up inference up to 1.50x,\nand improve power efficiency up to 5.33x compared with the state-of-the-art\nbaselines.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 07:46:58 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Lu", "Hang", ""], ["Wei", "Xin", ""], ["Lin", "Ning", ""], ["Yan", "Guihai", ""], ["Li", "and Xiaowei", ""]]}, {"id": "1811.06846", "submitter": "Gabriel Dahia", "authors": "Gabriel Dahia, Maur\\'icio Pamplona Segundo", "title": "Improving Fingerprint Pore Detection with a Small FCN", "comments": "arXiv admin note: text overlap with arXiv:1809.10229", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate if previously proposed CNNs for fingerprint pore\ndetection overestimate the number of required model parameters for this task.\nWe show that this is indeed the case by proposing a fully convolutional neural\nnetwork that has significantly fewer parameters. We evaluate this model using a\nrigorous and reproducible protocol, which was, prior to our work, not available\nto the community. Using our protocol, we show that the proposed model, when\ncombined with post-processing, performs better than previous methods, albeit\nbeing much more efficient. All our code is available at\nhttps://github.com/gdahia/fingerprint-pore-detection\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 22:29:33 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Dahia", "Gabriel", ""], ["Segundo", "Maur\u00edcio Pamplona", ""]]}, {"id": "1811.06847", "submitter": "Karan Aggarwal", "authors": "Karan Aggarwal, Shafiq Joty, Luis Fernandez-Luque, Jaideep Srivastava", "title": "Adversarial Unsupervised Representation Learning for Activity\n  Time-Series", "comments": "Accepted at AAAI'19. arXiv admin note: text overlap with\n  arXiv:1712.09527", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sufficient physical activity and restful sleep play a major role in the\nprevention and cure of many chronic conditions. Being able to proactively\nscreen and monitor such chronic conditions would be a big step forward for\noverall health. The rapid increase in the popularity of wearable devices\nprovides a significant new source, making it possible to track the user's\nlifestyle real-time. In this paper, we propose a novel unsupervised\nrepresentation learning technique called activity2vec that learns and\n\"summarizes\" the discrete-valued activity time-series. It learns the\nrepresentations with three components: (i) the co-occurrence and magnitude of\nthe activity levels in a time-segment, (ii) neighboring context of the\ntime-segment, and (iii) promoting subject-invariance with adversarial training.\nWe evaluate our method on four disorder prediction tasks using linear\nclassifiers. Empirical evaluation demonstrates that our proposed method scales\nand performs better than many strong baselines. The adversarial regime helps\nimprove the generalizability of our representations by promoting subject\ninvariant features. We also show that using the representations at the level of\na day works the best since human activity is structured in terms of daily\nroutines\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 21:33:24 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Aggarwal", "Karan", ""], ["Joty", "Shafiq", ""], ["Fernandez-Luque", "Luis", ""], ["Srivastava", "Jaideep", ""]]}, {"id": "1811.06885", "submitter": "Fayyaz Minhas", "authors": "Amina Asif, Muhammad Dawood, and Fayyaz ul Amir Afsar Minhas", "title": "A Generalized Meta-loss function for regression and classification using\n  privileged information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning using privileged information (LUPI) is a powerful heterogenous\nfeature space machine learning framework that allows a machine learning model\nto learn from highly informative or privileged features which are available\nduring training only to generate test predictions using input space features\nwhich are available both during training and testing. LUPI can significantly\nimprove prediction performance in a variety of machine learning problems.\nHowever, existing large margin and neural network implementations of learning\nusing privileged information are mostly designed for classification tasks. In\nthis work, we have proposed a simple yet effective formulation that allows us\nto perform regression using privileged information through a custom loss\nfunction. Apart from regression, our formulation allows general application of\nLUPI to classification and other related problems as well. We have verified the\ncorrectness, applicability and effectiveness of our method on regression and\nclassification problems over different synthetic and real-world problems. To\ntest the usefulness of the proposed model in real-world problems, we have\nevaluated our method on the problem of protein binding affinity prediction. The\nproposed LUPI regression-based model has shown to outperform the current\nstate-of-the-art predictor.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 16:07:23 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 07:05:04 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Asif", "Amina", ""], ["Dawood", "Muhammad", ""], ["Minhas", "Fayyaz ul Amir Afsar", ""]]}, {"id": "1811.06889", "submitter": "Lisa Lee", "authors": "Maruan Al-Shedivat, Lisa Lee, Ruslan Salakhutdinov, Eric Xing", "title": "On the Complexity of Exploration in Goal-Driven Navigation", "comments": "Relational Representation Learning Workshop (NIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building agents that can explore their environments intelligently is a\nchallenging open problem. In this paper, we make a step towards understanding\nhow a hierarchical design of the agent's policy can affect its exploration\ncapabilities. First, we design EscapeRoom environments, where the agent must\nfigure out how to navigate to the exit by accomplishing a number of\nintermediate tasks (\\emph{subgoals}), such as finding keys or opening doors.\nOur environments are procedurally generated and vary in complexity, which can\nbe controlled by the number of subgoals and relationships between them. Next,\nwe propose to measure the complexity of each environment by constructing\ndependency graphs between the goals and analytically computing \\emph{hitting\ntimes} of a random walk in the graph. We empirically evaluate Proximal Policy\nOptimization (PPO) with sparse and shaped rewards, a variation of policy\nsketches, and a hierarchical version of PPO (called HiPPO) akin to h-DQN. We\nshow that analytically estimated \\emph{hitting time} in goal dependency graphs\nis an informative metric of the environment complexity. We conjecture that the\nresult should hold for environments other than navigation. Finally, we show\nthat solving environments beyond certain level of complexity requires\nhierarchical approaches.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 16:17:27 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Al-Shedivat", "Maruan", ""], ["Lee", "Lisa", ""], ["Salakhutdinov", "Ruslan", ""], ["Xing", "Eric", ""]]}, {"id": "1811.06912", "submitter": "Mengyue Hang", "authors": "Mengyue Hang, Ian Pytlarz and Jennifer Neville", "title": "Exploring Student Check-In Behavior for Improved Point-of-Interest\n  Prediction", "comments": "published in KDD'18", "journal-ref": null, "doi": "10.1145/3219819.3219902", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the availability of vast amounts of user visitation history on\nlocation-based social networks (LBSN), the problem of Point-of-Interest (POI)\nprediction has been extensively studied. However, much of the research has been\nconducted solely on voluntary checkin datasets collected from social apps such\nas Foursquare or Yelp. While these data contain rich information about\nrecreational activities (e.g., restaurants, nightlife, and entertainment),\ninformation about more prosaic aspects of people's lives is sparse. This not\nonly limits our understanding of users' daily routines, but more importantly\nthe modeling assumptions developed based on characteristics of recreation-based\ndata may not be suitable for richer check-in data. In this work, we present an\nanalysis of education \"check-in\" data using WiFi access logs collected at\nPurdue University. We propose a heterogeneous graph-based method to encode the\ncorrelations between users, POIs, and activities, and then jointly learn\nembeddings for the vertices. We evaluate our method compared to previous\nstate-of-the-art POI prediction methods, and show that the assumptions made by\nprevious methods significantly degrade performance on our data with dense(r)\nactivity signals. We also show how our learned embeddings could be used to\nidentify similar students (e.g., for friend suggestions).\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 21:07:18 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Hang", "Mengyue", ""], ["Pytlarz", "Ian", ""], ["Neville", "Jennifer", ""]]}, {"id": "1811.06930", "submitter": "Nicol\\`o Navarin", "authors": "Nicol\\`o Navarin, Dinh V. Tran, Alessandro Sperduti", "title": "Pre-training Graph Neural Networks with Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning techniques have been proposed in the last few years to\nprocess data represented in graph-structured form. Graphs can be used to model\nseveral scenarios, from molecules and materials to RNA secondary structures.\nSeveral kernel functions have been defined on graphs that coupled with\nkernelized learning algorithms, have shown state-of-the-art performances on\nmany tasks. Recently, several definitions of Neural Networks for Graph (GNNs)\nhave been proposed, but their accuracy is not yet satisfying. In this paper, we\npropose a task-independent pre-training methodology that allows a GNN to learn\nthe representation induced by state-of-the-art graph kernels. Then, the\nsupervised learning phase will fine-tune this representation for the task at\nhand. The proposed technique is agnostic on the adopted GNN architecture and\nkernel function, and shows consistent improvements in the predictive\nperformance of GNNs in our preliminary experimental results.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 17:24:58 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Navarin", "Nicol\u00f2", ""], ["Tran", "Dinh V.", ""], ["Sperduti", "Alessandro", ""]]}, {"id": "1811.06931", "submitter": "Sam Cole", "authors": "Sam Cole, Yizhe Zhu", "title": "Exact Recovery in the Hypergraph Stochastic Block Model: a Spectral\n  Algorithm", "comments": null, "journal-ref": "Linear Algebra and its Applications, Volume 593, 2020, Pages 45-73", "doi": "10.1016/j.laa.2020.01.039", "report-no": null, "categories": "cs.LG cs.DM math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the exact recovery problem in the hypergraph stochastic block\nmodel (HSBM) with $k$ blocks of equal size. More precisely, we consider a\nrandom $d$-uniform hypergraph $H$ with $n$ vertices partitioned into $k$\nclusters of size $s = n / k$. Hyperedges $e$ are added independently with\nprobability $p$ if $e$ is contained within a single cluster and $q$ otherwise,\nwhere $0 \\leq q < p \\leq 1$. We present a spectral algorithm which recovers the\nclusters exactly with high probability, given mild conditions on $n, k, p, q$,\nand $d$. Our algorithm is based on the adjacency matrix of $H$, which is a\nsymmetric $n \\times n$ matrix whose $(u, v)$-th entry is the number of\nhyperedges containing both $u$ and $v$. To the best of our knowledge, our\nalgorithm is the first to guarantee exact recovery when the number of clusters\n$k=\\Theta(\\sqrt{n})$.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 17:26:50 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 21:02:23 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2019 23:06:00 GMT"}, {"version": "v4", "created": "Sun, 2 Feb 2020 23:40:14 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Cole", "Sam", ""], ["Zhu", "Yizhe", ""]]}, {"id": "1811.06937", "submitter": "Yong Man Ro", "authors": "Wissam J. Baddar, Yong Man Ro", "title": "Mode Variational LSTM Robust to Unseen Modes of Variation: Application\n  to Facial Expression Recognition", "comments": "Accepted in AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal feature encoding is essential for encoding the dynamics in\nvideo sequences. Recurrent neural networks, particularly long short-term memory\n(LSTM) units, have been popular as an efficient tool for encoding\nspatio-temporal features in sequences. In this work, we investigate the effect\nof mode variations on the encoded spatio-temporal features using LSTMs. We show\nthat the LSTM retains information related to the mode variation in the\nsequence, which is irrelevant to the task at hand (e.g. classification facial\nexpressions). Actually, the LSTM forget mechanism is not robust enough to mode\nvariations and preserves information that could negatively affect the encoded\nspatio-temporal features. We propose the mode variational LSTM to encode\nspatio-temporal features robust to unseen modes of variation. The mode\nvariational LSTM modifies the original LSTM structure by adding an additional\ncell state that focuses on encoding the mode variation in the input sequence.\nTo efficiently regulate what features should be stored in the additional cell\nstate, additional gating functionality is also introduced. The effectiveness of\nthe proposed mode variational LSTM is verified using the facial expression\nrecognition task. Comparative experiments on publicly available datasets\nverified that the proposed mode variational LSTM outperforms existing methods.\nMoreover, a new dynamic facial expression dataset with different modes of\nvariation, including various modes like pose and illumination variations, was\ncollected to comprehensively evaluate the proposed mode variational LSTM.\nExperimental results verified that the proposed mode variational LSTM encodes\nspatio-temporal features robust to unseen modes of variation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 17:40:13 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Baddar", "Wissam J.", ""], ["Ro", "Yong Man", ""]]}, {"id": "1811.06969", "submitter": "Nicholas Frosst", "authors": "Nicholas Frosst, Sara Sabour, Geoffrey Hinton", "title": "DARCCC: Detecting Adversaries by Reconstruction from Class Conditional\n  Capsules", "comments": "To be presented at NIPS 2018 Workshop on Security in Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple technique that allows capsule models to detect\nadversarial images. In addition to being trained to classify images, the\ncapsule model is trained to reconstruct the images from the pose parameters and\nidentity of the correct top-level capsule. Adversarial images do not look like\na typical member of the predicted class and they have much larger\nreconstruction errors when the reconstruction is produced from the top-level\ncapsule for that class. We show that setting a threshold on the $l2$ distance\nbetween the input image and its reconstruction from the winning capsule is very\neffective at detecting adversarial images for three different datasets. The\nsame technique works quite well for CNNs that have been trained to reconstruct\nthe image from all or part of the last hidden layer before the softmax. We then\nexplore a stronger, white-box attack that takes the reconstruction error into\naccount. This attack is able to fool our detection technique but in order to\nmake the model change its prediction to another class, the attack must\ntypically make the \"adversarial\" image resemble images of the other class.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 18:52:58 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Frosst", "Nicholas", ""], ["Sabour", "Sara", ""], ["Hinton", "Geoffrey", ""]]}, {"id": "1811.06981", "submitter": "Oren Rippel", "authors": "Oren Rippel, Sanjay Nair, Carissa Lew, Steve Branson, Alexander G.\n  Anderson, Lubomir Bourdev", "title": "Learned Video Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for video coding, learned end-to-end for the\nlow-latency mode. In this setting, our approach outperforms all existing video\ncodecs across nearly the entire bitrate range. To our knowledge, this is the\nfirst ML-based method to do so.\n  We evaluate our approach on standard video compression test sets of varying\nresolutions, and benchmark against all mainstream commercial codecs, in the\nlow-latency mode. On standard-definition videos, relative to our algorithm,\nHEVC/H.265, AVC/H.264 and VP9 typically produce codes up to 60% larger. On\nhigh-definition 1080p videos, H.265 and VP9 typically produce codes up to 20%\nlarger, and H.264 up to 35% larger. Furthermore, our approach does not suffer\nfrom blocking artifacts and pixelation, and thus produces videos that are more\nvisually pleasing.\n  We propose two main contributions. The first is a novel architecture for\nvideo compression, which (1) generalizes motion estimation to perform any\nlearned compensation beyond simple translations, (2) rather than strictly\nrelying on previously transmitted reference frames, maintains a state of\narbitrary information learned by the model, and (3) enables jointly compressing\nall transmitted signals (such as optical flow and residual).\n  Secondly, we present a framework for ML-based spatial rate control: namely, a\nmechanism for assigning variable bitrates across space for each frame. This is\na critical component for video coding, which to our knowledge had not been\ndeveloped within a machine learning setting.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 17:29:51 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Rippel", "Oren", ""], ["Nair", "Sanjay", ""], ["Lew", "Carissa", ""], ["Branson", "Steve", ""], ["Anderson", "Alexander G.", ""], ["Bourdev", "Lubomir", ""]]}, {"id": "1811.06992", "submitter": "Chris Ying", "authors": "Chris Ying, Sameer Kumar, Dehao Chen, Tao Wang, Youlong Cheng", "title": "Image Classification at Supercomputer Scale", "comments": "Presented as part of Systems for ML Workshop @ NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning is extremely computationally intensive, and hardware vendors\nhave responded by building faster accelerators in large clusters. Training deep\nlearning models at petaFLOPS scale requires overcoming both algorithmic and\nsystems software challenges. In this paper, we discuss three systems-related\noptimizations: (1) distributed batch normalization to control per-replica batch\nsizes, (2) input pipeline optimizations to sustain model throughput, and (3)\n2-D torus all-reduce to speed up gradient summation. We combine these\noptimizations to train ResNet-50 on ImageNet to 76.3% accuracy in 2.2 minutes\non a 1024-chip TPU v3 Pod with a training throughput of over 1.05 million\nimages/second and no accuracy drop.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 19:01:40 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 01:30:42 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Ying", "Chris", ""], ["Kumar", "Sameer", ""], ["Chen", "Dehao", ""], ["Wang", "Tao", ""], ["Cheng", "Youlong", ""]]}, {"id": "1811.07004", "submitter": "Tom Schaul", "authors": "Tom Schaul, Hado van Hasselt, Joseph Modayil, Martha White, Adam\n  White, Pierre-Luc Bacon, Jean Harb, Shibl Mourad, Marc Bellemare, Doina\n  Precup", "title": "The Barbados 2018 List of Open Issues in Continual Learning", "comments": "NIPS Continual Learning Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We want to make progress toward artificial general intelligence, namely\ngeneral-purpose agents that autonomously learn how to competently act in\ncomplex environments. The purpose of this report is to sketch a research\noutline, share some of the most important open issues we are facing, and\nstimulate further discussion in the community. The content is based on some of\nour discussions during a week-long workshop held in Barbados in February 2018.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 19:41:42 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Schaul", "Tom", ""], ["van Hasselt", "Hado", ""], ["Modayil", "Joseph", ""], ["White", "Martha", ""], ["White", "Adam", ""], ["Bacon", "Pierre-Luc", ""], ["Harb", "Jean", ""], ["Mourad", "Shibl", ""], ["Bellemare", "Marc", ""], ["Precup", "Doina", ""]]}, {"id": "1811.07006", "submitter": "Melanie F. Pradier", "authors": "Melanie F. Pradier, Weiwei Pan, Jiayu Yao, Soumya Ghosh, Finale\n  Doshi-velez", "title": "Projected BNNs: Avoiding weight-space pathologies by learning latent\n  representations of neural network weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning systems get widely adopted for high-stake decisions,\nquantifying uncertainty over predictions becomes crucial. While modern neural\nnetworks are making remarkable gains in terms of predictive accuracy,\ncharacterizing uncertainty over the parameters of these models is challenging\nbecause of the high dimensionality and complex correlations of the network\nparameter space. This paper introduces a novel variational inference framework\nfor Bayesian neural networks that (1) encodes complex distributions in\nhigh-dimensional parameter space with representations in a low-dimensional\nlatent space, and (2) performs inference efficiently on the low-dimensional\nrepresentations. Across a large array of synthetic and real-world datasets, we\nshow that our method improves uncertainty characterization and model\ngeneralization when compared with methods that work directly in the parameter\nspace.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 19:51:43 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 04:19:46 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 02:18:59 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Pradier", "Melanie F.", ""], ["Pan", "Weiwei", ""], ["Yao", "Jiayu", ""], ["Ghosh", "Soumya", ""], ["Doshi-velez", "Finale", ""]]}, {"id": "1811.07017", "submitter": "Shagun Sodhani", "authors": "Shagun Sodhani, Sarath Chandar, Yoshua Bengio", "title": "Towards Training Recurrent Neural Networks for Lifelong Learning", "comments": "Accepted at Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catastrophic forgetting and capacity saturation are the central challenges of\nany parametric lifelong learning system. In this work, we study these\nchallenges in the context of sequential supervised learning with an emphasis on\nrecurrent neural networks. To evaluate the models in the lifelong learning\nsetting, we propose a curriculum-based, simple, and intuitive benchmark where\nthe models are trained on tasks with increasing levels of difficulty. To\nmeasure the impact of catastrophic forgetting, the model is tested on all the\nprevious tasks as it completes any task. As a step towards developing true\nlifelong learning systems, we unify Gradient Episodic Memory (a catastrophic\nforgetting alleviation approach) and Net2Net(a capacity expansion approach).\nBoth these models are proposed in the context of feedforward networks and we\nevaluate the feasibility of using them for recurrent networks. Evaluation on\nthe proposed benchmark shows that the unified model is more suitable than the\nconstituent models for lifelong learning setting.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 20:13:23 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2018 16:21:06 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 05:23:46 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Sodhani", "Shagun", ""], ["Chandar", "Sarath", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1811.07023", "submitter": "Kathleen Greene", "authors": "K. G. Greene", "title": "An Infinite Parade of Giraffes: Expressive Augmentation and Complexity\n  Layers for Cartoon Drawing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore creative image generation constrained by small\ndata. To partially automate the creation of cartoon sketches consistent with a\nspecific designer's style, where acquiring a very large original image set is\nimpossible or cost prohibitive, we exploit domain specific knowledge for a huge\nreduction in original image requirements, creating an effectively infinite\nnumber of cartoon giraffes from just nine original drawings. We introduce\n\"expressive augmentations\" for cartoon sketches, mathematical transformations\nthat create broad domain appropriate variation, far beyond the usual affine\ntransformations, and we show that chained GANs models trained on the temporal\nstages of drawing or \"complexity layers\" can effectively add character\nappropriate details and finish new drawings in the designer's style.\n  We discuss the application of these tools in design processes for textiles,\ngraphics, architectural elements and interior design.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 01:28:12 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Greene", "K. G.", ""]]}, {"id": "1811.07029", "submitter": "Hangyu Mao", "authors": "Hangyu Mao, Zhengchao Zhang, Zhen Xiao, and Zhibo Gong", "title": "Modelling the Dynamic Joint Policy of Teammates with Attention\n  Multi-agent DDPG", "comments": "Attention-based Multi-agent DDPG. Experimental results show that it\n  not only outperforms the state-of-the-art RL-based methods and rule-based\n  methods by a large margin, but also achieves better performance in terms of\n  scalability and robustness", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Modelling and exploiting teammates' policies in cooperative multi-agent\nsystems have long been an interest and also a big challenge for the\nreinforcement learning (RL) community. The interest lies in the fact that if\nthe agent knows the teammates' policies, it can adjust its own policy\naccordingly to arrive at proper cooperations; while the challenge is that the\nagents' policies are changing continuously due to they are learning\nconcurrently, which imposes difficulty to model the dynamic policies of\nteammates accurately. In this paper, we present \\emph{ATTention Multi-Agent\nDeep Deterministic Policy Gradient} (ATT-MADDPG) to address this challenge.\nATT-MADDPG extends DDPG, a single-agent actor-critic RL method, with two\nspecial designs. First, in order to model the teammates' policies, the agent\nshould get access to the observations and actions of teammates. ATT-MADDPG\nadopts a centralized critic to collect such information. Second, to model the\nteammates' policies using the collected information in an effective way,\nATT-MADDPG enhances the centralized critic with an attention mechanism. This\nattention mechanism introduces a special structure to explicitly model the\ndynamic joint policy of teammates, making sure that the collected information\ncan be processed efficiently. We evaluate ATT-MADDPG on both benchmark tasks\nand the real-world packet routing tasks. Experimental results show that it not\nonly outperforms the state-of-the-art RL-based methods and rule-based methods\nby a large margin, but also achieves better performance in terms of scalability\nand robustness.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 11:30:29 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Mao", "Hangyu", ""], ["Zhang", "Zhengchao", ""], ["Xiao", "Zhen", ""], ["Gong", "Zhibo", ""]]}, {"id": "1811.07042", "submitter": "Anton Belyy", "authors": "Mariia Seleznova, Anton Belyy, Aleksei Sholokhov", "title": "Towards Large-Scale Exploratory Search over Heterogeneous Sources", "comments": "5 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Since time immemorial, people have been looking for ways to organize\nscientific knowledge into some systems to facilitate search and discovery of\nnew ideas. The problem was partially solved in the pre-Internet era using\nlibrary classifications, but nowadays it is nearly impossible to classify all\nscientific and popular scientific knowledge manually. There is a clear gap\nbetween the diversity and the amount of data available on the Internet and the\nalgorithms for automatic structuring of such data. In our preliminary study, we\napproach the problem of knowledge discovery on web-scale data with diverse text\nsources and propose an algorithm to aggregate multiple collections into a\nsingle hierarchical topic model. We implement a web service named Rysearch to\ndemonstrate the concept of topical exploratory search and make it available\nonline.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 01:48:48 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 10:13:59 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Seleznova", "Mariia", ""], ["Belyy", "Anton", ""], ["Sholokhov", "Aleksei", ""]]}, {"id": "1811.07051", "submitter": "Doron Bergman", "authors": "Doron L. Bergman", "title": "Symmetry constrained machine learning", "comments": null, "journal-ref": "In: Bi Y., Bhatia R., Kapoor S. (eds) Intelligent Systems and\n  Applications. IntelliSys 2019. Advances in Intelligent Systems and Computing,\n  vol 1038. Springer, Cham", "doi": "10.1007/978-3-030-29513-4_37", "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry, a central concept in understanding the laws of nature, has been\nused for centuries in physics, mathematics, and chemistry, to help make\nmathematical models tractable. Yet, despite its power, symmetry has not been\nused extensively in machine learning, until rather recently. In this article we\nshow a general way to incorporate symmetries into machine learning models. We\ndemonstrate this with a detailed analysis on a rather simple real world machine\nlearning system - a neural network for classifying handwritten digits, lacking\nbias terms for every neuron. We demonstrate that ignoring symmetries can have\ndire over-fitting consequences, and that incorporating symmetry into the model\nreduces over-fitting, while at the same time reducing complexity, ultimately\nrequiring less training data, and taking less time and resources to train.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 22:25:10 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 16:25:30 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Bergman", "Doron L.", ""]]}, {"id": "1811.07054", "submitter": "Tianyu Zhang", "authors": "Tianyu Zhang, Liwei Zhang, Philip R.O. Payne, Fuhai Li", "title": "Synergistic Drug Combination Prediction by Integrating Multi-omics Data\n  in Deep Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drug resistance is still a major challenge in cancer therapy. Drug\ncombination is expected to overcome drug resistance. However, the number of\npossible drug combinations is enormous, and thus it is infeasible to\nexperimentally screen all effective drug combinations considering the limited\nresources. Therefore, computational models to predict and prioritize effective\ndrug combinations is important for combinatory therapy discovery in cancer. In\nthis study, we proposed a novel deep learning model, AuDNNsynergy, to\nprediction drug combinations by integrating multi-omics data and chemical\nstructure data. In specific, three autoencoders were trained using the gene\nexpression, copy number and genetic mutation data of all tumor samples from The\nCancer Genome Atlas. Then the physicochemical properties of drugs combined with\nthe output of the three autoencoders, characterizing the individual cancer\ncell-lines, were used as the input of a deep neural network that predicts the\nsynergy value of given pair-wise drug combinations against the specific cancer\ncell-lines. The comparison results showed the proposed AuDNNsynergy model\noutperforms four state-of-art approaches, namely DeepSynergy, Gradient Boosting\nMachines, Random Forests, and Elastic Nets. Moreover, we conducted the\ninterpretation analysis of the deep learning model to investigate potential\nvital genetic predictors and the underlying mechanism of synergistic drug\ncombinations on specific cancer cell-lines.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 22:40:06 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Zhang", "Tianyu", ""], ["Zhang", "Liwei", ""], ["Payne", "Philip R. O.", ""], ["Li", "Fuhai", ""]]}, {"id": "1811.07055", "submitter": "Anastasios Kyrillidis", "authors": "Vatsal Shah, Anastasios Kyrillidis, Sujay Sanghavi", "title": "Minimum weight norm models do not always generalize well for\n  over-parameterized problems", "comments": "This work is substituted by the paper in arXiv:2011.14066", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is substituted by the paper in arXiv:2011.14066.\n  Stochastic gradient descent is the de facto algorithm for training deep\nneural networks (DNNs). Despite its popularity, it still requires fine tuning\nin order to achieve its best performance. This has led to the development of\nadaptive methods, that claim automatic hyper-parameter optimization. Recently,\nresearchers have studied both algorithmic classes via toy examples: e.g., for\nover-parameterized linear regression, Wilson et. al. (2017) shows that, while\nSGD always converges to the minimum-norm solution, adaptive methods show no\nsuch inclination, leading to worse generalization capabilities. Our aim is to\nstudy this conjecture further. We empirically show that the minimum weight norm\nis not necessarily the proper gauge of good generalization in simplified\nscenaria, and different models found by adaptive methods could outperform plain\ngradient methods. In practical DNN settings, we observe that adaptive methods\ncan outperform SGD, with larger weight norm output models, but without\nnecessarily reducing the amount of tuning required.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 22:46:16 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 16:59:54 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2020 06:54:59 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Shah", "Vatsal", ""], ["Kyrillidis", "Anastasios", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1811.07056", "submitter": "Jiquan Ngiam", "authors": "Jiquan Ngiam and Daiyi Peng and Vijay Vasudevan and Simon Kornblith\n  and Quoc V. Le and Ruoming Pang", "title": "Domain Adaptive Transfer Learning with Specialist Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Transfer learning is a widely used method to build high performing computer\nvision models. In this paper, we study the efficacy of transfer learning by\nexamining how the choice of data impacts performance. We find that more\npre-training data does not always help, and transfer performance depends on a\njudicious choice of pre-training data. These findings are important given the\ncontinued increase in dataset sizes. We further propose domain adaptive\ntransfer learning, a simple and effective pre-training method using importance\nweights computed based on the target dataset. Our method to compute importance\nweights follow from ideas in domain adaptation, and we show a novel application\nto transfer learning. Our methods achieve state-of-the-art results on multiple\nfine-grained classification datasets and are well-suited for use in practice.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 22:52:27 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 22:09:00 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Ngiam", "Jiquan", ""], ["Peng", "Daiyi", ""], ["Vasudevan", "Vijay", ""], ["Kornblith", "Simon", ""], ["Le", "Quoc V.", ""], ["Pang", "Ruoming", ""]]}, {"id": "1811.07062", "submitter": "Vardan Papyan", "authors": "Vardan Papyan", "title": "The Full Spectrum of Deepnet Hessians at Scale: Dynamics with SGD\n  Training and Sample Size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply state-of-the-art tools in modern high-dimensional numerical linear\nalgebra to approximate efficiently the spectrum of the Hessian of modern\ndeepnets, with tens of millions of parameters, trained on real data. Our\nresults corroborate previous findings, based on small-scale networks, that the\nHessian exhibits \"spiked\" behavior, with several outliers isolated from a\ncontinuous bulk. We decompose the Hessian into different components and study\nthe dynamics with training and sample size of each term individually.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 23:22:37 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 01:12:42 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Papyan", "Vardan", ""]]}, {"id": "1811.07073", "submitter": "Arash Vahdat", "authors": "Mostafa S. Ibrahim, Arash Vahdat, Mani Ranjbar, William G. Macready", "title": "Semi-Supervised Semantic Image Segmentation with Self-correcting\n  Networks", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a large image dataset with high-quality object masks for semantic\nsegmentation is costly and time consuming. In this paper, we introduce a\nprincipled semi-supervised framework that only uses a small set of fully\nsupervised images (having semantic segmentation labels and box labels) and a\nset of images with only object bounding box labels (we call it the weak set).\nOur framework trains the primary segmentation model with the aid of an\nancillary model that generates initial segmentation labels for the weak set and\na self-correction module that improves the generated labels during training\nusing the increasingly accurate primary model. We introduce two variants of the\nself-correction module using either linear or convolutional functions.\nExperiments on the PASCAL VOC 2012 and Cityscape datasets show that our models\ntrained with a small fully supervised set perform similar to, or better than,\nmodels trained with a large fully supervised set while requiring ~7x less\nannotation effort.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 01:20:03 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 23:11:23 GMT"}, {"version": "v3", "created": "Wed, 26 Feb 2020 04:58:15 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Ibrahim", "Mostafa S.", ""], ["Vahdat", "Arash", ""], ["Ranjbar", "Mani", ""], ["Macready", "William G.", ""]]}, {"id": "1811.07078", "submitter": "Peixiang Zhong", "authors": "Peixiang Zhong, Di Wang, Chunyan Miao", "title": "An Affect-Rich Neural Conversational Model with Biased Attention and\n  Weighted Cross-Entropy Loss", "comments": "AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affect conveys important implicit information in human communication. Having\nthe capability to correctly express affect during human-machine conversations\nis one of the major milestones in artificial intelligence. In recent years,\nextensive research on open-domain neural conversational models has been\nconducted. However, embedding affect into such models is still under explored.\nIn this paper, we propose an end-to-end affect-rich open-domain neural\nconversational model that produces responses not only appropriate in syntax and\nsemantics, but also with rich affect. Our model extends the Seq2Seq model and\nadopts VAD (Valence, Arousal and Dominance) affective notations to embed each\nword with affects. In addition, our model considers the effect of negators and\nintensifiers via a novel affective attention mechanism, which biases attention\ntowards affect-rich words in input sentences. Lastly, we train our model with\nan affect-incorporated objective function to encourage the generation of\naffect-rich words in the output responses. Evaluations based on both perplexity\nand human evaluations show that our model outperforms the state-of-the-art\nbaseline model of comparable size in producing natural and affect-rich\nresponses.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 02:29:18 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Zhong", "Peixiang", ""], ["Wang", "Di", ""], ["Miao", "Chunyan", ""]]}, {"id": "1811.07103", "submitter": "Aydogan Ozcan", "authors": "Yichen Wu, Yilin Luo, Gunvant Chaudhari, Yair Rivenson, Ayfer Calis,\n  Kevin De Haan, Aydogan Ozcan", "title": "Cross-modality deep learning brings bright-field microscopy contrast to\n  holography", "comments": "3 pages", "journal-ref": "Light: Science & Applications (2019)", "doi": "10.1038/s41377-019-0139-9", "report-no": null, "categories": "cs.CV cs.LG physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning brings bright-field microscopy contrast to holographic images\nof a sample volume, bridging the volumetric imaging capability of holography\nwith the speckle- and artifact-free image contrast of bright-field incoherent\nmicroscopy.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 05:20:13 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Wu", "Yichen", ""], ["Luo", "Yilin", ""], ["Chaudhari", "Gunvant", ""], ["Rivenson", "Yair", ""], ["Calis", "Ayfer", ""], ["De Haan", "Kevin", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1811.07108", "submitter": "Hengbiao Yu", "authors": "Chengdong Feng, Zhenbang Chen, Weijiang Hong, Hengbiao Yu, Wei Dong,\n  Ji Wang", "title": "Boosting the Robustness Verification of DNN by Identifying the\n  Achilles's Heel", "comments": "12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network (DNN) is a widely used deep learning technique. How to\nensure the safety of DNN-based system is a critical problem for the research\nand application of DNN. Robustness is an important safety property of DNN.\nHowever, existing work of verifying DNN's robustness is time-consuming and hard\nto scale to large-scale DNNs. In this paper, we propose a boosting method for\nDNN robustness verification, aiming to find counter-examples earlier. Our\nobservation is DNN's different inputs have different possibilities of existing\ncounter-examples around them, and the input with a small difference between the\nlargest output value and the second largest output value tends to be the\nachilles's heel of the DNN. We have implemented our method and applied it on\nReluplex, a state-of-the-art DNN verification tool, and four DNN attacking\nmethods. The results of the extensive experiments on two benchmarks indicate\nthe effectiveness of our boosting method.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 06:33:10 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Feng", "Chengdong", ""], ["Chen", "Zhenbang", ""], ["Hong", "Weijiang", ""], ["Yu", "Hengbiao", ""], ["Dong", "Wei", ""], ["Wang", "Ji", ""]]}, {"id": "1811.07131", "submitter": "Sreejith Kallummil", "authors": "Sreejith Kallummil and Sheetal Kalyani", "title": "High SNR Consistent Compressive Sensing Without Signal and Noise\n  Statistics", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering the support of sparse vectors in underdetermined linear regression\nmodels, \\textit{aka}, compressive sensing is important in many signal\nprocessing applications. High SNR consistency (HSC), i.e., the ability of a\nsupport recovery technique to correctly identify the support with increasing\nsignal to noise ratio (SNR) is an increasingly popular criterion to qualify the\nhigh SNR optimality of support recovery techniques. The HSC results available\nin literature for support recovery techniques applicable to underdetermined\nlinear regression models like least absolute shrinkage and selection operator\n(LASSO), orthogonal matching pursuit (OMP) etc. assume \\textit{a priori}\nknowledge of noise variance or signal sparsity. However, both these parameters\nare unavailable in most practical applications. Further, it is extremely\ndifficult to estimate noise variance or signal sparsity in underdetermined\nregression models. This limits the utility of existing HSC results. In this\narticle, we propose two techniques, \\textit{viz.}, residual ratio minimization\n(RRM) and residual ratio thresholding with adaptation (RRTA) to operate OMP\nalgorithm without the \\textit{a priroi} knowledge of noise variance and signal\nsparsity and establish their HSC analytically and numerically. To the best of\nour knowledge, these are the first and only noise statistics oblivious\nalgorithms to report HSC in underdetermined regression models.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 08:58:06 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Kallummil", "Sreejith", ""], ["Kalyani", "Sheetal", ""]]}, {"id": "1811.07134", "submitter": "Ambedkar Dukkipati", "authors": "Rohith AP and Ambedkar Dukkipati and Gaurav Pandey", "title": "Deep Discriminative Learning for Unsupervised Domain Adaptation", "comments": "There are some issues with our code and experimentation, hence we are\n  withdrawing the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary objective of domain adaptation methods is to transfer knowledge\nfrom a source domain to a target domain that has similar but different data\ndistributions. Thus, in order to correctly classify the unlabeled target domain\nsamples, the standard approach is to learn a common representation for both\nsource and target domain, thereby indirectly addressing the problem of learning\na classifier in the target domain. However, such an approach does not address\nthe task of classification in the target domain directly. In contrast, we\npropose an approach that directly addresses the problem of learning a\nclassifier in the unlabeled target domain. In particular, we train a classifier\nto correctly classify the training samples while simultaneously classifying the\nsamples in the target domain in an unsupervised manner. The corresponding model\nis referred to as Discriminative Encoding for Domain Adaptation (DEDA). We show\nthat this simple approach for performing unsupervised domain adaptation is\nindeed quite powerful. Our method achieves state of the art results in\nunsupervised adaptation tasks on various image classification benchmarks. We\nalso obtained state of the art performance on domain adaptation in Amazon\nreviews sentiment classification dataset. We perform additional experiments\nwhen the source data has less labeled examples and also on zero-shot domain\nadaptation task where no target domain samples are used for training.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 09:41:01 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 12:42:48 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["AP", "Rohith", ""], ["Dukkipati", "Ambedkar", ""], ["Pandey", "Gaurav", ""]]}, {"id": "1811.07143", "submitter": "Iddo Drori", "authors": "Iddo Drori, Isht Dwivedi, Pranav Shrestha, Jeffrey Wan, Yueqi Wang,\n  Yunchu He, Anthony Mazza, Hugh Krogh-Freeman, Dimitri Leggas, Kendal\n  Sandridge, Linyong Nan, Kaveri Thakoor, Chinmay Joshi, Sonam Goenka, Chen\n  Keasar, Itsik Pe'er", "title": "High Quality Prediction of Protein Q8 Secondary Structure by Diverse\n  Neural Network Architectures", "comments": "NIPS 2018 Workshop on Machine Learning for Molecules and Materials,\n  10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of protein secondary structure prediction using a\ncommon task framework. This lead to the introduction of multiple ideas for\nneural architectures based on state of the art building blocks, used in this\ntask for the first time. We take a principled machine learning approach, which\nprovides genuine, unbiased performance measures, correcting longstanding errors\nin the application domain. We focus on the Q8 resolution of secondary\nstructure, an active area for continuously improving methods. We use an\nensemble of strong predictors to achieve accuracy of 70.7% (on the CB513 test\nset using the CB6133filtered training set). These results are statistically\nindistinguishable from those of the top existing predictors. In the spirit of\nreproducible research we make our data, models and code available, aiming to\nset a gold standard for purity of training and testing sets. Such good\npractices lower entry barriers to this domain and facilitate reproducible,\nextendable research.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 10:47:22 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Drori", "Iddo", ""], ["Dwivedi", "Isht", ""], ["Shrestha", "Pranav", ""], ["Wan", "Jeffrey", ""], ["Wang", "Yueqi", ""], ["He", "Yunchu", ""], ["Mazza", "Anthony", ""], ["Krogh-Freeman", "Hugh", ""], ["Leggas", "Dimitri", ""], ["Sandridge", "Kendal", ""], ["Nan", "Linyong", ""], ["Thakoor", "Kaveri", ""], ["Joshi", "Chinmay", ""], ["Goenka", "Sonam", ""], ["Keasar", "Chen", ""], ["Pe'er", "Itsik", ""]]}, {"id": "1811.07153", "submitter": "Anatoly Shusterman", "authors": "Anatoly Shusterman, Lachlan Kang, Yarden Haskal, Yosef Meltser,\n  Prateek Mittal, Yossi Oren, Yuval Yarom", "title": "Robust Website Fingerprinting Through the Cache Occupancy Channel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Website fingerprinting attacks, which use statistical analysis on network\ntraffic to compromise user privacy, have been shown to be effective even if the\ntraffic is sent over anonymity-preserving networks such as Tor. The classical\nattack model used to evaluate website fingerprinting attacks assumes an on-path\nadversary, who can observe all traffic traveling between the user's computer\nand the Tor network. In this work we investigate these attacks under a\ndifferent attack model, in which the adversary is capable of running a small\namount of unprivileged code on the target user's computer. Under this model,\nthe attacker can mount cache side-channel attacks, which exploit the effects of\ncontention on the CPU's cache, to identify the website being browsed. In an\nimportant special case of this attack model, a JavaScript attack is launched\nwhen the target user visits a website controlled by the attacker. The\neffectiveness of this attack scenario has never been systematically analyzed,\nespecially in the open-world model which assumes that the user is visiting a\nmix of both sensitive and non-sensitive sites. In this work we show that cache\nwebsite fingerprinting attacks in JavaScript are highly feasible, even when\nthey are run from highly restrictive environments, such as the Tor Browser.\nSpecifically, we use machine learning techniques to classify traces of cache\nactivity. Unlike prior works, which try to identify cache conflicts, our work\nmeasures the overall occupancy of the last-level cache. We show that our\napproach achieves high classification accuracy in both the open-world and the\nclosed-world models. We further show that our techniques are resilient both to\nnetwork-based defenses and to side-channel countermeasures introduced to modern\nbrowsers as a response to the Spectre attack.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 12:25:00 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 08:57:41 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2019 14:48:25 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Shusterman", "Anatoly", ""], ["Kang", "Lachlan", ""], ["Haskal", "Yarden", ""], ["Meltser", "Yosef", ""], ["Mittal", "Prateek", ""], ["Oren", "Yossi", ""], ["Yarom", "Yuval", ""]]}, {"id": "1811.07155", "submitter": "Salvador Garc\\'ia", "authors": "Jos\\'e-Ram\\'on Cano and Pedro Antonio Guti\\'errez and Bartosz Krawczyk\n  and Micha{\\l} Wo\\'zniak and Salvador Garc\\'ia", "title": "Monotonic classification: an overview on algorithms, performance\n  measures and data sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, knowledge discovery in databases is an essential step to identify\nvalid, novel and useful patterns for decision making. There are many real-world\nscenarios, such as bankruptcy prediction, option pricing or medical diagnosis,\nwhere the classification models to be learned need to fulfil restrictions of\nmonotonicity (i.e. the target class label should not decrease when input\nattributes values increase). For instance, it is rational to assume that a\nhigher debt ratio of a company should never result in a lower level of\nbankruptcy risk. Consequently, there is a growing interest from the data mining\nresearch community concerning monotonic predictive models. This paper aims to\npresent an overview about the literature in the field, analyzing existing\ntechniques and proposing a taxonomy of the algorithms based on the type of\nmodel generated. For each method, we review the quality metrics considered in\nthe evaluation and the different data sets and monotonic problems used in the\nanalysis. In this way, this paper serves as an overview of the research about\nmonotonic classification in specialized literature and can be used as a\nfunctional guide of the field.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 12:36:38 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Cano", "Jos\u00e9-Ram\u00f3n", ""], ["Guti\u00e9rrez", "Pedro Antonio", ""], ["Krawczyk", "Bartosz", ""], ["Wo\u017aniak", "Micha\u0142", ""], ["Garc\u00eda", "Salvador", ""]]}, {"id": "1811.07157", "submitter": "Gurkirt Singh", "authors": "Gurkirt Singh, Fabio Cuzzolin", "title": "Recurrent Convolutions for Causal 3D CNNs", "comments": "Workshop on Large Scale Holistic Video Understanding, ICCVW, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, three dimensional (3D) convolutional neural networks (CNNs) have\nemerged as dominant methods to capture spatiotemporal representations in\nvideos, by adding to pre-existing 2D CNNs a third, temporal dimension. Such 3D\nCNNs, however, are anti-causal (i.e., they exploit information from both the\npast and the future frames to produce feature representations, thus preventing\ntheir use in online settings), constrain the temporal reasoning horizon to the\nsize of the temporal convolution kernel, and are not temporal\nresolution-preserving for video sequence-to-sequence modelling, as, for\ninstance, in action detection. To address these serious limitations, here we\npresent a new 3D CNN architecture for the causal/online processing of videos.\n  Namely, we propose a novel Recurrent Convolutional Network (RCN), which\nrelies on recurrence to capture the temporal context across frames at each\nnetwork level. Our network decomposes 3D convolutions into (1) a 2D spatial\nconvolution component, and (2) an additional hidden state $1\\times 1$\nconvolution, applied across time. The hidden state at any time $t$ is assumed\nto depend on the hidden state at $t-1$ and on the current output of the spatial\nconvolution component. As a result, the proposed network: (i) produces causal\noutputs, (ii) provides flexible temporal reasoning, (iii) preserves temporal\nresolution. Our experiments on the large-scale large Kinetics and MultiThumos\ndatasets show that the proposed method performs comparably to anti-causal 3D\nCNNs, while being causal and using fewer parameters.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 13:07:30 GMT"}, {"version": "v2", "created": "Sat, 31 Aug 2019 09:28:25 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Singh", "Gurkirt", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "1811.07174", "submitter": "Samuel Gomes Fadel", "authors": "Samuel G. Fadel and Ricardo da S. Torres", "title": "Link Prediction in Dynamic Graphs for Recommendation", "comments": "Workshop on Relational Representation Learning (R2L), NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in employing neural networks on graph domains helped push the\nstate of the art in link prediction tasks, particularly in recommendation\nservices. However, the use of temporal contextual information, often modeled as\ndynamic graphs that encode the evolution of user-item relationships over time,\nhas been overlooked in link prediction problems. In this paper, we consider the\nhypothesis that leveraging such information enables models to make better\npredictions, proposing a new neural network approach for this. Our experiments,\nperformed on the widely used ML-100k and ML-1M datasets, show that our approach\nproduces better predictions in scenarios where the pattern of user-item\nrelationships change over time. In addition, they suggest that existing\napproaches are significantly impacted by those changes.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 14:56:13 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Fadel", "Samuel G.", ""], ["Torres", "Ricardo da S.", ""]]}, {"id": "1811.07192", "submitter": "Yichuan Zhang", "authors": "Yichuan Zhang", "title": "The Theory and Algorithm of Ergodic Inference", "comments": "Ergodic inference, statistical inference, probability theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Approximate inference algorithm is one of the fundamental research fields in\nmachine learning. The two dominant theoretical inference frameworks in machine\nlearning are variational inference (VI) and Markov chain Monte Carlo (MCMC).\nHowever, because of the fundamental limitation in the theory, it is very\nchallenging to improve existing VI and MCMC methods on both the computational\nscalability and statistical efficiency. To overcome this obstacle, we propose a\nnew theoretical inference framework called ergodic Inference based on the\nfundamental property of ergodic transformations. The key contribution of this\nwork is to establish the theoretical foundation of ergodic inference for the\ndevelopment of practical algorithms in future work.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 17:21:14 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Zhang", "Yichuan", ""]]}, {"id": "1811.07199", "submitter": "Vidhi Lalchand Miss", "authors": "Vidhi Lalchand, A.C. Faul", "title": "A Fast and Greedy Subset-of-Data (SoD) Scheme for Sparsification in\n  Gaussian processes", "comments": "38th International Workshop on Bayesian Inference and Maximum Entropy\n  Methods in Science and Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their standard form Gaussian processes (GPs) provide a powerful\nnon-parametric framework for regression and classificaton tasks. Their one\nlimiting property is their $\\mathcal{O}(N^{3})$ scaling where $N$ is the number\nof training data points. In this paper we present a framework for GP training\nwith sequential selection of training data points using an intuitive selection\nmetric. The greedy forward selection strategy is devised to target two factors\n- regions of high predictive uncertainty and underfit. Under this technique the\ncomplexity of GP training is reduced to $\\mathcal{O}(M^{3})$ where $(M \\ll N)$\nif $M$ data points (out of $N$) are eventually selected. The sequential nature\nof the algorithm circumvents the need to invert the covariance matrix of\ndimension $N \\times N$ and enables the use of favourable matrix inverse update\nidentities. We outline the algorithm and sequential updates to the posterior\nmean and variance. We demonstrate our method on selected one dimensional\nfunctions and show that the loss in accuracy due to using a subset of data\npoints is marginal compared to the computational gains.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 18:04:41 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 18:23:00 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Lalchand", "Vidhi", ""], ["Faul", "A. C.", ""]]}, {"id": "1811.07201", "submitter": "John Martin Jr", "authors": "John Martin, Brendan Englot", "title": "Recursive Sparse Pseudo-input Gaussian Process SARSA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of Gaussian Process (GP) methods for Temporal Difference learning\nhas shown promise for data-efficient model-free Reinforcement Learning. In this\npaper, we consider a recent variant of the GP-SARSA algorithm, called Sparse\nPseudo-input Gaussian Process SARSA (SPGP-SARSA), and derive recursive formulas\nfor its predictive moments. This extension promotes greater memory efficiency,\nsince previous computations can be reused and, interestingly, it provides a\ntechnique for updating value estimates on a multiple timescales\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 18:06:37 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Martin", "John", ""], ["Englot", "Brendan", ""]]}, {"id": "1811.07209", "submitter": "Stefan Webb", "authors": "Stefan Webb, Tom Rainforth, Yee Whye Teh, M. Pawan Kumar", "title": "A Statistical Approach to Assessing Neural Network Robustness", "comments": "To appear at the 7th International Conference on Learning\n  Representations (ICLR 2019), New Orleans", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to assessing the robustness of neural networks\nbased on estimating the proportion of inputs for which a property is violated.\nSpecifically, we estimate the probability of the event that the property is\nviolated under an input model. Our approach critically varies from the formal\nverification framework in that when the property can be violated, it provides\nan informative notion of how robust the network is, rather than just the\nconventional assertion that the network is not verifiable. Furthermore, it\nprovides an ability to scale to larger networks than formal verification\napproaches. Though the framework still provides a formal guarantee of\nsatisfiability whenever it successfully finds one or more violations, these\nadvantages do come at the cost of only providing a statistical estimate of\nunsatisfiability whenever no violation is found. Key to the practical success\nof our approach is an adaptation of multi-level splitting, a Monte Carlo\napproach for estimating the probability of rare events, to our statistical\nrobustness framework. We demonstrate that our approach is able to emulate\nformal verification procedures on benchmark problems, while scaling to larger\nnetworks and providing reliable additional information in the form of accurate\nestimates of the violation probability.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 19:13:58 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 14:30:50 GMT"}, {"version": "v3", "created": "Sun, 23 Dec 2018 21:42:11 GMT"}, {"version": "v4", "created": "Thu, 21 Feb 2019 22:29:24 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Webb", "Stefan", ""], ["Rainforth", "Tom", ""], ["Teh", "Yee Whye", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "1811.07211", "submitter": "Jacob Springer", "authors": "Jacob M. Springer, Charles S. Strauss, Austin M. Thresher, Edward Kim,\n  Garrett T. Kenyon", "title": "Classifiers Based on Deep Sparse Coding Architectures are Robust to Deep\n  Learning Transferable Examples", "comments": "8 pages, 8 figures, fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning has shown great success in recent years, researchers\nhave discovered a critical flaw where small, imperceptible changes in the input\nto the system can drastically change the output classification. These attacks\nare exploitable in nearly all of the existing deep learning classification\nframeworks. However, the susceptibility of deep sparse coding models to\nadversarial examples has not been examined. Here, we show that classifiers\nbased on a deep sparse coding model whose classification accuracy is\ncompetitive with a variety of deep neural network models are robust to\nadversarial examples that effectively fool those same deep learning models. We\ndemonstrate both quantitatively and qualitatively that the robustness of deep\nsparse coding models to adversarial examples arises from two key properties.\nFirst, because deep sparse coding models learn general features corresponding\nto generators of the dataset as a whole, rather than highly discriminative\nfeatures for distinguishing specific classes, the resulting classifiers are\nless dependent on idiosyncratic features that might be more easily exploited.\nSecond, because deep sparse coding models utilize fixed point attractor\ndynamics with top-down feedback, it is more difficult to find small changes to\nthe input that drive the resulting representations out of the correct attractor\nbasin.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 19:39:54 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 18:55:55 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Springer", "Jacob M.", ""], ["Strauss", "Charles S.", ""], ["Thresher", "Austin M.", ""], ["Kim", "Edward", ""], ["Kenyon", "Garrett T.", ""]]}, {"id": "1811.07214", "submitter": "Phaniteja S", "authors": "Meha Kaushik, Phaniteja S and K. Madhava Krishna", "title": "Parameter Sharing Reinforcement Learning Architecture for Multi Agent\n  Driving Behaviors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent learning provides a potential framework for learning and\nsimulating traffic behaviors. This paper proposes a novel architecture to learn\nmultiple driving behaviors in a traffic scenario. The proposed architecture can\nlearn multiple behaviors independently as well as simultaneously. We take\nadvantage of the homogeneity of agents and learn in a parameter sharing\nparadigm. To further speed up the training process asynchronous updates are\nemployed into the architecture. While learning different behaviors\nsimultaneously, the given framework was also able to learn cooperation between\nthe agents, without any explicit communication. We applied this framework to\nlearn two important behaviors in driving: 1) Lane-Keeping and 2) Over-Taking.\nResults indicate faster convergence and learning of a more generic behavior,\nthat is scalable to any number of agents. When compared the results with\nexisting approaches, our results indicate equal and even better performance in\nsome cases.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 19:51:34 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Kaushik", "Meha", ""], ["S", "Phaniteja", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "1811.07216", "submitter": "Alexandre Yahi", "authors": "Natalia Antropova, Andrew L. Beam, Brett K. Beaulieu-Jones, Irene\n  Chen, Corey Chivers, Adrian Dalca, Sam Finlayson, Madalina Fiterau, Jason\n  Alan Fries, Marzyeh Ghassemi, Mike Hughes, Bruno Jedynak, Jasvinder S.\n  Kandola, Matthew McDermott, Tristan Naumann, Peter Schulam, Farah Shamout,\n  Alexandre Yahi", "title": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume represents the accepted submissions from the Machine Learning for\nHealth (ML4H) workshop at the conference on Neural Information Processing\nSystems (NeurIPS) 2018, held on December 8, 2018 in Montreal, Canada.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 20:14:43 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 19:47:09 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Antropova", "Natalia", ""], ["Beam", "Andrew L.", ""], ["Beaulieu-Jones", "Brett K.", ""], ["Chen", "Irene", ""], ["Chivers", "Corey", ""], ["Dalca", "Adrian", ""], ["Finlayson", "Sam", ""], ["Fiterau", "Madalina", ""], ["Fries", "Jason Alan", ""], ["Ghassemi", "Marzyeh", ""], ["Hughes", "Mike", ""], ["Jedynak", "Bruno", ""], ["Kandola", "Jasvinder S.", ""], ["McDermott", "Matthew", ""], ["Naumann", "Tristan", ""], ["Schulam", "Peter", ""], ["Shamout", "Farah", ""], ["Yahi", "Alexandre", ""]]}, {"id": "1811.07234", "submitter": "Yao Wan", "authors": "Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu,\n  Philip S. Yu", "title": "Improving Automatic Source Code Summarization via Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code summarization provides a high level natural language description of the\nfunction performed by code, as it can benefit the software maintenance, code\ncategorization and retrieval. To the best of our knowledge, most\nstate-of-the-art approaches follow an encoder-decoder framework which encodes\nthe code into a hidden space and then decode it into natural language space,\nsuffering from two major drawbacks: a) Their encoders only consider the\nsequential content of code, ignoring the tree structure which is also critical\nfor the task of code summarization, b) Their decoders are typically trained to\npredict the next word by maximizing the likelihood of next ground-truth word\nwith previous ground-truth word given. However, it is expected to generate the\nentire sequence from scratch at test time. This discrepancy can cause an\n\\textit{exposure bias} issue, making the learnt decoder suboptimal. In this\npaper, we incorporate an abstract syntax tree structure as well as sequential\ncontent of code snippets into a deep reinforcement learning framework (i.e.,\nactor-critic network). The actor network provides the confidence of predicting\nthe next word according to current state. On the other hand, the critic network\nevaluates the reward value of all possible extensions of the current state and\ncan provide global guidance for explorations. We employ an advantage reward\ncomposed of BLEU metric to train both networks. Comprehensive experiments on a\nreal-world dataset show the effectiveness of our proposed model when compared\nwith some state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 22:21:12 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Wan", "Yao", ""], ["Zhao", "Zhou", ""], ["Yang", "Min", ""], ["Xu", "Guandong", ""], ["Ying", "Haochao", ""], ["Wu", "Jian", ""], ["Yu", "Philip S.", ""]]}, {"id": "1811.07240", "submitter": "Kyle Kastner", "authors": "Kyle Kastner, Jo\\~ao Felipe Santos, Yoshua Bengio, Aaron Courville", "title": "Representation Mixing for TTS Synthesis", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent character and phoneme-based parametric TTS systems using deep learning\nhave shown strong performance in natural speech generation. However, the choice\nbetween character or phoneme input can create serious limitations for practical\ndeployment, as direct control of pronunciation is crucial in certain cases. We\ndemonstrate a simple method for combining multiple types of linguistic\ninformation in a single encoder, named representation mixing, enabling flexible\nchoice between character, phoneme, or mixed representations during inference.\nExperiments and user studies on a public audiobook corpus show the efficacy of\nour approach.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 22:45:15 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 23:16:10 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Kastner", "Kyle", ""], ["Santos", "Jo\u00e3o Felipe", ""], ["Bengio", "Yoshua", ""], ["Courville", "Aaron", ""]]}, {"id": "1811.07245", "submitter": "Mike Gartrell", "authors": "Mike Gartrell, Elvis Dohmatob, Jon Alberdi", "title": "Deep Determinantal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) have attracted significant attention as\nan elegant model that is able to capture the balance between quality and\ndiversity within sets. DPPs are parameterized by a positive semi-definite\nkernel matrix. While DPPs have substantial expressive power, they are\nfundamentally limited by the parameterization of the kernel matrix and their\ninability to capture nonlinear interactions between items within sets. We\npresent the deep DPP model as way to address these limitations, by using a deep\nfeed-forward neural network to learn the kernel matrix. In addition to allowing\nus to capture nonlinear item interactions, the deep DPP also allows easy\nincorporation of item metadata into DPP learning. Since the learning target is\nthe DPP kernel matrix, the deep DPP allows us to use existing DPP algorithms\nfor efficient learning, sampling, and prediction. Through an evaluation on\nseveral real-world datasets, we show experimentally that the deep DPP can\nprovide a considerable improvement in the predictive performance of DPPs, while\nalso outperforming strong baseline models in many cases.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 23:22:51 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 23:54:12 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 14:50:12 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Gartrell", "Mike", ""], ["Dohmatob", "Elvis", ""], ["Alberdi", "Jon", ""]]}, {"id": "1811.07249", "submitter": "Srikrishna Karanam", "authors": "Georgios Georgakis, Srikrishna Karanam, Ziyan Wu, Jana Kosecka", "title": "Learning Local RGB-to-CAD Correspondences for Object Pose Estimation", "comments": "10 pages, 6 figures, 4 tables, ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of 3D object pose estimation. While much recent work\nhas focused on the RGB domain, the reliance on accurately annotated images\nlimits their generalizability and scalability. On the other hand, the easily\navailable CAD models of objects are rich sources of data, providing a large\nnumber of synthetically rendered images. In this paper, we solve this key\nproblem of existing methods requiring expensive 3D pose annotations by\nproposing a new method that matches RGB images to CAD models for object pose\nestimation. Our key innovations compared to existing work include removing the\nneed for either real-world textures for CAD models or explicit 3D pose\nannotations for RGB images. We achieve this through a series of objectives that\nlearn how to select keypoints and enforce viewpoint and modality invariance\nacross RGB images and CAD model renderings. We conduct extensive experiments to\ndemonstrate that the proposed method can reliably estimate object pose in RGB\nimages, as well as generalize to object instances not seen during training.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 00:32:58 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 21:45:13 GMT"}, {"version": "v3", "created": "Mon, 6 May 2019 15:29:30 GMT"}, {"version": "v4", "created": "Wed, 31 Jul 2019 14:12:22 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Georgakis", "Georgios", ""], ["Karanam", "Srikrishna", ""], ["Wu", "Ziyan", ""], ["Kosecka", "Jana", ""]]}, {"id": "1811.07253", "submitter": "Yijun Xiao", "authors": "Yijun Xiao, William Yang Wang", "title": "Quantifying Uncertainties in Natural Language Processing Tasks", "comments": "To appear at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable uncertainty quantification is a first step towards building\nexplainable, transparent, and accountable artificial intelligent systems.\nRecent progress in Bayesian deep learning has made such quantification\nrealizable. In this paper, we propose novel methods to study the benefits of\ncharacterizing model and data uncertainties for natural language processing\n(NLP) tasks. With empirical experiments on sentiment analysis, named entity\nrecognition, and language modeling using convolutional and recurrent neural\nnetwork models, we show that explicitly modeling uncertainties is not only\nnecessary to measure output confidence levels, but also useful at enhancing\nmodel performances in various NLP tasks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 01:36:05 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Xiao", "Yijun", ""], ["Wang", "William Yang", ""]]}, {"id": "1811.07255", "submitter": "James Foulds", "authors": "James Foulds, Rashidul Islam, Kamrun Keya, Shimei Pan", "title": "Bayesian Modeling of Intersectional Fairness: The Variance of Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intersectionality is a framework that analyzes how interlocking systems of\npower and oppression affect individuals along overlapping dimensions including\nrace, gender, sexual orientation, class, and disability. Intersectionality\ntheory therefore implies it is important that fairness in artificial\nintelligence systems be protected with regard to multi-dimensional protected\nattributes. However, the measurement of fairness becomes statistically\nchallenging in the multi-dimensional setting due to data sparsity, which\nincreases rapidly in the number of dimensions, and in the values per dimension.\nWe present a Bayesian probabilistic modeling approach for the reliable,\ndata-efficient estimation of fairness with multi-dimensional protected\nattributes, which we apply to two existing intersectional fairness metrics.\nExperimental results on census data and the COMPAS criminal justice recidivism\ndataset demonstrate the utility of our methodology, and show that Bayesian\nmethods are valuable for the modeling and measurement of fairness in an\nintersectional context.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 01:54:24 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 16:58:05 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Foulds", "James", ""], ["Islam", "Rashidul", ""], ["Keya", "Kamrun", ""], ["Pan", "Shimei", ""]]}, {"id": "1811.07267", "submitter": "Francesco Fusco", "authors": "Francesco Fusco", "title": "Probabilistic Graphs for Sensor Data-driven Modelling of Power Systems\n  at Scale", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-04303-2_4", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing complexity of the power grid, driven by increasing share of\ndistributed energy resources and by massive deployment of intelligent\ninternet-connected devices, requires new modelling tools for planning and\noperation. Physics-based state estimation models currently used for data\nfiltering, prediction and anomaly detection are hard to maintain and adapt to\nthe ever-changing complex dynamics of the power system. A data-driven approach\nbased on probabilistic graphs is proposed, where custom non-linear, localised\nmodels of the joint density of subset of system variables can be combined to\nmodel arbitrarily large and complex systems. The graphical model allows to\nnaturally embed domain knowledge in the form of variables dependency structure\nor local quantitative relationships. A specific instance where neural-network\nmodels are used to represent the local joint densities is proposed, although\nthe methodology generalises to other model classes. Accuracy and scalability\nare evaluated on a large-scale data set representative of the European\ntransmission grid.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 04:00:32 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Fusco", "Francesco", ""]]}, {"id": "1811.07275", "submitter": "Aaditya Prakash", "authors": "Aaditya Prakash, James Storer, Dinei Florencio, Cha Zhang", "title": "RePr: Improved Training of Convolutional Filters", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-trained Convolutional Neural Network can easily be pruned without\nsignificant loss of performance. This is because of unnecessary overlap in the\nfeatures captured by the network's filters. Innovations in network architecture\nsuch as skip/dense connections and Inception units have mitigated this problem\nto some extent, but these improvements come with increased computation and\nmemory requirements at run-time. We attempt to address this problem from\nanother angle - not by changing the network structure but by altering the\ntraining method. We show that by temporarily pruning and then restoring a\nsubset of the model's filters, and repeating this process cyclically, overlap\nin the learned features is reduced, producing improved generalization. We show\nthat the existing model-pruning criteria are not optimal for selecting filters\nto prune in this context and introduce inter-filter orthogonality as the\nranking criteria to determine under-expressive filters. Our method is\napplicable both to vanilla convolutional networks and more complex modern\narchitectures, and improves the performance across a variety of tasks,\nespecially when applied to smaller networks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 05:15:27 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 04:10:34 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 06:04:16 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Prakash", "Aaditya", ""], ["Storer", "James", ""], ["Florencio", "Dinei", ""], ["Zhang", "Cha", ""]]}, {"id": "1811.07279", "submitter": "Kyubin Lee", "authors": "Kyubin Lee, Akshay Sood, Mark Craven", "title": "Understanding Learned Models by Identifying Important Features at the\n  Right Resolution", "comments": "First two authors contributed equally to this work, Accepted for\n  presentation at the Thirty-Third AAAI Conference on Artificial Intelligence\n  (AAAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many application domains, it is important to characterize how complex\nlearned models make their decisions across the distribution of instances. One\nway to do this is to identify the features and interactions among them that\ncontribute to a model's predictive accuracy. We present a model-agnostic\napproach to this task that makes the following specific contributions. Our\napproach (i) tests feature groups, in addition to base features, and tries to\ndetermine the level of resolution at which important features can be\ndetermined, (ii) uses hypothesis testing to rigorously assess the effect of\neach feature on the model's loss, (iii) employs a hierarchical approach to\ncontrol the false discovery rate when testing feature groups and individual\nbase features for importance, and (iv) uses hypothesis testing to identify\nimportant interactions among features and feature groups. We evaluate our\napproach by analyzing random forest and LSTM neural network models learned in\ntwo challenging biomedical applications.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 05:30:21 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 23:30:46 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Lee", "Kyubin", ""], ["Sood", "Akshay", ""], ["Craven", "Mark", ""]]}, {"id": "1811.07296", "submitter": "Jianlin Su", "authors": "Jianlin Su", "title": "GAN-QP: A Novel GAN Framework without Gradient Vanishing and Lipschitz\n  Constraint", "comments": "simplify some proofs; add reconstruction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We know SGAN may have a risk of gradient vanishing. A significant improvement\nis WGAN, with the help of 1-Lipschitz constraint on discriminator to prevent\nfrom gradient vanishing. Is there any GAN having no gradient vanishing and no\n1-Lipschitz constraint on discriminator? We do find one, called GAN-QP.\n  To construct a new framework of Generative Adversarial Network (GAN) usually\nincludes three steps: 1. choose a probability divergence; 2. convert it into a\ndual form; 3. play a min-max game. In this articles, we demonstrate that the\nfirst step is not necessary. We can analyse the property of divergence and even\nconstruct new divergence in dual space directly. As a reward, we obtain a\nsimpler alternative of WGAN: GAN-QP. We demonstrate that GAN-QP have a better\nperformance than WGAN in theory and practice.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 08:36:03 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 11:44:33 GMT"}, {"version": "v3", "created": "Sat, 8 Dec 2018 04:15:42 GMT"}, {"version": "v4", "created": "Sat, 15 Dec 2018 11:30:28 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Su", "Jianlin", ""]]}, {"id": "1811.07308", "submitter": "Wenhu Chen", "authors": "Wenhu Chen, Yilin Shen, Hongxia Jin, William Wang", "title": "A Variational Dirichlet Framework for Out-of-Distribution Detection", "comments": "Tech Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recently rapid development in deep learning, deep neural networks\nhave been widely adopted in many real-life applications. However, deep neural\nnetworks are also known to have very little control over its uncertainty for\nunseen examples, which potentially causes very harmful and annoying\nconsequences in practical scenarios. In this paper, we are particularly\ninterested in designing a higher-order uncertainty metric for deep neural\nnetworks and investigate its effectiveness under the out-of-distribution\ndetection task proposed by~\\cite{hendrycks2016baseline}. Our method first\nassumes there exists an underlying higher-order distribution $\\mathbb{P}(z)$,\nwhich controls label-wise categorical distribution $\\mathbb{P}(y)$ over classes\non the K-dimension simplex, and then approximate such higher-order distribution\nvia parameterized posterior function $p_{\\theta}(z|x)$ under variational\ninference framework, finally we use the entropy of learned posterior\ndistribution $p_{\\theta}(z|x)$ as uncertainty measure to detect\nout-of-distribution examples. Further, we propose an auxiliary objective\nfunction to discriminate against synthesized adversarial examples to further\nincrease the robustness of the proposed uncertainty measure. Through\ncomprehensive experiments on various datasets, our proposed framework is\ndemonstrated to consistently outperform competing algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 10:24:58 GMT"}, {"version": "v2", "created": "Sat, 2 Feb 2019 19:48:44 GMT"}, {"version": "v3", "created": "Wed, 27 Feb 2019 01:51:41 GMT"}, {"version": "v4", "created": "Sat, 20 Apr 2019 22:53:10 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Chen", "Wenhu", ""], ["Shen", "Yilin", ""], ["Jin", "Hongxia", ""], ["Wang", "William", ""]]}, {"id": "1811.07311", "submitter": "Yoel Shoshan", "authors": "Yoel Shoshan and Vadim Ratner", "title": "Regularized adversarial examples for model interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As machine learning algorithms continue to improve, there is an increasing\nneed for explaining why a model produces a certain prediction for a certain\ninput. In recent years, several methods for model interpretability have been\ndeveloped, aiming to provide explanation of which subset regions of the model\ninput is the main reason for the model prediction. In parallel, a significant\nresearch community effort is occurring in recent years for developing\nadversarial example generation methods for fooling models, while not altering\nthe true label of the input,as it would have been classified by a human\nannotator. In this paper, we bridge the gap between adversarial example\ngeneration and model interpretability, and introduce a modification to the\nadversarial example generation process which encourages better\ninterpretability. We analyze the proposed method on a public medical imaging\ndataset, both quantitatively and qualitatively, and show that it significantly\noutperforms the leading known alternative method. Our suggested method is\nsimple to implement, and can be easily plugged into most common adversarial\nexample generation frameworks. Additionally, we propose an explanation quality\nmetric - $APE$ - \"Adversarial Perturbative Explanation\", which measures how\nwell an explanation describes model decisions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 10:40:16 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 07:29:32 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Shoshan", "Yoel", ""], ["Ratner", "Vadim", ""]]}, {"id": "1811.07342", "submitter": "Xiao-Yang Liu", "authors": "Weijun Lu, Xiao-Yang Liu, Qingwei Wu, Yue Sun, Anwar Walid", "title": "Transform-Based Multilinear Dynamical System for Tensor Time Series\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel multilinear dynamical system (MLDS) in a transform domain,\nnamed $\\mathcal{L}$-MLDS, to model tensor time series. With transformations\napplied to a tensor data, the latent multidimensional correlations among the\nfrontal slices are built, and thus resulting in the computational independence\nin the transform domain. This allows the exact separability of the\nmulti-dimensional problem into multiple smaller LDS problems. To estimate the\nsystem parameters, we utilize the expectation-maximization (EM) algorithm to\ndetermine the parameters of each LDS. Further, $\\mathcal{L}$-MLDSs\nsignificantly reduce the model parameters and allows parallel processing. Our\ngeneral $\\mathcal{L}$-MLDS model is implemented based on different transforms:\ndiscrete Fourier transform, discrete cosine transform and discrete wavelet\ntransform. Due to the nonlinearity of these transformations, $\\mathcal{L}$-MLDS\nis able to capture the nonlinear correlations within the data unlike the MLDS\n\\cite{rogers2013multilinear} which assumes multi-way linear correlations. Using\nfour real datasets, the proposed $\\mathcal{L}$-MLDS is shown to achieve much\nhigher prediction accuracy than the state-of-the-art MLDS and LDS with an equal\nnumber of parameters under different noise models. In particular, the relative\nerrors are reduced by $50\\% \\sim 99\\%$. Simultaneously, $\\mathcal{L}$-MLDS\nachieves an exponential improvement in the model's training time than MLDS.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 15:45:31 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Lu", "Weijun", ""], ["Liu", "Xiao-Yang", ""], ["Wu", "Qingwei", ""], ["Sun", "Yue", ""], ["Walid", "Anwar", ""]]}, {"id": "1811.07350", "submitter": "Feiyang Pan", "authors": "Feiyang Pan, Qingpeng Cai, An-Xiang Zeng, Chun-Xiang Pan, Qing Da,\n  Hualin He, Qing He, Pingzhong Tang", "title": "Policy Optimization with Model-based Explorations", "comments": "Accepted at AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-free reinforcement learning methods such as the Proximal Policy\nOptimization algorithm (PPO) have successfully applied in complex\ndecision-making problems such as Atari games. However, these methods suffer\nfrom high variances and high sample complexity. On the other hand, model-based\nreinforcement learning methods that learn the transition dynamics are more\nsample efficient, but they often suffer from the bias of the transition\nestimation. How to make use of both model-based and model-free learning is a\ncentral problem in reinforcement learning. In this paper, we present a new\ntechnique to address the trade-off between exploration and exploitation, which\nregards the difference between model-free and model-based estimations as a\nmeasure of exploration value. We apply this new technique to the PPO algorithm\nand arrive at a new policy optimization method, named Policy Optimization with\nModel-based Explorations (POME). POME uses two components to predict the\nactions' target values: a model-free one estimated by Monte-Carlo sampling and\na model-based one which learns a transition model and predicts the value of the\nnext state. POME adds the error of these two target estimations as the\nadditional exploration value for each state-action pair, i.e, encourages the\nalgorithm to explore the states with larger target errors which are hard to\nestimate. We compare POME with PPO on Atari 2600 games, and it shows that POME\noutperforms PPO on 33 games out of 49 games.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 16:48:40 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Pan", "Feiyang", ""], ["Cai", "Qingpeng", ""], ["Zeng", "An-Xiang", ""], ["Pan", "Chun-Xiang", ""], ["Da", "Qing", ""], ["He", "Hualin", ""], ["He", "Qing", ""], ["Tang", "Pingzhong", ""]]}, {"id": "1811.07375", "submitter": "Ilia Shumailov", "authors": "Ilia Shumailov, Yiren Zhao, Robert Mullins, Ross Anderson", "title": "The Taboo Trap: Behavioural Detection of Adversarial Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Deep Neural Networks (DNNs) have become a powerful toolfor a wide range of\nproblems. Yet recent work has found an increasing variety of adversarial\nsamplesthat can fool them. Most existing detection mechanisms against\nadversarial attacksimpose significant costs, either by using additional\nclassifiers to spot adversarial samples, or by requiring the DNN to be\nrestructured. In this paper, we introduce a novel defence. We train our DNN so\nthat, as long as it is workingas intended on the kind of inputs we expect, its\nbehavior is constrained, in that some set of behaviors are taboo. If it is\nexposed to adversarial samples, they will often cause a taboo behavior, which\nwe can detect. Taboos can be both subtle and diverse, so their choice can\nencode and hide information. It is a well-established design principle that the\nsecurity of a system should not depend on the obscurity of its design, but on\nsome variable (the key) which can differ between implementations and bechanged\nas necessary. We discuss how taboos can be used to equip a classifier with just\nsuch a key, and how to tune the keying mechanism to adversaries of various\ncapabilities. We evaluate the performance of a prototype against a wide range\nof attacks and show how our simple defense can defend against cheap attacks at\nscale with zero run-time computation overhead, making it a suitable defense\nmethod for IoT devices.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 18:43:43 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 16:29:55 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Shumailov", "Ilia", ""], ["Zhao", "Yiren", ""], ["Mullins", "Robert", ""], ["Anderson", "Ross", ""]]}, {"id": "1811.07407", "submitter": "Faisal Mahmood", "authors": "Faisal Mahmood, Ziyun Yang, Thomas Ashley and Nicholas J. Durr", "title": "Multimodal Densenet", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans make accurate decisions by interpreting complex data from multiple\nsources. Medical diagnostics, in particular, often hinge on human\ninterpretation of multi-modal information. In order for artificial intelligence\nto make progress in automated, objective, and accurate diagnosis and prognosis,\nmethods to fuse information from multiple medical imaging modalities are\nrequired. However, combining information from multiple data sources has several\nchallenges, as current deep learning architectures lack the ability to extract\nuseful representations from multimodal information, and often simple\nconcatenation is used to fuse such information. In this work, we propose\nMultimodal DenseNet, a novel architecture for fusing multimodal data. Instead\nof focusing on concatenation or early and late fusion, our proposed\narchitectures fuses information over several layers and gives the model\nflexibility in how it combines information from multiple sources. We apply this\narchitecture to the challenge of polyp characterization and landmark\nidentification in endoscopy. Features from white light images are fused with\nfeatures from narrow band imaging or depth maps. This study demonstrates that\nMultimodal DenseNet outperforms monomodal classification as well as other\nmultimodal fusion techniques by a significant margin on two different datasets.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 21:31:22 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Mahmood", "Faisal", ""], ["Yang", "Ziyun", ""], ["Ashley", "Thomas", ""], ["Durr", "Nicholas J.", ""]]}, {"id": "1811.07426", "submitter": "Kyle Kastner", "authors": "Kyle Kastner, Rithesh Kumar, Tim Cooijmans, Aaron Courville", "title": "Harmonic Recomposition using Conditional Autoregressive Modeling", "comments": "3 pages, 2 figures. In Proceedings of The Joint Workshop on Machine\n  Learning for Music, ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We demonstrate a conditional autoregressive pipeline for efficient music\nrecomposition, based on methods presented in van den Oord et al.(2017).\nRecomposition (Casal & Casey, 2010) focuses on reworking existing musical\npieces, adhering to structure at a high level while also re-imagining other\naspects of the work. This can involve reuse of pre-existing themes or parts of\nthe original piece, while also requiring the flexibility to generate new\ncontent at different levels of granularity. Applying the aforementioned\nmodeling pipeline to recomposition, we show diverse and structured generation\nconditioned on chord sequence annotations.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 23:40:53 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Kastner", "Kyle", ""], ["Kumar", "Rithesh", ""], ["Cooijmans", "Tim", ""], ["Courville", "Aaron", ""]]}, {"id": "1811.07428", "submitter": "Georgios Tsitsikas", "authors": "Georgios Tsitsikas, Evangelos E. Papalexakis", "title": "The core consistency of a compressed tensor", "comments": "5 pages, 4 figures, submitted to International Conference on\n  Acoustics, Speech, and Signal Processing ( IEEE ICASSP 2019 )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor decomposition on big data has attracted significant attention\nrecently. Among the most popular methods is a class of algorithms that\nleverages compression in order to reduce the size of the tensor and potentially\nparallelize computations. A fundamental requirement for such methods to work\nproperly is that the low-rank tensor structure is retained upon compression. In\nlieu of efficient and realistic means of computing and studying the effects of\ncompression on the low rank of a tensor, we study the effects of compression on\nthe core consistency; a widely used heuristic that has been used as a proxy for\nestimating that low rank. We provide theoretical analysis, where we identify\nsufficient conditions for the compression such that the core consistency is\npreserved, and we conduct extensive experiments that validate our analysis.\nFurther, we explore popular compression schemes and how they affect the core\nconsistency.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 23:56:47 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Tsitsikas", "Georgios", ""], ["Papalexakis", "Evangelos E.", ""]]}, {"id": "1811.07429", "submitter": "Gabriel Peyr\\'e", "authors": "Gwendoline de Bie, Gabriel Peyr\\'e, Marco Cuturi", "title": "Stochastic Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is increasingly targeting areas where input data cannot be\naccurately described by a single vector, but can be modeled instead using the\nmore flexible concept of random vectors, namely probability measures or more\nsimply point clouds of varying cardinality. Using deep architectures on\nmeasures poses, however, many challenging issues. Indeed, deep architectures\nare originally designed to handle fixedlength vectors, or, using recursive\nmechanisms, ordered sequences thereof. In sharp contrast, measures describe a\nvarying number of weighted observations with no particular order. We propose in\nthis work a deep framework designed to handle crucial aspects of measures,\nnamely permutation invariances, variations in weights and cardinality.\nArchitectures derived from this pipeline can (i) map measures to measures -\nusing the concept of push-forward operators; (ii) bridge the gap between\nmeasures and Euclidean spaces - through integration steps. This allows to\ndesign discriminative networks (to classify or reduce the dimensionality of\ninput measures), generative architectures (to synthesize measures) and\nrecurrent pipelines (to predict measure dynamics). We provide a theoretical\nanalysis of these building blocks, review our architectures' approximation\nabilities and robustness w.r.t. perturbation, and try them on various\ndiscriminative and generative tasks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 00:11:06 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 09:25:41 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["de Bie", "Gwendoline", ""], ["Peyr\u00e9", "Gabriel", ""], ["Cuturi", "Marco", ""]]}, {"id": "1811.07441", "submitter": "Nadav Schor", "authors": "Nadav Schor, Oren Katzir, Hao Zhang and Daniel Cohen-Or", "title": "CompoNet: Learning to Generate the Unseen by Part Synthesis and\n  Composition", "comments": "Accepted to ICCV 2019. Code: https://github.com/nschor/CompoNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven generative modeling has made remarkable progress by leveraging\nthe power of deep neural networks. A reoccurring challenge is how to enable a\nmodel to generate a rich variety of samples from the entire target\ndistribution, rather than only from a distribution confined to the training\ndata. In other words, we would like the generative model to go beyond the\nobserved samples and learn to generate ``unseen'', yet still plausible, data.\nIn our work, we present CompoNet, a generative neural network for 2D or 3D\nshapes that is based on a part-based prior, where the key idea is for the\nnetwork to synthesize shapes by varying both the shape parts and their\ncompositions. Treating a shape not as an unstructured whole, but as a\n(re-)composable set of deformable parts, adds a combinatorial dimension to the\ngenerative process to enrich the diversity of the output, encouraging the\ngenerator to venture more into the ``unseen''. We show that our part-based\nmodel generates richer variety of plausible shapes compared with baseline\ngenerative models. To this end, we introduce two quantitative metrics to\nevaluate the diversity of a generative model and assess how well the generated\ndata covers both the training data and unseen data from the same target\ndistribution. Code is available at https://github.com/nschor/CompoNet.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 00:45:17 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 23:26:45 GMT"}, {"version": "v3", "created": "Sun, 4 Aug 2019 08:25:20 GMT"}, {"version": "v4", "created": "Sun, 1 Sep 2019 19:30:51 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Schor", "Nadav", ""], ["Katzir", "Oren", ""], ["Zhang", "Hao", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1811.07453", "submitter": "Mirco Ravanelli", "authors": "Mirco Ravanelli and Titouan Parcollet and Yoshua Bengio", "title": "The PyTorch-Kaldi Speech Recognition Toolkit", "comments": "Accepted at ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of open-source software is playing a remarkable role in the\npopularization of speech recognition and deep learning. Kaldi, for instance, is\nnowadays an established framework used to develop state-of-the-art speech\nrecognizers. PyTorch is used to build neural networks with the Python language\nand has recently spawn tremendous interest within the machine learning\ncommunity thanks to its simplicity and flexibility.\n  The PyTorch-Kaldi project aims to bridge the gap between these popular\ntoolkits, trying to inherit the efficiency of Kaldi and the flexibility of\nPyTorch. PyTorch-Kaldi is not only a simple interface between these software,\nbut it embeds several useful features for developing modern speech recognizers.\nFor instance, the code is specifically designed to naturally plug-in\nuser-defined acoustic models. As an alternative, users can exploit several\npre-implemented neural networks that can be customized using intuitive\nconfiguration files. PyTorch-Kaldi supports multiple feature and label streams\nas well as combinations of neural networks, enabling the use of complex neural\narchitectures. The toolkit is publicly-released along with a rich documentation\nand is designed to properly work locally or on HPC clusters.\n  Experiments, that are conducted on several datasets and tasks, show that\nPyTorch-Kaldi can effectively be used to develop modern state-of-the-art speech\nrecognizers.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 01:57:05 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 19:13:03 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Ravanelli", "Mirco", ""], ["Parcollet", "Titouan", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1811.07455", "submitter": "Hu Ding", "authors": "Hu Ding and Mingquan Ye", "title": "On Geometric Alignment in Low Doubling Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world, many problems can be formulated as the alignment between two\ngeometric patterns. Previously, a great amount of research focus on the\nalignment of 2D or 3D patterns, especially in the field of computer vision.\nRecently, the alignment of geometric patterns in high dimension finds several\nnovel applications, and has attracted more and more attentions. However, the\nresearch is still rather limited in terms of algorithms. To the best of our\nknowledge, most existing approaches for high dimensional alignment are just\nsimple extensions of their counterparts for 2D and 3D cases, and often suffer\nfrom the issues such as high complexities. In this paper, we propose an\neffective framework to compress the high dimensional geometric patterns and\napproximately preserve the alignment quality. As a consequence, existing\nalignment approach can be applied to the compressed geometric patterns and thus\nthe time complexity is significantly reduced. Our idea is inspired by the\nobservation that high dimensional data often has a low intrinsic dimension. We\nadopt the widely used notion \"doubling dimension\" to measure the extents of our\ncompression and the resulting approximation. Finally, we test our method on\nboth random and real datasets, the experimental results reveal that running the\nalignment algorithm on compressed patterns can achieve similar qualities,\ncomparing with the results on the original patterns, but the running times\n(including the times cost for compression) are substantially lower.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 02:03:35 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Ding", "Hu", ""], ["Ye", "Mingquan", ""]]}, {"id": "1811.07457", "submitter": "Farzan Farnia", "authors": "Farzan Farnia, Jesse M. Zhang, David Tse", "title": "Generalizable Adversarial Training via Spectral Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have set benchmarks on a wide array of supervised\nlearning tasks. Trained DNNs, however, often lack robustness to minor\nadversarial perturbations to the input, which undermines their true\npracticality. Recent works have increased the robustness of DNNs by fitting\nnetworks using adversarially-perturbed training samples, but the improved\nperformance can still be far below the performance seen in non-adversarial\nsettings. A significant portion of this gap can be attributed to the decrease\nin generalization performance due to adversarial training. In this work, we\nextend the notion of margin loss to adversarial settings and bound the\ngeneralization error for DNNs trained under several well-known gradient-based\nattack schemes, motivating an effective regularization scheme based on spectral\nnormalization of the DNN's weight matrices. We also provide a\ncomputationally-efficient method for normalizing the spectral norm of\nconvolutional layers with arbitrary stride and padding schemes in deep\nconvolutional networks. We evaluate the power of spectral normalization\nextensively on combinations of datasets, network architectures, and adversarial\ntraining schemes. The code is available at\nhttps://github.com/jessemzhang/dl_spectral_normalization.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 02:06:09 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Farnia", "Farzan", ""], ["Zhang", "Jesse M.", ""], ["Tse", "David", ""]]}, {"id": "1811.07465", "submitter": "Haoran You", "authors": "Haoran You, Yu Cheng, Tianheng Cheng, Chunliang Li, Pan Zhou", "title": "Bayesian Cycle-Consistent Generative Adversarial Networks via\n  Marginalizing Latent Sampling", "comments": "Accepted by IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent techniques built on Generative Adversarial Networks (GANs), such as\nCycle-Consistent GANs, are able to learn mappings among different domains built\nfrom unpaired datasets, through min-max optimization games between generators\nand discriminators. However, it remains challenging to stabilize the training\nprocess and thus cyclic models fall into mode collapse accompanied by the\nsuccess of discriminator. To address this problem, we propose an novel Bayesian\ncyclic model and an integrated cyclic framework for inter-domain mappings. The\nproposed method motivated by Bayesian GAN explores the full posteriors of\ncyclic model via sampling latent variables and optimizes the model with maximum\na posteriori (MAP) estimation. Hence, we name it Bayesian CycleGAN. In\naddition, original CycleGAN cannot generate diversified results. But it is\nfeasible for Bayesian framework to diversify generated images by replacing\nrestricted latent variables in inference process. We evaluate the proposed\nBayesian CycleGAN on multiple benchmark datasets, including Cityscapes, Maps,\nand Monet2photo. The proposed method improve the per-pixel accuracy by 15% for\nthe Cityscapes semantic segmentation task within origin framework and improve\n20% within the proposed integrated framework, showing better resilience to\nimbalance confrontation. The diversified results of Monet2Photo style transfer\nalso demonstrate its superiority over original cyclic model. We provide codes\nfor all of our experiments in https://github.com/ranery/Bayesian-CycleGAN.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 02:22:49 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 06:34:52 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2020 19:38:06 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["You", "Haoran", ""], ["Cheng", "Yu", ""], ["Cheng", "Tianheng", ""], ["Li", "Chunliang", ""], ["Zhou", "Pan", ""]]}, {"id": "1811.07476", "submitter": "Anant Gupta", "authors": "Anant Gupta", "title": "Best Arm Identification in Linked Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of best arm identification in a variant of\nmulti-armed bandits called linked bandits. In a single interaction with linked\nbandits, multiple arms are played sequentially until one of them receives a\npositive reward. Since each interaction provides feedback about more than one\narm, the sample complexity can be much lower than in the regular bandit\nsetting. We propose an algorithm for linked bandits, that combines a novel\nsubroutine to perform uniform sampling with a known optimal algorithm for\nregular bandits. We prove almost matching upper and lower bounds on the sample\ncomplexity of best arm identification in linked bandits. These bounds have an\ninteresting structure, with an explicit dependence on the mean rewards of the\narms, not just the gaps. We also corroborate our theoretical results with\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 03:09:30 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 04:27:11 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Gupta", "Anant", ""]]}, {"id": "1811.07484", "submitter": "Kuan-Chuan Peng", "authors": "Lezi Wang, Ziyan Wu, Srikrishna Karanam, Kuan-Chuan Peng, Rajat Vikram\n  Singh, Bo Liu, Dimitris N. Metaxas", "title": "Sharpen Focus: Learning with Attention Separability and Consistency", "comments": "This paper is accepted to ICCV 2019. The supplementary material\n  (appendix) can be found after the main paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in gradient-based attention modeling have seen attention\nmaps emerge as a powerful tool for interpreting convolutional neural networks.\nDespite good localization for an individual class of interest, these techniques\nproduce attention maps with substantially overlapping responses among different\nclasses, leading to the problem of visual confusion and the need for\ndiscriminative attention. In this paper, we address this problem by means of a\nnew framework that makes class-discriminative attention a principled part of\nthe learning process. Our key innovations include new learning objectives for\nattention separability and cross-layer consistency, which result in improved\nattention discriminability and reduced visual confusion. Extensive experiments\non image classification benchmarks show the effectiveness of our approach in\nterms of improved classification accuracy, including CIFAR-100 (+3.33%),\nCaltech-256 (+1.64%), ILSVRC2012 (+0.92%), CUB-200-2011 (+4.8%) and PASCAL\nVOC2012 (+5.73%).\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 03:49:19 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 13:41:43 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 21:10:26 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Wang", "Lezi", ""], ["Wu", "Ziyan", ""], ["Karanam", "Srikrishna", ""], ["Peng", "Kuan-Chuan", ""], ["Singh", "Rajat Vikram", ""], ["Liu", "Bo", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1811.07487", "submitter": "Srikrishna Karanam", "authors": "Meng Zheng, Srikrishna Karanam, Ziyan Wu, Richard J. Radke", "title": "Re-Identification with Consistent Attentive Siamese Networks", "comments": "10 pages, 8 figures, 3 tables, to appear in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new deep architecture for person re-identification (re-id).\nWhile re-id has seen much recent progress, spatial localization and\nview-invariant representation learning for robust cross-view matching remain\nkey, unsolved problems. We address these questions by means of a new\nattention-driven Siamese learning architecture, called the Consistent Attentive\nSiamese Network. Our key innovations compared to existing, competing methods\ninclude (a) a flexible framework design that produces attention with only\nidentity labels as supervision, (b) explicit mechanisms to enforce attention\nconsistency among images of the same person, and (c) a new Siamese framework\nthat integrates attention and attention consistency, producing principled\nsupervisory signals as well as the first mechanism that can explain the\nreasoning behind the Siamese framework's predictions. We conduct extensive\nevaluations on the CUHK03-NP, DukeMTMC-ReID, and Market-1501 datasets and\nreport competitive performance.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 03:59:51 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 17:07:25 GMT"}, {"version": "v3", "created": "Wed, 19 Dec 2018 18:37:34 GMT"}, {"version": "v4", "created": "Thu, 11 Apr 2019 14:25:28 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Zheng", "Meng", ""], ["Karanam", "Srikrishna", ""], ["Wu", "Ziyan", ""], ["Radke", "Richard J.", ""]]}, {"id": "1811.07490", "submitter": "Yunbo Wang", "authors": "Yunbo Wang, Jianjin Zhang, Hongyu Zhu, Mingsheng Long, Jianmin Wang,\n  Philip S Yu", "title": "Memory In Memory: A Predictive Neural Network for Learning Higher-Order\n  Non-Stationarity from Spatiotemporal Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural spatiotemporal processes can be highly non-stationary in many ways,\ne.g. the low-level non-stationarity such as spatial correlations or temporal\ndependencies of local pixel values; and the high-level variations such as the\naccumulation, deformation or dissipation of radar echoes in precipitation\nforecasting. From Cramer's Decomposition, any non-stationary process can be\ndecomposed into deterministic, time-variant polynomials, plus a zero-mean\nstochastic term. By applying differencing operations appropriately, we may turn\ntime-variant polynomials into a constant, making the deterministic component\npredictable. However, most previous recurrent neural networks for\nspatiotemporal prediction do not use the differential signals effectively, and\ntheir relatively simple state transition functions prevent them from learning\ntoo complicated variations in spacetime. We propose the Memory In Memory (MIM)\nnetworks and corresponding recurrent blocks for this purpose. The MIM blocks\nexploit the differential signals between adjacent recurrent states to model the\nnon-stationary and approximately stationary properties in spatiotemporal\ndynamics with two cascaded, self-renewed memory modules. By stacking multiple\nMIM blocks, we could potentially handle higher-order non-stationarity. The MIM\nnetworks achieve the state-of-the-art results on four spatiotemporal prediction\ntasks across both synthetic and real-world datasets. We believe that the\ngeneral idea of this work can be potentially applied to other time-series\nforecasting tasks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 04:07:49 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 17:24:36 GMT"}, {"version": "v3", "created": "Sun, 21 Apr 2019 05:11:19 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Wang", "Yunbo", ""], ["Zhang", "Jianjin", ""], ["Zhu", "Hongyu", ""], ["Long", "Mingsheng", ""], ["Wang", "Jianmin", ""], ["Yu", "Philip S", ""]]}, {"id": "1811.07503", "submitter": "Yu Pan", "authors": "Yu Pan, Jing Xu, Maolin Wang, Jinmian Ye, Fei Wang, Kun Bai, Zenglin\n  Xu", "title": "Compressing Recurrent Neural Networks with Tensor Ring for Action\n  Recognition", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) and their variants, such as Long-Short Term\nMemory (LSTM) networks, and Gated Recurrent Unit (GRU) networks, have achieved\npromising performance in sequential data modeling. The hidden layers in RNNs\ncan be regarded as the memory units, which are helpful in storing information\nin sequential contexts. However, when dealing with high dimensional input data,\nsuch as video and text, the input-to-hidden linear transformation in RNNs\nbrings high memory usage and huge computational cost. This makes the training\nof RNNs unscalable and difficult. To address this challenge, we propose a novel\ncompact LSTM model, named as TR-LSTM, by utilizing the low-rank tensor ring\ndecomposition (TRD) to reformulate the input-to-hidden transformation. Compared\nwith other tensor decomposition methods, TR-LSTM is more stable. In addition,\nTR-LSTM can complete an end-to-end training and also provide a fundamental\nbuilding block for RNNs in handling large input data. Experiments on real-world\naction recognition datasets have demonstrated the promising performance of the\nproposed TR-LSTM compared with the tensor train LSTM and other state-of-the-art\ncompetitors.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 05:10:14 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Pan", "Yu", ""], ["Xu", "Jing", ""], ["Wang", "Maolin", ""], ["Ye", "Jinmian", ""], ["Wang", "Fei", ""], ["Bai", "Kun", ""], ["Xu", "Zenglin", ""]]}, {"id": "1811.07514", "submitter": "Shobeir Fakhraei", "authors": "Shobeir Fakhraei, Joel Mathew, Jose Luis Ambite", "title": "NSEEN: Neural Semantic Embedding for Entity Normalization", "comments": "Accepted for publication at ECML-PKDD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DB cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of human knowledge is encoded in text, available in scientific\npublications, books, and the web. Given the rapid growth of these resources, we\nneed automated methods to extract such knowledge into machine-processable\nstructures, such as knowledge graphs. An important task in this process is\nentity normalization, which consists of mapping noisy entity mentions in text\nto canonical entities in well-known reference sets. However, entity\nnormalization is a challenging problem; there often are many textual forms for\na canonical entity that may not be captured in the reference set, and entities\nmentioned in text may include many syntactic variations, or errors. The problem\nis particularly acute in scientific domains, such as biology. To address this\nproblem, we have developed a general, scalable solution based on a deep Siamese\nneural network model to embed the semantic information about the entities, as\nwell as their syntactic variations. We use these embeddings for fast mapping of\nnew entities to large reference sets, and empirically show the effectiveness of\nour framework in challenging bio-entity normalization datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 06:04:13 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2019 07:19:08 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Fakhraei", "Shobeir", ""], ["Mathew", "Joel", ""], ["Ambite", "Jose Luis", ""]]}, {"id": "1811.07522", "submitter": "Xiao-Yang Liu", "authors": "Zhuoran Xiong, Xiao-Yang Liu, Shan Zhong, Hongyang Yang, and Anwar\n  Walid", "title": "Practical Deep Reinforcement Learning Approach for Stock Trading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.TR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stock trading strategy plays a crucial role in investment companies. However,\nit is challenging to obtain optimal strategy in the complex and dynamic stock\nmarket. We explore the potential of deep reinforcement learning to optimize\nstock trading strategy and thus maximize investment return. 30 stocks are\nselected as our trading stocks and their daily prices are used as the training\nand trading market environment. We train a deep reinforcement learning agent\nand obtain an adaptive trading strategy. The agent's performance is evaluated\nand compared with Dow Jones Industrial Average and the traditional min-variance\nportfolio allocation strategy. The proposed deep reinforcement learning\napproach is shown to outperform the two baselines in terms of both the Sharpe\nratio and cumulative returns.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 06:43:28 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 00:26:05 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Xiong", "Zhuoran", ""], ["Liu", "Xiao-Yang", ""], ["Zhong", "Shan", ""], ["Yang", "Hongyang", ""], ["Walid", "Anwar", ""]]}, {"id": "1811.07531", "submitter": "Aurelien Pelissier", "authors": "Aurelien Pelissier, Atsuyoshi Nakamura, Koji Tabata", "title": "Feature selection as Monte-Carlo Search in Growing Single Rooted\n  Directed Acyclic Graph by Best Leaf Identification", "comments": null, "journal-ref": "Proceedings of the 2019 SIAM International Conference on Data\n  Mining. 2019, 450-458", "doi": "10.1137/1.9781611975673.51", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo tree search (MCTS) has received considerable interest due to its\nspectacular success in the difficult problem of computer Go and also proved\nbeneficial in a range of other domains. A major issue that has received little\nattention in the MCTS literature is the fact that, in most games, different\nactions can lead to the same state, that may lead to a high degree of\nredundancy in tree representation and unnecessary additional computational\ncost. We extend MCTS to single rooted directed acyclic graph (SR-DAG), and\nconsider the Best Arm Identification (BAI) and the Best Leaf Identification\n(BLI) problem of an expanding SR-DAG of arbitrary depth. We propose algorithms\nthat are (epsilon, delta)-correct in the fixed confidence setting, and prove an\nasymptotic upper bounds of sample complexity for our BAI algorithm. As a major\napplication for our BLI algorithm, a novel approach for Feature Selection is\nproposed by representing the feature set space as a SR-DAG and repeatedly\nevaluating feature subsets until a candidate for the best leaf is returned, a\nproof of concept is shown on benchmark data sets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 07:29:51 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 17:57:17 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Pelissier", "Aurelien", ""], ["Nakamura", "Atsuyoshi", ""], ["Tabata", "Koji", ""]]}, {"id": "1811.07533", "submitter": "Yuhang Liu", "authors": "Yuhang Liu, Wenyong Dong, Lei Zhang, Dong Gong and Qinfeng Shi", "title": "Variational Bayesian Dropout with a Hierarchical Prior", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational dropout (VD) is a generalization of Gaussian dropout, which aims\nat inferring the posterior of network weights based on a log-uniform prior on\nthem to learn these weights as well as dropout rate simultaneously. The\nlog-uniform prior not only interprets the regularization capacity of Gaussian\ndropout in network training, but also underpins the inference of such\nposterior. However, the log-uniform prior is an improper prior (i.e., its\nintegral is infinite) which causes the inference of posterior to be ill-posed,\nthus restricting the regularization performance of VD. To address this problem,\nwe present a new generalization of Gaussian dropout, termed variational\nBayesian dropout (VBD), which turns to exploit a hierarchical prior on the\nnetwork weights and infer a new joint posterior. Specifically, we implement the\nhierarchical prior as a zero-mean Gaussian distribution with variance sampled\nfrom a uniform hyper-prior. Then, we incorporate such a prior into inferring\nthe joint posterior over network weights and the variance in the hierarchical\nprior, with which both the network training and the dropout rate estimation can\nbe cast into a joint optimization problem. More importantly, the hierarchical\nprior is a proper prior which enables the inference of posterior to be\nwell-posed. In addition, we further show that the proposed VBD can be\nseamlessly applied to network compression. Experiments on both classification\nand network compression tasks demonstrate the superior performance of the\nproposed VBD in terms of regularizing network training.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 07:31:46 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 00:11:31 GMT"}, {"version": "v3", "created": "Thu, 4 Apr 2019 07:48:12 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Liu", "Yuhang", ""], ["Dong", "Wenyong", ""], ["Zhang", "Lei", ""], ["Gong", "Dong", ""], ["Shi", "Qinfeng", ""]]}, {"id": "1811.07545", "submitter": "Yunxiao Qin", "authors": "Yunxiao Qin, Chenxu Zhao, Zezheng Wang, Junliang Xing, Jun Wan, Zhen\n  Lei", "title": "Representation based and Attention augmented Meta learning", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based computer vision fails to work when labeled images are\nscarce. Recently, Meta learning algorithm has been confirmed as a promising way\nto improve the ability of learning from few images for computer vision.\nHowever, previous Meta learning approaches expose problems:\n  1) they ignored the importance of attention mechanism for the Meta learner;\n  2) they didn't give the Meta learner the ability of well using the past\nknowledge which can help to express images into high representations, resulting\nin that the Meta learner has to solve few shot learning task directly from the\noriginal high dimensional RGB images.\n  In this paper, we argue that the attention mechanism and the past knowledge\nare crucial for the Meta learner, and the Meta learner should be trained on\nhigh representations of the RGB images instead of directly on the original\nones. Based on these arguments, we propose two methods: Attention augmented\nMeta Learning (AML) and Representation based and Attention augmented Meta\nLearning(RAML). The method AML aims to improve the Meta learner's attention\nability by explicitly embedding an attention model into its network. The method\nRAML aims to give the Meta learner the ability of leveraging the past learned\nknowledge to reduce the dimension of the original input data by expressing it\ninto high representations, and help the Meta learner to perform well. Extensive\nexperiments demonstrate the effectiveness of the proposed models, with\nstate-of-the-art few shot learning performances on several few shot learning\nbenchmarks. The source code of our proposed methods will be released soon to\nfacilitate further studies on those aforementioned problem.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 08:08:00 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 03:27:02 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 07:11:59 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Qin", "Yunxiao", ""], ["Zhao", "Chenxu", ""], ["Wang", "Zezheng", ""], ["Xing", "Junliang", ""], ["Wan", "Jun", ""], ["Lei", "Zhen", ""]]}, {"id": "1811.07550", "submitter": "Xiujun Li", "authors": "Yuexin Wu and Xiujun Li and Jingjing Liu and Jianfeng Gao and Yiming\n  Yang", "title": "Switch-based Active Deep Dyna-Q: Efficient Adaptive Planning for\n  Task-Completion Dialogue Policy Learning", "comments": "8 pages, 9 figures, AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training task-completion dialogue agents with reinforcement learning usually\nrequires a large number of real user experiences. The Dyna-Q algorithm extends\nQ-learning by integrating a world model, and thus can effectively boost\ntraining efficiency using simulated experiences generated by the world model.\nThe effectiveness of Dyna-Q, however, depends on the quality of the world model\n- or implicitly, the pre-specified ratio of real vs. simulated experiences used\nfor Q-learning. To this end, we extend the recently proposed Deep Dyna-Q (DDQ)\nframework by integrating a switcher that automatically determines whether to\nuse a real or simulated experience for Q-learning. Furthermore, we explore the\nuse of active learning for improving sample efficiency, by encouraging the\nworld model to generate simulated experiences in the state-action space where\nthe agent has not (fully) explored. Our results show that by combining switcher\nand active learning, the new framework named as Switch-based Active Deep Dyna-Q\n(Switch-DDQ), leads to significant improvement over DDQ and Q-learning\nbaselines in both simulation and human evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 08:23:34 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Wu", "Yuexin", ""], ["Li", "Xiujun", ""], ["Liu", "Jingjing", ""], ["Gao", "Jianfeng", ""], ["Yang", "Yiming", ""]]}, {"id": "1811.07555", "submitter": "Yuxin Zhang", "authors": "Yuxin Zhang, Huan Wang, Yang Luo, Lu Yu, Haoji Hu, Hangguan Shan, Tony\n  Q. S. Quek", "title": "Three Dimensional Convolutional Neural Network Pruning with\n  Regularization-Based Method", "comments": "ICIP 2019", "journal-ref": "ICIP 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite enjoying extensive applications in video analysis, three-dimensional\nconvolutional neural networks (3D CNNs)are restricted by their massive\ncomputation and storage consumption. To solve this problem, we propose a\nthreedimensional regularization-based neural network pruning method to assign\ndifferent regularization parameters to different weight groups based on their\nimportance to the network. Further we analyze the redundancy and computation\ncost for each layer to determine the different pruning ratios. Experiments show\nthat pruning based on our method can lead to 2x theoretical speedup with only\n0.41% accuracy loss for 3DResNet18 and 3.28% accuracy loss for C3D. The\nproposed method performs favorably against other popular methods for model\ncompression and acceleration.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 08:40:00 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 03:48:09 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Zhang", "Yuxin", ""], ["Wang", "Huan", ""], ["Luo", "Yang", ""], ["Yu", "Lu", ""], ["Hu", "Haoji", ""], ["Shan", "Hangguan", ""], ["Quek", "Tony Q. S.", ""]]}, {"id": "1811.07557", "submitter": "Kristy Choi", "authors": "Kristy Choi, Kedar Tatwawadi, Aditya Grover, Tsachy Weissman, Stefano\n  Ermon", "title": "Neural Joint Source-Channel Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For reliable transmission across a noisy communication channel, classical\nresults from information theory show that it is asymptotically optimal to\nseparate out the source and channel coding processes. However, this\ndecomposition can fall short in the finite bit-length regime, as it requires\nnon-trivial tuning of hand-crafted codes and assumes infinite computational\npower for decoding. In this work, we propose to jointly learn the encoding and\ndecoding processes using a new discrete variational autoencoder model. By\nadding noise into the latent codes to simulate the channel during training, we\nlearn to both compress and error-correct given a fixed bit-length and\ncomputational budget. We obtain codes that are not only competitive against\nseveral separation schemes, but also learn useful robust representations of the\ndata for downstream tasks such as classification. Finally, inference\namortization yields an extremely fast neural decoder, almost an order of\nmagnitude faster compared to standard decoding methods based on iterative\nbelief propagation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 08:43:15 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 08:51:04 GMT"}, {"version": "v3", "created": "Tue, 14 May 2019 04:35:46 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Choi", "Kristy", ""], ["Tatwawadi", "Kedar", ""], ["Grover", "Aditya", ""], ["Weissman", "Tsachy", ""], ["Ermon", "Stefano", ""]]}, {"id": "1811.07579", "submitter": "Yonatan Geifman", "authors": "Yonatan Geifman, Ran El-Yaniv", "title": "Deep Active Learning with a Neural Architecture Search", "comments": "Accepted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider active learning of deep neural networks. Most active learning\nworks in this context have focused on studying effective querying mechanisms\nand assumed that an appropriate network architecture is a priori known for the\nproblem at hand. We challenge this assumption and propose a novel active\nstrategy whereby the learning algorithm searches for effective architectures on\nthe fly, while actively learning. We apply our strategy using three known\nquerying techniques (softmax response, MC-dropout, and coresets) and show that\nthe proposed approach overwhelmingly outperforms active learning using fixed\narchitectures.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 09:45:20 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 11:05:25 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Geifman", "Yonatan", ""], ["El-Yaniv", "Ran", ""]]}, {"id": "1811.07591", "submitter": "Leonard Berrada", "authors": "Leonard Berrada, Andrew Zisserman, M. Pawan Kumar", "title": "Deep Frank-Wolfe For Neural Network Optimization", "comments": "Published as a conference paper at ICLR 2019, last version fixing an\n  inaccuracy (details in appendix A.5, Proposition 2)", "journal-ref": "International Conference on Learning Representations 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a deep neural network requires solving a challenging optimization\nproblem: it is a high-dimensional, non-convex and non-smooth minimization\nproblem with a large number of terms. The current practice in neural network\noptimization is to rely on the stochastic gradient descent (SGD) algorithm or\nits adaptive variants. However, SGD requires a hand-designed schedule for the\nlearning rate. In addition, its adaptive variants tend to produce solutions\nthat generalize less well on unseen data than SGD with a hand-designed\nschedule. We present an optimization method that offers empirically the best of\nboth worlds: our algorithm yields good generalization performance while\nrequiring only one hyper-parameter. Our approach is based on a composite\nproximal framework, which exploits the compositional nature of deep neural\nnetworks and can leverage powerful convex optimization algorithms by design.\nSpecifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computes\nan optimal step-size in closed-form at each time-step. We further show that the\ndescent direction is given by a simple backward pass in the network, yielding\nthe same computational cost per iteration as SGD. We present experiments on the\nCIFAR and SNLI data sets, where we demonstrate the significant superiority of\nour method over Adam, Adagrad, as well as the recently proposed BPGrad and\nAMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed\nlearning rate schedule, and show that it provides similar generalization while\nconverging faster. The code is publicly available at\nhttps://github.com/oval-group/dfw.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 10:23:27 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 10:52:26 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 18:08:34 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Berrada", "Leonard", ""], ["Zisserman", "Andrew", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "1811.07594", "submitter": "Lucas Lamata", "authors": "J. Olivares-S\\'anchez, J. Casanova, E. Solano, L. Lamata", "title": "Measurement-based adaptation protocol with quantum reinforcement\n  learning in a Rigetti quantum computer", "comments": null, "journal-ref": "Quantum Reports 2, 293 (2020)", "doi": "10.3390/quantum2020019", "report-no": null, "categories": "quant-ph cond-mat.mes-hall cs.AI cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an experimental realization of a measurement-based adaptation\nprotocol with quantum reinforcement learning in a Rigetti cloud quantum\ncomputer. The experiment in this few-qubit superconducting chip faithfully\nreproduces the theoretical proposal, setting the first steps towards a\nsemiautonomous quantum agent. This experiment paves the way towards quantum\nreinforcement learning with superconducting circuits.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 10:33:14 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 16:36:40 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Olivares-S\u00e1nchez", "J.", ""], ["Casanova", "J.", ""], ["Solano", "E.", ""], ["Lamata", "L.", ""]]}, {"id": "1811.07600", "submitter": "Anshuman Suri", "authors": "Parag Agrawal, Anshuman Suri, Tulasi Menon", "title": "A Trustworthy, Responsible and Interpretable System to Handle Chit Chat\n  in Conversational Bots", "comments": "7 pages, 5 figures, The Second AAAI Workshop on Reasoning and\n  Learning for Human-Machine Dialogues (DEEP-DIAL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most often, chat-bots are built to solve the purpose of a search engine or a\nhuman assistant: Their primary goal is to provide information to the user or\nhelp them complete a task. However, these chat-bots are incapable of responding\nto unscripted queries like \"Hi, what's up\", \"What's your favourite food\". Human\nevaluation judgments show that 4 humans come to a consensus on the intent of a\ngiven query which is from chat domain only 77% of the time, thus making it\nevident how non-trivial this task is. In our work, we show why it is difficult\nto break the chitchat space into clearly defined intents. We propose a system\nto handle this task in chat-bots, keeping in mind scalability,\ninterpretability, appropriateness, trustworthiness, relevance and coverage. Our\nwork introduces a pipeline for query understanding in chitchat using\nhierarchical intents as well as a way to use seq-seq auto-generation models in\nprofessional bots. We explore an interpretable model for chat domain detection\nand also show how various components such as adult/offensive classification,\ngrammars/regex patterns, curated personality based responses, generic guided\nevasive responses and response generation models can be combined in a scalable\nway to solve this problem.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 10:45:13 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 12:10:30 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Agrawal", "Parag", ""], ["Suri", "Anshuman", ""], ["Menon", "Tulasi", ""]]}, {"id": "1811.07605", "submitter": "Maciej Zamorski", "authors": "Maciej Zamorski, Maciej Zi\\k{e}ba, Piotr Klukowski, Rafa{\\l} Nowak,\n  Karol Kurach, Wojciech Stokowiec, Tomasz Trzci\\'nski", "title": "Adversarial Autoencoders for Compact Representations of 3D Point Clouds", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative architectures provide a way to model not only images but also\ncomplex, 3-dimensional objects, such as point clouds. In this work, we present\na novel method to obtain meaningful representations of 3D shapes that can be\nused for challenging tasks including 3D points generation, reconstruction,\ncompression, and clustering. Contrary to existing methods for 3D point cloud\ngeneration that train separate decoupled models for representation learning and\ngeneration, our approach is the first end-to-end solution that allows to\nsimultaneously learn a latent space of representation and generate 3D shape out\nof it. Moreover, our model is capable of learning meaningful compact binary\ndescriptors with adversarial training conducted on a latent space. To achieve\nthis goal, we extend a deep Adversarial Autoencoder model (AAE) to accept 3D\ninput and create 3D output. Thanks to our end-to-end training regime, the\nresulting method called 3D Adversarial Autoencoder (3dAAE) obtains either\nbinary or continuous latent space that covers a much wider portion of training\ndata distribution. Finally, our quantitative evaluation shows that 3dAAE\nprovides state-of-the-art results for 3D points clustering and 3D object\nretrieval.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 10:51:09 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 18:00:49 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 19:22:36 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Zamorski", "Maciej", ""], ["Zi\u0119ba", "Maciej", ""], ["Klukowski", "Piotr", ""], ["Nowak", "Rafa\u0142", ""], ["Kurach", "Karol", ""], ["Stokowiec", "Wojciech", ""], ["Trzci\u0144ski", "Tomasz", ""]]}, {"id": "1811.07609", "submitter": "Sambaran Bandyopadhyay", "authors": "Sambaran Bandyopadhyay, Lokesh N, M. N. Murty", "title": "Outlier Aware Network Embedding for Attributed Networks", "comments": "Accepted for publication in The Thirty-Third AAAI Conference on\n  Artificial Intelligence (AAAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attributed network embedding has received much interest from the research\ncommunity as most of the networks come with some content in each node, which is\nalso known as node attributes. Existing attributed network approaches work well\nwhen the network is consistent in structure and attributes, and nodes behave as\nexpected. But real world networks often have anomalous nodes. Typically these\noutliers, being relatively unexplainable, affect the embeddings of other nodes\nin the network. Thus all the downstream network mining tasks fail miserably in\nthe presence of such outliers. Hence an integrated approach to detect anomalies\nand reduce their overall effect on the network embedding is required.\n  Towards this end, we propose an unsupervised outlier aware network embedding\nalgorithm (ONE) for attributed networks, which minimizes the effect of the\noutlier nodes, and hence generates robust network embeddings. We align and\njointly optimize the loss functions coming from structure and attributes of the\nnetwork. To the best of our knowledge, this is the first generic network\nembedding approach which incorporates the effect of outliers for an attributed\nnetwork without any supervision. We experimented on publicly available real\nnetworks and manually planted different types of outliers to check the\nperformance of the proposed algorithm. Results demonstrate the superiority of\nour approach to detect the network outliers compared to the state-of-the-art\napproaches. We also consider different downstream machine learning applications\non networks to show the efficiency of ONE as a generic network embedding\ntechnique. The source code is made available at\nhttps://github.com/sambaranban/ONE.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 10:59:34 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Bandyopadhyay", "Sambaran", ""], ["N", "Lokesh", ""], ["Murty", "M. N.", ""]]}, {"id": "1811.07615", "submitter": "Renato Cordeiro De Amorim", "authors": "Stiphen Chowdhury, Renato Cordeiro de Amorim", "title": "An efficient density-based clustering algorithm using reverse nearest\n  neighbour", "comments": "Accepted in: Computing Conference 2019 in London, UK.\n  http://saiconference.com/Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Density-based clustering is the task of discovering high-density regions of\nentities (clusters) that are separated from each other by contiguous regions of\nlow-density. DBSCAN is, arguably, the most popular density-based clustering\nalgorithm. However, its cluster recovery capabilities depend on the combination\nof the two parameters. In this paper we present a new density-based clustering\nalgorithm which uses reverse nearest neighbour (RNN) and has a single\nparameter. We also show that it is possible to estimate a good value for this\nparameter using a clustering validity index. The RNN queries enable our\nalgorithm to estimate densities taking more than a single entity into account,\nand to recover clusters that are not well-separated or have different\ndensities. Our experiments on synthetic and real-world data sets show our\nproposed algorithm outperforms DBSCAN and its recent variant ISDBSCAN.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 11:11:14 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Chowdhury", "Stiphen", ""], ["de Amorim", "Renato Cordeiro", ""]]}, {"id": "1811.07624", "submitter": "Cristian Rusu", "authors": "Cristian Rusu", "title": "Approximate Eigenvalue Decompositions of Linear Transformations with a\n  Few Householder Reflectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to decompose a signal in an orthonormal basis (a set of\northogonal components, each normalized to have unit length) using a fast\nnumerical procedure rests at the heart of many signal processing methods and\napplications. The classic examples are the Fourier and wavelet transforms that\nenjoy numerically efficient implementations (FFT and FWT, respectively).\nUnfortunately, orthonormal transformations are in general unstructured, and\ntherefore they do not enjoy low computational complexity properties. In this\npaper, based on Householder reflectors, we introduce a class of orthonormal\nmatrices that are numerically efficient to manipulate: we control the\ncomplexity of matrix-vector multiplications with these matrices using a given\nparameter. We provide numerical algorithms that approximate any orthonormal or\nsymmetric transform with a new orthonormal or symmetric structure made up of\nproducts of a given number of Householder reflectors. We show analyses and\nnumerical evidence to highlight the accuracy of the proposed approximations and\nprovide an application to the case of learning fast Mahanalobis distance metric\ntransformations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 11:34:16 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 12:19:05 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Rusu", "Cristian", ""]]}, {"id": "1811.07627", "submitter": "Samuel Murray", "authors": "Samuel Murray and Hedvig Kjellstr\\\"om", "title": "Mixed Likelihood Gaussian Process Latent Variable Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Mixed Likelihood Gaussian process latent variable model\n(GP-LVM), capable of modeling data with attributes of different types. The\nstandard formulation of GP-LVM assumes that each observation is drawn from a\nGaussian distribution, which makes the model unsuited for data with e.g.\ncategorical or nominal attributes. Our model, for which we use a sampling based\nvariational inference, instead assumes a separate likelihood for each observed\ndimension. This formulation results in more meaningful latent representations,\nand give better predictive performance for real world data with dimensions of\ndifferent types.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 11:39:17 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Murray", "Samuel", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "1811.07630", "submitter": "Elizaveta Logacheva", "authors": "Pavel Ostyakov, Roman Suvorov, Elizaveta Logacheva, Oleg Khomenko,\n  Sergey I. Nikolenko", "title": "SEIGAN: Towards Compositional Image Generation by Simultaneously\n  Learning to Segment, Enhance, and Inpaint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to image manipulation and understanding by\nsimultaneously learning to segment object masks, paste objects to another\nbackground image, and remove them from original images. For this purpose, we\ndevelop a novel generative model for compositional image generation, SEIGAN\n(Segment-Enhance-Inpaint Generative Adversarial Network), which learns these\nthree operations together in an adversarial architecture with additional cycle\nconsistency losses. To train, SEIGAN needs only bounding box supervision and\ndoes not require pairing or ground truth masks. SEIGAN produces better\ngenerated images (evaluated by human assessors) than other approaches and\nproduces high-quality segmentation masks, improving over other adversarially\ntrained approaches and getting closer to the results of fully supervised\ntraining.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 11:50:20 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 19:33:07 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Ostyakov", "Pavel", ""], ["Suvorov", "Roman", ""], ["Logacheva", "Elizaveta", ""], ["Khomenko", "Oleg", ""], ["Nikolenko", "Sergey I.", ""]]}, {"id": "1811.07672", "submitter": "Ryad Benjamin Benosman", "authors": "Marco Macanovic, Fabian Chersi, Felix Rutard, Sio-Hoi Ieng, Ryad\n  Benosman", "title": "When Conventional machine learning meets neuromorphic engineering: Deep\n  Temporal Networks (DTNets) a machine learning frawmework allowing to operate\n  on Events and Frames and implantable on Tensor Flow Like Hardware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We introduce in this paper the principle of Deep Temporal Networks that allow\nto add time to convolutional networks by allowing deep integration principles\nnot only using spatial information but also increasingly large temporal window.\nThe concept can be used for conventional image inputs but also event based\ndata. Although inspired by the architecture of brain that inegrates information\nover increasingly larger spatial but also temporal scales it can operate on\nconventional hardware using existing architectures. We introduce preliminary\nresults to show the efficiency of the method. More in-depth results and\nanalysis will be reported soon!\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 13:32:30 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Macanovic", "Marco", ""], ["Chersi", "Fabian", ""], ["Rutard", "Felix", ""], ["Ieng", "Sio-Hoi", ""], ["Benosman", "Ryad", ""]]}, {"id": "1811.07674", "submitter": "Wenfang Lin", "authors": "Wenfang Lin, Zhenyu Wu, Yang Ji", "title": "An Adaptive Oversampling Learning Method for Class-Imbalanced Fault\n  Diagnostics and Prognostics", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven fault diagnostics and prognostics suffers from class-imbalance\nproblem in industrial systems and it raises challenges to common machine\nlearning algorithms as it becomes difficult to learn the features of the\nminority class samples. Synthetic oversampling methods are commonly used to\ntackle these problems by generating the minority class samples to balance the\ndistributions between majority and minority classes. However, many of\noversampling methods are inappropriate that they cannot generate effective and\nuseful minority class samples according to different distributions of data,\nwhich further complicate the process of learning samples. Thus, this paper\nproposes a novel adaptive oversampling technique: EM-based Weighted Minority\nOversampling TEchnique (EWMOTE) for industrial fault diagnostics and\nprognostics. The methods comprises a weighted minority sampling strategy to\nidentify hard-to-learn informative minority fault samples and Expectation\nMaximization (EM) based imputation algorithm to generate fault samples. To\nvalidate the performance of the proposed methods, experiments are conducted in\ntwo real datasets. The results show that the method could achieve better\nperformance on not only binary class, but multi-class imbalance learning task\nin different imbalance ratios than other oversampling-based baseline models.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 13:33:07 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Lin", "Wenfang", ""], ["Wu", "Zhenyu", ""], ["Ji", "Yang", ""]]}, {"id": "1811.07684", "submitter": "Alice Coucke", "authors": "Alice Coucke, Mohammed Chlieh, Thibault Gisselbrecht, David Leroy,\n  Mathieu Poumeyrol, Thibaut Lavril", "title": "Efficient keyword spotting using dilated convolutions and gating", "comments": "Accepted for publication to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the application of end-to-end stateless temporal modeling to\nsmall-footprint keyword spotting as opposed to recurrent networks that model\nlong-term temporal dependencies using internal states. We propose a model\ninspired by the recent success of dilated convolutions in sequence modeling\napplications, allowing to train deeper architectures in resource-constrained\nconfigurations. Gated activations and residual connections are also added,\nfollowing a similar configuration to WaveNet. In addition, we apply a custom\ntarget labeling that back-propagates loss from specific frames of interest,\ntherefore yielding higher accuracy and only requiring to detect the end of the\nkeyword. Our experimental results show that our model outperforms a max-pooling\nloss trained recurrent neural network using LSTM cells, with a significant\ndecrease in false rejection rate. The underlying dataset - \"Hey Snips\"\nutterances recorded by over 2.2K different speakers - has been made publicly\navailable to establish an open reference for wake-word detection.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 13:51:10 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 16:21:04 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Coucke", "Alice", ""], ["Chlieh", "Mohammed", ""], ["Gisselbrecht", "Thibault", ""], ["Leroy", "David", ""], ["Poumeyrol", "Mathieu", ""], ["Lavril", "Thibaut", ""]]}, {"id": "1811.07691", "submitter": "Zheng Lian", "authors": "Zheng Lian, Ya Li, Jianhua Tao, Jian Huang", "title": "Improving speech emotion recognition via Transformer-based Predictive\n  Coding through transfer learning", "comments": "I have submitted a new version to arXiv:1910.13806. I forget to\n  choose to replace the old version, but submitted a new one. It's my mistake", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I have submitted a new version to arXiv:1910.13806. I forget to choose to\nreplace the old version, but submitted a new one. It's my mistake.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 15:39:13 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 01:15:22 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Lian", "Zheng", ""], ["Li", "Ya", ""], ["Tao", "Jianhua", ""], ["Huang", "Jian", ""]]}, {"id": "1811.07698", "submitter": "Irene Unceta", "authors": "Irene Unceta, Jordi Nin, Oriol Pujol", "title": "Towards Global Explanations for Credit Risk Scoring", "comments": "FEAP-AI4Fin 2018 : NIPS 2018 Worksop on Challenges and Opportunities\n  for AI in Financial Services: the Impact of Fairness, Explainability,\n  Accuracy, and Privacy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a method to obtain global explanations for trained\nblack-box classifiers by sampling their decision function to learn alternative\ninterpretable models. The envisaged approach provides a unified solution to\napproximate non-linear decision boundaries with simpler classifiers while\nretaining the original classification accuracy. We use a private residential\nmortgage default dataset as a use case to illustrate the feasibility of this\napproach to ensure the decomposability of attributes during pre-processing.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 14:12:59 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 08:06:40 GMT"}, {"version": "v3", "created": "Fri, 23 Nov 2018 08:24:02 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Unceta", "Irene", ""], ["Nin", "Jordi", ""], ["Pujol", "Oriol", ""]]}, {"id": "1811.07707", "submitter": "Jialin Song", "authors": "Jialin Song, Yury S. Tokpanov, Yuxin Chen, Dagny Fleischman, Kate T.\n  Fountaine, Harry A. Atwater, Yisong Yue", "title": "Optimizing Photonic Nanostructures via Multi-fidelity Gaussian Processes", "comments": "NIPS 2018 Workshop on Machine Learning for Molecules and Materials.\n  arXiv admin note: substantial text overlap with arXiv:1811.00755", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply numerical methods in combination with finite-difference-time-domain\n(FDTD) simulations to optimize transmission properties of plasmonic mirror\ncolor filters using a multi-objective figure of merit over a five-dimensional\nparameter space by utilizing novel multi-fidelity Gaussian processes approach.\nWe compare these results with conventional derivative-free global search\nalgorithms, such as (single-fidelity) Gaussian Processes optimization scheme,\nand Particle Swarm Optimization---a commonly used method in nanophotonics\ncommunity, which is implemented in Lumerical commercial photonics software. We\ndemonstrate the performance of various numerical optimization approaches on\nseveral pre-collected real-world datasets and show that by properly trading off\nexpensive information sources with cheap simulations, one can more effectively\noptimize the transmission properties with a fixed budget.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 22:26:55 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Song", "Jialin", ""], ["Tokpanov", "Yury S.", ""], ["Chen", "Yuxin", ""], ["Fleischman", "Dagny", ""], ["Fountaine", "Kate T.", ""], ["Atwater", "Harry A.", ""], ["Yue", "Yisong", ""]]}, {"id": "1811.07727", "submitter": "Ping Luo", "authors": "Ping Luo, Zhanglin Peng, Jiamin Ren, Ruimao Zhang", "title": "Do Normalization Layers in a Deep ConvNet Really Need to Be Distinct?", "comments": "Preprint. Work in Progress. 14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yes, they do. This work investigates a perspective for deep learning: whether\ndifferent normalization layers in a ConvNet require different normalizers. This\nis the first step towards understanding this phenomenon. We allow each\nconvolutional layer to be stacked before a switchable normalization (SN) that\nlearns to choose a normalizer from a pool of normalization methods. Through\nsystematic experiments in ImageNet, COCO, Cityscapes, and ADE20K, we answer\nthree questions: (a) Is it useful to allow each normalization layer to select\nits own normalizer? (b) What impacts the choices of normalizers? (c) Do\ndifferent tasks and datasets prefer different normalizers? Our results suggest\nthat (1) using distinct normalizers improves both learning and generalization\nof a ConvNet; (2) the choices of normalizers are more related to depth and\nbatch size, but less relevant to parameter initialization, learning rate decay,\nand solver; (3) different tasks and datasets have different behaviors when\nlearning to select normalizers.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 14:36:25 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Luo", "Ping", ""], ["Peng", "Zhanglin", ""], ["Ren", "Jiamin", ""], ["Zhang", "Ruimao", ""]]}, {"id": "1811.07745", "submitter": "Ariel Keselman", "authors": "Ariel Keselman, Sergey Ten, Adham Ghazali, Majed Jubeh", "title": "Reinforcement Learning with A* and a Deep Heuristic", "comments": "6 pages 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A* is a popular path-finding algorithm, but it can only be applied to those\ndomains where a good heuristic function is known. Inspired by recent methods\ncombining Deep Neural Networks (DNNs) and trees, this study demonstrates how to\ntrain a heuristic represented by a DNN and combine it with A*. This new\nalgorithm which we call aleph-star can be used efficiently in domains where the\ninput to the heuristic could be processed by a neural network. We compare\naleph-star to N-Step Deep Q-Learning (DQN Mnih et al. 2013) in a driving\nsimulation with pixel-based input, and demonstrate significantly better\nperformance in this scenario.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 15:15:18 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Keselman", "Ariel", ""], ["Ten", "Sergey", ""], ["Ghazali", "Adham", ""], ["Jubeh", "Majed", ""]]}, {"id": "1811.07746", "submitter": "Kiran Karra", "authors": "Kiran Karra, Samarth Swarup, Justus Graham", "title": "An Empirical Assessment of the Complexity and Realism of Synthetic\n  Social Contact Networks", "comments": "8 pages, 6 figures, accepted at GTA3 2018 in Conjunction with the\n  2018 IEEE Big Data Conference", "journal-ref": null, "doi": "10.1109/BigData.2018.8622199", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use multiple measures of graph complexity to evaluate the realism of\nsynthetically-generated networks of human activity, in comparison with several\nstylized network models as well as a collection of empirical networks from the\nliterature. The synthetic networks are generated by integrating data about\nhuman populations from several sources, including the Census, transportation\nsurveys, and geographical data. The resulting networks represent an\napproximation of daily or weekly human interaction. Our results indicate that\nthe synthetically generated graphs according to our methodology are closer to\nthe real world graphs, as measured across multiple structural measures, than a\nrange of stylized graphs generated using common network models from the\nliterature.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 17:48:44 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 21:37:51 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Karra", "Kiran", ""], ["Swarup", "Samarth", ""], ["Graham", "Justus", ""]]}, {"id": "1811.07747", "submitter": "Jinwei Zhao", "authors": "Jinwei Zhao, Qizhou Wang, Yufei Wang, Xinhong Hei, Yu Liu", "title": "How far from automatically interpreting deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning researchers have focused on how to find the\ninterpretability behind deep learning models. However, today cognitive\ncompetence of human has not completely covered the deep learning model. In\nother words, there is a gap between the deep learning model and the cognitive\nmode. How to evaluate and shrink the cognitive gap is a very important issue.\nIn this paper, the interpretability evaluation, the relationship between the\ngeneralization performance and the interpretability of the model and the method\nfor improving the interpretability are concerned. A universal learning\nframework is put forward to solve the equilibrium problem between the two\nperformances. The uniqueness of solution of the problem is proved and condition\nof unique solution is obtained. Probability upper bound of the sum of the two\nperformances is analyzed.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 15:25:02 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Zhao", "Jinwei", ""], ["Wang", "Qizhou", ""], ["Wang", "Yufei", ""], ["Hei", "Xinhong", ""], ["Liu", "Yu", ""]]}, {"id": "1811.07755", "submitter": "Ritchie Zhao", "authors": "Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang", "title": "Building Efficient Deep Neural Networks with Unitary Group Convolutions", "comments": "8 pages, 2 figures", "journal-ref": "Computer Vision and Pattern Recognition (CVPR) 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose unitary group convolutions (UGConvs), a building block for CNNs\nwhich compose a group convolution with unitary transforms in feature space to\nlearn a richer set of representations than group convolution alone. UGConvs\ngeneralize two disparate ideas in CNN architecture, channel shuffling (i.e.\nShuffleNet) and block-circulant networks (i.e. CirCNN), and provide unifying\ninsights that lead to a deeper understanding of each technique. We\nexperimentally demonstrate that dense unitary transforms can outperform channel\nshuffling in DNN accuracy. On the other hand, different dense transforms\nexhibit comparable accuracy performance. Based on these observations we propose\nHadaNet, a UGConv network using Hadamard transforms. HadaNets achieve similar\naccuracy to circulant networks with lower computation complexity, and better\naccuracy than ShuffleNets with the same number of parameters and floating-point\nmultiplies.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 15:48:12 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 06:28:33 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Zhao", "Ritchie", ""], ["Hu", "Yuwei", ""], ["Dotzel", "Jordan", ""], ["De Sa", "Christopher", ""], ["Zhang", "Zhiru", ""]]}, {"id": "1811.07763", "submitter": "Raphael F\\'eraud", "authors": "Rapha\\\"el F\\'eraud, R\\'eda Alami, Romain Laroche", "title": "Decentralized Exploration in Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the decentralized exploration problem: a set of players\ncollaborate to identify the best arm by asynchronously interacting with the\nsame stochastic environment. The objective is to insure privacy in the best arm\nidentification problem between asynchronous, collaborative, and thrifty\nplayers. In the context of a digital service, we advocate that this\ndecentralized approach allows a good balance between the interests of users and\nthose of service providers: the providers optimize their services, while\nprotecting the privacy of the users and saving resources. We define the privacy\nlevel as the amount of information an adversary could infer by intercepting the\nmessages concerning a single user. We provide a generic algorithm Decentralized\nElimination, which uses any best arm identification algorithm as a subroutine.\nWe prove that this algorithm insures privacy, with a low communication cost,\nand that in comparison to the lower bound of the best arm identification\nproblem, its sample complexity suffers from a penalty depending on the inverse\nof the probability of the most frequent players. Then, thanks to the genericity\nof the approach, we extend the proposed algorithm to the non-stationary\nbandits. Finally, experiments illustrate and complete the analysis.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 15:56:27 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 14:31:53 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 09:11:24 GMT"}, {"version": "v4", "created": "Mon, 13 May 2019 08:52:10 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["F\u00e9raud", "Rapha\u00ebl", ""], ["Alami", "R\u00e9da", ""], ["Laroche", "Romain", ""]]}, {"id": "1811.07765", "submitter": "Aaron Roth", "authors": "Seth Neel, Aaron Roth, Zhiwei Steven Wu", "title": "How to Use Heuristics for Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop theory for using heuristics to solve computationally hard problems\nin differential privacy. Heuristic approaches have enjoyed tremendous success\nin machine learning, for which performance can be empirically evaluated.\nHowever, privacy guarantees cannot be evaluated empirically, and must be proven\n--- without making heuristic assumptions. We show that learning problems over\nbroad classes of functions can be solved privately and efficiently, assuming\nthe existence of a non-private oracle for solving the same problem. Our first\nalgorithm yields a privacy guarantee that is contingent on the correctness of\nthe oracle. We then give a reduction which applies to a class of heuristics\nwhich we call certifiable, which allows us to convert oracle-dependent privacy\nguarantees to worst-case privacy guarantee that hold even when the heuristic\nstanding in for the oracle might fail in adversarial ways. Finally, we consider\na broad class of functions that includes most classes of simple boolean\nfunctions studied in the PAC learning literature, including conjunctions,\ndisjunctions, parities, and discrete halfspaces. We show that there is an\nefficient algorithm for privately constructing synthetic data for any such\nclass, given a non-private learning oracle. This in particular gives the first\noracle-efficient algorithm for privately generating synthetic data for\ncontingency tables. The most intriguing question left open by our work is\nwhether or not every problem that can be solved differentially privately can be\nprivately solved with an oracle-efficient algorithm. While we do not resolve\nthis, we give a barrier result that suggests that any generic oracle-efficient\nreduction must fall outside of a natural class of algorithms (which includes\nthe algorithms given in this paper).\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 15:57:08 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Neel", "Seth", ""], ["Roth", "Aaron", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1811.07768", "submitter": "Edgard Chammas", "authors": "Edgard Chammas, Chafic Mokbel, Laurence Likforman-Sulem", "title": "Handwriting Recognition of Historical Documents with few labeled data", "comments": null, "journal-ref": null, "doi": "10.1109/DAS.2018.15", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical documents present many challenges for offline handwriting\nrecognition systems, among them, the segmentation and labeling steps. Carefully\nannotated textlines are needed to train an HTR system. In some scenarios,\ntranscripts are only available at the paragraph level with no text-line\ninformation. In this work, we demonstrate how to train an HTR system with few\nlabeled data. Specifically, we train a deep convolutional recurrent neural\nnetwork (CRNN) system on only 10% of manually labeled text-line data from a\ndataset and propose an incremental training procedure that covers the rest of\nthe data. Performance is further increased by augmenting the training set with\nspecially crafted multiscale data. We also propose a model-based normalization\nscheme which considers the variability in the writing scale at the recognition\nphase. We apply this approach to the publicly available READ dataset. Our\nsystem achieved the second best result during the ICDAR2017 competition.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 23:21:12 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Chammas", "Edgard", ""], ["Mokbel", "Chafic", ""], ["Likforman-Sulem", "Laurence", ""]]}, {"id": "1811.07769", "submitter": "Ilke Demir", "authors": "Ilke Demir, Ramesh Raskar", "title": "Addressing the Invisible: Street Address Generation for Developing\n  Countries with Deep Learning", "comments": "Presented at NIPS 2018 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More than half of the world's roads lack adequate street addressing systems.\nLack of addresses is even more visible in daily lives of people in developing\ncountries. We would like to object to the assumption that having an address is\na luxury, by proposing a generative address design that maps the world in\naccordance with streets. The addressing scheme is designed considering several\ntraditional street addressing methodologies employed in the urban development\nscenarios around the world. Our algorithm applies deep learning to extract\nroads from satellite images, converts the road pixel confidences into a road\nnetwork, partitions the road network to find neighborhoods, and labels the\nregions, roads, and address units using graph- and proximity-based algorithms.\nWe present our results on a sample US city, and several developing cities,\ncompare travel times of users using current ad hoc and new complete addresses,\nand contrast our addressing solution to current industrial and open geocoding\nalternatives.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 07:34:04 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Demir", "Ilke", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1811.07770", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Stefanos Zafeiriou", "title": "Aff-Wild2: Extending the Aff-Wild Database for Affect Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic understanding of human affect using visual signals is a problem\nthat has attracted significant interest over the past 20 years. However, human\nemotional states are quite complex. To appraise such states displayed in\nreal-world settings, we need expressive emotional descriptors that are capable\nof capturing and describing this complexity. The circumplex model of affect,\nwhich is described in terms of valence (i.e., how positive or negative is an\nemotion) and arousal (i.e., power of the activation of the emotion), can be\nused for this purpose. Recent progress in the emotion recognition domain has\nbeen achieved through the development of deep neural architectures and the\navailability of very large training databases. To this end, Aff-Wild has been\nthe first large-scale \"in-the-wild\" database, containing around 1,200,000\nframes. In this paper, we build upon this database, extending it with 260 more\nsubjects and 1,413,000 new video frames. We call the union of Aff-Wild with the\nadditional data, Aff-Wild2. The videos are downloaded from Youtube and have\nlarge variations in pose, age, illumination conditions, ethnicity and\nprofession. Both database-specific as well as cross-database experiments are\nperformed in this paper, by utilizing the Aff-Wild2, along with the RECOLA\ndatabase. The developed deep neural architectures are based on the joint\ntraining of state-of-the-art convolutional and recurrent neural networks with\nattention mechanism; thus exploiting both the invariant properties of\nconvolutional features, while modeling temporal dynamics that arise in human\nbehaviour via the recurrent layers. The obtained results show premise for\nutilization of the extended Aff-Wild, as well as of the developed deep neural\narchitectures for visual analysis of human behaviour in terms of continuous\nemotion dimensions.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 01:57:15 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 23:44:20 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1811.07771", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Stefanos Zafeiriou", "title": "A Multi-Task Learning & Generation Framework: Valence-Arousal, Action\n  Units & Primary Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years many research efforts have been devoted to the field\nof affect analysis. Various approaches have been proposed for: i) discrete\nemotion recognition in terms of the primary facial expressions; ii) emotion\nanalysis in terms of facial Action Units (AUs), assuming a fixed expression\nintensity; iii) dimensional emotion analysis, in terms of valence and arousal\n(VA). These approaches can only be effective, if they are developed using\nlarge, appropriately annotated databases, showing behaviors of people\nin-the-wild, i.e., in uncontrolled environments. Aff-Wild has been the first,\nlarge-scale, in-the-wild database (including around 1,200,000 frames of 300\nvideos), annotated in terms of VA. In the vast majority of existing emotion\ndatabases, their annotation is limited to either primary expressions, or\nvalence-arousal, or action units. In this paper, we first annotate a part\n(around $234,000$ frames) of the Aff-Wild database in terms of $8$ AUs and\nanother part (around $288,000$ frames) in terms of the $7$ basic emotion\ncategories, so that parts of this database are annotated in terms of VA, as\nwell as AUs, or primary expressions. Then, we set up and tackle multi-task\nlearning for emotion recognition, as well as for facial image generation.\nMulti-task learning is performed using: i) a deep neural network with shared\nhidden layers, which learns emotional attributes by exploiting their\ninter-dependencies; ii) a discriminator of a generative adversarial network\n(GAN). On the other hand, image generation is implemented through the generator\nof the GAN. For these two tasks, we carefully design loss functions that fit\nthe examined set-up. Experiments are presented which illustrate the good\nperformance of the proposed approach when applied to the new annotated parts of\nthe Aff-Wild database.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 15:40:23 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 23:39:14 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1811.07791", "submitter": "David Fuentes-Jimenez", "authors": "David Fuentes-Jimenez, David Casillas-Perez, Daniel Pizarro, Toby\n  Collins and Adrien Bartoli", "title": "Deep Shape-from-Template: Wide-Baseline, Dense and Fast Registration and\n  Deformable Reconstruction from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Deep Shape-from-Template (DeepSfT), a novel Deep Neural Network\n(DNN) method for solving real-time automatic registration and 3D reconstruction\nof a deformable object viewed in a single monocular image.DeepSfT advances the\nstate-of-the-art in various aspects. Compared to existing DNN SfT methods, it\nis the first fully convolutional real-time approach that handles an arbitrary\nobject geometry, topology and surface representation. It also does not require\nground truth registration with real data and scales well to very complex object\nmodels with large numbers of elements. Compared to previous non-DNN SfT\nmethods, it does not involve numerical optimization at run-time, and is a\ndense, wide-baseline solution that does not demand, and does not suffer from,\nfeature-based matching. It is able to process a single image with significant\ndeformation and viewpoint changes, and handles well the core challenges of\nocclusions, weak texture and blur. DeepSfT is based on residual encoder-decoder\nstructures and refining blocks. It is trained end-to-end with a novel\ncombination of supervised learning from simulated renderings of the object\nmodel and semi-supervised automatic fine-tuning using real data captured with a\nstandard RGB-D camera. The cameras used for fine-tuning and run-time can be\ndifferent, making DeepSfT practical for real-world use. We show that DeepSfT\nsignificantly outperforms state-of-the-art wide-baseline approaches for\nnon-trivial templates, with quantitative and qualitative evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 16:39:27 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 15:13:33 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 03:12:50 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Fuentes-Jimenez", "David", ""], ["Casillas-Perez", "David", ""], ["Pizarro", "Daniel", ""], ["Collins", "Toby", ""], ["Bartoli", "Adrien", ""]]}, {"id": "1811.07799", "submitter": "Kaiqing Zhang", "authors": "Kaiqing Zhang, Yang Liu, Ji Liu, Mingyan Liu, Tamer Ba\\c{s}ar", "title": "Distributed Learning of Average Belief Over Networks Using Sequential\n  Observations", "comments": "Accepted to Automatica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of distributed learning of average belief\nwith sequential observations, in which a network of $n>1$ agents aim to reach a\nconsensus on the average value of their beliefs, by exchanging information only\nwith their neighbors. Each agent has sequentially arriving samples of its\nbelief in an online manner. The neighbor relationships among the $n$ agents are\ndescribed by a graph which is possibly time-varying, whose vertices correspond\nto agents and whose edges depict neighbor relationships. Two distributed online\nalgorithms are introduced for undirected and directed graphs, which are both\nshown to converge to the average belief almost surely. Moreover, the sequences\ngenerated by both algorithms are shown to reach consensus with an $O(1/t)$ rate\nwith high probability, where $t$ is the number of iterations. For undirected\ngraphs, the corresponding algorithm is modified for the case with quantized\ncommunication and limited precision of the division operation. It is shown that\nthe modified algorithm causes all $n$ agents to either reach a quantized\nconsensus or enter a small neighborhood around the average of their beliefs.\nNumerical simulations are then provided to corroborate the theoretical results.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 16:55:30 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Zhang", "Kaiqing", ""], ["Liu", "Yang", ""], ["Liu", "Ji", ""], ["Liu", "Mingyan", ""], ["Ba\u015far", "Tamer", ""]]}, {"id": "1811.07819", "submitter": "Dibya Ghosh", "authors": "Dibya Ghosh, Abhishek Gupta, Sergey Levine", "title": "Learning Actionable Representations with Goal-Conditioned Policies", "comments": "To be presented at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning is a central challenge across a range of machine\nlearning areas. In reinforcement learning, effective and functional\nrepresentations have the potential to tremendously accelerate learning progress\nand solve more challenging problems. Most prior work on representation learning\nhas focused on generative approaches, learning representations that capture all\nunderlying factors of variation in the observation space in a more disentangled\nor well-ordered manner. In this paper, we instead aim to learn functionally\nsalient representations: representations that are not necessarily complete in\nterms of capturing all factors of variation in the observation space, but\nrather aim to capture those factors of variation that are important for\ndecision making -- that are \"actionable.\" These representations are aware of\nthe dynamics of the environment, and capture only the elements of the\nobservation that are necessary for decision making rather than all factors of\nvariation, without explicit reconstruction of the observation. We show how\nthese representations can be useful to improve exploration for sparse reward\nproblems, to enable long horizon hierarchical reinforcement learning, and as a\nstate representation for learning policies for downstream tasks. We evaluate\nour method on a number of simulated environments, and compare it to prior\nmethods for representation learning, exploration, and hierarchical\nreinforcement learning.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 17:30:36 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 06:44:13 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Ghosh", "Dibya", ""], ["Gupta", "Abhishek", ""], ["Levine", "Sergey", ""]]}, {"id": "1811.07821", "submitter": "Yihong Wu", "authors": "Jian Ding, Zongming Ma, Yihong Wu, Jiaming Xu", "title": "Efficient random graph matching via degree profiles", "comments": "Proof of Theorem 4 expanded and revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random graph matching refers to recovering the underlying vertex\ncorrespondence between two random graphs with correlated edges; a prominent\nexample is when the two random graphs are given by Erd\\H{o}s-R\\'{e}nyi graphs\n$G(n,\\frac{d}{n})$. This can be viewed as an average-case and noisy version of\nthe graph isomorphism problem. Under this model, the maximum likelihood\nestimator is equivalent to solving the intractable quadratic assignment\nproblem. This work develops an $\\tilde{O}(n d^2+n^2)$-time algorithm which\nperfectly recovers the true vertex correspondence with high probability,\nprovided that the average degree is at least $d = \\Omega(\\log^2 n)$ and the two\ngraphs differ by at most $\\delta = O( \\log^{-2}(n) )$ fraction of edges. For\ndense graphs and sparse graphs, this can be improved to $\\delta = O(\n\\log^{-2/3}(n) )$ and $\\delta = O( \\log^{-2}(d) )$ respectively, both in\npolynomial time. The methodology is based on appropriately chosen distance\nstatistics of the degree profiles (empirical distribution of the degrees of\nneighbors). Before this work, the best known result achieves $\\delta=O(1)$ and\n$n^{o(1)} \\leq d \\leq n^c$ for some constant $c$ with an $n^{O(\\log n)}$-time\nalgorithm \\cite{barak2018nearly} and $\\delta=\\tilde O((d/n)^4)$ and $d =\n\\tilde{\\Omega}(n^{4/5})$ with a polynomial-time algorithm\n\\cite{dai2018performance}.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 17:33:48 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 18:55:50 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ding", "Jian", ""], ["Ma", "Zongming", ""], ["Wu", "Yihong", ""], ["Xu", "Jiaming", ""]]}, {"id": "1811.07842", "submitter": "Bander Alsulami", "authors": "Bander Alsulami, Spiros Mancoridis", "title": "Behavioral Malware Classification using Convolutional Recurrent Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavioral malware detection aims to improve on the performance of static\nsignature-based techniques used by anti-virus systems, which are less effective\nagainst modern polymorphic and metamorphic malware. Behavioral malware\nclassification aims to go beyond the detection of malware by also identifying a\nmalware's family according to a naming scheme such as the ones used by\nanti-virus vendors. Behavioral malware classification techniques use run-time\nfeatures, such as file system or network activities, to capture the behavioral\ncharacteristic of running processes. The increasing volume of malware samples,\ndiversity of malware families, and the variety of naming schemes given to\nmalware samples by anti-virus vendors present challenges to behavioral malware\nclassifiers. We describe a behavioral classifier that uses a Convolutional\nRecurrent Neural Network and data from Microsoft Windows Prefetch files. We\ndemonstrate the model's improvement on the state-of-the-art using a large\ndataset of malware families and four major anti-virus vendor naming schemes.\nThe model is effective in classifying malware samples that belong to common and\nrare malware families and can incrementally accommodate the introduction of new\nmalware samples and families.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 18:02:10 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Alsulami", "Bander", ""], ["Mancoridis", "Spiros", ""]]}, {"id": "1811.07863", "submitter": "Manuel Gomez Rodriguez", "authors": "Khashayar Gatmiry and Manuel Gomez-Rodriguez", "title": "Non-submodular Function Maximization subject to a Matroid Constraint,\n  with Applications", "comments": "Added missing citations and changed strong submodularity ratio to\n  generalized curvature", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard greedy algorithm has been recently shown to enjoy approximation\nguarantees for constrained non-submodular nondecreasing set function\nmaximization. While these recent results allow to better characterize the\nempirical success of the greedy algorithm, they are only applicable to simple\ncardinality constraints. In this paper, we study the problem of maximizing a\nnon-submodular nondecreasing set function subject to a general matroid\nconstraint. We first show that the standard greedy algorithm offers an\napproximation factor of $\\frac{0.4 {\\gamma}^{2}}{\\sqrt{\\gamma r} + 1}$, where\n$\\gamma$ is the submodularity ratio of the function and $r$ is the rank of the\nmatroid. Then, we show that the same greedy algorithm offers a constant\napproximation factor of $(1 + 1/(1-\\alpha))^{-1}$, where $\\alpha$ is the\ngeneralized curvature of the function. In addition, we demonstrate that these\napproximation guarantees are applicable to several real-world applications in\nwhich the submodularity ratio and the generalized curvature can be bounded.\nFinally, we show that our greedy algorithm does achieve a competitive\nperformance in practice using a variety of experiments on synthetic and\nreal-world data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 18:31:21 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 22:23:02 GMT"}, {"version": "v3", "created": "Mon, 20 May 2019 10:09:26 GMT"}, {"version": "v4", "created": "Tue, 21 May 2019 06:27:47 GMT"}, {"version": "v5", "created": "Tue, 8 Oct 2019 15:57:22 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Gatmiry", "Khashayar", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "1811.07871", "submitter": "Jan Leike", "authors": "Jan Leike and David Krueger and Tom Everitt and Miljan Martic and\n  Vishal Maini and Shane Legg", "title": "Scalable agent alignment via reward modeling: a research direction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One obstacle to applying reinforcement learning algorithms to real-world\nproblems is the lack of suitable reward functions. Designing such reward\nfunctions is difficult in part because the user only has an implicit\nunderstanding of the task objective. This gives rise to the agent alignment\nproblem: how do we create agents that behave in accordance with the user's\nintentions? We outline a high-level research direction to solve the agent\nalignment problem centered around reward modeling: learning a reward function\nfrom interaction with the user and optimizing the learned reward function with\nreinforcement learning. We discuss the key challenges we expect to face when\nscaling reward modeling to complex and general domains, concrete approaches to\nmitigate these challenges, and ways to establish trust in the resulting agents.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 18:48:04 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Leike", "Jan", ""], ["Krueger", "David", ""], ["Everitt", "Tom", ""], ["Martic", "Miljan", ""], ["Maini", "Vishal", ""], ["Legg", "Shane", ""]]}, {"id": "1811.07882", "submitter": "John Co-Reyes", "authors": "John D. Co-Reyes, Abhishek Gupta, Suvansh Sanjeev, Nick Altieri, Jacob\n  Andreas, John DeNero, Pieter Abbeel, Sergey Levine", "title": "Guiding Policies with Language via Meta-Learning", "comments": "Accepted at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavioral skills or policies for autonomous agents are conventionally\nlearned from reward functions, via reinforcement learning, or from\ndemonstrations, via imitation learning. However, both modes of task\nspecification have their disadvantages: reward functions require manual\nengineering, while demonstrations require a human expert to be able to actually\nperform the task in order to generate the demonstration. Instruction following\nfrom natural language instructions provides an appealing alternative: in the\nsame way that we can specify goals to other humans simply by speaking or\nwriting, we would like to be able to specify tasks for our machines. However, a\nsingle instruction may be insufficient to fully communicate our intent or, even\nif it is, may be insufficient for an autonomous agent to actually understand\nhow to perform the desired task. In this work, we propose an interactive\nformulation of the task specification problem, where iterative language\ncorrections are provided to an autonomous agent, guiding it in acquiring the\ndesired skill. Our proposed language-guided policy learning algorithm can\nintegrate an instruction and a sequence of corrections to acquire new skills\nvery quickly. In our experiments, we show that this method can enable a policy\nto follow instructions and corrections for simulated navigation and\nmanipulation tasks, substantially outperforming direct, non-interactive\ninstruction following.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 18:58:42 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 18:54:15 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Co-Reyes", "John D.", ""], ["Gupta", "Abhishek", ""], ["Sanjeev", "Suvansh", ""], ["Altieri", "Nick", ""], ["Andreas", "Jacob", ""], ["DeNero", "John", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1811.07886", "submitter": "Hai Leong Chieu Dr.", "authors": "Jing Lim, Joshua Wong, Minn Xuan Wong, Lee Han Eric Tan, Hai Leong\n  Chieu, Davin Choo, Neng Kai Nigel Neo", "title": "Chemical Structure Elucidation from Mass Spectrometry by Matching\n  Substructures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemical structure elucidation is a serious bottleneck in analytical\nchemistry today. We address the problem of identifying an unknown chemical\nthreat given its mass spectrum and its chemical formula, a task which might\ntake well trained chemists several days to complete. Given a chemical formula,\nthere could be over a million possible candidate structures. We take a data\ndriven approach to rank these structures by using neural networks to predict\nthe presence of substructures given the mass spectrum, and matching these\nsubstructures to the candidate structures. Empirically, we evaluate our\napproach on a data set of chemical agents built for unknown chemical threat\nidentification. We show that our substructure classifiers can attain over 90%\nmicro F1-score, and we can find the correct structure among the top 20\ncandidates in 88% and 71% of test cases for two compound classes.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 16:24:31 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Lim", "Jing", ""], ["Wong", "Joshua", ""], ["Wong", "Minn Xuan", ""], ["Tan", "Lee Han Eric", ""], ["Chieu", "Hai Leong", ""], ["Choo", "Davin", ""], ["Neo", "Neng Kai Nigel", ""]]}, {"id": "1811.07896", "submitter": "Shishira Maiya", "authors": "Shishira R Maiya and Sudharshan Chandra Babu", "title": "Slum Segmentation and Change Detection : A Deep Learning Approach", "comments": "Presented at NIPS 2018 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More than one billion people live in slums around the world. In some\ndeveloping countries, slum residents make up for more than half of the\npopulation and lack reliable sanitation services, clean water, electricity,\nother basic services. Thus, slum rehabilitation and improvement is an important\nglobal challenge, and a significant amount of effort and resources have been\nput into this endeavor. These initiatives rely heavily on slum mapping and\nmonitoring, and it is essential to have robust and efficient methods for\nmapping and monitoring existing slum settlements. In this work, we introduce an\napproach to segment and map individual slums from satellite imagery, leveraging\nregional convolutional neural networks for instance segmentation using transfer\nlearning. In addition, we also introduce a method to perform change detection\nand monitor slum change over time. We show that our approach effectively learns\nslum shape and appearance, and demonstrates strong quantitative results,\nresulting in a maximum AP of 80.0.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 15:45:06 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Maiya", "Shishira R", ""], ["Babu", "Sudharshan Chandra", ""]]}, {"id": "1811.07939", "submitter": "Xiaokang Zhang", "authors": "Xiaokang Zhang, Inge Jonassen", "title": "EFSIS: Ensemble Feature Selection Integrating Stability", "comments": "20 pages, 3 figures", "journal-ref": "2019 IEEE International Conference on Bioinformatics and\n  Biomedicine (BIBM), San Diego, CA, USA, 2019, pp. 2792-2798", "doi": "10.1109/BIBM47256.2019.8983310", "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble learning that can be used to combine the predictions from multiple\nlearners has been widely applied in pattern recognition, and has been reported\nto be more robust and accurate than the individual learners. This ensemble\nlogic has recently also been more applied in feature selection. There are\nbasically two strategies for ensemble feature selection, namely data\nperturbation and function perturbation. Data perturbation performs feature\nselection on data subsets sampled from the original dataset and then selects\nthe features consistently ranked highly across those data subsets. This has\nbeen found to improve both the stability of the selector and the prediction\naccuracy for a classifier. Function perturbation frees the user from having to\ndecide on the most appropriate selector for any given situation and works by\naggregating multiple selectors. This has been found to maintain or improve\nclassification performance. Here we propose a framework, EFSIS, combining these\ntwo strategies. Empirical results indicate that EFSIS gives both high\nprediction accuracy and stability.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 19:19:55 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Zhang", "Xiaokang", ""], ["Jonassen", "Inge", ""]]}, {"id": "1811.07957", "submitter": "Yuheng Bu", "authors": "Yuheng Bu, Jiaxun Lu and Venugopal V. Veeravalli", "title": "Model change detection with application to machine learning", "comments": "5 pages, ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model change detection is studied, in which there are two sets of samples\nthat are independently and identically distributed (i.i.d.) according to a\npre-change probabilistic model with parameter $\\theta$, and a post-change model\nwith parameter $\\theta'$, respectively. The goal is to detect whether the\nchange in the model is significant, i.e., whether the difference between the\npre-change parameter and the post-change parameter $\\|\\theta-\\theta'\\|_2$ is\nlarger than a pre-determined threshold $\\rho$. The problem is considered in a\nNeyman-Pearson setting, where the goal is to maximize the probability of\ndetection under a false alarm constraint. Since the generalized likelihood\nratio test (GLRT) is difficult to compute in this problem, we construct an\nempirical difference test (EDT), which approximates the GLRT and has low\ncomputational complexity. Moreover, we provide an approximation method to set\nthe threshold of the EDT to meet the false alarm constraint. Experiments with\nlinear regression and logistic regression are conducted to validate the\nproposed algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 20:06:18 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Bu", "Yuheng", ""], ["Lu", "Jiaxun", ""], ["Veeravalli", "Venugopal V.", ""]]}, {"id": "1811.07971", "submitter": "Jingcheng Liu", "authors": "Jingcheng Liu and Kunal Talwar", "title": "Private Selection from Private Candidates", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentially Private algorithms often need to select the best amongst many\ncandidate options. Classical works on this selection problem require that the\ncandidates' goodness, measured as a real-valued score function, does not change\nby much when one person's data changes. In many applications such as\nhyperparameter optimization, this stability assumption is much too strong. In\nthis work, we consider the selection problem under a much weaker stability\nassumption on the candidates, namely that the score functions are\ndifferentially private. Under this assumption, we present algorithms that are\nnear-optimal along the three relevant dimensions: privacy, utility and\ncomputational efficiency.\n  Our result can be seen as a generalization of the exponential mechanism and\nits existing generalizations. We also develop an online version of our\nalgorithm, that can be seen as a generalization of the sparse vector technique\nto this weaker stability assumption. We show how our results imply better\nalgorithms for hyperparameter selection in differentially private machine\nlearning, as well as for adaptive data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 20:48:42 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Liu", "Jingcheng", ""], ["Talwar", "Kunal", ""]]}, {"id": "1811.07985", "submitter": "Krit Pongpirul", "authors": "Seelwan Sathitratanacheewin (1 and 2) and Krit Pongpirul (1, 2, and 3)\n  ((1) Department of Preventive and Social Medicine, Faculty of Medicine,\n  Chulalongkorn University, Bangkok, Thailand, (2) Thai Health AI Foundation,\n  Bangkok, Thailand, (3) Department of International Health and Department of\n  Health, Behavior, and Society, Johns Hopkins Bloomberg School of Public\n  Health, Baltimore, MD, USA)", "title": "Deep Learning for Automated Classification of Tuberculosis-Related Chest\n  X-Ray: Dataset Specificity Limits Diagnostic Performance Generalizability", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Machine learning has been an emerging tool for various aspects of infectious\ndiseases including tuberculosis surveillance and detection. However, WHO\nprovided no recommendations on using computer-aided tuberculosis detection\nsoftware because of the small number of studies, methodological limitations,\nand limited generalizability of the findings. To quantify the generalizability\nof the machine-learning model, we developed a Deep Convolutional Neural Network\n(DCNN) model using a TB-specific CXR dataset of one population (National\nLibrary of Medicine Shenzhen No.3 Hospital) and tested it with non-TB-specific\nCXR dataset of another population (National Institute of Health Clinical\nCenters). The findings suggested that a supervised deep learning model\ndeveloped by using the training dataset from one population may not have the\nsame diagnostic performance in another population. Technical specification of\nCXR images, disease severity distribution, overfitting, and overdiagnosis\nshould be examined before implementation in other settings.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 16:32:42 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 08:44:47 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Sathitratanacheewin", "Seelwan", "", "1 and 2"], ["Pongpirul", "Krit", "", "1, 2, and 3"]]}, {"id": "1811.07988", "submitter": "Zhanli Chen", "authors": "Zhanli Chen, Rashid Ansari, Diana Wilkie", "title": "Automated Pain Detection from Facial Expressions using FACS: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial pain expression is an important modality for assessing pain,\nespecially when the patient's verbal ability to communicate is impaired. The\nfacial muscle-based action units (AUs), which are defined by the Facial Action\nCoding System (FACS), have been widely studied and are highly reliable as a\nmethod for detecting facial expressions (FE) including valid detection of pain.\nUnfortunately, FACS coding by humans is a very time-consuming task that makes\nits clinical use prohibitive. Significant progress on automated facial\nexpression recognition (AFER) has led to its numerous successful applications\nin FACS-based affective computing problems. However, only a handful of studies\nhave been reported on automated pain detection (APD), and its application in\nclinical settings is still far from a reality. In this paper, we review the\nprogress in research that has contributed to automated pain detection, with\nfocus on 1) the framework-level similarity between spontaneous AFER and APD\nproblems; 2) the evolution of system design including the recent development of\ndeep learning methods; 3) the strategies and considerations in developing a\nFACS-based pain detection framework from existing research; and 4) introduction\nof the most relevant databases that are available for AFER and APD studies. We\nattempt to present key considerations in extending a general AFER framework to\nan APD framework in clinical settings. In addition, the performance metrics are\nalso highlighted in evaluating an AFER or an APD system.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 22:59:24 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Chen", "Zhanli", ""], ["Ansari", "Rashid", ""], ["Wilkie", "Diana", ""]]}, {"id": "1811.07996", "submitter": "Abon Chaudhuri", "authors": "Abon Chaudhuri, Paolo Messina, Samrat Kokkula, Aditya Subramanian,\n  Abhinandan Krishnan, Shreyansh Gandhi, Alessandro Magnani, Venkatesh\n  Kandaswamy", "title": "A Smart System for Selection of Optimal Product Images in E-Commerce", "comments": "Accepted in IEEE Big Data Conference 2018 (Industry & Government\n  Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In e-commerce, content quality of the product catalog plays a key role in\ndelivering a satisfactory experience to the customers. In particular, visual\ncontent such as product images influences customers' engagement and purchase\ndecisions. With the rapid growth of e-commerce and the advent of artificial\nintelligence, traditional content management systems are giving way to\nautomated scalable systems. In this paper, we present a machine learning driven\nvisual content management system for extremely large e-commerce catalogs. For a\ngiven product, the system aggregates images from various suppliers, understands\nand analyzes them to produce a superior image set with optimal image count and\nquality, and arranges them in an order tailored to the demands of the\ncustomers. The system makes use of an array of technologies, ranging from deep\nlearning to traditional computer vision, at different stages of analysis. In\nthis paper, we outline how the system works and discuss the unique challenges\nrelated to applying machine learning techniques to real-world data from\ne-commerce domain. We emphasize how we tune state-of-the-art image\nclassification techniques to develop solutions custom made for a massive,\ndiverse, and constantly evolving product catalog. We also provide the details\nof how we measure the system's impact on various customer engagement metrics.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 02:35:48 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Chaudhuri", "Abon", ""], ["Messina", "Paolo", ""], ["Kokkula", "Samrat", ""], ["Subramanian", "Aditya", ""], ["Krishnan", "Abhinandan", ""], ["Gandhi", "Shreyansh", ""], ["Magnani", "Alessandro", ""], ["Kandaswamy", "Venkatesh", ""]]}, {"id": "1811.07999", "submitter": "Steven Kommrusch", "authors": "Steve Kommrusch and Louis-No\\\"el Pouchet", "title": "Synthetic Lung Nodule 3D Image Generation Using Autoencoders", "comments": "19 pages, 12 figures, full paper for work initially presented at\n  IJCAI 2018", "journal-ref": null, "doi": null, "report-no": "CS-18-101", "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges of using machine learning techniques with medical data\nis the frequent dearth of source image data on which to train. A representative\nexample is automated lung cancer diagnosis, where nodule images need to be\nclassified as suspicious or benign. In this work we propose an automatic\nsynthetic lung nodule image generator. Our 3D shape generator is designed to\naugment the variety of 3D images. Our proposed system takes root in autoencoder\ntechniques, and we provide extensive experimental characterization that\ndemonstrates its ability to produce quality synthetic images.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 21:51:38 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2018 05:58:20 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 05:58:21 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Kommrusch", "Steve", ""], ["Pouchet", "Louis-No\u00ebl", ""]]}, {"id": "1811.08004", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Shiyang Cheng and Maja Pantic and Stefanos\n  Zafeiriou", "title": "Photorealistic Facial Synthesis in the Dimensional Affect Space", "comments": "arXiv admin note: substantial text overlap with arXiv:1811.05027", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for synthesizing facial affect, which is\nbased on our annotating 600,000 frames of the 4DFAB database in terms of\nvalence and arousal. The input of this approach is a pair of these emotional\nstate descriptors and a neutral 2D image of a person to whom the corresponding\naffect will be synthesized. Given this target pair, a set of 3D facial meshes\nis selected, which is used to build a blendshape model and generate the new\nfacial affect. To synthesize the affect on the 2D neutral image, 3DMM fitting\nis performed and the reconstructed face is deformed to generate the target\nfacial expressions. Last, the new face is rendered into the original image.\nBoth qualitative and quantitative experimental studies illustrate the\ngeneration of realistic images, when the neutral image is sampled from a\nvariety of well known databases, such as the Aff-Wild, AFEW, Multi-PIE,\nAFEW-VA, BU-3DFE, Bosphorus.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 01:30:21 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Cheng", "Shiyang", ""], ["Pantic", "Maja", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1811.08006", "submitter": "Xiaogang Cheng", "authors": "Xiaogang Cheng, Bin Yang, Anders Hedman, Thomas Olofsson, Haibo Li,\n  Luc Van Gool", "title": "Non-invasive thermal comfort perception based on subtleness\n  magnification and deep learning for energy efficiency", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human thermal comfort measurement plays a critical role in giving feedback\nsignals for building energy efficiency. A non-invasive measuring method based\non subtleness magnification and deep learning (NIDL) was designed to achieve a\ncomfortable, energy efficient built environment. The method relies on skin\nfeature data, e.g., subtle motion and texture variation, and a 315-layer deep\nneural network for constructing the relationship between skin features and skin\ntemperature. A physiological experiment was conducted for collecting feature\ndata (1.44 million) and algorithm validation. The non-invasive measurement\nalgorithm based on a partly-personalized saturation temperature model (NIPST)\nwas used for algorithm performance comparisons. The results show that the mean\nerror and median error of the NIDL are 0.4834 Celsius and 0.3464 Celsius which\nis equivalent to accuracy improvements of 16.28% and 4.28%, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 18:24:48 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Cheng", "Xiaogang", ""], ["Yang", "Bin", ""], ["Hedman", "Anders", ""], ["Olofsson", "Thomas", ""], ["Li", "Haibo", ""], ["Van Gool", "Luc", ""]]}, {"id": "1811.08008", "submitter": "Daniel Gillick", "authors": "Daniel Gillick, Alessandro Presta, Gaurav Singh Tomar", "title": "End-to-End Retrieval in Continuous Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most text-based information retrieval (IR) systems index objects by words or\nphrases. These discrete systems have been augmented by models that use\nembeddings to measure similarity in continuous space. But continuous-space\nmodels are typically used just to re-rank the top candidates. We consider the\nproblem of end-to-end continuous retrieval, where standard approximate nearest\nneighbor (ANN) search replaces the usual discrete inverted index, and rely\nentirely on distances between learned embeddings. By training simple models\nspecifically for retrieval, with an appropriate model architecture, we improve\non a discrete baseline by 8% and 26% (MAP) on two similar-question retrieval\ntasks. We also discuss the problem of evaluation for retrieval systems, and\nshow how to modify existing pairwise similarity datasets for this purpose.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 22:23:59 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Gillick", "Daniel", ""], ["Presta", "Alessandro", ""], ["Tomar", "Gaurav Singh", ""]]}, {"id": "1811.08009", "submitter": "Istvan Fehervari", "authors": "Istvan Fehervari and Srikar Appalaraju", "title": "Scalable Logo Recognition using Proxies", "comments": "Accepted at IEEE WACV 2019, Hawaii USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logo recognition is the task of identifying and classifying logos. Logo\nrecognition is a challenging problem as there is no clear definition of a logo\nand there are huge variations of logos, brands and re-training to cover every\nvariation is impractical. In this paper, we formulate logo recognition as a\nfew-shot object detection problem. The two main components in our pipeline are\nuniversal logo detector and few-shot logo recognizer. The universal logo\ndetector is a class-agnostic deep object detector network which tries to learn\nthe characteristics of what makes a logo. It predicts bounding boxes on likely\nlogo regions. These logo regions are then classified by logo recognizer using\nnearest neighbor search, trained by triplet loss using proxies. We also\nannotated a first of its kind product logo dataset containing 2000 logos from\n295K images collected from Amazon called PL2K. Our pipeline achieves 97% recall\nwith 0.6 mAP on PL2K test dataset and state-of-the-art 0.565 mAP on the\npublicly available FlickrLogos-32 test set without fine-tuning.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 22:28:13 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Fehervari", "Istvan", ""], ["Appalaraju", "Srikar", ""]]}, {"id": "1811.08010", "submitter": "Hongyang Zhang", "authors": "Hongyang Zhang and Susu Xu and Jiantao Jiao and Pengtao Xie and Ruslan\n  Salakhutdinov and Eric P. Xing", "title": "Stackelberg GAN: Towards Provable Minimax Equilibrium via\n  Multi-Generator Architectures", "comments": "27 pages, 13 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of alleviating the instability issue in the GAN training\nprocedure via new architecture design. The discrepancy between the minimax and\nmaximin objective values could serve as a proxy for the difficulties that the\nalternating gradient descent encounters in the optimization of GANs. In this\nwork, we give new results on the benefits of multi-generator architecture of\nGANs. We show that the minimax gap shrinks to $\\epsilon$ as the number of\ngenerators increases with rate $\\widetilde{O}(1/\\epsilon)$. This improves over\nthe best-known result of $\\widetilde{O}(1/\\epsilon^2)$. At the core of our\ntechniques is a novel application of Shapley-Folkman lemma to the generic\nminimax problem, where in the literature the technique was only known to work\nwhen the objective function is restricted to the Lagrangian function of a\nconstraint optimization problem. Our proposed Stackelberg GAN performs well\nexperimentally in both synthetic and real-world datasets, improving Fr\\'echet\nInception Distance by $14.61\\%$ over the previous multi-generator GANs on the\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 22:38:36 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Zhang", "Hongyang", ""], ["Xu", "Susu", ""], ["Jiao", "Jiantao", ""], ["Xie", "Pengtao", ""], ["Salakhutdinov", "Ruslan", ""], ["Xing", "Eric P.", ""]]}, {"id": "1811.08019", "submitter": "George Berry", "authors": "George Berry", "title": "Role action embeddings: scalable representation of network positions", "comments": "7 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the question of embedding nodes with similar local neighborhoods\ntogether in embedding space, commonly referred to as \"role embeddings.\" We\npropose RAE, an unsupervised framework that learns role embeddings. It combines\na within-node loss function and a graph neural network (GNN) architecture to\nplace nodes with similar local neighborhoods close in embedding space. We also\npropose a faster way of generating negative examples called neighbor shuffling,\nwhich quickly creates negative examples directly within batches. These\ntechniques can be easily combined with existing GNN methods to create\nunsupervised role embeddings at scale. We then explore role action embeddings,\nwhich summarize the non-structural features in a node's neighborhood, leading\nto better performance on node classification tasks. We find that the model\narchitecture proposed here provides strong performance on both graph and node\nclassification tasks, in some cases competitive with semi-supervised methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 23:23:46 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 03:46:49 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Berry", "George", ""]]}, {"id": "1811.08027", "submitter": "Guanya Shi", "authors": "Guanya Shi, Xichen Shi, Michael O'Connell, Rose Yu, Kamyar\n  Azizzadenesheli, Animashree Anandkumar, Yisong Yue, Soon-Jo Chung", "title": "Neural Lander: Stable Drone Landing Control using Learned Dynamics", "comments": "7 pages, 5 figures, https://youtu.be/FLLsG0S78ik", "journal-ref": "International Conferenceon Robotics and Automation (ICRA), 2019,\n  pp. 9784-9790", "doi": "10.1109/ICRA.2019.8794351", "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise near-ground trajectory control is difficult for multi-rotor drones,\ndue to the complex aerodynamic effects caused by interactions between\nmulti-rotor airflow and the environment. Conventional control methods often\nfail to properly account for these complex effects and fall short in\naccomplishing smooth landing. In this paper, we present a novel\ndeep-learning-based robust nonlinear controller (Neural Lander) that improves\ncontrol performance of a quadrotor during landing. Our approach combines a\nnominal dynamics model with a Deep Neural Network (DNN) that learns high-order\ninteractions. We apply spectral normalization (SN) to constrain the Lipschitz\nconstant of the DNN. Leveraging this Lipschitz property, we design a nonlinear\nfeedback linearization controller using the learned model and prove system\nstability with disturbance rejection. To the best of our knowledge, this is the\nfirst DNN-based nonlinear feedback controller with stability guarantees that\ncan utilize arbitrarily large neural nets. Experimental results demonstrate\nthat the proposed controller significantly outperforms a Baseline Nonlinear\nTracking Controller in both landing and cross-table trajectory tracking cases.\nWe also empirically show that the DNN generalizes well to unseen data outside\nthe training domain.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 23:59:28 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 23:12:01 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Shi", "Guanya", ""], ["Shi", "Xichen", ""], ["O'Connell", "Michael", ""], ["Yu", "Rose", ""], ["Azizzadenesheli", "Kamyar", ""], ["Anandkumar", "Animashree", ""], ["Yue", "Yisong", ""], ["Chung", "Soon-Jo", ""]]}, {"id": "1811.08035", "submitter": "Kahkashan Afrin", "authors": "Kahkashan Afrin, Parikshit Verma, Sanjay S. Srivatsa, and Satish T.S.\n  Bukkapatnam", "title": "Simultaneous 12-Lead Electrocardiogram Synthesis using a Single-Lead ECG\n  Signal: Application to Handheld ECG Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent introduction of wearable single-lead ECG devices of diverse\nconfigurations has caught the intrigue of the medical community. While these\ndevices provide a highly affordable support tool for the caregivers for\ncontinuous monitoring and to detect acute conditions, such as arrhythmia, their\nutility for cardiac diagnostics remains limited. This is because clinical\ndiagnosis of many cardiac pathologies is rooted in gleaning patterns from\nsynchronous 12-lead ECG. If synchronous 12-lead signals of clinical quality can\nbe synthesized from these single-lead devices, it can transform cardiac care by\nsubstantially reducing the costs and enhancing access to cardiac diagnostics.\nHowever, prior attempts to synthesize synchronous 12-lead ECG have not been\nsuccessful. Vectorcardiography (VCG) analysis suggests that cardiac axis\nsynthesized from earlier attempts deviates significantly from that estimated\nfrom 12-lead and/or Frank lead measurements. This work is perhaps the first\nsuccessful attempt to synthesize clinically equivalent synchronous 12-lead ECG\nfrom single-lead ECG. Our method employs a random forest machine learning model\nthat uses a subject's historical 12-lead recordings to estimate the morphology\nincluding the actual timing of various ECG events (relative to the measured\nsingle-lead ECG) for all 11 missing leads of the subject. Our method was\nvalidated on two benchmark datasets as well as paper ECG and AliveCor-Kardia\ndata obtained from the Heart, Artery, and Vein Center of Fresno, California.\nResults suggest that this approach can synthesize synchronous ECG with\naccuracies (R2) exceeding 90%. Accurate synthesis of 12-lead ECG from a\nsingle-lead device can ultimately enable its wider application and improved\npoint-of-care (POC) diagnostics.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 00:29:34 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Afrin", "Kahkashan", ""], ["Verma", "Parikshit", ""], ["Srivatsa", "Sanjay S.", ""], ["Bukkapatnam", "Satish T. S.", ""]]}, {"id": "1811.08039", "submitter": "Armin Askari", "authors": "Fangda Gu, Armin Askari, Laurent El Ghaoui", "title": "Fenchel Lifted Networks: A Lagrange Relaxation of Neural Network\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent successes of deep neural networks, the corresponding\ntraining problem remains highly non-convex and difficult to optimize. Classes\nof models have been proposed that introduce greater structure to the objective\nfunction at the cost of lifting the dimension of the problem. However, these\nlifted methods sometimes perform poorly compared to traditional neural\nnetworks. In this paper, we introduce a new class of lifted models, Fenchel\nlifted networks, that enjoy the same benefits as previous lifted models,\nwithout suffering a degradation in performance over classical networks. Our\nmodel represents activation functions as equivalent biconvex constraints and\nuses Lagrange Multipliers to arrive at a rigorous lower bound of the\ntraditional neural network training problem. This model is efficiently trained\nusing block-coordinate descent and is parallelizable across data points and/or\nlayers. We compare our model against standard fully connected and convolutional\nnetworks and show that we are able to match or beat their performance.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 00:58:17 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 01:23:18 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2019 07:06:04 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Gu", "Fangda", ""], ["Askari", "Armin", ""], ["Ghaoui", "Laurent El", ""]]}, {"id": "1811.08045", "submitter": "John Thickstun", "authors": "John Thickstun, Zaid Harchaoui, Dean P. Foster, Sham M. Kakade", "title": "Coupled Recurrent Models for Polyphonic Music Composition", "comments": "13 pages; long version of the paper appearing in ISMIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel recurrent model for music composition that is\ntailored to the structure of polyphonic music. We propose an efficient new\nconditional probabilistic factorization of musical scores, viewing a score as a\ncollection of concurrent, coupled sequences: i.e. voices. To model the\nconditional distributions, we borrow ideas from both convolutional and\nrecurrent neural models; we argue that these ideas are natural for capturing\nmusic's pitch invariances, temporal structure, and polyphony. We train models\nfor single-voice and multi-voice composition on 2,300 scores from the\nKernScores dataset.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 02:45:44 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 21:55:06 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Thickstun", "John", ""], ["Harchaoui", "Zaid", ""], ["Foster", "Dean P.", ""], ["Kakade", "Sham M.", ""]]}, {"id": "1811.08051", "submitter": "Kuan-Chuan Peng", "authors": "Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, Rama\n  Chellappa", "title": "Learning without Memorizing", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental learning (IL) is an important task aimed at increasing the\ncapability of a trained model, in terms of the number of classes recognizable\nby the model. The key problem in this task is the requirement of storing data\n(e.g. images) associated with existing classes, while teaching the classifier\nto learn new classes. However, this is impractical as it increases the memory\nrequirement at every incremental step, which makes it impossible to implement\nIL algorithms on edge devices with limited memory. Hence, we propose a novel\napproach, called `Learning without Memorizing (LwM)', to preserve the\ninformation about existing (base) classes, without storing any of their data,\nwhile making the classifier progressively learn the new classes. In LwM, we\npresent an information preserving penalty: Attention Distillation Loss\n($L_{AD}$), and demonstrate that penalizing the changes in classifiers'\nattention maps helps to retain information of the base classes, as new classes\nare added. We show that adding $L_{AD}$ to the distillation loss which is an\nexisting information preserving loss consistently outperforms the\nstate-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets in\nterms of the overall accuracy of base and incrementally learned classes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 03:20:16 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 15:39:30 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Dhar", "Prithviraj", ""], ["Singh", "Rajat Vikram", ""], ["Peng", "Kuan-Chuan", ""], ["Wu", "Ziyan", ""], ["Chellappa", "Rama", ""]]}, {"id": "1811.08052", "submitter": "Changyou Chen", "authors": "Jianyi Zhang, Yang Zhao, Changyou Chen", "title": "Variance Reduction in Stochastic Particle-Optimization Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic particle-optimization sampling (SPOS) is a recently-developed\nscalable Bayesian sampling framework that unifies stochastic gradient MCMC\n(SG-MCMC) and Stein variational gradient descent (SVGD) algorithms based on\nWasserstein gradient flows. With a rigorous non-asymptotic convergence theory\ndeveloped recently, SPOS avoids the particle-collapsing pitfall of SVGD.\nNevertheless, variance reduction in SPOS has never been studied. In this paper,\nwe bridge the gap by presenting several variance-reduction techniques for SPOS.\nSpecifically, we propose three variants of variance-reduced SPOS, called SAGA\nparticle-optimization sampling (SAGA-POS), SVRG particle-optimization sampling\n(SVRG-POS) and a variant of SVRG-POS which avoids full gradient computations,\ndenoted as SVRG-POS$^+$. Importantly, we provide non-asymptotic convergence\nguarantees for these algorithms in terms of 2-Wasserstein metric and analyze\ntheir complexities. Remarkably, the results show our algorithms yield better\nconvergence rates than existing variance-reduced variants of stochastic\nLangevin dynamics, even though more space is required to store the particles in\ntraining. Our theory well aligns with experimental results on both synthetic\nand real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 03:25:19 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Zhang", "Jianyi", ""], ["Zhao", "Yang", ""], ["Chen", "Changyou", ""]]}, {"id": "1811.08055", "submitter": "Chuxu Zhang", "authors": "Chuxu Zhang, Dongjin Song, Yuncong Chen, Xinyang Feng, Cristian\n  Lumezanu, Wei Cheng, Jingchao Ni, Bo Zong, Haifeng Chen, Nitesh V. Chawla", "title": "A Deep Neural Network for Unsupervised Anomaly Detection and Diagnosis\n  in Multivariate Time Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, multivariate time series data are increasingly collected in various\nreal world systems, e.g., power plants, wearable devices, etc. Anomaly\ndetection and diagnosis in multivariate time series refer to identifying\nabnormal status in certain time steps and pinpointing the root causes. Building\nsuch a system, however, is challenging since it not only requires to capture\nthe temporal dependency in each time series, but also need encode the\ninter-correlations between different pairs of time series. In addition, the\nsystem should be robust to noise and provide operators with different levels of\nanomaly scores based upon the severity of different incidents. Despite the fact\nthat a number of unsupervised anomaly detection algorithms have been developed,\nfew of them can jointly address these challenges. In this paper, we propose a\nMulti-Scale Convolutional Recurrent Encoder-Decoder (MSCRED), to perform\nanomaly detection and diagnosis in multivariate time series data. Specifically,\nMSCRED first constructs multi-scale (resolution) signature matrices to\ncharacterize multiple levels of the system statuses in different time steps.\nSubsequently, given the signature matrices, a convolutional encoder is employed\nto encode the inter-sensor (time series) correlations and an attention based\nConvolutional Long-Short Term Memory (ConvLSTM) network is developed to capture\nthe temporal patterns. Finally, based upon the feature maps which encode the\ninter-sensor correlations and temporal information, a convolutional decoder is\nused to reconstruct the input signature matrices and the residual signature\nmatrices are further utilized to detect and diagnose anomalies. Extensive\nempirical studies based on a synthetic dataset and a real power plant dataset\ndemonstrate that MSCRED can outperform state-of-the-art baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 03:36:45 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Zhang", "Chuxu", ""], ["Song", "Dongjin", ""], ["Chen", "Yuncong", ""], ["Feng", "Xinyang", ""], ["Lumezanu", "Cristian", ""], ["Cheng", "Wei", ""], ["Ni", "Jingchao", ""], ["Zong", "Bo", ""], ["Chen", "Haifeng", ""], ["Chawla", "Nitesh V.", ""]]}, {"id": "1811.08056", "submitter": "Dae Hoon Park", "authors": "Dae Hoon Park, Chiu Man Ho, Yi Chang, Huaqing Zhang", "title": "Gradient-Coherent Strong Regularization for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization plays an important role in generalization of deep neural\nnetworks, which are often prone to overfitting with their numerous parameters.\nL1 and L2 regularizers are common regularization tools in machine learning with\ntheir simplicity and effectiveness. However, we observe that imposing strong L1\nor L2 regularization with stochastic gradient descent on deep neural networks\neasily fails, which limits the generalization ability of the underlying neural\nnetworks. To understand this phenomenon, we first investigate how and why\nlearning fails when strong regularization is imposed on deep neural networks.\nWe then propose a novel method, gradient-coherent strong regularization, which\nimposes regularization only when the gradients are kept coherent in the\npresence of strong regularization. Experiments are performed with multiple deep\narchitectures on three benchmark data sets for image recognition. Experimental\nresults show that our proposed approach indeed endures strong regularization\nand significantly improves both accuracy and compression (up to 9.9x), which\ncould not be achieved otherwise.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 03:41:56 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 01:52:12 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Park", "Dae Hoon", ""], ["Ho", "Chiu Man", ""], ["Chang", "Yi", ""], ["Zhang", "Huaqing", ""]]}, {"id": "1811.08067", "submitter": "Arpit Agarwal Mr.", "authors": "Ricson Cheng, Arpit Agarwal, Katerina Fragkiadaki", "title": "Reinforcement Learning of Active Vision for Manipulating Objects under\n  Occlusions", "comments": "The paper was present in Conference of Robot Learning 2018", "journal-ref": "Proceedings of Machine Learning Research 87 (2018) 422--431", "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider artificial agents that learn to jointly control their gripperand\ncamera in order to reinforcement learn manipulation policies in the presenceof\nocclusions from distractor objects. Distractors often occlude the object of\nin-terest and cause it to disappear from the field of view. We propose hand/eye\ncon-trollers that learn to move the camera to keep the object within the field\nof viewand visible, in coordination to manipulating it to achieve the desired\ngoal, e.g.,pushing it to a target location. We incorporate structural biases of\nobject-centricattention within our actor-critic architectures, which our\nexperiments suggest tobe a key for good performance. Our results further\nhighlight the importance ofcurriculum with regards to environment difficulty.\nThe resulting active vision /manipulation policies outperform static camera\nsetups for a variety of clutteredenvironments.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 04:24:38 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 15:10:51 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Cheng", "Ricson", ""], ["Agarwal", "Arpit", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "1811.08069", "submitter": "Ka-Ho Chow", "authors": "Ka-Ho Chow, Anish Hiranandani, Yifeng Zhang, S.-H. Gary Chan", "title": "Representation Learning of Pedestrian Trajectories Using Actor-Critic\n  Sequence-to-Sequence Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning of pedestrian trajectories transforms variable-length\ntimestamp-coordinate tuples of a trajectory into a fixed-length vector\nrepresentation that summarizes spatiotemporal characteristics. It is a crucial\ntechnique to connect feature-based data mining with trajectory data. Trajectory\nrepresentation is a challenging problem, because both environmental constraints\n(e.g., wall partitions) and temporal user dynamics should be meticulously\nconsidered and accounted for. Furthermore, traditional sequence-to-sequence\nautoencoders using maximum log-likelihood often require dataset covering all\nthe possible spatiotemporal characteristics to perform well. This is infeasible\nor impractical in reality. We propose TREP, a practical pedestrian trajectory\nrepresentation learning algorithm which captures the environmental constraints\nand the pedestrian dynamics without the need of any training dataset. By\nformulating a sequence-to-sequence autoencoder with a spatial-aware objective\nfunction under the paradigm of actor-critic reinforcement learning, TREP\nintelligently encodes spatiotemporal characteristics of trajectories with the\ncapability of handling diverse trajectory patterns. Extensive experiments on\nboth synthetic and real datasets validate the high fidelity of TREP to\nrepresent trajectories.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 04:28:29 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Chow", "Ka-Ho", ""], ["Hiranandani", "Anish", ""], ["Zhang", "Yifeng", ""], ["Chan", "S. -H. Gary", ""]]}, {"id": "1811.08073", "submitter": "Pengyuan Ren", "authors": "Pengyuan Ren, Jianmin Li", "title": "Factorized Distillation: Training Holistic Person Re-identification\n  Model by Distilling an Ensemble of Partial ReID Models", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) is aimed at identifying the same person\nacross videos captured from different cameras. In the view that networks\nextracting global features using ordinary network architectures are difficult\nto extract local features due to their weak attention mechanisms, researchers\nhave proposed a lot of elaborately designed ReID networks, while greatly\nimproving the accuracy, the model size and the feature extraction latency are\nalso soaring. We argue that a relatively compact ordinary network extracting\nglobally pooled features has the capability to extract discriminative local\nfeatures and can achieve state-of-the-art precision if only the model's\nparameters are properly learnt. In order to reduce the difficulty in learning\nhard identity labels, we propose a novel knowledge distillation method:\nFactorized Distillation, which factorizes both feature maps and retrieval\nfeatures of holistic ReID network to mimic representations of multiple partial\nReID models, thus transferring the knowledge from partial ReID models to the\nholistic network. Experiments show that the performance of model trained with\nthe proposed method can outperform state-of-the-art with relatively few network\nparameters.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 04:50:09 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Ren", "Pengyuan", ""], ["Li", "Jianmin", ""]]}, {"id": "1811.08080", "submitter": "Kazuya Kakizaki", "authors": "Hajime Ono, Tsubasa Takahashi, Kazuya Kakizaki", "title": "Lightweight Lipschitz Margin Training for Certified Defense against\n  Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we make machine learning provably robust against adversarial examples\nin a scalable way? Since certified defense methods, which ensure\n$\\epsilon$-robust, consume huge resources, they can only achieve small degree\nof robustness in practice. Lipschitz margin training (LMT) is a scalable\ncertified defense, but it can also only achieve small robustness due to\nover-regularization. How can we make certified defense more efficiently? We\npresent LC-LMT, a light weight Lipschitz margin training which solves the above\nproblem. Our method has the following properties; (a) efficient: it can achieve\n$\\epsilon$-robustness at early epoch, and (b) robust: it has a potential to get\nhigher robustness than LMT. In the evaluation, we demonstrate the benefits of\nthe proposed method. LC-LMT can achieve required robustness more than 30 epoch\nearlier than LMT in MNIST, and shows more than 90 $\\%$ accuracy against both\nlegitimate and adversarial inputs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 05:22:55 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Ono", "Hajime", ""], ["Takahashi", "Tsubasa", ""], ["Kakizaki", "Kazuya", ""]]}, {"id": "1811.08081", "submitter": "Yuchen Li", "authors": "Safwan Hossain, Kiarash Jamali, Yuchen Li, Frank Rudzicz", "title": "ChainGAN: A sequential approach to GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new architecture and training methodology for generative\nadversarial networks. Current approaches attempt to learn the transformation\nfrom a noise sample to a generated data sample in one shot. Our proposed\ngenerator architecture, called $\\textit{ChainGAN}$, uses a two-step process. It\nfirst attempts to transform a noise vector into a crude sample, similar to a\ntraditional generator. Next, a chain of networks, called $\\textit{editors}$,\nattempt to sequentially enhance this sample. We train each of these units\nindependently, instead of with end-to-end backpropagation on the entire chain.\nOur model is robust, efficient, and flexible as we can apply it to various\nnetwork architectures. We provide rationale for our choices and experimentally\nevaluate our model, achieving competitive results on several datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 05:30:32 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 18:56:15 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Hossain", "Safwan", ""], ["Jamali", "Kiarash", ""], ["Li", "Yuchen", ""], ["Rudzicz", "Frank", ""]]}, {"id": "1811.08084", "submitter": "Daiki Suehiro", "authors": "Daiki Suehiro, Kohei Hatano, Eiji Takimoto, Shuji Yamamoto, Kenichi\n  Bannai, Akiko Takeda", "title": "Multiple-Instance Learning by Boosting Infinitely Many Shapelet-based\n  Classifiers", "comments": "The preliminary version of this paper is arXiv:1709.01300. which only\n  focuses on shapelet-based time-series classification but not\n  Muptiple-Instance Learning. Note that the preliminary version has not been\n  published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new formulation of Multiple-Instance Learning (MIL). In typical\nMIL settings, a unit of data is given as a set of instances called a bag and\nthe goal is to find a good classifier of bags based on similarity from a single\nor finitely many \"shapelets\" (or patterns), where the similarity of the bag\nfrom a shapelet is the maximum similarity of instances in the bag. Classifiers\nbased on a single shapelet are not sufficiently strong for certain\napplications. Additionally, previous work with multiple shapelets has\nheuristically chosen some of the instances as shapelets with no theoretical\nguarantee of its generalization ability. Our formulation provides a richer\nclass of the final classifiers based on infinitely many shapelets. We provide\nan efficient algorithm for the new formulation, in addition to generalization\nbound. Our empirical study demonstrates that our approach is effective not only\nfor MIL tasks but also for Shapelet Learning for time-series classification.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 05:51:22 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 04:55:51 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Suehiro", "Daiki", ""], ["Hatano", "Kohei", ""], ["Takimoto", "Eiji", ""], ["Yamamoto", "Shuji", ""], ["Bannai", "Kenichi", ""], ["Takeda", "Akiko", ""]]}, {"id": "1811.08086", "submitter": "Arpit Agarwal Mr.", "authors": "Arpit Agarwal, Katharina Muelling and Katerina Fragkiadaki", "title": "Model Learning for Look-ahead Exploration in Continuous Control", "comments": "This is a pre-print of our paper which is accepted in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an exploration method that incorporates look-ahead search over\nbasic learnt skills and their dynamics, and use it for reinforcement learning\n(RL) of manipulation policies . Our skills are multi-goal policies learned in\nisolation in simpler environments using existing multigoal RL formulations,\nanalogous to options or macroactions. Coarse skill dynamics, i.e., the state\ntransition caused by a (complete) skill execution, are learnt and are unrolled\nforward during lookahead search. Policy search benefits from temporal\nabstraction during exploration, though itself operates over low-level primitive\nactions, and thus the resulting policies does not suffer from suboptimality and\ninflexibility caused by coarse skill chaining. We show that the proposed\nexploration strategy results in effective learning of complex manipulation\npolicies faster than current state-of-the-art RL methods, and converges to\nbetter policies than methods that use options or parametrized skills as\nbuilding blocks of the policy itself, as opposed to guiding exploration. We\nshow that the proposed exploration strategy results in effective learning of\ncomplex manipulation policies faster than current state-of-the-art RL methods,\nand converges to better policies than methods that use options or parameterized\nskills as building blocks of the policy itself, as opposed to guiding\nexploration.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 06:11:26 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Agarwal", "Arpit", ""], ["Muelling", "Katharina", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "1811.08102", "submitter": "Nora Speicher", "authors": "Nora K. Speicher and Nico Pfeifer", "title": "An interpretable multiple kernel learning approach for the discovery of\n  integrative cancer subtypes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the complexity of cancer, clustering algorithms have been used to\ndisentangle the observed heterogeneity and identify cancer subtypes that can be\ntreated specifically. While kernel based clustering approaches allow the use of\nmore than one input matrix, which is an important factor when considering a\nmultidimensional disease like cancer, the clustering results remain hard to\nevaluate and, in many cases, it is unclear which piece of information had which\nimpact on the final result. In this paper, we propose an extension of multiple\nkernel learning clustering that enables the characterization of each identified\npatient cluster based on the features that had the highest impact on the\nresult. To this end, we combine feature clustering with multiple kernel\ndimensionality reduction and introduce FIPPA, a score which measures the\nfeature cluster impact on a patient cluster. Results: We applied the approach\nto different cancer types described by four different data types with the aim\nof identifying integrative patient subtypes and understanding which features\nwere most important for their identification. Our results show that our method\ndoes not only have state-of-the-art performance according to standard measures\n(e.g., survival analysis), but, based on the high impact features, it also\nproduces meaningful explanations for the molecular bases of the subtypes. This\ncould provide an important step in the validation of potential cancer subtypes\nand enable the formulation of new hypotheses concerning individual patient\ngroups. Similar analysis are possible for other disease phenotypes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 07:28:13 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Speicher", "Nora K.", ""], ["Pfeifer", "Nico", ""]]}, {"id": "1811.08106", "submitter": "Donghui Sun", "authors": "Donghui Sun, Qing Zhang and Jun Yang", "title": "Pyramid Embedded Generative Adversarial Network for Automated Font\n  Generation", "comments": "6 pages, 7 figures, accepted by International Conference on Pattern\n  Recognition (ICPR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the Chinese font synthesis problem and propose\na Pyramid Embedded Generative Adversarial Network (PEGAN) to automatically\ngenerate Chinese character images. The PEGAN consists of one generator and one\ndiscriminator. The generator is built using one encoder-decoder structure with\ncascaded refinement connections and mirror skip connections. The cascaded\nrefinement connections embed a multiscale pyramid of downsampled original input\ninto the encoder feature maps of different layers, and multi-scale feature maps\nfrom the encoder are connected to the corresponding feature maps in the decoder\nto make the mirror skip connections. Through combining the generative\nadversarial loss, pixel-wise loss, category loss and perceptual loss, the\ngenerator and discriminator can be trained alternately to synthesize character\nimages. In order to verify the effectiveness of our proposed PEGAN, we first\nbuild one evaluation set, in which the characters are selected according to\ntheir stroke number and frequency of use, and then use both qualitative and\nquantitative metrics to measure the performance of our model comparing with the\nbaseline method. The experimental results demonstrate the effectiveness of our\nproposed model, it shows the potential to automatically extend small font banks\ninto complete ones.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 07:37:46 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Sun", "Donghui", ""], ["Zhang", "Qing", ""], ["Yang", "Jun", ""]]}, {"id": "1811.08117", "submitter": "Yi Sun", "authors": "Yi Sun, Yan Tian, Yiping Xu and Jianxiang Li", "title": "Limited Gradient Descent: Learning With Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label noise may affect the generalization of classifiers, and the effective\nlearning of main patterns from samples with noisy labels is an important\nchallenge. Recent studies have shown that deep neural networks tend to\nprioritize the learning of simple patterns over the memorization of noise\npatterns. This suggests a possible method to search for the best generalization\nthat learns the main pattern until the noise begins to be memorized.\nTraditional approaches often employ a clean validation set to find the best\nstop timing of learning, i.e., early stopping. However, the generalization\nperformance of such methods relies on the quality of validation sets. Further,\nin practice, a clean validation set is sometimes difficult to obtain. To solve\nthis problem, we propose a method that can estimate the optimal stopping timing\nwithout a clean validation set, called limited gradient descent. We modified\nthe labels of a few samples in a noisy dataset to obtain false labels and to\ncreate a reverse pattern. By monitoring the learning progress of the noisy and\nreverse samples, we can determine the stop timing of learning. In this paper,\nwe also theoretically provide some necessary conditions on learning with noisy\nlabels. Experimental results on CIFAR-10 and CIFAR-100 datasets demonstrate\nthat our approach has a comparable generalization performance to methods\nrelying on a clean validation set. Thus, on the noisy Clothing-1M dataset, our\napproach surpasses methods that rely on a clean validation set.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 08:24:17 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 15:17:11 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 08:17:00 GMT"}, {"version": "v4", "created": "Thu, 5 Dec 2019 02:55:02 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Sun", "Yi", ""], ["Tian", "Yan", ""], ["Xu", "Yiping", ""], ["Li", "Jianxiang", ""]]}, {"id": "1811.08120", "submitter": "Weiyu Cheng", "authors": "Weiyu Cheng, Yanyan Shen, Yanmin Zhu, Linpeng Huang", "title": "Explaining Latent Factor Models for Recommendation with Influence\n  Functions", "comments": null, "journal-ref": null, "doi": "10.1145/3292500.3330857", "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent factor models (LFMs) such as matrix factorization achieve the\nstate-of-the-art performance among various Collaborative Filtering (CF)\napproaches for recommendation. Despite the high recommendation accuracy of\nLFMs, a critical issue to be resolved is the lack of explainability. Extensive\nefforts have been made in the literature to incorporate explainability into\nLFMs. However, they either rely on auxiliary information which may not be\navailable in practice, or fail to provide easy-to-understand explanations. In\nthis paper, we propose a fast influence analysis method named FIA, which\nsuccessfully enforces explicit neighbor-style explanations to LFMs with the\ntechnique of influence functions stemmed from robust statistics. We first\ndescribe how to employ influence functions to LFMs to deliver neighbor-style\nexplanations. Then we develop a novel influence computation algorithm for\nmatrix factorization with high efficiency. We further extend it to the more\ngeneral neural collaborative filtering and introduce an approximation algorithm\nto accelerate influence analysis over neural network models. Experimental\nresults on real datasets demonstrate the correctness, efficiency and usefulness\nof our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 08:31:24 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Cheng", "Weiyu", ""], ["Shen", "Yanyan", ""], ["Zhu", "Yanmin", ""], ["Huang", "Linpeng", ""]]}, {"id": "1811.08127", "submitter": "Alireza Abedin Varamin", "authors": "Alireza Abedin Varamin, Ehsan Abbasnejad, Qinfeng Shi, Damith\n  Ranasinghe, Hamid Rezatofighi", "title": "Deep Auto-Set: A Deep Auto-Encoder-Set Network for Activity Recognition\n  Using Wearables", "comments": "Accepted at MobiQuitous 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic recognition of human activities from time-series sensor data\n(referred to as HAR) is a growing area of research in ubiquitous computing.\nMost recent research in the field adopts supervised deep learning paradigms to\nautomate extraction of intrinsic features from raw signal inputs and addresses\nHAR as a multi-class classification problem where detecting a single activity\nclass within the duration of a sensory data segment suffices. However, due to\nthe innate diversity of human activities and their corresponding duration, no\ndata segment is guaranteed to contain sensor recordings of a single activity\ntype. In this paper, we express HAR more naturally as a set prediction problem\nwhere the predictions are sets of ongoing activity elements with unfixed and\nunknown cardinality. For the first time, we address this problem by presenting\na novel HAR approach that learns to output activity sets using deep neural\nnetworks. Moreover, motivated by the limited availability of annotated HAR\ndatasets as well as the unfortunate immaturity of existing unsupervised\nsystems, we complement our supervised set learning scheme with a prior\nunsupervised feature learning process that adopts convolutional auto-encoders\nto exploit unlabeled data. The empirical experiments on two widely adopted HAR\ndatasets demonstrate the substantial improvement of our proposed methodology\nover the baseline models.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 08:54:31 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Varamin", "Alireza Abedin", ""], ["Abbasnejad", "Ehsan", ""], ["Shi", "Qinfeng", ""], ["Ranasinghe", "Damith", ""], ["Rezatofighi", "Hamid", ""]]}, {"id": "1811.08150", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi, Jiaoyang Huang, Leslie Pack Kaelbling", "title": "Effect of Depth and Width on Local Minima in Deep Learning", "comments": null, "journal-ref": "Neural computation, volume 31, pages 1462-1498 (2019)", "doi": "10.1162/neco_a_01195", "report-no": null, "categories": "cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the effects of depth and width on the quality of\nlocal minima, without strong over-parameterization and simplification\nassumptions in the literature. Without any simplification assumption, for deep\nnonlinear neural networks with the squared loss, we theoretically show that the\nquality of local minima tends to improve towards the global minimum value as\ndepth and width increase. Furthermore, with a locally-induced structure on deep\nnonlinear neural networks, the values of local minima of neural networks are\ntheoretically proven to be no worse than the globally optimal values of\ncorresponding classical machine learning models. We empirically support our\ntheoretical observation with a synthetic dataset as well as MNIST, CIFAR-10 and\nSVHN datasets. When compared to previous studies with strong\nover-parameterization assumptions, the results in this paper do not require\nover-parameterization, and instead show the gradual effects of\nover-parameterization as consequences of general results.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 09:41:52 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 19:48:28 GMT"}, {"version": "v3", "created": "Mon, 17 Jun 2019 18:40:43 GMT"}, {"version": "v4", "created": "Tue, 9 Jul 2019 15:32:37 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Huang", "Jiaoyang", ""], ["Kaelbling", "Leslie Pack", ""]]}, {"id": "1811.08159", "submitter": "Hamed Azarnoush", "authors": "Samaneh Siyar (1,2), Hamed Azarnoush (1,2), Saeid Rashidi (3),\n  Alexandre Winkler-Schwartz (1), Vincent Bissonnette (1), Nirros Ponnudurai\n  (1), Rolando F. Del Maestro (1), ((1) Neurosurgical Simulation Research and\n  Training Centre, McGill University, Canada, (2) Department of Biomedical\n  Engineering, Amirkabir University of Technology, Iran, (3) Science and\n  Research Branch, Islamic Azad University, Iran)", "title": "Machine Learning Distinguishes Neurosurgical Skill Levels in a Virtual\n  Reality Tumor Resection Task", "comments": null, "journal-ref": null, "doi": "10.1007/s11517-020-02155-3", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Virtual reality simulators and machine learning have the\npotential to augment understanding, assessment and training of psychomotor\nperformance in neurosurgery residents. Objective: This study outlines the first\napplication of machine learning to distinguish \"skilled\" and \"novice\"\npsychomotor performance during a virtual reality neurosurgical task. Methods:\nTwenty-three neurosurgeons and senior neurosurgery residents comprising the\n\"skilled\" group and 92 junior neurosurgery residents and medical students the\n\"novice\" group. The task involved removing a series of virtual brain tumors\nwithout causing injury to surrounding tissue. Over 100 features were extracted\nand 68 selected using t-test analysis. These features were provided to 4\nclassifiers: K-Nearest Neighbors, Parzen Window, Support Vector Machine, and\nFuzzy K-Nearest Neighbors. Equal Error Rate was used to assess classifier\nperformance. Results: Ratios of train set size to test set size from 10% to 90%\nand 5 to 30 features, chosen by the forward feature selection algorithm, were\nemployed. A working point of 50% train to test set size ratio and 15 features\nresulted in an equal error rates as low as 8.3% using the Fuzzy K-Nearest\nNeighbors classifier. Conclusion: Machine learning may be one component helping\nrealign the traditional apprenticeship educational paradigm to a more objective\nmodel based on proven performance standards.\n  Keywords: Artificial intelligence, Classifiers, Machine learning,\nNeurosurgery skill assessment, Surgical education, Tumor resection, Virtual\nreality simulation\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 10:09:02 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Siyar", "Samaneh", ""], ["Azarnoush", "Hamed", ""], ["Rashidi", "Saeid", ""], ["Winkler-Schwartz", "Alexandre", ""], ["Bissonnette", "Vincent", ""], ["Ponnudurai", "Nirros", ""], ["Del Maestro", "Rolando F.", ""]]}, {"id": "1811.08163", "submitter": "Tao Yi", "authors": "Tao Yi, Xingxuan Wang", "title": "Variance Suppression: Balanced Training Process in Deep Learning", "comments": "More experiments are needed to prove this theory, but I'm not quite\n  sure yet", "journal-ref": null, "doi": "10.1088/1742-6596/1207/1/012013", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic gradient descent updates parameters with summation gradient\ncomputed from a random data batch. This summation will lead to unbalanced\ntraining process if the data we obtained is unbalanced. To address this issue,\nthis paper takes the error variance and error mean both into consideration. The\nadaptively adjusting approach of two terms trading off is also given in our\nalgorithm. Due to this algorithm can suppress error variance, we named it\nVariance Suppression Gradient Descent (VSSGD). Experimental results have\ndemonstrated that VSSGD can accelerate the training process, effectively\nprevent overfitting, improve the networks learning capacity from small samples.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 10:16:31 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 09:05:42 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 08:21:15 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Yi", "Tao", ""], ["Wang", "Xingxuan", ""]]}, {"id": "1811.08180", "submitter": "Ning Yu", "authors": "Ning Yu, Larry Davis, Mario Fritz", "title": "Attributing Fake Images to GANs: Learning and Analyzing GAN Fingerprints", "comments": "Accepted to ICCV'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.CY cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Generative Adversarial Networks (GANs) have shown\nincreasing success in generating photorealistic images. But they also raise\nchallenges to visual forensics and model attribution. We present the first\nstudy of learning GAN fingerprints towards image attribution and using them to\nclassify an image as real or GAN-generated. For GAN-generated images, we\nfurther identify their sources. Our experiments show that (1) GANs carry\ndistinct model fingerprints and leave stable fingerprints in their generated\nimages, which support image attribution; (2) even minor differences in GAN\ntraining can result in different fingerprints, which enables fine-grained model\nauthentication; (3) fingerprints persist across different image frequencies and\npatches and are not biased by GAN artifacts; (4) fingerprint finetuning is\neffective in immunizing against five types of adversarial image perturbations;\nand (5) comparisons also show our learned fingerprints consistently outperform\nseveral baselines in a variety of setups.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 11:11:21 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 13:19:40 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 17:11:32 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Yu", "Ning", ""], ["Davis", "Larry", ""], ["Fritz", "Mario", ""]]}, {"id": "1811.08212", "submitter": "Alexandre Garcia", "authors": "Christelle Marfaing, Alexandre Garcia", "title": "Computer-Assisted Fraud Detection, From Active Learning to Reward\n  Maximization", "comments": "NeurIPS 2018 Workshop on Challenges and Opportunities for AI in\n  Financial Services", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic detection of frauds in banking transactions has been recently\nstudied as a way to help the analysts finding fraudulent operations. Due to the\navailability of a human feedback, this task has been studied in the framework\nof active learning: the fraud predictor is allowed to sequentially call on an\noracle. This human intervention is used to label new examples and improve the\nclassification accuracy of the latter. Such a setting is not adapted in the\ncase of fraud detection with financial data in European countries. Actually, as\na human verification is mandatory to consider a fraud as really detected, it is\nnot necessary to focus on improving the classifier. We introduce the setting of\n'Computer-assisted fraud detection' where the goal is to minimize the number of\nnon fraudulent operations submitted to an oracle. The existing methods are\napplied to this task and we show that a simple meta-algorithm provides\ncompetitive results in this scenario on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 12:37:55 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Marfaing", "Christelle", ""], ["Garcia", "Alexandre", ""]]}, {"id": "1811.08214", "submitter": "Danilo Vasconcellos  Vargas", "authors": "Danilo Vasconcellos Vargas and Hirotaka Takano and Junichi Murata", "title": "Contingency Training", "comments": null, "journal-ref": "Proc. of SICE Annual Conference 2013", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When applied to high-dimensional datasets, feature selection algorithms might\nstill leave dozens of irrelevant variables in the dataset. Therefore, even\nafter feature selection has been applied, classifiers must be prepared to the\npresence of irrelevant variables. This paper investigates a new training method\ncalled Contingency Training which increases the accuracy as well as the\nrobustness against irrelevant attributes. Contingency training is classifier\nindependent. By subsampling and removing information from each sample, it\ncreates a set of constraints. These constraints aid the method to automatically\nfind proper importance weights of the dataset's features. Experiments are\nconducted with the contingency training applied to neural networks over\ntraditional datasets as well as datasets with additional irrelevant variables.\nFor all of the tests, contingency training surpassed the unmodified training on\ndatasets with irrelevant variables and even outperformed slightly when only a\nfew or no irrelevant variables were present.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 12:40:31 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Vargas", "Danilo Vasconcellos", ""], ["Takano", "Hirotaka", ""], ["Murata", "Junichi", ""]]}, {"id": "1811.08223", "submitter": "Ramanarayan Mohanty", "authors": "Ramanarayan Mohanty, SL Happy and Aurobinda Routray", "title": "A Semi-supervised Spatial Spectral Regularized Manifold Local Scaling\n  Cut With HGF for Dimensionality Reduction of Hyperspectral Images", "comments": null, "journal-ref": "IEEE Transaction on Geoscience and Remote Sensing, 2018", "doi": "10.1109/TGRS.2018.2884771", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral images (HSI) contain a wealth of information over hundreds of\ncontiguous spectral bands, making it possible to classify materials through\nsubtle spectral discrepancies. However, the classification of this rich\nspectral information is accompanied by the challenges like high dimensionality,\nsingularity, limited training samples, lack of labeled data samples,\nheteroscedasticity and nonlinearity. To address these challenges, we propose a\nsemi-supervised graph based dimensionality reduction method named\n`semi-supervised spatial spectral regularized manifold local scaling cut'\n(S3RMLSC). The underlying idea of the proposed method is to exploit the limited\nlabeled information from both the spectral and spatial domains along with the\nabundant unlabeled samples to facilitate the classification task by retaining\nthe original distribution of the data. In S3RMLSC, a hierarchical guided filter\n(HGF) is initially used to smoothen the pixels of the HSI data to preserve the\nspatial pixel consistency. This step is followed by the construction of linear\npatches from the nonlinear manifold by using the maximal linear patch (MLP)\ncriterion. Then the inter-patch and intra-patch dissimilarity matrices are\nconstructed in both spectral and spatial domains by regularized manifold local\nscaling cut (RMLSC) and neighboring pixel manifold local scaling cut (NPMLSC)\nrespectively. Finally, we obtain the projection matrix by optimizing the\nupdated semi-supervised spatial-spectral between-patch and total-patch\ndissimilarity. The effectiveness of the proposed DR algorithm is illustrated\nwith publicly available real-world HSI datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 12:56:39 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Mohanty", "Ramanarayan", ""], ["Happy", "SL", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1811.08225", "submitter": "Danilo Vasconcellos  Vargas", "authors": "Danilo Vasconcellos Vargas and Hirotaka Takano and Junichi Murata", "title": "Self Organizing Classifiers: First Steps in Structured Evolutionary\n  Machine Learning", "comments": null, "journal-ref": "Evolutionary Intelligence 6 (2), 57-72 (2013)", "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning classifier systems (LCSs) are evolutionary machine learning\nalgorithms, flexible enough to be applied to reinforcement, supervised and\nunsupervised learning problems with good performance. Recently, self organizing\nclassifiers were proposed which are similar to LCSs but have the advantage that\nin its structured population no balance between niching and fitness pressure is\nnecessary. However, more tests and analysis are required to verify its\nbenefits. Here, a variation of the first algorithm is proposed which uses a\nparameterless self organizing map (SOM). This algorithm is applied in\nchallenging problems such as big, noisy as well as dynamically changing\ncontinuous input-action mazes (growing and compressing mazes are included) with\ngood performance. Moreover, a genetic operator is proposed which utilizes the\ntopological information of the SOM's population structure, improving the\nresults. Thus, the first steps in structured evolutionary machine learning are\nshown, nonetheless, the problems faced are more difficult than the state-of-art\ncontinuous input-action multi-step ones.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 13:00:51 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Vargas", "Danilo Vasconcellos", ""], ["Takano", "Hirotaka", ""], ["Murata", "Junichi", ""]]}, {"id": "1811.08226", "submitter": "Danilo Vasconcellos  Vargas", "authors": "Danilo Vasconcellos Vargas and Hirotaka Takano and Junichi Murata", "title": "Self Organizing Classifiers and Niched Fitness", "comments": "arXiv admin note: text overlap with arXiv:1811.08225", "journal-ref": "Proceedings of the 15th annual conference on Genetic and\n  evolutionary computation (GECCO 2013)", "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning classifier systems are adaptive learning systems which have been\nwidely applied in a multitude of application domains. However, there are still\nsome generalization problems unsolved. The hurdle is that fitness and niching\npressures are difficult to balance. Here, a new algorithm called Self\nOrganizing Classifiers is proposed which faces this problem from a different\nperspective. Instead of balancing the pressures, both pressures are separated\nand no balance is necessary. In fact, the proposed algorithm possesses a\ndynamical population structure that self-organizes itself to better project the\ninput space into a map. The niched fitness concept is defined along with its\ndynamical population structure, both are indispensable for the understanding of\nthe proposed method. Promising results are shown on two continuous multi-step\nproblems. One of which is yet more challenging than previous problems of this\nclass in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 13:01:29 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Vargas", "Danilo Vasconcellos", ""], ["Takano", "Hirotaka", ""], ["Murata", "Junichi", ""]]}, {"id": "1811.08227", "submitter": "Kar-Ann Toh", "authors": "Kar-Ann Toh", "title": "Analytic Network Learning", "comments": "Some of the preliminary ideas of this work has been presented in the\n  IEEE/ACIS 17th International Conference on Computer and Information Science:\n  \"Learning from the kernel and the range space\" (ICIS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the property that solving the system of linear matrix equations via\nthe column space and the row space projections boils down to an approximation\nin the least squares error sense, a formulation for learning the weight\nmatrices of the multilayer network can be derived. By exploiting into the vast\nnumber of feasible solutions of these interdependent weight matrices, the\nlearning can be performed analytically layer by layer without needing of\ngradient computation after an initialization. Possible initialization schemes\ninclude utilizing the data matrix as initial weights and random initialization.\nThe study is followed by an investigation into the representation capability\nand the output variance of the learning scheme. An extensive experimentation on\nsynthetic and real-world data sets validates its numerical feasibility.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 13:03:07 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Toh", "Kar-Ann", ""]]}, {"id": "1811.08241", "submitter": "Martin  Biehl", "authors": "Martin Biehl", "title": "Geometry of Friston's active inference", "comments": "6 pages, 3 figures, Extended abstract accepted as a poster at\n  AABI2018, 1st Symposium on Advances in Approximate Bayesian Inference, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reconstruct Karl Friston's active inference and give a geometrical\ninterpretation of it.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 13:33:02 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Biehl", "Martin", ""]]}, {"id": "1811.08252", "submitter": "Regev Cohen", "authors": "Oren Solomon, Regev Cohen, Yi Zhang, Yi Yang, He Qiong, Jianwen Luo,\n  Ruud J.G. van Sloun and Yonina C. Eldar", "title": "Deep Unfolded Robust PCA with Application to Clutter Suppression in\n  Ultrasound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrast enhanced ultrasound is a radiation-free imaging modality which uses\nencapsulated gas microbubbles for improved visualization of the vascular bed\ndeep within the tissue. It has recently been used to enable imaging with\nunprecedented subwavelength spatial resolution by relying on super-resolution\ntechniques. A typical preprocessing step in super-resolution ultrasound is to\nseparate the microbubble signal from the cluttering tissue signal. This step\nhas a crucial impact on the final image quality. Here, we propose a new\napproach to clutter removal based on robust principle component analysis (PCA)\nand deep learning. We begin by modeling the acquired contrast enhanced\nultrasound signal as a combination of a low rank and sparse components. This\nmodel is used in robust PCA and was previously suggested in the context of\nultrasound Doppler processing and dynamic magnetic resonance imaging. We then\nillustrate that an iterative algorithm based on this model exhibits improved\nseparation of microbubble signal from the tissue signal over commonly practiced\nmethods. Next, we apply the concept of deep unfolding to suggest a deep network\narchitecture tailored to our clutter filtering problem which exhibits improved\nconvergence speed and accuracy with respect to its iterative counterpart. We\ncompare the performance of the suggested deep network on both simulations and\nin-vivo rat brain scans, with a commonly practiced deep-network architecture\nand the fast iterative shrinkage algorithm, and show that our architecture\nexhibits better image quality and contrast.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 14:02:53 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Solomon", "Oren", ""], ["Cohen", "Regev", ""], ["Zhang", "Yi", ""], ["Yang", "Yi", ""], ["Qiong", "He", ""], ["Luo", "Jianwen", ""], ["van Sloun", "Ruud J. G.", ""], ["Eldar", "Yonina C.", ""]]}, {"id": "1811.08270", "submitter": "Qiran Gong", "authors": "Hao Peng, Jianxin Li, Qiran Gong, Senzhang Wang, Yuanxing Ning, Philip\n  S. Yu", "title": "Graph Convolutional Neural Networks via Motif-based Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world problems can be represented as graph-based learning problems.\nIn this paper, we propose a novel framework for learning spatial and\nattentional convolution neural networks on arbitrary graphs. Different from\nprevious convolutional neural networks on graphs, we first design a\nmotif-matching guided subgraph normalization method to capture neighborhood\ninformation. Then we implement subgraph-level self-attentional layers to learn\ndifferent importances from different subgraphs to solve graph classification\nproblems. Analogous to image-based attentional convolution networks that\noperate on locally connected and weighted regions of the input, we also extend\ngraph normalization from one-dimensional node sequence to two-dimensional node\ngrid by leveraging motif-matching, and design self-attentional layers without\nrequiring any kinds of cost depending on prior knowledge of the graph\nstructure. Our results on both bioinformatics and social network datasets show\nthat we can significantly improve graph classification benchmarks over\ntraditional graph kernel and existing deep models.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 14:15:23 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 06:50:27 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Peng", "Hao", ""], ["Li", "Jianxin", ""], ["Gong", "Qiran", ""], ["Wang", "Senzhang", ""], ["Ning", "Yuanxing", ""], ["Yu", "Philip S.", ""]]}, {"id": "1811.08283", "submitter": "Arindam Paul", "authors": "Arindam Paul, Dipendra Jha, Reda Al-Bahrani, Wei-keng Liao, Alok\n  Choudhary and Ankit Agrawal", "title": "CheMixNet: Mixed DNN Architectures for Predicting Chemical Properties\n  using Multiple Molecular Representations", "comments": "13 pages total, 9 pages text, 4 pages citations, 8 figures, NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SMILES is a linear representation of chemical structures which encodes the\nconnection table, and the stereochemistry of a molecule as a line of text with\na grammar structure denoting atoms, bonds, rings and chains, and this\ninformation can be used to predict chemical properties. Molecular fingerprints\nare representations of chemical structures, successfully used in similarity\nsearch, clustering, classification, drug discovery, and virtual screening and\nare a standard and computationally efficient abstract representation where\nstructural features are represented as a bit string. Both SMILES and molecular\nfingerprints are different representations for describing the structure of a\nmolecule. There exist several predictive models for learning chemical\nproperties based on either SMILES or molecular fingerprints. Here, our goal is\nto build predictive models that can leverage both these molecular\nrepresentations. In this work, we present CheMixNet -- a set of neural networks\nfor predicting chemical properties from a mixture of features learned from the\ntwo molecular representations -- SMILES as sequences and molecular fingerprints\nas vector inputs. We demonstrate the efficacy of CheMixNet architectures by\nevaluating on six different datasets. The proposed CheMixNet models not only\noutperforms the candidate neural architectures such as contemporary fully\nconnected networks that uses molecular fingerprints and 1-D CNN and RNN models\ntrained SMILES sequences, but also other state-of-the-art architectures such as\nChemception and Molecular Graph Convolutions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 20:21:33 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 00:31:38 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Paul", "Arindam", ""], ["Jha", "Dipendra", ""], ["Al-Bahrani", "Reda", ""], ["Liao", "Wei-keng", ""], ["Choudhary", "Alok", ""], ["Agrawal", "Ankit", ""]]}, {"id": "1811.08284", "submitter": "Raghav Menon", "authors": "Raghav Menon, Herman Kamper, Ewald van der Westhuizen, John Quinn,\n  Thomas Niesler", "title": "Feature exploration for almost zero-resource ASR-free keyword spotting\n  using a multilingual bottleneck extractor and correspondence autoencoders", "comments": "5 pages, 2 figures, 2 tables, 38 references, Accepted at Interspeech\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare features for dynamic time warping (DTW) when used to bootstrap\nkeyword spotting (KWS) in an almost zero-resource setting. Such\nquickly-deployable systems aim to support United Nations (UN) humanitarian\nrelief efforts in parts of Africa with severely under-resourced languages. Our\nobjective is to identify acoustic features that provide acceptable KWS\nperformance in such environments. As supervised resource, we restrict ourselves\nto a small, easily acquired and independently compiled set of isolated\nkeywords. For feature extraction, a multilingual bottleneck feature (BNF)\nextractor, trained on well-resourced out-of-domain languages, is integrated\nwith a correspondence autoencoder (CAE) trained on extremely sparse in-domain\ndata. On their own, BNFs and CAE features are shown to achieve a more than 2%\nabsolute performance improvement over baseline MFCCs. However, by using BNFs as\ninput to the CAE, even better performance is achieved, with a more than 11%\nabsolute improvement in ROC AUC over MFCCs and more than twice as many top-10\nretrievals for two evaluated languages, English and Luganda. We conclude that\nintegrating BNFs with the CAE allows both large out-of-domain and sparse\nin-domain resources to be exploited for improved ASR-free keyword spotting.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 09:29:11 GMT"}, {"version": "v2", "created": "Sat, 13 Jul 2019 01:58:31 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Menon", "Raghav", ""], ["Kamper", "Herman", ""], ["van der Westhuizen", "Ewald", ""], ["Quinn", "John", ""], ["Niesler", "Thomas", ""]]}, {"id": "1811.08295", "submitter": "Giorgia Ramponi", "authors": "Giorgia Ramponi, Pavlos Protopapas, Marco Brambilla, Ryan Janssen", "title": "T-CGAN: Conditional Generative Adversarial Network for Data Augmentation\n  in Noisy Time Series with Irregular Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a data augmentation method for time series with\nirregular sampling, Time-Conditional Generative Adversarial Network (T-CGAN).\nOur approach is based on Conditional Generative Adversarial Networks (CGAN),\nwhere the generative step is implemented by a deconvolutional NN and the\ndiscriminative step by a convolutional NN. Both the generator and the\ndiscriminator are conditioned on the sampling timestamps, to learn the hidden\nrelationship between data and timestamps, and consequently to generate new time\nseries. We evaluate our model with synthetic and real-world datasets. For the\nsynthetic data, we compare the performance of a classifier trained with\nT-CGAN-generated data, against the performance of the same classifier trained\non the original data. Results show that classifiers trained on T-CGAN-generated\ndata perform the same as classifiers trained on real data, even with very short\ntime series and small training sets. For the real world datasets, we compare\nour method with other techniques of data augmentation for time series, such as\ntime slicing and time warping, over a classification problem with unbalanced\ndatasets. Results show that our method always outperforms the other approaches,\nboth in case of regularly sampled and irregularly sampled time series. We\nachieve particularly good performance in case with a small training set and\nshort, noisy, irregularly-sampled time series.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 14:54:24 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 15:14:44 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Ramponi", "Giorgia", ""], ["Protopapas", "Pavlos", ""], ["Brambilla", "Marco", ""], ["Janssen", "Ryan", ""]]}, {"id": "1811.08297", "submitter": "Soroush Pakniat", "authors": "Farzad Eskandari and Soroush Pakniat", "title": "Finite Mixture Model of Nonparametric Density Estimation using Sampling\n  Importance Resampling for Persistence Landscape", "comments": "arXiv admin note: text overlap with arXiv:1803.03677", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering the creation of persistence landscape on a parametrized curve and\nstructure of sampling, there exists a random process for which a finite mixture\nmodel of persistence landscape (FMMPL) can provide a better description for a\ngiven dataset. In this paper, a nonparametric approach for computing integrated\nmean of square error (IMSE) in persistence landscape has been presented. As a\nresult, FMMPL is more accurate than the another way. Also, the sampling\nimportance resampling (SIR) has been presented a better description of\nimportant landmark from parametrized curve. The result, provides more accuracy\nand less space complexity than the landmarks selected with simple sampling.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 06:55:32 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Eskandari", "Farzad", ""], ["Pakniat", "Soroush", ""]]}, {"id": "1811.08337", "submitter": "Thomas Ryder", "authors": "Tom Ryder, Andrew Golighty, A. Stephen McGough, Dennis Prangle", "title": "Black-Box Autoregressive Density Estimation for State-Space Models", "comments": "V2", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-space models (SSMs) provide a flexible framework for modelling\ntime-series data. Consequently, SSMs are ubiquitously applied in areas such as\nengineering, econometrics and epidemiology. In this paper we provide a fast\napproach for approximate Bayesian inference in SSMs using the tools of deep\nlearning and variational inference.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 16:05:05 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 08:19:51 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Ryder", "Tom", ""], ["Golighty", "Andrew", ""], ["McGough", "A. Stephen", ""], ["Prangle", "Dennis", ""]]}, {"id": "1811.08338", "submitter": "Aleks Kissinger", "authors": "Bart Jacobs and Aleks Kissinger and Fabio Zanasi", "title": "Causal Inference by String Diagram Surgery", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.LG math.CT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extracting causal relationships from observed correlations is a growing area\nin probabilistic reasoning, originating with the seminal work of Pearl and\nothers from the early 1990s. This paper develops a new, categorically oriented\nview based on a clear distinction between syntax (string diagrams) and\nsemantics (stochastic matrices), connected via interpretations as\nstructure-preserving functors. A key notion in the identification of causal\neffects is that of an intervention, whereby a variable is forcefully set to a\nparticular value independent of any prior propensities. We represent the effect\nof such an intervention as an endofunctor which performs `string diagram\nsurgery' within the syntactic category of string diagrams. This diagram surgery\nin turn yields a new, interventional distribution via the interpretation\nfunctor. While in general there is no way to compute interventional\ndistributions purely from observed data, we show that this is possible in\ncertain special cases using a calculational tool called comb disintegration. We\ndemonstrate the use of this technique on a well-known toy example, where we\npredict the causal effect of smoking on cancer in the presence of a confounding\ncommon cause. After developing this specific example, we show this technique\nprovides simple sufficient conditions for computing interventions which apply\nto a wide variety of situations considered in the causal inference literature.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 16:05:25 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 11:16:30 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Jacobs", "Bart", ""], ["Kissinger", "Aleks", ""], ["Zanasi", "Fabio", ""]]}, {"id": "1811.08357", "submitter": "Danica J. Sutherland", "authors": "Li Wenliang, Danica J. Sutherland, Heiko Strathmann, Arthur Gretton", "title": "Learning deep kernels for exponential family densities", "comments": null, "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning (ICML 2019), PMLR 97:6737-6746", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The kernel exponential family is a rich class of distributions, which can be\nfit efficiently and with statistical guarantees by score matching. Being\nrequired to choose a priori a simple kernel such as the Gaussian, however,\nlimits its practical applicability. We provide a scheme for learning a kernel\nparameterized by a deep network, which can find complex location-dependent\nlocal features of the data geometry. This gives a very rich class of density\nmodels, capable of fitting complex structures on moderate-dimensional problems.\nCompared to deep density models fit via maximum likelihood, our approach\nprovides a complementary set of strengths and tradeoffs: in empirical studies,\nthe former can yield higher likelihoods, whereas the latter gives better\nestimates of the gradient of the log density, the score, which describes the\ndistribution's shape.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 16:40:45 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 18:32:27 GMT"}, {"version": "v3", "created": "Mon, 13 May 2019 22:32:00 GMT"}, {"version": "v4", "created": "Thu, 14 Jan 2021 18:37:55 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wenliang", "Li", ""], ["Sutherland", "Danica J.", ""], ["Strathmann", "Heiko", ""], ["Gretton", "Arthur", ""]]}, {"id": "1811.08359", "submitter": "Joey Huchette", "authors": "Ross Anderson, Joey Huchette, Christian Tjandraatmadja, Juan Pablo\n  Vielma", "title": "Strong mixed-integer programming formulations for trained neural\n  networks", "comments": "Extended abstract of arXiv:1811.01988 [math.OC]", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an ideal mixed-integer programming (MIP) formulation for a\nrectified linear unit (ReLU) appearing in a trained neural network. Our\nformulation requires a single binary variable and no additional continuous\nvariables beyond the input and output variables of the ReLU. We contrast it\nwith an ideal \"extended\" formulation with a linear number of additional\ncontinuous variables, derived through standard techniques. An apparent drawback\nof our formulation is that it requires an exponential number of inequality\nconstraints, but we provide a routine to separate the inequalities in linear\ntime. We also prove that these exponentially-many constraints are\nfacet-defining under mild conditions. Finally, we study network verification\nproblems and observe that dynamically separating from the exponential\ninequalities 1) is much more computationally efficient and scalable than the\nextended formulation, 2) decreases the solve time of a state-of-the-art MIP\nsolver by a factor of 7 on smaller instances, and 3) nearly matches the dual\nbounds of a state-of-the-art MIP solver on harder instances, after just a few\nrounds of separation and in orders of magnitude less time.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 16:46:37 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 19:12:35 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Anderson", "Ross", ""], ["Huchette", "Joey", ""], ["Tjandraatmadja", "Christian", ""], ["Vielma", "Juan Pablo", ""]]}, {"id": "1811.08366", "submitter": "Stephen Bonner", "authors": "Stephen Bonner, John Brennan, Ibad Kureshi, Georgios Theodoropoulos,\n  Andrew Stephen McGough, Boguslaw Obara", "title": "Temporal Graph Offset Reconstruction: Towards Temporally Robust Graph\n  Representation Learning", "comments": "Accepted as a workshop paper at IEEE Big Data 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are a commonly used construct for representing relationships between\nelements in complex high dimensional datasets. Many real-world phenomenon are\ndynamic in nature, meaning that any graph used to represent them is inherently\ntemporal. However, many of the machine learning models designed to capture\nknowledge about the structure of these graphs ignore this rich temporal\ninformation when creating representations of the graph. This results in models\nwhich do not perform well when used to make predictions about the future state\nof the graph -- especially when the delta between time stamps is not small. In\nthis work, we explore a novel training procedure and an associated unsupervised\nmodel which creates graph representations optimised to predict the future state\nof the graph. We make use of graph convolutional neural networks to encode the\ngraph into a latent representation, which we then use to train our temporal\noffset reconstruction method, inspired by auto-encoders, to predict a later\ntime point -- multiple time steps into the future. Using our method, we\ndemonstrate superior performance for the task of future link prediction\ncompared with none-temporal state-of-the-art baselines. We show our approach to\nbe capable of outperforming non-temporal baselines by 38% on a real world\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 17:01:16 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Bonner", "Stephen", ""], ["Brennan", "John", ""], ["Kureshi", "Ibad", ""], ["Theodoropoulos", "Georgios", ""], ["McGough", "Andrew Stephen", ""], ["Obara", "Boguslaw", ""]]}, {"id": "1811.08374", "submitter": "Md Mofijul Islam", "authors": "Md Mofijul Islam, Amar Debnath, Tahsin Al Sayeed, Jyotirmay Nag Setu,\n  Md Mahmudur Rahman, Md Sadman Sakib, Md Abdur Razzaque, Md. Mosaddek Khan,\n  Swakkhar Shatabda", "title": "A Gray Box Interpretable Visual Debugging Approach for Deep Sequence\n  Learning Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning algorithms are often used as black box type learning and they\nare too complex to understand. The widespread usability of Deep Learning\nalgorithms to solve various machine learning problems demands deep and\ntransparent understanding of the internal representation as well as decision\nmaking. Moreover, the learning models, trained on sequential data, such as\naudio and video data, have intricate internal reasoning process due to their\ncomplex distribution of features. Thus, a visual simulator might be helpful to\ntrace the internal decision making mechanisms in response to adversarial input\ndata, and it would help to debug and design appropriate deep learning models.\nHowever, interpreting the internal reasoning of deep learning model is not well\nstudied in the literature. In this work, we have developed a visual interactive\nweb application, namely d-DeVIS, which helps to visualize the internal\nreasoning of the learning model which is trained on the audio data. The\nproposed system allows to perceive the behavior as well as to debug the model\nby interactively generating adversarial audio data point. The web application\nof d-DeVIS is available at ddevis.herokuapp.com.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 17:13:49 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Islam", "Md Mofijul", ""], ["Debnath", "Amar", ""], ["Sayeed", "Tahsin Al", ""], ["Setu", "Jyotirmay Nag", ""], ["Rahman", "Md Mahmudur", ""], ["Sakib", "Md Sadman", ""], ["Razzaque", "Md Abdur", ""], ["Khan", "Md. Mosaddek", ""], ["Shatabda", "Swakkhar", ""]]}, {"id": "1811.08380", "submitter": "Ke Chen", "authors": "Ke Chen, Weilin Zhang, Shlomo Dubnov, Gus Xia, Wei Li", "title": "The Effect of Explicit Structure Encoding of Deep Neural Networks for\n  Symbolic Music Generation", "comments": "8 pages, 13 figures", "journal-ref": "2019 International Workshop on Multilayer Music Representation and\n  Processing (MMRP)", "doi": "10.1109/MMRP.2019.00022", "report-no": null, "categories": "cs.SD cs.AI cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent breakthroughs in artificial neural networks, deep generative\nmodels have become one of the leading techniques for computational creativity.\nDespite very promising progress on image and short sequence generation,\nsymbolic music generation remains a challenging problem since the structure of\ncompositions are usually complicated. In this study, we attempt to solve the\nmelody generation problem constrained by the given chord progression. This\nmusic meta-creation problem can also be incorporated into a plan recognition\nsystem with user inputs and predictive structural outputs. In particular, we\nexplore the effect of explicit architectural encoding of musical structure via\ncomparing two sequential generative models: LSTM (a type of RNN) and WaveNet\n(dilated temporal-CNN). As far as we know, this is the first study of applying\nWaveNet to symbolic music generation, as well as the first systematic\ncomparison between temporal-CNN and RNN for music generation. We conduct a\nsurvey for evaluation in our generations and implemented Variable Markov Oracle\nin music pattern discovery. Experimental results show that to encode structure\nmore explicitly using a stack of dilated convolution layers improved the\nperformance significantly, and a global encoding of underlying chord\nprogression into the generation procedure gains even more.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 17:35:43 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 16:53:15 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 12:42:22 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chen", "Ke", ""], ["Zhang", "Weilin", ""], ["Dubnov", "Shlomo", ""], ["Xia", "Gus", ""], ["Li", "Wei", ""]]}, {"id": "1811.08382", "submitter": "Matthew Joseph", "authors": "Matthew Joseph, Janardhan Kulkarni, Jieming Mao, Zhiwei Steven Wu", "title": "Locally Private Gaussian Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a basic private estimation problem: each of $n$ users draws a single\ni.i.d. sample from an unknown Gaussian distribution, and the goal is to\nestimate the mean of this Gaussian distribution while satisfying local\ndifferential privacy for each user. Informally, local differential privacy\nrequires that each data point is individually and independently privatized\nbefore it is passed to a learning algorithm. Locally private Gaussian\nestimation is therefore difficult because the data domain is unbounded: users\nmay draw arbitrarily different inputs, but local differential privacy\nnonetheless mandates that different users have (worst-case) similar privatized\noutput distributions. We provide both adaptive two-round solutions and\nnonadaptive one-round solutions for locally private Gaussian estimation. We\nthen partially match these upper bounds with an information-theoretic lower\nbound. This lower bound shows that our accuracy guarantees are tight up to\nlogarithmic factors for all sequentially interactive\n$(\\varepsilon,\\delta)$-locally private protocols.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 17:37:26 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 15:47:04 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Joseph", "Matthew", ""], ["Kulkarni", "Janardhan", ""], ["Mao", "Jieming", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1811.08390", "submitter": "Huan Wang", "authors": "Huan Wang, Qiming Zhang, Yuehai Wang, Haoji Hu", "title": "Structured Pruning for Efficient ConvNets via Incremental Regularization", "comments": "Accepted by NIPS 2018 workshop on \"Compact Deep Neural Network\n  Representation with Industrial Applications\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter pruning is a promising approach for CNN compression and\nacceleration by eliminating redundant model parameters with tolerable\nperformance loss. Despite its effectiveness, existing regularization-based\nparameter pruning methods usually drive weights towards zero with large and\nconstant regularization factors, which neglects the fact that the\nexpressiveness of CNNs is fragile and needs a more gentle way of regularization\nfor the networks to adapt during pruning. To solve this problem, we propose a\nnew regularization-based pruning method (named IncReg) to incrementally assign\ndifferent regularization factors to different weight groups based on their\nrelative importance, whose effectiveness is proved on popular CNNs compared\nwith state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 17:52:54 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 23:49:32 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Wang", "Huan", ""], ["Zhang", "Qiming", ""], ["Wang", "Yuehai", ""], ["Hu", "Haoji", ""]]}, {"id": "1811.08393", "submitter": "Kush Bhatia", "authors": "Kush Bhatia, Aldo Pacchiano, Nicolas Flammarion, Peter L. Bartlett,\n  Michael I. Jordan", "title": "Gen-Oja: A Two-time-scale approach for Streaming CCA", "comments": "Accepted at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problems of principal Generalized Eigenvector\ncomputation and Canonical Correlation Analysis in the stochastic setting. We\npropose a simple and efficient algorithm, Gen-Oja, for these problems. We prove\nthe global convergence of our algorithm, borrowing ideas from the theory of\nfast-mixing Markov chains and two-time-scale stochastic approximation, showing\nthat it achieves the optimal rate of convergence. In the process, we develop\ntools for understanding stochastic processes with Markovian noise which might\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 17:57:13 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 01:19:06 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Bhatia", "Kush", ""], ["Pacchiano", "Aldo", ""], ["Flammarion", "Nicolas", ""], ["Bartlett", "Peter L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1811.08412", "submitter": "Qian Wang", "authors": "Qian Wang, Ning Jia, Toby P. Breckon", "title": "A Baseline for Multi-Label Image Classification Using An Ensemble of\n  Deep Convolutional Neural Networks", "comments": "IEEE International Conference on Image Processing 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on multi-label image classification have focused on designing\nmore complex architectures of deep neural networks such as the use of attention\nmechanisms and region proposal networks. Although performance gains have been\nreported, the backbone deep models of the proposed approaches and the\nevaluation metrics employed in different works vary, making it difficult to\ncompare each fairly. Moreover, due to the lack of properly investigated\nbaselines, the advantage introduced by the proposed techniques are often\nambiguous. To address these issues, we make a thorough investigation of the\nmainstream deep convolutional neural network architectures for multi-label\nimage classification and present a strong baseline. With the use of proper data\naugmentation techniques and model ensembles, the basic deep architectures can\nachieve better performance than many existing more complex ones on three\nbenchmark datasets, providing great insight for the future studies on\nmulti-label image classification.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 18:34:22 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 15:30:10 GMT"}, {"version": "v3", "created": "Thu, 9 May 2019 10:06:24 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Wang", "Qian", ""], ["Jia", "Ning", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1811.08413", "submitter": "Yi-An Ma", "authors": "Yi-An Ma, Yuansi Chen, Chi Jin, Nicolas Flammarion, and Michael I.\n  Jordan", "title": "Sampling Can Be Faster Than Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization algorithms and Monte Carlo sampling algorithms have provided the\ncomputational foundations for the rapid growth in applications of statistical\nmachine learning in recent years. There is, however, limited theoretical\nunderstanding of the relationships between these two kinds of methodology, and\nlimited understanding of relative strengths and weaknesses. Moreover, existing\nresults have been obtained primarily in the setting of convex functions (for\noptimization) and log-concave functions (for sampling). In this setting, where\nlocal properties determine global properties, optimization algorithms are\nunsurprisingly more efficient computationally than sampling algorithms. We\ninstead examine a class of nonconvex objective functions that arise in mixture\nmodeling and multi-stable systems. In this nonconvex setting, we find that the\ncomputational complexity of sampling algorithms scales linearly with the model\ndimension while that of optimization algorithms scales exponentially.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 18:41:29 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 21:50:10 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Ma", "Yi-An", ""], ["Chen", "Yuansi", ""], ["Jin", "Chi", ""], ["Flammarion", "Nicolas", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1811.08417", "submitter": "Ananda Theertha Suresh", "authors": "Ehsan Variani, Ananda Theertha Suresh, Mitchel Weintraub", "title": "WEST: Word Encoded Sequence Transducers", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the parameters in large vocabulary models are used in embedding layer\nto map categorical features to vectors and in softmax layer for classification\nweights. This is a bottle-neck in memory constraint on-device training\napplications like federated learning and on-device inference applications like\nautomatic speech recognition (ASR). One way of compressing the embedding and\nsoftmax layers is to substitute larger units such as words with smaller\nsub-units such as characters. However, often the sub-unit models perform poorly\ncompared to the larger unit models. We propose WEST, an algorithm for encoding\ncategorical features and output classes with a sequence of random or domain\ndependent sub-units and demonstrate that this transduction can lead to\nsignificant compression without compromising performance. WEST bridges the gap\nbetween larger unit and sub-unit models and can be interpreted as a MaxEnt\nmodel over sub-unit features, which can be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 18:47:50 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Variani", "Ehsan", ""], ["Suresh", "Ananda Theertha", ""], ["Weintraub", "Mitchel", ""]]}, {"id": "1811.08425", "submitter": "Felipe Oviedo", "authors": "Felipe Oviedo, Zekun Ren, Shijing Sun, Charlie Settens, Zhe Liu, Noor\n  Titan Putri Hartono, Ramasamy Savitha, Brian L. DeCost, Siyu I.P. Tian,\n  Giuseppe Romano, Aaron Gilad Kusne, and Tonio Buonassisi", "title": "Fast and interpretable classification of small X-ray diffraction\n  datasets using data augmentation and deep neural networks", "comments": "Accepted with minor revisions in npj Computational Materials,\n  Presented in NIPS 2018 Workshop: Machine Learning for Molecules and Materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cond-mat.mtrl-sci cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  X-ray diffraction (XRD) data acquisition and analysis is among the most\ntime-consuming steps in the development cycle of novel thin-film materials. We\npropose a machine-learning-enabled approach to predict crystallographic\ndimensionality and space group from a limited number of thin-film XRD patterns.\nWe overcome the scarce-data problem intrinsic to novel materials development by\ncoupling a supervised machine learning approach with a model agnostic,\nphysics-informed data augmentation strategy using simulated data from the\nInorganic Crystal Structure Database (ICSD) and experimental data. As a test\ncase, 115 thin-film metal halides spanning 3 dimensionalities and 7\nspace-groups are synthesized and classified. After testing various algorithms,\nwe develop and implement an all convolutional neural network, with cross\nvalidated accuracies for dimensionality and space-group classification of 93%\nand 89%, respectively. We propose average class activation maps, computed from\na global average pooling layer, to allow high model interpretability by human\nexperimentalists, elucidating the root causes of misclassification. Finally, we\nsystematically evaluate the maximum XRD pattern step size (data acquisition\nrate) before loss of predictive accuracy occurs, and determine it to be\n0.16{\\deg}, which enables an XRD pattern to be obtained and classified in 5.5\nminutes or less.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 21:05:21 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 20:34:50 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Oviedo", "Felipe", ""], ["Ren", "Zekun", ""], ["Sun", "Shijing", ""], ["Settens", "Charlie", ""], ["Liu", "Zhe", ""], ["Hartono", "Noor Titan Putri", ""], ["Savitha", "Ramasamy", ""], ["DeCost", "Brian L.", ""], ["Tian", "Siyu I. P.", ""], ["Romano", "Giuseppe", ""], ["Kusne", "Aaron Gilad", ""], ["Buonassisi", "Tonio", ""]]}, {"id": "1811.08458", "submitter": "Isay Katsman", "authors": "Qian Huang, Zeqi Gu, Isay Katsman, Horace He, Pian Pawakapan, Zhiqiu\n  Lin, Serge Belongie, Ser-Nam Lim", "title": "Intermediate Level Adversarial Attack for Enhanced Transferability", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are vulnerable to adversarial examples, malicious inputs\ncrafted to fool trained models. Adversarial examples often exhibit black-box\ntransfer, meaning that adversarial examples for one model can fool another\nmodel. However, adversarial examples may be overfit to exploit the particular\narchitecture and feature representation of a source model, resulting in\nsub-optimal black-box transfer attacks to other target models. This leads us to\nintroduce the Intermediate Level Attack (ILA), which attempts to fine-tune an\nexisting adversarial example for greater black-box transferability by\nincreasing its perturbation on a pre-specified layer of the source model. We\nshow that our method can effectively achieve this goal and that we can decide a\nnearly-optimal layer of the source model to perturb without any knowledge of\nthe target models.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 19:40:24 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Huang", "Qian", ""], ["Gu", "Zeqi", ""], ["Katsman", "Isay", ""], ["He", "Horace", ""], ["Pawakapan", "Pian", ""], ["Lin", "Zhiqiu", ""], ["Belongie", "Serge", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "1811.08469", "submitter": "Alistair Letcher", "authors": "Alistair Letcher, Jakob Foerster, David Balduzzi, Tim Rockt\\\"aschel,\n  Shimon Whiteson", "title": "Stable Opponent Shaping in Differentiable Games", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of learning methods are actually differentiable games whose\nplayers optimise multiple, interdependent objectives in parallel -- from GANs\nand intrinsic curiosity to multi-agent RL. Opponent shaping is a powerful\napproach to improve learning dynamics in these games, accounting for player\ninfluence on others' updates. Learning with Opponent-Learning Awareness (LOLA)\nis a recent algorithm that exploits this response and leads to cooperation in\nsettings like the Iterated Prisoner's Dilemma. Although experimentally\nsuccessful, we show that LOLA agents can exhibit 'arrogant' behaviour directly\nat odds with convergence. In fact, remarkably few algorithms have theoretical\nguarantees applying across all (n-player, non-convex) games. In this paper we\npresent Stable Opponent Shaping (SOS), a new method that interpolates between\nLOLA and a stable variant named LookAhead. We prove that LookAhead converges\nlocally to equilibria and avoids strict saddles in all differentiable games.\nSOS inherits these essential guarantees, while also shaping the learning of\nopponents and consistently either matching or outperforming LOLA\nexperimentally.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 20:06:37 GMT"}, {"version": "v2", "created": "Sun, 27 Jan 2019 15:56:30 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2021 09:21:26 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Letcher", "Alistair", ""], ["Foerster", "Jakob", ""], ["Balduzzi", "David", ""], ["Rockt\u00e4schel", "Tim", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1811.08511", "submitter": "Yunfeng Zhang", "authors": "Yunfeng Zhang, Irina Gaynanova", "title": "Joint association and classification analysis of multi-view data", "comments": "Major revision of the paper structure. More simulation and data\n  analysis results were added to the manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view data, that is matched sets of measurements on the same subjects,\nhave become increasingly common with advances in multi-omics technology. Often,\nit is of interest to find associations between the views that are related to\nthe intrinsic class memberships. Existing association methods cannot directly\nincorporate class information, while existing classification methods do not\ntake into account between-views associations. In this work, we propose a\nframework for Joint Association and Classification Analysis of multi-view data\n(JACA). Our goal is not to merely improve the misclassification rates, but to\nprovide a latent representation of high-dimensional data that is both relevant\nfor the subtype discrimination and coherent across the views. We motivate the\nmethodology by establishing a connection between canonical correlation analysis\nand discriminant analysis. We also establish the estimation consistency of JACA\nin high-dimensional settings. A distinct advantage of JACA is that it can be\napplied to the multi-view data with block-missing structure, that is to cases\nwhere a subset of views or class labels is missing for some subjects. The\napplication of JACA to quantify the associations between RNAseq and miRNA views\nwith respect to consensus molecular subtypes in colorectal cancer data from The\nCancer Genome Atlas project leads to improved misclassification rates and\nstronger found associations compared to existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 22:15:19 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 08:47:23 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Zhang", "Yunfeng", ""], ["Gaynanova", "Irina", ""]]}, {"id": "1811.08537", "submitter": "Till Hartmann", "authors": "Till S. Hartmann", "title": "Seeing in the dark with recurrent convolutional neural networks", "comments": "12 pages (with appendix), 6 figure (main text), 3 supplementary\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical convolutional neural networks (cCNNs) are very good at categorizing\nobjects in images. But, unlike human vision which is relatively robust to noise\nin images, the performance of cCNNs declines quickly as image quality worsens.\nHere we propose to use recurrent connections within the convolutional layers to\nmake networks robust against pixel noise such as could arise from imaging at\nlow light levels, and thereby significantly increase their performance when\ntested with simulated noisy video sequences. We show that cCNNs classify images\nwith high signal to noise ratios (SNRs) well, but are easily outperformed when\ntested with low SNR images (high noise levels) by convolutional neural networks\nthat have recurrency added to convolutional layers, henceforth referred to as\ngruCNNs. Addition of Bayes-optimal temporal integration to allow the cCNN to\nintegrate multiple image frames still does not match gruCNN performance.\nAdditionally, we show that at low SNRs, the probabilities predicted by the\ngruCNN (after calibration) have higher confidence than those predicted by the\ncCNN. We propose to consider recurrent connections in the early stages of\nneural networks as a solution to computer vision under imperfect lighting\nconditions and noisy environments; challenges faced during real-time video\nstreams of autonomous driving at night, during rain or snow, and other\nnon-ideal situations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 01:05:48 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Hartmann", "Till S.", ""]]}, {"id": "1811.08540", "submitter": "Wen Sun", "authors": "Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford", "title": "Model-based RL in Contextual Decision Processes: PAC bounds and\n  Exponential Improvements over Model-free Approaches", "comments": "COLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the sample complexity of model-based reinforcement learning\n(henceforth RL) in general contextual decision processes that require strategic\nexploration to find a near-optimal policy. We design new algorithms for RL with\na generic model class and analyze their statistical properties. Our algorithms\nhave sample complexity governed by a new structural parameter called the\nwitness rank, which we show to be small in several settings of interest,\nincluding factored MDPs. We also show that the witness rank is never larger\nthan the recently proposed Bellman rank parameter governing the sample\ncomplexity of the model-free algorithm OLIVE (Jiang et al., 2017), the only\nother provably sample-efficient algorithm for global exploration at this level\nof generality. Focusing on the special case of factored MDPs, we prove an\nexponential lower bound for a general class of model-free approaches, including\nOLIVE, which, when combined with our algorithmic results, demonstrates\nexponential separation between model-based and model-free RL in some\nrich-observation settings.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 01:48:17 GMT"}, {"version": "v2", "created": "Sat, 2 Feb 2019 04:05:04 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 05:35:14 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Sun", "Wen", ""], ["Jiang", "Nan", ""], ["Krishnamurthy", "Akshay", ""], ["Agarwal", "Alekh", ""], ["Langford", "John", ""]]}, {"id": "1811.08549", "submitter": "Alexander Peysakhovich", "authors": "Alexander Peysakhovich", "title": "Reinforcement Learning and Inverse Reinforcement Learning with System 1\n  and System 2", "comments": "Published in AAAI-AIES 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring a person's goal from their behavior is an important problem in\napplications of AI (e.g. automated assistants, recommender systems). The\nworkhorse model for this task is the rational actor model - this amounts to\nassuming that people have stable reward functions, discount the future\nexponentially, and construct optimal plans. Under the rational actor assumption\ntechniques such as inverse reinforcement learning (IRL) can be used to infer a\nperson's goals from their actions. A competing model is the dual-system model.\nHere decisions are the result of an interplay between a fast, automatic,\nheuristic-based system 1 and a slower, deliberate, calculating system 2. We\ngeneralize the dual system framework to the case of Markov decision problems\nand show how to compute optimal plans for dual-system agents. We show that\ndual-system agents exhibit behaviors that are incompatible with rational actor\nassumption. We show that naive applications of rational-actor IRL to the\nbehavior of dual-system agents can generate wrong inference about the agents'\ngoals and suggest interventions that actually reduce the agent's overall\nutility. Finally, we adapt a simple IRL algorithm to correctly infer the goals\nof dual system decision-makers. This allows us to make interventions that help,\nrather than hinder, the dual-system agent's ability to reach their true goals.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 22:36:53 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 18:41:08 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Peysakhovich", "Alexander", ""]]}, {"id": "1811.08552", "submitter": "Soumitro Chakrabarty", "authors": "Soumitro Chakrabarty and Emanu\\\"el A. P. Habets", "title": "Multi-scale aggregation of phase information for reducing computational\n  cost of CNN based DOA estimation", "comments": "arXiv admin note: text overlap with arXiv:1807.11722", "journal-ref": null, "doi": "10.23919/EUSIPCO.2019.8903176", "report-no": null, "categories": "eess.AS cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent work on direction-of-arrival (DOA) estimation of multiple\nspeakers with convolutional neural networks (CNNs), the phase component of\nshort-time Fourier transform (STFT) coefficients of the microphone signal is\ngiven as input and small filters are used to learn the phase relations between\nneighboring microphones. Due to this chosen filter size, $M-1$ convolution\nlayers are required to achieve the best performance for a microphone array with\nM microphones. For arrays with large number of microphones, this requirement\nleads to a high computational cost making the method practically infeasible. In\nthis work, we propose to use systematic dilations of the convolution filters in\neach of the convolution layers of the previously proposed CNN for expansion of\nthe receptive field of the filters to reduce the computational cost of the\nmethod. Different strategies for expansion of the receptive field of the\nfilters for a specific microphone array are explored. With experimental\nanalysis of the different strategies, it is shown that an aggressive expansion\nstrategy results in a considerable reduction in computational cost while a\nrelatively gradual expansion of the receptive field exhibits the best DOA\nestimation performance along with reduction in the computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 12:29:51 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Chakrabarty", "Soumitro", ""], ["Habets", "Emanu\u00ebl A. P.", ""]]}, {"id": "1811.08561", "submitter": "Yong Liu", "authors": "Yong Liu, Lin Shang, Andy Song", "title": "Adaptive Re-ranking of Deep Feature for Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical person re-identification (re-ID) methods train a deep CNN to extract\ndeep features and combine them with a distance metric for the final evaluation.\nIn this work, we focus on exploiting the full information encoded in the deep\nfeature to boost the re-ID performance. First, we propose a Deep Feature Fusion\n(DFF) method to exploit the diverse information embedded in a deep feature. DFF\ntreats each sub-feature as an information carrier and employs a diffusion\nprocess to exchange their information. Second, we propose an Adaptive\nRe-Ranking (ARR) method to exploit the contextual information encoded in the\nfeatures of neighbors. ARR utilizes the contextual information to re-rank the\nretrieval results in an iterative manner. Particularly, it adds more contextual\ninformation after each iteration automatically to consider more matches. Third,\nwe propose a strategy that combines DFF and ARR to enhance the performance.\nExtensive comparative evaluations demonstrate the superiority of the proposed\nmethods on three large benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 02:22:03 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Liu", "Yong", ""], ["Shang", "Lin", ""], ["Song", "Andy", ""]]}, {"id": "1811.08568", "submitter": "Jonas Gehring", "authors": "Jonas Gehring, Da Ju, Vegard Mella, Daniel Gant, Nicolas Usunier,\n  Gabriel Synnaeve", "title": "High-Level Strategy Selection under Partial Observability in StarCraft:\n  Brood War", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of high-level strategy selection in the adversarial\nsetting of real-time strategy games from a reinforcement learning perspective,\nwhere taking an action corresponds to switching to the respective strategy.\nHere, a good strategy successfully counters the opponent's current and possible\nfuture strategies which can only be estimated using partial observations. We\ninvestigate whether we can utilize the full game state information during\ntraining time (in the form of an auxiliary prediction task) to increase\nperformance. Experiments carried out within a StarCraft: Brood War bot against\nstrong community bots show substantial win rate improvements over a\nfixed-strategy baseline and encouraging results when learning with the\nauxiliary task.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 02:27:16 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Gehring", "Jonas", ""], ["Ju", "Da", ""], ["Mella", "Vegard", ""], ["Gant", "Daniel", ""], ["Usunier", "Nicolas", ""], ["Synnaeve", "Gabriel", ""]]}, {"id": "1811.08577", "submitter": "Utku Ozbulak", "authors": "Utku Ozbulak, Wesley De Neve, Arnout Van Messem", "title": "How the Softmax Output is Misleading for Evaluating the Strength of\n  Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even before deep learning architectures became the de facto models for\ncomplex computer vision tasks, the softmax function was, given its elegant\nproperties, already used to analyze the predictions of feedforward neural\nnetworks. Nowadays, the output of the softmax function is also commonly used to\nassess the strength of adversarial examples: malicious data points designed to\nfail machine learning models during the testing phase. However, in this paper,\nwe show that it is possible to generate adversarial examples that take\nadvantage of some properties of the softmax function, leading to undesired\noutcomes when interpreting the strength of the adversarial examples at hand.\nSpecifically, we argue that the output of the softmax function is a poor\nindicator when the strength of an adversarial example is analyzed and that this\nindicator can be easily tricked by already existing methods for adversarial\nexample generation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 02:52:52 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Ozbulak", "Utku", ""], ["De Neve", "Wesley", ""], ["Van Messem", "Arnout", ""]]}, {"id": "1811.08579", "submitter": "Vishwali Mhasawade", "authors": "Vishwali Mhasawade, Nabeel Abdur Rehman, Rumi Chunara", "title": "Population-aware Hierarchical Bayesian Domain Adaptation", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/105", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population attributes are essential in health for understanding who the data\nrepresents and precision medicine efforts. Even within disease infection\nlabels, patients can exhibit significant variability; \"fever\" may mean\nsomething different when reported in a doctor's office versus from an online\napp, precluding directly learning across different datasets for the same\nprediction task. This problem falls into the domain adaptation paradigm.\nHowever, research in this area has to-date not considered who generates the\ndata; symptoms reported by a woman versus a man, for example, could also have\ndifferent implications. We propose a novel population-aware domain adaptation\napproach by formulating the domain adaptation task as a multi-source\nhierarchical Bayesian framework. The model improves prediction in the case of\nlargely unlabelled target data by harnessing both domain and population\ninvariant information.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 02:59:16 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Mhasawade", "Vishwali", ""], ["Rehman", "Nabeel Abdur", ""], ["Chunara", "Rumi", ""]]}, {"id": "1811.08581", "submitter": "Chuanxing Geng", "authors": "Chuanxing Geng, Sheng-jun Huang and Songcan Chen", "title": "Recent Advances in Open Set Recognition: A Survey", "comments": "Accepted by IEEE TPAMI", "journal-ref": null, "doi": "10.1109/TPAMI.2020.2981604", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world recognition/classification tasks, limited by various objective\nfactors, it is usually difficult to collect training samples to exhaust all\nclasses when training a recognizer or classifier. A more realistic scenario is\nopen set recognition (OSR), where incomplete knowledge of the world exists at\ntraining time, and unknown classes can be submitted to an algorithm during\ntesting, requiring the classifiers to not only accurately classify the seen\nclasses, but also effectively deal with the unseen ones. This paper provides a\ncomprehensive survey of existing open set recognition techniques covering\nvarious aspects ranging from related definitions, representations of models,\ndatasets, evaluation criteria, and algorithm comparisons. Furthermore, we\nbriefly analyze the relationships between OSR and its related tasks including\nzero-shot, one-shot (few-shot) recognition/learning techniques, classification\nwith reject option, and so forth. Additionally, we also overview the open world\nrecognition which can be seen as a natural extension of OSR. Importantly, we\nhighlight the limitations of existing approaches and point out some promising\nsubsequent research directions in this field.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 03:20:47 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 12:51:14 GMT"}, {"version": "v3", "created": "Sun, 23 Feb 2020 01:35:04 GMT"}, {"version": "v4", "created": "Sat, 21 Mar 2020 13:54:47 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Geng", "Chuanxing", ""], ["Huang", "Sheng-jun", ""], ["Chen", "Songcan", ""]]}, {"id": "1811.08585", "submitter": "Chaoqi Chen", "authors": "Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong, Xinghao Ding, Yue\n  Huang, Tingyang Xu, Junzhou Huang", "title": "Progressive Feature Alignment for Unsupervised Domain Adaptation", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) transfers knowledge from a label-rich\nsource domain to a fully-unlabeled target domain. To tackle this task, recent\napproaches resort to discriminative domain transfer in virtue of pseudo-labels\nto enforce the class-level distribution alignment across the source and target\ndomains. These methods, however, are vulnerable to the error accumulation and\nthus incapable of preserving cross-domain category consistency, as the\npseudo-labeling accuracy is not guaranteed explicitly. In this paper, we\npropose the Progressive Feature Alignment Network (PFAN) to align the\ndiscriminative features across domains progressively and effectively, via\nexploiting the intra-class variation in the target domain. To be specific, we\nfirst develop an Easy-to-Hard Transfer Strategy (EHTS) and an Adaptive\nPrototype Alignment (APA) step to train our model iteratively and\nalternatively. Moreover, upon observing that a good domain adaptation usually\nrequires a non-saturated source classifier, we consider a simple yet efficient\nway to retard the convergence speed of the source classification loss by\nfurther involving a temperature variate into the soft-max function. The\nextensive experimental results reveal that the proposed PFAN exceeds the\nstate-of-the-art performance on three UDA datasets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 03:32:31 GMT"}, {"version": "v2", "created": "Sun, 19 May 2019 11:43:21 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Chen", "Chaoqi", ""], ["Xie", "Weiping", ""], ["Huang", "Wenbing", ""], ["Rong", "Yu", ""], ["Ding", "Xinghao", ""], ["Huang", "Yue", ""], ["Xu", "Tingyang", ""], ["Huang", "Junzhou", ""]]}, {"id": "1811.08586", "submitter": "Changjian Li", "authors": "Changjian Li, Krzysztof Czarnecki", "title": "Urban Driving with Multi-Objective Deep Reinforcement Learning", "comments": "Accepted at AAMAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous driving is a challenging domain that entails multiple aspects: a\nvehicle should be able to drive to its destination as fast as possible while\navoiding collision, obeying traffic rules and ensuring the comfort of\npassengers. In this paper, we present a deep learning variant of thresholded\nlexicographic Q-learning for the task of urban driving. Our multi-objective DQN\nagent learns to drive on multi-lane roads and intersections, yielding and\nchanging lanes according to traffic rules. We also propose an extension for\nfactored Markov Decision Processes to the DQN architecture that provides\nauxiliary features for the Q function. This is shown to significantly improve\ndata efficiency. We then show that the learned policy is able to zero-shot\ntransfer to a ring road without sacrificing performance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 03:36:52 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 22:03:26 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Li", "Changjian", ""], ["Czarnecki", "Krzysztof", ""]]}, {"id": "1811.08589", "submitter": "Mengdi Wang", "authors": "Mengdi Wang, Qing Zhang, Jun Yang, Xiaoyuan Cui and Wei Lin", "title": "Graph-Adaptive Pruning for Efficient Inference of Convolutional Neural\n  Networks", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a graph-adaptive pruning (GAP) method for efficient\ninference of convolutional neural networks (CNNs). In this method, the network\nis viewed as a computational graph, in which the vertices denote the\ncomputation nodes and edges represent the information flow. Through topology\nanalysis, GAP is capable of adapting to different network structures,\nespecially the widely used cross connections and multi-path data flow in recent\nnovel convolutional models. The models can be adaptively pruned at vertex-level\nas well as edge-level without any post-processing, thus GAP can directly get\npractical model compression and inference speed-up. Moreover, it does not need\nany customized computation library or hardware support. Finetuning is conducted\nafter pruning to restore the model performance. In the finetuning step, we\nadopt a self-taught knowledge distillation (KD) strategy by utilizing\ninformation from the original model, through which, the performance of the\noptimized model can be sufficiently improved, without introduction of any other\nteacher model. Experimental results show the proposed GAP can achieve promising\nresult to make inference more efficient, e.g., for ResNeXt-29 on CIFAR10, it\ncan get 13X model compression and 4.3X practical speed-up with marginal loss of\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 03:43:38 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Wang", "Mengdi", ""], ["Zhang", "Qing", ""], ["Yang", "Jun", ""], ["Cui", "Xiaoyuan", ""], ["Lin", "Wei", ""]]}, {"id": "1811.08611", "submitter": "Wanchen Sui", "authors": "Wanchen Sui, Qing Zhang, Jun Yang, Wei Chu", "title": "A Novel Integrated Framework for Learning both Text Detection and\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel integrated framework for learning both text\ndetection and recognition. For most of the existing methods, detection and\nrecognition are treated as two isolated tasks and trained separately, since\nparameters of detection and recognition models are different and two models\ntarget to optimize their own loss functions during individual training\nprocesses. In contrast to those methods, by sharing model parameters, we merge\nthe detection model and recognition model into a single end-to-end trainable\nmodel and train the joint model for two tasks simultaneously. The shared\nparameters not only help effectively reduce the computational load in inference\nprocess, but also improve the end-to-end text detection-recognition accuracy.\nIn addition, we design a simpler and faster sequence learning method for the\nrecognition network based on a succession of stacked convolutional layers\nwithout any recurrent structure, this is proved feasible and dramatically\nimproves inference speed. Extensive experiments on different datasets\ndemonstrate that the proposed method achieves very promising results.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 07:14:34 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Sui", "Wanchen", ""], ["Zhang", "Qing", ""], ["Yang", "Jun", ""], ["Chu", "Wei", ""]]}, {"id": "1811.08615", "submitter": "Tzu-Ming Harry Hsu", "authors": "Tzu-Ming Harry Hsu, Wei-Hung Weng, Willie Boag, Matthew McDermott,\n  Peter Szolovits", "title": "Unsupervised Multimodal Representation Learning across Medical Images\n  and Reports", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/215", "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint embeddings between medical imaging modalities and associated radiology\nreports have the potential to offer significant benefits to the clinical\ncommunity, ranging from cross-domain retrieval to conditional generation of\nreports to the broader goals of multimodal representation learning. In this\nwork, we establish baseline joint embedding results measured via both local and\nglobal retrieval methods on the soon to be released MIMIC-CXR dataset\nconsisting of both chest X-ray images and the associated radiology reports. We\nexamine both supervised and unsupervised methods on this task and show that for\ndocument retrieval tasks with the learned representations, only a limited\namount of supervision is needed to yield results comparable to those of\nfully-supervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 07:24:31 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Hsu", "Tzu-Ming Harry", ""], ["Weng", "Wei-Hung", ""], ["Boag", "Willie", ""], ["McDermott", "Matthew", ""], ["Szolovits", "Peter", ""]]}, {"id": "1811.08633", "submitter": "Kazuki Tachikawa", "authors": "Kazuki Tachikawa, Yuji Kawai, Jihoon Park, Minoru Asada", "title": "Compensated Integrated Gradients to Reliably Interpret EEG\n  Classification", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/63", "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrated gradients are widely employed to evaluate the contribution of\ninput features in classification models because it satisfies the axioms for\nattribution of prediction. This method, however, requires an appropriate\nbaseline for reliable determination of the contributions. We propose a\ncompensated integrated gradients method that does not require a baseline. In\nfact, the method compensates the attributions calculated by integrated\ngradients at an arbitrary baseline using Shapley sampling. We prove that the\nmethod retrieves reliable attributions if the processes of input features in a\nclassifier are mutually independent, and they are identical like shared weights\nin convolutional neural networks. Using three electroencephalogram datasets, we\nexperimentally demonstrate that the attributions of the proposed method are\nmore reliable than those of the original integrated gradients, and its\ncomputational complexity is much lower than that of Shapley sampling.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 08:37:40 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Tachikawa", "Kazuki", ""], ["Kawai", "Yuji", ""], ["Park", "Jihoon", ""], ["Asada", "Minoru", ""]]}, {"id": "1811.08674", "submitter": "Raghavendra Selvan", "authors": "Raghavendra Selvan, Thomas Kipf, Max Welling, Antonio Garcia-Uceda\n  Juarez, Jesper H Pedersen, Jens Petersen, Marleen de Bruijne", "title": "Graph Refinement based Airway Extraction using Mean-Field Networks and\n  Graph Neural Networks", "comments": "Accepted for publication at Medical Image Analysis. 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph refinement, or the task of obtaining subgraphs of interest from\nover-complete graphs, can have many varied applications. In this work, we\nextract trees or collection of sub-trees from image data by, first deriving a\ngraph-based representation of the volumetric data and then, posing the tree\nextraction as a graph refinement task. We present two methods to perform graph\nrefinement. First, we use mean-field approximation (MFA) to approximate the\nposterior density over the subgraphs from which the optimal subgraph of\ninterest can be estimated. Mean field networks (MFNs) are used for inference\nbased on the interpretation that iterations of MFA can be seen as feed-forward\noperations in a neural network. This allows us to learn the model parameters\nusing gradient descent. Second, we present a supervised learning approach using\ngraph neural networks (GNNs) which can be seen as generalisations of MFNs.\nSubgraphs are obtained by training a GNN-based graph refinement model to\ndirectly predict edge probabilities. We discuss connections between the two\nclasses of methods and compare them for the task of extracting airways from 3D,\nlow-dose, chest CT data. We show that both the MFN and GNN models show\nsignificant improvement when compared to one baseline method, that is similar\nto a top performing method in the EXACT'09 Challenge, and a 3D U-Net based\nairway segmentation model, in detecting more branches with fewer false\npositives.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 10:50:31 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 16:14:58 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Selvan", "Raghavendra", ""], ["Kipf", "Thomas", ""], ["Welling", "Max", ""], ["Juarez", "Antonio Garcia-Uceda", ""], ["Pedersen", "Jesper H", ""], ["Petersen", "Jens", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1811.08687", "submitter": "Rohitash Chandra", "authors": "Rohitash Chandra, Konark Jain, Arpit Kapoor, and Ashray Aman", "title": "Surrogate-assisted parallel tempering for Bayesian neural learning", "comments": "Engineering Applications of Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the need for robust uncertainty quantification, Bayesian neural\nlearning has gained attention in the era of deep learning and big data. Markov\nChain Monte-Carlo (MCMC) methods typically implement Bayesian inference which\nfaces several challenges given a large number of parameters, complex and\nmultimodal posterior distributions, and computational complexity of large\nneural network models. Parallel tempering MCMC addresses some of these\nlimitations given that they can sample multimodal posterior distributions and\nutilize high-performance computing. However, certain challenges remain given\nlarge neural network models and big data. Surrogate-assisted optimization\nfeatures the estimation of an objective function for models which are\ncomputationally expensive. In this paper, we address the inefficiency of\nparallel tempering MCMC for large-scale problems by combining parallel\ncomputing features with surrogate assisted likelihood estimation that describes\nthe plausibility of a model parameter value, given specific observed data.\nHence, we present surrogate-assisted parallel tempering for Bayesian neural\nlearning for simple to computationally expensive models. Our results\ndemonstrate that the methodology significantly lowers the computational cost\nwhile maintaining quality in decision making with Bayesian neural networks. The\nmethod has applications for a Bayesian inversion and uncertainty quantification\nfor a broad range of numerical models.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 11:14:05 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 06:28:07 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 12:41:47 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Chandra", "Rohitash", ""], ["Jain", "Konark", ""], ["Kapoor", "Arpit", ""], ["Aman", "Ashray", ""]]}, {"id": "1811.08695", "submitter": "Irene Giacomelli", "authors": "Irene Giacomelli, Somesh Jha, Ross Kleiman, David Page, Kyonghwan Yoon", "title": "Privacy-Preserving Collaborative Prediction using Random Forests", "comments": "Accepted at the AMIA Informatics Summit 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of privacy-preserving machine learning (PPML) for\nensemble methods, focusing our effort on random forests. In collaborative\nanalysis, PPML attempts to solve the conflict between the need for data sharing\nand privacy. This is especially important in privacy sensitive applications\nsuch as learning predictive models for clinical decision support from EHR data\nfrom different clinics, where each clinic has a responsibility for its\npatients' privacy. We propose a new approach for ensemble methods: each entity\nlearns a model, from its own data, and then when a client asks the prediction\nfor a new private instance, the answers from all the locally trained models are\nused to compute the prediction in such a way that no extra information is\nrevealed. We implement this approach for random forests and we demonstrate its\nhigh efficiency and potential accuracy benefit via experiments on real-world\ndatasets, including actual EHR data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 11:44:52 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Giacomelli", "Irene", ""], ["Jha", "Somesh", ""], ["Kleiman", "Ross", ""], ["Page", "David", ""], ["Yoon", "Kyonghwan", ""]]}, {"id": "1811.08701", "submitter": "Shahin Pourbahrami", "authors": "Shahin Pourbahrami", "title": "Improving PSO Global Method for Feature Selection According to\n  Iterations Global Search and Chaotic Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Making a simple model by choosing a limited number of features with the\npurpose of reducing the computational complexity of the algorithms involved in\nclassification is one of the main issues in machine learning and data mining.\nThe aim of Feature Selection (FS) is to reduce the number of redundant and\nirrelevant features and improve the accuracy of classification in a data set.\nWe propose an efficient ISPSO-GLOBAL (Improved Seeding Particle Swarm\nOptimization GLOBAL) method which investigates the specified iterations to\nproduce prominent features and store them in storage list. The goal is to find\ninformative features based on its iteration frequency with favorable fitness\nfor the next generation and high exploration. Our method exploits of a new\ninitialization strategy in PSO which improves space search and utilizes chaos\ntheory to enhance the population initialization, then we offer a new formula to\ndetermine the features size used in proposed method. Our experiments with\nreal-world data sets show that the performance of the ISPSO-GLOBAL is superior\ncomparing with state-of-the-art methods in most of the data sets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 11:53:58 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Pourbahrami", "Shahin", ""]]}, {"id": "1811.08705", "submitter": "Joewie Koh", "authors": "Joewie J. Koh and Barton Rhodes", "title": "Inline Detection of Domain Generation Algorithms with Context-Sensitive\n  Word Embeddings", "comments": "6 pages, 5 figures, 2 tables", "journal-ref": "Proceedings of the 2018 IEEE International Conference on Big Data,\n  2018, pp. 2966-2971", "doi": "10.1109/BigData.2018.8622066", "report-no": null, "categories": "cs.CR cs.CL cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain generation algorithms (DGAs) are frequently employed by malware to\ngenerate domains used for connecting to command-and-control (C2) servers.\nRecent work in DGA detection leveraged deep learning architectures like\nconvolutional neural networks (CNNs) and character-level long short-term memory\nnetworks (LSTMs) to classify domains. However, these classifiers perform poorly\nwith wordlist-based DGA families, which generate domains by pseudorandomly\nconcatenating dictionary words. We propose a novel approach that combines\ncontext-sensitive word embeddings with a simple fully-connected classifier to\nperform classification of domains based on word-level information. The word\nembeddings were pre-trained on a large unrelated corpus and left frozen during\nthe training on domain data. The resulting small number of trainable parameters\nenabled extremely short training durations, while the transfer of language\nknowledge stored in the representations allowed for high-performing models with\nsmall training datasets. We show that this architecture reliably outperformed\nexisting techniques on wordlist-based DGA families with just 30 DGA training\nexamples and achieved state-of-the-art performance with around 100 DGA training\nexamples, all while requiring an order of magnitude less time to train compared\nto current techniques. Of special note is the technique's performance on the\nmatsnu DGA: the classifier attained a 89.5% detection rate with a 1:1,000 false\npositive rate (FPR) after training on only 30 examples of the DGA domains, and\na 91.2% detection rate with a 1:10,000 FPR after 90 examples. Considering that\nsome of these DGAs have wordlists of several hundred words, our results\ndemonstrate that this technique does not rely on the classifier learning the\nDGA wordlists. Instead, the classifier is able to learn the semantic signatures\nof the wordlist-based DGA families.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 12:14:12 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Koh", "Joewie J.", ""], ["Rhodes", "Barton", ""]]}, {"id": "1811.08723", "submitter": "Conor Durkan", "authors": "Conor Durkan, George Papamakarios, Iain Murray", "title": "Sequential Neural Methods for Likelihood-free Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood-free inference refers to inference when a likelihood function\ncannot be explicitly evaluated, which is often the case for models based on\nsimulators. Most of the literature is based on sample-based `Approximate\nBayesian Computation' methods, but recent work suggests that approaches based\non deep neural conditional density estimators can obtain state-of-the-art\nresults with fewer simulations. The neural approaches vary in how they choose\nwhich simulations to run and what they learn: an approximate posterior or a\nsurrogate likelihood. This work provides some direct controlled comparisons\nbetween these choices.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 13:39:40 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Durkan", "Conor", ""], ["Papamakarios", "George", ""], ["Murray", "Iain", ""]]}, {"id": "1811.08725", "submitter": "Tatiana Shpakova", "authors": "Tatiana Shpakova, Francis Bach, Anton Osokin", "title": "Marginal Weighted Maximum Log-likelihood for Efficient Learning of\n  Perturb-and-Map models", "comments": "Published in Proceedings of the Conference of Uncertainty in\n  Artificial Intelligence (UAI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the structured-output prediction problem through probabilistic\napproaches and generalize the \"perturb-and-MAP\" framework to more challenging\nweighted Hamming losses, which are crucial in applications. While in principle\nour approach is a straightforward marginalization, it requires solving many\nrelated MAP inference problems. We show that for log-supermodular pairwise\nmodels these operations can be performed efficiently using the machinery of\ndynamic graph cuts. We also propose to use double stochastic gradient descent,\nboth on the data and on the perturbations, for efficient learning. Our\nframework can naturally take weak supervision (e.g., partial labels) into\naccount. We conduct a set of experiments on medium-scale character recognition\nand image segmentation, showing the benefits of our algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 13:41:21 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Shpakova", "Tatiana", ""], ["Bach", "Francis", ""], ["Osokin", "Anton", ""]]}, {"id": "1811.08737", "submitter": "Honghui Shi", "authors": "Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana\n  Rosing, Rogerio Feris", "title": "SpotTune: Transfer Learning through Adaptive Fine-tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning, which allows a source task to affect the inductive bias of\nthe target task, is widely used in computer vision. The typical way of\nconducting transfer learning with deep neural networks is to fine-tune a model\npre-trained on the source task using data from the target task. In this paper,\nwe propose an adaptive fine-tuning approach, called SpotTune, which finds the\noptimal fine-tuning strategy per instance for the target data. In SpotTune,\ngiven an image from the target task, a policy network is used to make routing\ndecisions on whether to pass the image through the fine-tuned layers or the\npre-trained layers. We conduct extensive experiments to demonstrate the\neffectiveness of the proposed approach. Our method outperforms the traditional\nfine-tuning approach on 12 out of 14 standard datasets.We also compare SpotTune\nwith other state-of-the-art fine-tuning strategies, showing superior\nperformance. On the Visual Decathlon datasets, our method achieves the highest\nscore across the board without bells and whistles.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 14:02:03 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Guo", "Yunhui", ""], ["Shi", "Honghui", ""], ["Kumar", "Abhishek", ""], ["Grauman", "Kristen", ""], ["Rosing", "Tajana", ""], ["Feris", "Rogerio", ""]]}, {"id": "1811.08764", "submitter": "Etai Littwin", "authors": "Etai Littwin, Lior Wolf", "title": "Regularizing by the Variance of the Activations' Sample-Variances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization techniques play an important role in supporting efficient and\noften more effective training of deep neural networks. While conventional\nmethods explicitly normalize the activations, we suggest to add a loss term\ninstead. This new loss term encourages the variance of the activations to be\nstable and not vary from one random mini-batch to the next. As we prove, this\nencourages the activations to be distributed around a few distinct modes. We\nalso show that if the inputs are from a mixture of two Gaussians, the new loss\nwould either join the two together, or separate between them optimally in the\nLDA sense, depending on the prior probabilities. Finally, we are able to link\nthe new regularization term to the batchnorm method, which provides it with a\nregularization perspective. Our experiments demonstrate an improvement in\naccuracy over the batchnorm technique for both CNNs and fully connected\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 14:58:38 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Littwin", "Etai", ""], ["Wolf", "Lior", ""]]}, {"id": "1811.08790", "submitter": "Yan Leng", "authors": "Yan Leng and Xiaowen Dong and Junfeng Wu and Alex Pentland", "title": "Learning Quadratic Games on Networks", "comments": null, "journal-ref": "Proceedings of the 37th International Conference on Machine\n  Learning (ICML) 2020", "doi": null, "report-no": null, "categories": "cs.GT cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuals, or organizations, cooperate with or compete against one another\nin a wide range of practical situations. Such strategic interactions are often\nmodeled as games played on networks, where an individual's payoff depends not\nonly on her action but also on that of her neighbors. The current literature\nhas largely focused on analyzing the characteristics of network games in the\nscenario where the structure of the network, which is represented by a graph,\nis known beforehand. It is often the case, however, that the actions of the\nplayers are readily observable while the underlying interaction network remains\nhidden. In this paper, we propose two novel frameworks for learning, from the\nobservations on individual actions, network games with linear-quadratic\npayoffs, and in particular, the structure of the interaction network. Our\nframeworks are based on the Nash equilibrium of such games and involve solving\na joint optimization problem for the graph structure and the individual\nmarginal benefits. Both synthetic and real-world experiments demonstrate the\neffectiveness of the proposed frameworks, which have theoretical as well as\npractical implications for understanding strategic interactions in a network\nenvironment.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 15:40:57 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 06:33:03 GMT"}, {"version": "v3", "created": "Sun, 20 Sep 2020 04:47:48 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Leng", "Yan", ""], ["Dong", "Xiaowen", ""], ["Wu", "Junfeng", ""], ["Pentland", "Alex", ""]]}, {"id": "1811.08800", "submitter": "Mahsa Ghorbani", "authors": "Mahsa Ghorbani, Mahdieh Soleymani Baghshah, Hamid R. Rabiee", "title": "MGCN: Semi-supervised Classification in Multi-layer Graphs with Graph\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3341161.3342942", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embedding is an important approach for graph analysis tasks such as\nnode classification and link prediction. The goal of graph embedding is to find\na low dimensional representation of graph nodes that preserves the graph\ninformation. Recent methods like Graph Convolutional Network (GCN) try to\nconsider node attributes (if available) besides node relations and learn node\nembeddings for unsupervised and semi-supervised tasks on graphs. On the other\nhand, multi-layer graph analysis has been received attention recently. However,\nthe existing methods for multi-layer graph embedding cannot incorporate all\navailable information (like node attributes). Moreover, most of them consider\neither type of nodes or type of edges, and they do not treat within and between\nlayer edges differently. In this paper, we propose a method called MGCN that\nutilizes the GCN for multi-layer graphs. MGCN embeds nodes of multi-layer\ngraphs using both within and between layers relations and nodes attributes. We\nevaluate our method on the semi-supervised node classification task.\nExperimental results demonstrate the superiority of the proposed method to\nother multi-layer and single-layer competitors and also show the positive\neffect of using cross-layer edges.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 15:54:56 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 09:01:05 GMT"}, {"version": "v3", "created": "Sat, 24 Aug 2019 09:50:18 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Ghorbani", "Mahsa", ""], ["Baghshah", "Mahdieh Soleymani", ""], ["Rabiee", "Hamid R.", ""]]}, {"id": "1811.08803", "submitter": "Luke O'Connor", "authors": "Luke J. O'Connor and Alkes L. Price", "title": "Distinguishing correlation from causation using genome-wide association\n  studies", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": "O'Connor, Luke J. and Alkes L. Price. \"Distinguishing genetic\n  correlation from causation across 52 diseases and complex traits.\" Nature\n  genetics (2018)", "doi": null, "report-no": "ML4H/2018/4", "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Genome-wide association studies (GWAS) have emerged as a rich source of\ngenetic clues into disease biology, and they have revealed strong genetic\ncorrelations among many diseases and traits. Some of these genetic correlations\nmay reflect causal relationships. We developed a method to quantify causal\nrelationships between genetically correlated traits using GWAS summary\nassociation statistics. In particular, our method quantifies what part of the\ngenetic component of trait 1 is also causal for trait 2 using mixed fourth\nmoments $E(\\alpha_1^2\\alpha_1\\alpha_2)$ and $E(\\alpha_2^2\\alpha_1\\alpha_2)$ of\nthe bivariate effect size distribution. If trait 1 is causal for trait 2, then\nSNPs affecting trait 1 (large $\\alpha_1^2$) will have correlated effects on\ntrait 2 (large $\\alpha_1\\alpha_2$), but not vice versa. We validated this\napproach in extensive simulations. Across 52 traits (average $N=331$k), we\nidentified 30 putative genetically causal relationships, many novel, including\nan effect of LDL cholesterol on decreased bone mineral density. More broadly,\nwe demonstrate that it is possible to distinguish between genetic correlation\nand causation using genetic association data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 16:01:51 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["O'Connor", "Luke J.", ""], ["Price", "Alkes L.", ""]]}, {"id": "1811.08812", "submitter": "Mahsa Ghorbani", "authors": "Ehsan Montahaei, Mahsa Ghorbani, Mahdieh Soleymani Baghshah, Hamid R.\n  Rabiee", "title": "Adversarial Classifier for Imbalanced Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial approach has been widely used for data generation in the last few\nyears. However, this approach has not been extensively utilized for classifier\ntraining. In this paper, we propose an adversarial framework for classifier\ntraining that can also handle imbalanced data. Indeed, a network is trained via\nan adversarial approach to give weights to samples of the majority class such\nthat the obtained classification problem becomes more challenging for the\ndiscriminator and thus boosts its classification capability. In addition to the\ngeneral imbalanced classification problems, the proposed method can also be\nused for problems such as graph representation learning in which it is desired\nto discriminate similar nodes from dissimilar nodes. Experimental results on\nimbalanced data classification and on the tasks like graph link prediction show\nthe superiority of the proposed method compared to the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 16:29:54 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Montahaei", "Ehsan", ""], ["Ghorbani", "Mahsa", ""], ["Baghshah", "Mahdieh Soleymani", ""], ["Rabiee", "Hamid R.", ""]]}, {"id": "1811.08839", "submitter": "Anuroop Sriram", "authors": "Jure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan\n  Huang, Matthew J. Muckley, Aaron Defazio, Ruben Stern, Patricia Johnson, Mary\n  Bruno, Marc Parente, Krzysztof J. Geras, Joe Katsnelson, Hersh Chandarana,\n  Zizhao Zhang, Michal Drozdzal, Adriana Romero, Michael Rabbat, Pascal\n  Vincent, Nafissa Yakubova, James Pinkerton, Duo Wang, Erich Owens, C.\n  Lawrence Zitnick, Michael P. Recht, Daniel K. Sodickson, Yvonne W. Lui", "title": "fastMRI: An Open Dataset and Benchmarks for Accelerated MRI", "comments": "35 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerating Magnetic Resonance Imaging (MRI) by taking fewer measurements\nhas the potential to reduce medical costs, minimize stress to patients and make\nMRI possible in applications where it is currently prohibitively slow or\nexpensive. We introduce the fastMRI dataset, a large-scale collection of both\nraw MR measurements and clinical MR images, that can be used for training and\nevaluation of machine-learning approaches to MR image reconstruction. By\nintroducing standardized evaluation criteria and a freely-accessible dataset,\nour goal is to help the community make rapid advances in the state of the art\nfor MR image reconstruction. We also provide a self-contained introduction to\nMRI for machine learning researchers with no medical imaging background.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 17:32:14 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 10:31:39 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Zbontar", "Jure", ""], ["Knoll", "Florian", ""], ["Sriram", "Anuroop", ""], ["Murrell", "Tullie", ""], ["Huang", "Zhengnan", ""], ["Muckley", "Matthew J.", ""], ["Defazio", "Aaron", ""], ["Stern", "Ruben", ""], ["Johnson", "Patricia", ""], ["Bruno", "Mary", ""], ["Parente", "Marc", ""], ["Geras", "Krzysztof J.", ""], ["Katsnelson", "Joe", ""], ["Chandarana", "Hersh", ""], ["Zhang", "Zizhao", ""], ["Drozdzal", "Michal", ""], ["Romero", "Adriana", ""], ["Rabbat", "Michael", ""], ["Vincent", "Pascal", ""], ["Yakubova", "Nafissa", ""], ["Pinkerton", "James", ""], ["Wang", "Duo", ""], ["Owens", "Erich", ""], ["Zitnick", "C. Lawrence", ""], ["Recht", "Michael P.", ""], ["Sodickson", "Daniel K.", ""], ["Lui", "Yvonne W.", ""]]}, {"id": "1811.08840", "submitter": "Woochan Hwang", "authors": "Sejin Park, Woochan Hwang, Kyu-Hwan Jung", "title": "Integrating Reinforcement Learning to Self Training for Pulmonary Nodule\n  Segmentation in Chest X-rays", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/75", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning applications in medical imaging are frequently limited by\nthe lack of quality labeled data. In this paper, we explore the self training\nmethod, a form of semi-supervised learning, to address the labeling burden. By\nintegrating reinforcement learning, we were able to expand the application of\nself training to complex segmentation networks without any further human\nannotation. The proposed approach, reinforced self training (ReST), fine tunes\na semantic segmentation networks by introducing a policy network that learns to\ngenerate pseudolabels. We incorporate an expert demonstration network, based on\ninverse reinforcement learning, to enhance clinical validity and convergence of\nthe policy network. The model was tested on a pulmonary nodule segmentation\ntask in chest X-rays and achieved the performance of a standard U-Net while\nusing only 50% of the labeled data, by exploiting unlabeled data. When the same\nnumber of labeled data was used, a moderate to significant cross validation\naccuracy improvement was achieved depending on the absolute number of labels\nused.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 17:37:22 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Park", "Sejin", ""], ["Hwang", "Woochan", ""], ["Jung", "Kyu-Hwan", ""]]}, {"id": "1811.08871", "submitter": "Shali Jiang", "authors": "Shali Jiang, Gustavo Malkomes, Benjamin Moseley, Roman Garnett", "title": "Efficient nonmyopic active search with applications in drug and\n  materials discovery", "comments": "Machine Learning for Molecules and Materials (NeurIPS 2018 Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active search is a learning paradigm for actively identifying as many members\nof a given class as possible. A critical target scenario is high-throughput\nscreening for scientific discovery, such as drug or materials discovery. In\nthis paper, we approach this problem in Bayesian decision framework. We first\nderive the Bayesian optimal policy under a natural utility, and establish a\ntheoretical hardness of active search, proving that the optimal policy can not\nbe approximated for any constant ratio. We also study the batch setting for the\nfirst time, where a batch of $b>1$ points can be queried at each iteration. We\ngive an asymptotic lower bound, linear in batch size, on the adaptivity gap:\nhow much we could lose if we query $b$ points at a time for $t$ iterations,\ninstead of one point at a time for $bt$ iterations. We then introduce a novel\napproach to nonmyopic approximations of the optimal policy that admits\nefficient computation. Our proposed policy can automatically trade off\nexploration and exploitation, without relying on any tuning parameters. We also\ngeneralize our policy to batch setting, and propose two approaches to tackle\nthe combinatorial search challenge. We evaluate our proposed policies on a\nlarge database of drug discovery and materials science. Results demonstrate the\nsuperior performance of our proposed policy in both sequential and batch\nsetting; the nonmyopic behavior is also illustrated in various aspects.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 18:32:33 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 20:26:28 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Jiang", "Shali", ""], ["Malkomes", "Gustavo", ""], ["Moseley", "Benjamin", ""], ["Garnett", "Roman", ""]]}, {"id": "1811.08888", "submitter": "Quanquan Gu", "authors": "Difan Zou, Yuan Cao, Dongruo Zhou, Quanquan Gu", "title": "Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU\n  Networks", "comments": "54 pages. This version relaxes the assumptions on the loss functions\n  and data distribution, and improves the dependency on the problem-specific\n  parameters in the main theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of training deep neural networks with Rectified Linear\nUnit (ReLU) activation function using gradient descent and stochastic gradient\ndescent. In particular, we study the binary classification problem and show\nthat for a broad family of loss functions, with proper random weight\ninitialization, both gradient descent and stochastic gradient descent can find\nthe global minima of the training loss for an over-parameterized deep ReLU\nnetwork, under mild assumption on the training data. The key idea of our proof\nis that Gaussian random initialization followed by (stochastic) gradient\ndescent produces a sequence of iterates that stay inside a small perturbation\nregion centering around the initial weights, in which the empirical loss\nfunction of deep ReLU networks enjoys nice local curvature properties that\nensure the global convergence of (stochastic) gradient descent. Our theoretical\nresults shed light on understanding the optimization for deep learning, and\npave the way for studying the optimization dynamics of training modern deep\nneural networks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 18:58:46 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 18:44:38 GMT"}, {"version": "v3", "created": "Thu, 27 Dec 2018 18:57:43 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Zou", "Difan", ""], ["Cao", "Yuan", ""], ["Zhou", "Dongruo", ""], ["Gu", "Quanquan", ""]]}, {"id": "1811.08890", "submitter": "Nils Holzenberger", "authors": "Nils Holzenberger, Shruti Palaskar, Pranava Madhyastha, Florian Metze,\n  Raman Arora", "title": "Learning from Multiview Correlations in Open-Domain Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of datasets contain multiple views, such as video, sound\nand automatic captions. A basic challenge in representation learning is how to\nleverage multiple views to learn better representations. This is further\ncomplicated by the existence of a latent alignment between views, such as\nbetween speech and its transcription, and by the multitude of choices for the\nlearning objective. We explore an advanced, correlation-based representation\nlearning method on a 4-way parallel, multimodal dataset, and assess the quality\nof the learned representations on retrieval-based tasks. We show that the\nproposed approach produces rich representations that capture most of the\ninformation shared across views. Our best models for speech and textual\nmodalities achieve retrieval rates from 70.7% to 96.9% on open-domain,\nuser-generated instructional videos. This shows it is possible to learn\nreliable representations across disparate, unaligned and noisy modalities, and\nencourages using the proposed approach on larger datasets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 19:57:11 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 18:21:28 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Holzenberger", "Nils", ""], ["Palaskar", "Shruti", ""], ["Madhyastha", "Pranava", ""], ["Metze", "Florian", ""], ["Arora", "Raman", ""]]}, {"id": "1811.08919", "submitter": "Xu Chen", "authors": "Xu Chen, Saratendu Sethi", "title": "Robust Active Learning for Electrocardiographic Signal Classification", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216 3", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/5", "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of electrocardiographic (ECG) signals is a challenging\nproblem for healthcare industry. Traditional supervised learning methods\nrequire a large number of labeled data which is usually expensive and difficult\nto obtain for ECG signals. Active learning is well-suited for ECG signal\nclassification as it aims at selecting the best set of labeled data in order to\nmaximize the classification performance. Motivated by the fact that ECG data\nare usually heavily unbalanced among different classes and the class labels are\nnoisy as they are manually labeled, this paper proposes a novel solution based\non robust active learning for addressing these challenges. The key idea is to\nfirst apply the clustering of the data in a low dimensional embedded space and\nthen select the most information instances within local clusters. By selecting\nthe most informative instances relying on local average minimal distances, the\nalgorithm tends to select the data for labelling in a more diversified way.\nFinally, the robustness of the model is further enhanced by adding a novel\nnoisy label reduction scheme after the selection of the labeled data.\nExperiments on the ECG signal classification from the MIT-BIH arrhythmia\ndatabase demonstrate the effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 19:24:59 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Chen", "Xu", ""], ["Sethi", "Saratendu", ""]]}, {"id": "1811.08929", "submitter": "Changyou Chen", "authors": "Yang Zhao and Jianyi Zhang and Changyou Chen", "title": "Self-Adversarially Learned Bayesian Sampling", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable Bayesian sampling is playing an important role in modern machine\nlearning, especially in the fast-developed unsupervised-(deep)-learning models.\nWhile tremendous progresses have been achieved via scalable Bayesian sampling\nsuch as stochastic gradient MCMC (SG-MCMC) and Stein variational gradient\ndescent (SVGD), the generated samples are typically highly correlated.\nMoreover, their sample-generation processes are often criticized to be\ninefficient. In this paper, we propose a novel self-adversarial learning\nframework that automatically learns a conditional generator to mimic the\nbehavior of a Markov kernel (transition kernel). High-quality samples can be\nefficiently generated by direct forward passes though a learned generator. Most\nimportantly, the learning process adopts a self-learning paradigm, requiring no\ninformation on existing Markov kernels, e.g., knowledge of how to draw samples\nfrom them. Specifically, our framework learns to use current samples, either\nfrom the generator or pre-provided training data, to update the generator such\nthat the generated samples progressively approach a target distribution, thus\nit is called self-learning. Experiments on both synthetic and real datasets\nverify advantages of our framework, outperforming related methods in terms of\nboth sampling efficiency and sample quality.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 20:03:05 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Zhao", "Yang", ""], ["Zhang", "Jianyi", ""], ["Chen", "Changyou", ""]]}, {"id": "1811.08936", "submitter": "Halil Mutuk", "authors": "Halil Mutuk", "title": "A Neural Network Study of Blasius Equation", "comments": null, "journal-ref": "Neural Processing Letters, 2020", "doi": "10.1007/s11063-019-10184-9", "report-no": null, "categories": "cs.LG math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we applied a feed forward neural network to solve Blasius\nequation which is a third-order nonlinear differential equation. Blasius\nequation is a kind of boundary layer flow. We solved Blasius equation without\nreducing it into a system of first order equation. Numerical results are\npresented and a comparison according to some studies is made in the form of\ntheir results. Obtained results are found to be in good agreement with the\ngiven studies.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 13:16:43 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 19:03:33 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Mutuk", "Halil", ""]]}, {"id": "1811.08943", "submitter": "Changhee Lee", "authors": "Changhee Lee, Nicholas Mastronarde, Mihaela van der Schaar", "title": "Estimation of Individual Treatment Effect in Latent Confounder Models\n  via Adversarial Learning", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/33", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the individual treatment effect (ITE) from observational data is\nessential in medicine. A central challenge in estimating the ITE is handling\nconfounders, which are factors that affect both an intervention and its\noutcome. Most previous work relies on the unconfoundedness assumption, which\nposits that all the confounders are measured in the observational data.\nHowever, if there are unmeasurable (latent) confounders, then confounding bias\nis introduced. Fortunately, noisy proxies for the latent confounders are often\navailable and can be used to make an unbiased estimate of the ITE. In this\npaper, we develop a novel adversarial learning framework to make unbiased\nestimates of the ITE using noisy proxies.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 20:46:27 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Lee", "Changhee", ""], ["Mastronarde", "Nicholas", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1811.08955", "submitter": "Fangkai Yang", "authors": "Yuqian Jiang, Fangkai Yang, Shiqi Zhang, Peter Stone", "title": "Integrating Task-Motion Planning with Reinforcement Learning for Robust\n  Decision Making in Mobile Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-motion planning (TMP) addresses the problem of efficiently generating\nexecutable and low-cost task plans in a discrete space such that the (initially\nunknown) action costs are determined by motion plans in a corresponding\ncontinuous space. However, a task-motion plan can be sensitive to unexpected\ndomain uncertainty and changes, leading to suboptimal behaviors or execution\nfailures. In this paper, we propose a novel framework, TMP-RL, which is an\nintegration of TMP and reinforcement learning (RL) from the execution\nexperience, to solve the problem of robust task-motion planning in dynamic and\nuncertain domains. TMP-RL features two nested planning-learning loops. In the\ninner TMP loop, the robot generates a low-cost, feasible task-motion plan by\niteratively planning in the discrete space and updating relevant action costs\nevaluated by the motion planner in continuous space. In the outer loop, the\nplan is executed, and the robot learns from the execution experience via\nmodel-free RL, to further improve its task-motion plans. RL in the outer loop\nis more accurate to the current domain but also more expensive, and using less\ncostly task and motion planning leads to a jump-start for learning in the real\nworld. Our approach is evaluated on a mobile service robot conducting\nnavigation tasks in an office area. Results show that TMP-RL approach\nsignificantly improves adaptability and robustness (in comparison to TMP\nmethods) and leads to rapid convergence (in comparison to task planning (TP)-RL\nmethods). We also show that TMP-RL can reuse learned values to smoothly adapt\nto new scenarios during long-term deployments.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 21:20:24 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Jiang", "Yuqian", ""], ["Yang", "Fangkai", ""], ["Zhang", "Shiqi", ""], ["Stone", "Peter", ""]]}, {"id": "1811.08963", "submitter": "Ganapathy Natarajan", "authors": "Ganapathy S. Natarajan and Aishwarya Ashok", "title": "Multivariate Forecasting of Crude Oil Spot Prices using Neural Networks", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crude oil is a major component in most advanced economies of the world.\nAccurately predicting and understanding the behavior of crude oil prices is\nimportant for economists, analysts, forecasters, and traders, to name a few.\nThe price of crude oil has declined in the past decade and is seeing a phase of\nstability; but will this stability last? This work is an empirical study on how\nmultivariate analysis may be employed to predict crude oil spot prices using\nneural networks. The concept of using neural networks showed promising\npotential. A very simple neural network model was able to perform on par with\nARIMA models - the state-of-the-art model in time-series forecasting. Advanced\nneural network models using larger datasets may be used in the future to extend\nthis proof-of-concept to a full scale framework.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 21:45:52 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Natarajan", "Ganapathy S.", ""], ["Ashok", "Aishwarya", ""]]}, {"id": "1811.08968", "submitter": "Mingtian Zhang", "authors": "Mingtian Zhang, Peter Hayes, Tom Bird, Raza Habib, David Barber", "title": "Spread Divergences", "comments": null, "journal-ref": "Volume 119: International Conference on Machine Learning, 13-18\n  July 2020, Virtual", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For distributions p and q with different supports, the divergence D(p|q) may\nnot exist. We define a spread divergence on modified p and q and describe\nsufficient conditions for the existence of such a divergence. We demonstrate\nhow to maximize the discriminatory power of a given divergence by\nparameterizing and learning the spread. We also give examples of using a spread\ndivergence to train and improve implicit generative models, including linear\nmodels (Independent Components Analysis) and non-linear models (Deep Generative\nNetworks).\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 22:35:08 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 10:29:46 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2019 13:32:11 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2021 03:07:02 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Zhang", "Mingtian", ""], ["Hayes", "Peter", ""], ["Bird", "Tom", ""], ["Habib", "Raza", ""], ["Barber", "David", ""]]}, {"id": "1811.08979", "submitter": "Xiangxiang Xu", "authors": "Lichen Wang, Jiaxiang Wu, Shao-Lun Huang, Lizhong Zheng, Xiangxiang\n  Xu, Lin Zhang, Junzhou Huang", "title": "An Efficient Approach to Informative Feature Extraction from Multimodal\n  Data", "comments": "accepted to AAAI 2019, 8 pages; typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One primary focus in multimodal feature extraction is to find the\nrepresentations of individual modalities that are maximally correlated. As a\nwell-known measure of dependence, the Hirschfeld-Gebelein-R\\'{e}nyi (HGR)\nmaximal correlation becomes an appealing objective because of its operational\nmeaning and desirable properties. However, the strict whitening constraints\nformalized in the HGR maximal correlation limit its application. To address\nthis problem, this paper proposes Soft-HGR, a novel framework to extract\ninformative features from multiple data modalities. Specifically, our framework\nprevents the \"hard\" whitening constraints, while simultaneously preserving the\nsame feature geometry as in the HGR maximal correlation. The objective of\nSoft-HGR is straightforward, only involving two inner products, which\nguarantees the efficiency and stability in optimization. We further generalize\nthe framework to handle more than two modalities and missing modalities. When\nlabels are partially available, we enhance the discriminative power of the\nfeature representations by making a semi-supervised adaptation. Empirical\nevaluation implies that our approach learns more informative feature mappings\nand is more efficient to optimize.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 00:43:12 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 05:21:40 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Wang", "Lichen", ""], ["Wu", "Jiaxiang", ""], ["Huang", "Shao-Lun", ""], ["Zheng", "Lizhong", ""], ["Xu", "Xiangxiang", ""], ["Zhang", "Lin", ""], ["Huang", "Junzhou", ""]]}, {"id": "1811.08990", "submitter": "Tao Sun", "authors": "Tao Sun, Yuejiao Sun, Yangyang Xu, Wotao Yin", "title": "Markov Chain Block Coordinate Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of block coordinate gradient descent (BCD) has been a powerful\nmethod for large-scale optimization. This paper considers the BCD method that\nsuccessively updates a series of blocks selected according to a Markov chain.\nThis kind of block selection is neither i.i.d. random nor cyclic. On the other\nhand, it is a natural choice for some applications in distributed optimization\nand Markov decision process, where i.i.d. random and cyclic selections are\neither infeasible or very expensive. By applying mixing-time properties of a\nMarkov chain, we prove convergence of Markov chain BCD for minimizing Lipschitz\ndifferentiable functions, which can be nonconvex. When the functions are convex\nand strongly convex, we establish both sublinear and linear convergence rates,\nrespectively. We also present a method of Markov chain inertial BCD. Finally,\nwe discuss potential applications.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 01:51:15 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Sun", "Tao", ""], ["Sun", "Yuejiao", ""], ["Xu", "Yangyang", ""], ["Yin", "Wotao", ""]]}, {"id": "1811.08996", "submitter": "Shipeng Wang", "authors": "Shipeng Wang, Jian Sun and Zongben Xu", "title": "HyperAdam: A Learnable Task-Adaptive Adam for Network Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are traditionally trained using human-designed\nstochastic optimization algorithms, such as SGD and Adam. Recently, the\napproach of learning to optimize network parameters has emerged as a promising\nresearch topic. However, these learned black-box optimizers sometimes do not\nfully utilize the experience in human-designed optimizers, therefore have\nlimitation in generalization ability. In this paper, a new optimizer, dubbed as\n\\textit{HyperAdam}, is proposed that combines the idea of \"learning to\noptimize\" and traditional Adam optimizer. Given a network for training, its\nparameter update in each iteration generated by HyperAdam is an adaptive\ncombination of multiple updates generated by Adam with varying decay rates. The\ncombination weights and decay rates in HyperAdam are adaptively learned\ndepending on the task. HyperAdam is modeled as a recurrent neural network with\nAdamCell, WeightCell and StateCell. It is justified to be state-of-the-art for\nvarious network training, such as multilayer perceptron, CNN and LSTM.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 02:37:53 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Wang", "Shipeng", ""], ["Sun", "Jian", ""], ["Xu", "Zongben", ""]]}, {"id": "1811.09003", "submitter": "Fenglei Fan", "authors": "Fenglei Fan, Dayang Wang, Hengtao Guo, Qikui Zhu, Pingkun Yan, Ge Wang\n  and Hengyong Yu", "title": "On a Sparse Shortcut Topology of Artificial Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over recent years, deep learning has become the mainstream data-driven\napproach to solve many important real-world problems. In the successful network\narchitectures, shortcut connections are well established to take the outputs of\nearlier layers as additional inputs to later layers, which have produced\nexcellent results. Despite the extraordinary effectiveness of shortcuts, there\nremain important questions on the underlying mechanism and associated\nfunctionalities. For example, why are shortcuts powerful? Why shortcuts\ngeneralize well? To address these questions, we investigate the representation\nand generalization ability of a sparse shortcut topology. Specifically, we\nfirst demonstrate that this topology can empower a one-neuron-wide deep network\nto approximate any univariate continuous function. Then, we present a novel\nwidth-bounded universal approximator in contrast to depth-bounded universal\napproximators, and also extend the approximation result to a family of networks\nsuch that in the view of approximation ability, these networks are equally\ncompetent. Furthermore, we use the generalization bound theory to show that the\ninvestigated shortcut topology enjoys an excellent generalizability. Finally,\nwe corroborate our theoretical analyses with experiments on some well-known\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 03:23:44 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 03:07:46 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2021 02:51:08 GMT"}, {"version": "v4", "created": "Thu, 17 Jun 2021 17:58:33 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Fan", "Fenglei", ""], ["Wang", "Dayang", ""], ["Guo", "Hengtao", ""], ["Zhu", "Qikui", ""], ["Yan", "Pingkun", ""], ["Wang", "Ge", ""], ["Yu", "Hengyong", ""]]}, {"id": "1811.09008", "submitter": "Dong Eui Chang", "authors": "Muhammad Usama, Dong Eui Chang", "title": "Towards Robust Neural Networks with Lipschitz Continuity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have shown remarkable performance across a wide range of\nvision-based tasks, particularly due to the availability of large-scale\ndatasets for training and better architectures. However, data seen in the real\nworld are often affected by distortions that not accounted for by the training\ndatasets. In this paper, we address the challenge of robustness and stability\nof neural networks and propose a general training method that can be used to\nmake the existing neural network architectures more robust and stable to input\nvisual perturbations while using only available datasets for training. Proposed\ntraining method is convenient to use as it does not require data augmentation\nor changes in the network architecture. We provide theoretical proof as well as\nempirical evidence for the efficiency of the proposed training method by\nperforming experiments with existing neural network architectures and\ndemonstrate that same architecture when trained with the proposed training\nmethod perform better than when trained with conventional training approach in\nthe presence of noisy datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 03:42:17 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Usama", "Muhammad", ""], ["Chang", "Dong Eui", ""]]}, {"id": "1811.09013", "submitter": "Eric Graves", "authors": "Ehsan Imani, Eric Graves, Martha White", "title": "An Off-policy Policy Gradient Theorem Using Emphatic Weightings", "comments": "Updated to final NeurIPS version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy gradient methods are widely used for control in reinforcement\nlearning, particularly for the continuous action setting. There have been a\nhost of theoretically sound algorithms proposed for the on-policy setting, due\nto the existence of the policy gradient theorem which provides a simplified\nform for the gradient. In off-policy learning, however, where the behaviour\npolicy is not necessarily attempting to learn and follow the optimal policy for\nthe given task, the existence of such a theorem has been elusive. In this work,\nwe solve this open problem by providing the first off-policy policy gradient\ntheorem. The key to the derivation is the use of $emphatic$ $weightings$. We\ndevelop a new actor-critic algorithm$\\unicode{x2014}$called Actor Critic with\nEmphatic weightings (ACE)$\\unicode{x2014}$that approximates the simplified\ngradients provided by the theorem. We demonstrate in a simple counterexample\nthat previous off-policy policy gradient methods$\\unicode{x2014}$particularly\nOffPAC and DPG$\\unicode{x2014}$converge to the wrong solution whereas ACE finds\nthe optimal solution.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 03:58:11 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 04:58:36 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Imani", "Ehsan", ""], ["Graves", "Eric", ""], ["White", "Martha", ""]]}, {"id": "1811.09021", "submitter": "Bo Li", "authors": "Bo Li, Yu Zhang, Tara Sainath, Yonghui Wu, William Chan", "title": "Bytes are All You Need: End-to-End Multilingual Speech Recognition and\n  Synthesis with Bytes", "comments": "submitted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two end-to-end models: Audio-to-Byte (A2B) and Byte-to-Audio\n(B2A), for multilingual speech recognition and synthesis. Prior work has\npredominantly used characters, sub-words or words as the unit of choice to\nmodel text. These units are difficult to scale to languages with large\nvocabularies, particularly in the case of multilingual processing. In this\nwork, we model text via a sequence of Unicode bytes, specifically, the UTF-8\nvariable length byte sequence for each character. Bytes allow us to avoid large\nsoftmaxes in languages with large vocabularies, and share representations in\nmultilingual models. We show that bytes are superior to grapheme characters\nover a wide variety of languages in monolingual end-to-end speech recognition.\nAdditionally, our multilingual byte model outperform each respective single\nlanguage baseline on average by 4.4% relatively. In Japanese-English\ncode-switching speech, our multilingual byte model outperform our monolingual\nbaseline by 38.6% relatively. Finally, we present an end-to-end multilingual\nspeech synthesis model using byte representations which matches the performance\nof our monolingual baselines.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 04:37:55 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Li", "Bo", ""], ["Zhang", "Yu", ""], ["Sainath", "Tara", ""], ["Wu", "Yonghui", ""], ["Chan", "William", ""]]}, {"id": "1811.09026", "submitter": "Priyank Agrawal", "authors": "Priyank Agrawal and Theja Tulabandhula", "title": "Bandits with Temporal Stochastic Constraints", "comments": "An extended abstract appeared in the 4th Multi-disciplinary\n  Conference on Reinforcement Learning and Decision Making (RLDM 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effect of impairment on stochastic multi-armed bandits and\ndevelop new ways to mitigate it. Impairment effect is the phenomena where an\nagent only accrues reward for an action if they have played it at least a few\ntimes in the recent past. It is practically motivated by repetition and recency\neffects in domains such as advertising (here consumer behavior may require\nrepeat actions by advertisers) and vocational training (here actions are\ncomplex skills that can only be mastered with repetition to get a payoff).\nImpairment can be naturally modelled as a temporal constraint on the strategy\nspace, and we provide two novel algorithms that achieve sublinear regret, each\nworking with different assumptions on the impairment effect. We introduce a new\nnotion called bucketing in our algorithm design, and show how it can\neffectively address impairment as well as a broader class of temporal\nconstraints. Our regret bounds explicitly capture the cost of impairment and\nshow that it scales (sub-)linearly with the degree of impairment. Our work\ncomplements recent work on modeling delays and corruptions, and we provide\nexperimental evidence supporting our claims.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 05:40:53 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2020 01:24:54 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Agrawal", "Priyank", ""], ["Tulabandhula", "Theja", ""]]}, {"id": "1811.09030", "submitter": "Takashi Matsubara", "authors": "Ryo Takahashi, Takashi Matsubara, Kuniaki Uehara", "title": "Data Augmentation using Random Image Cropping and Patching for Deep CNNs", "comments": "accepted version, 16 pages", "journal-ref": "EEE Transactions on Circuits and Systems for Video Technology,\n  2019", "doi": "10.1109/TCSVT.2019.2935128", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have achieved remarkable results in\nimage processing tasks. However, their high expression ability risks\noverfitting. Consequently, data augmentation techniques have been proposed to\nprevent overfitting while enriching datasets. Recent CNN architectures with\nmore parameters are rendering traditional data augmentation techniques\ninsufficient. In this study, we propose a new data augmentation technique\ncalled random image cropping and patching (RICAP) which randomly crops four\nimages and patches them to create a new training image. Moreover, RICAP mixes\nthe class labels of the four images, resulting in an advantage similar to label\nsmoothing. We evaluated RICAP with current state-of-the-art CNNs (e.g., the\nshake-shake regularization model) by comparison with competitive data\naugmentation techniques such as cutout and mixup. RICAP achieves a new\nstate-of-the-art test error of $2.19\\%$ on CIFAR-10. We also confirmed that\ndeep CNNs with RICAP achieve better results on classification tasks using\nCIFAR-100 and ImageNet and an image-caption retrieval task using Microsoft\nCOCO.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 06:07:40 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 14:21:32 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Takahashi", "Ryo", ""], ["Matsubara", "Takashi", ""], ["Uehara", "Kuniaki", ""]]}, {"id": "1811.09043", "submitter": "Ziv Katzir", "authors": "Ziv Katzir, Yuval Elovici", "title": "Detecting Adversarial Perturbations Through Spatial Behavior in\n  Activation Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network based classifiers are still prone to manipulation through\nadversarial perturbations. State of the art attacks can overcome most of the\ndefense or detection mechanisms suggested so far, and adversaries have the\nupper hand in this arms race. Adversarial examples are designed to resemble the\nnormal input from which they were constructed, while triggering an incorrect\nclassification. This basic design goal leads to a characteristic spatial\nbehavior within the context of Activation Spaces, a term coined by the authors\nto refer to the hyperspaces formed by the activation values of the network's\nlayers. Within the output of the first layers of the network, an adversarial\nexample is likely to resemble normal instances of the source class, while in\nthe final layers such examples will diverge towards the adversary's target\nclass. The steps below enable us to leverage this inherent shift from one class\nto another in order to form a novel adversarial example detector. We construct\nEuclidian spaces out of the activation values of each of the deep neural\nnetwork layers. Then, we induce a set of k-nearest neighbor classifiers (k-NN),\none per activation space of each neural network layer, using the\nnon-adversarial examples. We leverage those classifiers to produce a sequence\nof class labels for each nonperturbed input sample and estimate the a priori\nprobability for a class label change between one activation space and another.\nDuring the detection phase we compute a sequence of classification labels for\neach input using the trained classifiers. We then estimate the likelihood of\nthose classification sequences and show that adversarial sequences are far less\nlikely than normal ones. We evaluated our detection method against the state of\nthe art C&W attack method, using two image classification datasets (MNIST,\nCIFAR-10) reaching an AUC 0f 0.95 for the CIFAR-10 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 07:17:32 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 10:33:27 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Katzir", "Ziv", ""], ["Elovici", "Yuval", ""]]}, {"id": "1811.09054", "submitter": "Ke Wang", "authors": "Jian-Feng Cai, Dong Li, Jiaze Sun, Ke Wang", "title": "Enhanced Expressive Power and Fast Training of Neural Networks by Random\n  Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random projections are able to perform dimension reduction efficiently for\ndatasets with nonlinear low-dimensional structures. One well-known example is\nthat random matrices embed sparse vectors into a low-dimensional subspace\nnearly isometrically, known as the restricted isometric property in compressed\nsensing. In this paper, we explore some applications of random projections in\ndeep neural networks. We provide the expressive power of fully connected neural\nnetworks when the input data are sparse vectors or form a low-dimensional\nsmooth manifold. We prove that the number of neurons required for approximating\na Lipschitz function with a prescribed precision depends on the sparsity or the\ndimension of the manifold and weakly on the dimension of the input vector. The\nkey in our proof is that random projections embed stably the set of sparse\nvectors or a low-dimensional smooth manifold into a low-dimensional subspace.\nBased on this fact, we also propose some new neural network models, where at\neach layer the input is first projected onto a low-dimensional subspace by a\nrandom projection and then the standard linear connection and non-linear\nactivation are applied. In this way, the number of parameters in neural\nnetworks is significantly reduced, and therefore the training of neural\nnetworks can be accelerated without too much performance loss.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 07:52:56 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 10:35:19 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Cai", "Jian-Feng", ""], ["Li", "Dong", ""], ["Sun", "Jiaze", ""], ["Wang", "Ke", ""]]}, {"id": "1811.09065", "submitter": "Oleg Sysoev", "authors": "Oleg Sysoev, Krzysztof Bartoszek, Eva-Charlotte Ekstrom, Katarina\n  Ekholm Selling", "title": "PSICA: decision trees for probabilistic subgroup identification with\n  categorical treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized medicine aims at identifying best treatments for a patient with\ngiven characteristics. It has been shown in the literature that these methods\ncan lead to great improvements in medicine compared to traditional methods\nprescribing the same treatment to all patients. Subgroup identification is a\nbranch of personalized medicine which aims at finding subgroups of the patients\nwith similar characteristics for which some of the investigated treatments have\na better effect than the other treatments. A number of approaches based on\ndecision trees has been proposed to identify such subgroups, but most of them\nfocus on the two-arm trials (control/treatment) while a few methods consider\nquantitative treatments (defined by the dose). However, no subgroup\nidentification method exists that can predict the best treatments in a scenario\nwith a categorical set of treatments. We propose a novel method for subgroup\nidentification in categorical treatment scenarios. This method outputs a\ndecision tree showing the probabilities of a given treatment being the best for\na given group of patients as well as labels showing the possible best\ntreatments. The method is implemented in an R package \\textbf{psica} available\nat CRAN. In addition to numerical simulations based on artificial data, we\npresent an analysis of a community-based nutrition intervention trial that\njustifies the validity of our method.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 09:08:22 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Sysoev", "Oleg", ""], ["Bartoszek", "Krzysztof", ""], ["Ekstrom", "Eva-Charlotte", ""], ["Selling", "Katarina Ekholm", ""]]}, {"id": "1811.09067", "submitter": "Kehinde Owoeye Mr", "authors": "Kehinde Owoeye and Stephen Hailes", "title": "Online Collective Animal Movement Activity Recognition", "comments": "5 pages, 2 figures, To be presented at the Workshop on Modeling and\n  Decision-Making in the Spatiotemporal Domain, 32nd Conference on Neural\n  Information Processing Systems (NIPS 2018), Montr\\'eal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the activities of animals is important for the purpose of monitoring\ntheir welfare vis a vis their behaviour with respect to their environment and\nconspecifics. While previous works have largely focused on activity recognition\nin a single animal, little or no work has been done in learning the collective\nbehaviour of animals. In this work, we address the problem of recognising the\ncollective movement activities of a group of sheep in a flock. We present a\ndiscriminative framework that learns to track the positions and velocities of\nall the animals in the flock in an online manner whilst estimating their\ncollective activity. We investigate the performance of two simple deep network\narchitectures and show that we can learn the collective activities with good\naccuracy even when the distribution of the activities is skewed.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 09:14:24 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Owoeye", "Kehinde", ""], ["Hailes", "Stephen", ""]]}, {"id": "1811.09083", "submitter": "Sainbayar Sukhbaatar", "authors": "Sainbayar Sukhbaatar, Emily Denton, Arthur Szlam and Rob Fergus", "title": "Learning Goal Embeddings via Self-Play for Hierarchical Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In hierarchical reinforcement learning a major challenge is determining\nappropriate low-level policies. We propose an unsupervised learning scheme,\nbased on asymmetric self-play from Sukhbaatar et al. (2018), that automatically\nlearns a good representation of sub-goals in the environment and a low-level\npolicy that can execute them. A high-level policy can then direct the lower one\nby generating a sequence of continuous sub-goal vectors. We evaluate our model\nusing Mazebase and Mujoco environments, including the challenging AntGather\ntask. Visualizations of the sub-goal embeddings reveal a logical decomposition\nof tasks within the environment. Quantitatively, our approach obtains\ncompelling performance gains over non-hierarchical approaches.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 10:15:52 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Sukhbaatar", "Sainbayar", ""], ["Denton", "Emily", ""], ["Szlam", "Arthur", ""], ["Fergus", "Rob", ""]]}, {"id": "1811.09192", "submitter": "Frederik Pahde", "authors": "Frederik Pahde, Oleksiy Ostapenko, Patrick J\\\"ahnichen, Tassilo Klein,\n  Moin Nabi", "title": "Self Paced Adversarial Training for Multimodal Few-shot Learning", "comments": "To appear at WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep learning algorithms yield remarkable results in many\nvisual recognition tasks. However, they still fail to provide satisfactory\nresults in scarce data regimes. To a certain extent this lack of data can be\ncompensated by multimodal information. Missing information in one modality of a\nsingle data point (e.g. an image) can be made up for in another modality (e.g.\na textual description). Therefore, we design a few-shot learning task that is\nmultimodal during training (i.e. image and text) and single-modal during test\ntime (i.e. image). In this regard, we propose a self-paced class-discriminative\ngenerative adversarial network incorporating multimodality in the context of\nfew-shot learning. The proposed approach builds upon the idea of cross-modal\ndata generation in order to alleviate the data sparsity problem. We improve\nfew-shot learning accuracies on the finegrained CUB and Oxford-102 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 14:29:45 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Pahde", "Frederik", ""], ["Ostapenko", "Oleksiy", ""], ["J\u00e4hnichen", "Patrick", ""], ["Klein", "Tassilo", ""], ["Nabi", "Moin", ""]]}, {"id": "1811.09271", "submitter": "Mehmet Emre Ozfatura", "authors": "Emre Ozfatura and Sennur Ulukus and Deniz Gunduz", "title": "Distributed Gradient Descent with Coded Partial Gradient Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coded computation techniques provide robustness against straggling servers in\ndistributed computing, with the following limitations: First, they increase\ndecoding complexity. Second, they ignore computations carried out by straggling\nservers; and they are typically designed to recover the full gradient, and\nthus, cannot provide a balance between the accuracy of the gradient and\nper-iteration completion time. Here we introduce a hybrid approach, called\ncoded partial gradient computation (CPGC), that benefits from the advantages of\nboth coded and uncoded computation schemes, and reduces both the computation\ntime and decoding complexity.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 18:39:40 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Ozfatura", "Emre", ""], ["Ulukus", "Sennur", ""], ["Gunduz", "Deniz", ""]]}, {"id": "1811.09300", "submitter": "Edward Grefenstette", "authors": "Edward Grefenstette, Robert Stanforth, Brendan O'Donoghue, Jonathan\n  Uesato, Grzegorz Swirszcz, Pushmeet Kohli", "title": "Strength in Numbers: Trading-off Robustness and Computation via\n  Adversarially-Trained Ensembles", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has led to remarkable results on a number of challenging\nproblems, researchers have discovered a vulnerability of neural networks in\nadversarial settings, where small but carefully chosen perturbations to the\ninput can make the models produce extremely inaccurate outputs. This makes\nthese models particularly unsuitable for safety-critical application domains\n(e.g. self-driving cars) where robustness is extremely important. Recent work\nhas shown that augmenting training with adversarially generated data provides\nsome degree of robustness against test-time attacks. In this paper we\ninvestigate how this approach scales as we increase the computational budget\ngiven to the defender. We show that increasing the number of parameters in\nadversarially-trained models increases their robustness, and in particular that\nensembling smaller models while adversarially training the entire ensemble as a\nsingle model is a more efficient way of spending said budget than simply using\na larger single model. Crucially, we show that it is the adversarial training\nof the ensemble, rather than the ensembling of adversarially trained models,\nwhich provides robustness.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 20:32:58 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Grefenstette", "Edward", ""], ["Stanforth", "Robert", ""], ["O'Donoghue", "Brendan", ""], ["Uesato", "Jonathan", ""], ["Swirszcz", "Grzegorz", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1811.09310", "submitter": "Adnan Siraj Rakin", "authors": "Adnan Siraj Rakin and Zhezhi He, Deliang Fan", "title": "Parametric Noise Injection: Trainable Randomness to Improve Deep Neural\n  Network Robustness against Adversarial Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent development in the field of Deep Learning have exposed the underlying\nvulnerability of Deep Neural Network (DNN) against adversarial examples. In\nimage classification, an adversarial example is a carefully modified image that\nis visually imperceptible to the original image but can cause DNN model to\nmisclassify it. Training the network with Gaussian noise is an effective\ntechnique to perform model regularization, thus improving model robustness\nagainst input variation. Inspired by this classical method, we explore to\nutilize the regularization characteristic of noise injection to improve DNN's\nrobustness against adversarial attack. In this work, we propose\nParametric-Noise-Injection (PNI) which involves trainable Gaussian noise\ninjection at each layer on either activation or weights through solving the\nmin-max optimization problem, embedded with adversarial training. These\nparameters are trained explicitly to achieve improved robustness. To the best\nof our knowledge, this is the first work that uses trainable noise injection to\nimprove network robustness against adversarial attacks, rather than manually\nconfiguring the injected noise level through cross-validation. The extensive\nresults show that our proposed PNI technique effectively improves the\nrobustness against a variety of powerful white-box and black-box attacks such\nas PGD, C & W, FGSM, transferable attack and ZOO attack. Last but not the\nleast, PNI method improves both clean- and perturbed-data accuracy in\ncomparison to the state-of-the-art defense methods, which outperforms current\nunbroken PGD defense by 1.1 % and 6.8 % on clean test data and perturbed test\ndata respectively using Resnet-20 architecture.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 21:10:52 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Rakin", "Adnan Siraj", ""], ["He", "Zhezhi", ""], ["Fan", "Deliang", ""]]}, {"id": "1811.09317", "submitter": "Carl Rietschel", "authors": "Carl Rietschel, Jinsung Yoon, Mihaela van der Schaar", "title": "Feature Selection for Survival Analysis with Competing Risks using Deep\n  Learning", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/35", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models for survival analysis have gained significant attention\nin the literature, but they suffer from severe performance deficits when the\ndataset contains many irrelevant features. We give empirical evidence for this\nproblem in real-world medical settings using the state-of-the-art model\nDeepHit. Furthermore, we develop methods to improve the deep learning model\nthrough novel approaches to feature selection in survival analysis. We propose\nfilter methods for hard feature selection and a neural network architecture\nthat weights features for soft feature selection. Our experiments on two\nreal-world medical datasets demonstrate that substantial performance\nimprovements against the original models are achievable.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 22:25:46 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 15:34:05 GMT"}, {"version": "v3", "created": "Wed, 16 Jan 2019 01:31:58 GMT"}, {"version": "v4", "created": "Thu, 7 Mar 2019 15:16:53 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Rietschel", "Carl", ""], ["Yoon", "Jinsung", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1811.09346", "submitter": "Jun Liu", "authors": "Jun Liu, Kai Mei, Dongtang Ma and Jibo Wei", "title": "Deep Neural Network Aided Scenario Identification in Wireless Multi-path\n  Fading Channels", "comments": "Draft of a four-page letter with 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter illustrates our preliminary works in deep nerual network (DNN)\nfor wireless communication scenario identification in wireless multi-path\nfading channels. In this letter, six kinds of channel scenarios referring to\nCOST 207 channel model have been performed. 100% identification accuracy has\nbeen observed given signal-to-noise (SNR) over 20dB whereas a 88.4% average\naccuracy has been obtained where SNR ranged from 0dB to 40dB. The proposed\nmethod has tested under fast time-varying conditions, which were similar with\nreal world wireless multi-path fading channels, enabling it to work feasibly in\npractical scenario identification.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 02:37:35 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Liu", "Jun", ""], ["Mei", "Kai", ""], ["Ma", "Dongtang", ""], ["Wei", "Jibo", ""]]}, {"id": "1811.09347", "submitter": "Bowen Cheng", "authors": "Bowen Cheng, Yunchao Wei, Jiahui Yu, Shiyu Chang, Jinjun Xiong,\n  Wen-Mei Hwu, Thomas S. Huang, Humphrey Shi", "title": "A Simple Non-i.i.d. Sampling Approach for Efficient Training and Better\n  Generalization", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While training on samples drawn from independent and identical distribution\nhas been a de facto paradigm for optimizing image classification networks,\nhumans learn new concepts in an easy-to-hard manner and on the selected\nexamples progressively. Driven by this fact, we investigate the training\nparadigms where the samples are not drawn from independent and identical\ndistribution. We propose a data sampling strategy, named Drop-and-Refresh\n(DaR), motivated by the learning behaviors of humans that selectively drop easy\nsamples and refresh them only periodically. We show in our experiments that the\nproposed DaR strategy can maintain (and in many cases improve) the predictive\naccuracy even when the training cost is reduced by 15% on various datasets\n(CIFAR 10, CIFAR 100 and ImageNet) and with different backbone architectures\n(ResNets, DenseNets and MobileNets). Furthermore and perhaps more importantly,\nwe find the ImageNet pre-trained models using our DaR sampling strategy\nachieves better transferability for the downstream tasks including object\ndetection (+0.3 AP), instance segmentation (+0.3 AP), scene parsing (+0.5 mIoU)\nand human pose estimation (+0.6 AP). Our investigation encourages people to\nrethink the connections between the sampling strategy for training and the\ntransferability of its learned features for pre-training ImageNet models.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 02:49:47 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 03:39:15 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Cheng", "Bowen", ""], ["Wei", "Yunchao", ""], ["Yu", "Jiahui", ""], ["Chang", "Shiyu", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-Mei", ""], ["Huang", "Thomas S.", ""], ["Shi", "Humphrey", ""]]}, {"id": "1811.09350", "submitter": "Rafael Sousa", "authors": "Rafael T. Sousa, Lucas A. Pereira, Anderson S. Soares", "title": "Predicting Diabetes Disease Evolution Using Financial Records and\n  Recurrent Neural Networks", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/70", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managing patients with chronic diseases is a major and growing healthcare\nchallenge in several countries. A chronic condition, such as diabetes, is an\nillness that lasts a long time and does not go away, and often leads to the\npatient's health gradually getting worse. While recent works involve raw\nelectronic health record (EHR) from hospitals, this work uses only financial\nrecords from health plan providers (medical claims) to predict diabetes disease\nevolution with a self-attentive recurrent neural network. The use of financial\ndata is due to the possibility of being an interface to international\nstandards, as the records standard encodes medical procedures. The main goal\nwas to assess high risk diabetics, so we predict records related to diabetes\nacute complications such as amputations and debridements, revascularization and\nhemodialysis. Our work succeeds to anticipate complications between 60 to 240\ndays with an area under ROC curve ranging from 0.81 to 0.94. In this paper we\ndescribe the first half of a work-in-progress developed within a health plan\nprovider with ROC curve ranging from 0.81 to 0.83. This assessment will give\nhealthcare providers the chance to intervene earlier and head off\nhospitalizations. We are aiming to deliver personalized predictions and\npersonalized recommendations to individual patients, with the goal of improving\noutcomes and reducing costs\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 03:15:05 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 14:21:19 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Sousa", "Rafael T.", ""], ["Pereira", "Lucas A.", ""], ["Soares", "Anderson S.", ""]]}, {"id": "1811.09358", "submitter": "Li Shen", "authors": "Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang and Wei Liu", "title": "A Sufficient Condition for Convergences of Adam and RMSProp", "comments": "Accepted by CVPR2019 as an Oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adam and RMSProp are two of the most influential adaptive stochastic\nalgorithms for training deep neural networks, which have been pointed out to be\ndivergent even in the convex setting via a few simple counterexamples. Many\nattempts, such as decreasing an adaptive learning rate, adopting a big batch\nsize, incorporating a temporal decorrelation technique, seeking an analogous\nsurrogate, etc., have been tried to promote Adam/RMSProp-type algorithms to\nconverge. In contrast with existing approaches, we introduce an alternative\neasy-to-check sufficient condition, which merely depends on the parameters of\nthe base learning rate and combinations of historical second-order moments, to\nguarantee the global convergence of generic Adam/RMSProp for solving\nlarge-scale non-convex stochastic optimization. Moreover, we show that the\nconvergences of several variants of Adam, such as AdamNC, AdaEMA, etc., can be\ndirectly implied via the proposed sufficient condition in the non-convex\nsetting. In addition, we illustrate that Adam is essentially a specifically\nweighted AdaGrad with exponential moving average momentum, which provides a\nnovel perspective for understanding Adam and RMSProp. This observation coupled\nwith this sufficient condition gives much deeper interpretations on their\ndivergences. At last, we validate the sufficient condition by applying Adam and\nRMSProp to tackle a certain counterexample and train deep neural networks.\nNumerical results are exactly in accord with our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 04:26:47 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 08:59:14 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 03:39:53 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Zou", "Fangyu", ""], ["Shen", "Li", ""], ["Jie", "Zequn", ""], ["Zhang", "Weizhong", ""], ["Liu", "Wei", ""]]}, {"id": "1811.09364", "submitter": "Younggun Lee", "authors": "Younggun Lee and Suwon Shon and Taesu Kim", "title": "Learning pronunciation from a foreign language in speech synthesis\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there are more than 6,500 languages in the world, the pronunciations\nof many phonemes sound similar across the languages. When people learn a\nforeign language, their pronunciation often reflects their native language's\ncharacteristics. This motivates us to investigate how the speech synthesis\nnetwork learns the pronunciation from datasets from different languages. In\nthis study, we are interested in analyzing and taking advantage of multilingual\nspeech synthesis network. First, we train the speech synthesis network\nbilingually in English and Korean and analyze how the network learns the\nrelations of phoneme pronunciation between the languages. Our experimental\nresult shows that the learned phoneme embedding vectors are located closer if\ntheir pronunciations are similar across the languages. Consequently, the\ntrained networks can synthesize the English speakers' Korean speech and vice\nversa. Using this result, we propose a training framework to utilize\ninformation from a different language. To be specific, we pre-train a speech\nsynthesis network using datasets from both high-resource language and\nlow-resource language, then we fine-tune the network using the low-resource\nlanguage dataset. Finally, we conducted more simulations on 10 different\nlanguages to show it is generally extendable to other languages.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 05:24:15 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 07:50:01 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2019 10:47:30 GMT"}, {"version": "v4", "created": "Wed, 24 Jun 2020 00:57:25 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Lee", "Younggun", ""], ["Shon", "Suwon", ""], ["Kim", "Taesu", ""]]}, {"id": "1811.09380", "submitter": "Yu Cheng", "authors": "Yu Cheng, Ilias Diakonikolas, Rong Ge", "title": "High-Dimensional Robust Mean Estimation in Nearly-Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problem of high-dimensional mean estimation in a\nrobust model where a constant fraction of the samples are adversarially\ncorrupted. Recent work gave the first polynomial time algorithms for this\nproblem with dimension-independent error guarantees for several families of\nstructured distributions.\n  In this work, we give the first nearly-linear time algorithms for\nhigh-dimensional robust mean estimation. Specifically, we focus on\ndistributions with (i) known covariance and sub-gaussian tails, and (ii)\nunknown bounded covariance. Given $N$ samples on $\\mathbb{R}^d$, an\n$\\epsilon$-fraction of which may be arbitrarily corrupted, our algorithms run\nin time $\\tilde{O}(Nd) / \\mathrm{poly}(\\epsilon)$ and approximate the true mean\nwithin the information-theoretically optimal error, up to constant factors.\nPrevious robust algorithms with comparable error guarantees have running times\n$\\tilde{\\Omega}(N d^2)$, for $\\epsilon = \\Omega(1)$.\n  Our algorithms rely on a natural family of SDPs parameterized by our current\nguess $\\nu$ for the unknown mean $\\mu^\\star$. We give a win-win analysis\nestablishing the following: either a near-optimal solution to the primal SDP\nyields a good candidate for $\\mu^\\star$ -- independent of our current guess\n$\\nu$ -- or the dual SDP yields a new guess $\\nu'$ whose distance from\n$\\mu^\\star$ is smaller by a constant factor. We exploit the special structure\nof the corresponding SDPs to show that they are approximately solvable in\nnearly-linear time. Our approach is quite general, and we believe it can also\nbe applied to obtain nearly-linear time algorithms for other high-dimensional\nrobust learning problems.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 07:51:35 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Cheng", "Yu", ""], ["Diakonikolas", "Ilias", ""], ["Ge", "Rong", ""]]}, {"id": "1811.09385", "submitter": "Jishnu Mukhoti", "authors": "Jishnu Mukhoti, Pontus Stenetorp, Yarin Gal", "title": "On the Importance of Strong Baselines in Bayesian Deep Learning", "comments": "Bayesian Deep Learning Workshop, NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like all sub-fields of machine learning Bayesian Deep Learning is driven by\nempirical validation of its theoretical proposals. Given the many aspects of an\nexperiment it is always possible that minor or even major experimental flaws\ncan slip by both authors and reviewers. One of the most popular experiments\nused to evaluate approximate inference techniques is the regression experiment\non UCI datasets. However, in this experiment, models which have been trained to\nconvergence have often been compared with baselines trained only for a fixed\nnumber of iterations. We find that a well-established baseline, Monte Carlo\ndropout, when evaluated under the same experimental settings shows significant\nimprovements. In fact, the baseline outperforms or performs competitively with\nmethods that claimed to be superior to the very same baseline method when they\nwere introduced. Hence, by exposing this flaw in experimental procedure, we\nhighlight the importance of using identical experimental setups to evaluate,\ncompare, and benchmark methods in Bayesian Deep Learning.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 08:22:17 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 19:13:36 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Mukhoti", "Jishnu", ""], ["Stenetorp", "Pontus", ""], ["Gal", "Yarin", ""]]}, {"id": "1811.09386", "submitter": "Cunxiao Du", "authors": "Cunxiao Du, Zhaozheng Chin, Fuli Feng, Lei Zhu, Tian Gan, Liqiang Nie", "title": "Explicit Interaction Model towards Text Classification", "comments": "8 pages", "journal-ref": "AAAI 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification is one of the fundamental tasks in natural language\nprocessing. Recently, deep neural networks have achieved promising performance\nin the text classification task compared to shallow models. Despite of the\nsignificance of deep models, they ignore the fine-grained (matching signals\nbetween words and classes) classification clues since their classifications\nmainly rely on the text-level representations. To address this problem, we\nintroduce the interaction mechanism to incorporate word-level matching signals\ninto the text classification task. In particular, we design a novel framework,\nEXplicit interAction Model (dubbed as EXAM), equipped with the interaction\nmechanism. We justified the proposed approach on several benchmark datasets\nincluding both multi-label and multi-class text classification tasks. Extensive\nexperimental results demonstrate the superiority of the proposed method. As a\nbyproduct, we have released the codes and parameter settings to facilitate\nother researches.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 08:30:19 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Du", "Cunxiao", ""], ["Chin", "Zhaozheng", ""], ["Feng", "Fuli", ""], ["Zhu", "Lei", ""], ["Gan", "Tian", ""], ["Nie", "Liqiang", ""]]}, {"id": "1811.09393", "submitter": "Mengyu Chu", "authors": "Mengyu Chu, You Xie, Jonas Mayer, Laura Leal-Taix\\'e, Nils Thuerey", "title": "Learning Temporal Coherence via Self-Supervision for GAN-based Video\n  Generation", "comments": "Project page: https://ge.in.tum.de/publications/2019-tecogan-chu/,\n  code link: https://github.com/thunil/TecoGAN", "journal-ref": null, "doi": "10.1145/3386569.3392457", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work explores temporal self-supervision for GAN-based video generation\ntasks. While adversarial training successfully yields generative models for a\nvariety of areas, temporal relationships in the generated data are much less\nexplored. Natural temporal changes are crucial for sequential generation tasks,\ne.g. video super-resolution and unpaired video translation. For the former,\nstate-of-the-art methods often favor simpler norm losses such as $L^2$ over\nadversarial training. However, their averaging nature easily leads to\ntemporally smooth results with an undesirable lack of spatial detail. For\nunpaired video translation, existing approaches modify the generator networks\nto form spatio-temporal cycle consistencies. In contrast, we focus on improving\nlearning objectives and propose a temporally self-supervised algorithm. For\nboth tasks, we show that temporal adversarial learning is key to achieving\ntemporally coherent solutions without sacrificing spatial detail. We also\npropose a novel Ping-Pong loss to improve the long-term temporal consistency.\nIt effectively prevents recurrent networks from accumulating artifacts\ntemporally without depressing detailed features. Additionally, we propose a\nfirst set of metrics to quantitatively evaluate the accuracy as well as the\nperceptual quality of the temporal evolution. A series of user studies confirm\nthe rankings computed with these metrics. Code, data, models, and results are\nprovided at https://github.com/thunil/TecoGAN. The project page\nhttps://ge.in.tum.de/publications/2019-tecogan-chu/ contains supplemental\nmaterials.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 09:16:22 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 14:10:51 GMT"}, {"version": "v3", "created": "Tue, 24 Dec 2019 08:31:06 GMT"}, {"version": "v4", "created": "Thu, 21 May 2020 15:06:30 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Chu", "Mengyu", ""], ["Xie", "You", ""], ["Mayer", "Jonas", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Thuerey", "Nils", ""]]}, {"id": "1811.09409", "submitter": "Florian Pfisterer", "authors": "Florian Pfisterer and Jan N. van Rijn and Philipp Probst and Andreas\n  M\\\"uller and Bernd Bischl", "title": "Learning Multiple Defaults for Machine Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of modern machine learning methods highly depends on their\nhyperparameter configurations. One simple way of selecting a configuration is\nto use default settings, often proposed along with the publication and\nimplementation of a new algorithm. Those default values are usually chosen in\nan ad-hoc manner to work good enough on a wide variety of datasets. To address\nthis problem, different automatic hyperparameter configuration algorithms have\nbeen proposed, which select an optimal configuration per dataset. This\nprincipled approach usually improves performance but adds additional\nalgorithmic complexity and computational costs to the training procedure. As an\nalternative to this, we propose learning a set of complementary default values\nfrom a large database of prior empirical results. Selecting an appropriate\nconfiguration on a new dataset then requires only a simple, efficient and\nembarrassingly parallel search over this set. We demonstrate the effectiveness\nand efficiency of the approach we propose in comparison to random search and\nBayesian Optimization.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 09:48:19 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 11:04:27 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 06:57:13 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Pfisterer", "Florian", ""], ["van Rijn", "Jan N.", ""], ["Probst", "Philipp", ""], ["M\u00fcller", "Andreas", ""], ["Bischl", "Bernd", ""]]}, {"id": "1811.09426", "submitter": "Chen Yukang", "authors": "Yukang Chen, Gaofeng Meng, Qian Zhang, Xinbang Zhang, Liangchen Song,\n  Shiming Xiang and Chunhong Pan", "title": "Joint Neural Architecture Search and Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing neural architectures is a fundamental step in deep learning\napplications. As a partner technique, model compression on neural networks has\nbeen widely investigated to gear the needs that the deep learning algorithms\ncould be run with the limited computation resources on mobile devices.\nCurrently, both the tasks of architecture design and model compression require\nexpertise tricks and tedious trials. In this paper, we integrate these two\ntasks into one unified framework, which enables the joint architecture search\nwith quantization (compression) policies for neural networks. This method is\nnamed JASQ. Here our goal is to automatically find a compact neural network\nmodel with high performance that is suitable for mobile devices. Technically, a\nmulti-objective evolutionary search algorithm is introduced to search the\nmodels under the balance between model size and performance accuracy. In\nexperiments, we find that our approach outperforms the methods that search only\nfor architectures or only for quantization policies. 1) Specifically, given\nexisting networks, our approach can provide them with learning-based\nquantization policies, and outperforms their 2 bits, 4 bits, 8 bits, and 16\nbits counterparts. It can yield higher accuracies than the float models, for\nexample, over 1.02% higher accuracy on MobileNet-v1. 2) What is more, under the\nbalance between model size and performance accuracy, two models are obtained\nwith joint search of architectures and quantization policies: a high-accuracy\nmodel and a small model, JASQNet and JASQNet-Small that achieves 2.97% error\nrate with 0.9 MB on CIFAR-10.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 10:58:46 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Chen", "Yukang", ""], ["Meng", "Gaofeng", ""], ["Zhang", "Qian", ""], ["Zhang", "Xinbang", ""], ["Song", "Liangchen", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1811.09491", "submitter": "Quanming Yao", "authors": "Quanming Yao, Xiawei Guo, James T. Kwok, WeiWei Tu, Yuqiang Chen,\n  Wenyuan Dai, Qiang Yang", "title": "Differential Private Stack Generalization with an Application to\n  Diabetes Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To meet the standard of differential privacy, noise is usually added into the\noriginal data, which inevitably deteriorates the predicting performance of\nsubsequent learning algorithms. In this paper, motivated by the success of\nimproving predicting performance by ensemble learning, we propose to enhance\nprivacy-preserving logistic regression by stacking. We show that this can be\ndone either by sample-based or feature-based partitioning. However, we prove\nthat when privacy-budgets are the same, feature-based partitioning requires\nfewer samples than sample-based one, and thus likely has better empirical\nperformance. As transfer learning is difficult to be integrated with a\ndifferential privacy guarantee, we further combine the proposed method with\nhypothesis transfer learning to address the problem of learning across\ndifferent organizations. Finally, we not only demonstrate the effectiveness of\nour method on two benchmark data sets, i.e., MNIST and NEWS20, but also apply\nit into a real application of cross-organizational diabetes prediction from\nRUIJIN data set, where privacy is of significant concern.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 14:26:03 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 16:17:41 GMT"}, {"version": "v3", "created": "Sun, 2 Jun 2019 06:57:25 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Yao", "Quanming", ""], ["Guo", "Xiawei", ""], ["Kwok", "James T.", ""], ["Tu", "WeiWei", ""], ["Chen", "Yuqiang", ""], ["Dai", "Wenyuan", ""], ["Yang", "Qiang", ""]]}, {"id": "1811.09496", "submitter": "Christian Sch\\\"on", "authors": "Christian Sch\\\"on (1), Jens Dittrich (1), Richard M\\\"uller (2) ((1)\n  Saarland Informatics Campus, (2) Deutscher Wetterdienst)", "title": "The Error is the Feature: how to Forecast Lightning using a Model\n  Prediction Error", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": "10.1145/3292500.3330682", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the progress within the last decades, weather forecasting is still a\nchallenging and computationally expensive task. Current satellite-based\napproaches to predict thunderstorms are usually based on the analysis of the\nobserved brightness temperatures in different spectral channels and emit a\nwarning if a critical threshold is reached. Recent progress in data science\nhowever demonstrates that machine learning can be successfully applied to many\nresearch fields in science, especially in areas dealing with large datasets. We\ntherefore present a new approach to the problem of predicting thunderstorms\nbased on machine learning. The core idea of our work is to use the error of\ntwo-dimensional optical flow algorithms applied to images of meteorological\nsatellites as a feature for machine learning models. We interpret that optical\nflow error as an indication of convection potentially leading to thunderstorms\nand lightning. To factor in spatial proximity we use various manual convolution\nsteps. We also consider effects such as the time of day or the geographic\nlocation. We train different tree classifier models as well as a neural network\nto predict lightning within the next few hours (called nowcasting in\nmeteorology) based on these features. In our evaluation section we compare the\npredictive power of the different models and the impact of different features\non the classification result. Our results show a high accuracy of 96% for\npredictions over the next 15 minutes which slightly decreases with increasing\nforecast period but still remains above 83% for forecasts of up to five hours.\nThe high false positive rate of nearly 6% however needs further investigation\nto allow for an operational use of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 14:36:23 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 12:42:40 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Sch\u00f6n", "Christian", ""], ["Dittrich", "Jens", ""], ["M\u00fcller", "Richard", ""]]}, {"id": "1811.09539", "submitter": "Josef Lorenz Rumberger", "authors": "Elias Baumann and Josef Lorenz Rumberger", "title": "State of the Art in Fair ML: From Moral Philosophy and Legislation to\n  Fair Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is becoming an ever present part in our lives as many\ndecisions, e.g. to lend a credit, are no longer made by humans but by machine\nlearning algorithms. However those decisions are often unfair and\ndiscriminating individuals belonging to protected groups based on race or\ngender. With the recent General Data Protection Regulation (GDPR) coming into\neffect, new awareness has been raised for such issues and with computer\nscientists having such a large impact on peoples lives it is necessary that\nactions are taken to discover and prevent discrimination. This work aims to\ngive an introduction into discrimination, legislative foundations to counter it\nand strategies to detect and prevent machine learning algorithms from showing\nsuch behavior.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 12:03:55 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Baumann", "Elias", ""], ["Rumberger", "Josef Lorenz", ""]]}, {"id": "1811.09556", "submitter": "Yan Wu", "authors": "Yan Wu, Greg Wayne, Karol Gregor, Timothy Lillicrap", "title": "Learning Attractor Dynamics for Generative Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central challenge faced by memory systems is the robust retrieval of a\nstored pattern in the presence of interference due to other stored patterns and\nnoise. A theoretically well-founded solution to robust retrieval is given by\nattractor dynamics, which iteratively clean up patterns during recall. However,\nincorporating attractor dynamics into modern deep learning systems poses\ndifficulties: attractor basins are characterised by vanishing gradients, which\nare known to make training neural networks difficult. In this work, we avoid\nthe vanishing gradient problem by training a generative distributed memory\nwithout simulating the attractor dynamics. Based on the idea of memory writing\nas inference, as proposed in the Kanerva Machine, we show that a\nlikelihood-based Lyapunov function emerges from maximising the variational\nlower-bound of a generative memory. Experiments shows it converges to correct\npatterns upon iterative retrieval and achieves competitive performance as both\na memory model and a generative model.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 16:49:02 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Wu", "Yan", ""], ["Wayne", "Greg", ""], ["Gregor", "Karol", ""], ["Lillicrap", "Timothy", ""]]}, {"id": "1811.09557", "submitter": "Abbas Rahimi", "authors": "Abbas Rahimi, Tony F. Wu, Haitong Li, Jan M. Rabaey, H.-S. Philip\n  Wong, Max M. Shulaker, Subhasish Mitra", "title": "Hyperdimensional Computing Nanosystem", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One viable solution for continuous reduction in energy-per-operation is to\nrethink functionality to cope with uncertainty by adopting computational\napproaches that are inherently robust to uncertainty. It requires a novel look\nat data representations, associated operations, and circuits, and at materials\nand substrates that enable them. 3D integrated nanotechnologies combined with\nnovel brain-inspired computational paradigms that support fast learning and\nfault tolerance could lead the way. Recognizing the very size of the brain's\ncircuits, hyperdimensional (HD) computing can model neural activity patterns\nwith points in a HD space, that is, with hypervectors as large randomly\ngenerated patterns. At its very core, HD computing is about manipulating and\ncomparing these patterns inside memory. Emerging nanotechnologies such as\ncarbon nanotube field effect transistors (CNFETs) and resistive RAM (RRAM), and\ntheir monolithic 3D integration offer opportunities for hardware\nimplementations of HD computing through tight integration of logic and memory,\nenergy-efficient computation, and unique device characteristics. We\nexperimentally demonstrate and characterize an end-to-end HD computing\nnanosystem built using monolithic 3D integration of CNFETs and RRAM. With our\nnanosystem, we experimentally demonstrate classification of 21 languages with\nmeasured accuracy of up to 98% on >20,000 sentences (6.4 million characters),\ntraining using one text sample (~100,000 characters) per language, and\nresilient operation (98% accuracy) despite 78% hardware errors in HD\nrepresentation (outputs stuck at 0 or 1). By exploiting the unique properties\nof the underlying nanotechnologies, we show that HD computing, when implemented\nwith monolithic 3D integration, can be up to 420X more energy-efficient while\nusing 25X less area compared to traditional silicon CMOS implementations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 16:52:16 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Rahimi", "Abbas", ""], ["Wu", "Tony F.", ""], ["Li", "Haitong", ""], ["Rabaey", "Jan M.", ""], ["Wong", "H. -S. Philip", ""], ["Shulaker", "Max M.", ""], ["Mitra", "Subhasish", ""]]}, {"id": "1811.09558", "submitter": "Zi Wang", "authors": "Zi Wang and Beomjoon Kim and Leslie Pack Kaelbling", "title": "Regret bounds for meta Bayesian optimization with an unknown Gaussian\n  process prior", "comments": "Proceedings of the Thirty-second Conference on Neural Information\n  Processing Systems, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization usually assumes that a Bayesian prior is given.\nHowever, the strong theoretical guarantees in Bayesian optimization are often\nregrettably compromised in practice because of unknown parameters in the prior.\nIn this paper, we adopt a variant of empirical Bayes and show that, by\nestimating the Gaussian process prior from offline data sampled from the same\nprior and constructing unbiased estimators of the posterior, variants of both\nGP-UCB and probability of improvement achieve a near-zero regret bound, which\ndecreases to a constant proportional to the observational noise as the number\nof offline data and the number of online evaluations increase. Empirically, we\nhave verified our approach on challenging simulated robotic problems featuring\ntask and motion planning.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 16:54:45 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Wang", "Zi", ""], ["Kim", "Beomjoon", ""], ["Kaelbling", "Leslie Pack", ""]]}, {"id": "1811.09562", "submitter": "Amina Houari Phd", "authors": "Amina Houari", "title": "Contributions to Biclustering of Microarray Data Using Formal Concept\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biclustering is an unsupervised data mining technique that aims to unveil\npatterns (biclusters) from gene expression data matrices. In the framework of\nthis thesis, we propose new biclustering algorithms for microarray data. The\nlatter is done using data mining techniques. The objective is to identify\npositively and negatively correlated biclusters.\n  This thesis is divided into two part: In the first part, we present an\noverview of the pattern-mining techniques and the biclustering of microarray\ndata. In the second part, we present our proposed biclustering algorithms where\nwe rely on two axes. In the first axis, we initially focus on extracting\nbiclusters of positive correlations. For this, we use both Formal Concept\nAnalysis and Association Rules. In the second axis, we focus on the extraction\nof negatively correlated biclusters.\n  The performed experimental studies highlight the very promising results\noffered by the proposed algorithms. Our biclustering algorithms are evaluated\nand compared statistically and biologically.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 17:05:45 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Houari", "Amina", ""]]}, {"id": "1811.09567", "submitter": "Yipeng Qin", "authors": "Yipeng Qin, Niloy Mitra, Peter Wonka", "title": "How does Lipschitz Regularization Influence GAN Training?", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of Lipschitz regularization in stabilizing GAN training,\nthe exact reason of its effectiveness remains poorly understood. The direct\neffect of $K$-Lipschitz regularization is to restrict the $L2$-norm of the\nneural network gradient to be smaller than a threshold $K$ (e.g., $K=1$) such\nthat $\\|\\nabla f\\| \\leq K$. In this work, we uncover an even more important\neffect of Lipschitz regularization by examining its impact on the loss\nfunction: It degenerates GAN loss functions to almost linear ones by\nrestricting their domain and interval of attainable gradient values. Our\nanalysis shows that loss functions are only successful if they are degenerated\nto almost linear ones. We also show that loss functions perform poorly if they\nare not degenerated and that a wide range of functions can be used as loss\nfunction as long as they are sufficiently degenerated by regularization.\nBasically, Lipschitz regularization ensures that all loss functions effectively\nwork in the same way. Empirically, we verify our proposition on the MNIST,\nCIFAR10 and CelebA datasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 17:18:00 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 14:00:53 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 07:39:24 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Qin", "Yipeng", ""], ["Mitra", "Niloy", ""], ["Wonka", "Peter", ""]]}, {"id": "1811.09568", "submitter": "George Moustakides", "authors": "Kalliopi Basioti, George V.Moustakides, Emmanouil Z. Psarakis", "title": "Kernel-Based Training of Generative Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are designed with the help of min-max\noptimization problems that are solved with stochastic gradient-type algorithms\nwhich are known to be non-robust. In this work we revisit a non-adversarial\nmethod based on kernels which relies on a pure minimization problem and propose\na simple stochastic gradient algorithm for the computation of its solution.\nUsing simplified tools from Stochastic Approximation theory we demonstrate that\nbatch versions of the algorithm or smoothing of the gradient do not improve\nconvergence. These observations allow for the development of a training\nalgorithm that enjoys reduced computational complexity and increased robustness\nwhile exhibiting similar synthesis characteristics as classical GANs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 17:18:48 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Basioti", "Kalliopi", ""], ["Moustakides", "George V.", ""], ["Psarakis", "Emmanouil Z.", ""]]}, {"id": "1811.09577", "submitter": "Iqbal H. Sarker", "authors": "Iqbal H. Sarker, Alan Colman, MA Kabir, Jun Han", "title": "Individualized Time-Series Segmentation for Mining Mobile Phone User\n  Behavior", "comments": "20 pages", "journal-ref": "The Computer Journal, Section C: Computational Intelligence,\n  Machine Learning and Data Analytics, Publisher: Oxford University, UK, 2017", "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile phones can record individual's daily behavioral data as a time-series.\nIn this paper, we present an effective time-series segmentation technique that\nextracts optimal time segments of individual's similar behavioral\ncharacteristics utilizing their mobile phone data. One of the determinants of\nan individual's behavior is the various activities undertaken at various\ntimes-of-the-day and days-of-the-week. In many cases, such behavior will follow\ntemporal patterns. Currently, researchers use either equal or unequal\ninterval-based segmentation of time for mining mobile phone users' behavior.\nMost of them take into account static temporal coverage of 24-h-a-day and few\nof them take into account the number of incidences in time-series data.\nHowever, such segmentations do not necessarily map to the patterns of\nindividual user activity and subsequent behavior because of not taking into\naccount the diverse behaviors of individuals over time-of-the-week. Therefore,\nwe propose a behavior-oriented time segmentation (BOTS) technique that takes\ninto account not only the temporal coverage of the week but also the number of\nincidences of diverse behaviors dynamically for producing similar behavioral\ntime segments over the week utilizing time-series data. Experiments on the real\nmobile phone datasets show that our proposed segmentation technique better\ncaptures the user's dominant behavior at various times-of-the-day and\ndays-of-the-week enabling the generation of high confidence temporal rules in\norder to mine individual mobile phone users' behavior.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 11:09:58 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Sarker", "Iqbal H.", ""], ["Colman", "Alan", ""], ["Kabir", "MA", ""], ["Han", "Jun", ""]]}, {"id": "1811.09595", "submitter": "Boris Knyazev", "authors": "Boris Knyazev, Xiao Lin, Mohamed R. Amer, Graham W. Taylor", "title": "Spectral Multigraph Networks for Discovering and Fusing Relationships in\n  Molecules", "comments": "11 pages, 5 figures, NIPS 2018 Workshop on Machine Learning for\n  Molecules and Materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral Graph Convolutional Networks (GCNs) are a generalization of\nconvolutional networks to learning on graph-structured data. Applications of\nspectral GCNs have been successful, but limited to a few problems where the\ngraph is fixed, such as shape correspondence and node classification. In this\nwork, we address this limitation by revisiting a particular family of spectral\ngraph networks, Chebyshev GCNs, showing its efficacy in solving graph\nclassification tasks with a variable graph structure and size. Chebyshev GCNs\nrestrict graphs to have at most one edge between any pair of nodes. To this\nend, we propose a novel multigraph network that learns from multi-relational\ngraphs. We model learned edges with abstract meaning and experiment with\ndifferent ways to fuse the representations extracted from annotated and learned\nedges, achieving competitive results on a variety of chemical classification\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 18:46:59 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Knyazev", "Boris", ""], ["Lin", "Xiao", ""], ["Amer", "Mohamed R.", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1811.09600", "submitter": "Luiz Gustavo Hafemann", "authors": "J\\'er\\^ome Rony, Luiz G. Hafemann, Luiz S. Oliveira, Ismail Ben Ayed,\n  Robert Sabourin, Eric Granger", "title": "Decoupling Direction and Norm for Efficient Gradient-Based L2\n  Adversarial Attacks and Defenses", "comments": "Accepted as a conference paper to the 2019 IEEE/CVF Conference on\n  Computer Vision and Pattern Recognition (CVPR oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on adversarial examples in computer vision tasks has shown that\nsmall, often imperceptible changes to an image can induce misclassification,\nwhich has security implications for a wide range of image processing systems.\nConsidering $L_2$ norm distortions, the Carlini and Wagner attack is presently\nthe most effective white-box attack in the literature. However, this method is\nslow since it performs a line-search for one of the optimization terms, and\noften requires thousands of iterations. In this paper, an efficient approach is\nproposed to generate gradient-based attacks that induce misclassifications with\nlow $L_2$ norm, by decoupling the direction and the norm of the adversarial\nperturbation that is added to the image. Experiments conducted on the MNIST,\nCIFAR-10 and ImageNet datasets indicate that our attack achieves comparable\nresults to the state-of-the-art (in terms of $L_2$ norm) with considerably\nfewer iterations (as few as 100 iterations), which opens the possibility of\nusing these attacks for adversarial training. Models trained with our attack\nachieve state-of-the-art robustness against white-box gradient-based $L_2$\nattacks on the MNIST and CIFAR-10 datasets, outperforming the Madry defense\nwhen the attacks are limited to a maximum norm.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 18:54:47 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 21:11:22 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2019 21:11:11 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Rony", "J\u00e9r\u00f4me", ""], ["Hafemann", "Luiz G.", ""], ["Oliveira", "Luiz S.", ""], ["Ayed", "Ismail Ben", ""], ["Sabourin", "Robert", ""], ["Granger", "Eric", ""]]}, {"id": "1811.09602", "submitter": "Aniruddh Raghu", "authors": "Aniruddh Raghu, Matthieu Komorowski, Sumeetpal Singh", "title": "Model-Based Reinforcement Learning for Sepsis Treatment", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "Report number: ML4H/2018/41", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sepsis is a dangerous condition that is a leading cause of patient mortality.\nTreating sepsis is highly challenging, because individual patients respond very\ndifferently to medical interventions and there is no universally agreed-upon\ntreatment for sepsis. In this work, we explore the use of continuous\nstate-space model-based reinforcement learning (RL) to discover high-quality\ntreatment policies for sepsis patients. Our quantitative evaluation reveals\nthat by blending the treatment strategy discovered with RL with what clinicians\nfollow, we can obtain improved policies, potentially allowing for better\nmedical treatment for sepsis.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 18:57:30 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Raghu", "Aniruddh", ""], ["Komorowski", "Matthieu", ""], ["Singh", "Sumeetpal", ""]]}, {"id": "1811.09619", "submitter": "Bianca-Cristina Cristescu", "authors": "Bianca-Cristina Cristescu, Zal\\'an Borsos, John Lygeros, Mar\\'ia\n  Rodr\\'iguez Mart\\'inez, Maria Anna Rapsomaniki", "title": "Inference of the three-dimensional chromatin structure and its temporal\n  behavior", "comments": "10 pages, 7 figures, 1 algorithm. Neural Information Processing\n  Systems, Machine Learning for Molecules and Materials, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the three-dimensional (3D) structure of the genome is essential\nfor elucidating vital biological processes and their links to human disease. To\ndetermine how the genome folds within the nucleus, chromosome conformation\ncapture methods such as HiC have recently been employed. However, computational\nmethods that exploit the resulting high-throughput, high-resolution data are\nstill suffering from important limitations. In this work, we explore the idea\nof manifold learning for the 3D chromatin structure inference and present a\nnovel method, REcurrent Autoencoders for CHromatin 3D structure prediction\n(REACH-3D). Our framework employs autoencoders with recurrent neural units to\nreconstruct the chromatin structure. In comparison to existing methods,\nREACH-3D makes no transfer function assumption and permits dynamic analysis.\nEvaluating REACH-3D on synthetic data indicated high agreement with the ground\ntruth. When tested on real experimental HiC data, REACH-3D recovered most\nfaithfully the expected biological properties and obtained the highest\ncorrelation coefficient with microscopy measurements. Last, REACH-3D was\napplied to dynamic HiC data, where it successfully modeled chromatin\nconformation during the cell cycle.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 15:19:33 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Cristescu", "Bianca-Cristina", ""], ["Borsos", "Zal\u00e1n", ""], ["Lygeros", "John", ""], ["Mart\u00ednez", "Mar\u00eda Rodr\u00edguez", ""], ["Rapsomaniki", "Maria Anna", ""]]}, {"id": "1811.09620", "submitter": "Sicong(Sheldon) Huang", "authors": "Sicong Huang, Qiyang Li, Cem Anil, Xuchan Bao, Sageev Oore, Roger B.\n  Grosse", "title": "TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre\n  Transfer", "comments": "17 pages, published as a conference paper at ICLR 2019", "journal-ref": "ICLR 2019", "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of musical timbre transfer, where the\ngoal is to manipulate the timbre of a sound sample from one instrument to match\nanother instrument while preserving other musical content, such as pitch,\nrhythm, and loudness. In principle, one could apply image-based style transfer\ntechniques to a time-frequency representation of an audio signal, but this\ndepends on having a representation that allows independent manipulation of\ntimbre as well as high-quality waveform generation. We introduce TimbreTron, a\nmethod for musical timbre transfer which applies \"image\" domain style transfer\nto a time-frequency representation of the audio signal, and then produces a\nhigh-quality waveform using a conditional WaveNet synthesizer. We show that the\nConstant Q Transform (CQT) representation is particularly well-suited to\nconvolutional architectures due to its approximate pitch equivariance. Based on\nhuman perceptual evaluations, we confirmed that TimbreTron recognizably\ntransferred the timbre while otherwise preserving the musical content, for both\nmonophonic and polyphonic samples.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 17:46:51 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 02:40:24 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Huang", "Sicong", ""], ["Li", "Qiyang", ""], ["Anil", "Cem", ""], ["Bao", "Xuchan", ""], ["Oore", "Sageev", ""], ["Grosse", "Roger B.", ""]]}, {"id": "1811.09621", "submitter": "Marwin Segler", "authors": "Nathan Brown, Marco Fiscato, Marwin H.S. Segler, Alain C. Vaucher", "title": "GuacaMol: Benchmarking Models for De Novo Molecular Design", "comments": null, "journal-ref": null, "doi": "10.1021/acs.jcim.8b00839", "report-no": null, "categories": "q-bio.QM cs.LG physics.chem-ph q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  De novo design seeks to generate molecules with required property profiles by\nvirtual design-make-test cycles. With the emergence of deep learning and neural\ngenerative models in many application areas, models for molecular design based\non neural networks appeared recently and show promising results. However, the\nnew models have not been profiled on consistent tasks, and comparative studies\nto well-established algorithms have only seldom been performed.\n  To standardize the assessment of both classical and neural models for de novo\nmolecular design, we propose an evaluation framework, GuacaMol, based on a\nsuite of standardized benchmarks. The benchmark tasks encompass measuring the\nfidelity of the models to reproduce the property distribution of the training\nsets, the ability to generate novel molecules, the exploration and exploitation\nof chemical space, and a variety of single and multi-objective optimization\ntasks. The benchmarking open-source Python code, and a leaderboard can be found\non https://benevolent.ai/guacamol\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 18:08:13 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 18:25:21 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Brown", "Nathan", ""], ["Fiscato", "Marco", ""], ["Segler", "Marwin H. S.", ""], ["Vaucher", "Alain C.", ""]]}, {"id": "1811.09623", "submitter": "Qing Xu", "authors": "Qing Xu, Xiaohua Xuan", "title": "Nonlinear Regression without i.i.d. Assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a class of nonlinear regression problems without\nthe assumption of being independent and identically distributed. We propose a\ncorrespondent mini-max problem for nonlinear regression and give a numerical\nalgorithm. Such an algorithm can be applied in regression and machine learning\nproblems, and yield better results than traditional least square and machine\nlearning methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 06:25:20 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 10:33:21 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Xu", "Qing", ""], ["Xuan", "Xiaohua", ""]]}, {"id": "1811.09669", "submitter": "Viraj Jayminkumar Shah", "authors": "Rahul Singh, Viraj Shah, Balaji Pokuri, Soumik Sarkar, Baskar\n  Ganapathysubramanian, Chinmay Hegde", "title": "Physics-aware Deep Generative Models for Creating Synthetic\n  Microstructures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key problem in computational material science deals with understanding the\neffect of material distribution (i.e., microstructure) on material performance.\nThe challenge is to synthesize microstructures, given a finite number of\nmicrostructure images, and/or some physical invariances that the microstructure\nexhibits. Conventional approaches are based on stochastic optimization and are\ncomputationally intensive. We introduce three generative models for the fast\nsynthesis of binary microstructure images. The first model is a WGAN model that\nuses a finite number of training images to synthesize new microstructures that\nweakly satisfy the physical invariances respected by the original data. The\nsecond model explicitly enforces known physical invariances by replacing the\ntraditional discriminator in a GAN with an invariance checker. Our third model\ncombines the first two models to reconstruct microstructures that respect both\nexplicit physics invariances as well as implicit constraints learned from the\nimage data. We illustrate these models by reconstructing two-phase\nmicrostructures that exhibit coarsening behavior. The trained models also\nexhibit interesting latent variable interpolation behavior, and the results\nindicate considerable promise for enforcing user-defined physics constraints\nduring microstructure synthesis.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 06:47:02 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Singh", "Rahul", ""], ["Shah", "Viraj", ""], ["Pokuri", "Balaji", ""], ["Sarkar", "Soumik", ""], ["Ganapathysubramanian", "Baskar", ""], ["Hegde", "Chinmay", ""]]}, {"id": "1811.09673", "submitter": "Veronica Tozzo", "authors": "Veronica Tozzo, Federico Tomasi, Margherita Squillario, Annalisa Barla", "title": "Group induced graphical lasso allows for discovery of molecular\n  pathways-pathways interactions", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex systems may contain heterogeneous types of variables that interact in\na multi-level and multi-scale manner. In this context, high-level layers may\nconsidered as groups of variables interacting in lower-level layers. This is\nparticularly true in biology, where, for example, genes are grouped in pathways\nand two types of interactions are present: pathway-pathway interactions and\ngene-gene interactions. However, from data it is only possible to measure the\nexpression of genes while it is impossible to directly measure the activity of\npathways. Nevertheless, the knowledge on the inter-dependence between the\ngroups and the variables allows for a multi-layer network inference, on both\nobserved variables and groups, even if no direct information on the latter is\npresent in the data (hence groups are considered as latent). In this paper, we\npropose an extension of the latent graphical lasso method that leverages on the\nknowledge of the inter-links between the hidden (groups) and observed layers.\nThe method exploits the knowledge of group structure that influence the\nbehaviour of observed variables to retrieve a two layers network. Its efficacy\nwas tested on synthetic data to check its ability in retrieving the network\nstructure compared to the ground truth. We present a case study on\nNeuroblastoma, which shows how our multi-level inference is relevant in real\ncontexts to infer biologically meaningful connections.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 10:39:59 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Tozzo", "Veronica", ""], ["Tomasi", "Federico", ""], ["Squillario", "Margherita", ""], ["Barla", "Annalisa", ""]]}, {"id": "1811.09702", "submitter": "Jooyeon Kim", "authors": "Jooyeon Kim, Dongkwan Kim, Alice Oh", "title": "Homogeneity-Based Transmissive Process to Model True and False News in\n  Social Networks", "comments": "To appear in proceedings of the 12th ACM International Conference on\n  Web Search and Data Mining (WSDM 2019)", "journal-ref": null, "doi": "10.1145/3289600.3291009", "report-no": null, "categories": "cs.CY cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An overwhelming number of true and false news stories are posted and shared\nin social networks, and users diffuse the stories based on multiple factors.\nDiffusion of news stories from one user to another depends not only on the\nstories' content and the genuineness but also on the alignment of the topical\ninterests between the users. In this paper, we propose a novel Bayesian\nnonparametric model that incorporates homogeneity of news stories as the key\ncomponent that regulates the topical similarity between the posting and sharing\nusers' topical interests. Our model extends hierarchical Dirichlet process to\nmodel the topics of the news stories and incorporates Bayesian Gaussian process\nlatent variable model to discover the homogeneity values. We train our model on\na real-world social network dataset and find homogeneity values of news stories\nthat strongly relate to their labels of genuineness and their contents.\nFinally, we show that the supervised version of our model predicts the labels\nof news stories better than the state-of-the-art neural network and Bayesian\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 03:22:53 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Kim", "Jooyeon", ""], ["Kim", "Dongkwan", ""], ["Oh", "Alice", ""]]}, {"id": "1811.09712", "submitter": "Clement Fung", "authors": "Clement Fung, Jamie Koerner, Stewart Grant, Ivan Beschastnikh", "title": "Dancing in the Dark: Private Multi-Party Machine Learning in an\n  Untrusted Setting", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed machine learning (ML) systems today use an unsophisticated threat\nmodel: data sources must trust a central ML process. We propose a brokered\nlearning abstraction that allows data sources to contribute towards a\nglobally-shared model with provable privacy guarantees in an untrusted setting.\nWe realize this abstraction by building on federated learning, the state of the\nart in multi-party ML, to construct TorMentor: an anonymous hidden service that\nsupports private multi-party ML.\n  We define a new threat model by characterizing, developing and evaluating new\nattacks in the brokered learning setting, along with new defenses for these\nattacks. We show that TorMentor effectively protects data providers against\nknown ML attacks while providing them with a tunable trade-off between model\naccuracy and privacy. We evaluate TorMentor with local and geo-distributed\ndeployments on Azure/Tor. In an experiment with 200 clients and 14 MB of data\nper client, our prototype trained a logistic regression model using stochastic\ngradient descent in 65s.\n  Code is available at: https://github.com/DistributedML/TorML\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 22:00:39 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2019 00:40:45 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Fung", "Clement", ""], ["Koerner", "Jamie", ""], ["Grant", "Stewart", ""], ["Beschastnikh", "Ivan", ""]]}, {"id": "1811.09714", "submitter": "C\\u{a}t\\u{a}lina Cangea", "authors": "C\\u{a}t\\u{a}lina Cangea, Arturas Grauslys, Pietro Li\\`o, Francesco\n  Falciani", "title": "Structure-Based Networks for Drug Validation", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/89", "categories": "q-bio.QM cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying chemicals according to putative modes of action (MOAs) is of\nparamount importance in the context of risk assessment. However, current\nmethods are only able to handle a very small proportion of the existing\nchemicals. We address this issue by proposing an integrative deep learning\narchitecture that learns a joint representation from molecular structures of\ndrugs and their effects on human cells. Our choice of architecture is motivated\nby the significant influence of a drug's chemical structure on its MOA. We\nimprove on the strong ability of a unimodal architecture (F1 score of 0.803) to\nclassify drugs by their toxic MOAs (Verhaar scheme) through adding another\nlearning stream that processes transcriptional responses of human cells\naffected by drugs. Our integrative model achieves an even higher classification\nperformance on the LINCS L1000 dataset - the error is reduced by 4.6%. We\nbelieve that our method can be used to extend the current Verhaar scheme and\nconstitute a basis for fast drug validation and risk assessment.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 12:39:19 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Cangea", "C\u0103t\u0103lina", ""], ["Grauslys", "Arturas", ""], ["Li\u00f2", "Pietro", ""], ["Falciani", "Francesco", ""]]}, {"id": "1811.09716", "submitter": "Seyed-Mohsen Moosavi-Dezfooli", "authors": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato,\n  Pascal Frossard", "title": "Robustness via curvature regularization, and vice versa", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art classifiers have been shown to be largely vulnerable to\nadversarial perturbations. One of the most effective strategies to improve\nrobustness is adversarial training. In this paper, we investigate the effect of\nadversarial training on the geometry of the classification landscape and\ndecision boundaries. We show in particular that adversarial training leads to a\nsignificant decrease in the curvature of the loss surface with respect to\ninputs, leading to a drastically more \"linear\" behaviour of the network. Using\na locally quadratic approximation, we provide theoretical evidence on the\nexistence of a strong relation between large robustness and small curvature. To\nfurther show the importance of reduced curvature for improving the robustness,\nwe propose a new regularizer that directly minimizes curvature of the loss\nsurface, and leads to adversarial robustness that is on par with adversarial\ntraining. Besides being a more efficient and principled alternative to\nadversarial training, the proposed regularizer confirms our claims on the\nimportance of exhibiting quasi-linear behavior in the vicinity of data points\nin order to achieve robustness.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 22:03:40 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Fawzi", "Alhussein", ""], ["Uesato", "Jonathan", ""], ["Frossard", "Pascal", ""]]}, {"id": "1811.09720", "submitter": "Chih-Kuan Yeh", "authors": "Chih-Kuan Yeh, Joon Sik Kim, Ian E.H. Yen, Pradeep Ravikumar", "title": "Representer Point Selection for Explaining Deep Neural Networks", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to explain the predictions of a deep neural network, by pointing\nto the set of what we call representer points in the training set, for a given\ntest point prediction. Specifically, we show that we can decompose the\npre-activation prediction of a neural network into a linear combination of\nactivations of training points, with the weights corresponding to what we call\nrepresenter values, which thus capture the importance of that training point on\nthe learned parameters of the network. But it provides a deeper understanding\nof the network than simply training point influence: with positive representer\nvalues corresponding to excitatory training points, and negative values\ncorresponding to inhibitory points, which as we show provides considerably more\ninsight. Our method is also much more scalable, allowing for real-time feedback\nin a manner not feasible with influence functions.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 22:34:17 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Yeh", "Chih-Kuan", ""], ["Kim", "Joon Sik", ""], ["Yen", "Ian E. H.", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1811.09724", "submitter": "Aditya Balu", "authors": "Rahul Singh, Aayush Sharma, Onur Rauf Bingol, Aditya Balu, Ganesh\n  Balasubramanian, Duane D. Johnson and Soumik Sarkar", "title": "3D Deep Learning with voxelized atomic configurations for modeling\n  atomistic potentials in complex solid-solution alloys", "comments": "Presenting in Machine Learning for Molecules and Materials NeurIPS\n  2018 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for advanced materials has led to the development of complex,\nmulti-component alloys or solid-solution alloys. These materials have shown\nexceptional properties like strength, toughness, ductility, electrical and\nelectronic properties. Current development of such material systems are\nhindered by expensive experiments and computationally demanding\nfirst-principles simulations. Atomistic simulations can provide reasonable\ninsights on properties in such material systems. However, the issue of\ndesigning robust potentials still exists. In this paper, we explore a deep\nconvolutional neural-network based approach to develop the atomistic potential\nfor such complex alloys to investigate materials for insights into controlling\nproperties. In the present work, we propose a voxel representation of the\natomic configuration of a cell and design a 3D convolutional neural network to\nlearn the interaction of the atoms. Our results highlight the performance of\nthe 3D convolutional neural network and its efficacy in machine-learning the\natomistic potential. We also explore the role of voxel resolution and provide\ninsights into the two bounding box methodologies implemented for voxelization.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 23:12:22 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Singh", "Rahul", ""], ["Sharma", "Aayush", ""], ["Bingol", "Onur Rauf", ""], ["Balu", "Aditya", ""], ["Balasubramanian", "Ganesh", ""], ["Johnson", "Duane D.", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1811.09725", "submitter": "Mirco Ravanelli", "authors": "Mirco Ravanelli, Yoshua Bengio", "title": "Interpretable Convolutional Filters with SincNet", "comments": "In Proceedings of NIPS@IRASL 2018. arXiv admin note: substantial text\n  overlap with arXiv:1808.00158", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is currently playing a crucial role toward higher levels of\nartificial intelligence. This paradigm allows neural networks to learn complex\nand abstract representations, that are progressively obtained by combining\nsimpler ones. Nevertheless, the internal \"black-box\" representations\nautomatically discovered by current neural architectures often suffer from a\nlack of interpretability, making of primary interest the study of explainable\nmachine learning techniques. This paper summarizes our recent efforts to\ndevelop a more interpretable neural model for directly processing speech from\nthe raw waveform. In particular, we propose SincNet, a novel Convolutional\nNeural Network (CNN) that encourages the first layer to discover more\nmeaningful filters by exploiting parametrized sinc functions. In contrast to\nstandard CNNs, which learn all the elements of each filter, only low and high\ncutoff frequencies of band-pass filters are directly learned from data. This\ninductive bias offers a very compact way to derive a customized filter-bank\nfront-end, that only depends on some parameters with a clear physical meaning.\nOur experiments, conducted on both speaker and speech recognition, show that\nthe proposed architecture converges faster, performs better, and is more\ninterpretable than standard CNNs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 23:13:09 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 16:09:38 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Ravanelli", "Mirco", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1811.09732", "submitter": "Abdul Dakkak", "authors": "Abdul Dakkak, Cheng Li, Simon Garcia de Gonzalo, Jinjun Xiong, Wen-mei\n  Hwu", "title": "TrIMS: Transparent and Isolated Model Sharing for Low Latency Deep\n  LearningInference in Function as a Service Environments", "comments": "In Proceedings CLOUD 2019", "journal-ref": null, "doi": "10.1109/CLOUD.2019.00067", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks (DNNs) have become core computation components within\nlow latency Function as a Service (FaaS) prediction pipelines: including image\nrecognition, object detection, natural language processing, speech synthesis,\nand personalized recommendation pipelines. Cloud computing, as the de-facto\nbackbone of modern computing infrastructure for both enterprise and consumer\napplications, has to be able to handle user-defined pipelines of diverse DNN\ninference workloads while maintaining isolation and latency guarantees, and\nminimizing resource waste. The current solution for guaranteeing isolation\nwithin FaaS is suboptimal -- suffering from \"cold start\" latency. A major cause\nof such inefficiency is the need to move large amount of model data within and\nacross servers. We propose TrIMS as a novel solution to address these issues.\nOur proposed solution consists of a persistent model store across the GPU, CPU,\nlocal storage, and cloud storage hierarchy, an efficient resource management\nlayer that provides isolation, and a succinct set of application APIs and\ncontainer technologies for easy and transparent integration with FaaS, Deep\nLearning (DL) frameworks, and user code. We demonstrate our solution by\ninterfacing TrIMS with the Apache MXNet framework and demonstrate up to 24x\nspeedup in latency for image classification models and up to 210x speedup for\nlarge models. We achieve up to 8x system throughput improvement.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 00:52:11 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Dakkak", "Abdul", ""], ["Li", "Cheng", ""], ["de Gonzalo", "Simon Garcia", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "1811.09735", "submitter": "Long Nguyen", "authors": "Sisheng Liang, Long Nguyen, Fang Jin", "title": "A Multi-variable Stacked Long-Short Term Memory Network for Wind Speed\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precisely forecasting wind speed is essential for wind power producers and\ngrid operators. However, this task is challenging due to the stochasticity of\nwind speed. To accurately predict short-term wind speed under uncertainties,\nthis paper proposed a multi-variable stacked LSTMs model (MSLSTM). The proposed\nmethod utilizes multiple historical meteorological variables, such as wind\nspeed, temperature, humidity, pressure, dew point and solar radiation to\naccurately predict wind speeds. The prediction performance is extensively\nassessed using real data collected in West Texas, USA. The experimental results\nshow that the proposed MSLSTM can preferably capture and learn uncertainties\nwhile output competitive performance.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 01:12:31 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Liang", "Sisheng", ""], ["Nguyen", "Long", ""], ["Jin", "Fang", ""]]}, {"id": "1811.09737", "submitter": "Abdul Dakkak", "authors": "Abdul Dakkak, Cheng Li, Jinjun Xiong, Wen-Mei Hwu", "title": "Frustrated with Replicating Claims of a Shared Model? A Solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine Learning (ML) and Deep Learning (DL) innovations are being introduced\nat such a rapid pace that model owners and evaluators are hard-pressed\nanalyzing and studying them. This is exacerbated by the complicated procedures\nfor evaluation. The lack of standard systems and efficient techniques for\nspecifying and provisioning ML/DL evaluation is the main cause of this \"pain\npoint\". This work discusses common pitfalls for replicating DL model\nevaluation, and shows that these subtle pitfalls can affect both accuracy and\nperformance. It then proposes a solution to remedy these pitfalls called\nMLModelScope, a specification for repeatable model evaluation and a runtime to\nprovision and measure experiments. We show that by easing the model\nspecification and evaluation process, MLModelScope facilitates rapid adoption\nof ML/DL innovations.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 01:18:00 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 17:15:01 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Dakkak", "Abdul", ""], ["Li", "Cheng", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-Mei", ""]]}, {"id": "1811.09740", "submitter": "Zhiting Hu", "authors": "Bowen Tan, Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, Eric Xing", "title": "Connecting the Dots Between MLE and RL for Sequence Prediction", "comments": "Major revision. The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence prediction models can be learned from example sequences with a\nvariety of training algorithms. Maximum likelihood learning is simple and\nefficient, yet can suffer from compounding error at test time. Reinforcement\nlearning such as policy gradient addresses the issue but can have prohibitively\npoor exploration efficiency. A rich set of other algorithms such as RAML, SPG,\nand data noising, have also been developed from different perspectives. This\npaper establishes a formal connection between these algorithms. We present a\ngeneralized entropy regularized policy optimization formulation, and show that\nthe apparently distinct algorithms can all be reformulated as special instances\nof the framework, with the only difference being the configurations of a reward\nfunction and a couple of hyperparameters. The unified interpretation offers a\nsystematic view of the varying properties of exploration and learning\nefficiency. Besides, inspired from the framework, we present a new algorithm\nthat dynamically interpolates among the family of algorithms for scheduled\nsequence model learning. Experiments on machine translation, text\nsummarization, and game imitation learning demonstrate the superiority of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 01:33:39 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2019 19:44:06 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Tan", "Bowen", ""], ["Hu", "Zhiting", ""], ["Yang", "Zichao", ""], ["Salakhutdinov", "Ruslan", ""], ["Xing", "Eric", ""]]}, {"id": "1811.09747", "submitter": "Ari Pakman", "authors": "Ari Pakman and Liam Paninski", "title": "Amortized Bayesian inference for clustering models", "comments": "Presented at BNP@NeurIPS 2018 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methods for efficient amortized approximate Bayesian inference\nover posterior distributions of probabilistic clustering models, such as\nDirichlet process mixture models. The approach is based on mapping distributed,\nsymmetry-invariant representations of cluster arrangements into conditional\nprobabilities. The method parallelizes easily, yields iid samples from the\napproximate posterior of cluster assignments with the same computational cost\nof a single Gibbs sampler sweep, and can easily be applied to both conjugate\nand non-conjugate models, as training only requires samples from the generative\nmodel.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 02:17:20 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Pakman", "Ari", ""], ["Paninski", "Liam", ""]]}, {"id": "1811.09751", "submitter": "Zirui Wang", "authors": "Zirui Wang, Zihang Dai, Barnab\\'as P\\'oczos, Jaime Carbonell", "title": "Characterizing and Avoiding Negative Transfer", "comments": "Published at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When labeled data is scarce for a specific target task, transfer learning\noften offers an effective solution by utilizing data from a related source\ntask. However, when transferring knowledge from a less related source, it may\ninversely hurt the target performance, a phenomenon known as negative transfer.\nDespite its pervasiveness, negative transfer is usually described in an\ninformal manner, lacking rigorous definition, careful analysis, or systematic\ntreatment. This paper proposes a formal definition of negative transfer and\nanalyzes three important aspects thereof. Stemming from this analysis, a novel\ntechnique is proposed to circumvent negative transfer by filtering out\nunrelated source data. Based on adversarial networks, the technique is highly\ngeneric and can be applied to a wide range of transfer learning algorithms. The\nproposed approach is evaluated on six state-of-the-art deep transfer methods\nvia experiments on four benchmark datasets with varying levels of difficulty.\nEmpirically, the proposed method consistently improves the performance of all\nbaseline methods and largely avoids negative transfer, even when the source\ndata is degenerate.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 03:26:10 GMT"}, {"version": "v2", "created": "Sun, 31 Mar 2019 21:31:51 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2019 02:05:54 GMT"}, {"version": "v4", "created": "Sat, 5 Oct 2019 03:43:52 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Wang", "Zirui", ""], ["Dai", "Zihang", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Carbonell", "Jaime", ""]]}, {"id": "1811.09755", "submitter": "Xinzhi Wang", "authors": "Xinzhi Wang, Shengcheng Yuan, Hui Zhang, Yi Liu", "title": "Estimation of Inter-Sentiment Correlations Employing Deep Neural Network\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on sentiment mining and sentiment correlation analysis of\nweb events. Although neural network models have contributed a lot to mining\ntext information, little attention is paid to analysis of the inter-sentiment\ncorrelations. This paper fills the gap between sentiment calculation and\ninter-sentiment correlations. In this paper, the social emotion is divided into\nsix categories: love, joy, anger, sadness, fear, and surprise. Two deep neural\nnetwork models are presented for sentiment calculation. Three datasets - the\ntitles, the bodies, the comments of news articles - are collected, covering\nboth objective and subjective texts in varying lengths (long and short). From\neach dataset, three kinds of features are extracted: explicit expression,\nimplicit expression, and alphabet characters. The performance of the two models\nare analyzed, with respect to each of the three kinds of the features. There is\ncontroversial phenomenon on the interpretation of anger (fn) and love (gd). In\nsubjective text, other emotions are easily to be considered as anger. By\ncontrast, in objective news bodies and titles, it is easy to regard text as\ncaused love (gd). It means, journalist may want to arouse emotion love by\nwriting news, but cause anger after the news is published. This result reflects\nthe sentiment complexity and unpredictability.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 03:47:04 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Wang", "Xinzhi", ""], ["Yuan", "Shengcheng", ""], ["Zhang", "Hui", ""], ["Liu", "Yi", ""]]}, {"id": "1811.09757", "submitter": "Xiu Yang", "authors": "Xiu Yang and David Barajas-Solano and Guzel Tartakovsky and Alexandre\n  Tartakovsky", "title": "Physics-Informed CoKriging: A Gaussian-Process-Regression-Based\n  Multifidelity Method for Data-Model Convergence", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2019.06.041", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new Gaussian process regression (GPR)-based\nmultifidelity method: physics-informed CoKriging (CoPhIK). In CoKriging-based\nmultifidelity methods, the quantities of interest are modeled as linear\ncombinations of multiple parameterized stationary Gaussian processes (GPs), and\nthe hyperparameters of these GPs are estimated from data via optimization. In\nCoPhIK, we construct a GP representing low-fidelity data using physics-informed\nKriging (PhIK), and model the discrepancy between low- and high-fidelity data\nusing a parameterized GP with hyperparameters identified via optimization. Our\napproach reduces the cost of optimization for inferring hyperparameters by\nincorporating partial physical knowledge. We prove that the physical\nconstraints in the form of deterministic linear operators are satisfied up to\nan error bound. Furthermore, we combine CoPhIK with a greedy active learning\nalgorithm for guiding the selection of additional observation locations. The\nefficiency and accuracy of CoPhIK are demonstrated for reconstructing the\npartially observed modified Branin function, reconstructing the sparsely\nobserved state of a steady state heat transport problem, and learning a\nconservative tracer distribution from sparse tracer concentration measurements.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 03:56:10 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Yang", "Xiu", ""], ["Barajas-Solano", "David", ""], ["Tartakovsky", "Guzel", ""], ["Tartakovsky", "Alexandre", ""]]}, {"id": "1811.09759", "submitter": "Minhae Kwon", "authors": "Minhae Kwon and Juhyeon Lee and Hyunggon Park", "title": "Learning to Activate Relay Nodes: Deep Reinforcement Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a distributed solution to design a multi-hop ad hoc\nnetwork where mobile relay nodes strategically determine their wireless\ntransmission ranges based on a deep reinforcement learning approach. We\nconsider scenarios where only a limited networking infrastructure is available\nbut a large number of wireless mobile relay nodes are deployed in building a\nmulti-hop ad hoc network to deliver source data to the destination. A mobile\nrelay node is considered as a decision-making agent that strategically\ndetermines its transmission range in a way that maximizes network throughput\nwhile minimizing the corresponding transmission power consumption. Each relay\nnode collects information from its partial observations and learns its\nenvironment through a sequence of experiences. Hence, the proposed solution\nrequires only a minimal amount of information from the system. We show that the\nactions that the relay nodes take from its policy are determined as to activate\nor inactivate its transmission, i.e., only necessary relay nodes are activated\nwith the maximum transmit power, and nonessential nodes are deactivated to\nminimize power consumption. Using extensive experiments, we confirm that the\nproposed solution builds a network with higher network performance than current\nstate-of-the-art solutions in terms of system goodput and connectivity ratio.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 04:02:55 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Kwon", "Minhae", ""], ["Lee", "Juhyeon", ""], ["Park", "Hyunggon", ""]]}, {"id": "1811.09763", "submitter": "Pak Lun Kevin Ding", "authors": "Pak Lun Kevin Ding, Yikang Li, Baoxin Li", "title": "Mean Local Group Average Precision (mLGAP): A New Performance Metric for\n  Hashing-based Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research on hashing techniques for visual data is gaining increased\nattention in recent years due to the need for compact representations\nsupporting efficient search/retrieval in large-scale databases such as online\nimages. Among many possibilities, Mean Average Precision(mAP) has emerged as\nthe dominant performance metric for hashing-based retrieval. One glaring\nshortcoming of mAP is its inability in balancing retrieval accuracy and\nutilization of hash codes: pushing a system to attain higher mAP will\ninevitably lead to poorer utilization of the hash codes. Poor utilization of\nthe hash codes hinders good retrieval because of increased collision of samples\nin the hash space. This means that a model giving a higher mAP values does not\nnecessarily do a better job in retrieval. In this paper, we introduce a new\nmetric named Mean Local Group Average Precision (mLGAP) for better evaluation\nof the performance of hashing-based retrieval. The new metric provides a\nretrieval performance measure that also reconciles the utilization of hash\ncodes, leading to a more practically meaningful performance metric than\nconventional ones like mAP. To this end, we start by mathematical analysis of\nthe deficiencies of mAP for hashing-based retrieval. We then propose mLGAP and\nshow why it is more appropriate for hashing-based retrieval. Experiments on\nimage retrieval are used to demonstrate the effectiveness of the proposed\nmetric.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 04:31:41 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Ding", "Pak Lun Kevin", ""], ["Li", "Yikang", ""], ["Li", "Baoxin", ""]]}, {"id": "1811.09766", "submitter": "Rim Assouel", "authors": "Rim Assouel, Mohamed Ahmed, Marwin H Segler, Amir Saffari, Yoshua\n  Bengio", "title": "DEFactor: Differentiable Edge Factorization-based Probabilistic Graph\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating novel molecules with optimal properties is a crucial step in many\nindustries such as drug discovery. Recently, deep generative models have shown\na promising way of performing de-novo molecular design. Although graph\ngenerative models are currently available they either have a graph size\ndependency in their number of parameters, limiting their use to only very small\ngraphs or are formulated as a sequence of discrete actions needed to construct\na graph, making the output graph non-differentiable w.r.t the model parameters,\ntherefore preventing them to be used in scenarios such as conditional graph\ngeneration. In this work we propose a model for conditional graph generation\nthat is computationally efficient and enables direct optimisation of the graph.\nWe demonstrate favourable performance of our model on prototype-based molecular\ngraph conditional generation tasks.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 05:23:39 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Assouel", "Rim", ""], ["Ahmed", "Mohamed", ""], ["Segler", "Marwin H", ""], ["Saffari", "Amir", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1811.09782", "submitter": "Sabri Boughorbel", "authors": "Sabri Boughorbel, Fethi Jarray, Neethu Venugopal, Haithum Elhadi", "title": "Alternating Loss Correction for Preterm-Birth Prediction from EHR Data\n  with Noisy Labels", "comments": "Submission Id: 79, Machine Learning for Health (ML4H) Workshop at\n  NeurIPS 2018 arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/79", "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are interested in the prediction of preterm birth based on\ndiagnosis codes from longitudinal EHR. We formulate the prediction problem as a\nsupervised classification with noisy labels. Our base classifier is a Recurrent\nNeural Network with an attention mechanism. We assume the availability of a\ndata subset with both noisy and clean labels. For the cohort definition, most\nof the diagnosis codes on mothers' records related to pregnancy are ambiguous\nfor the definition of full-term and preterm classes. On the other hand,\ndiagnosis codes on babies' records provide fine-grained information on\nprematurity. Due to data de-identification, the links between mothers and\nbabies are not available. We developed a heuristic based on admission and\ndischarge times to match babies to their mothers and hence enrich mothers'\nrecords with additional information on delivery status. The obtained additional\ndataset from the matching heuristic has noisy labels and was used to leverage\nthe training of the deep learning model. We propose an Alternating Loss\nCorrection (ALC) method to train deep models with both clean and noisy labels.\nFirst, the label corruption matrix is estimated using the data subset with both\nnoisy and clean labels. Then it is used in the model as a dense output layer to\ncorrect for the label noise. The network is alternately trained on epochs with\nthe clean dataset with a simple cross-entropy loss and on next epoch with the\nnoisy dataset and a loss corrected with the estimated corruption matrix. The\nexperiments for the prediction of preterm birth at 90 days before delivery\nshowed an improvement in performance compared with baseline and state\nof-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 07:47:01 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Boughorbel", "Sabri", ""], ["Jarray", "Fethi", ""], ["Venugopal", "Neethu", ""], ["Elhadi", "Haithum", ""]]}, {"id": "1811.09794", "submitter": "Hyeoncheol Cho", "authors": "Hyeoncheol Cho, Insung S. Choi", "title": "Three-Dimensionally Embedded Graph Convolutional Network (3DGCN) for\n  Molecule Interpretation", "comments": "39 pages, 14 figures, 5 tables", "journal-ref": "ChemMedChem, 2019", "doi": "10.1002/cmdc.201900458", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a three-dimensional graph convolutional network (3DGCN), which\npredicts molecular properties and biochemical activities, based on 3D molecular\ngraph. In the 3DGCN, graph convolution is unified with learning operations on\nthe vector to handle the spatial information from molecular topology. The 3DGCN\nmodel exhibits significantly higher performance on various tasks compared with\nother deep-learning models, and has the ability of generalizing a given\nconformer to targeted features regardless of its rotations in the 3D space.\nMore significantly, our model also can distinguish the 3D rotations of a\nmolecule and predict the target value, depending upon the rotation degree, in\nthe protein-ligand docking problem, when trained with orientation-dependent\ndatasets. The rotation distinguishability of 3DGCN, along with rotation\nequivariance, provides a key milestone in the implementation of\nthree-dimensionality to the field of deep-learning chemistry that solves\nchallenging biochemical problems.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 08:57:26 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 04:22:08 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 11:42:23 GMT"}, {"version": "v4", "created": "Tue, 16 Apr 2019 05:39:57 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Cho", "Hyeoncheol", ""], ["Choi", "Insung S.", ""]]}, {"id": "1811.09801", "submitter": "Chuan Zhang", "authors": "Weihong Xu (1 and 2), Xiaohu You (2), Chuan Zhang (1 and 2), and Yair\n  Be'ery (3) ((1) Lab of Efficient Architectures for Digital-communication and\n  Signal-processing (LEADS), (2) National Mobile Communications Research\n  Laboratory, (3) School of Electrical Engineering, Tel-Aviv University,\n  Tel-Aviv, Israel)", "title": "Polar Decoding on Sparse Graphs with Deep Learning", "comments": "submitted to Asilomar 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a sparse neural network decoder (SNND) of polar\ncodes based on belief propagation (BP) and deep learning. At first, the\nconventional factor graph of polar BP decoding is converted to the bipartite\nTanner graph similar to low-density parity-check (LDPC) codes. Then the Tanner\ngraph is unfolded and translated into the graphical representation of deep\nneural network (DNN). The complex sum-product algorithm (SPA) is modified to\nmin-sum (MS) approximation with low complexity. We dramatically reduce the\nnumber of weight by using single weight to parameterize the networks. Optimized\nby the training techniques of deep learning, proposed SNND achieves comparative\ndecoding performance of SPA and obtains about $0.5$ dB gain over MS decoding on\n($128,64$) and ($256,128$) codes. Moreover, $60 \\%$ complexity reduction is\nachieved and the decoding latency is significantly lower than the conventional\npolar BP.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 09:50:23 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Xu", "Weihong", "", "1 and 2"], ["You", "Xiaohu", "", "1 and 2"], ["Zhang", "Chuan", "", "1 and 2"], ["Be'ery", "Yair", ""]]}, {"id": "1811.09813", "submitter": "Aditya Grover", "authors": "Aditya Grover, Tudor Achim, Stefano Ermon", "title": "Streamlining Variational Inference for Constraint Satisfaction Problems", "comments": "NeurIPS 2018", "journal-ref": null, "doi": "10.1088/1742-5468/ab371f", "report-no": null, "categories": "cs.AI cs.LG cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several algorithms for solving constraint satisfaction problems are based on\nsurvey propagation, a variational inference scheme used to obtain approximate\nmarginal probability estimates for variable assignments. These marginals\ncorrespond to how frequently each variable is set to true among satisfying\nassignments, and are used to inform branching decisions during search; however,\nmarginal estimates obtained via survey propagation are approximate and can be\nself-contradictory. We introduce a more general branching strategy based on\nstreamlining constraints, which sidestep hard assignments to variables. We show\nthat streamlined solvers consistently outperform decimation-based solvers on\nrandom k-SAT instances for several problem sizes, shrinking the gap between\nempirical performance and theoretical limits of satisfiability by 16.3% on\naverage for k=3,4,5,6.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 11:08:14 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Grover", "Aditya", ""], ["Achim", "Tudor", ""], ["Ermon", "Stefano", ""]]}, {"id": "1811.09828", "submitter": "Krzysztof Maziarz", "authors": "Krzysztof Maziarz, Mingxing Tan, Andrey Khorlin, Marin Georgiev,\n  Andrea Gesmundo", "title": "Evolutionary-Neural Hybrid Agents for Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search has shown potential to automate the design of\nneural networks. Deep Reinforcement Learning based agents can learn complex\narchitectural patterns, as well as explore a vast and compositional search\nspace. On the other hand, evolutionary algorithms offer higher sample\nefficiency, which is critical for such a resource intensive application. In\norder to capture the best of both worlds, we propose a class of\nEvolutionary-Neural hybrid agents (Evo-NAS). We show that the Evo-NAS agent\noutperforms both neural and evolutionary agents when applied to architecture\nsearch for a suite of text and image classification benchmarks. On a\nhigh-complexity architecture search space for image classification, the Evo-NAS\nagent surpasses the accuracy achieved by commonly used agents with only 1/3 of\nthe search cost.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 13:00:47 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 16:05:51 GMT"}, {"version": "v3", "created": "Tue, 4 Jun 2019 15:27:38 GMT"}, {"version": "v4", "created": "Sat, 15 Feb 2020 13:25:42 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Maziarz", "Krzysztof", ""], ["Tan", "Mingxing", ""], ["Khorlin", "Andrey", ""], ["Georgiev", "Marin", ""], ["Gesmundo", "Andrea", ""]]}, {"id": "1811.09834", "submitter": "Yongxi Lu", "authors": "Ziyao Tang, Yongxi Lu and Tara Javidi", "title": "Efficient Video Understanding via Layered Multi Frame-Rate Analysis", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the greatest challenges in the design of a real-time perception system\nfor autonomous driving vehicles and drones is the conflicting requirement of\nsafety (high prediction accuracy) and efficiency. Traditional approaches use a\nsingle frame rate for the entire system. Motivated by the observation that the\nlack of robustness against environmental factors is the major weakness of\ncompact ConvNet architectures, we propose a dual frame-rate system that brings\nin the best of both worlds: A modulator stream that executes an expensive\nmodels robust to environmental factors at a low frame rate to extract slowly\nchanging features describing the environment, and a prediction stream that\nexecutes a light-weight model at real-time to extract transient signals that\ndescribes particularities of the current frame. The advantage of our design is\nvalidated by our extensive empirical study, showing that our solution leads to\nconsistent improvements using a variety of backbone architecture choice and\ninput resolutions. These findings suggest multiple frame-rate systems as a\npromising direction in designing efficient perception for autonomous agents.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 13:43:34 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Tang", "Ziyao", ""], ["Lu", "Yongxi", ""], ["Javidi", "Tara", ""]]}, {"id": "1811.09842", "submitter": "Guozhu Dong", "authors": "Guozhu Dong and Sai Kiran Pentukar", "title": "OCLEP+: One-class Anomaly and Intrusion Detection Using Minimal Length\n  of Emerging Patterns", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method called One-class Classification using Length\nstatistics of Emerging Patterns Plus (OCLEP+).\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 14:21:50 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Dong", "Guozhu", ""], ["Pentukar", "Sai Kiran", ""]]}, {"id": "1811.09862", "submitter": "Maxim Naumov", "authors": "Maxim Naumov and Utku Diril and Jongsoo Park and Benjamin Ray and\n  Jedrzej Jablonski and Andrew Tulloch", "title": "On Periodic Functions as Regularizers for Quantization of Neural\n  Networks", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have been successfully used in computer vision and many\nother fields. We propose an unorthodox algorithm for performing quantization of\nthe model parameters. In contrast with popular quantization schemes based on\nthresholds, we use a novel technique based on periodic functions, such as\ncontinuous trigonometric sine or cosine as well as non-continuous hat\nfunctions. We apply these functions component-wise and add the sum over the\nmodel parameters as a regularizer to the model loss during training. The\nfrequency and amplitude hyper-parameters of these functions can be adjusted\nduring training. The regularization pushes the weights into discrete points\nthat can be encoded as integers. We show that using this technique the\nresulting quantized models exhibit the same accuracy as the original ones on\nCIFAR-10 and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 17:24:28 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Naumov", "Maxim", ""], ["Diril", "Utku", ""], ["Park", "Jongsoo", ""], ["Ray", "Benjamin", ""], ["Jablonski", "Jedrzej", ""], ["Tulloch", "Andrew", ""]]}, {"id": "1811.09863", "submitter": "Anton Belyy", "authors": "Anton Belyy, Aleksei Sholokhov", "title": "MEMOIR: Multi-class Extreme Classification with Inexact Margin", "comments": "11 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-class classification with a very large number of classes, or extreme\nclassification, is a challenging problem from both statistical and\ncomputational perspectives. Most of the classical approaches to multi-class\nclassification, including one-vs-rest or multi-class support vector machines,\nrequire the exact estimation of the classifier's margin, at both the training\nand the prediction steps making them intractable in extreme classification\nscenarios. In this paper, we study the impact of computing an approximate\nmargin using nearest neighbor (ANN) search structures combined with\nlocality-sensitive hashing (LSH). This approximation allows to dramatically\nreduce both the training and the prediction time without a significant loss in\nperformance. We theoretically prove that this approximation does not lead to a\nsignificant loss of the risk of the model and provide empirical evidence over\nfive publicly available large scale datasets, showing that the proposed\napproach is highly competitive with respect to state-of-the-art approaches on\ntime, memory and performance measures.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 17:26:20 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 22:08:23 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Belyy", "Anton", ""], ["Sholokhov", "Aleksei", ""]]}, {"id": "1811.09864", "submitter": "Tao Chen", "authors": "Tao Chen, Adithyavairavan Murali, Abhinav Gupta", "title": "Hardware Conditioned Policies for Multi-Robot Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning could be used to learn dexterous robotic policies\nbut it is challenging to transfer them to new robots with vastly different\nhardware properties. It is also prohibitively expensive to learn a new policy\nfrom scratch for each robot hardware due to the high sample complexity of\nmodern state-of-the-art algorithms. We propose a novel approach called\n\\textit{Hardware Conditioned Policies} where we train a universal policy\nconditioned on a vector representation of robot hardware. We considered robots\nin simulation with varied dynamics, kinematic structure, kinematic lengths and\ndegrees-of-freedom. First, we use the kinematic structure directly as the\nhardware encoding and show great zero-shot transfer to completely novel robots\nnot seen during training. For robots with lower zero-shot success rate, we also\ndemonstrate that fine-tuning the policy network is significantly more\nsample-efficient than training a model from scratch. In tasks where knowing the\nagent dynamics is important for success, we learn an embedding for robot\nhardware and show that policies conditioned on the encoding of hardware tend to\ngeneralize and transfer well. The code and videos are available on the project\nwebpage: https://sites.google.com/view/robot-transfer-hcp.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 17:29:11 GMT"}, {"version": "v2", "created": "Sun, 13 Jan 2019 03:19:49 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Chen", "Tao", ""], ["Murali", "Adithyavairavan", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1811.09878", "submitter": "Karanbir Chahal", "authors": "Vaibhav Mathur and Karanbir Chahal", "title": "Hydra: A Peer to Peer Distributed Training & Data Collection Framework", "comments": "10 pages. arXiv admin note: text overlap with arXiv:1611.01578 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world needs diverse and unbiased data to train deep learning models.\nCurrently data comes from a variety of sources that are unmoderated to a large\nextent. The outcomes of training neural networks with unverified data yields\nbiased models with various strains of homophobia, sexism and racism. Another\ntrend observed in the world of deep learning is the rise of distributed\ntraining. Although cloud companies provide high performance compute for\ntraining models in the form of GPU's connected with a low latency network,\nusing these services comes at a high cost. We propose Hydra, a system that\nseeks to solve both of these problems in a novel manner by proposing a\ndecentralized distributed framework which utilizes the substantial amount of\nidle compute of everyday electronic devices like smartphones and desktop\ncomputers for training and data collection purposes. Hydra couples a\nspecialized distributed training framework on a network of these low powered\ndevices with a reward scheme that incentivizes users to provide high quality\ndata to unleash the compute capability on this training framework. Such a\nsystem has the ability to capture data from a wide variety of diverse sources\nwhich has been an issue in the current scenario of deep learning. Hydra brings\nin several new innovations in training on low powered devices including a fault\ntolerant version of the All Reduce algorithm. Furthermore we introduce a\nreinforcement learning policy to decide the size of training jobs on different\nmachines on a heterogeneous cluster of devices with varying network latencies\nfor Synchronous SGD. The novel thing about such a network is the ability of\neach machine to shut down and resume training capabilities at any point of time\nwithout restarting the overall training. To enable such an asynchronous\nbehaviour we propose a communication framework inspired by the Bittorrent\nprotocol and the Kademlia DHT.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 19:11:41 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Mathur", "Vaibhav", ""], ["Chahal", "Karanbir", ""]]}, {"id": "1811.09886", "submitter": "Jongsoo Park", "authors": "Jongsoo Park, Maxim Naumov, Protonu Basu, Summer Deng, Aravind\n  Kalaiah, Daya Khudia, James Law, Parth Malani, Andrey Malevich, Satish\n  Nadathur, Juan Pino, Martin Schatz, Alexander Sidorov, Viswanath Sivakumar,\n  Andrew Tulloch, Xiaodong Wang, Yiming Wu, Hector Yuen, Utku Diril, Dmytro\n  Dzhulgakov, Kim Hazelwood, Bill Jia, Yangqing Jia, Lin Qiao, Vijay Rao, Nadav\n  Rotem, Sungjoo Yoo, Mikhail Smelyanskiy", "title": "Deep Learning Inference in Facebook Data Centers: Characterization,\n  Performance Optimizations and Hardware Implications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of deep learning techniques resulted in remarkable\nimprovement of machine learning models. In this paper provides detailed\ncharacterizations of deep learning models used in many Facebook social network\nservices. We present computational characteristics of our models, describe high\nperformance optimizations targeting existing systems, point out their\nlimitations and make suggestions for the future general-purpose/accelerated\ninference hardware. Also, we highlight the need for better co-design of\nalgorithms, numerics and computing platforms to address the challenges of\nworkloads often run in data centers.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 19:52:02 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 07:25:11 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Park", "Jongsoo", ""], ["Naumov", "Maxim", ""], ["Basu", "Protonu", ""], ["Deng", "Summer", ""], ["Kalaiah", "Aravind", ""], ["Khudia", "Daya", ""], ["Law", "James", ""], ["Malani", "Parth", ""], ["Malevich", "Andrey", ""], ["Nadathur", "Satish", ""], ["Pino", "Juan", ""], ["Schatz", "Martin", ""], ["Sidorov", "Alexander", ""], ["Sivakumar", "Viswanath", ""], ["Tulloch", "Andrew", ""], ["Wang", "Xiaodong", ""], ["Wu", "Yiming", ""], ["Yuen", "Hector", ""], ["Diril", "Utku", ""], ["Dzhulgakov", "Dmytro", ""], ["Hazelwood", "Kim", ""], ["Jia", "Bill", ""], ["Jia", "Yangqing", ""], ["Qiao", "Lin", ""], ["Rao", "Vijay", ""], ["Rotem", "Nadav", ""], ["Yoo", "Sungjoo", ""], ["Smelyanskiy", "Mikhail", ""]]}, {"id": "1811.09904", "submitter": "Muhammad Shayan", "authors": "Muhammad Shayan, Clement Fung, Chris J.M. Yoon, Ivan Beschastnikh", "title": "Biscotti: A Ledger for Private and Secure Peer-to-Peer Machine Learning", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning is the current state of the art in supporting secure\nmulti-party machine learning (ML): data is maintained on the owner's device and\nthe updates to the model are aggregated through a secure protocol. However,\nthis process assumes a trusted centralized infrastructure for coordination, and\nclients must trust that the central service does not use the byproducts of\nclient data. In addition to this, a group of malicious clients could also harm\nthe performance of the model by carrying out a poisoning attack.\n  As a response, we propose Biscotti: a fully decentralized peer to peer (P2P)\napproach to multi-party ML, which uses blockchain and cryptographic primitives\nto coordinate a privacy-preserving ML process between peering clients. Our\nevaluation demonstrates that Biscotti is scalable, fault tolerant, and defends\nagainst known attacks. For example, Biscotti is able to protect the privacy of\nan individual client's update and the performance of the global model at scale\nwhen 30% of adversaries are trying to poison the model.\n  The implementation can be found at: https://github.com/DistributedML/Biscotti\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 22:24:38 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 21:39:04 GMT"}, {"version": "v3", "created": "Sat, 23 Feb 2019 01:40:22 GMT"}, {"version": "v4", "created": "Thu, 12 Dec 2019 04:29:53 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Shayan", "Muhammad", ""], ["Fung", "Clement", ""], ["Yoon", "Chris J. M.", ""], ["Beschastnikh", "Ivan", ""]]}, {"id": "1811.09919", "submitter": "Saturnino Luz", "authors": "Saturnino Luz, Sofia de la Fuente, Pierre Albert", "title": "A Method for Analysis of Patient Speech in Dialogue for Dementia\n  Detection", "comments": "8 pages, Resources and ProcessIng of linguistic, paralinguistic and\n  extra-linguistic Data from people with various forms of cognitive impairment,\n  LREC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an approach to automatic detection of Alzheimer's type dementia\nbased on characteristics of spontaneous spoken language dialogue consisting of\ninterviews recorded in natural settings. The proposed method employs additive\nlogistic regression (a machine learning boosting method) on content-free\nfeatures extracted from dialogical interaction to build a predictive model. The\nmodel training data consisted of 21 dialogues between patients with Alzheimer's\nand interviewers, and 17 dialogues between patients with other health\nconditions and interviewers. Features analysed included speech rate,\nturn-taking patterns and other speech parameters. Despite relying solely on\ncontent-free features, our method obtains overall accuracy of 86.5\\%, a result\ncomparable to those of state-of-the-art methods that employ more complex\nlexical, syntactic and semantic features. While further investigation is\nneeded, the fact that we were able to obtain promising results using only\nfeatures that can be easily extracted from spontaneous dialogues suggests the\npossibility of designing non-invasive and low-cost mental health monitoring\ntools for use at scale.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 01:30:16 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Luz", "Saturnino", ""], ["de la Fuente", "Sofia", ""], ["Albert", "Pierre", ""]]}, {"id": "1811.09923", "submitter": "Ido Nachum", "authors": "Ido Nachum and Amir Yehudayoff", "title": "Average-Case Information Complexity of Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How many bits of information are revealed by a learning algorithm for a\nconcept class of VC-dimension $d$? Previous works have shown that even for\n$d=1$ the amount of information may be unbounded (tend to $\\infty$ with the\nuniverse size). Can it be that all concepts in the class require leaking a\nlarge amount of information? We show that typically concepts do not require\nleakage. There exists a proper learning algorithm that reveals $O(d)$ bits of\ninformation for most concepts in the class. This result is a special case of a\nmore general phenomenon we explore. If there is a low information learner when\nthe algorithm {\\em knows} the underlying distribution on inputs, then there is\na learner that reveals little information on an average concept {\\em without\nknowing} the distribution on inputs.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 01:47:24 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Nachum", "Ido", ""], ["Yehudayoff", "Amir", ""]]}, {"id": "1811.09955", "submitter": "Binbin Liu", "authors": "Binbin Liu, Jundong Li, Yunquan Song, Xijun Liang, Ling Jian and Huan\n  Liu", "title": "Online Newton Step Algorithm with Estimated Gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning with limited information feedback (bandit) tries to solve the\nproblem where an online learner receives partial feedback information from the\nenvironment in the course of learning. Under this setting, Flaxman et al.[8]\nextended Zinkevich's classical Online Gradient Descent (OGD) algorithm [29] by\nproposing the Online Gradient Descent with Expected Gradient (OGDEG) algorithm.\nSpecifically, it uses a simple trick to approximate the gradient of the loss\nfunction $f_t$ by evaluating it at a single point and bounds the expected\nregret as $\\mathcal{O}(T^{5/6})$ [8], where the number of rounds is $T$.\nMeanwhile, past research efforts have shown that compared with the first-order\nalgorithms, second-order online learning algorithms such as Online Newton Step\n(ONS) [11] can significantly accelerate the convergence rate of traditional\nonline learning algorithms. Motivated by this, this paper aims to exploit the\nsecond-order information to speed up the convergence of the OGDEG algorithm. In\nparticular, we extend the ONS algorithm with the trick of expected gradient and\ndevelop a novel second-order online learning algorithm, i.e., Online Newton\nStep with Expected Gradient (ONSEG). Theoretically, we show that the proposed\nONSEG algorithm significantly reduces the expected regret of OGDEG algorithm\nfrom $\\mathcal{O}(T^{5/6})$ to $\\mathcal{O}(T^{2/3})$ in the bandit feedback\nscenario. Empirically, we further demonstrate the advantages of the proposed\nalgorithm on multiple real-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 05:58:57 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 06:14:05 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 02:01:45 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Liu", "Binbin", ""], ["Li", "Jundong", ""], ["Song", "Yunquan", ""], ["Liang", "Xijun", ""], ["Jian", "Ling", ""], ["Liu", "Huan", ""]]}, {"id": "1811.09956", "submitter": "Gurunath Reddy M", "authors": "Gurunath Reddy M, Tanumay Mandal, Krothapalli Sreenivasa Rao", "title": "Glottal Closure Instants Detection From Pathological Acoustic Speech\n  Signal Using Deep Learning", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/39", "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a classification based glottal closure instants\n(GCI) detection from pathological acoustic speech signal, which finds many\napplications in vocal disorder analysis. Till date, GCI for pathological\ndisorder is extracted from laryngeal (glottal source) signal recorded from\nElectroglottograph, a dedicated device designed to measure the vocal folds\nvibration around the larynx. We have created a pathological dataset which\nconsists of simultaneous recordings of glottal source and acoustic speech\nsignal of six different disorders from vocal disordered patients. The GCI\nlocations are manually annotated for disorder analysis and supervised learning.\nWe have proposed convolutional neural network based GCI detection method by\nfusing deep acoustic speech and linear prediction residual features for robust\nGCI detection. The experimental results showed that the proposed method is\nsignificantly better than the state-of-the-art GCI detection methods.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 06:18:24 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["M", "Gurunath Reddy", ""], ["Mandal", "Tanumay", ""], ["Rao", "Krothapalli Sreenivasa", ""]]}, {"id": "1811.09960", "submitter": "Jack Fitzsimons", "authors": "Jack Fitzsimons, Michael Osborne and Stephen Roberts", "title": "Intersectionality: Multiple Group Fairness in Expectation Constraints", "comments": "NeurIPS (previously NIPS) 2018, Workshop on Ethical, Social and\n  Governance Issues in AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group fairness is an important concern for machine learning researchers,\ndevelopers, and regulators. However, the strictness to which models must be\nconstrained to be considered fair is still under debate. The focus of this work\nis on constraining the expected outcome of subpopulations in kernel regression\nand, in particular, decision tree regression, with application to random\nforests, boosted trees and other ensemble models. While individual constraints\nwere previously addressed, this work addresses concerns about incorporating\nmultiple constraints simultaneously. The proposed solution does not affect the\norder of computational or memory complexity of the decision trees and is easily\nintegrated into models post training.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 06:31:13 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Fitzsimons", "Jack", ""], ["Osborne", "Michael", ""], ["Roberts", "Stephen", ""]]}, {"id": "1811.09975", "submitter": "Giuseppe Manco", "authors": "Noveen Sachdeva, Giuseppe Manco, Ettore Ritacco, Vikram Pudi", "title": "Sequential Variational Autoencoders for Collaborative Filtering", "comments": "9 pages, 6 figures, 2 tables, WSDM2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational autoencoders were proven successful in domains such as computer\nvision and speech processing. Their adoption for modeling user preferences is\nstill unexplored, although recently it is starting to gain attention in the\ncurrent literature. In this work, we propose a model which extends variational\nautoencoders by exploiting the rich information present in the past preference\nhistory. We introduce a recurrent version of the VAE, where instead of passing\na subset of the whole history regardless of temporal dependencies, we rather\npass the consumption sequence subset through a recurrent neural network. At\neach time-step of the RNN, the sequence is fed through a series of\nfully-connected layers, the output of which models the probability distribution\nof the most likely future preferences. We show that handling temporal\ninformation is crucial for improving the accuracy of the VAE: In fact, our\nmodel beats the current state-of-the-art by valuable margins because of its\nability to capture temporal dependencies among the user-consumption sequence\nusing the recurrent encoder still keeping the fundamentals of variational\nautoencoders intact.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 09:19:18 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Sachdeva", "Noveen", ""], ["Manco", "Giuseppe", ""], ["Ritacco", "Ettore", ""], ["Pudi", "Vikram", ""]]}, {"id": "1811.09977", "submitter": "Junzi Zhang", "authors": "Andrea Zanette, Junzi Zhang, Mykel J. Kochenderfer", "title": "Robust Super-Level Set Estimation using Gaussian Processes", "comments": "Accepted to ECML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the problem of determining as large a region as\npossible where a function exceeds a given threshold with high probability. We\nassume that we only have access to a noise-corrupted version of the function\nand that function evaluations are costly. To select the next query point, we\npropose maximizing the expected volume of the domain identified as above the\nthreshold as predicted by a Gaussian process, robustified by a variance term.\nWe also give asymptotic guarantees on the exploration effect of the algorithm,\nregardless of the prior misspecification. We show by various numerical examples\nthat our approach also outperforms existing techniques in the literature in\npractice.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 09:38:43 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Zanette", "Andrea", ""], ["Zhang", "Junzi", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "1811.09982", "submitter": "Battista Biggio", "authors": "Battista Biggio, Ignazio Pillai, Samuel Rota Bul\\`o, Davide Ariu,\n  Marcello Pelillo, Fabio Roli", "title": "Is Data Clustering in Adversarial Settings Secure?", "comments": null, "journal-ref": "Proceedings of the 2013 ACM Workshop on Artificial Intelligence\n  and Security, AISec '13, pages 87-98, New York, NY, USA, 2013. ACM", "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering algorithms have been increasingly adopted in security applications\nto spot dangerous or illicit activities. However, they have not been originally\ndevised to deal with deliberate attack attempts that may aim to subvert the\nclustering process itself. Whether clustering can be safely adopted in such\nsettings remains thus questionable. In this work we propose a general framework\nthat allows one to identify potential attacks against clustering algorithms,\nand to evaluate their impact, by making specific assumptions on the adversary's\ngoal, knowledge of the attacked system, and capabilities of manipulating the\ninput data. We show that an attacker may significantly poison the whole\nclustering process by adding a relatively small percentage of attack samples to\nthe input data, and that some attack samples may be obfuscated to be hidden\nwithin some existing clusters. We present a case study on single-linkage\nhierarchical clustering, and report experiments on clustering of malware\nsamples and handwritten digits.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 10:21:59 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Biggio", "Battista", ""], ["Pillai", "Ignazio", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Ariu", "Davide", ""], ["Pelillo", "Marcello", ""], ["Roli", "Fabio", ""]]}, {"id": "1811.09985", "submitter": "Battista Biggio", "authors": "Battista Biggio, Konrad Rieck, Davide Ariu, Christian Wressnegger,\n  Igino Corona, Giorgio Giacinto, Fabio Roli", "title": "Poisoning Behavioral Malware Clustering", "comments": null, "journal-ref": "2014 ACM CCS Workshop on Artificial Intelligent and Security,\n  AISec '14, pages 27-36, New York, NY, USA, 2014. ACM", "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering algorithms have become a popular tool in computer security to\nanalyze the behavior of malware variants, identify novel malware families, and\ngenerate signatures for antivirus systems. However, the suitability of\nclustering algorithms for security-sensitive settings has been recently\nquestioned by showing that they can be significantly compromised if an attacker\ncan exercise some control over the input data. In this paper, we revisit this\nproblem by focusing on behavioral malware clustering approaches, and\ninvestigate whether and to what extent an attacker may be able to subvert these\napproaches through a careful injection of samples with poisoning behavior. To\nthis end, we present a case study on Malheur, an open-source tool for\nbehavioral malware clustering. Our experiments not only demonstrate that this\ntool is vulnerable to poisoning attacks, but also that it can be significantly\ncompromised even if the attacker can only inject a very small percentage of\nattacks into the input data. As a remedy, we discuss possible countermeasures\nand highlight the need for more secure clustering algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 10:31:53 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Biggio", "Battista", ""], ["Rieck", "Konrad", ""], ["Ariu", "Davide", ""], ["Wressnegger", "Christian", ""], ["Corona", "Igino", ""], ["Giacinto", "Giorgio", ""], ["Roli", "Fabio", ""]]}, {"id": "1811.10002", "submitter": "Hwann-Tzong Chen", "authors": "Shou-Yao Roy Tseng, Hwann-Tzong Chen, Shao-Heng Tai, Tyng-Luh Liu", "title": "Non-local RoI for Cross-Object Perception", "comments": "NIPS 2018 Workshop on Relational Representation Learning. arXiv admin\n  note: substantial text overlap with arXiv:1807.05361", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generic and flexible module that encodes region proposals by\nboth their intrinsic features and the extrinsic correlations to the others. The\nproposed non-local region of interest (NL-RoI) can be seamlessly adapted into\ndifferent generalized R-CNN architectures to better address various perception\ntasks. Observe that existing techniques from R-CNN treat RoIs independently and\nperform the prediction solely based on image features within each region\nproposal. However, the pairwise relationships between proposals could further\nprovide useful information for detection and segmentation. NL-RoI is thus\nformulated to enrich each RoI representation with the information from all\nother RoIs, and yield a simple, low-cost, yet effective module for region-based\nconvolutional networks. Our experimental results show that NL-RoI can improve\nthe performance of Faster/Mask R-CNN for object detection and instance\nsegmentation.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 13:05:49 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Tseng", "Shou-Yao Roy", ""], ["Chen", "Hwann-Tzong", ""], ["Tai", "Shao-Heng", ""], ["Liu", "Tyng-Luh", ""]]}, {"id": "1811.10052", "submitter": "Alexander Lundervold", "authors": "Alexander Selvikv{\\aa}g Lundervold and Arvid Lundervold", "title": "An overview of deep learning in medical imaging focusing on MRI", "comments": "Minor updates. Close to the version published in Zeitschrift f\\\"ur\n  Medizinische Physik (Available online 13 December 2018)", "journal-ref": "Zeitschrift f\\\"ur Medizinische Physik, Volume 29, Issue 2, May\n  2019", "doi": "10.1016/j.zemedi.2018.11.002", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What has happened in machine learning lately, and what does it mean for the\nfuture of medical image analysis? Machine learning has witnessed a tremendous\namount of attention over the last few years. The current boom started around\n2009 when so-called deep artificial neural networks began outperforming other\nestablished models on a number of important benchmarks. Deep neural networks\nare now the state-of-the-art machine learning models across a variety of areas,\nfrom image analysis to natural language processing, and widely deployed in\nacademia and industry. These developments have a huge potential for medical\nimaging technology, medical data analysis, medical diagnostics and healthcare\nin general, slowly being realized. We provide a short overview of recent\nadvances and some associated challenges in machine learning applied to medical\nimage processing and image analysis. As this has become a very broad and fast\nexpanding field we will not survey the entire landscape of applications, but\nput particular focus on deep learning in MRI.\n  Our aim is threefold: (i) give a brief introduction to deep learning with\npointers to core references; (ii) indicate how deep learning has been applied\nto the entire MRI processing chain, from acquisition to image retrieval, from\nsegmentation to disease prediction; (iii) provide a starting point for people\ninterested in experimenting and perhaps contributing to the field of machine\nlearning for medical imaging by pointing out good educational resources,\nstate-of-the-art open-source code, and interesting sources of data and problems\nrelated medical imaging.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 16:40:42 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 09:58:09 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Lundervold", "Alexander Selvikv\u00e5g", ""], ["Lundervold", "Arvid", ""]]}, {"id": "1811.10072", "submitter": "Nicolas Brosse", "authors": "Nicolas Brosse, Alain Durmus, Eric Moulines", "title": "The promises and pitfalls of Stochastic Gradient Langevin Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Langevin Dynamics (SGLD) has emerged as a key MCMC\nalgorithm for Bayesian learning from large scale datasets. While SGLD with\ndecreasing step sizes converges weakly to the posterior distribution, the\nalgorithm is often used with a constant step size in practice and has\ndemonstrated successes in machine learning tasks. The current practice is to\nset the step size inversely proportional to $N$ where $N$ is the number of\ntraining samples. As $N$ becomes large, we show that the SGLD algorithm has an\ninvariant probability measure which significantly departs from the target\nposterior and behaves like Stochastic Gradient Descent (SGD). This difference\nis inherently due to the high variance of the stochastic gradients. Several\nstrategies have been suggested to reduce this effect; among them, SGLD Fixed\nPoint (SGLDFP) uses carefully designed control variates to reduce the variance\nof the stochastic gradients. We show that SGLDFP gives approximate samples from\nthe posterior distribution, with an accuracy comparable to the Langevin Monte\nCarlo (LMC) algorithm for a computational cost sublinear in the number of data\npoints. We provide a detailed analysis of the Wasserstein distances between\nLMC, SGLD, SGLDFP and SGD and explicit expressions of the means and covariance\nmatrices of their invariant distributions. Our findings are supported by\nlimited numerical experiments.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 18:58:39 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Brosse", "Nicolas", ""], ["Durmus", "Alain", ""], ["Moulines", "Eric", ""]]}, {"id": "1811.10097", "submitter": "Johanna Hansen", "authors": "Johanna Hansen, Kyle Kastner, Aaron Courville, Gregory Dudek", "title": "Planning in Dynamic Environments with Conditional Autoregressive Models", "comments": "6 pages, 1 figure, in Proceedings of the Prediction and Generative\n  Modeling in Reinforcement Learning Workshop at the International Conference\n  on Machine Learning (ICML) in 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We demonstrate the use of conditional autoregressive generative models (van\nden Oord et al., 2016a) over a discrete latent space (van den Oord et al.,\n2017b) for forward planning with MCTS. In order to test this method, we\nintroduce a new environment featuring varying difficulty levels, along with\nmoving goals and obstacles. The combination of high-quality frame generation\nand classical planning approaches nearly matches true environment performance\nfor our task, demonstrating the usefulness of this method for model-based\nplanning in dynamic environments.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 21:10:10 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Hansen", "Johanna", ""], ["Kastner", "Kyle", ""], ["Courville", "Aaron", ""], ["Dudek", "Gregory", ""]]}, {"id": "1811.10104", "submitter": "Ben Hutchinson", "authors": "Ben Hutchinson and Margaret Mitchell", "title": "50 Years of Test (Un)fairness: Lessons for Machine Learning", "comments": "FAT* '19: Conference on Fairness, Accountability, and Transparency\n  (FAT* '19), January 29--31, 2019, Atlanta, GA, USA", "journal-ref": null, "doi": "10.1145/3287560.3287600", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative definitions of what is unfair and what is fair have been\nintroduced in multiple disciplines for well over 50 years, including in\neducation, hiring, and machine learning. We trace how the notion of fairness\nhas been defined within the testing communities of education and hiring over\nthe past half century, exploring the cultural and social context in which\ndifferent fairness definitions have emerged. In some cases, earlier definitions\nof fairness are similar or identical to definitions of fairness in current\nmachine learning research, and foreshadow current formal work. In other cases,\ninsights into what fairness means and how to measure it have largely gone\noverlooked. We compare past and current notions of fairness along several\ndimensions, including the fairness criteria, the focus of the criteria (e.g., a\ntest, a model, or its use), the relationship of fairness to individuals,\ngroups, and subgroups, and the mathematical method for measuring fairness\n(e.g., classification, regression). This work points the way towards future\nresearch and measurement of (un)fairness that builds from our modern\nunderstanding of fairness while incorporating insights from the past.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 21:48:19 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 23:18:49 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Hutchinson", "Ben", ""], ["Mitchell", "Margaret", ""]]}, {"id": "1811.10105", "submitter": "Lam Nguyen", "authors": "Lam M. Nguyen, Katya Scheinberg, Martin Tak\\'a\\v{c}", "title": "Inexact SARAH Algorithm for Stochastic Optimization", "comments": "Optimization Methods and Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and analyze a variant of the SARAH algorithm, which does not\nrequire computation of the exact gradient. Thus this new method can be applied\nto general expectation minimization problems rather than only finite sum\nproblems. While the original SARAH algorithm, as well as its predecessor, SVRG,\nrequire an exact gradient computation on each outer iteration, the inexact\nvariant of SARAH (iSARAH), which we develop here, requires only stochastic\ngradient computed on a mini-batch of sufficient size. The proposed method\ncombines variance reduction via sample size selection and iterative stochastic\ngradient updates. We analyze the convergence rate of the algorithms for\nstrongly convex and non-strongly convex cases, under smooth assumption with\nappropriate mini-batch size selected for each case. We show that with an\nadditional, reasonable, assumption iSARAH achieves the best known complexity\namong stochastic methods in the case of non-strongly convex stochastic\nfunctions.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 21:54:02 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 15:37:55 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Nguyen", "Lam M.", ""], ["Scheinberg", "Katya", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1811.10106", "submitter": "Sung Min Park", "authors": "Guy Bresler, Sung Min Park, Madalina Persu", "title": "Sparse PCA from Sparse Linear Regression", "comments": "To appear in NeurIPS'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Principal Component Analysis (SPCA) and Sparse Linear Regression (SLR)\nhave a wide range of applications and have attracted a tremendous amount of\nattention in the last two decades as canonical examples of statistical problems\nin high dimension. A variety of algorithms have been proposed for both SPCA and\nSLR, but an explicit connection between the two had not been made. We show how\nto efficiently transform a black-box solver for SLR into an algorithm for SPCA:\nassuming the SLR solver satisfies prediction error guarantees achieved by\nexisting efficient algorithms such as those based on the Lasso, the SPCA\nalgorithm derived from it achieves near state of the art guarantees for testing\nand for support recovery for the single spiked covariance model as obtained by\nthe current best polynomialtime algorithms. Our reduction not only highlights\nthe inherent similarity between the two problems, but also, from a practical\nstandpoint, allows one to obtain a collection of algorithms for SPCA directly\nfrom known algorithms for SLR. We provide experimental results on simulated\ndata comparing our proposed framework to other algorithms for SPCA.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 21:54:08 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Bresler", "Guy", ""], ["Park", "Sung Min", ""], ["Persu", "Madalina", ""]]}, {"id": "1811.10109", "submitter": "Jiahua Xu", "authors": "Jiahua Xu, Benjamin Livshits", "title": "The Anatomy of a Cryptocurrency Pump-and-Dump Scheme", "comments": null, "journal-ref": "Proceedings of the 28th USENIX Security Symposium (2019) 1609-1625", "doi": null, "report-no": null, "categories": "q-fin.TR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While pump-and-dump schemes have attracted the attention of cryptocurrency\nobservers and regulators alike, this paper represents the first detailed\nempirical query of pump-and-dump activities in cryptocurrency markets. We\npresent a case study of a recent pump-and-dump event, investigate 412\npump-and-dump activities organized in Telegram channels from June 17, 2018 to\nFebruary 26, 2019, and discover patterns in crypto-markets associated with\npump-and-dump schemes. We then build a model that predicts the pump likelihood\nof all coins listed in a crypto-exchange prior to a pump. The model exhibits\nhigh precision as well as robustness, and can be used to create a simple, yet\nvery effective trading strategy, which we empirically demonstrate can generate\na return as high as 60% on small retail investments within a span of two and\nhalf months. The study provides a proof of concept for strategic crypto-trading\nand sheds light on the application of machine learning for crime detection.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 22:09:26 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 16:13:55 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Xu", "Jiahua", ""], ["Livshits", "Benjamin", ""]]}, {"id": "1811.10111", "submitter": "Abhay Koushik", "authors": "Abhay Koushik, Judith Amores and Pattie Maes", "title": "Real-Time Sleep Staging using Deep Learning on a Smartphone for a\n  Wearable EEG", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/114", "categories": "cs.HC cs.LG eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first real-time sleep staging system that uses deep learning\nwithout the need for servers in a smartphone application for a wearable EEG. We\nemploy real-time adaptation of a single channel Electroencephalography (EEG) to\ninfer from a Time-Distributed 1-D Deep Convolutional Neural Network.\nPolysomnography (PSG)-the gold standard for sleep staging, requires a human\nscorer and is both complex and resource-intensive. Our work demonstrates an\nend-to-end on-smartphone pipeline that can infer sleep stages in just single\n30-second epochs, with an overall accuracy of 83.5% on 20-fold cross validation\nfor five-class classification of sleep stages using the open Sleep-EDF dataset.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 22:25:31 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 02:30:23 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Koushik", "Abhay", ""], ["Amores", "Judith", ""], ["Maes", "Pattie", ""]]}, {"id": "1811.10112", "submitter": "R\\'emi Besson", "authors": "R\\'emi Besson, Erwan Le Pennec, St\\'ephanie Allassonni\\`ere, Julien\n  Stirnemann, Emmanuel Spaggiari, Antoine Neuraz", "title": "A Model-Based Reinforcement Learning Approach for a Rare Disease\n  Diagnostic Task", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present our various contributions to the objective of\nbuilding a decision support tool for the diagnosis of rare diseases. Our goal\nis to achieve a state of knowledge where the uncertainty about the patient's\ndisease is below a predetermined threshold. We aim to reach such states while\nminimizing the average number of medical tests to perform. In doing so, we take\ninto account the need, in many medical applications, to avoid, as much as\npossible, any misdiagnosis. To solve this optimization task, we investigate\nseveral reinforcement learning algorithm and make them operable in our\nhigh-dimensional and sparse-reward setting. We also present a way to combine\nexpert knowledge, expressed as conditional probabilities, with real clinical\ndata. This is crucial because the scarcity of data in the field of rare\ndiseases prevents any approach based solely on clinical data. Finally we show\nthat it is possible to integrate the ontological information about symptoms\nwhile remaining in our probabilistic reasoning. It enables our decision support\ntool to process information given at different level of precision by the user.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 22:44:54 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Besson", "R\u00e9mi", ""], ["Pennec", "Erwan Le", ""], ["Allassonni\u00e8re", "St\u00e9phanie", ""], ["Stirnemann", "Julien", ""], ["Spaggiari", "Emmanuel", ""], ["Neuraz", "Antoine", ""]]}, {"id": "1811.10115", "submitter": "Giang Tran", "authors": "Lam Si Tung Ho, Hayden Schaeffer, Giang Tran, Rachel Ward", "title": "Recovery guarantees for polynomial approximation from dependent data\n  with outliers", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning non-linear systems from noisy, limited, and/or dependent data is an\nimportant task across various scientific fields including statistics,\nengineering, computer science, mathematics, and many more. In general, this\nlearning task is ill-posed; however, additional information about the data's\nstructure or on the behavior of the unknown function can make the task\nwell-posed. In this work, we study the problem of learning nonlinear functions\nfrom corrupted and dependent data. The learning problem is recast as a sparse\nrobust linear regression problem where we incorporate both the unknown\ncoefficients and the corruptions in a basis pursuit framework. The main\ncontribution of our paper is to provide a reconstruction guarantee for the\nassociated $\\ell_1$-optimization problem where the sampling matrix is formed\nfrom dependent data. Specifically, we prove that the sampling matrix satisfies\nthe null space property and the stable null space property, provided that the\ndata is compact and satisfies a suitable concentration inequality. We show that\nour recovery results are applicable to various types of dependent data such as\nexponentially strongly $\\alpha$-mixing data, geometrically $\\mathcal{C}$-mixing\ndata, and uniformly ergodic Markov chain. Our theoretical results are verified\nvia several numerical simulations.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 23:24:59 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Ho", "Lam Si Tung", ""], ["Schaeffer", "Hayden", ""], ["Tran", "Giang", ""], ["Ward", "Rachel", ""]]}, {"id": "1811.10119", "submitter": "Alexander Amini", "authors": "Alexander Amini, Guy Rosman, Sertac Karaman, Daniela Rus", "title": "Variational End-to-End Navigation and Localization", "comments": "Published in IEEE International Conference on Robotics and Automation\n  (ICRA) 2019. Best Paper Award Finalist", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has revolutionized the ability to learn \"end-to-end\" autonomous\nvehicle control directly from raw sensory data. While there have been recent\nextensions to handle forms of navigation instruction, these works are unable to\ncapture the full distribution of possible actions that could be taken and to\nreason about localization of the robot within the environment. In this paper,\nwe extend end-to-end driving networks with the ability to perform\npoint-to-point navigation as well as probabilistic localization using only\nnoisy GPS data. We define a novel variational network capable of learning from\nraw camera data of the environment as well as higher level roadmaps to predict\n(1) a full probability distribution over the possible control commands; and (2)\na deterministic control command capable of navigating on the route specified\nwithin the map. Additionally, we formulate how our model can be used to\nlocalize the robot according to correspondences between the map and the\nobserved visual road topology, inspired by the rough localization that human\ndrivers can perform. We test our algorithms on real-world driving data that the\nvehicle has never driven through before, and integrate our point-to-point\nnavigation algorithms onboard a full-scale autonomous vehicle for real-time\nperformance. Our localization algorithm is also evaluated over a new set of\nroads and intersections to demonstrates rough pose localization even in\nsituations without any GPS prior.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 23:45:30 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 20:51:25 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Amini", "Alexander", ""], ["Rosman", "Guy", ""], ["Karaman", "Sertac", ""], ["Rus", "Daniela", ""]]}, {"id": "1811.10121", "submitter": "Abhishek Sharma", "authors": "Abhishek Sharma", "title": "Foreground Clustering for Joint Segmentation and Localization in Videos\n  and Images", "comments": "In Proceedings of NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework in which video/image segmentation and\nlocalization are cast into a single optimization problem that integrates\ninformation from low level appearance cues with that of high level localization\ncues in a very weakly supervised manner. The proposed framework leverages two\nrepresentations at different levels, exploits the spatial relationship between\nbounding boxes and superpixels as linear constraints and simultaneously\ndiscriminates between foreground and background at bounding box and superpixel\nlevel. Different from previous approaches that mainly rely on discriminative\nclustering, we incorporate a foreground model that minimizes the histogram\ndifference of an object across all image frames. Exploiting the geometric\nrelation between the superpixels and bounding boxes enables the transfer of\nsegmentation cues to improve localization output and vice-versa. Inclusion of\nthe foreground model generalizes our discriminative framework to video data\nwhere the background tends to be similar and thus, not discriminative. We\ndemonstrate the effectiveness of our unified framework on the YouTube Object\nvideo dataset, Internet Object Discovery dataset and Pascal VOC 2007.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 00:00:20 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Sharma", "Abhishek", ""]]}, {"id": "1811.10146", "submitter": "Zhiqin Xu", "authors": "Zhi-Qin John Xu", "title": "Frequency Principle in Deep Learning with General Loss Functions and Its\n  Potential Application", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies have shown that deep neural networks (DNNs) with common\nsettings often capture target functions from low to high frequency, which is\ncalled Frequency Principle (F-Principle). It has also been shown that\nF-Principle can provide an understanding to the often observed good\ngeneralization ability of DNNs. However, previous studies focused on the loss\nfunction of mean square error, while various loss functions are used in\npractice. In this work, we show that the F-Principle holds for a general loss\nfunction (e.g., mean square error, cross entropy, etc.). In addition, DNN's\nF-Principle may be applied to develop numerical schemes for solving various\nproblems which would benefit from a fast converging of low frequency. As an\nexample of the potential usage of F-Principle, we apply DNN in solving\ndifferential equations, in which conventional methods (e.g., Jacobi method) is\nusually slow in solving problems due to the convergence from high to low\nfrequency.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 02:27:44 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Xu", "Zhi-Qin John", ""]]}, {"id": "1811.10153", "submitter": "Ryohei Suzuki", "authors": "Ryohei Suzuki, Masanori Koyama, Takeru Miyato, Taizan Yonetsuji,\n  Huachun Zhu", "title": "Spatially Controllable Image Synthesis with Internal Representation\n  Collaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel CNN-based image editing strategy that allows the user to\nchange the semantic information of an image over an arbitrary region by\nmanipulating the feature-space representation of the image in a trained GAN\nmodel. We will present two variants of our strategy: (1) spatial conditional\nbatch normalization (sCBN), a type of conditional batch normalization with\nuser-specifiable spatial weight maps, and (2) feature-blending, a method of\ndirectly modifying the intermediate features. Our methods can be used to edit\nboth artificial image and real image, and they both can be used together with\nany GAN with conditional normalization layers. We will demonstrate the power of\nour method through experiments on various types of GANs trained on different\ndatasets. Code will be available at\nhttps://github.com/pfnet-research/neural-collage.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 03:00:08 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 06:19:53 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Suzuki", "Ryohei", ""], ["Koyama", "Masanori", ""], ["Miyato", "Takeru", ""], ["Yonetsuji", "Taizan", ""], ["Zhu", "Huachun", ""]]}, {"id": "1811.10154", "submitter": "Cynthia Rudin", "authors": "Cynthia Rudin", "title": "Stop Explaining Black Box Machine Learning Models for High Stakes\n  Decisions and Use Interpretable Models Instead", "comments": "Author's pre-publication version of a 2019 Nature Machine\n  Intelligence article. Shorter Version was published in NIPS 2018 Workshop on\n  Critiquing and Correcting Trends in Machine Learning. Expands also on NSF\n  Statistics at a Crossroads Webinar", "journal-ref": "Nature Machine Intelligence, Vol 1, May 2019, 206-215", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Black box machine learning models are currently being used for high stakes\ndecision-making throughout society, causing problems throughout healthcare,\ncriminal justice, and in other domains. People have hoped that creating methods\nfor explaining these black box models will alleviate some of these problems,\nbut trying to \\textit{explain} black box models, rather than creating models\nthat are \\textit{interpretable} in the first place, is likely to perpetuate bad\npractices and can potentially cause catastrophic harm to society. There is a\nway forward -- it is to design models that are inherently interpretable. This\nmanuscript clarifies the chasm between explaining black boxes and using\ninherently interpretable models, outlines several key reasons why explainable\nblack boxes should be avoided in high-stakes decisions, identifies challenges\nto interpretable machine learning, and provides several example applications\nwhere interpretable models could potentially replace black box models in\ncriminal justice, healthcare, and computer vision.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 03:00:25 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 04:09:42 GMT"}, {"version": "v3", "created": "Sun, 22 Sep 2019 03:05:09 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Rudin", "Cynthia", ""]]}, {"id": "1811.10158", "submitter": "Chenchen Li", "authors": "Chenchen Li, Xiang Yan, Xiaotie Deng, Yuan Qi, Wei Chu, Le Song,\n  Junlong Qiao, Jianshan He, Junwu Xiong", "title": "Reinforcement Learning for Uplift Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uplift modeling aims to directly model the incremental impact of a treatment\non an individual response. In this work, we address the problem from a new\nangle and reformulate it as a Markov Decision Process (MDP). We conducted\nextensive experiments on both a synthetic dataset and real-world scenarios, and\nshowed that our method can achieve significant improvement over previous\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 03:17:38 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 04:37:45 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Li", "Chenchen", ""], ["Yan", "Xiang", ""], ["Deng", "Xiaotie", ""], ["Qi", "Yuan", ""], ["Chu", "Wei", ""], ["Song", "Le", ""], ["Qiao", "Junlong", ""], ["He", "Jianshan", ""], ["Xiong", "Junwu", ""]]}, {"id": "1811.10201", "submitter": "AnChieh Cheng", "authors": "An-Chieh Cheng, Chieh Hubert Lin, Da-Cheng Juan, Wei Wei, Min Sun", "title": "InstaNAS: Instance-aware Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional Neural Architecture Search (NAS) aims at finding a single\narchitecture that achieves the best performance, which usually optimizes task\nrelated learning objectives such as accuracy. However, a single architecture\nmay not be representative enough for the whole dataset with high diversity and\nvariety. Intuitively, electing domain-expert architectures that are proficient\nin domain-specific features can further benefit architecture related objectives\nsuch as latency. In this paper, we propose InstaNAS---an instance-aware NAS\nframework---that employs a controller trained to search for a \"distribution of\narchitectures\" instead of a single architecture; This allows the model to use\nsophisticated architectures for the difficult samples, which usually comes with\nlarge architecture related cost, and shallow architectures for those easy\nsamples. During the inference phase, the controller assigns each of the unseen\ninput samples with a domain expert architecture that can achieve high accuracy\nwith customized inference costs. Experiments within a search space inspired by\nMobileNetV2 show InstaNAS can achieve up to 48.8% latency reduction without\ncompromising accuracy on a series of datasets against MobileNetV2.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 06:29:39 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 14:12:40 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 09:25:04 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Cheng", "An-Chieh", ""], ["Lin", "Chieh Hubert", ""], ["Juan", "Da-Cheng", ""], ["Wei", "Wei", ""], ["Sun", "Min", ""]]}, {"id": "1811.10203", "submitter": "Dan Levi", "authors": "Noa Garnett, Rafi Cohen, Tomer Pe'er, Roee Lahav, Dan Levi", "title": "3D-LaneNet: End-to-End 3D Multiple Lane Detection", "comments": "To be presented in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a network that directly predicts the 3D layout of lanes in a\nroad scene from a single image. This work marks a first attempt to address this\ntask with on-board sensing without assuming a known constant lane width or\nrelying on pre-mapped environments. Our network architecture, 3D-LaneNet,\napplies two new concepts: intra-network inverse-perspective mapping (IPM) and\nanchor-based lane representation. The intra-network IPM projection facilitates\na dual-representation information flow in both regular image-view and top-view.\nAn anchor-per-column output representation enables our end-to-end approach\nwhich replaces common heuristics such as clustering and outlier rejection,\ncasting lane estimation as an object detection problem. In addition, our\napproach explicitly handles complex situations such as lane merges and splits.\nResults are shown on two new 3D lane datasets, a synthetic and a real one. For\ncomparison with existing methods, we test our approach on the image-only\ntuSimple lane detection benchmark, achieving performance competitive with\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 06:34:28 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 11:59:05 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2019 06:48:14 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Garnett", "Noa", ""], ["Cohen", "Rafi", ""], ["Pe'er", "Tomer", ""], ["Lahav", "Roee", ""], ["Levi", "Dan", ""]]}, {"id": "1811.10256", "submitter": "Natasha Fernandes", "authors": "Natasha Fernandes, Mark Dras, Annabelle McIver", "title": "Generalised Differential Privacy for Text Document Processing", "comments": "Typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of how to \"obfuscate\" texts by removing stylistic\nclues which can identify authorship, whilst preserving (as much as possible)\nthe content of the text. In this paper we combine ideas from \"generalised\ndifferential privacy\" and machine learning techniques for text processing to\nmodel privacy for text documents. We define a privacy mechanism that operates\nat the level of text documents represented as \"bags-of-words\" - these\nrepresentations are typical in machine learning and contain sufficient\ninformation to carry out many kinds of classification tasks including topic\nidentification and authorship attribution (of the original documents). We show\nthat our mechanism satisfies privacy with respect to a metric for semantic\nsimilarity, thereby providing a balance between utility, defined by the\nsemantic content of texts, with the obfuscation of stylistic clues. We\ndemonstrate our implementation on a \"fan fiction\" dataset, confirming that it\nis indeed possible to disguise writing style effectively whilst preserving\nenough information and variation for accurate content classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 09:54:13 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 15:49:36 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Fernandes", "Natasha", ""], ["Dras", "Mark", ""], ["McIver", "Annabelle", ""]]}, {"id": "1811.10264", "submitter": "Qihao Liu", "authors": "Qihao Liu, Xiaofeng Liu, Guoping Cai", "title": "PNS: Population-Guided Novelty Search Learning Method for Reinforcement\n  Learning", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning (RL) has made remarkable achievements, but it still\nsuffers from inadequate exploration strategies, sparse reward signals, and\ndeceptive reward functions. These problems motivate the need for a more\nefficient and directed exploration. For solving this, a Population-guided\nNovelty Search (PNS) parallel learning method is proposed. In PNS, the\npopulation is divided into multiple sub-populations, each of which has one\nchief agent and several exploring agents. The role of the chief agent is to\nevaluate the policies learned by exploring agents and to share the optimal\npolicy with all sub-populations. The role of exploring agents is to learn their\npolicies in collaboration with the guidance of the optimal policy and,\nsimultaneously, upload their policies to the chief agent. To balance\nexploration and exploitation, the Novelty Search (NS) is employed in chief\nagents to encourage policies with high novelty while maximizing per-episode\nperformance. The introduction of sub-populations and NS mechanisms promote\ndirected exploration and enables better policy search. In the numerical\nexperiment section, the proposed scheme is applied to the twin delayed deep\ndeterministic (TD3) policy gradient algorithm, and the effectiveness of PNS to\npromote exploration and improve performance in both continuous control domains\nand discrete control domains is demonstrated. Notably, the proposed method\nachieves rewards that far exceed the SOTA methods in Delayed MoJoCo\nenvironments.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 10:05:15 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 07:01:59 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 04:58:44 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Liu", "Qihao", ""], ["Liu", "Xiaofeng", ""], ["Cai", "Guoping", ""]]}, {"id": "1811.10275", "submitter": "Francois-Xavier Briol", "authors": "Francois-Xavier Briol, Chris J. Oates, Mark Girolami, Michael A.\n  Osborne, Dino Sejdinovic", "title": "Rejoinder for \"Probabilistic Integration: A Role in Statistical\n  Computation?\"", "comments": "Accepted to Statistical Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is the rejoinder for the paper \"Probabilistic Integration: A\nRole in Statistical Computation?\" to appear in Statistical Science with\ndiscussion. We would first like to thank the reviewers and many of our\ncolleagues who helped shape this paper, the editor for selecting our paper for\ndiscussion, and of course all of the discussants for their thoughtful,\ninsightful and constructive comments. In this rejoinder, we respond to some of\nthe points raised by the discussants and comment further on the fundamental\nquestions underlying the paper: (i) Should Bayesian ideas be used in numerical\nanalysis?, and (ii) If so, what role should such approaches have in statistical\ncomputation?\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 10:30:38 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Briol", "Francois-Xavier", ""], ["Oates", "Chris J.", ""], ["Girolami", "Mark", ""], ["Osborne", "Michael A.", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1811.10276", "submitter": "Maurizio Pierini", "authors": "Olmo Cerri and Thong Q. Nguyen and Maurizio Pierini and Maria\n  Spiropulu and Jean-Roch Vlimant", "title": "Variational Autoencoders for New Physics Mining at the Large Hadron\n  Collider", "comments": "29 pages, 12 figures, 5 tables", "journal-ref": "J. High Energ. Phys. (2019) 2019: 36", "doi": "10.1007/JHEP05(2019)036", "report-no": null, "categories": "hep-ex cs.LG hep-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using variational autoencoders trained on known physics processes, we develop\na one-sided threshold test to isolate previously unseen processes as outlier\nevents. Since the autoencoder training does not depend on any specific new\nphysics signature, the proposed procedure doesn't make specific assumptions on\nthe nature of new physics. An event selection based on this algorithm would be\ncomplementary to classic LHC searches, typically based on model-dependent\nhypothesis testing. Such an algorithm would deliver a list of anomalous events,\nthat the experimental collaborations could further scrutinize and even release\nas a catalog, similarly to what is typically done in other scientific domains.\nEvent topologies repeating in this dataset could inspire new-physics model\nbuilding and new experimental searches. Running in the trigger system of the\nLHC experiments, such an application could identify anomalous events that would\nbe otherwise lost, extending the scientific reach of the LHC.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 10:33:55 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 10:33:04 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 16:26:21 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Cerri", "Olmo", ""], ["Nguyen", "Thong Q.", ""], ["Pierini", "Maurizio", ""], ["Spiropulu", "Maria", ""], ["Vlimant", "Jean-Roch", ""]]}, {"id": "1811.10304", "submitter": "Hao Shen", "authors": "Hao Shen", "title": "A Differential Topological View of Challenges in Learning with\n  Feedforward Neural Networks", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among many unsolved puzzles in theories of Deep Neural Networks (DNNs), there\nare three most fundamental challenges that highly demand solutions, namely,\nexpressibility, optimisability, and generalisability. Although there have been\nsignificant progresses in seeking answers using various theories, e.g.\ninformation bottleneck theory, sparse representation, statistical inference,\nRiemannian geometry, etc., so far there is no single theory that is able to\nprovide solutions to all these challenges. In this work, we propose to engage\nthe theory of differential topology to address the three problems. By modelling\nthe dataset of interest as a smooth manifold, DNNs can be considered as\ncompositions of smooth maps between smooth manifolds. Specifically, our work\noffers a differential topological view of loss landscape of DNNs, interplay\nbetween width and depth in expressibility, and regularisations for\ngeneralisability. Finally, in the setting of deep representation learning, we\nfurther apply the quotient topology to investigate the architecture of DNNs,\nwhich enables to capture nuisance factors in data with respect to a specific\nlearning task.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 11:47:15 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Shen", "Hao", ""]]}, {"id": "1811.10323", "submitter": "Tarun Kalluri Mr.", "authors": "Tarun Kalluri, Girish Varma, Manmohan Chandraker, C V Jawahar", "title": "Universal Semi-Supervised Semantic Segmentation", "comments": "Accepted as poster presentation at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the need for semantic segmentation has arisen across several\ndifferent applications and environments. However, the expense and redundancy of\nannotation often limits the quantity of labels available for training in any\ndomain, while deployment is easier if a single model works well across domains.\nIn this paper, we pose the novel problem of universal semi-supervised semantic\nsegmentation and propose a solution framework, to meet the dual needs of lower\nannotation and deployment costs. In contrast to counterpoints such as fine\ntuning, joint training or unsupervised domain adaptation, universal\nsemi-supervised segmentation ensures that across all domains: (i) a single\nmodel is deployed, (ii) unlabeled data is used, (iii) performance is improved,\n(iv) only a few labels are needed and (v) label spaces may differ. To address\nthis, we minimize supervised as well as within and cross-domain unsupervised\nlosses, introducing a novel feature alignment objective based on pixel-aware\nentropy regularization for the latter. We demonstrate quantitative advantages\nover other approaches on several combinations of segmentation datasets across\ndifferent geographies (Germany, England, India) and environments (outdoors,\nindoors), as well as qualitative insights on the aligned representations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 12:36:03 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 01:55:56 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 06:13:53 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Kalluri", "Tarun", ""], ["Varma", "Girish", ""], ["Chandraker", "Manmohan", ""], ["Jawahar", "C V", ""]]}, {"id": "1811.10364", "submitter": "Joeran Beel", "authors": "Joeran Beel, Andrew Collins, Akiko Aizawa", "title": "The Architecture of Mr. DLib's Scientific Recommender-System API", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems in academia are not widely available. This may be in part\ndue to the difficulty and cost of developing and maintaining recommender\nsystems. Many operators of academic products such as digital libraries and\nreference managers avoid this effort, although a recommender system could\nprovide significant benefits to their users. In this paper, we introduce Mr.\nDLib's \"Recommendations as-a-Service\" (RaaS) API that allows operators of\nacademic products to easily integrate a scientific recommender system into\ntheir products. Mr. DLib generates recommendations for research articles but in\nthe future, recommendations may include call for papers, grants, etc. Operators\nof academic products can request recommendations from Mr. DLib and display\nthese recommendations to their users. Mr. DLib can be integrated in just a few\nhours or days; creating an equivalent recommender system from scratch would\nrequire several months for an academic operator. Mr. DLib has been used by\nGESIS Sowiport and by the reference manager JabRef. Mr. DLib is open source and\nits goal is to facilitate the application of, and research on, scientific\nrecommender systems. In this paper, we present the motivation for Mr. DLib, the\narchitecture and details about the effectiveness. Mr. DLib has delivered 94m\nrecommendations over a span of two years with an average click-through rate of\n0.12%.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 13:41:03 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Beel", "Joeran", ""], ["Collins", "Andrew", ""], ["Aizawa", "Akiko", ""]]}, {"id": "1811.10369", "submitter": "Joeran Beel", "authors": "Dominika Tkaczyk, Rohit Gupta, Riccardo Cinti, Joeran Beel", "title": "ParsRec: A Novel Meta-Learning Approach to Recommending Bibliographic\n  Reference Parsers", "comments": "Accepted at the 26th Irish Conference on Artificial Intelligence and\n  Cognitive Science. This paper is an extended version of a poster published at\n  the 12th ACM Conference on Recommender Systems, Proceedings of the 26th Irish\n  Conference on Artificial Intelligence and Cognitive Science (AICS). Dublin,\n  Ireland 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bibliographic reference parsers extract machine-readable metadata such as\nauthor names, title, journal, and year from bibliographic reference strings. To\nextract the metadata, the parsers apply heuristics or machine learning.\nHowever, no reference parser, and no algorithm, consistently gives the best\nresults in every scenario. For instance, one tool may be best in extracting\ntitles in ACM citation style, but only third best when APA is used. Another\ntool may be best in extracting English author names, while another one is best\nfor noisy data (i.e. inconsistent citation styles). In this paper, which is an\nextended version of our recent RecSys poster, we address the problem of\nreference parsing from a recommender-systems and meta-learning perspective. We\npropose ParsRec, a meta-learning based recommender-system that recommends the\npotentially most effective parser for a given reference string. ParsRec\nrecommends one out of 10 open-source parsers: Anystyle-Parser, Biblio, CERMINE,\nCitation, Citation-Parser, GROBID, ParsCit, PDFSSA4MET, Reference Tagger, and\nScience Parse. We evaluate ParsRec on 105k references from chemistry. We\npropose two approaches to meta-learning recommendations. The first approach\nlearns the best parser for an entire reference string. The second approach\nlearns the best parser for each metadata type in a reference string. The second\napproach achieved a 2.6% increase in F1 (0.909 vs. 0.886) over the best single\nparser (GROBID), reducing the false positive rate by 20.2% (0.075 vs. 0.094),\nand the false negative rate by 18.9% (0.107 vs. 0.132).\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 13:56:57 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Tkaczyk", "Dominika", ""], ["Gupta", "Rohit", ""], ["Cinti", "Riccardo", ""], ["Beel", "Joeran", ""]]}, {"id": "1811.10376", "submitter": "Yi-Te Hsu", "authors": "Yi-Te Hsu, Zining Zhu, Chi-Te Wang, Shih-Hau Fang, Frank Rudzicz, Yu\n  Tsao", "title": "Robustness against the channel effect in pathological voice detection", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/200", "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many people are suffering from voice disorders, which can adversely affect\nthe quality of their lives. In response, some researchers have proposed\nalgorithms for automatic assessment of these disorders, based on voice signals.\nHowever, these signals can be sensitive to the recording devices. Indeed, the\nchannel effect is a pervasive problem in machine learning for healthcare. In\nthis study, we propose a detection system for pathological voice, which is\nrobust against the channel effect. This system is based on a bidirectional LSTM\nnetwork. To increase the performance robustness against channel mismatch, we\nintegrate domain adversarial training (DAT) to eliminate the differences\nbetween the devices. When we train on data recorded on a high-quality\nmicrophone and evaluate on smartphone data without labels, our robust detection\nsystem increases the PR-AUC from 0.8448 to 0.9455 (and 0.9522 with target\nsample labels). To the best of our knowledge, this is the first study applying\nunsupervised domain adaptation to pathological voice detection. Notably, our\nsystem does not need target device sample labels, which allows for\ngeneralization to many new devices.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 14:11:12 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 14:52:39 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Hsu", "Yi-Te", ""], ["Zhu", "Zining", ""], ["Wang", "Chi-Te", ""], ["Fang", "Shih-Hau", ""], ["Rudzicz", "Frank", ""], ["Tsao", "Yu", ""]]}, {"id": "1811.10396", "submitter": "Arash Ardakani", "authors": "Arash Ardakani, Zhengyun Ji, Warren J. Gross", "title": "Learning to Skip Ineffectual Recurrent Computations in LSTMs", "comments": "Accepted as a conference paper for presentation at DATE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory (LSTM) is a special class of recurrent neural network,\nwhich has shown remarkable successes in processing sequential data. The typical\narchitecture of an LSTM involves a set of states and gates: the states retain\ninformation over arbitrary time intervals and the gates regulate the flow of\ninformation. Due to the recursive nature of LSTMs, they are computationally\nintensive to deploy on edge devices with limited hardware resources. To reduce\nthe computational complexity of LSTMs, we first introduce a method that learns\nto retain only the important information in the states by pruning redundant\ninformation. We then show that our method can prune over 90% of information in\nthe states without incurring any accuracy degradation over a set of temporal\ntasks. This observation suggests that a large fraction of the recurrent\ncomputations are ineffectual and can be avoided to speed up the process during\nthe inference as they involve noncontributory multiplications/accumulations\nwith zero-valued states. Finally, we introduce a custom hardware accelerator\nthat can perform the recurrent computations using both sparse and dense states.\nExperimental measurements show that performing the computations using the\nsparse states speeds up the process and improves energy efficiency by up to\n5.2x when compared to implementation results of the accelerator performing the\ncomputations using dense states.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 15:51:40 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 22:44:01 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Ardakani", "Arash", ""], ["Ji", "Zhengyun", ""], ["Gross", "Warren J.", ""]]}, {"id": "1811.10399", "submitter": "Kedar Potdar", "authors": "Kedar Potdar, Chinmay D. Pai, Sukrut Akolkar", "title": "A Convolutional Neural Network based Live Object Recognition System as\n  Blind Aid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a live object recognition system that serves as a blind\naid. Visually impaired people heavily rely on their other senses such as touch\nand auditory signals for understanding the environment around them. The act of\nknowing what object is in front of the blind person without touching it (by\nhand or some other tool) is very difficult. In some cases, the physical contact\nbetween the person and object can be dangerous, and even lethal.\n  This project employs a Convolutional Neural Network for recognition of\npre-trained objects on the ImageNet dataset. A camera, aligned with the\nsystem's predetermined orientation serves as input to the computer system,\nwhich has the object recognition Neural Network deployed to carry out real-time\nobject detection. Output from the network can then be parsed to present to the\nvisually impaired person either in the form of audio or Braille text.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 14:38:25 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Potdar", "Kedar", ""], ["Pai", "Chinmay D.", ""], ["Akolkar", "Sukrut", ""]]}, {"id": "1811.10422", "submitter": "Nikola Milo\\v{s}evi\\'c MSc", "authors": "Nikola Milosevic, Goran Nenadic", "title": "Creating a contemporary corpus of similes in Serbian by using natural\n  language processing", "comments": "15 pages, submitted to journal Slovo, however, later withdrawn to\n  correct. Additional work was not done on it, so it is still waiting to be\n  extended. Output of the system can be seen here:\n  http://ezbirka.starisloveni.com/. arXiv admin note: text overlap with\n  arXiv:1605.06319", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simile is a figure of speech that compares two things through the use of\nconnection words, but where comparison is not intended to be taken literally.\nThey are often used in everyday communication, but they are also a part of\nlinguistic cultural heritage. In this paper we present a methodology for\nsemi-automated collection of similes from the World Wide Web using text mining\nand machine learning techniques. We expanded an existing corpus by collecting\n442 similes from the internet and adding them to the existing corpus collected\nby Vuk Stefanovic Karadzic that contained 333 similes. We, also, introduce\ncrowdsourcing to the collection of figures of speech, which helped us to build\ncorpus containing 787 unique similes.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 12:55:40 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Milosevic", "Nikola", ""], ["Nenadic", "Goran", ""]]}, {"id": "1811.10427", "submitter": "Qunwei Li", "authors": "Qunwei Li, Bhavya Kailkhura, Rushil Anirudh, Yi Zhou, Yingbin Liang,\n  Pramod Varshney", "title": "MR-GAN: Manifold Regularized Generative Adversarial Networks", "comments": "arXiv admin note: text overlap with arXiv:1706.04156 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growing interest in generative adversarial networks (GANs),\ntraining GANs remains a challenging problem, both from a theoretical and a\npractical standpoint. To address this challenge, in this paper, we propose a\nnovel way to exploit the unique geometry of the real data, especially the\nmanifold information. More specifically, we design a method to regularize GAN\ntraining by adding an additional regularization term referred to as manifold\nregularizer. The manifold regularizer forces the generator to respect the\nunique geometry of the real data manifold and generate high quality data.\nFurthermore, we theoretically prove that the addition of this regularization\nterm in any class of GANs including DCGAN and Wasserstein GAN leads to improved\nperformance in terms of generalization, existence of equilibrium, and\nstability. Preliminary experiments show that the proposed manifold\nregularization helps in avoiding mode collapse and leads to stable training.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 21:21:02 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Li", "Qunwei", ""], ["Kailkhura", "Bhavya", ""], ["Anirudh", "Rushil", ""], ["Zhou", "Yi", ""], ["Liang", "Yingbin", ""], ["Varshney", "Pramod", ""]]}, {"id": "1811.10435", "submitter": "Nicol\\`o Navarin", "authors": "Dinh Van Tran, Nicol\\`o Navarin, Alessandro Sperduti", "title": "On Filter Size in Graph Convolutional Networks", "comments": "arXiv admin note: text overlap with arXiv:1811.06930", "journal-ref": "IEEE Symposium on Deep Learning, 2018 Symposium Series on\n  Computational Intelligence, 18 - 21 November, 2018, Bengaluru, India", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many researchers have been focusing on the definition of neural\nnetworks for graphs. The basic component for many of these approaches remains\nthe graph convolution idea proposed almost a decade ago. In this paper, we\nextend this basic component, following an intuition derived from the well-known\nconvolutional filters over multi-dimensional tensors. In particular, we derive\na simple, efficient and effective way to introduce a hyper-parameter on graph\nconvolutions that influences the filter size, i.e. its receptive field over the\nconsidered graph. We show with experimental results on real-world graph\ndatasets that the proposed graph convolutional filter improves the predictive\nperformance of Deep Graph Convolutional Networks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 13:45:30 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Van Tran", "Dinh", ""], ["Navarin", "Nicol\u00f2", ""], ["Sperduti", "Alessandro", ""]]}, {"id": "1811.10455", "submitter": "Harry Clifford MSci DPhil", "authors": "Geoffroy Dubourg-Felonneau, Timothy Cannings, Fergal Cotter, Hannah\n  Thompson, Nirmesh Patel, John W Cassidy, Harry W Clifford", "title": "A Framework for Implementing Machine Learning on Omics Data", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/102", "categories": "cs.LG cs.AI q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential benefits of applying machine learning methods to -omics data\nare becoming increasingly apparent, especially in clinical settings. However,\nthe unique characteristics of these data are not always well suited to machine\nlearning techniques. These data are often generated across different\ntechnologies in different labs, and frequently with high dimensionality. In\nthis paper we present a framework for combining -omics data sets, and for\nhandling high dimensional data, making -omics research more accessible to\nmachine learning applications. We demonstrate the success of this framework\nthrough integration and analysis of multi-analyte data for a set of 3,533\nbreast cancers. We then use this data-set to predict breast cancer patient\nsurvival for individuals at risk of an impending event, with higher accuracy\nand lower variance than methods trained on individual data-sets. We hope that\nour pipelines for data-set generation and transformation will open up -omics\ndata to machine learning researchers. We have made these freely available for\nnoncommercial use at www.ccg.ai.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 15:35:57 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Dubourg-Felonneau", "Geoffroy", ""], ["Cannings", "Timothy", ""], ["Cotter", "Fergal", ""], ["Thompson", "Hannah", ""], ["Patel", "Nirmesh", ""], ["Cassidy", "John W", ""], ["Clifford", "Harry W", ""]]}, {"id": "1811.10469", "submitter": "Jinwei Zhao", "authors": "Jinwei Zhao, Qizhou Wang, Yufei Wang, Yu Liu, Zhenghao Shi, Xinhong\n  Hei", "title": "How to improve the interpretability of kernel learning", "comments": "arXiv admin note: text overlap with arXiv:1811.07747", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, machine learning researchers have focused on methods to\nconstruct flexible and interpretable prediction models. However, an\ninterpretability evaluation, a relationship between generalization performance\nand an interpretability of the model and a method for improving the\ninterpretability have to be considered. In this paper, a quantitative index of\nthe interpretability is proposed and its rationality is proved, and equilibrium\nproblem between the interpretability and the generalization performance is\nanalyzed. Probability upper bound of the sum of the two performances is\nanalyzed. For traditional supervised kernel machine learning problem, a\nuniversal learning framework is put forward to solve the equilibrium problem\nbetween the two performances. The condition for global optimal solution based\non the framework is deduced. The learning framework is applied to the\nleast-squares support vector machine and is evaluated by some experiments.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 15:36:35 GMT"}, {"version": "v2", "created": "Sat, 5 Oct 2019 04:24:27 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Zhao", "Jinwei", ""], ["Wang", "Qizhou", ""], ["Wang", "Yufei", ""], ["Liu", "Yu", ""], ["Shi", "Zhenghao", ""], ["Hei", "Xinhong", ""]]}, {"id": "1811.10475", "submitter": "Lei Yu", "authors": "Lei Yu, Cyprien de Masson d'Autume, Chris Dyer, Phil Blunsom, Lingpeng\n  Kong, Wang Ling", "title": "Sentence Encoding with Tree-constrained Relation Networks", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The meaning of a sentence is a function of the relations that hold between\nits words. We instantiate this relational view of semantics in a series of\nneural models based on variants of relation networks (RNs) which represent a\nset of objects (for us, words forming a sentence) in terms of representations\nof pairs of objects. We propose two extensions to the basic RN model for\nnatural language. First, building on the intuition that not all word pairs are\nequally informative about the meaning of a sentence, we use constraints based\non both supervised and unsupervised dependency syntax to control which\nrelations influence the representation. Second, since higher-order relations\nare poorly captured by a sum of pairwise relations, we use a recurrent\nextension of RNs to propagate information so as to form representations of\nhigher order relations. Experiments on sentence classification, sentence pair\nclassification, and machine translation reveal that, while basic RNs are only\nmodestly effective for sentence representation, recurrent RNs with latent\nsyntax are a reliably powerful representational device.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 16:07:36 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Yu", "Lei", ""], ["d'Autume", "Cyprien de Masson", ""], ["Dyer", "Chris", ""], ["Blunsom", "Phil", ""], ["Kong", "Lingpeng", ""], ["Ling", "Wang", ""]]}, {"id": "1811.10481", "submitter": "Mariana Souza", "authors": "Rafael M. O. Cruz, Mariana A. Souza, Robert Sabourin and George D. C.\n  Cavalcanti", "title": "ICPRAI 2018 SI: On dynamic ensemble selection and data preprocessing for\n  multi-class imbalance learning", "comments": "Manuscript of the extended journal version of arXiv:1803.03877. This\n  manuscript was accepted for publication in the IJPRAI as a Special Issue\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class-imbalance refers to classification problems in which many more\ninstances are available for certain classes than for others. Such imbalanced\ndatasets require special attention because traditional classifiers generally\nfavor the majority class which has a large number of instances. Ensemble of\nclassifiers have been reported to yield promising results. However, the\nmajority of ensemble methods applied to imbalanced learning are static ones.\nMoreover, they only deal with binary imbalanced problems. Hence, this paper\npresents an empirical analysis of dynamic selection techniques and data\npreprocessing methods for dealing with multi-class imbalanced problems. We\nconsidered five variations of preprocessing methods and fourteen dynamic\nselection schemes. Our experiments conducted on 26 multi-class imbalanced\nproblems show that the dynamic ensemble improves the AUC and the G-mean as\ncompared to the static ensemble. Moreover, data preprocessing plays an\nimportant role in such cases.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 14:27:28 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 00:32:07 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Cruz", "Rafael M. O.", ""], ["Souza", "Mariana A.", ""], ["Sabourin", "Robert", ""], ["Cavalcanti", "George D. C.", ""]]}, {"id": "1811.10495", "submitter": "Shuxuan Guo", "authors": "Shuxuan Guo, Jose M. Alvarez, Mathieu Salzmann", "title": "ExpandNets: Linear Over-parameterization to Train Compact Convolutional\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce an approach to training a given compact network. To this end, we\nleverage over-parameterization, which typically improves both neural network\noptimization and generalization. Specifically, we propose to expand each linear\nlayer of the compact network into multiple consecutive linear layers, without\nadding any nonlinearity. As such, the resulting expanded network, or ExpandNet,\ncan be contracted back to the compact one algebraically at inference. In\nparticular, we introduce two convolutional expansion strategies and demonstrate\ntheir benefits on several tasks, including image classification, object\ndetection, and semantic segmentation. As evidenced by our experiments, our\napproach outperforms both training the compact network from scratch and\nperforming knowledge distillation from a teacher. Furthermore, our linear\nover-parameterization empirically reduces gradient confusion during training\nand improves the network generalization.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 16:40:24 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 13:33:58 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 11:08:09 GMT"}, {"version": "v4", "created": "Thu, 20 Feb 2020 17:26:52 GMT"}, {"version": "v5", "created": "Wed, 14 Apr 2021 11:55:22 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Guo", "Shuxuan", ""], ["Alvarez", "Jose M.", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1811.10501", "submitter": "Edward De Brouwer", "authors": "Edward De Brouwer, Jaak Simm, Adam Arany and Yves Moreau", "title": "Deep Ensemble Tensor Factorization for Longitudinal Patient Trajectories\n  Classification", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generative approach to classify scarcely observed longitudinal\npatient trajectories. The available time series are represented as tensors and\nfactorized using generative deep recurrent neural networks. The learned factors\nrepresent the patient data in a compact way and can then be used in a\ndownstream classification task. For more robustness and accuracy in the\npredictions, we used an ensemble of those deep generative models to mimic\nBayesian posterior sampling. We illustrate the performance of our architecture\non an intensive-care case study of in-hospital mortality prediction with 96\nlongitudinal measurement types measured across the first 48-hour from\nadmission. Our combination of generative and ensemble strategies achieves an\nAUC of over 0.85, and outperforms the SAPS-II mortality score and GRU\nbaselines.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 16:50:41 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 07:56:51 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["De Brouwer", "Edward", ""], ["Simm", "Jaak", ""], ["Arany", "Adam", ""], ["Moreau", "Yves", ""]]}, {"id": "1811.10553", "submitter": "Alvaro Ulloa Cerna", "authors": "Alvaro Ulloa, Linyuan Jing, Christopher W Good, David P vanMaanen,\n  Sushravya Raghunath, Jonathan D Suever, Christopher D Nevius, Gregory J\n  Wehner, Dustin Hartzel, Joseph B Leader, Amro Alsaid, Aalpen A Patel, H\n  Lester Kirchner, Marios S Pattichis, Christopher M Haggerty, Brandon K\n  Fornwalt", "title": "A deep neural network to enhance prediction of 1-year mortality using\n  echocardiographic videos of the heart", "comments": "We updated results with improved performance after dropout bug in\n  tensorflow v1.12. We also added learning curves showing promise in video\n  model with more samples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future clinical events helps physicians guide appropriate\nintervention. Machine learning has tremendous promise to assist physicians with\npredictions based on the discovery of complex patterns from historical data,\nsuch as large, longitudinal electronic health records (EHR). This study is a\nfirst attempt to demonstrate such capabilities using raw echocardiographic\nvideos of the heart. We show that a large dataset of 723,754\nclinically-acquired echocardiographic videos (~45 million images) linked to\nlongitudinal follow-up data in 27,028 patients can be used to train a deep\nneural network to predict 1-year mortality with good accuracy (area under the\ncurve (AUC) in an independent test set = 0.839). Prediction accuracy was\nfurther improved by adding EHR data (AUC = 0.858). Finally, we demonstrate that\nthe trained neural network was more accurate in mortality prediction than two\nexpert cardiologists. These results highlight the potential of neural networks\nto add new power to clinical predictions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 17:58:57 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 20:36:00 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Ulloa", "Alvaro", ""], ["Jing", "Linyuan", ""], ["Good", "Christopher W", ""], ["vanMaanen", "David P", ""], ["Raghunath", "Sushravya", ""], ["Suever", "Jonathan D", ""], ["Nevius", "Christopher D", ""], ["Wehner", "Gregory J", ""], ["Hartzel", "Dustin", ""], ["Leader", "Joseph B", ""], ["Alsaid", "Amro", ""], ["Patel", "Aalpen A", ""], ["Kirchner", "H Lester", ""], ["Pattichis", "Marios S", ""], ["Haggerty", "Christopher M", ""], ["Fornwalt", "Brandon K", ""]]}, {"id": "1811.10559", "submitter": "Pravendra Singh", "authors": "Pravendra Singh, Vinay Kumar Verma, Piyush Rai and Vinay P. Namboodiri", "title": "Leveraging Filter Correlations for Deep Model Compression", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a filter correlation based model compression approach for deep\nconvolutional neural networks. Our approach iteratively identifies pairs of\nfilters with the largest pairwise correlations and drops one of the filters\nfrom each such pair. However, instead of discarding one of the filters from\neach such pair na\\\"{i}vely, the model is re-optimized to make the filters in\nthese pairs maximally correlated, so that discarding one of the filters from\nthe pair results in minimal information loss. Moreover, after discarding the\nfilters in each round, we further finetune the model to recover from the\npotential small loss incurred by the compression. We evaluate our proposed\napproach using a comprehensive set of experiments and ablation studies. Our\ncompression method yields state-of-the-art FLOPs compression rates on various\nbenchmarks, such as LeNet-5, VGG-16, and ResNet-50,56, while still achieving\nexcellent predictive performance for tasks such as object detection on\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 18:05:18 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 20:16:50 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Singh", "Pravendra", ""], ["Verma", "Vinay Kumar", ""], ["Rai", "Piyush", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1811.10561", "submitter": "Jerome Abdelnour", "authors": "Jerome Abdelnour, Giampiero Salvi, Jean Rouat", "title": "CLEAR: A Dataset for Compositional Language and Elementary Acoustic\n  Reasoning", "comments": "NeurIPS 2018 Visually Grounded Interaction and Language (ViGIL)\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the task of acoustic question answering (AQA) in the area of\nacoustic reasoning. In this task an agent learns to answer questions on the\nbasis of acoustic context. In order to promote research in this area, we\npropose a data generation paradigm adapted from CLEVR (Johnson et al. 2017). We\ngenerate acoustic scenes by leveraging a bank elementary sounds. We also\nprovide a number of functional programs that can be used to compose questions\nand answers that exploit the relationships between the attributes of the\nelementary sounds in each scene. We provide AQA datasets of various sizes as\nwell as the data generation code. As a preliminary experiment to validate our\ndata, we report the accuracy of current state of the art visual question\nanswering models when they are applied to the AQA task without modifications.\nAlthough there is a plethora of question answering tasks based on text, image\nor video data, to our knowledge, we are the first to propose answering\nquestions directly on audio streams. We hope this contribution will facilitate\nthe development of research in the area.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 18:06:36 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Abdelnour", "Jerome", ""], ["Salvi", "Giampiero", ""], ["Rouat", "Jean", ""]]}, {"id": "1811.10564", "submitter": "Chenyu You", "authors": "Chenyu You, Linfeng Yang, Yi Zhang, Ge Wang", "title": "Low-Dose CT via Deep CNN with Skip Connection and Network in Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in computed tomography (CT) is how to minimize patient\nradiation exposure without compromising image quality and diagnostic\nperformance. The use of deep convolutional (Conv) neural networks for noise\nreduction in Low-Dose CT (LDCT) images has recently shown a great potential in\nthis important application. In this paper, we present a highly efficient and\neffective neural network model for LDCT image noise reduction. Specifically, to\ncapture local anatomical features we integrate Deep Convolutional Neural\nNetworks (CNNs) and Skip connection layers for feature extraction. Also, we\nintroduce parallelized $1\\times 1$ CNN, called Network in Network, to lower the\ndimensionality of the output from the previous layer, achieving faster\ncomputational speed at less feature loss. To optimize the performance of the\nnetwork, we adopt a Wasserstein generative adversarial network (WGAN)\nframework. Quantitative and qualitative comparisons demonstrate that our\nproposed network model can produce images with lower noise and more structural\ndetails than state-of-the-art noise-reduction methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 18:08:44 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 02:53:26 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["You", "Chenyu", ""], ["Yang", "Linfeng", ""], ["Zhang", "Yi", ""], ["Wang", "Ge", ""]]}, {"id": "1811.10574", "submitter": "Federico Galatolo", "authors": "Federico A. Galatolo, Mario G.C.A. Cimino, Gigliola Vaglini", "title": "Using stigmergy to incorporate the time into artificial neural networks", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-05918-7_22", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A current research trend in neurocomputing involves the design of novel\nartificial neural networks incorporating the concept of time into their\noperating model. In this paper, a novel architecture that employs stigmergy is\nproposed. Computational stigmergy is used to dynamically increase (or decrease)\nthe strength of a connection, or the activation level, of an artificial neuron\nwhen stimulated (or released). This study lays down a basic framework for the\nderivation of a stigmergic NN with a related training algorithm. To show its\npotential, some pilot experiments have been reported. The XOR problem is solved\nby using only one single stigmergic neuron with one input and one output. A\nstatic NN, a stigmergic NN, a recurrent NN and a long short-term memory NN have\nbeen trained to solve the MNIST digits recognition benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 00:12:11 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Galatolo", "Federico A.", ""], ["Cimino", "Mario G. C. A.", ""], ["Vaglini", "Gigliola", ""]]}, {"id": "1811.10581", "submitter": "Nishanth Dikkala", "authors": "Constantinos Daskalakis and Nishanth Dikkala and Siddhartha Jayanti", "title": "HOGWILD!-Gibbs can be PanAccurate", "comments": "19 pages, 3 figures, published at NeurIPS2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous Gibbs sampling has been recently shown to be fast-mixing and an\naccurate method for estimating probabilities of events on a small number of\nvariables of a graphical model satisfying Dobrushin's\ncondition~\\cite{DeSaOR16}. We investigate whether it can be used to accurately\nestimate expectations of functions of {\\em all the variables} of the model.\nUnder the same condition, we show that the synchronous (sequential) and\nasynchronous Gibbs samplers can be coupled so that the expected Hamming\ndistance between their (multivariate) samples remains bounded by $O(\\tau \\log\nn),$ where $n$ is the number of variables in the graphical model, and $\\tau$ is\na measure of the asynchronicity. A similar bound holds for any constant power\nof the Hamming distance. Hence, the expectation of any function that is\nLipschitz with respect to a power of the Hamming distance, can be estimated\nwith a bias that grows logarithmically in $n$. Going beyond Lipschitz\nfunctions, we consider the bias arising from asynchronicity in estimating the\nexpectation of polynomial functions of all variables in the model. Using recent\nconcentration of measure results, we show that the bias introduced by the\nasynchronicity is of smaller order than the standard deviation of the function\nvalue already present in the true model. We perform experiments on a\nmulti-processor machine to empirically illustrate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 18:36:51 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Dikkala", "Nishanth", ""], ["Jayanti", "Siddhartha", ""]]}, {"id": "1811.10597", "submitter": "David Bau iii", "authors": "David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B.\n  Tenenbaum, William T. Freeman, Antonio Torralba", "title": "GAN Dissection: Visualizing and Understanding Generative Adversarial\n  Networks", "comments": "18 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have recently achieved impressive\nresults for many real-world applications, and many GAN variants have emerged\nwith improvements in sample quality and training stability. However, they have\nnot been well visualized or understood. How does a GAN represent our visual\nworld internally? What causes the artifacts in GAN results? How do\narchitectural choices affect GAN learning? Answering such questions could\nenable us to develop new insights and better models.\n  In this work, we present an analytic framework to visualize and understand\nGANs at the unit-, object-, and scene-level. We first identify a group of\ninterpretable units that are closely related to object concepts using a\nsegmentation-based network dissection method. Then, we quantify the causal\neffect of interpretable units by measuring the ability of interventions to\ncontrol objects in the output. We examine the contextual relationship between\nthese units and their surroundings by inserting the discovered object concepts\ninto new images. We show several practical applications enabled by our\nframework, from comparing internal representations across different layers,\nmodels, and datasets, to improving GANs by locating and removing\nartifact-causing units, to interactively manipulating objects in a scene. We\nprovide open source interpretation tools to help researchers and practitioners\nbetter understand their GAN models.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 18:59:07 GMT"}, {"version": "v2", "created": "Sat, 8 Dec 2018 22:56:10 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Bau", "David", ""], ["Zhu", "Jun-Yan", ""], ["Strobelt", "Hendrik", ""], ["Zhou", "Bolei", ""], ["Tenenbaum", "Joshua B.", ""], ["Freeman", "William T.", ""], ["Torralba", "Antonio", ""]]}, {"id": "1811.10636", "submitter": "Michael S. Ryoo", "authors": "AJ Piergiovanni, Anelia Angelova, Alexander Toshev, Michael S. Ryoo", "title": "Evolving Space-Time Neural Architectures for Videos", "comments": null, "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for finding video CNN architectures that capture rich\nspatio-temporal information in videos. Previous work, taking advantage of 3D\nconvolutions, obtained promising results by manually designing video CNN\narchitectures. We here develop a novel evolutionary search algorithm that\nautomatically explores models with different types and combinations of layers\nto jointly learn interactions between spatial and temporal aspects of video\nrepresentations. We demonstrate the generality of this algorithm by applying it\nto two meta-architectures, obtaining new architectures superior to manually\ndesigned architectures. Further, we propose a new component, the iTGM layer,\nwhich more efficiently utilizes its parameters to allow learning of space-time\ninteractions over longer time horizons. The iTGM layer is often preferred by\nthe evolutionary algorithm and allows building cost-efficient networks. The\nproposed approach discovers new and diverse video architectures that were\npreviously unknown. More importantly they are both more accurate and faster\nthan prior models, and outperform the state-of-the-art results on multiple\ndatasets we test, including HMDB, Kinetics, and Moments in Time. We will open\nsource the code and models, to encourage future model development.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 19:00:12 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 18:17:46 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Angelova", "Anelia", ""], ["Toshev", "Alexander", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "1811.10649", "submitter": "Minghai Qin", "authors": "Minghai Qin, Dejan Vucinic", "title": "Noisy Computations during Inference: Harmful or Helpful?", "comments": "20 pages, 11 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two aspects of noisy computations during inference. The first aspect\nis how to mitigate their side effects for naturally trained deep learning\nsystems. One of the motivations for looking into this problem is to reduce the\nhigh power cost of conventional computing of neural networks through the use of\nanalog neuromorphic circuits. Traditional GPU/CPU-centered deep learning\narchitectures exhibit bottlenecks in power-restricted applications (e.g.,\nembedded systems). The use of specialized neuromorphic circuits, where analog\nsignals passed through memory-cell arrays are sensed to accomplish\nmatrix-vector multiplications, promises large power savings and speed gains but\nbrings with it the problems of limited precision of computations and\nunavoidable analog noise. We manage to improve inference accuracy from 21.1% to\n99.5% for MNIST images, from 29.9% to 89.1% for CIFAR10, and from 15.5% to\n89.6% for MNIST stroke sequences with the presence of strong noise (with\nsignal-to-noise power ratio being 0 dB) by noise-injected training and a voting\nmethod. This observation promises neural networks that are insensitive to\ninference noise, which reduces the quality requirements on neuromorphic\ncircuits and is crucial for their practical usage. The second aspect is how to\nutilize the noisy inference as a defensive architecture against black-box\nadversarial attacks. During inference, by injecting proper noise to signals in\nthe neural networks, the robustness of adversarially-trained neural networks\nagainst black-box attacks has been further enhanced by 0.5% and 1.13% for two\nadversarially trained models for MNIST and CIFAR10, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 19:18:18 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Qin", "Minghai", ""], ["Vucinic", "Dejan", ""]]}, {"id": "1811.10658", "submitter": "Kyle Brown", "authors": "Kyle Brown, Derek Doran, Ryan Kramer, Brad Reynolds", "title": "HELOC Applicant Risk Performance Evaluation by Topological Hierarchical\n  Decomposition", "comments": "10 pages, 4 figures, to be published in the NIPS 2018 Workshop on\n  Challenges and Opportunities for AI in Financial Services: the Impact of\n  Fairness, Explainability, Accuracy, and Privacy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strong regulations in the financial industry mean that any decisions based on\nmachine learning need to be explained. This precludes the use of powerful\nsupervised techniques such as neural networks. In this study we propose a new\nunsupervised and semi-supervised technique known as the topological\nhierarchical decomposition (THD). This process breaks a dataset down into ever\nsmaller groups, where groups are associated with a simplicial complex that\napproximate the underlying topology of a dataset. We apply THD to the FICO\nmachine learning challenge dataset, consisting of anonymized home equity loan\napplications using the MAPPER algorithm to build simplicial complexes. We\nidentify different groups of individuals unable to pay back loans, and\nillustrate how the distribution of feature values in a simplicial complex can\nbe used to explain the decision to grant or deny a loan by extracting\nillustrative explanations from two THDs on the dataset.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 19:35:11 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Brown", "Kyle", ""], ["Doran", "Derek", ""], ["Kramer", "Ryan", ""], ["Reynolds", "Brad", ""]]}, {"id": "1811.10665", "submitter": "Christopher Rosin", "authors": "Christopher D. Rosin", "title": "Stepping Stones to Inductive Synthesis of Low-Level Looping Programs", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inductive program synthesis, from input/output examples, can provide an\nopportunity to automatically create programs from scratch without presupposing\nthe algorithmic form of the solution. For induction of general programs with\nloops (as opposed to loop-free programs, or synthesis for domain-specific\nlanguages), the state of the art is at the level of introductory programming\nassignments. Most problems that require algorithmic subtlety, such as fast\nsorting, have remained out of reach without the benefit of significant\nproblem-specific background knowledge. A key challenge is to identify cues that\nare available to guide search towards correct looping programs. We present\nMAKESPEARE, a simple delayed-acceptance hillclimbing method that synthesizes\nlow-level looping programs from input/output examples. During search, delayed\nacceptance bypasses small gains to identify significantly-improved stepping\nstone programs that tend to generalize and enable further progress. The method\nperforms well on a set of established benchmarks, and succeeds on the\npreviously unsolved \"Collatz Numbers\" program synthesis problem. Additional\nbenchmarks include the problem of rapidly sorting integer arrays, in which we\nobserve the emergence of comb sort (a Shell sort variant that is empirically\nfast). MAKESPEARE has also synthesized a record-setting program on one of the\npuzzles from the TIS-100 assembly language programming game.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 19:51:27 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Rosin", "Christopher D.", ""]]}, {"id": "1811.10678", "submitter": "Navin Anwani", "authors": "Navin Anwani and Bipin Rajendran", "title": "Training Multi-layer Spiking Neural Networks using NormAD based\n  Spatio-Temporal Error Backpropagation", "comments": "19 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) have garnered a great amount of interest for\nsupervised and unsupervised learning applications. This paper deals with the\nproblem of training multi-layer feedforward SNNs. The non-linear\nintegrate-and-fire dynamics employed by spiking neurons make it difficult to\ntrain SNNs to generate desired spike trains in response to a given input. To\ntackle this, first the problem of training a multi-layer SNN is formulated as\nan optimization problem such that its objective function is based on the\ndeviation in membrane potential rather than the spike arrival instants. Then,\nan optimization method named Normalized Approximate Descent (NormAD),\nhand-crafted for such non-convex optimization problems, is employed to derive\nthe iterative synaptic weight update rule. Next, it is reformulated to\nefficiently train multi-layer SNNs, and is shown to be effectively performing\nspatio-temporal error backpropagation. The learning rule is validated by\ntraining $2$-layer SNNs to solve a spike based formulation of the XOR problem\nas well as training $3$-layer SNNs for generic spike based training problems.\nThus, the new algorithm is a key step towards building deep spiking neural\nnetworks capable of efficient event-triggered learning.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 19:19:44 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 03:37:40 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Anwani", "Navin", ""], ["Rajendran", "Bipin", ""]]}, {"id": "1811.10689", "submitter": "Ieva Kazlauskaite", "authors": "Ieva Kazlauskaite, Ivan Ustyuzhaninov, Carl Henrik Ek, Neill D. F.\n  Campbell", "title": "Sequence Alignment with Dirichlet Process Mixtures", "comments": "6 pages, 3 figures, \"All Of Bayesian Nonparametrics\" Workshop at the\n  32nd Annual Conference on Neural Information Processing Systems\n  (BNP@NeurIPS2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic model for unsupervised alignment of\nhigh-dimensional time-warped sequences based on the Dirichlet Process Mixture\nModel (DPMM). We follow the approach introduced in (Kazlauskaite, 2018) of\nsimultaneously representing each data sequence as a composition of a true\nunderlying function and a time-warping, both of which are modelled using\nGaussian processes (GPs) (Rasmussen, 2005), and aligning the underlying\nfunctions using an unsupervised alignment method. In (Kazlauskaite, 2018) the\nalignment is performed using the GP latent variable model (GP-LVM) (Lawrence,\n2005) as a model of sequences, while our main contribution is extending this\napproach to using DPMM, which allows us to align the sequences temporally and\ncluster them at the same time. We show that the DPMM achieves competitive\nresults in comparison to the GP-LVM on synthetic and real-world data sets, and\ndiscuss the different properties of the estimated underlying functions and the\ntime-warps favoured by these models.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 21:08:54 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Kazlauskaite", "Ieva", ""], ["Ustyuzhaninov", "Ivan", ""], ["Ek", "Carl Henrik", ""], ["Campbell", "Neill D. F.", ""]]}, {"id": "1811.10708", "submitter": "Marcel Lederle", "authors": "Marcel Lederle and Benjamin Wilhelm", "title": "Combining High-Level Features of Raw Audio Waves and Mel-Spectrograms\n  for Audio Tagging", "comments": "Detection and Classification of Acoustic Scenes and Events 2018\n  (DCASE 2018), 19-20 November 2018, Surrey, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we describe our contribution to Task 2 of the DCASE 2018 Audio\nChallenge. While it has become ubiquitous to utilize an ensemble of machine\nlearning methods for classification tasks to obtain better predictive\nperformance, the majority of ensemble methods combine predictions rather than\nlearned features. We propose a single-model method that combines learned\nhigh-level features computed from log-scaled mel-spectrograms and raw audio\ndata. These features are learned separately by two Convolutional Neural\nNetworks, one for each input type, and then combined by densely connected\nlayers within a single network. This relatively simple approach along with data\naugmentation ranks among the best two percent in the Freesound General-Purpose\nAudio Tagging Challenge on Kaggle.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 22:02:20 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Lederle", "Marcel", ""], ["Wilhelm", "Benjamin", ""]]}, {"id": "1811.10714", "submitter": "Taylor Killian", "authors": "Justin A. Goodwin, Olivia M. Brown, Taylor W. Killian, Sung-Hyun Son", "title": "Learning Robust Representations for Automatic Target Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radio frequency (RF) sensors are used alongside other sensing modalities to\nprovide rich representations of the world. Given the high variability of\ncomplex-valued target responses, RF systems are susceptible to attacks masking\ntrue target characteristics from accurate identification. In this work, we\nevaluate different techniques for building robust classification architectures\nexploiting learned physical structure in received synthetic aperture radar\nsignals of simulated 3D targets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 22:08:21 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Goodwin", "Justin A.", ""], ["Brown", "Olivia M.", ""], ["Killian", "Taylor W.", ""], ["Son", "Sung-Hyun", ""]]}, {"id": "1811.10732", "submitter": "Shagun Sodhani", "authors": "Khimya Khetarpal, Shagun Sodhani, Sarath Chandar, Doina Precup", "title": "Environments for Lifelong Reinforcement Learning", "comments": "Accepted at 2nd Continual Learning Workshop, Neural Information\n  Processing Systems (NeurIPS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve general artificial intelligence, reinforcement learning (RL)\nagents should learn not only to optimize returns for one specific task but also\nto constantly build more complex skills and scaffold their knowledge about the\nworld, without forgetting what has already been learned. In this paper, we\ndiscuss the desired characteristics of environments that can support the\ntraining and evaluation of lifelong reinforcement learning agents, review\nexisting environments from this perspective, and propose recommendations for\ndevising suitable environments in the future.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 23:01:49 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 06:17:16 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Khetarpal", "Khimya", ""], ["Sodhani", "Shagun", ""], ["Chandar", "Sarath", ""], ["Precup", "Doina", ""]]}, {"id": "1811.10734", "submitter": "Palash Goyal", "authors": "Palash Goyal, Sujit Rokka Chhetri, Ninareh Mehrabi, Emilio Ferrara,\n  Arquimedes Canedo", "title": "DynamicGEM: A Library for Dynamic Graph Embedding Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DynamicGEM is an open-source Python library for learning node representations\nof dynamic graphs. It consists of state-of-the-art algorithms for defining\nembeddings of nodes whose connections evolve over time. The library also\ncontains the evaluation framework for four downstream tasks on the network:\ngraph reconstruction, static and temporal link prediction, node classification,\nand temporal visualization. We have implemented various metrics to evaluate the\nstate-of-the-art methods, and examples of evolving networks from various\ndomains. We have easy-to-use functions to call and evaluate the methods and\nhave extensive usage documentation. Furthermore, DynamicGEM provides a template\nto add new algorithms with ease to facilitate further research on the topic.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 23:05:38 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Goyal", "Palash", ""], ["Chhetri", "Sujit Rokka", ""], ["Mehrabi", "Ninareh", ""], ["Ferrara", "Emilio", ""], ["Canedo", "Arquimedes", ""]]}, {"id": "1811.10735", "submitter": "Chapman Siu", "authors": "Chapman Siu", "title": "Automatic Induction of Neural Network Decision Tree Algorithms", "comments": "This is a pre-print of a contribution \"Chapman Siu, Automatic\n  Induction of Neural Network Decision Tree Algorithms.\" To appear in Computing\n  Conference 2019 Proceedings. Advances in Intelligent Systems and Computing.\n  Implementation:\n  https://github.com/chappers/automatic-induction-neural-decision-tree", "journal-ref": null, "doi": "10.1007/978-3-030-22871-2_48", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an approach to automatically induction for non-greedy\ndecision trees constructed from neural network architecture. This construction\ncan be used to transfer weights when growing or pruning a decision tree,\nallowing non-greedy decision tree algorithms to automatically learn and adapt\nto the ideal architecture. In this work, we examine the underpinning ideas\nwithin ensemble modelling and Bayesian model averaging which allow our neural\nnetwork to asymptotically approach the ideal architecture through weights\ntransfer. Experimental results demonstrate that this approach improves models\nover fixed set of hyperparameters for decision tree models and decision forest\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 23:06:38 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 11:51:26 GMT"}, {"version": "v3", "created": "Tue, 29 Jan 2019 03:09:17 GMT"}, {"version": "v4", "created": "Thu, 25 Apr 2019 03:21:04 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Siu", "Chapman", ""]]}, {"id": "1811.10736", "submitter": "Loren Lugosch", "authors": "Loren Lugosch, Samuel Myer, Vikrant Singh Tomar", "title": "DONUT: CTC-based Query-by-Example Keyword Spotting", "comments": "Accepted to NeurIPS 2018 Workshop on Interpretability and Robustness\n  for Audio, Speech, and Language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword spotting--or wakeword detection--is an essential feature for\nhands-free operation of modern voice-controlled devices. With such devices\nbecoming ubiquitous, users might want to choose a personalized custom wakeword.\nIn this work, we present DONUT, a CTC-based algorithm for online\nquery-by-example keyword spotting that enables custom wakeword detection. The\nalgorithm works by recording a small number of training examples from the user,\ngenerating a set of label sequence hypotheses from these training examples, and\ndetecting the wakeword by aggregating the scores of all the hypotheses given a\nnew audio recording. Our method combines the generalization and\ninterpretability of CTC-based keyword spotting with the user-adaptation and\nconvenience of a conventional query-by-example system. DONUT has low\ncomputational requirements and is well-suited for both learning and inference\non embedded systems without requiring private user data to be uploaded to the\ncloud.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 23:13:25 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Lugosch", "Loren", ""], ["Myer", "Samuel", ""], ["Tomar", "Vikrant Singh", ""]]}, {"id": "1811.10740", "submitter": "Subba Reddy Oota", "authors": "Subba Reddy Oota, Adithya Avvaru, Naresh Manwani, Raju S. Bapi", "title": "Mixture of Regression Experts in fMRI Encoding", "comments": "8 pages, 3 figures, Workshop on Visually Grounded Interaction and\n  Language @ 32nd Conference on Neural Information Processing Systems (NeurIPS\n  2018), Montr\\'eal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  fMRI semantic category understanding using linguistic encoding models attempt\nto learn a forward mapping that relates stimuli to the corresponding brain\nactivation. Classical encoding models use linear multi-variate methods to\npredict the brain activation (all voxels) given the stimulus. However, these\nmethods essentially assume multiple regions as one large uniform region or\nseveral independent regions, ignoring connections among them. In this paper, we\npresent a mixture of experts-based model where a group of experts captures\nbrain activity patterns related to particular regions of interest (ROI) and\nalso show the discrimination across different experts. The model is trained\nword stimuli encoded as 25-dimensional feature vectors as input and the\ncorresponding brain responses as output. Given a new word (25-dimensional\nfeature vector), it predicts the entire brain activation as the linear\ncombination of multiple experts brain activations. We argue that each expert\nlearns a certain region of brain activations corresponding to its category of\nwords, which solves the problem of identifying the regions with a simple\nencoding model. We showcase that proposed mixture of experts-based model indeed\nlearns region-based experts to predict the brain activations with high spatial\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 23:21:30 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 17:14:03 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Oota", "Subba Reddy", ""], ["Avvaru", "Adithya", ""], ["Manwani", "Naresh", ""], ["Bapi", "Raju S.", ""]]}, {"id": "1811.10745", "submitter": "Bao Wang", "authors": "Bao Wang and Binjie Yuan and Zuoqiang Shi and Stanley J. Osher", "title": "ResNets Ensemble via the Feynman-Kac Formalism to Improve Natural and\n  Robust Accuracies", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical adversarial risk minimization (EARM) is a widely used mathematical\nframework to robustly train deep neural nets (DNNs) that are resistant to\nadversarial attacks. However, both natural and robust accuracies, in\nclassifying clean and adversarial images, respectively, of the trained robust\nmodels are far from satisfactory. In this work, we unify the theory of optimal\ncontrol of transport equations with the practice of training and testing of\nResNets. Based on this unified viewpoint, we propose a simple yet effective\nResNets ensemble algorithm to boost the accuracy of the robustly trained model\non both clean and adversarial images. The proposed algorithm consists of two\ncomponents: First, we modify the base ResNets by injecting a variance specified\nGaussian noise to the output of each residual mapping. Second, we average over\nthe production of multiple jointly trained modified ResNets to get the final\nprediction. These two steps give an approximation to the Feynman-Kac formula\nfor representing the solution of a transport equation with viscosity, or a\nconvection-diffusion equation. For the CIFAR10 benchmark, this simple algorithm\nleads to a robust model with a natural accuracy of {\\bf 85.62}\\% on clean\nimages and a robust accuracy of ${\\bf 57.94 \\%}$ under the 20 iterations of the\nIFGSM attack, which outperforms the current state-of-the-art in defending\nagainst IFGSM attack on the CIFAR10. Both natural and robust accuracies of the\nproposed ResNets ensemble can be improved dynamically as the building block\nResNet advances. The code is available at:\n\\url{https://github.com/BaoWangMath/EnResNet}.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 23:46:09 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 05:07:37 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Wang", "Bao", ""], ["Yuan", "Binjie", ""], ["Shi", "Zuoqiang", ""], ["Osher", "Stanley J.", ""]]}, {"id": "1811.10746", "submitter": "Daniel Jarrett", "authors": "Daniel Jarrett, Jinsung Yoon, Mihaela van der Schaar", "title": "MATCH-Net: Dynamic Prediction in Survival Analysis using Convolutional\n  Neural Networks", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/36", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate prediction of disease trajectories is critical for early\nidentification and timely treatment of patients at risk. Conventional methods\nin survival analysis are often constrained by strong parametric assumptions and\nlimited in their ability to learn from high-dimensional data, while existing\nneural network models are not readily-adapted to the longitudinal setting. This\npaper develops a novel convolutional approach that addresses these drawbacks.\nWe present MATCH-Net: a Missingness-Aware Temporal Convolutional Hitting-time\nNetwork, designed to capture temporal dependencies and heterogeneous\ninteractions in covariate trajectories and patterns of missingness. To the best\nof our knowledge, this is the first investigation of temporal convolutions in\nthe context of dynamic prediction for personalized risk prognosis. Using\nreal-world data from the Alzheimer's Disease Neuroimaging Initiative, we\ndemonstrate state-of-the-art performance without making any assumptions\nregarding underlying longitudinal or time-to-event processes attesting to the\nmodel's potential utility in clinical decision support.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 23:50:24 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Jarrett", "Daniel", ""], ["Yoon", "Jinsung", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1811.10751", "submitter": "Sanghamitra Dutta", "authors": "Sanghamitra Dutta, Ziqian Bai, Haewon Jeong, Tze Meng Low and Pulkit\n  Grover", "title": "A Unified Coded Deep Neural Network Training Strategy Based on\n  Generalized PolyDot Codes for Matrix Multiplication", "comments": "Presented in part at the IEEE International Symposium on Information\n  Theory 2018 (Submission Date: Jan 12 2018); Currently under review at the\n  IEEE Transactions on Information Theory", "journal-ref": null, "doi": "10.1109/ISIT.2018.8437852", "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has two contributions. First, we propose a novel coded matrix\nmultiplication technique called Generalized PolyDot codes that advances on\nexisting methods for coded matrix multiplication under storage and\ncommunication constraints. This technique uses \"garbage alignment,\" i.e.,\naligning computations in coded computing that are not a part of the desired\noutput. Generalized PolyDot codes bridge between Polynomial codes and MatDot\ncodes, trading off between recovery threshold and communication costs. Second,\nwe demonstrate that Generalized PolyDot can be used for training large Deep\nNeural Networks (DNNs) on unreliable nodes prone to soft-errors. This requires\nus to address three additional challenges: (i) prohibitively large overhead of\ncoding the weight matrices in each layer of the DNN at each iteration; (ii)\nnonlinear operations during training, which are incompatible with linear\ncoding; and (iii) not assuming presence of an error-free master node, requiring\nus to architect a fully decentralized implementation without any \"single point\nof failure.\" We allow all primary DNN training steps, namely, matrix\nmultiplication, nonlinear activation, Hadamard product, and update steps as\nwell as the encoding/decoding to be error-prone. We consider the case of\nmini-batch size $B=1$, as well as $B>1$, leveraging coded matrix-vector\nproducts, and matrix-matrix products respectively. The problem of DNN training\nunder soft-errors also motivates an interesting, probabilistic error model\nunder which a real number $(P,Q)$ MDS code is shown to correct $P-Q-1$ errors\nwith probability $1$ as compared to $\\lfloor \\frac{P-Q}{2} \\rfloor$ for the\nmore conventional, adversarial error model. We also demonstrate that our\nproposed strategy can provide unbounded gains in error tolerance over a\ncompeting replication strategy and a preliminary MDS-code-based strategy for\nboth these error models.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 00:06:47 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Dutta", "Sanghamitra", ""], ["Bai", "Ziqian", ""], ["Jeong", "Haewon", ""], ["Low", "Tze Meng", ""], ["Grover", "Pulkit", ""]]}, {"id": "1811.10789", "submitter": "Enya Shen", "authors": "Enya Shen, Zhidong Cao, Changqing Zou, Jianmin Wang", "title": "Flexible Attributed Network Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network embedding aims to find a way to encode network by learning an\nembedding vector for each node in the network. The network often has property\ninformation which is highly informative with respect to the node's position and\nrole in the network. Most network embedding methods fail to utilize this\ninformation during network representation learning. In this paper, we propose a\nnovel framework, FANE, to integrate structure and property information in the\nnetwork embedding process. In FANE, we design a network to unify heterogeneity\nof the two information sources, and define a new random walking strategy to\nleverage property information and make the two information compensate. FANE is\nconceptually simple and empirically powerful. It improves over the\nstate-of-the-art methods on Cora dataset classification task by over 5%, more\nthan 10% on WebKB dataset classification task. Experiments also show that the\nresults improve more than the state-of-the-art methods as increasing training\nsize. Moreover, qualitative visualization show that our framework is helpful in\nnetwork property information exploration. In all, we present a new way for\nefficiently learning state-of-the-art task-independent representations in\ncomplex attributed networks. The source code and datasets of this paper can be\nobtained from https://github.com/GraphWorld/FANE.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 03:28:51 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Shen", "Enya", ""], ["Cao", "Zhidong", ""], ["Zou", "Changqing", ""], ["Wang", "Jianmin", ""]]}, {"id": "1811.10790", "submitter": "Sen Na", "authors": "Sen Na, Mladen Kolar", "title": "High-dimensional Index Volatility Models via Stein's Identity", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the estimation of the parametric components of single and multiple\nindex volatility models. Using the first- and second-order Stein's identities,\nwe develop methods that are applicable for the estimation of the variance index\nin the high-dimensional setting requiring finite moment condition, which allows\nfor heavy-tailed data. Our approach complements the existing literature in the\nlow-dimensional setting, while relaxing the conditions on estimation, and\nprovides a novel approach in the high-dimensional setting. We prove that the\nstatistical rate of convergence of our variance index estimators consists of a\nparametric rate and a nonparametric rate, where the latter appears from the\nestimation of the mean link function. However, under standard assumptions, the\nparametric rate dominates the rate of convergence and our results match the\nminimax optimal rate for the mean index estimation. Simulation results\nillustrate finite sample properties of our methodology and back our theoretical\nconclusions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 03:32:03 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 16:40:28 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 21:22:52 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Na", "Sen", ""], ["Kolar", "Mladen", ""]]}, {"id": "1811.10791", "submitter": "Wenqian Ronny Huang", "authors": "W. Ronny Huang and Miguel A. Perez", "title": "Accurate, Data-Efficient Learning from Noisy, Choice-Based Labels for\n  Inherent Risk Scoring", "comments": "Presented as an oral at the NIPS 2018 Workshop on Challenges and\n  Opportunities for AI in Financial Services: the Impact of Fairness,\n  Explainability, Accuracy, and Privacy (FEAP-AI4Fin 2018). 9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inherent risk scoring is an important function in anti-money laundering, used\nfor determining the riskiness of an individual during onboarding\n$\\textit{before}$ fraudulent transactions occur. It is, however, often fraught\nwith two challenges: (1) inconsistent notions of what constitutes as high or\nlow risk by experts and (2) the lack of labeled data. This paper explores a new\nparadigm of data labeling and data collection to tackle these issues. The data\nlabeling is choice-based; the expert does not provide an absolute risk score\nbut merely chooses the most/least risky example out of a small choice set,\nwhich reduces inconsistency because experts make only relative judgments of\nrisk. The data collection is synthetic; examples are crafted using optimal\nexperimental design methods, obviating the need for real data which is often\ndifficult to obtain due to regulatory concerns. We present the methodology of\nan end-to-end inherent risk scoring algorithm that we built for a large\nfinancial institution. The system was trained on a small set of synthetic data\n(188 examples, 24 features) whose labels are obtained via the choice-based\nparadigm using an efficient number of expert labelers. The system achieves 89%\naccuracy on a test set of 52 examples, with an area under the ROC curve of 93%.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 03:38:54 GMT"}], "update_date": "2018-12-02", "authors_parsed": [["Huang", "W. Ronny", ""], ["Perez", "Miguel A.", ""]]}, {"id": "1811.10792", "submitter": "Mahmoud Assran", "authors": "Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, Michael Rabbat", "title": "Stochastic Gradient Push for Distributed Deep Learning", "comments": "ICML 2019", "journal-ref": "International Conference on Machine Learning 97 (2019) 344-353", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.MA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed data-parallel algorithms aim to accelerate the training of deep\nneural networks by parallelizing the computation of large mini-batch gradient\nupdates across multiple nodes. Approaches that synchronize nodes using exact\ndistributed averaging (e.g., via AllReduce) are sensitive to stragglers and\ncommunication delays. The PushSum gossip algorithm is robust to these issues,\nbut only performs approximate distributed averaging. This paper studies\nStochastic Gradient Push (SGP), which combines PushSum with stochastic gradient\nupdates. We prove that SGP converges to a stationary point of smooth,\nnon-convex objectives at the same sub-linear rate as SGD, and that all nodes\nachieve consensus. We empirically validate the performance of SGP on image\nclassification (ResNet-50, ImageNet) and machine translation (Transformer,\nWMT'16 En-De) workloads. Our code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 03:47:26 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 02:58:36 GMT"}, {"version": "v3", "created": "Tue, 14 May 2019 19:59:00 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Assran", "Mahmoud", ""], ["Loizou", "Nicolas", ""], ["Ballas", "Nicolas", ""], ["Rabbat", "Michael", ""]]}, {"id": "1811.10797", "submitter": "Dimitrios Berberidis", "authors": "Dimitris Berberidis and Georgios B. Giannakis", "title": "Node Embedding with Adaptive Similarities for Scalable Learning over\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Node embedding is the task of extracting informative and descriptive features\nover the nodes of a graph. The importance of node embeddings for graph\nanalytics, as well as learning tasks such as node classification, link\nprediction and community detection, has led to increased interest on the\nproblem leading to a number of recent advances. Much like PCA in the feature\ndomain, node embedding is an inherently \\emph{unsupervised} task; in lack of\nmetadata used for validation, practical methods may require standardization and\nlimiting the use of tunable hyperparameters. Finally, node embedding methods\nare faced with maintaining scalability in the face of large-scale real-world\ngraphs of ever-increasing sizes. In the present work, we propose an adaptive\nnode embedding framework that adjusts the embedding process to a given\nunderlying graph, in a fully unsupervised manner. To achieve this, we adopt the\nnotion of a tunable node similarity matrix that assigns weights on paths of\ndifferent length. The design of the multilength similarities ensures that the\nresulting embeddings also inherit interpretable spectral properties. The\nproposed model is carefully studied, interpreted, and numerically evaluated\nusing stochastic block models. Moreover, an algorithmic scheme is proposed for\ntraining the model parameters effieciently and in an unsupervised manner. We\nperform extensive node classification, link prediction, and clustering\nexperiments on many real world graphs from various domains, and compare with\nstate-of-the-art scalable and unsupervised node embedding alternatives. The\nproposed method enjoys superior performance in many cases, while also yielding\ninterpretable information on the underlying structure of the graph.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 04:15:14 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 20:52:13 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 21:29:09 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Berberidis", "Dimitris", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1811.10799", "submitter": "Nicholas Mastronarde", "authors": "Owen Lahav, Nicholas Mastronarde, Mihaela van der Schaar", "title": "What is Interpretable? Using Machine Learning to Design Interpretable\n  Decision-Support Systems", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/28", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent efforts in Machine Learning (ML) interpretability have focused on\ncreating methods for explaining black-box ML models. However, these methods\nrely on the assumption that simple approximations, such as linear models or\ndecision-trees, are inherently human-interpretable, which has not been\nempirically tested. Additionally, past efforts have focused exclusively on\ncomprehension, neglecting to explore the trust component necessary to convince\nnon-technical experts, such as clinicians, to utilize ML models in practice. In\nthis paper, we posit that reinforcement learning (RL) can be used to learn what\nis interpretable to different users and, consequently, build their trust in ML\nmodels. To validate this idea, we first train a neural network to provide risk\nassessments for heart failure patients. We then design a RL-based clinical\ndecision-support system (DSS) around the neural network model, which can learn\nfrom its interactions with users. We conduct an experiment involving a diverse\nset of clinicians from multiple institutions in three different countries. Our\nresults demonstrate that ML experts cannot accurately predict which system\noutputs will maximize clinicians' confidence in the underlying neural network\nmodel, and suggest additional findings that have broad implications to the\nfuture of research into ML interpretability and the use of ML in medicine.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 04:26:36 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 19:37:05 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Lahav", "Owen", ""], ["Mastronarde", "Nicholas", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1811.10811", "submitter": "Mahesh Subedar", "authors": "Mahesh Subedar, Ranganath Krishnan, Paulo Lopez Meyer, Omesh Tickoo,\n  Jonathan Huang", "title": "Uncertainty aware audiovisual activity recognition using deep Bayesian\n  variational inference", "comments": "Accepted at ICCV 2019 for Oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) provide state-of-the-art results for a multitude\nof applications, but the approaches using DNNs for multimodal audiovisual\napplications do not consider predictive uncertainty associated with individual\nmodalities. Bayesian deep learning methods provide principled confidence and\nquantify predictive uncertainty. Our contribution in this work is to propose an\nuncertainty aware multimodal Bayesian fusion framework for activity\nrecognition. We demonstrate a novel approach that combines deterministic and\nvariational layers to scale Bayesian DNNs to deeper architectures. Our\nexperiments using in- and out-of-distribution samples selected from a subset of\nMoments-in-Time (MiT) dataset show a more reliable confidence measure as\ncompared to the non-Bayesian baseline and the Monte Carlo dropout (MC dropout)\napproximate Bayesian inference. We also demonstrate the uncertainty estimates\nobtained from the proposed framework can identify out-of-distribution data on\nthe UCF101 and MiT datasets. In the multimodal setting, the proposed framework\nimproved precision-recall AUC by 10.2% on the subset of MiT dataset as compared\nto non-Bayesian baseline.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 04:51:54 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 06:01:04 GMT"}, {"version": "v3", "created": "Fri, 20 Sep 2019 05:35:30 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Subedar", "Mahesh", ""], ["Krishnan", "Ranganath", ""], ["Meyer", "Paulo Lopez", ""], ["Tickoo", "Omesh", ""], ["Huang", "Jonathan", ""]]}, {"id": "1811.10828", "submitter": "Quanquan Gu", "authors": "Jinghui Chen, Dongruo Zhou, Jinfeng Yi, Quanquan Gu", "title": "A Frank-Wolfe Framework for Efficient and Effective Adversarial Attacks", "comments": "25 pages, 1 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depending on how much information an adversary can access to, adversarial\nattacks can be classified as white-box attack and black-box attack. For\nwhite-box attack, optimization-based attack algorithms such as projected\ngradient descent (PGD) can achieve relatively high attack success rates within\nmoderate iterates. However, they tend to generate adversarial examples near or\nupon the boundary of the perturbation set, resulting in large distortion.\nFurthermore, their corresponding black-box attack algorithms also suffer from\nhigh query complexities, thereby limiting their practical usefulness. In this\npaper, we focus on the problem of developing efficient and effective\noptimization-based adversarial attack algorithms. In particular, we propose a\nnovel adversarial attack framework for both white-box and black-box settings\nbased on a variant of Frank-Wolfe algorithm. We show in theory that the\nproposed attack algorithms are efficient with an $O(1/\\sqrt{T})$ convergence\nrate. The empirical results of attacking the ImageNet and MNIST datasets also\nverify the efficiency and effectiveness of the proposed algorithms. More\nspecifically, our proposed algorithms attain the best attack performances in\nboth white-box and black-box attacks among all baselines, and are more time and\nquery efficient than the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 06:11:31 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 06:42:48 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Chen", "Jinghui", ""], ["Zhou", "Dongruo", ""], ["Yi", "Jinfeng", ""], ["Gu", "Quanquan", ""]]}, {"id": "1811.10829", "submitter": "Semih Cayci", "authors": "Semih Cayci, Atilla Eryilmaz", "title": "Optimal Learning for Dynamic Coding in Deadline-Constrained\n  Multi-Channel Networks", "comments": "Submitted to IEEE/ACM Transactions on Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of serving randomly arriving and delay-sensitive traffic\nover a multi-channel communication system with time-varying channel states and\nunknown statistics. This problem deviates from the classical\nexploration-exploitation setting in that the design and analysis must\naccommodate the dynamics of packet availability and urgency as well as the cost\nof each channel use at the time of decision. To that end, we have developed and\ninvestigated an index-based policy UCB-Deadline, which performs dynamic channel\nallocation decisions that incorporate these traffic requirements and costs.\nUnder symmetric channel conditions, we have proved that the UCB-Deadline policy\ncan achieve bounded regret in the likely case where the cost of using a channel\nis not too high to prevent all transmissions, and logarithmic regret otherwise.\nIn this case, we show that UCB-Deadline is order-optimal. We also perform\nnumerical investigations to validate the theoretical findings, and also compare\nthe performance of the UCB-Deadline to another learning algorithm that we\npropose based on Thompson Sampling.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 06:20:03 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Cayci", "Semih", ""], ["Eryilmaz", "Atilla", ""]]}, {"id": "1811.10847", "submitter": "Byung-Cheol Min", "authors": "Arabinda Samantaray, Baijian Yang, J. Eric Dietz, Byung-Cheol Min", "title": "Algae Detection Using Computer Vision and Deep Learning", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A disconcerting ramification of water pollution caused by burgeoning\npopulations, rapid industrialization and modernization of agriculture, has been\nthe exponential increase in the incidence of algal growth across the globe.\nHarmful algal blooms (HABs) have devastated fisheries, contaminated drinking\nwater and killed livestock, resulting in economic losses to the tune of\nmillions of dollars. Therefore, it is important to constantly monitor water\nbodies and identify any algae build-up so that prompt action against its\naccumulation can be taken and the harmful consequences can be avoided. In this\npaper, we propose a computer vision system based on deep learning for algae\nmonitoring. The proposed system is fast, accurate and cheap, and it can be\ninstalled on any robotic platforms such as USVs and UAVs for autonomous algae\nmonitoring. The experimental results demonstrate that the proposed system can\ndetect algae in distinct environments regardless of the underlying hardware\nwith high accuracy and in real time.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 07:31:26 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Samantaray", "Arabinda", ""], ["Yang", "Baijian", ""], ["Dietz", "J. Eric", ""], ["Min", "Byung-Cheol", ""]]}, {"id": "1811.10866", "submitter": "Neha Gupta", "authors": "Neha Gupta and Aaron Sidford", "title": "Exploiting Numerical Sparsity for Efficient Learning : Faster\n  Eigenvector Computation and Regression", "comments": "To appear in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we obtain improved running times for regression and top\neigenvector computation for numerically sparse matrices. Given a data matrix $A\n\\in \\mathbb{R}^{n \\times d}$ where every row $a \\in \\mathbb{R}^d$ has\n$\\|a\\|_2^2 \\leq L$ and numerical sparsity at most $s$, i.e. $\\|a\\|_1^2 /\n\\|a\\|_2^2 \\leq s$, we provide faster algorithms for these problems in many\nparameter settings.\n  For top eigenvector computation, we obtain a running time of $\\tilde{O}(nd +\nr(s + \\sqrt{r s}) / \\mathrm{gap}^2)$ where $\\mathrm{gap} > 0$ is the relative\ngap between the top two eigenvectors of $A^\\top A$ and $r$ is the stable rank\nof $A$. This running time improves upon the previous best unaccelerated running\ntime of $O(nd + r d / \\mathrm{gap}^2)$ as it is always the case that $r \\leq d$\nand $s \\leq d$.\n  For regression, we obtain a running time of $\\tilde{O}(nd + (nL / \\mu)\n\\sqrt{s nL / \\mu})$ where $\\mu > 0$ is the smallest eigenvalue of $A^\\top A$.\nThis running time improves upon the previous best unaccelerated running time of\n$\\tilde{O}(nd + n L d / \\mu)$. This result expands the regimes where regression\ncan be solved in nearly linear time from when $L/\\mu = \\tilde{O}(1)$ to when $L\n/ \\mu = \\tilde{O}(d^{2/3} / (sn)^{1/3})$.\n  Furthermore, we obtain similar improvements even when row norms and numerical\nsparsities are non-uniform and we show how to achieve even faster running times\nby accelerating using approximate proximal point [Frostig et. al. 2015] /\ncatalyst [Lin et. al. 2015]. Our running times depend only on the size of the\ninput and natural numerical measures of the matrix, i.e. eigenvalues and\n$\\ell_p$ norms, making progress on a key open problem regarding optimal running\ntimes for efficient large-scale learning.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 08:22:54 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Gupta", "Neha", ""], ["Sidford", "Aaron", ""]]}, {"id": "1811.10869", "submitter": "Chaim Baskin", "authors": "Natan Liss, Chaim Baskin, Avi Mendelson, Alex M. Bronstein, Raja\n  Giryes", "title": "Efficient non-uniform quantizer for quantized neural network targeting\n  reconfigurable hardware", "comments": "In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional Neural Networks (CNN) has become more popular choice for\nvarious tasks such as computer vision, speech recognition and natural language\nprocessing. Thanks to their large computational capability and throughput, GPUs\n,which are not power efficient and therefore does not suit low power systems\nsuch as mobile devices, are the most common platform for both training and\ninferencing tasks. Recent studies has shown that FPGAs can provide a good\nalternative to GPUs as a CNN accelerator, due to their re-configurable nature,\nlow power and small latency. In order for FPGA-based accelerators outperform\nGPUs in inference task, both the parameters of the network and the activations\nmust be quantized. While most works use uniform quantizers for both parameters\nand activations, it is not always the optimal one, and a non-uniform quantizer\nneed to be considered. In this work we introduce a custom hardware-friendly\napproach to implement non-uniform quantizers. In addition, we use a single\nscale integer representation of both parameters and activations, for both\ntraining and inference. The combined method yields a hardware efficient\nnon-uniform quantizer, fit for real-time applications. We have tested our\nmethod on CIFAR-10 and CIFAR-100 image classification datasets with ResNet-18\nand VGG-like architectures, and saw little degradation in accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 08:28:52 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Liss", "Natan", ""], ["Baskin", "Chaim", ""], ["Mendelson", "Avi", ""], ["Bronstein", "Alex M.", ""], ["Giryes", "Raja", ""]]}, {"id": "1811.10892", "submitter": "Claudio Gallicchio", "authors": "Claudio Gallicchio", "title": "Chasing the Echo State Property", "comments": "This paper is a preprint of the paper presented at ESANN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir Computing (RC) provides an efficient way for designing dynamical\nrecurrent neural models. While training is restricted to a simple output\ncomponent, the recurrent connections are left untrained after initialization,\nsubject to stability constraints specified by the Echo State Property (ESP).\nLiterature conditions for the ESP typically fail to properly account for the\neffects of driving input signals, often limiting the potentialities of the RC\napproach. In this paper, we study the fundamental aspect of asymptotic\nstability of RC models in presence of driving input, introducing an empirical\nESP index that enables to easily analyze the stability regimes of reservoirs.\nResults on two benchmark datasets reveal interesting insights on the dynamical\nproperties of input-driven reservoirs, suggesting that the actual domain of ESP\nvalidity is much wider than what covered by literature conditions commonly used\nin RC practice.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 09:44:35 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 15:57:00 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Gallicchio", "Claudio", ""]]}, {"id": "1811.10900", "submitter": "Sebastian Kauschke", "authors": "Lukas Fleckenstein, Sebastian Kauschke, Johannes F\\\"urnkranz", "title": "Beta Distribution Drift Detection for Adaptive Classifiers", "comments": null, "journal-ref": "Proceedings ESANN 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With today's abundant streams of data, the only constant we can rely on is\nchange. For stream classification algorithms, it is necessary to adapt to\nconcept drift. This can be achieved by monitoring the model error, and\ntriggering counter measures as changes occur. In this paper, we propose a drift\ndetection mechanism that fits a beta distribution to the model error, and\ntreats abnormal behavior as drift. It works with any given model, leverages\nprior knowledge about this model, and allows to set application-specific\nconfidence thresholds. Experiments confirm that it performs well, in particular\nwhen drift occurs abruptly.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 10:17:54 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Fleckenstein", "Lukas", ""], ["Kauschke", "Sebastian", ""], ["F\u00fcrnkranz", "Johannes", ""]]}, {"id": "1811.10902", "submitter": "Xiaoxiao Wang", "authors": "Xiaoxiao Wang, Xueying Guo, Jie Chuai, Zhitang Chen, Xin Liu", "title": "Kernel-based Multi-Task Contextual Bandits in Cellular Network\n  Configuration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular network configuration plays a critical role in network performance.\nIn current practice, network configuration depends heavily on field experience\nof engineers and often remains static for a long period of time. This practice\nis far from optimal. To address this limitation, online-learning-based\napproaches have great potentials to automate and optimize network\nconfiguration. Learning-based approaches face the challenges of learning a\nhighly complex function for each base station and balancing the fundamental\nexploration-exploitation tradeoff while minimizing the exploration cost.\nFortunately, in cellular networks, base stations (BSs) often have similarities\neven though they are not identical. To leverage such similarities, we propose\nkernel-based multi-BS contextual bandit algorithm based on multi-task learning.\nIn the algorithm, we leverage the similarity among different BSs defined by\nconditional kernel embedding. We present theoretical analysis of the proposed\nalgorithm in terms of regret and multi-task-learning efficiency. We evaluate\nthe effectiveness of our algorithm based on a simulator built by real traces.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 10:39:56 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 09:13:23 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Wang", "Xiaoxiao", ""], ["Guo", "Xueying", ""], ["Chuai", "Jie", ""], ["Chen", "Zhitang", ""], ["Liu", "Xin", ""]]}, {"id": "1811.10914", "submitter": "Kaicheng Yu", "authors": "Wei Wang, Kaicheng Yu, Joachim Hugonot, Pascal Fua, Mathieu Salzmann", "title": "Beyond One Glance: Gated Recurrent Architecture for Hand Segmentation", "comments": "The first two authors contribute equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As mixed reality is gaining increased momentum, the development of effective\nand efficient solutions to egocentric hand segmentation is becoming critical.\nTraditional segmentation techniques typically follow a one-shot approach, where\nthe image is passed forward only once through a model that produces a\nsegmentation mask. This strategy, however, does not reflect the perception of\nhumans, who continuously refine their representation of the world. In this\npaper, we therefore introduce a novel gated recurrent architecture. It goes\nbeyond both iteratively passing the predicted segmentation mask through the\nnetwork and adding a standard recurrent unit to it. Instead, it incorporates\nmultiple encoder-decoder layers of the segmentation network, so as to keep\ntrack of its internal state in the refinement process. As evidenced by our\nresults on standard hand segmentation benchmarks and on our own dataset, our\napproach outperforms these other, simpler recurrent segmentation techniques, as\nwell as the state-of-the-art hand segmentation one. Furthermore, we demonstrate\nthe generality of our approach by applying it to road segmentation, where it\nalso outperforms other baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 11:16:41 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 13:10:49 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2018 13:26:20 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Wang", "Wei", ""], ["Yu", "Kaicheng", ""], ["Hugonot", "Joachim", ""], ["Fua", "Pascal", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1811.10943", "submitter": "Francis Williams", "authors": "Francis Williams, Teseo Schneider, Claudio Silva, Denis Zorin, Joan\n  Bruna, Daniele Panozzo", "title": "Deep Geometric Prior for Surface Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconstruction of a discrete surface from a point cloud is a fundamental\ngeometry processing problem that has been studied for decades, with many\nmethods developed. We propose the use of a deep neural network as a geometric\nprior for surface reconstruction. Specifically, we overfit a neural network\nrepresenting a local chart parameterization to part of an input point cloud\nusing the Wasserstein distance as a measure of approximation. By jointly\nfitting many such networks to overlapping parts of the point cloud, while\nenforcing a consistency condition, we compute a manifold atlas. By sampling\nthis atlas, we can produce a dense reconstruction of the surface approximating\nthe input cloud. The entire procedure does not require any training data or\nexplicit regularization, yet, we show that it is able to perform remarkably\nwell: not introducing typical overfitting artifacts, and approximating sharp\nfeatures closely at the same time. We experimentally show that this geometric\nprior produces good results for both man-made objects containing sharp features\nand smoother organic objects, as well as noisy inputs. We compare our method\nwith a number of well-known reconstruction methods on a standard surface\nreconstruction benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 12:50:46 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 00:31:43 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Williams", "Francis", ""], ["Schneider", "Teseo", ""], ["Silva", "Claudio", ""], ["Zorin", "Denis", ""], ["Bruna", "Joan", ""], ["Panozzo", "Daniele", ""]]}, {"id": "1811.10947", "submitter": "Xiuming Liu", "authors": "Xiuming Liu, Dave Zachariah, Johan W{\\aa}gberg, Thomas B. Sch\\\"on", "title": "Reliable Semi-Supervised Learning when Labels are Missing at Random", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning methods are motivated by the availability of large\ndatasets with unlabeled features in addition to labeled data. Unlabeled data\nis, however, not guaranteed to improve classification performance and has in\nfact been reported to impair the performance in certain cases. A fundamental\nsource of error arises from restrictive assumptions about the unlabeled\nfeatures, which result in unreliable classifiers that underestimate their\nprediction error probabilities. In this paper, we develop a semi-supervised\nlearning approach that relaxes such assumptions and is capable of providing\nclassifiers that reliably quantify the label uncertainty. The approach is\napplicable using any generative model with a supervised learning algorithm. We\nillustrate the approach using both handwritten digit and cloth classification\ndata where the labels are missing at random.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 12:54:46 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 07:19:55 GMT"}, {"version": "v3", "created": "Mon, 11 Feb 2019 20:05:10 GMT"}, {"version": "v4", "created": "Wed, 13 Feb 2019 16:52:18 GMT"}, {"version": "v5", "created": "Thu, 24 Oct 2019 13:43:24 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Liu", "Xiuming", ""], ["Zachariah", "Dave", ""], ["W\u00e5gberg", "Johan", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1811.10949", "submitter": "Oguzhan Gencoglu", "authors": "Oguzhan Gencoglu, Miikka Ermes", "title": "Predicting the Flu from Instagram", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional surveillance systems for monitoring infectious diseases, such as\ninfluenza, face challenges due to shortage of skilled healthcare professionals,\nremoteness of communities and absence of communication infrastructures.\nInternet-based approaches for surveillance are appealing logistically as well\nas economically. Search engine queries and Twitter have been the primarily used\ndata sources in such approaches. The aim of this study is to assess the\npredictive power of an alternative data source, Instagram. By using 317 weeks\nof publicly available data from Instagram, we trained several machine learning\nalgorithms to both nowcast and forecast the number of official influenza-like\nillness incidents in Finland where population-wide official statistics about\nthe weekly incidents are available. In addition to date and hashtag count\nfeatures of online posts, we were able to utilize also the visual content of\nthe posted images with the help of deep convolutional neural networks. Our best\nnowcasting model reached a mean absolute error of 11.33 incidents per week and\na correlation coefficient of 0.963 on the test data. Forecasting models for\npredicting 1 week and 2 weeks ahead showed statistical significance as well by\nreaching correlation coefficients of 0.903 and 0.862, respectively. This study\ndemonstrates how social media and in particular, digital photographs shared in\nthem, can be a valuable source of information for the field of infodemiology.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 13:00:18 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Gencoglu", "Oguzhan", ""], ["Ermes", "Miikka", ""]]}, {"id": "1811.10958", "submitter": "Marko J\\\"arvenp\\\"a\\\"a", "authors": "Marko J\\\"arvenp\\\"a\\\"a, Mohamad R. Abdul Sater, Georgia K. Lagoudas,\n  Paul C. Blainey, Loren G. Miller, James A. McKinnell, Susan S. Huang, Yonatan\n  H. Grad, Pekka Marttinen", "title": "A Bayesian model of acquisition and clearance of bacterial colonization", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/87", "categories": "q-bio.PE cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bacterial populations that colonize a host play important roles in host\nhealth, including serving as a reservoir that transmits to other hosts and from\nwhich invasive strains emerge, thus emphasizing the importance of understanding\nrates of acquisition and clearance of colonizing populations. Studies of\ncolonization dynamics have been based on assessment of whether serial samples\nrepresent a single population or distinct colonization events. A common\nsolution to estimate acquisition and clearance rates is to use a fixed genetic\ndistance threshold. However, this approach is often inadequate to account for\nthe diversity of the underlying within-host evolving population, the time\nintervals between consecutive measurements, and the uncertainty in the\nestimated acquisition and clearance rates. Here, we summarize recently\nsubmitted work \\cite{jarvenpaa2018named} and present a Bayesian model that\nprovides probabilities of whether two strains should be considered the same,\nallowing to determine bacterial clearance and acquisition from genomes sampled\nover time. We explicitly model the within-host variation using population\ngenetic simulation, and the inference is done by combining information from\nmultiple data sources by using a combination of Approximate Bayesian\nComputation (ABC) and Markov Chain Monte Carlo (MCMC). We use the method to\nanalyse a collection of methicillin resistant Staphylococcus aureus (MRSA)\nisolates.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 13:17:44 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["J\u00e4rvenp\u00e4\u00e4", "Marko", ""], ["Sater", "Mohamad R. Abdul", ""], ["Lagoudas", "Georgia K.", ""], ["Blainey", "Paul C.", ""], ["Miller", "Loren G.", ""], ["McKinnell", "James A.", ""], ["Huang", "Susan S.", ""], ["Grad", "Yonatan H.", ""], ["Marttinen", "Pekka", ""]]}, {"id": "1811.10959", "submitter": "Tongzhou Wang", "authors": "Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, Alexei A. Efros", "title": "Dataset Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model distillation aims to distill the knowledge of a complex model into a\nsimpler one. In this paper, we consider an alternative formulation called\ndataset distillation: we keep the model fixed and instead attempt to distill\nthe knowledge from a large training dataset into a small one. The idea is to\nsynthesize a small number of data points that do not need to come from the\ncorrect data distribution, but will, when given to the learning algorithm as\ntraining data, approximate the model trained on the original data. For example,\nwe show that it is possible to compress 60,000 MNIST training images into just\n10 synthetic distilled images (one per class) and achieve close to original\nperformance with only a few gradient descent steps, given a fixed network\ninitialization. We evaluate our method in various initialization settings and\nwith different learning objectives. Experiments on multiple datasets show the\nadvantage of our approach compared to alternative methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 13:17:45 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 14:26:04 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 23:25:50 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Wang", "Tongzhou", ""], ["Zhu", "Jun-Yan", ""], ["Torralba", "Antonio", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1811.10978", "submitter": "Sami Remes", "authors": "Sami Remes, Markus Heinonen, Samuel Kaski", "title": "Neural Non-Stationary Spectral Kernel", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard kernels such as Mat\\'ern or RBF kernels only encode simple monotonic\ndependencies within the input space. Spectral mixture kernels have been\nproposed as general-purpose, flexible kernels for learning and discovering more\ncomplicated patterns in the data. Spectral mixture kernels have recently been\ngeneralized into non-stationary kernels by replacing the mixture weights,\nfrequency means and variances by input-dependent functions. These functions\nhave also been modelled as Gaussian processes on their own. In this paper we\npropose modelling the hyperparameter functions with neural networks, and\nprovide an experimental comparison between the stationary spectral mixture and\nthe two non-stationary spectral mixtures. Scalable Gaussian process inference\nis implemented within the sparse variational framework for all the kernels\nconsidered. We show that the neural variant of the kernel is able to achieve\nthe best performance, among alternatives, on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 13:43:37 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Remes", "Sami", ""], ["Heinonen", "Markus", ""], ["Kaski", "Samuel", ""]]}, {"id": "1811.10988", "submitter": "Xavier Favory", "authors": "Xavier Favory, Eduardo Fonseca, Frederic Font, Xavier Serra", "title": "Facilitating the Manual Annotation of Sounds When Using Large Taxonomies", "comments": "5 pages, 5 figures, IEEE FRUCT International Workshop on Semantic\n  Audio and the Internet of Things", "journal-ref": "Proceedings of the 23rd Conference of Open Innovations Association\n  FRUCT, Bologna, Italy. 2018. ISSN 2305-7254, ISBN 978-952-68653-6-2, FRUCT\n  Oy, e-ISSN 2343-0737 (license CC BY-ND)", "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Properly annotated multimedia content is crucial for supporting advances in\nmany Information Retrieval applications. It enables, for instance, the\ndevelopment of automatic tools for the annotation of large and diverse\nmultimedia collections. In the context of everyday sounds and online\ncollections, the content to describe is very diverse and involves many\ndifferent types of concepts, often organised in large hierarchical structures\ncalled taxonomies. This makes the task of manually annotating content arduous.\nIn this paper, we present our user-centered development of two tools for the\nmanual annotation of audio content from a wide range of types. We conducted a\npreliminary evaluation of functional prototypes involving real users. The goal\nis to evaluate them in a real context, engage in discussions with users, and\ninspire new ideas. A qualitative analysis was carried out including usability\nquestionnaires and semi-structured interviews. This revealed interesting\naspects to consider when developing tools for the manual annotation of audio\ncontent with labels drawn from large hierarchical taxonomies.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 16:43:11 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Favory", "Xavier", ""], ["Fonseca", "Eduardo", ""], ["Font", "Frederic", ""], ["Serra", "Xavier", ""]]}, {"id": "1811.10990", "submitter": "Chenyang Huang", "authors": "Chenyang Huang and Osmar R. Za\\\"iane", "title": "Generating Responses Expressing Emotion in an Open-domain Dialogue\n  System", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-17705-8", "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network-based Open-ended conversational agents automatically generate\nresponses based on predictive models learned from a large number of pairs of\nutterances. The generated responses are typically acceptable as a sentence but\nare often dull, generic, and certainly devoid of any emotion. In this paper, we\npresent neural models that learn to express a given emotion in the generated\nresponse. We propose four models and evaluate them against 3 baselines. An\nencoder-decoder framework-based model with multiple attention layers provides\nthe best overall performance in terms of expressing the required emotion. While\nit does not outperform other models on all emotions, it presents promising\nresults in most cases.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 22:59:25 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Huang", "Chenyang", ""], ["Za\u00efane", "Osmar R.", ""]]}, {"id": "1811.10996", "submitter": "Ning Miao", "authors": "Ning Miao, Hao Zhou, Lili Mou, Rui Yan, Lei Li", "title": "CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling", "comments": "AAAI19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world applications of natural language generation, there are often\nconstraints on the target sentences in addition to fluency and naturalness\nrequirements. Existing language generation techniques are usually based on\nrecurrent neural networks (RNNs). However, it is non-trivial to impose\nconstraints on RNNs while maintaining generation quality, since RNNs generate\nsentences sequentially (or with beam search) from the first word to the last.\nIn this paper, we propose CGMH, a novel approach using Metropolis-Hastings\nsampling for constrained sentence generation. CGMH allows complicated\nconstraints such as the occurrence of multiple keywords in the target\nsentences, which cannot be handled in traditional RNN-based approaches.\nMoreover, CGMH works in the inference stage, and does not require parallel\ncorpora for training. We evaluate our method on a variety of tasks, including\nkeywords-to-sentence generation, unsupervised sentence paraphrasing, and\nunsupervised sentence error correction. CGMH achieves high performance compared\nwith previous supervised methods for sentence generation. Our code is released\nat https://github.com/NingMiao/CGMH\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 15:46:57 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Miao", "Ning", ""], ["Zhou", "Hao", ""], ["Mou", "Lili", ""], ["Yan", "Rui", ""], ["Li", "Lei", ""]]}, {"id": "1811.10999", "submitter": "Zheng Li", "authors": "Zheng Li, Ying Wei, Yu Zhang, Xiang Zhang, Xin Li, Qiang Yang", "title": "Exploiting Coarse-to-Fine Task Transfer for Aspect-level Sentiment\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-level sentiment classification (ASC) aims at identifying sentiment\npolarities towards aspects in a sentence, where the aspect can behave as a\ngeneral Aspect Category (AC) or a specific Aspect Term (AT). However, due to\nthe especially expensive and labor-intensive labeling, existing public corpora\nin AT-level are all relatively small. Meanwhile, most of the previous methods\nrely on complicated structures with given scarce data, which largely limits the\nefficacy of the neural models. In this paper, we exploit a new direction named\ncoarse-to-fine task transfer, which aims to leverage knowledge learned from a\nrich-resource source domain of the coarse-grained AC task, which is more easily\naccessible, to improve the learning in a low-resource target domain of the\nfine-grained AT task. To resolve both the aspect granularity inconsistency and\nfeature mismatch between domains, we propose a Multi-Granularity Alignment\nNetwork (MGAN). In MGAN, a novel Coarse2Fine attention guided by an auxiliary\ntask can help the AC task modeling at the same fine-grained level with the AT\ntask. To alleviate the feature false alignment, a contrastive feature alignment\nmethod is adopted to align aspect-specific feature representations\nsemantically. In addition, a large-scale multi-domain dataset for the AC task\nis provided. Empirically, extensive experiments demonstrate the effectiveness\nof the MGAN.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 07:09:30 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Li", "Zheng", ""], ["Wei", "Ying", ""], ["Zhang", "Yu", ""], ["Zhang", "Xiang", ""], ["Li", "Xin", ""], ["Yang", "Qiang", ""]]}, {"id": "1811.11001", "submitter": "Tianlin Liu", "authors": "Tianlin Liu, Lyle Ungar, Jo\\~ao Sedoc", "title": "Unsupervised Post-processing of Word Vectors via Conceptor Negation", "comments": "Accepted by AAAI-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word vectors are at the core of many natural language processing tasks.\nRecently, there has been interest in post-processing word vectors to enrich\ntheir semantic information. In this paper, we introduce a novel word vector\npost-processing technique based on matrix conceptors (Jaeger2014), a family of\nregularized identity maps. More concretely, we propose to use conceptors to\nsuppress those latent features of word vectors having high variances. The\nproposed method is purely unsupervised: it does not rely on any corpus or\nexternal linguistic database. We evaluate the post-processed word vectors on a\nbattery of intrinsic lexical evaluation tasks, showing that the proposed method\nconsistently outperforms existing state-of-the-art alternatives. We also show\nthat post-processed word vectors can be used for the downstream natural\nlanguage processing task of dialogue state tracking, yielding improved results\nin different dialogue domains.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 20:12:40 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 10:27:57 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Liu", "Tianlin", ""], ["Ungar", "Lyle", ""], ["Sedoc", "Jo\u00e3o", ""]]}, {"id": "1811.11002", "submitter": "Tianlin Liu", "authors": "Tianlin Liu, Jo\\~ao Sedoc, Lyle Ungar", "title": "Correcting the Common Discourse Bias in Linear Representation of\n  Sentences using Conceptors", "comments": "Accepted by the BioCreative/OHNLP workshop of ACM-BCB 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed representations of words, better known as word embeddings, have\nbecome important building blocks for natural language processing tasks.\nNumerous studies are devoted to transferring the success of unsupervised word\nembeddings to sentence embeddings. In this paper, we introduce a simple\nrepresentation of sentences in which a sentence embedding is represented as a\nweighted average of word vectors followed by a soft projection. We demonstrate\nthe effectiveness of this proposed method on the clinical semantic textual\nsimilarity task of the BioCreative/OHNLP Challenge 2018.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 20:20:20 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Liu", "Tianlin", ""], ["Sedoc", "Jo\u00e3o", ""], ["Ungar", "Lyle", ""]]}, {"id": "1811.11005", "submitter": "Maria Pikoula", "authors": "Spiros Denaxas, Pontus Stenetorp, Sebastian Riedel, Maria Pikoula,\n  Richard Dobson, Harry Hemingway", "title": "Application of Clinical Concept Embeddings for Heart Failure Prediction\n  in UK EHR data", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/37", "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health records (EHR) are increasingly being used for constructing\ndisease risk prediction models. Feature engineering in EHR data however is\nchallenging due to their highly dimensional and heterogeneous nature.\nLow-dimensional representations of EHR data can potentially mitigate these\nchallenges. In this paper, we use global vectors (GloVe) to learn word\nembeddings for diagnoses and procedures recorded using 13 million ontology\nterms across 2.7 million hospitalisations in national UK EHR. We demonstrate\nthe utility of these embeddings by evaluating their performance in identifying\npatients which are at higher risk of being hospitalised for congestive heart\nfailure. Our findings indicate that embeddings can enable the creation of\nrobust EHR-derived disease risk prediction models and address some the\nlimitations associated with manual clinical feature engineering.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 13:04:12 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 15:01:56 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Denaxas", "Spiros", ""], ["Stenetorp", "Pontus", ""], ["Riedel", "Sebastian", ""], ["Pikoula", "Maria", ""], ["Dobson", "Richard", ""], ["Hemingway", "Harry", ""]]}, {"id": "1811.11008", "submitter": "Srikumar Krishnamoorthy", "authors": "Srikumar Krishnamoorthy", "title": "Sentiment Analysis of Financial News Articles using Performance\n  Indicators", "comments": "Knowledge and Information Systems Nov 2017", "journal-ref": null, "doi": "10.1007/s10115-017-1134-1", "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining financial text documents and understanding the sentiments of\nindividual investors, institutions and markets is an important and challenging\nproblem in the literature. Current approaches to mine sentiments from financial\ntexts largely rely on domain specific dictionaries. However, dictionary based\nmethods often fail to accurately predict the polarity of financial texts. This\npaper aims to improve the state-of-the-art and introduces a novel sentiment\nanalysis approach that employs the concept of financial and non-financial\nperformance indicators. It presents an association rule mining based\nhierarchical sentiment classifier model to predict the polarity of financial\ntexts as positive, neutral or negative. The performance of the proposed model\nis evaluated on a benchmark financial dataset. The model is also compared\nagainst other state-of-the-art dictionary and machine learning based approaches\nand the results are found to be quite promising. The novel use of performance\nindicators for financial sentiment analysis offers interesting and useful\ninsights.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 01:36:12 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Krishnamoorthy", "Srikumar", ""]]}, {"id": "1811.11017", "submitter": "Mohan Zhang", "authors": "Mohan Zhang, Zhichao Luo, Hai Lu", "title": "Latent Dirichlet Allocation with Residual Convolutional Neural Network\n  Applied in Evaluating Credibility of Chinese Listed Companies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project demonstrated a methodology to estimating cooperate credibility\nwith a Natural Language Processing approach. As cooperate transparency impacts\nboth the credibility and possible future earnings of the firm, it is an\nimportant factor to be considered by banks and investors on risk assessments of\nlisted firms. This approach of estimating cooperate credibility can bypass\nhuman bias and inconsistency in the risk assessment, the use of large\nquantitative data and neural network models provides more accurate estimation\nin a more efficient manner compare to manual assessment. At the beginning, the\nmodel will employs Latent Dirichlet Allocation and THU Open Chinese Lexicon\nfrom Tsinghua University to classify topics in articles which are potentially\nrelated to corporate credibility. Then with the keywords related to each\ntopics, we trained a residual convolutional neural network with data labeled\naccording to surveys of fund manager and accountant's opinion on corporate\ncredibility. After the training, we run the model with preprocessed news\nreports regarding to all of the 3065 listed companies, the model is supposed to\ngive back companies ranking based on the level of their transparency.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 17:50:41 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Zhang", "Mohan", ""], ["Luo", "Zhichao", ""], ["Lu", "Hai", ""]]}, {"id": "1811.11019", "submitter": "Chenhe Zhang", "authors": "Chenhe Zhang and Peiyuan Sun", "title": "Arena Model: Inference About Competitions", "comments": "31 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors propose a parametric model called the arena model for prediction\nin paired competitions, i.e. paired comparisons with eliminations and\nbifurcations. The arena model has a number of appealing advantages. First, it\npredicts the results of competitions without rating many individuals. Second,\nit takes full advantage of the structure of competitions. Third, the model\nprovides an easy method to quantify the uncertainty in competitions. Fourth,\nsome of our methods can be directly generalized for comparisons among three or\nmore individuals. Furthermore, the authors identify an invariant Bayes\nestimator with regard to the prior distribution and prove the consistency of\nthe estimations of uncertainty. Currently, the arena model is not effective in\ntracking the change of strengths of individuals, but its basic framework\nprovides a solid foundation for future study of such cases.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 17:35:36 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Zhang", "Chenhe", ""], ["Sun", "Peiyuan", ""]]}, {"id": "1811.11043", "submitter": "Michal Valko", "authors": "Julien Seznec, Andrea Locatelli, Alexandra Carpentier, Alessandro\n  Lazaric, Michal Valko", "title": "Rotting bandits are not harder than stochastic ones", "comments": null, "journal-ref": "International Conference on Artificial Intelligence and Statistics\n  (AISTATS 2019)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In stochastic multi-armed bandits, the reward distribution of each arm is\nassumed to be stationary. This assumption is often violated in practice (e.g.,\nin recommendation systems), where the reward of an arm may change whenever is\nselected, i.e., rested bandit setting. In this paper, we consider the\nnon-parametric rotting bandit setting, where rewards can only decrease. We\nintroduce the filtering on expanding window average (FEWA) algorithm that\nconstructs moving averages of increasing windows to identify arms that are more\nlikely to return high rewards when pulled once more. We prove that for an\nunknown horizon $T$, and without any knowledge on the decreasing behavior of\nthe $K$ arms, FEWA achieves problem-dependent regret bound of\n$\\widetilde{\\mathcal{O}}(\\log{(KT)}),$ and a problem-independent one of\n$\\widetilde{\\mathcal{O}}(\\sqrt{KT})$. Our result substantially improves over\nthe algorithm of Levine et al. (2017), which suffers regret\n$\\widetilde{\\mathcal{O}}(K^{1/3}T^{2/3})$. FEWA also matches known bounds for\nthe stochastic bandit setting, thus showing that the rotting bandits are not\nharder. Finally, we report simulations confirming the theoretical improvements\nof FEWA.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 15:07:04 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 19:34:31 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Seznec", "Julien", ""], ["Locatelli", "Andrea", ""], ["Carpentier", "Alexandra", ""], ["Lazaric", "Alessandro", ""], ["Valko", "Michal", ""]]}, {"id": "1811.11067", "submitter": "Elizaveta Logacheva", "authors": "Pavel Solovev, Vladimir Aliev, Pavel Ostyakov, Gleb Sterkin, Elizaveta\n  Logacheva, Stepan Troeshestov, Roman Suvorov, Anton Mashikhin, Oleg Khomenko,\n  Sergey I. Nikolenko", "title": "Learning State Representations in Complex Systems with Multimodal Data", "comments": "Fixed references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning becomes especially important for complex systems with\nmultimodal data sources such as cameras or sensors. Recent advances in\nreinforcement learning and optimal control make it possible to design control\nalgorithms on these latent representations, but the field still lacks a\nlarge-scale standard dataset for unified comparison. In this work, we present a\nlarge-scale dataset and evaluation framework for representation learning for\nthe complex task of landing an airplane. We implement and compare several\napproaches to representation learning on this dataset in terms of the quality\nof simple supervised learning tasks and disentanglement scores. The resulting\nrepresentations can be used for further tasks such as anomaly detection,\noptimal control, model-based reinforcement learning, and other applications.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 15:55:42 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 13:48:24 GMT"}, {"version": "v3", "created": "Tue, 15 Jan 2019 20:13:43 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Solovev", "Pavel", ""], ["Aliev", "Vladimir", ""], ["Ostyakov", "Pavel", ""], ["Sterkin", "Gleb", ""], ["Logacheva", "Elizaveta", ""], ["Troeshestov", "Stepan", ""], ["Suvorov", "Roman", ""], ["Mashikhin", "Anton", ""], ["Khomenko", "Oleg", ""], ["Nikolenko", "Sergey I.", ""]]}, {"id": "1811.11079", "submitter": "Suproteem Sarkar", "authors": "Suproteem K. Sarkar, Kojin Oshiba, Daniel Giebisch, Yaron Singer", "title": "Robust Classification of Financial Risk", "comments": "NIPS 2018 Workshop on Challenges and Opportunities for AI in\n  Financial Services", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms are increasingly common components of high-impact decision-making,\nand a growing body of literature on adversarial examples in laboratory settings\nindicates that standard machine learning models are not robust. This suggests\nthat real-world systems are also susceptible to manipulation or\nmisclassification, which especially poses a challenge to machine learning\nmodels used in financial services. We use the loan grade classification problem\nto explore how machine learning models are sensitive to small changes in\nuser-reported data, using adversarial attacks documented in the literature and\nan original, domain-specific attack. Our work shows that a robust optimization\nalgorithm can build models for financial services that are resistant to\nmisclassification on perturbations. To the best of our knowledge, this is the\nfirst study of adversarial attacks and defenses for deep learning in financial\nservices.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 16:28:32 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Sarkar", "Suproteem K.", ""], ["Oshiba", "Kojin", ""], ["Giebisch", "Daniel", ""], ["Singer", "Yaron", ""]]}, {"id": "1811.11083", "submitter": "Kevin Liang", "authors": "Kevin J Liang, Chunyuan Li, Guoyin Wang, Lawrence Carin", "title": "Generative Adversarial Network Training is a Continual Learning Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have proven to be a powerful framework\nfor learning to draw samples from complex distributions. However, GANs are also\nnotoriously difficult to train, with mode collapse and oscillations a common\nproblem. We hypothesize that this is at least in part due to the evolution of\nthe generator distribution and the catastrophic forgetting tendency of neural\nnetworks, which leads to the discriminator losing the ability to remember\nsynthesized samples from previous instantiations of the generator. Recognizing\nthis, our contributions are twofold. First, we show that GAN training makes for\na more interesting and realistic benchmark for continual learning methods\nevaluation than some of the more canonical datasets. Second, we propose\nleveraging continual learning techniques to augment the discriminator,\npreserving its ability to recognize previous generator samples. We show that\nthe resulting methods add only a light amount of computation, involve minimal\nchanges to the model, and result in better overall performance on the examined\nimage and text generation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 16:41:58 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Liang", "Kevin J", ""], ["Li", "Chunyuan", ""], ["Wang", "Guoyin", ""], ["Carin", "Lawrence", ""]]}, {"id": "1811.11103", "submitter": "Soumyasundar Pal", "authors": "Yingxue Zhang, Soumyasundar Pal, Mark Coates, Deniz \\\"Ustebay", "title": "Bayesian graph convolutional neural networks for semi-supervised\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, techniques for applying convolutional neural networks to\ngraph-structured data have emerged. Graph convolutional neural networks (GCNNs)\nhave been used to address node and graph classification and matrix completion.\nAlthough the performance has been impressive, the current implementations have\nlimited capability to incorporate uncertainty in the graph structure. Almost\nall GCNNs process a graph as though it is a ground-truth depiction of the\nrelationship between nodes, but often the graphs employed in applications are\nthemselves derived from noisy data or modelling assumptions. Spurious edges may\nbe included; other edges may be missing between nodes that have very strong\nrelationships. In this paper we adopt a Bayesian approach, viewing the observed\ngraph as a realization from a parametric family of random graphs. We then\ntarget inference of the joint posterior of the random graph parameters and the\nnode (or graph) labels. We present the Bayesian GCNN framework and develop an\niterative learning procedure for the case of assortative mixed-membership\nstochastic block models. We present the results of experiments that demonstrate\nthat the Bayesian formulation can provide better performance when there are\nvery few labels available during the training process.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 16:54:47 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Zhang", "Yingxue", ""], ["Pal", "Soumyasundar", ""], ["Coates", "Mark", ""], ["\u00dcstebay", "Deniz", ""]]}, {"id": "1811.11124", "submitter": "Hsin-Pai Cheng", "authors": "Hsin-Pai Cheng, Patrick Yu, Haojing Hu, Feng Yan, Shiyu Li, Hai Li,\n  Yiran Chen", "title": "LEASGD: an Efficient and Privacy-Preserving Decentralized Algorithm for\n  Distributed Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed learning systems have enabled training large-scale models over\nlarge amount of data in significantly shorter time. In this paper, we focus on\ndecentralized distributed deep learning systems and aim to achieve differential\nprivacy with good convergence rate and low communication cost. To achieve this\ngoal, we propose a new learning algorithm LEASGD (Leader-Follower Elastic\nAveraging Stochastic Gradient Descent), which is driven by a novel\nLeader-Follower topology and a differential privacy model.We provide a\ntheoretical analysis of the convergence rate and the trade-off between the\nperformance and privacy in the private setting.The experimental results show\nthat LEASGD outperforms state-of-the-art decentralized learning algorithm DPSGD\nby achieving steadily lower loss within the same iterations and by reducing the\ncommunication cost by 30%. In addition, LEASGD spends less differential privacy\nbudget and has higher final accuracy result than DPSGD under private setting.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 17:34:27 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Cheng", "Hsin-Pai", ""], ["Yu", "Patrick", ""], ["Hu", "Haojing", ""], ["Yan", "Feng", ""], ["Li", "Shiyu", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "1811.11127", "submitter": "Tim Brooks", "authors": "Tim Brooks, Ben Mildenhall, Tianfan Xue, Jiawen Chen, Dillon Sharlet,\n  Jonathan T. Barron", "title": "Unprocessing Images for Learned Raw Denoising", "comments": "http://timothybrooks.com/tech/unprocessing/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine learning techniques work best when the data used for training\nresembles the data used for evaluation. This holds true for learned\nsingle-image denoising algorithms, which are applied to real raw camera sensor\nreadings but, due to practical constraints, are often trained on synthetic\nimage data. Though it is understood that generalizing from synthetic to real\ndata requires careful consideration of the noise properties of image sensors,\nthe other aspects of a camera's image processing pipeline (gain, color\ncorrection, tone mapping, etc) are often overlooked, despite their significant\neffect on how raw measurements are transformed into finished images. To address\nthis, we present a technique to \"unprocess\" images by inverting each step of an\nimage processing pipeline, thereby allowing us to synthesize realistic raw\nsensor measurements from commonly available internet photos. We additionally\nmodel the relevant components of an image processing pipeline when evaluating\nour loss function, which allows training to be aware of all relevant\nphotometric processing that will occur after denoising. By processing and\nunprocessing model outputs and training data in this way, we are able to train\na simple convolutional neural network that has 14%-38% lower error rates and is\n9x-18x faster than the previous state of the art on the Darmstadt Noise\nDataset, and generalizes to sensors outside of that dataset as well.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 17:38:14 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Brooks", "Tim", ""], ["Mildenhall", "Ben", ""], ["Xue", "Tianfan", ""], ["Chen", "Jiawen", ""], ["Sharlet", "Dillon", ""], ["Barron", "Jonathan T.", ""]]}, {"id": "1811.11145", "submitter": "Iddo Friedberg", "authors": "Md-Nafiz Hamid and Iddo Friedberg", "title": "Reliable uncertainty estimate for antibiotic resistance classification\n  with Stochastic Gradient Langevin Dynamics", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/17", "categories": "q-bio.QM cs.LG q-bio.GN", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Antibiotic resistance monitoring is of paramount importance in the face of\nthis on-going global epidemic. Deep learning models trained with traditional\noptimization algorithms (e.g. Adam, SGD) provide poor posterior estimates when\ntested against out-of-distribution (OoD) antibiotic resistant/non-resistant\ngenes. In this paper, we introduce a deep learning model trained with\nStochastic Gradient Langevin Dynamics (SGLD) to classify antibiotic resistant\ngenes. The model provides better uncertainty estimates when tested against OoD\ndata compared to traditional optimization methods such as Adam.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:15:58 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Hamid", "Md-Nafiz", ""], ["Friedberg", "Iddo", ""]]}, {"id": "1811.11148", "submitter": "Gautam Kamath", "authors": "Cl\\'ement L. Canonne, Gautam Kamath, Audra McMillan, Adam Smith,\n  Jonathan Ullman", "title": "The Structure of Optimal Private Tests for Simple Hypotheses", "comments": "To appear in STOC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis testing plays a central role in statistical inference, and is used\nin many settings where privacy concerns are paramount. This work answers a\nbasic question about privately testing simple hypotheses: given two\ndistributions $P$ and $Q$, and a privacy level $\\varepsilon$, how many i.i.d.\nsamples are needed to distinguish $P$ from $Q$ subject to\n$\\varepsilon$-differential privacy, and what sort of tests have optimal sample\ncomplexity? Specifically, we characterize this sample complexity up to constant\nfactors in terms of the structure of $P$ and $Q$ and the privacy level\n$\\varepsilon$, and show that this sample complexity is achieved by a certain\nrandomized and clamped variant of the log-likelihood ratio test. Our result is\nan analogue of the classical Neyman-Pearson lemma in the setting of private\nhypothesis testing. We also give an application of our result to the private\nchange-point detection. Our characterization applies more generally to\nhypothesis tests satisfying essentially any notion of algorithmic stability,\nwhich is known to imply strong generalization bounds in adaptive data analysis,\nand thus our results have applications even when privacy is not a primary\nconcern.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:21:33 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 21:46:45 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["Kamath", "Gautam", ""], ["McMillan", "Audra", ""], ["Smith", "Adam", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1811.11152", "submitter": "Kevin Chen", "authors": "Kevin K. Chen, Anthony C. Gamst, Alden K. Walker", "title": "Knots in random neural networks", "comments": "Presented at the Workshop on Bayesian Deep Learning, NIPS 2016,\n  Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The weights of a neural network are typically initialized at random, and one\ncan think of the functions produced by such a network as having been generated\nby a prior over some function space. Studying random networks, then, is useful\nfor a Bayesian understanding of the network evolution in early stages of\ntraining. In particular, one can investigate why neural networks with huge\nnumbers of parameters do not immediately overfit. We analyze the properties of\nrandom scalar-input feed-forward rectified linear unit architectures, which are\nrandom linear splines. With weights and biases sampled from certain common\ndistributions, empirical tests show that the number of knots in the spline\nproduced by the network is equal to the number of neurons, to very close\napproximation. We describe our progress towards a completely analytic\nexplanation of this phenomenon. In particular, we show that random single-layer\nneural networks are equivalent to integrated random walks with variable step\nsizes. That each neuron produces one knot on average is equivalent to the\nassociated integrated random walk having one zero crossing on average. We\nexplore how properties of the integrated random walk, including the step sizes\nand initial conditions, affect the number of crossings. The number of knots in\nrandom neural networks can be related to the behavior of extreme learning\nmachines, but it also establishes a prior preventing optimizers from\nimmediately overfitting to noisy training data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:33:24 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Chen", "Kevin K.", ""], ["Gamst", "Anthony C.", ""], ["Walker", "Alden K.", ""]]}, {"id": "1811.11155", "submitter": "Krishna Kumar Singh", "authors": "Krishna Kumar Singh, Utkarsh Ojha, Yong Jae Lee", "title": "FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained\n  Object Generation and Discovery", "comments": null, "journal-ref": "CVPR 2019 (Oral Presentation)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose FineGAN, a novel unsupervised GAN framework, which disentangles\nthe background, object shape, and object appearance to hierarchically generate\nimages of fine-grained object categories. To disentangle the factors without\nsupervision, our key idea is to use information theory to associate each factor\nto a latent code, and to condition the relationships between the codes in a\nspecific way to induce the desired hierarchy. Through extensive experiments, we\nshow that FineGAN achieves the desired disentanglement to generate realistic\nand diverse images belonging to fine-grained classes of birds, dogs, and cars.\nUsing FineGAN's automatically learned features, we also cluster real images as\na first attempt at solving the novel problem of unsupervised fine-grained\nobject category discovery. Our code/models/demo can be found at\nhttps://github.com/kkanshul/finegan\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:44:37 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 17:44:24 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Singh", "Krishna Kumar", ""], ["Ojha", "Utkarsh", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1811.11163", "submitter": "Takuhiro Kaneko", "authors": "Takuhiro Kaneko, Yoshitaka Ushiku, Tatsuya Harada", "title": "Class-Distinct and Class-Mutual Image Generation with GANs", "comments": "Accepted to BMVC 2019 (Spotlight). Project page:\n  https://takuhirok.github.io/CP-GAN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class-conditional extensions of generative adversarial networks (GANs), such\nas auxiliary classifier GAN (AC-GAN) and conditional GAN (cGAN), have garnered\nattention owing to their ability to decompose representations into class labels\nand other factors and to boost the training stability. However, a limitation is\nthat they assume that each class is separable and ignore the relationship\nbetween classes even though class overlapping frequently occurs in a real-world\nscenario when data are collected on the basis of diverse or ambiguous criteria.\nTo overcome this limitation, we address a novel problem called class-distinct\nand class-mutual image generation, in which the goal is to construct a\ngenerator that can capture between-class relationships and generate an image\nselectively conditioned on the class specificity. To solve this problem without\nadditional supervision, we propose classifier's posterior GAN (CP-GAN), in\nwhich we redesign the generator input and the objective function of AC-GAN for\nclass-overlapping data. Precisely, we incorporate the classifier's posterior\ninto the generator input and optimize the generator so that the classifier's\nposterior of generated data corresponds with that of real data. We demonstrate\nthe effectiveness of CP-GAN using both controlled and real-world\nclass-overlapping data with a model configuration analysis and comparative\nstudy. Our code is available at https://github.com/takuhirok/CP-GAN/.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:56:19 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 17:51:04 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Kaneko", "Takuhiro", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1811.11165", "submitter": "Takuhiro Kaneko", "authors": "Takuhiro Kaneko, Yoshitaka Ushiku, Tatsuya Harada", "title": "Label-Noise Robust Generative Adversarial Networks", "comments": "Accepted to CVPR 2019 (Oral). Project page:\n  https://takuhirok.github.io/rGAN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are a framework that learns a\ngenerative distribution through adversarial training. Recently, their\nclass-conditional extensions (e.g., conditional GAN (cGAN) and auxiliary\nclassifier GAN (AC-GAN)) have attracted much attention owing to their ability\nto learn the disentangled representations and to improve the training\nstability. However, their training requires the availability of large-scale\naccurate class-labeled data, which are often laborious or impractical to\ncollect in a real-world scenario. To remedy this, we propose a novel family of\nGANs called label-noise robust GANs (rGANs), which, by incorporating a noise\ntransition model, can learn a clean label conditional generative distribution\neven when training labels are noisy. In particular, we propose two variants:\nrAC-GAN, which is a bridging model between AC-GAN and the label-noise robust\nclassification model, and rcGAN, which is an extension of cGAN and solves this\nproblem with no reliance on any classifier. In addition to providing the\ntheoretical background, we demonstrate the effectiveness of our models through\nextensive experiments using diverse GAN configurations, various noise settings,\nand multiple evaluation metrics (in which we tested 402 conditions in total).\nOur code is available at https://github.com/takuhirok/rGAN/.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:56:21 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 18:42:42 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Kaneko", "Takuhiro", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1811.11190", "submitter": "Alexander New", "authors": "Alexander New, Sabbir M. Rashid, John S. Erickson, Deborah L.\n  McGuinness, and Kristin P. Bennett", "title": "Semantically-aware population health risk analyses", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:cs/0101200", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One primary task of population health analysis is the identification of risk\nfactors that, for some subpopulation, have a significant association with some\nhealth condition. Examples include finding lifestyle factors associated with\nchronic diseases and finding genetic mutations associated with diseases in\nprecision health. We develop a combined semantic and machine learning system\nthat uses a health risk ontology and knowledge graph (KG) to dynamically\ndiscover risk factors and their associated subpopulations. Semantics and the\nnovel supervised cadre model make our system explainable. Future population\nhealth studies are easily performed and documented with provenance by\nspecifying additional input and output KG cartridges.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 19:00:07 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["New", "Alexander", ""], ["Rashid", "Sabbir M.", ""], ["Erickson", "John S.", ""], ["McGuinness", "Deborah L.", ""], ["Bennett", "Kristin P.", ""]]}, {"id": "1811.11205", "submitter": "Zhourong Chen", "authors": "Zhourong Chen, Yang Li, Samy Bengio, Si Si", "title": "You Look Twice: GaterNet for Dynamic Filter Selection in CNNs", "comments": "CVPR2019; Google Research, The Hong Kong University of Science and\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of conditional computation for deep nets has been proposed\npreviously to improve model performance by selectively using only parts of the\nmodel conditioned on the sample it is processing. In this paper, we investigate\ninput-dependent dynamic filter selection in deep convolutional neural networks\n(CNNs). The problem is interesting because the idea of forcing different parts\nof the model to learn from different types of samples may help us acquire\nbetter filters in CNNs, improve the model generalization performance and\npotentially increase the interpretability of model behavior. We propose a novel\nyet simple framework called GaterNet, which involves a backbone and a gater\nnetwork. The backbone network is a regular CNN that performs the major\ncomputation needed for making a prediction, while a global gater network is\nintroduced to generate binary gates for selectively activating filters in the\nbackbone network based on each input. Extensive experiments on CIFAR and\nImageNet datasets show that our models consistently outperform the original\nmodels with a large margin. On CIFAR-10, our model also improves upon\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 19:14:49 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 17:57:14 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Chen", "Zhourong", ""], ["Li", "Yang", ""], ["Bengio", "Samy", ""], ["Si", "Si", ""]]}, {"id": "1811.11206", "submitter": "Thang Bui", "authors": "Thang D. Bui, Cuong V. Nguyen, Siddharth Swaroop, Richard E. Turner", "title": "Partitioned Variational Inference: A unified framework encompassing\n  federated and continual learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference (VI) has become the method of choice for fitting many\nmodern probabilistic models. However, practitioners are faced with a fragmented\nliterature that offers a bewildering array of algorithmic options. First, the\nvariational family. Second, the granularity of the updates e.g. whether the\nupdates are local to each data point and employ message passing or global.\nThird, the method of optimization (bespoke or blackbox, closed-form or\nstochastic updates, etc.). This paper presents a new framework, termed\nPartitioned Variational Inference (PVI), that explicitly acknowledges these\nalgorithmic dimensions of VI, unifies disparate literature, and provides\nguidance on usage. Crucially, the proposed PVI framework allows us to identify\nnew ways of performing VI that are ideally suited to challenging learning\nscenarios including federated learning (where distributed computing is\nleveraged to process non-centralized data) and continual learning (where new\ndata and tasks arrive over time and must be accommodated quickly). We showcase\nthese new capabilities by developing communication-efficient federated training\nof Bayesian neural networks and continual learning for Gaussian process models\nwith private pseudo-points. The new methods significantly outperform the\nstate-of-the-art, whilst being almost as straightforward to implement as\nstandard VI.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 19:16:00 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Bui", "Thang D.", ""], ["Nguyen", "Cuong V.", ""], ["Swaroop", "Siddharth", ""], ["Turner", "Richard E.", ""]]}, {"id": "1811.11209", "submitter": "Wentao Yuan", "authors": "Wentao Yuan, David Held, Christoph Mertz, Martial Hebert", "title": "Iterative Transformer Network for 3D Point Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D point cloud is an efficient and flexible representation of 3D structures.\nRecently, neural networks operating on point clouds have shown superior\nperformance on 3D understanding tasks such as shape classification and part\nsegmentation. However, performance on such tasks is evaluated on complete\nshapes aligned in a canonical frame, while real world 3D data are partial and\nunaligned. A key challenge in learning from partial, unaligned point cloud data\nis to learn features that are invariant or equivariant with respect to\ngeometric transformations. To address this challenge, we propose the Iterative\nTransformer Network (IT-Net), a network module that canonicalizes the pose of a\npartial object with a series of 3D rigid transformations predicted in an\niterative fashion. We demonstrate the efficacy of IT-Net as an anytime pose\nestimator from partial point clouds without using complete object models.\nFurther, we show that IT-Net achieves superior performance over alternative 3D\ntransformer networks on various tasks, such as partial shape classification and\nobject part segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 19:22:24 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 02:48:32 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Yuan", "Wentao", ""], ["Held", "David", ""], ["Mertz", "Christoph", ""], ["Hebert", "Martial", ""]]}, {"id": "1811.11210", "submitter": "Buu Phan", "authors": "Buu Phan, Rick Salay, Krzysztof Czarnecki, Vahdat Abdelzad, Taylor\n  Denouden, Sachin Vernekar", "title": "Calibrating Uncertainties in Object Localization Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many safety-critical applications such as autonomous driving and surgical\nrobots, it is desirable to obtain prediction uncertainties from object\ndetection modules to help support safe decision-making. Specifically, such\nmodules need to estimate the probability of each predicted object in a given\nregion and the confidence interval for its bounding box. While recent Bayesian\ndeep learning methods provide a principled way to estimate this uncertainty,\nthe estimates for the bounding boxes obtained using these methods are\nuncalibrated. In this paper, we address this problem for the single-object\nlocalization task by adapting an existing technique for calibrating regression\nmodels. We show, experimentally, that the resulting calibrated model obtains\nmore reliable uncertainty estimates.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 19:27:29 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Phan", "Buu", ""], ["Salay", "Rick", ""], ["Czarnecki", "Krzysztof", ""], ["Abdelzad", "Vahdat", ""], ["Denouden", "Taylor", ""], ["Vernekar", "Sachin", ""]]}, {"id": "1811.11212", "submitter": "Neil Houlsby", "authors": "Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, Neil Houlsby", "title": "Self-Supervised GANs via Auxiliary Rotation Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional GANs are at the forefront of natural image synthesis. The main\ndrawback of such models is the necessity for labeled data. In this work we\nexploit two popular unsupervised learning techniques, adversarial training and\nself-supervision, and take a step towards bridging the gap between conditional\nand unconditional GANs. In particular, we allow the networks to collaborate on\nthe task of representation learning, while being adversarial with respect to\nthe classic GAN game. The role of self-supervision is to encourage the\ndiscriminator to learn meaningful feature representations which are not\nforgotten during training. We test empirically both the quality of the learned\nimage representations, and the quality of the synthesized images. Under the\nsame conditions, the self-supervised GAN attains a similar performance to\nstate-of-the-art conditional counterparts. Finally, we show that this approach\nto fully unsupervised learning can be scaled to attain an FID of 23.4 on\nunconditional ImageNet generation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 19:30:40 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 14:25:35 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Chen", "Ting", ""], ["Zhai", "Xiaohua", ""], ["Ritter", "Marvin", ""], ["Lucic", "Mario", ""], ["Houlsby", "Neil", ""]]}, {"id": "1811.11213", "submitter": "Ryan Chard", "authors": "Ryan Chard, Zhuozhao Li, Kyle Chard, Logan Ward, Yadu Babuji, Anna\n  Woodard, Steve Tuecke, Ben Blaiszik, Michael J. Franklin, and Ian Foster", "title": "DLHub: Model and Data Serving for Science", "comments": "10 pages, 8 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the Machine Learning (ML) landscape is evolving rapidly, there has been\na relative lag in the development of the \"learning systems\" needed to enable\nbroad adoption. Furthermore, few such systems are designed to support the\nspecialized requirements of scientific ML. Here we present the Data and\nLearning Hub for science (DLHub), a multi-tenant system that provides both\nmodel repository and serving capabilities with a focus on science applications.\nDLHub addresses two significant shortcomings in current systems. First, its\nselfservice model repository allows users to share, publish, verify, reproduce,\nand reuse models, and addresses concerns related to model reproducibility by\npackaging and distributing models and all constituent components. Second, it\nimplements scalable and low-latency serving capabilities that can leverage\nparallel and distributed computing resources to democratize access to published\nmodels through a simple web interface. Unlike other model serving frameworks,\nDLHub can store and serve any Python 3-compatible model or processing function,\nplus multiple-function pipelines. We show that relative to other model serving\nsystems including TensorFlow Serving, SageMaker, and Clipper, DLHub provides\ngreater capabilities, comparable performance without memoization and batching,\nand significantly better performance when the latter two techniques can be\nemployed. We also describe early uses of DLHub for scientific applications.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 19:31:29 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Chard", "Ryan", ""], ["Li", "Zhuozhao", ""], ["Chard", "Kyle", ""], ["Ward", "Logan", ""], ["Babuji", "Yadu", ""], ["Woodard", "Anna", ""], ["Tuecke", "Steve", ""], ["Blaiszik", "Ben", ""], ["Franklin", "Michael J.", ""], ["Foster", "Ian", ""]]}, {"id": "1811.11214", "submitter": "Zafarali Ahmed", "authors": "Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, Dale Schuurmans", "title": "Understanding the impact of entropy on policy optimization", "comments": "Accepted to ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropy regularization is commonly used to improve policy optimization in\nreinforcement learning. It is believed to help with \\emph{exploration} by\nencouraging the selection of more stochastic policies. In this work, we analyze\nthis claim using new visualizations of the optimization landscape based on\nrandomly perturbing the loss function. We first show that even with access to\nthe exact gradient, policy optimization is difficult due to the geometry of the\nobjective function. Then, we qualitatively show that in some environments, a\npolicy with higher entropy can make the optimization landscape smoother,\nthereby connecting local optima and enabling the use of larger learning rates.\nThis paper presents new tools for understanding the optimization landscape,\nshows that policy entropy serves as a regularizer, and highlights the challenge\nof designing general-purpose policy optimization algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 19:32:27 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 04:01:21 GMT"}, {"version": "v3", "created": "Tue, 5 Feb 2019 16:01:48 GMT"}, {"version": "v4", "created": "Wed, 15 May 2019 19:13:10 GMT"}, {"version": "v5", "created": "Fri, 7 Jun 2019 18:17:04 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Ahmed", "Zafarali", ""], ["Roux", "Nicolas Le", ""], ["Norouzi", "Mohammad", ""], ["Schuurmans", "Dale", ""]]}, {"id": "1811.11222", "submitter": "Egor Kraev", "authors": "Egor Kraev", "title": "Grammars and reinforcement learning for molecule optimization", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.chem-ph stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We seek to automate the design of molecules based on specific chemical\nproperties. Our primary contributions are a simpler method for generating\nSMILES strings guaranteed to be chemically valid, using a combination of a new\ncontext-free grammar for SMILES and additional masking logic; and casting the\nmolecular property optimization as a reinforcement learning problem,\nspecifically best-of-batch policy gradient applied to a Transformer model\narchitecture. This approach uses substantially fewer model steps per atom than\nearlier approaches, thus enabling generation of larger molecules, and beats\nprevious state-of-the art baselines by a significant margin. Applying\nreinforcement learning to a combination of a custom context-free grammar with\nadditional masking to enforce non-local constraints is applicable to any\noptimization of a graph structure under a mixture of local and nonlocal\nconstraints.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 19:48:02 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Kraev", "Egor", ""]]}, {"id": "1811.11249", "submitter": "Gaetano Manzo", "authors": "Gaetano Manzo, Juan Sebastian Otalora, Marco Ajmone Marsan, and\n  Gianluca Rizzo", "title": "A Deep Learning Strategy for Vehicular Floating Content Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Floating Content (FC) is a communication paradigm for the local dissemination\nof contextualized information through D2D connectivity, in a way which\nminimizes the use of resources while achieving some specified performance\ntarget. Existing approaches to FC dimensioning are based on unrealistic system\nassumptions that make them, highly inaccurate and overly conservative when\napplied in realistic settings. In this paper, we present a first step towards\nthe development of a cognitive approach to efficient dynamic management of FC.\nWe propose a deep learning strategy for FC dimensioning, which exploits a\nConvolutional Neural Network(CNN) to efficiently modulate over time the\nresources employed by FC in a QoS-aware manner. Numerical evaluations show that\nour approach achieves a maximum rejection rate of3%, and resource savings of\n37.5% with respect to the benchmark strategy\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 08:22:51 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Manzo", "Gaetano", ""], ["Otalora", "Juan Sebastian", ""], ["Marsan", "Marco Ajmone", ""], ["Rizzo", "Gianluca", ""]]}, {"id": "1811.11259", "submitter": "Francesco Fraternali", "authors": "Francesco Fraternali, Bharathan Balaji, Rajesh Gupta", "title": "Scaling Configuration of Energy Harvesting Sensors with Reinforcement\n  Learning", "comments": "7 pages, 5 figures", "journal-ref": "ENSsys '18: International Workshop on Energy Harvesting &\n  Energy-Neutral Sensing Systems}{November 4, 2018}{Shenzhen, China", "doi": "10.1145/3279755.3279760", "report-no": null, "categories": "cs.LG cs.AI cs.DS cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of the Internet of Things (IoT), an increasing number of\nenergy harvesting methods are being used to supplement or supplant battery\nbased sensors. Energy harvesting sensors need to be configured according to the\napplication, hardware, and environmental conditions to maximize their\nusefulness. As of today, the configuration of sensors is either manual or\nheuristics based, requiring valuable domain expertise. Reinforcement learning\n(RL) is a promising approach to automate configuration and efficiently scale\nIoT deployments, but it is not yet adopted in practice. We propose solutions to\nbridge this gap: reduce the training phase of RL so that nodes are operational\nwithin a short time after deployment and reduce the computational requirements\nto scale to large deployments. We focus on configuration of the sampling rate\nof indoor solar panel based energy harvesting sensors. We created a simulator\nbased on 3 months of data collected from 5 sensor nodes subject to different\nlighting conditions. Our simulation results show that RL can effectively learn\nenergy availability patterns and configure the sampling rate of the sensor\nnodes to maximize the sensing data while ensuring that energy storage is not\ndepleted. The nodes can be operational within the first day by using our\nmethods. We show that it is possible to reduce the number of RL policies by\nusing a single policy for nodes that share similar lighting conditions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 21:05:43 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Fraternali", "Francesco", ""], ["Balaji", "Bharathan", ""], ["Gupta", "Rajesh", ""]]}, {"id": "1811.11264", "submitter": "Lei Xu", "authors": "Lei Xu, Kalyan Veeramachaneni", "title": "Synthesizing Tabular Data using Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) implicitly learn the probability\ndistribution of a dataset and can draw samples from the distribution. This\npaper presents, Tabular GAN (TGAN), a generative adversarial network which can\ngenerate tabular data like medical or educational records. Using the power of\ndeep neural networks, TGAN generates high-quality and fully synthetic tables\nwhile simultaneously generating discrete and continuous variables. When we\nevaluate our model on three datasets, we find that TGAN outperforms\nconventional statistical generative models in both capturing the correlation\nbetween columns and scaling up for large datasets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 21:13:54 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Xu", "Lei", ""], ["Veeramachaneni", "Kalyan", ""]]}, {"id": "1811.11269", "submitter": "Greg Olmschenk", "authors": "Greg Olmschenk, Zhigang Zhu, Hao Tang", "title": "Generalizing semi-supervised generative adversarial networks to\n  regression using feature contrasting", "comments": null, "journal-ref": "Computer Vision and Image Understanding, Volume 186, September\n  2019, Pages 1-12", "doi": "10.1016/j.cviu.2019.06.004", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we generalize semi-supervised generative adversarial networks\n(GANs) from classification problems to regression problems. In the last few\nyears, the importance of improving the training of neural networks using\nsemi-supervised training has been demonstrated for classification problems. We\npresent a novel loss function, called feature contrasting, resulting in a\ndiscriminator which can distinguish between fake and real data based on feature\nstatistics. This method avoids potential biases and limitations of alternative\napproaches. The generalization of semi-supervised GANs to the regime of\nregression problems of opens their use to countless applications as well as\nproviding an avenue for a deeper understanding of how GANs function. We first\ndemonstrate the capabilities of semi-supervised regression GANs on a toy\ndataset which allows for a detailed understanding of how they operate in\nvarious circumstances. This toy dataset is used to provide a theoretical basis\nof the semi-supervised regression GAN. We then apply the semi-supervised\nregression GANs to a number of real-world computer vision applications: age\nestimation, driving steering angle prediction, and crowd counting from single\nimages. We perform extensive tests of what accuracy can be achieved with\nsignificantly reduced annotated data. Through the combination of the\ntheoretical example and real-world scenarios, we demonstrate how\nsemi-supervised GANs can be generalized to regression problems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 21:31:33 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 21:37:15 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2019 17:36:13 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Olmschenk", "Greg", ""], ["Zhu", "Zhigang", ""], ["Tang", "Hao", ""]]}, {"id": "1811.11286", "submitter": "Wang Yifan", "authors": "Wang Yifan, Shihao Wu, Hui Huang, Daniel Cohen-Or, Olga\n  Sorkine-Hornung", "title": "Patch-based Progressive 3D Point Set Upsampling", "comments": "accepted to cvpr2019, code available at https://github.com/yifita/P3U", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a detail-driven deep neural network for point set upsampling. A\nhigh-resolution point set is essential for point-based rendering and surface\nreconstruction. Inspired by the recent success of neural image super-resolution\ntechniques, we progressively train a cascade of patch-based upsampling networks\non different levels of detail end-to-end. We propose a series of architectural\ndesign contributions that lead to a substantial performance boost. The effect\nof each technical contribution is demonstrated in an ablation study.\nQualitative and quantitative experiments show that our method significantly\noutperforms the state-of-the-art learning-based and optimazation-based\napproaches, both in terms of handling low-resolution inputs and revealing\nhigh-fidelity details.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 22:01:55 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 12:42:31 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2019 17:08:50 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Yifan", "Wang", ""], ["Wu", "Shihao", ""], ["Huang", "Hui", ""], ["Cohen-Or", "Daniel", ""], ["Sorkine-Hornung", "Olga", ""]]}, {"id": "1811.11287", "submitter": "Ben Moews", "authors": "Ben Moews, J. Michael Herrmann, Gbenga Ibikunle", "title": "Lagged correlation-based deep learning for directional trend change\n  prediction in financial time series", "comments": "11 pages, 4 figures", "journal-ref": "Expert Syst. Appl. 120 (2019) 197-206", "doi": "10.1016/j.eswa.2018.11.027", "report-no": null, "categories": "q-fin.CP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trend change prediction in complex systems with a large number of noisy time\nseries is a problem with many applications for real-world phenomena, with stock\nmarkets as a notoriously difficult to predict example of such systems. We\napproach predictions of directional trend changes via complex lagged\ncorrelations between them, excluding any information about the target series\nfrom the respective inputs to achieve predictions purely based on such\ncorrelations with other series. We propose the use of deep neural networks that\nemploy step-wise linear regressions with exponential smoothing in the\npreparatory feature engineering for this task, with regression slopes as trend\nstrength indicators for a given time interval. We apply this method to\nhistorical stock market data from 2011 to 2016 as a use case example of lagged\ncorrelations between large numbers of time series that are heavily influenced\nby externally arising new information as a random factor. The results\ndemonstrate the viability of the proposed approach, with state-of-the-art\naccuracies and accounting for the statistical significance of the results for\nadditional validation, as well as important implications for modern financial\neconomics.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 22:03:41 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 09:35:15 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Moews", "Ben", ""], ["Herrmann", "J. Michael", ""], ["Ibikunle", "Gbenga", ""]]}, {"id": "1811.11293", "submitter": "Bogdan Kulynych", "authors": "Rebekah Overdorf, Bogdan Kulynych, Ero Balsa, Carmela Troncoso, Seda\n  G\\\"urses", "title": "Questioning the assumptions behind fairness solutions", "comments": "Presented at Critiquing and Correcting Trends in Machine Learning\n  (NeurIPS 2018 Workshop), Montreal, Canada. This is a short version of\n  arXiv:1806.02711", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition to their benefits, optimization systems can have negative\neconomic, moral, social, and political effects on populations as well as their\nenvironments. Frameworks like fairness have been proposed to aid service\nproviders in addressing subsequent bias and discrimination during data\ncollection and algorithm design. However, recent reports of neglect,\nunresponsiveness, and malevolence cast doubt on whether service providers can\neffectively implement fairness solutions. These reports invite us to revisit\nassumptions made about the service providers in fairness solutions. Namely,\nthat service providers have (i) the incentives or (ii) the means to mitigate\noptimization externalities. Moreover, the environmental impact of these systems\nsuggests that we need (iii) novel frameworks that consider systems other than\nalgorithmic decision-making and recommender systems, and (iv) solutions that go\nbeyond removing related algorithmic biases. Going forward, we propose\nProtective Optimization Technologies that enable optimization subjects to\ndefend against negative consequences of optimization systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 22:23:00 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Overdorf", "Rebekah", ""], ["Kulynych", "Bogdan", ""], ["Balsa", "Ero", ""], ["Troncoso", "Carmela", ""], ["G\u00fcrses", "Seda", ""]]}, {"id": "1811.11296", "submitter": "Richard Marriott", "authors": "Richard T. Marriott, Sami Romdhani and Liming Chen", "title": "Taking Control of Intra-class Variation in Conditional GANs Under Weak\n  Supervision", "comments": null, "journal-ref": "in 2020 15th IEEE International Conference on Automatic Face and\n  Gesture Recognition (FG 2020) (FG), Buenos Aires, AR, 2020 pp. 283-290", "doi": "10.1109/FG47880.2020.00042", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are able to learn mappings between\nsimple, relatively low-dimensional, random distributions and points on the\nmanifold of realistic images in image-space. The semantics of this mapping,\nhowever, are typically entangled such that meaningful image properties cannot\nbe controlled independently of one another. Conditional GANs (cGANs) provide a\npotential solution to this problem, allowing specific semantics to be enforced\nduring training. This solution, however, depends on the availability of precise\nlabels, which are sometimes difficult or near impossible to obtain, e.g. labels\nrepresenting lighting conditions or describing the background. In this paper we\nintroduce a new formulation of the cGAN that is able to learn disentangled,\nmultivariate models of semantically meaningful variation and which has the\nadvantage of requiring only the weak supervision of binary attribute labels.\nFor example, given only labels of ambient / non-ambient lighting, our method is\nable to learn multivariate lighting models disentangled from other factors such\nas the identity and pose. We coin the method intra-class variation isolation\n(IVI) and the resulting network the IVI-GAN. We evaluate IVI-GAN on the CelebA\ndataset and on synthetic 3D morphable model data, learning to disentangle\nattributes such as lighting, pose, expression, and even the background.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 22:38:29 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2020 00:10:05 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Marriott", "Richard T.", ""], ["Romdhani", "Sami", ""], ["Chen", "Liming", ""]]}, {"id": "1811.11298", "submitter": "Arash Tavakoli", "authors": "Arash Tavakoli, Vitaly Levdik, Riashat Islam, Christopher M. Smith,\n  Petar Kormushev", "title": "Exploring Restart Distributions", "comments": "RLDM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the generic approach of using an experience memory to help\nexploration by adapting a restart distribution. That is, given the capacity to\nreset the state with those corresponding to the agent's past observations, we\nhelp exploration by promoting faster state-space coverage via restarting the\nagent from a more diverse set of initial states, as well as allowing it to\nrestart in states associated with significant past experiences. This approach\nis compatible with both on-policy and off-policy methods. However, a caveat is\nthat altering the distribution of initial states could change the optimal\npolicies when searching within a restricted class of policies. To reduce this\nunsought learning bias, we evaluate our approach in deep reinforcement learning\nwhich benefits from the high representational capacity of deep neural networks.\nWe instantiate three variants of our approach, each inspired by an idea in the\ncontext of experience replay. Using these variants, we show that performance\ngains can be achieved, especially in hard exploration problems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 22:40:01 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 21:28:54 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 03:42:32 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Tavakoli", "Arash", ""], ["Levdik", "Vitaly", ""], ["Islam", "Riashat", ""], ["Smith", "Christopher M.", ""], ["Kormushev", "Petar", ""]]}, {"id": "1811.11304", "submitter": "Mahyar Najibi", "authors": "Ali Shafahi, Mahyar Najibi, Zheng Xu, John Dickerson, Larry S. Davis,\n  Tom Goldstein", "title": "Universal Adversarial Training", "comments": "Accepted to AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard adversarial attacks change the predicted class label of a selected\nimage by adding specially tailored small perturbations to its pixels. In\ncontrast, a universal perturbation is an update that can be added to any image\nin a broad class of images, while still changing the predicted class label. We\nstudy the efficient generation of universal adversarial perturbations, and also\nefficient methods for hardening networks to these attacks. We propose a simple\noptimization-based universal attack that reduces the top-1 accuracy of various\nnetwork architectures on ImageNet to less than 20%, while learning the\nuniversal perturbation 13X faster than the standard method.\n  To defend against these perturbations, we propose universal adversarial\ntraining, which models the problem of robust classifier generation as a\ntwo-player min-max game, and produces robust models with only 2X the cost of\nnatural training. We also propose a simultaneous stochastic gradient method\nthat is almost free of extra computation, which allows us to do universal\nadversarial training on ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 23:09:27 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 20:57:36 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Shafahi", "Ali", ""], ["Najibi", "Mahyar", ""], ["Xu", "Zheng", ""], ["Dickerson", "John", ""], ["Davis", "Larry S.", ""], ["Goldstein", "Tom", ""]]}, {"id": "1811.11307", "submitter": "Craig Macartney", "authors": "Craig Macartney and Tillman Weyde", "title": "Improved Speech Enhancement with the Wave-U-Net", "comments": "5 pages (including 1 for References), 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE eess.AS eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the use of the Wave-U-Net architecture for speech enhancement, a\nmodel introduced by Stoller et al for the separation of music vocals and\naccompaniment. This end-to-end learning method for audio source separation\noperates directly in the time domain, permitting the integrated modelling of\nphase information and being able to take large temporal contexts into account.\nOur experiments show that the proposed method improves several metrics, namely\nPESQ, CSIG, CBAK, COVL and SSNR, over the state-of-the-art with respect to the\nspeech enhancement task on the Voice Bank corpus (VCTK) dataset. We find that a\nreduced number of hidden layers is sufficient for speech enhancement in\ncomparison to the original system designed for singing voice separation in\nmusic. We see this initial result as an encouraging signal to further explore\nspeech enhancement in the time-domain, both as an end in itself and as a\npre-processing step to speech recognition systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 23:11:05 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Macartney", "Craig", ""], ["Weyde", "Tillman", ""]]}, {"id": "1811.11310", "submitter": "Kevin McCloskey", "authors": "Kevin McCloskey, Ankur Taly, Federico Monti, Michael P. Brenner, Lucy\n  Colwell", "title": "Using Attribution to Decode Dataset Bias in Neural Network Models for\n  Chemistry", "comments": null, "journal-ref": null, "doi": "10.1073/pnas.1820657116", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved state of the art accuracy at classifying\nmolecules with respect to whether they bind to specific protein targets. A key\nbreakthrough would occur if these models could reveal the fragment\npharmacophores that are causally involved in binding. Extracting chemical\ndetails of binding from the networks could potentially lead to scientific\ndiscoveries about the mechanisms of drug actions. But doing so requires shining\nlight into the black box that is the trained neural network model, a task that\nhas proved difficult across many domains. Here we show how the binding\nmechanism learned by deep neural network models can be interrogated, using a\nrecently described attribution method. We first work with carefully constructed\nsynthetic datasets, in which the 'fragment logic' of binding is fully known. We\nfind that networks that achieve perfect accuracy on held out test datasets\nstill learn spurious correlations due to biases in the datasets, and we are\nable to exploit this non-robustness to construct adversarial examples that fool\nthe model. The dataset bias makes these models unreliable for accurately\nrevealing information about the mechanisms of protein-ligand binding. In light\nof our findings, we prescribe a test that checks for dataset bias given a\nhypothesis. If the test fails, it indicates that either the model must be\nsimplified or regularized and/or that the training dataset requires\naugmentation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 23:33:05 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 20:05:44 GMT"}, {"version": "v3", "created": "Sun, 19 May 2019 04:29:23 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["McCloskey", "Kevin", ""], ["Taly", "Ankur", ""], ["Monti", "Federico", ""], ["Brenner", "Michael P.", ""], ["Colwell", "Lucy", ""]]}, {"id": "1811.11312", "submitter": "Rivindu Weerasekera", "authors": "Shamane Siriwardhana, Rivindu Weerasekera, Suranga Nanayakkara", "title": "Target Driven Visual Navigation with Hybrid Asynchronous Universal\n  Successor Representations", "comments": "Deep Reinforcement Learning Workshop, NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to navigate to a target with minimal supervision and prior\nknowledge is critical to creating human-like assistive agents. Prior work on\nmap-based and map-less approaches have limited generalizability. In this paper,\nwe present a novel approach, Hybrid Asynchronous Universal Successor\nRepresentations (HAUSR), which overcomes the problem of generalizability to new\ngoals by adapting recent work on Universal Successor Representations with\nAsynchronous Actor-Critic Agents. We show that the agent was able to\nsuccessfully reach novel goals and we were able to quickly fine-tune the\nnetwork for adapting to new scenes. This opens up novel application scenarios\nwhere intelligent agents could learn from and adapt to a wide range of\nenvironments with minimal human input.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 23:37:19 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Siriwardhana", "Shamane", ""], ["Weerasekera", "Rivindu", ""], ["Nanayakkara", "Suranga", ""]]}, {"id": "1811.11320", "submitter": "Yu Shi", "authors": "Yu Shi and Xinwei He and Naijing Zhang and Carl Yang and Jiawei Han", "title": "User-Guided Clustering in Heterogeneous Information Networks via\n  Motif-Based Comprehensive Transcription", "comments": "24 pages including additional supplementary materials. In Proceedings\n  of the 2019 European Conference on Machine Learning and Principles and\n  Practice of Knowledge Discovery in Databases, W\\\"urzburg, Germany, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous information networks (HINs) with rich semantics are ubiquitous\nin real-world applications. For a given HIN, many reasonable clustering results\nwith distinct semantic meaning can simultaneously exist. User-guided clustering\nis hence of great practical value for HINs where users provide labels to a\nsmall portion of nodes. To cater to a broad spectrum of user guidance evidenced\nby different expected clustering results, carefully exploiting the signals\nresiding in the data is potentially useful. Meanwhile, as one type of complex\nnetworks, HINs often encapsulate higher-order interactions that reflect the\ninterlocked nature among nodes and edges. Network motifs, sometimes referred to\nas meta-graphs, have been used as tools to capture such higher-order\ninteractions and reveal the many different semantics. We therefore approach the\nproblem of user-guided clustering in HINs with network motifs. In this process,\nwe identify the utility and importance of directly modeling higher-order\ninteractions without collapsing them to pairwise interactions. To achieve this,\nwe comprehensively transcribe the higher-order interaction signals to a series\nof tensors via motifs and propose the MoCHIN model based on joint non-negative\ntensor factorization. This approach applies to arbitrarily many, arbitrary\nforms of HIN motifs. An inference algorithm with speed-up methods is also\nproposed to tackle the challenge that tensor size grows exponentially as the\nnumber of nodes in a motif increases. We validate the effectiveness of the\nproposed method on two real-world datasets and three tasks, and MoCHIN\noutperforms all baselines in three evaluation tasks under three different\nmetrics. Additional experiments demonstrated the utility of motifs and the\nbenefit of directly modeling higher-order information especially when user\nguidance is limited.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 00:16:03 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 02:51:29 GMT"}, {"version": "v3", "created": "Sun, 22 Sep 2019 22:39:09 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Shi", "Yu", ""], ["He", "Xinwei", ""], ["Zhang", "Naijing", ""], ["Yang", "Carl", ""], ["Han", "Jiawei", ""]]}, {"id": "1811.11329", "submitter": "Xinshuo Weng", "authors": "Sen Wang, Daoyuan Jia, Xinshuo Weng", "title": "Deep Reinforcement Learning for Autonomous Driving", "comments": "no time for further improvement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning has steadily improved and outperform human in lots of\ntraditional games since the resurgence of deep neural network. However, these\nsuccess is not easy to be copied to autonomous driving because the state spaces\nin real world are extreme complex and action spaces are continuous and fine\ncontrol is required. Moreover, the autonomous driving vehicles must also keep\nfunctional safety under the complex environments. To deal with these\nchallenges, we first adopt the deep deterministic policy gradient (DDPG)\nalgorithm, which has the capacity to handle complex state and action spaces in\ncontinuous domain. We then choose The Open Racing Car Simulator (TORCS) as our\nenvironment to avoid physical damage. Meanwhile, we select a set of appropriate\nsensor information from TORCS and design our own rewarder. In order to fit DDPG\nalgorithm to TORCS, we design our network architecture for both actor and\ncritic inside DDPG paradigm. To demonstrate the effectiveness of our model, We\nevaluate on different modes in TORCS and show both quantitative and qualitative\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 00:56:57 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 02:57:33 GMT"}, {"version": "v3", "created": "Sun, 19 May 2019 20:51:26 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Wang", "Sen", ""], ["Jia", "Daoyuan", ""], ["Weng", "Xinshuo", ""]]}, {"id": "1811.11339", "submitter": "Hanshen Xiao", "authors": "Nan Du, Zhikang Wang and Hanshen Xiao", "title": "Statistical Robust Chinese Remainder Theorem for Multiple Numbers:\n  Wrapped Gaussian Mixture Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Chinese Remainder Theorem (CRT) has been shown to be a powerful\napproach to solve the ambiguity resolution problem. However, with its close\nrelationship to number theory, study in this area is mainly from a coding\ntheory perspective under deterministic conditions. Nevertheless, it can be\nproved that even with the best deterministic condition known, the probability\nof success in robust reconstruction degrades exponentially as the number of\nestimand increases. In this paper, we present the first rigorous analysis on\nthe underlying statistical model of CRT-based multiple parameter estimation,\nwhere a generalized Gaussian mixture with background knowledge on samplings is\nproposed. To address the problem, two novel approaches are introduced. One is\nto directly calculate the conditional maximal a posteriori probability (MAP)\nestimation of residue clustering, and the other is to iteratively search for\nMAP of both common residues and clustering. Moreover, remainder\nerror-correcting codes are introduced to improve the robustness further. It is\nshown that this statistically based scheme achieves much stronger robustness\ncompared to state-of-the-art deterministic schemes, especially in low and\nmedian Signal Noise Ratio (SNR) scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 01:44:47 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Du", "Nan", ""], ["Wang", "Zhikang", ""], ["Xiao", "Hanshen", ""]]}, {"id": "1811.11347", "submitter": "Humza Haider", "authors": "Humza Haider, Bret Hoehn, Sarah Davis, Russell Greiner", "title": "Effective Ways to Build and Evaluate Individual Survival Distributions", "comments": "34 pages (main text), 12 figures", "journal-ref": "Journal of Machine Learning Research (JMLR) Volume 21 (2020)\n  18-772", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accurate model of a patient's individual survival distribution can help\ndetermine the appropriate treatment for terminal patients. Unfortunately, risk\nscores (e.g., from Cox Proportional Hazard models) do not provide survival\nprobabilities, single-time probability models (e.g., the Gail model, predicting\n5 year probability) only provide for a single time point, and standard\nKaplan-Meier survival curves provide only population averages for a large class\nof patients meaning they are not specific to individual patients. This\nmotivates an alternative class of tools that can learn a model which provides\nan individual survival distribution which gives survival probabilities across\nall times - such as extensions to the Cox model, Accelerated Failure Time, an\nextension to Random Survival Forests, and Multi-Task Logistic Regression. This\npaper first motivates such \"individual survival distribution\" (ISD) models, and\nexplains how they differ from standard models. It then discusses ways to\nevaluate such models - namely Concordance, 1-Calibration, Brier score, and\nvarious versions of L1-loss - and then motivates and defines a novel approach\n\"D-Calibration\", which determines whether a model's probability estimates are\nmeaningful. We also discuss how these measures differ, and use them to evaluate\nseveral ISD prediction tools, over a range of survival datasets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 02:00:54 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Haider", "Humza", ""], ["Hoehn", "Bret", ""], ["Davis", "Sarah", ""], ["Greiner", "Russell", ""]]}, {"id": "1811.11353", "submitter": "Alex De Sa'", "authors": "Alex G. C. de S\\'a, Cristiano G. Pimenta, Gisele L. Pappa and Alex A.\n  Freitas", "title": "Multi-label classification search space in the MEKA software", "comments": "Supplementary Material (GECCO'2020): Proposed Search Spaces", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This supplementary material aims to describe the proposed multi-label\nclassification (MLC) search spaces based on the MEKA and WEKA softwares. First,\nwe overview 26 MLC algorithms and meta-algorithms in MEKA, presenting their\nmain characteristics, such as hyper-parameters, dependencies and constraints.\nSecond, we review 28 single-label classification (SLC) algorithms,\npreprocessing algorithms and meta-algorithms in the WEKA software. These SLC\nalgorithms were also studied because they are part of the proposed MLC search\nspaces. Fundamentally, this occurs due to the problem transformation nature of\nseveral MLC algorithms used in this work. These algorithms transform an MLC\nproblem into one or several SLC problems in the first place and solve them with\nSLC model(s) in a next step. Therefore, understanding their main\ncharacteristics is crucial to this work. Finally, we present a formal\ndescription of the search spaces by proposing a context-free grammar that\nencompasses the 54 learning algorithms. This grammar basically comprehends the\npossible combinations, the constraints and dependencies among the learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 02:19:33 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 02:42:25 GMT"}, {"version": "v3", "created": "Wed, 13 May 2020 13:52:01 GMT"}, {"version": "v4", "created": "Thu, 14 May 2020 14:32:48 GMT"}, {"version": "v5", "created": "Fri, 31 Jul 2020 17:06:56 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["de S\u00e1", "Alex G. C.", ""], ["Pimenta", "Cristiano G.", ""], ["Pappa", "Gisele L.", ""], ["Freitas", "Alex A.", ""]]}, {"id": "1811.11357", "submitter": "Ryan Turner", "authors": "Ryan Turner, Jane Hung, Eric Frank, Yunus Saatci, Jason Yosinski", "title": "Metropolis-Hastings Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Metropolis-Hastings generative adversarial network (MH-GAN),\nwhich combines aspects of Markov chain Monte Carlo and GANs. The MH-GAN draws\nsamples from the distribution implicitly defined by a GAN's\ndiscriminator-generator pair, as opposed to standard GANs which draw samples\nfrom the distribution defined only by the generator. It uses the discriminator\nfrom GAN training to build a wrapper around the generator for improved\nsampling. With a perfect discriminator, this wrapped generator samples from the\ntrue distribution on the data exactly even when the generator is imperfect. We\ndemonstrate the benefits of the improved generator on multiple benchmark\ndatasets, including CIFAR-10 and CelebA, using the DCGAN, WGAN, and progressive\nGAN.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 02:28:33 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 22:33:37 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Turner", "Ryan", ""], ["Hung", "Jane", ""], ["Frank", "Eric", ""], ["Saatci", "Yunus", ""], ["Yosinski", "Jason", ""]]}, {"id": "1811.11359", "submitter": "David Warde-Farley", "authors": "David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu,\n  Steven Hansen, Volodymyr Mnih", "title": "Unsupervised Control Through Non-Parametric Discriminative Rewards", "comments": "10 pages + references & 5 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to control an environment without hand-crafted rewards or expert\ndata remains challenging and is at the frontier of reinforcement learning\nresearch. We present an unsupervised learning algorithm to train agents to\nachieve perceptually-specified goals using only a stream of observations and\nactions. Our agent simultaneously learns a goal-conditioned policy and a goal\nachievement reward function that measures how similar a state is to the goal\nstate. This dual optimization leads to a co-operative game, giving rise to a\nlearned reward function that reflects similarity in controllable aspects of the\nenvironment instead of distance in the space of observations. We demonstrate\nthe efficacy of our agent to learn, in an unsupervised manner, to reach a\ndiverse set of goals on three domains -- Atari, the DeepMind Control Suite and\nDeepMind Lab.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 02:35:24 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Warde-Farley", "David", ""], ["Van de Wiele", "Tom", ""], ["Kulkarni", "Tejas", ""], ["Ionescu", "Catalin", ""], ["Hansen", "Steven", ""], ["Mnih", "Volodymyr", ""]]}, {"id": "1811.11368", "submitter": "Yichen Zhang", "authors": "Xi Chen, Weidong Liu, Yichen Zhang", "title": "First-order Newton-type Estimator for Distributed Estimation and\n  Inference", "comments": "60 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies distributed estimation and inference for a general\nstatistical problem with a convex loss that could be non-differentiable. For\nthe purpose of efficient computation, we restrict ourselves to stochastic\nfirst-order optimization, which enjoys low per-iteration complexity. To\nmotivate the proposed method, we first investigate the theoretical properties\nof a straightforward Divide-and-Conquer Stochastic Gradient Descent (DC-SGD)\napproach. Our theory shows that there is a restriction on the number of\nmachines and this restriction becomes more stringent when the dimension $p$ is\nlarge. To overcome this limitation, this paper proposes a new multi-round\ndistributed estimation procedure that approximates the Newton step only using\nstochastic subgradient. The key component in our method is the proposal of a\ncomputationally efficient estimator of $\\Sigma^{-1} w$, where $\\Sigma$ is the\npopulation Hessian matrix and $w$ is any given vector. Instead of estimating\n$\\Sigma$ (or $\\Sigma^{-1}$) that usually requires the second-order\ndifferentiability of the loss, the proposed First-Order Newton-type Estimator\n(FONE) directly estimates the vector of interest $\\Sigma^{-1} w$ as a whole and\nis applicable to non-differentiable losses. Our estimator also facilitates the\ninference for the empirical risk minimizer. It turns out that the key term in\nthe limiting covariance has the form of $\\Sigma^{-1} w$, which can be estimated\nby FONE.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 02:58:28 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 17:10:35 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Chen", "Xi", ""], ["Liu", "Weidong", ""], ["Zhang", "Yichen", ""]]}, {"id": "1811.11373", "submitter": "Alessio Lomuscio", "authors": "Panagiotis Kouvaros and Alessio Lomuscio", "title": "Formal Verification of CNN-based Perception Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of verifying neural-based perception systems\nimplemented by convolutional neural networks. We define a notion of local\nrobustness based on affine and photometric transformations. We show the notion\ncannot be captured by previously employed notions of robustness. The method\nproposed is based on reachability analysis for feed-forward neural networks and\nrelies on MILP encodings of both the CNNs and transformations under question.\nWe present an implementation and discuss the experimental results obtained for\na CNN trained from the MNIST data set.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 03:36:25 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Kouvaros", "Panagiotis", ""], ["Lomuscio", "Alessio", ""]]}, {"id": "1811.11390", "submitter": "Kerem Camsari", "authors": "Ramtin Zand, Kerem Y. Camsari, Supriyo Datta and Ronald F. DeMara", "title": "Composable Probabilistic Inference Networks Using MRAM-based Stochastic\n  Neurons", "comments": null, "journal-ref": "ACM Journal on Emerging Technologies in Computing Systems (JETC)\n  (2019)", "doi": "10.1145/3304105", "report-no": null, "categories": "cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetoresistive random access memory (MRAM) technologies with thermally\nunstable nanomagnets are leveraged to develop an intrinsic stochastic neuron as\na building block for restricted Boltzmann machines (RBMs) to form deep belief\nnetworks (DBNs). The embedded MRAM-based neuron is modeled using precise\nphysics equations. The simulation results exhibit the desired sigmoidal\nrelation between the input voltages and probability of the output state. A\nprobabilistic inference network simulator (PIN-Sim) is developed to realize a\ncircuit-level model of an RBM utilizing resistive crossbar arrays along with\ndifferential amplifiers to implement the positive and negative weight values.\nThe PIN-Sim is composed of five main blocks to train a DBN, evaluate its\naccuracy, and measure its power consumption. The MNIST dataset is leveraged to\ninvestigate the energy and accuracy tradeoffs of seven distinct network\ntopologies in SPICE using the 14nm HP-FinFET technology library with the\nnominal voltage of 0.8V, in which an MRAM-based neuron is used as the\nactivation function. The software and hardware level simulations indicate that\na $784\\times200\\times10$ topology can achieve less than 5% error rates with\n$\\sim400 pJ$ energy consumption. The error rates can be reduced to 2.5% by\nusing a $784\\times500\\times500\\times500\\times10$ DBN at the cost of\n$\\sim10\\times$ higher energy consumption and significant area overhead.\nFinally, the effects of specific hardware-level parameters on power dissipation\nand accuracy tradeoffs are identified via the developed PIN-Sim framework.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 05:23:19 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Zand", "Ramtin", ""], ["Camsari", "Kerem Y.", ""], ["Datta", "Supriyo", ""], ["DeMara", "Ronald F.", ""]]}, {"id": "1811.11400", "submitter": "Dianbo Liu Dr", "authors": "Dianbo Liu, Timothy Miller, Raheel Sayeed and Kenneth D. Mandl", "title": "FADL:Federated-Autonomous Deep Learning for Distributed Electronic\n  Health Record", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:cs/0101200", "journal-ref": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018", "doi": null, "report-no": "ML4H/2018/54", "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health record (EHR) data is collected by individual institutions\nand often stored across locations in silos. Getting access to these data is\ndifficult and slow due to security, privacy, regulatory, and operational\nissues. We show, using ICU data from 58 different hospitals, that machine\nlearning models to predict patient mortality can be trained efficiently without\nmoving health data out of their silos using a distributed machine learning\nstrategy. We propose a new method, called Federated-Autonomous Deep Learning\n(FADL) that trains part of the model using all data sources in a distributed\nmanner and other parts using data from specific data sources. We observed that\nFADL outperforms traditional federated learning strategy and conclude that\nbalance between global and local training is an important factor to consider\nwhen design distributed machine learning methods , especially in healthcare.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 06:06:38 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 02:08:28 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Liu", "Dianbo", ""], ["Miller", "Timothy", ""], ["Sayeed", "Raheel", ""], ["Mandl", "Kenneth D.", ""]]}, {"id": "1811.11402", "submitter": "Siddique Latif", "authors": "Siddique Latif, Rajib Rana, and Junaid Qadir", "title": "Adversarial Machine Learning And Speech Emotion Recognition: Utilizing\n  Generative Adversarial Networks For Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has undoubtedly offered tremendous improvements in the\nperformance of state-of-the-art speech emotion recognition (SER) systems.\nHowever, recent research on adversarial examples poses enormous challenges on\nthe robustness of SER systems by showing the susceptibility of deep neural\nnetworks to adversarial examples as they rely only on small and imperceptible\nperturbations. In this study, we evaluate how adversarial examples can be used\nto attack SER systems and propose the first black-box adversarial attack on SER\nsystems. We also explore potential defenses including adversarial training and\ngenerative adversarial network (GAN) to enhance robustness. Experimental\nevaluations suggest various interesting aspects of the effective utilization of\nadversarial examples useful for achieving robustness for SER systems opening up\nopportunities for researchers to further innovate in this space.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 06:26:03 GMT"}, {"version": "v2", "created": "Sun, 30 Dec 2018 10:44:21 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Latif", "Siddique", ""], ["Rana", "Rajib", ""], ["Qadir", "Junaid", ""]]}, {"id": "1811.11419", "submitter": "Emilie Kaufmann", "authors": "Emilie Kaufmann (SEQUEL), Wouter Koolen (CWI)", "title": "Mixture Martingales Revisited with Applications to Sequential Tests and\n  Confidence Intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents new deviation inequalities that are valid uniformly in\ntime under adaptive sampling in a multi-armed bandit model. The deviations are\nmeasured using the Kullback-Leibler divergence in a given one-dimensional\nexponential family, and may take into account several arms at a time. They are\nobtained by constructing for each arm a mixture martingale based on a\nhierarchical prior, and by multiplying those martingales. Our deviation\ninequalities allow us to analyze stopping rules based on generalized likelihood\nratios for a large class of sequential identification problems, and to\nconstruct tight confidence intervals for some functions of the means of the\narms.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 07:36:18 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Kaufmann", "Emilie", "", "SEQUEL"], ["Koolen", "Wouter", "", "CWI"]]}, {"id": "1811.11426", "submitter": "Maciej Zamorski", "authors": "Maciej Zamorski and Maciej Zi\\k{e}ba", "title": "Semi-supervised learning with Bidirectional GANs", "comments": "12 pages, 3 figures", "journal-ref": "Intelligent Information and Database Systems. ACIIDS 2019. Lecture\n  Notes in Computer Science, vol 11431 (2019) 649-660", "doi": "10.1007/978-3-030-14799-0_56", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a novel approach to train Bidirectional Generative\nAdversarial Model (BiGAN) in a semi-supervised manner. The presented method\nutilizes triplet loss function as an additional component of the objective\nfunction used to train discriminative data representation in the latent space\nof the BiGAN model. This representation can be further used as a seed for\ngenerating artificial images, but also as a good feature embedding for\nclassification and image retrieval tasks. We evaluate the quality of the\nproposed method in the two mentioned challenging tasks using two benchmark\ndatasets: CIFAR10 and SVHN.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 07:51:21 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Zamorski", "Maciej", ""], ["Zi\u0119ba", "Maciej", ""]]}, {"id": "1811.11427", "submitter": "Ragunathan Mariappan", "authors": "Ragunathan Mariappan and Vaibhav Rajan", "title": "Deep Collective Matrix Factorization for Augmented Multi-View Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning by integrating multiple heterogeneous data sources is a common\nrequirement in many tasks. Collective Matrix Factorization (CMF) is a technique\nto learn shared latent representations from arbitrary collections of matrices.\nIt can be used to simultaneously complete one or more matrices, for predicting\nthe unknown entries. Classical CMF methods assume linearity in the interaction\nof latent factors which can be restrictive and fails to capture complex\nnon-linear interactions. In this paper, we develop the first deep-learning\nbased method, called dCMF, for unsupervised learning of multiple shared\nrepresentations, that can model such non-linear interactions, from an arbitrary\ncollection of matrices. We address optimization challenges that arise due to\ndependencies between shared representations through Multi-Task Bayesian\nOptimization and design an acquisition function adapted for collective learning\nof hyperparameters. Our experiments show that dCMF significantly outperforms\nprevious CMF algorithms in integrating heterogeneous data for predictive\nmodeling. Further, on two tasks - recommendation and prediction of gene-disease\nassociation - dCMF outperforms state-of-the-art matrix completion algorithms\nthat can utilize auxiliary sources of information.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 07:52:48 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 05:28:25 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Mariappan", "Ragunathan", ""], ["Rajan", "Vaibhav", ""]]}, {"id": "1811.11433", "submitter": "Pierre Ablin", "authors": "Pierre Ablin (PARIETAL), Jean-Fran\\c{c}ois Cardoso (CNRS, IAP),\n  Alexandre Gramfort (PARIETAL)", "title": "Beyond Pham's algorithm for joint diagonalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approximate joint diagonalization of a set of matrices consists in\nfinding a basis in which these matrices are as diagonal as possible. This\nproblem naturally appears in several statistical learning tasks such as blind\nsignal separation. We consider the diagonalization criterion studied in a\nseminal paper by Pham (2001), and propose a new quasi-Newton method for its\noptimization. Through numerical experiments on simulated and real datasets, we\nshow that the proposed method outper-forms Pham's algorithm. An open source\nPython package is released.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 08:03:18 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Ablin", "Pierre", "", "PARIETAL"], ["Cardoso", "Jean-Fran\u00e7ois", "", "CNRS, IAP"], ["Gramfort", "Alexandre", "", "PARIETAL"]]}, {"id": "1811.11441", "submitter": "Sujoy Paul", "authors": "Sujoy Paul and Jeroen van Baar", "title": "Trajectory-based Learning for Ball-in-Maze Games", "comments": "Accepted at NIPS 2018 Workshop on Imitation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Reinforcement Learning has shown tremendous success in solving several\ngames and tasks in robotics. However, unlike humans, it generally requires a\nlot of training instances. Trajectories imitating to solve the task at hand can\nhelp to increase sample-efficiency of deep RL methods. In this paper, we\npresent a simple approach to use such trajectories, applied to the challenging\nBall-in-Maze Games, recently introduced in the literature. We show that in\nspite of not using human-generated trajectories and just using the simulator as\na model to generate a limited number of trajectories, we can get a speed-up of\nabout 2-3x in the learning process. We also discuss some challenges we observed\nwhile using trajectory-based learning for very sparse reward functions.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 08:38:40 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 17:03:08 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Paul", "Sujoy", ""], ["van Baar", "Jeroen", ""]]}, {"id": "1811.11456", "submitter": "Soumen Chakrabarti", "authors": "Divam Gupta, Tanmoy Chakraborty, Soumen Chakrabarti", "title": "GIRNet: Interleaved Multi-Task Recurrent State Sequence Models", "comments": "Accepted at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several natural language tasks, labeled sequences are available in\nseparate domains (say, languages), but the goal is to label sequences with\nmixed domain (such as code-switched text). Or, we may have available models for\nlabeling whole passages (say, with sentiments), which we would like to exploit\ntoward better position-specific label inference (say, target-dependent\nsentiment annotation). A key characteristic shared across such tasks is that\ndifferent positions in a primary instance can benefit from different `experts'\ntrained from auxiliary data, but labeled primary instances are scarce, and\nlabeling the best expert for each position entails unacceptable cognitive\nburden. We propose GITNet, a unified position-sensitive multi-task recurrent\nneural network (RNN) architecture for such applications. Auxiliary and primary\ntasks need not share training instances. Auxiliary RNNs are trained over\nauxiliary instances. A primary instance is also submitted to each auxiliary\nRNN, but their state sequences are gated and merged into a novel composite\nstate sequence tailored to the primary inference task. Our approach is in sharp\ncontrast to recent multi-task networks like the cross-stitch and sluice\nnetwork, which do not control state transfer at such fine granularity. We\ndemonstrate the superiority of GIRNet using three applications: sentiment\nclassification of code-switched passages, part-of-speech tagging of\ncode-switched text, and target position-sensitive annotation of sentiment in\nmonolingual passages. In all cases, we establish new state-of-the-art\nperformance beyond recent competitive baselines.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 09:20:13 GMT"}, {"version": "v2", "created": "Tue, 25 Dec 2018 07:04:27 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Gupta", "Divam", ""], ["Chakraborty", "Tanmoy", ""], ["Chakrabarti", "Soumen", ""]]}, {"id": "1811.11474", "submitter": "Jakub Pr\\\"uher", "authors": "Jakub Pr\\\"uher, Toni Karvonen, Chris J. Oates, Ond\\v{r}ej Straka, Simo\n  S\\\"arkk\\\"a", "title": "Improved Calibration of Numerical Integration Error in Sigma-Point\n  Filters", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The sigma-point filters, such as the UKF, which exploit numerical quadrature\nto obtain an additional order of accuracy in the moment transformation step,\nare popular alternatives to the ubiquitous EKF. The classical quadrature rules\nused in the sigma-point filters are motivated via polynomial approximation of\nthe integrand, however in the applied context these assumptions cannot always\nbe justified. As a result, quadrature error can introduce bias into estimated\nmoments, for which there is no compensatory mechanism in the classical\nsigma-point filters. This can lead in turn to estimates and predictions that\nare poorly calibrated. In this article, we investigate the Bayes-Sard\nquadrature method in the context of sigma-point filters, which enables\nuncertainty due to quadrature error to be formalised within a probabilistic\nmodel. Our first contribution is to derive the well-known classical quadratures\nas special cases of the Bayes-Sard quadrature method. Then a general-purpose\nmoment transform is developed and utilised in the design of novel sigma-point\nfilters, so that uncertainty due to quadrature error is explicitly quantified.\nNumerical experiments on a challenging tracking example with misspecified\ninitial conditions show that the additional uncertainty quantification built\ninto our method leads to better-calibrated state estimates with improved RMSE.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 10:07:16 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 11:37:33 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Pr\u00fcher", "Jakub", ""], ["Karvonen", "Toni", ""], ["Oates", "Chris J.", ""], ["Straka", "Ond\u0159ej", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1811.11479", "submitter": "Eunjeong Jeong", "authors": "Eunjeong Jeong, Seungeun Oh, Hyesung Kim, Jihong Park, Mehdi Bennis,\n  and Seong-Lyun Kim", "title": "Communication-Efficient On-Device Machine Learning: Federated\n  Distillation and Augmentation under Non-IID Private Data", "comments": "to be presented at the 32nd Conference on Neural Information\n  Processing Systems (NIPS 2018), 2nd Workshop on Machine Learning on the Phone\n  and other Consumer Devices (MLPCD 2), Montr\\'eal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-device machine learning (ML) enables the training process to exploit a\nmassive amount of user-generated private data samples. To enjoy this benefit,\ninter-device communication overhead should be minimized. With this end, we\npropose federated distillation (FD), a distributed model training algorithm\nwhose communication payload size is much smaller than a benchmark scheme,\nfederated learning (FL), particularly when the model size is large. Moreover,\nuser-generated data samples are likely to become non-IID across devices, which\ncommonly degrades the performance compared to the case with an IID dataset. To\ncope with this, we propose federated augmentation (FAug), where each device\ncollectively trains a generative model, and thereby augments its local data\ntowards yielding an IID dataset. Empirical studies demonstrate that FD with\nFAug yields around 26x less communication overhead while achieving 95-98% test\naccuracy compared to FL.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 10:16:18 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Jeong", "Eunjeong", ""], ["Oh", "Seungeun", ""], ["Kim", "Hyesung", ""], ["Park", "Jihong", ""], ["Bennis", "Mehdi", ""], ["Kim", "Seong-Lyun", ""]]}, {"id": "1811.11482", "submitter": "Shu Kong", "authors": "Shu Kong, Charless Fowlkes", "title": "Image Reconstruction with Predictive Filter Flow", "comments": "https://www.ics.uci.edu/~skong2/pff.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple, interpretable framework for solving a wide range of\nimage reconstruction problems such as denoising and deconvolution. Given a\ncorrupted input image, the model synthesizes a spatially varying linear filter\nwhich, when applied to the input image, reconstructs the desired output. The\nmodel parameters are learned using supervised or self-supervised training. We\ntest this model on three tasks: non-uniform motion blur removal,\nlossy-compression artifact reduction and single image super resolution. We\ndemonstrate that our model substantially outperforms state-of-the-art methods\non all these tasks and is significantly faster than optimization-based\napproaches to deconvolution. Unlike models that directly predict output pixel\nvalues, the predicted filter flow is controllable and interpretable, which we\ndemonstrate by visualizing the space of predicted filters for different tasks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 10:17:14 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Kong", "Shu", ""], ["Fowlkes", "Charless", ""]]}, {"id": "1811.11493", "submitter": "Francesco Croce", "authors": "Francesco Croce, Matthias Hein", "title": "A randomized gradient-free attack on ReLU networks", "comments": "In GCPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been shown that neural networks but also other classifiers\nare vulnerable to so called adversarial attacks e.g. in object recognition an\nalmost non-perceivable change of the image changes the decision of the\nclassifier. Relatively fast heuristics have been proposed to produce these\nadversarial inputs but the problem of finding the optimal adversarial input,\nthat is with the minimal change of the input, is NP-hard. While methods based\non mixed-integer optimization which find the optimal adversarial input have\nbeen developed, they do not scale to large networks. Currently, the attack\nscheme proposed by Carlini and Wagner is considered to produce the best\nadversarial inputs. In this paper we propose a new attack scheme for the class\nof ReLU networks based on a direct optimization on the resulting linear\nregions. In our experimental validation we improve in all except one experiment\nout of 18 over the Carlini-Wagner attack with a relative improvement of up to\n9\\%. As our approach is based on the geometrical structure of ReLU networks, it\nis less susceptible to defences targeting their functional properties.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 11:03:26 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Croce", "Francesco", ""], ["Hein", "Matthias", ""]]}, {"id": "1811.11534", "submitter": "Mostafa Darvishi", "authors": "Mostafa Darvishi", "title": "Linear and Nonlinear Identification of Dryer System Using Artificial\n  Intelligence and Neural Networks", "comments": "This version removed by arXiv administrators for inappropriate format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As you read these words you are using a complex biological neural network.\nYou have a highly interconnected set of some neurons to facilitate your\nreading, breathing, motion and thinking. Each of your biological neurons, a\nrich assembly of tissue and chemistry, has the complexity, if not the speed, of\na microprocessor. Some of your neural structure was with you at birth. Other\nparts have been established by experience.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 05:42:37 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 15:05:00 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Darvishi", "Mostafa", ""]]}, {"id": "1811.11540", "submitter": "Austin Benson", "authors": "Austin R. Benson and Jon Kleinberg", "title": "Link Prediction in Networks with Core-Fringe Data", "comments": null, "journal-ref": null, "doi": "10.1145/3308558.3313626", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data collection often involves the partial measurement of a larger system. A\ncommon example arises in collecting network data: we often obtain network\ndatasets by recording all of the interactions among a small set of core nodes,\nso that we end up with a measurement of the network consisting of these core\nnodes along with a potentially much larger set of fringe nodes that have links\nto the core. Given the ubiquity of this process for assembling network data, it\nis crucial to understand the role of such a `core-fringe' structure.\n  Here we study how the inclusion of fringe nodes affects the standard task of\nnetwork link prediction. One might initially think the inclusion of any\nadditional data is useful, and hence that it should be beneficial to include\nall fringe nodes that are available. However, we find that this is not true; in\nfact, there is substantial variability in the value of the fringe nodes for\nprediction. Once an algorithm is selected, in some datasets, including any\nadditional data from the fringe can actually hurt prediction performance; in\nother datasets, including some amount of fringe information is useful before\nprediction performance saturates or even declines; and in further cases,\nincluding the entire fringe leads to the best performance. While such variety\nmight seem surprising, we show that these behaviors are exhibited by simple\nrandom graph models.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 13:22:04 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 17:24:49 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Benson", "Austin R.", ""], ["Kleinberg", "Jon", ""]]}, {"id": "1811.11549", "submitter": "Eli Chien", "authors": "I Chien, Huozhi Zhou, Pan Li", "title": "$HS^2$: Active Learning over Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hypergraph-based active learning scheme which we term $HS^2$,\n$HS^2$ generalizes the previously reported algorithm $S^2$ originally proposed\nfor graph-based active learning with pointwise queries [Dasarathy et al., COLT\n2015]. Our $HS^2$ method can accommodate hypergraph structures and allows one\nto ask both pointwise queries and pairwise queries. Based on a novel parametric\nsystem particularly designed for hypergraphs, we derive theoretical results on\nthe query complexity of $HS^2$ for the above described generalized settings.\nBoth the theoretical and empirical results show that $HS^2$ requires a\nsignificantly fewer number of queries than $S^2$ when one uses $S^2$ over a\ngraph obtained from the corresponding hypergraph via clique expansion.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 18:00:56 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Chien", "I", ""], ["Zhou", "Huozhi", ""], ["Li", "Pan", ""]]}, {"id": "1811.11553", "submitter": "Michael Alcorn", "authors": "Michael A. Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai,\n  Wei-Shinn Ku, Anh Nguyen", "title": "Strike (with) a Pose: Neural Networks Are Easily Fooled by Strange Poses\n  of Familiar Objects", "comments": "Poster at the 2019 Conference on Computer Vision and Pattern\n  Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite excellent performance on stationary test sets, deep neural networks\n(DNNs) can fail to generalize to out-of-distribution (OoD) inputs, including\nnatural, non-adversarial ones, which are common in real-world settings. In this\npaper, we present a framework for discovering DNN failures that harnesses 3D\nrenderers and 3D models. That is, we estimate the parameters of a 3D renderer\nthat cause a target DNN to misbehave in response to the rendered image. Using\nour framework and a self-assembled dataset of 3D objects, we investigate the\nvulnerability of DNNs to OoD poses of well-known objects in ImageNet. For\nobjects that are readily recognized by DNNs in their canonical poses, DNNs\nincorrectly classify 97% of their pose space. In addition, DNNs are highly\nsensitive to slight pose perturbations. Importantly, adversarial poses transfer\nacross models and datasets. We find that 99.9% and 99.4% of the poses\nmisclassified by Inception-v3 also transfer to the AlexNet and ResNet-50 image\nclassifiers trained on the same ImageNet dataset, respectively, and 75.5%\ntransfer to the YOLOv3 object detector trained on MS COCO.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 13:39:27 GMT"}, {"version": "v2", "created": "Sun, 13 Jan 2019 23:55:45 GMT"}, {"version": "v3", "created": "Thu, 18 Apr 2019 13:54:20 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Alcorn", "Michael A.", ""], ["Li", "Qi", ""], ["Gong", "Zhitao", ""], ["Wang", "Chengfei", ""], ["Mai", "Long", ""], ["Ku", "Wei-Shinn", ""], ["Nguyen", "Anh", ""]]}, {"id": "1811.11569", "submitter": "Teofilo de Campos", "authors": "Fabricio Ataides Braz, Nilton Correia da Silva, Teofilo Emidio de\n  Campos, Felipe Borges S. Chaves, Marcelo H. S. Ferreira, Pedro Henrique\n  Inazawa, Victor H. D. Coelho, Bernardo Pablo Sukiennik, Ana Paula Goncalves\n  Soares de Almeida, Flavio Barros Vidal, Davi Alves Bezerra, Davi B. Gusmao,\n  Gabriel G. Ziegler, Ricardo V. C. Fernandes, Roberta Zumblick, Fabiano\n  Hartmann Peixoto", "title": "Document classification using a Bi-LSTM to unclog Brazil's supreme court", "comments": "This work was presented at NIPS 2018 Workshop on Machine Learning for\n  the Developing World (ML4D)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Brazilian court system is currently the most clogged up judiciary system\nin the world. Thousands of lawsuit cases reach the supreme court every day.\nThese cases need to be analyzed in order to be associated to relevant tags and\nallocated to the right team. Most of the cases reach the court as raster\nscanned documents with widely variable levels of quality. One of the first\nsteps for the analysis is to classify these documents. In this paper we present\na Bidirectional Long Short-Term Memory network (Bi-LSTM) to classify these\npieces of legal document.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 11:30:01 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Braz", "Fabricio Ataides", ""], ["da Silva", "Nilton Correia", ""], ["de Campos", "Teofilo Emidio", ""], ["Chaves", "Felipe Borges S.", ""], ["Ferreira", "Marcelo H. S.", ""], ["Inazawa", "Pedro Henrique", ""], ["Coelho", "Victor H. D.", ""], ["Sukiennik", "Bernardo Pablo", ""], ["de Almeida", "Ana Paula Goncalves Soares", ""], ["Vidal", "Flavio Barros", ""], ["Bezerra", "Davi Alves", ""], ["Gusmao", "Davi B.", ""], ["Ziegler", "Gabriel G.", ""], ["Fernandes", "Ricardo V. C.", ""], ["Zumblick", "Roberta", ""], ["Peixoto", "Fabiano Hartmann", ""]]}, {"id": "1811.11582", "submitter": "Radu Tudor Ionescu", "authors": "Petru Soviany, Radu Tudor Ionescu", "title": "Continuous Trade-off Optimization between Fast and Accurate Deep Face\n  Detectors", "comments": "Accepted at ICONIP 2018. arXiv admin note: substantial text overlap\n  with arXiv:1803.08707", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep neural networks offer better face detection results than\nshallow or handcrafted models, their complex architectures come with higher\ncomputational requirements and slower inference speeds than shallow neural\nnetworks. In this context, we study five straightforward approaches to achieve\nan optimal trade-off between accuracy and speed in face detection. All the\napproaches are based on separating the test images in two batches, an easy\nbatch that is fed to a faster face detector and a difficult batch that is fed\nto a more accurate yet slower detector. We conduct experiments on the AFW and\nthe FDDB data sets, using MobileNet-SSD as the fast face detector and S3FD\n(Single Shot Scale-invariant Face Detector) as the accurate face detector, both\nmodels being pre-trained on the WIDER FACE data set. Our experiments show that\nthe proposed difficulty metrics compare favorably to a random split of the\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 12:16:22 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Soviany", "Petru", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "1811.11597", "submitter": "Pascal Kerschke", "authors": "Pascal Kerschke, Holger H. Hoos, Frank Neumann, Heike Trautmann", "title": "Automated Algorithm Selection: Survey and Perspectives", "comments": "This is the author's final version, and the article has been accepted\n  for publication in Evolutionary Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been observed that for practically any computational problem that\nhas been intensely studied, different instances are best solved using different\nalgorithms. This is particularly pronounced for computationally hard problems,\nwhere in most cases, no single algorithm defines the state of the art; instead,\nthere is a set of algorithms with complementary strengths. This performance\ncomplementarity can be exploited in various ways, one of which is based on the\nidea of selecting, from a set of given algorithms, for each problem instance to\nbe solved the one expected to perform best. The task of automatically selecting\nan algorithm from a given set is known as the per-instance algorithm selection\nproblem and has been intensely studied over the past 15 years, leading to major\nimprovements in the state of the art in solving a growing number of discrete\ncombinatorial problems, including propositional satisfiability and AI planning.\nPer-instance algorithm selection also shows much promise for boosting\nperformance in solving continuous and mixed discrete/continuous optimisation\nproblems.\n  This survey provides an overview of research in automated algorithm\nselection, ranging from early and seminal works to recent and promising\napplication areas. Different from earlier work, it covers applications to\ndiscrete and continuous problems, and discusses algorithm selection in context\nwith conceptually related approaches, such as algorithm configuration,\nscheduling or portfolio selection. Since informative and cheaply computable\nproblem instance features provide the basis for effective per-instance\nalgorithm selection systems, we also provide an overview of such features for\ndiscrete and continuous problems. Finally, we provide perspectives on future\nwork in the area and discuss a number of open research challenges.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 14:43:49 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Kerschke", "Pascal", ""], ["Hoos", "Holger H.", ""], ["Neumann", "Frank", ""], ["Trautmann", "Heike", ""]]}, {"id": "1811.11615", "submitter": "Gabriel Hartmann", "authors": "Gabriel Hartmann, Zvi Shiller, Amos Azaria", "title": "Deep Reinforcement Learning for Time Optimal Velocity Control using\n  Prior Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous navigation has recently gained great interest in the field of\nreinforcement learning. However, little attention was given to the time optimal\nvelocity control problem, i.e. controlling a vehicle such that it travels at\nthe maximal speed without becoming dynamically unstable (roll-over or sliding).\n  Time optimal velocity control can be solved numerically using existing\nmethods that are based on optimal control and vehicle dynamics. In this paper,\nwe use deep reinforcement learning to generate the time optimal velocity\ncontrol. Furthermore, we use the numerical solution to further improve the\nperformance of the reinforcement learner. It is shown that the reinforcement\nlearner outperforms the numerically derived solution, and that the hybrid\napproach (combining learning with the numerical solution) speeds up the\ntraining process.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 15:14:39 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 07:49:48 GMT"}, {"version": "v3", "created": "Thu, 25 Jul 2019 08:06:45 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Hartmann", "Gabriel", ""], ["Shiller", "Zvi", ""], ["Azaria", "Amos", ""]]}, {"id": "1811.11618", "submitter": "Eric Benhamou", "authors": "Eric Benhamou", "title": "Kalman filter demystified: from intuition to probabilistic graphical\n  model to real case in financial markets", "comments": "44 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we revisit the Kalman filter theory. After giving the\nintuition on a simplified financial markets example, we revisit the maths\nunderlying it. We then show that Kalman filter can be presented in a very\ndifferent fashion using graphical models. This enables us to establish the\nconnection between Kalman filter and Hidden Markov Models. We then look at\ntheir application in financial markets and provide various intuitions in terms\nof their applicability for complex systems such as financial markets. Although\nthis paper has been written more like a self contained work connecting Kalman\nfilter to Hidden Markov Models and hence revisiting well known and establish\nresults, it contains new results and brings additional contributions to the\nfield. First, leveraging on the link between Kalman filter and HMM, it gives\nnew algorithms for inference for extended Kalman filters. Second, it presents\nan alternative to the traditional estimation of parameters using EM algorithm\nthanks to the usage of CMA-ES optimization. Third, it examines the application\nof Kalman filter and its Hidden Markov models version to financial markets,\nproviding various dynamics assumptions and tests. We conclude by connecting\nKalman filter approach to trend following technical analysis system and showing\ntheir superior performances for trend following detection.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 15:19:11 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 07:16:05 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Benhamou", "Eric", ""]]}, {"id": "1811.11620", "submitter": "Waddah Waheeb", "authors": "Waddah Waheeb and Rozaida Ghazali", "title": "Multi-step Time Series Forecasting Using Ridge Polynomial Neural Network\n  with Error-Output Feedbacks", "comments": "This is a pre-print of an article published in the International\n  Conference on Soft Computing in Data Science, 2016. The final authenticated\n  version is available online at:\n  http://link.springer.com/chapter/10.1007/978-981-10-2777-2_5", "journal-ref": null, "doi": "10.1007/978-981-10-2777-2_5", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series forecasting gets much attention due to its impact on many\npractical applications. Higher-order neural network with recurrent feedback is\na powerful technique which used successfully for forecasting. It maintains fast\nlearning and the ability to learn the dynamics of the series over time. For\nthat, in this paper, we propose a novel model which is called Ridge Polynomial\nNeural Network with Error-Output Feedbacks (RPNN-EOFs) that combines the\nproperties of higher order and error-output feedbacks. The well-known\nMackey-Glass time series is used to test the forecasting capability of\nRPNN-EOFS. Simulation results showed that the proposed RPNN-EOFs provides\nbetter understanding for the Mackey-Glass time series with root mean square\nerror equal to 0.00416. This result is smaller than other models in the\nliterature. Therefore, we can conclude that the RPNN-EOFs can be applied\nsuccessfully for time series forecasting.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 15:19:45 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Waheeb", "Waddah", ""], ["Ghazali", "Rozaida", ""]]}, {"id": "1811.11644", "submitter": "Li Jing", "authors": "Li Jing, Rumen Dangovski, Marin Soljacic", "title": "WaveletNet: Logarithmic Scale Efficient Convolutional Neural Networks\n  for Edge Devices", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a logarithmic-scale efficient convolutional neural network\narchitecture for edge devices, named WaveletNet. Our model is based on the\nwell-known depthwise convolution, and on two new layers, which we introduce in\nthis work: a wavelet convolution and a depthwise fast wavelet transform. By\nbreaking the symmetry in channel dimensions and applying a fast algorithm,\nWaveletNet shrinks the complexity of convolutional blocks by an O(logD/D)\nfactor, where D is the number of channels. Experiments on CIFAR-10 and ImageNet\nclassification show superior and comparable performances of WaveletNet compared\nto state-of-the-art models such as MobileNetV2.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 16:04:30 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Jing", "Li", ""], ["Dangovski", "Rumen", ""], ["Soljacic", "Marin", ""]]}, {"id": "1811.11646", "submitter": "Arghyadip Roy", "authors": "Arghyadip Roy, Vivek Borkar, Abhay Karandikar, Prasanna Chaporkar", "title": "A Structure-aware Online Learning Algorithm for Markov Decision\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To overcome the curse of dimensionality and curse of modeling in Dynamic\nProgramming (DP) methods for solving classical Markov Decision Process (MDP)\nproblems, Reinforcement Learning (RL) algorithms are popular. In this paper, we\nconsider an infinite-horizon average reward MDP problem and prove the\noptimality of the threshold policy under certain conditions. Traditional RL\ntechniques do not exploit the threshold nature of optimal policy while\nlearning. In this paper, we propose a new RL algorithm which utilizes the known\nthreshold structure of the optimal policy while learning by reducing the\nfeasible policy space. We establish that the proposed algorithm converges to\nthe optimal policy. It provides a significant improvement in convergence speed\nand computational and storage complexity over traditional RL algorithms. The\nproposed technique can be applied to a wide variety of optimization problems\nthat include energy efficient data transmission and management of queues. We\nexhibit the improvement in convergence speed of the proposed algorithm over\nother RL algorithms through simulations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 16:05:21 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Roy", "Arghyadip", ""], ["Borkar", "Vivek", ""], ["Karandikar", "Abhay", ""], ["Chaporkar", "Prasanna", ""]]}, {"id": "1811.11668", "submitter": "Sebastian Benthall", "authors": "Sebastian Benthall and Bruce D. Haynes", "title": "Racial categories in machine learning", "comments": null, "journal-ref": null, "doi": "10.1145/3287560.3287575", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controversies around race and machine learning have sparked debate among\ncomputer scientists over how to design machine learning systems that guarantee\nfairness. These debates rarely engage with how racial identity is embedded in\nour social experience, making for sociological and psychological complexity.\nThis complexity challenges the paradigm of considering fairness to be a formal\nproperty of supervised learning with respect to protected personal attributes.\nRacial identity is not simply a personal subjective quality. For people labeled\n\"Black\" it is an ascribed political category that has consequences for social\ndifferentiation embedded in systemic patterns of social inequality achieved\nthrough both social and spatial segregation. In the United States, racial\nclassification can best be understood as a system of inherently unequal status\ncategories that places whites as the most privileged category while signifying\nthe Negro/black category as stigmatized. Social stigma is reinforced through\nthe unequal distribution of societal rewards and goods along racial lines that\nis reinforced by state, corporate, and civic institutions and practices. This\ncreates a dilemma for society and designers: be blind to racial group\ndisparities and thereby reify racialized social inequality by no longer\nmeasuring systemic inequality, or be conscious of racial categories in a way\nthat itself reifies race. We propose a third option. By preceding group\nfairness interventions with unsupervised learning to dynamically detect\npatterns of segregation, machine learning systems can mitigate the root cause\nof social disparities, social segregation and stratification, without further\nanchoring status categories of disadvantage.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 16:47:36 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Benthall", "Sebastian", ""], ["Haynes", "Bruce D.", ""]]}, {"id": "1811.11669", "submitter": "Michael Kl\\\"as", "authors": "Michael Kl\\\"as", "title": "Towards Identifying and Managing Sources of Uncertainty in AI and\n  Machine Learning Models - An Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying and managing uncertainties that occur when data-driven models\nsuch as those provided by AI and machine learning methods are applied is\ncrucial. This whitepaper provides a brief motivation and first overview of the\nstate of the art in identifying and quantifying sources of uncertainty for\ndata-driven components as well as means for analyzing their impact.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 16:49:37 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Kl\u00e4s", "Michael", ""]]}, {"id": "1811.11674", "submitter": "Harry Clifford", "authors": "Luke R Harries, Suyi Zhang, Geoffroy Dubourg-Felonneau, James H R\n  Farmery, Jonathan Sinai, Belle Taylor, Nirmesh Patel, John W Cassidy, John\n  Shawe-Taylor, Harry W Clifford", "title": "Interlacing Personal and Reference Genomes for Machine Learning\n  Disease-Variant Detection", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:cs/0101200", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/103", "categories": "q-bio.GN cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNA sequencing to identify genetic variants is becoming increasingly valuable\nin clinical settings. Assessment of variants in such sequencing data is\ncommonly implemented through Bayesian heuristic algorithms. Machine learning\nhas shown great promise in improving on these variant calls, but the input for\nthese is still a standardized \"pile-up\" image, which is not always best suited.\nIn this paper, we present a novel method for generating images from DNA\nsequencing data, which interlaces the human reference genome with personalized\nsequencing output, to maximize usage of sequencing reads and improve machine\nlearning algorithm performance. We demonstrate the success of this in improving\nstandard germline variant calling. We also furthered this approach to include\nsomatic variant calling across tumor/normal data with Siamese networks. These\napproaches can be used in machine learning applications on sequencing data with\nthe hope of improving clinical outcomes, and are freely available for\nnoncommercial use at www.ccg.ai.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 15:38:29 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Harries", "Luke R", ""], ["Zhang", "Suyi", ""], ["Dubourg-Felonneau", "Geoffroy", ""], ["Farmery", "James H R", ""], ["Sinai", "Jonathan", ""], ["Taylor", "Belle", ""], ["Patel", "Nirmesh", ""], ["Cassidy", "John W", ""], ["Shawe-Taylor", "John", ""], ["Clifford", "Harry W", ""]]}, {"id": "1811.11682", "submitter": "David Rolnick", "authors": "David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy P. Lillicrap,\n  Greg Wayne", "title": "Experience Replay for Continual Learning", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning is the problem of learning new tasks or knowledge while\nprotecting old knowledge and ideally generalizing from old experience to learn\nnew tasks faster. Neural networks trained by stochastic gradient descent often\ndegrade on old tasks when trained successively on new tasks with different data\ndistributions. This phenomenon, referred to as catastrophic forgetting, is\nconsidered a major hurdle to learning with non-stationary data or sequences of\nnew tasks, and prevents networks from continually accumulating knowledge and\nskills. We examine this issue in the context of reinforcement learning, in a\nsetting where an agent is exposed to tasks in a sequence. Unlike most other\nwork, we do not provide an explicit indication to the model of task boundaries,\nwhich is the most general circumstance for a learning agent exposed to\ncontinuous experience. While various methods to counteract catastrophic\nforgetting have recently been proposed, we explore a straightforward, general,\nand seemingly overlooked solution - that of using experience replay buffers for\nall past events - with a mixture of on- and off-policy learning, leveraging\nbehavioral cloning. We show that this strategy can still learn new tasks\nquickly yet can substantially reduce catastrophic forgetting in both Atari and\nDMLab domains, even matching the performance of methods that require task\nidentities. When buffer storage is constrained, we confirm that a simple\nmechanism for randomly discarding data allows a limited size buffer to perform\nalmost as well as an unbounded one.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 17:04:27 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 13:01:45 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Rolnick", "David", ""], ["Ahuja", "Arun", ""], ["Schwarz", "Jonathan", ""], ["Lillicrap", "Timothy P.", ""], ["Wayne", "Greg", ""]]}, {"id": "1811.11683", "submitter": "Hassan Akbari", "authors": "Hassan Akbari, Svebor Karaman, Surabhi Bhargava, Brian Chen, Carl\n  Vondrick, and Shih-Fu Chang", "title": "Multi-level Multimodal Common Semantic Space for Image-Phrase Grounding", "comments": "Accepted in CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of phrase grounding by lear ing a multi-level common\nsemantic space shared by the textual and visual modalities. We exploit multiple\nlevels of feature maps of a Deep Convolutional Neural Network, as well as\ncontextualized word and sentence embeddings extracted from a character-based\nlanguage model. Following dedicated non-linear mappings for visual features at\neach level, word, and sentence embeddings, we obtain multiple instantiations of\nour common semantic space in which comparisons between any target text and the\nvisual content is performed with cosine similarity. We guide the model by a\nmulti-level multimodal attention mechanism which outputs attended visual\nfeatures at each level. The best level is chosen to be compared with text\ncontent for maximizing the pertinence scores of image-sentence pairs of the\nground truth. Experiments conducted on three publicly available datasets show\nsignificant performance gains (20%-60% relative) over the state-of-the-art in\nphrase localization and set a new performance record on those datasets. We\nprovide a detailed ablation study to show the contribution of each element of\nour approach and release our code on GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 17:05:27 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 20:49:53 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Akbari", "Hassan", ""], ["Karaman", "Svebor", ""], ["Bhargava", "Surabhi", ""], ["Chen", "Brian", ""], ["Vondrick", "Carl", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1811.11684", "submitter": "Qihong Lu", "authors": "Qihong Lu, Po-Hsuan Chen, Jonathan W. Pillow, Peter J. Ramadge,\n  Kenneth A. Norman, Uri Hasson", "title": "Shared Representational Geometry Across Neural Networks", "comments": "Integration of Deep Learning Theories workshop, NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different neural networks trained on the same dataset often learn similar\ninput-output mappings with very different weights. Is there some correspondence\nbetween these neural network solutions? For linear networks, it has been shown\nthat different instances of the same network architecture encode the same\nrepresentational similarity matrix, and their neural activity patterns are\nconnected by orthogonal transformations. However, it is unclear if this holds\nfor non-linear networks. Using a shared response model, we show that different\nneural networks encode the same input examples as different orthogonal\ntransformations of an underlying shared representation. We test this claim\nusing both standard convolutional neural networks and residual networks on\nCIFAR10 and CIFAR100.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 17:07:30 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2019 18:02:29 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Lu", "Qihong", ""], ["Chen", "Po-Hsuan", ""], ["Pillow", "Jonathan W.", ""], ["Ramadge", "Peter J.", ""], ["Norman", "Kenneth A.", ""], ["Hasson", "Uri", ""]]}, {"id": "1811.11705", "submitter": "Daniel L. Marino", "authors": "Daniel L. Marino, Chathurika S. Wickramasinghe, Milos Manic", "title": "An Adversarial Approach for Explainable AI in Intrusion Detection\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growing popularity of modern machine learning techniques (e.g.\nDeep Neural Networks) in cyber-security applications, most of these models are\nperceived as a black-box for the user. Adversarial machine learning offers an\napproach to increase our understanding of these models. In this paper we\npresent an approach to generate explanations for incorrect classifications made\nby data-driven Intrusion Detection Systems (IDSs). An adversarial approach is\nused to find the minimum modifications (of the input features) required to\ncorrectly classify a given set of misclassified samples. The magnitude of such\nmodifications is used to visualize the most relevant features that explain the\nreason for the misclassification. The presented methodology generated\nsatisfactory explanations that describe the reasoning behind the\nmis-classifications, with descriptions that match expert knowledge. The\nadvantages of the presented methodology are: 1) applicable to any classifier\nwith defined gradients. 2) does not require any modification of the classifier\nmodel. 3) can be extended to perform further diagnosis (e.g. vulnerability\nassessment) and gain further understanding of the system. Experimental\nevaluation was conducted on the NSL-KDD99 benchmark dataset using Linear and\nMultilayer perceptron classifiers. The results are shown using intuitive\nvisualizations in order to improve the interpretability of the results.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 17:48:11 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Marino", "Daniel L.", ""], ["Wickramasinghe", "Chathurika S.", ""], ["Manic", "Milos", ""]]}, {"id": "1811.11711", "submitter": "Josh Merel", "authors": "Josh Merel, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu\n  Pham, Greg Wayne, Yee Whye Teh, Nicolas Heess", "title": "Neural probabilistic motor primitives for humanoid control", "comments": "Accepted as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of learning a single motor module that can flexibly\nexpress a range of behaviors for the control of high-dimensional physically\nsimulated humanoids. To do this, we propose a motor architecture that has the\ngeneral structure of an inverse model with a latent-variable bottleneck. We\nshow that it is possible to train this model entirely offline to compress\nthousands of expert policies and learn a motor primitive embedding space. The\ntrained neural probabilistic motor primitive system can perform one-shot\nimitation of whole-body humanoid behaviors, robustly mimicking unseen\ntrajectories. Additionally, we demonstrate that it is also straightforward to\ntrain controllers to reuse the learned motor primitive space to solve tasks,\nand the resulting movements are relatively naturalistic. To support the\ntraining of our model, we compare two approaches for offline policy cloning,\nincluding an experience efficient method which we call linear feedback policy\ncloning. We encourage readers to view a supplementary video (\nhttps://youtu.be/CaDEf-QcKwA ) summarizing our results.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 18:00:03 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 18:02:52 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Merel", "Josh", ""], ["Hasenclever", "Leonard", ""], ["Galashov", "Alexandre", ""], ["Ahuja", "Arun", ""], ["Pham", "Vu", ""], ["Wayne", "Greg", ""], ["Teh", "Yee Whye", ""], ["Heess", "Nicolas", ""]]}, {"id": "1811.11728", "submitter": "Chengbin Hou", "authors": "Chengbin Hou, Shan He, Ke Tang", "title": "Attributed Network Embedding for Incomplete Attributed Networks", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attributed networks are ubiquitous since a network often comes with auxiliary\nattribute information e.g. a social network with user profiles. Attributed\nNetwork Embedding (ANE) has recently attracted considerable attention, which\naims to learn unified low dimensional node embeddings while preserving both\nstructural and attribute information. The resulting node embeddings can then\nfacilitate various network downstream tasks e.g. link prediction. Although\nthere are several ANE methods, most of them cannot deal with incomplete\nattributed networks with missing links and/or missing node attributes, which\noften occur in real-world scenarios. To address this issue, we propose a robust\nANE method, the general idea of which is to reconstruct a unified denser\nnetwork by fusing two sources of information for information enhancement, and\nthen employ a random walks based network embedding method for learning node\nembeddings. The experiments of link prediction, node classification,\nvisualization, and parameter sensitivity analysis on six real-world datasets\nvalidate the effectiveness of our method to incomplete attributed networks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 18:29:16 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 20:02:23 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Hou", "Chengbin", ""], ["He", "Shan", ""], ["Tang", "Ke", ""]]}, {"id": "1811.11745", "submitter": "Tim Brooks", "authors": "Tim Brooks, Jonathan T. Barron", "title": "Learning to Synthesize Motion Blur", "comments": "http://timothybrooks.com/tech/motion-blur/ . IEEE Conference on\n  Computer Vision and Pattern Recognition (CVPR), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a technique for synthesizing a motion blurred image from a pair of\nunblurred images captured in succession. To build this system we motivate and\ndesign a differentiable \"line prediction\" layer to be used as part of a neural\nnetwork architecture, with which we can learn a system to regress from image\npairs to motion blurred images that span the capture time of the input image\npair. Training this model requires an abundance of data, and so we design and\nexecute a strategy for using frame interpolation techniques to generate a\nlarge-scale synthetic dataset of motion blurred images and their respective\ninputs. We additionally capture a high quality test set of real motion blurred\nimages, synthesized from slow motion videos, with which we evaluate our model\nagainst several baseline techniques that can be used to synthesize motion blur.\nOur model produces higher accuracy output than our baselines, and is\nsignificantly faster than baselines with competitive accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 21:55:55 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 07:28:50 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Brooks", "Tim", ""], ["Barron", "Jonathan T.", ""]]}, {"id": "1811.11790", "submitter": "Miriam Shiffman", "authors": "Miriam Shiffman, William T. Stephenson, Geoffrey Schiebinger, Jonathan\n  Huggins, Trevor Campbell, Aviv Regev, Tamara Broderick", "title": "Reconstructing probabilistic trees of cellular differentiation from\n  single-cell RNA-seq data", "comments": "18 pages, 6 figures. Preliminary work appeared in the 2017 NeurIPS\n  workshops in Advances in Approximate Bayesian Inference\n  (http://approximateinference.org/2017) and Machine Learning for Computational\n  Biology (https://mlcb.github.io)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until recently, transcriptomics was limited to bulk RNA sequencing, obscuring\nthe underlying expression patterns of individual cells in favor of a global\naverage. Thanks to technological advances, we can now profile gene expression\nacross thousands or millions of individual cells in parallel. This new type of\ndata has led to the intriguing discovery that individual cell profiles can\nreflect the imprint of time or dynamic processes. However, synthesizing this\ninformation to reconstruct dynamic biological phenomena from data that are\nnoisy, heterogenous, and sparse---and from processes that may unfold\nasynchronously---poses a complex computational and statistical challenge. Here,\nwe develop a full generative model for probabilistically reconstructing trees\nof cellular differentiation from single-cell RNA-seq data. Specifically, we\nextend the framework of the classical Dirichlet diffusion tree to\nsimultaneously infer branch topology and latent cell states along continuous\ntrajectories over the full tree. In tandem, we construct a novel Markov chain\nMonte Carlo sampler that interleaves Metropolis-Hastings and message passing to\nleverage model structure for efficient inference. Finally, we demonstrate that\nthese techniques can recover latent trajectories from simulated single-cell\ntranscriptomes. While this work is motivated by cellular differentiation, we\nderive a tractable model that provides flexible densities for any data (coupled\nwith an appropriate noise model) that arise from continuous evolution along a\nlatent nonparametric tree.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 19:20:42 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Shiffman", "Miriam", ""], ["Stephenson", "William T.", ""], ["Schiebinger", "Geoffrey", ""], ["Huggins", "Jonathan", ""], ["Campbell", "Trevor", ""], ["Regev", "Aviv", ""], ["Broderick", "Tamara", ""]]}, {"id": "1811.11813", "submitter": "Juan B Gutierrez", "authors": "Saeid Safaei, Vahid Safaei, Solmazi Safaei, Zerotti Woods, Hamid R.\n  Arabnia and Juan B. Gutierrez", "title": "The SWAG Algorithm; a Mathematical Approach that Outperforms Traditional\n  Deep Learning. Theory and Implementation", "comments": "20 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance of artificial neural networks (ANNs) is influenced by weight\ninitialization, the nature of activation functions, and their architecture.\nThere is a wide range of activation functions that are traditionally used to\ntrain a neural network, e.g. sigmoid, tanh, and Rectified Linear Unit (ReLU). A\nwidespread practice is to use the same type of activation function in all\nneurons in a given layer. In this manuscript, we present a type of neural\nnetwork in which the activation functions in every layer form a polynomial\nbasis; we name this method SWAG after the initials of the last names of the\nauthors. We tested SWAG on three complex highly non-linear functions as well as\nthe MNIST handwriting data set. SWAG outperforms and converges faster than the\nstate of the art performance in fully connected neural networks. Given the low\ncomputational complexity of SWAG, and the fact that it was capable of solving\nproblems current architectures cannot, it has the potential to change the way\nthat we approach deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 20:25:18 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Safaei", "Saeid", ""], ["Safaei", "Vahid", ""], ["Safaei", "Solmazi", ""], ["Woods", "Zerotti", ""], ["Arabnia", "Hamid R.", ""], ["Gutierrez", "Juan B.", ""]]}, {"id": "1811.11815", "submitter": "Georgios Mastorakis", "authors": "Georgios Mastorakis", "title": "Unrepresentative video data: A review and evaluation", "comments": "35 pages, 14 figures. Submitted to Computer Vision and Image\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the quality and quantity of training data are\nsignificant factors which affect the development and performance of machine\nintelligence algorithms. Without representative data, neither scientists nor\nalgorithms would be able to accurately capture the visual details of objects,\nactions or scenes. An evaluation methodology which filters data quality does\nnot yet exist, and currently, the validation of the data depends solely on\nhuman factor. This study reviews several public datasets and discusses their\nlimitations and issues regarding quality, feasibility, adaptation and\navailability of training data. A simple approach to evaluate (i.e.\nautomatically \"clean\" samples) training data is proposed with the use of real\nevents recorded on the YouTube platform. This study focuses on action\nrecognition data and particularly on human fall detection datasets. However,\nthe limitations described in this paper apply in virtually all datasets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 20:28:11 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2019 21:14:00 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Mastorakis", "Georgios", ""]]}, {"id": "1811.11818", "submitter": "Sina Rashidian", "authors": "Sina Rashidian, Janos Hajagos, Richard Moffitt, Fusheng Wang, Xinyu\n  Dong, Kayley Abell-Hart, Kimberly Noel, Rajarsi Gupta, Mathew Tharakan, Veena\n  Lingam, Joel Saltz, Mary Saltz", "title": "Disease phenotyping using deep learning: A diabetes case study", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:cs/0101200", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/38", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterization of a patient clinical phenotype is central to biomedical\ninformatics. ICD codes, assigned to inpatient encounters by coders, is\nimportant for population health and cohort discovery when clinical information\nis limited. While ICD codes are assigned to patients by professionals trained\nand certified in coding there is substantial variability in coding. We present\na methodology that uses deep learning methods to model coder decision making\nand that predicts ICD codes. Our approach predicts codes based on demographics,\nlab results, and medications, as well as codes from previous encounters. We are\nable to predict existing codes with high accuracy for all three of the test\ncases we investigated: diabetes, acute renal failure, and chronic kidney\ndisease. We employed a panel of clinicians, in a blinded manner, to assess\nground truth and compared the predictions of coders, model and clinicians. When\ndisparities between the model prediction and coder assigned codes were\nreviewed, our model outperformed coder assigned ICD codes.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 20:37:04 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Rashidian", "Sina", ""], ["Hajagos", "Janos", ""], ["Moffitt", "Richard", ""], ["Wang", "Fusheng", ""], ["Dong", "Xinyu", ""], ["Abell-Hart", "Kayley", ""], ["Noel", "Kimberly", ""], ["Gupta", "Rajarsi", ""], ["Tharakan", "Mathew", ""], ["Lingam", "Veena", ""], ["Saltz", "Joel", ""], ["Saltz", "Mary", ""]]}, {"id": "1811.11819", "submitter": "Siavash Khodadadeh", "authors": "Siavash Khodadadeh, Ladislau B\\\"ol\\\"oni and Mubarak Shah", "title": "Unsupervised Meta-Learning For Few-Shot Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot or one-shot learning of classifiers requires a significant inductive\nbias towards the type of task to be learned. One way to acquire this is by\nmeta-learning on tasks similar to the target task. In this paper, we propose\nUMTRA, an algorithm that performs unsupervised, model-agnostic meta-learning\nfor classification tasks. The meta-learning step of UMTRA is performed on a\nflat collection of unlabeled images. While we assume that these images can be\ngrouped into a diverse set of classes and are relevant to the target task, no\nexplicit information about the classes or any labels are needed. UMTRA uses\nrandom sampling and augmentation to create synthetic training tasks for\nmeta-learning phase. Labels are only needed at the final target task learning\nstep, and they can be as little as one sample per class. On the Omniglot and\nMini-Imagenet few-shot learning benchmarks, UMTRA outperforms every tested\napproach based on unsupervised learning of representations, while alternating\nfor the best performance with the recent CACTUs algorithm. Compared to\nsupervised model-agnostic meta-learning approaches, UMTRA trades off some\nclassification accuracy for a reduction in the required labels of several\norders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 20:38:59 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 00:01:57 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Khodadadeh", "Siavash", ""], ["B\u00f6l\u00f6ni", "Ladislau", ""], ["Shah", "Mubarak", ""]]}, {"id": "1811.11851", "submitter": "Pavel Kuptsov", "authors": "Pavel V. Kuptsov and Anna V. Kuptsova", "title": "Estimating of the inertial manifold dimension for a chaotic attractor of\n  complex Ginzburg-Landau equation using a neural network", "comments": "10 pages and 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension of an inertial manifold for a chaotic attractor of spatially\ndistributed system is estimated using autoencoder neural network. The inertial\nmanifold is a low dimensional manifold where the chaotic attractor is embedded.\nThe autoencoder maps system state vectors onto themselves letting them pass\nthrough an inner state with a reduced dimension. The training processes of the\nautoencoder is shown to depend dramatically on the reduced dimension: a\nlearning curve saturates when the dimension is too small and decays if it is\nsufficient for a lossless information transfer. The smallest sufficient value\nis considered as a dimension of the inertial manifold, and the autoencoder\nimplements a mapping onto the inertial manifold and back. The correctness of\nthe computed dimension is confirmed by its remarkable coincidence with the one\nobtained as a number of covariant Lyapunov vectors with vanishing pairwise\nangles. These vectors are called physical modes. Unlike never having zero\nangles residual ones they are known to span a tangent subspace for the inertial\nmanifold.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 16:24:40 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Kuptsov", "Pavel V.", ""], ["Kuptsova", "Anna V.", ""]]}, {"id": "1811.11880", "submitter": "Andrew McGough", "authors": "Daniel Justus, John Brennan, Stephen Bonner, Andrew Stephen McGough", "title": "Predicting the Computational Cost of Deep Learning Models", "comments": "Accepted for publication at the IEEE International Conference on Big\n  Data, (C) IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is rapidly becoming a go-to tool for many artificial\nintelligence problems due to its ability to outperform other approaches and\neven humans at many problems. Despite its popularity we are still unable to\naccurately predict the time it will take to train a deep learning network to\nsolve a given problem. This training time can be seen as the product of the\ntraining time per epoch and the number of epochs which need to be performed to\nreach the desired level of accuracy. Some work has been carried out to predict\nthe training time for an epoch -- most have been based around the assumption\nthat the training time is linearly related to the number of floating point\noperations required. However, this relationship is not true and becomes\nexacerbated in cases where other activities start to dominate the execution\ntime. Such as the time to load data from memory or loss of performance due to\nnon-optimal parallel execution. In this work we propose an alternative approach\nin which we train a deep learning network to predict the execution time for\nparts of a deep learning network. Timings for these individual parts can then\nbe combined to provide a prediction for the whole execution time. This has\nadvantages over linear approaches as it can model more complex scenarios. But,\nalso, it has the ability to predict execution times for scenarios unseen in the\ntraining data. Therefore, our approach can be used not only to infer the\nexecution time for a batch, or entire epoch, but it can also support making a\nwell-informed choice for the appropriate hardware and model.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 23:36:50 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Justus", "Daniel", ""], ["Brennan", "John", ""], ["Bonner", "Stephen", ""], ["McGough", "Andrew Stephen", ""]]}, {"id": "1811.11881", "submitter": "Karthik Abinav Sankararaman", "authors": "Nicole Immorlica and Karthik Abinav Sankararaman and Robert Schapire\n  and Aleksandrs Slivkins", "title": "Adversarial Bandits with Knapsacks", "comments": "Extended abstract appeared in FOCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bandits with Knapsacks (henceforth, BwK), a general model for\nmulti-armed bandits under supply/budget constraints. In particular, a bandit\nalgorithm needs to solve a well-known knapsack problem: find an optimal packing\nof items into a limited-size knapsack. The BwK problem is a common\ngeneralization of numerous motivating examples, which range from dynamic\npricing to repeated auctions to dynamic ad allocation to network routing and\nscheduling. While the prior work on BwK focused on the stochastic version, we\npioneer the other extreme in which the outcomes can be chosen adversarially.\nThis is a considerably harder problem, compared to both the stochastic version\nand the \"classic\" adversarial bandits, in that regret minimization is no longer\nfeasible. Instead, the objective is to minimize the competitive ratio: the\nratio of the benchmark reward to the algorithm's reward.\n  We design an algorithm with competitive ratio O(log T) relative to the best\nfixed distribution over actions, where T is the time horizon; we also prove a\nmatching lower bound. The key conceptual contribution is a new perspective on\nthe stochastic version of the problem. We suggest a new algorithm for the\nstochastic version, which builds on the framework of regret minimization in\nrepeated games and admits a substantially simpler analysis compared to prior\nwork. We then analyze this algorithm for the adversarial version and use it as\na subroutine to solve the latter.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 23:43:11 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 02:13:00 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2019 17:12:51 GMT"}, {"version": "v4", "created": "Fri, 22 Mar 2019 22:17:04 GMT"}, {"version": "v5", "created": "Sun, 13 Oct 2019 05:01:32 GMT"}, {"version": "v6", "created": "Fri, 6 Nov 2020 19:18:05 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Immorlica", "Nicole", ""], ["Sankararaman", "Karthik Abinav", ""], ["Schapire", "Robert", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "1811.11891", "submitter": "Samson Koelle", "authors": "Samson Koelle, Hanyu Zhang, Marina Meila, Yu-Chia Chen", "title": "Manifold Coordinates with Physical Meaning", "comments": "Submitted to JMLR. Improved over v2 (added appendix). Improved over\n  v1 (revisions)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold embedding algorithms map high-dimensional data down to coordinates\nin a much lower-dimensional space. One of the aims of dimension reduction is to\nfind intrinsic coordinates that describe the data manifold. The coordinates\nreturned by the embedding algorithm are abstract, and finding their physical or\ndomain-related meaning is not formalized and often left to domain experts. This\npaper studies the problem of recovering the meaning of the new low-dimensional\nrepresentation in an automatic, principled fashion. We propose a method to\nexplain embedding coordinates of a manifold as non-linear compositions of\nfunctions from a user-defined dictionary. We show that this problem can be set\nup as a sparse linear Group Lasso recovery problem, find sufficient recovery\nconditions, and demonstrate its effectiveness on data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 00:32:25 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 16:44:49 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 16:32:13 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Koelle", "Samson", ""], ["Zhang", "Hanyu", ""], ["Meila", "Marina", ""], ["Chen", "Yu-Chia", ""]]}, {"id": "1811.11896", "submitter": "Pai Liu", "authors": "Pai Liu, Jingwei Gan, and Rajan K. Chakrabarty", "title": "Variational Autoencoding the Lagrangian Trajectories of Particles in a\n  Combustion System", "comments": "2nd version: typo corrected, corresponding author changed 19 pages, 9\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep learning method to simulate the motion of particles\ntrapped in a chaotic recirculating flame. The Lagrangian trajectories of\nparticles, captured using a high-speed camera and subsequently reconstructed in\n3-dimensional space, were used to train a variational autoencoder (VAE) which\ncomprises multiple layers of convolutional neural networks. We show that the\ntrajectories, which are statistically representative of those determined in\nexperiments, can be generated using the VAE network. The performance of our\nmodel is evaluated with respect to the accuracy and generalization of the\noutputs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 00:44:58 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 03:18:25 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Liu", "Pai", ""], ["Gan", "Jingwei", ""], ["Chakrabarty", "Rajan K.", ""]]}, {"id": "1811.11922", "submitter": "Zhuoyi Yang", "authors": "Xiaozhou Wang, Zhuoyi Yang, Xi Chen, Weidong Liu", "title": "Distributed Inference for Linear Support Vector Machine", "comments": "50 pages, 11 figures", "journal-ref": "Journal of Machine Learning Research (JMLR), v20(113):1-41, 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing size of modern data brings many new challenges to existing\nstatistical inference methodologies and theories, and calls for the development\nof distributed inferential approaches. This paper studies distributed inference\nfor linear support vector machine (SVM) for the binary classification task.\nDespite a vast literature on SVM, much less is known about the inferential\nproperties of SVM, especially in a distributed setting. In this paper, we\npropose a multi-round distributed linear-type (MDL) estimator for conducting\ninference for linear SVM. The proposed estimator is computationally efficient.\nIn particular, it only requires an initial SVM estimator and then successively\nrefines the estimator by solving simple weighted least squares problem.\nTheoretically, we establish the Bahadur representation of the estimator. Based\non the representation, the asymptotic normality is further derived, which shows\nthat the MDL estimator achieves the optimal statistical efficiency, i.e., the\nsame efficiency as the classical linear SVM applying to the entire data set in\na single machine setup. Moreover, our asymptotic result avoids the condition on\nthe number of machines or data batches, which is commonly assumed in\ndistributed estimation literature, and allows the case of diverging dimension.\nWe provide simulation studies to demonstrate the performance of the proposed\nMDL estimator.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 02:05:09 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 20:23:16 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Wang", "Xiaozhou", ""], ["Yang", "Zhuoyi", ""], ["Chen", "Xi", ""], ["Liu", "Weidong", ""]]}, {"id": "1811.11925", "submitter": "Vaneet Aggarwal", "authors": "Mridul Agarwal and Vaneet Aggarwal", "title": "Regret Bounds for Stochastic Combinatorial Multi-Armed Bandits with\n  Linear Space Complexity", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world problems face the dilemma of choosing best $K$ out of $N$\noptions at a given time instant. This setup can be modelled as combinatorial\nbandit which chooses $K$ out of $N$ arms at each time, with an aim to achieve\nan efficient tradeoff between exploration and exploitation. This is the first\nwork for combinatorial bandit where the reward received can be a non-linear\nfunction of the chosen $K$ arms. The direct use of multi-armed bandit requires\nchoosing among $N$-choose-$K$ options making the state space large. In this\npaper, we present a novel algorithm which is computationally efficient and the\nstorage is linear in $N$. The proposed algorithm is a divide-and-conquer based\nstrategy, that we call CMAB-SM. Further, the proposed algorithm achieves a\nregret bound of $\\tilde O(K^\\frac{1}{2}N^\\frac{1}{3}T^\\frac{2}{3})$ for a time\nhorizon $T$, which is sub-linear in all parameters $T$, $N$, and $K$. The\nevaluation results on different reward functions and arm distribution functions\nshow significantly improved performance as compared to standard multi-armed\nbandit approach with $\\binom{N}{K}$ choices.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 02:12:37 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Agarwal", "Mridul", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "1811.11926", "submitter": "Dustin Tran", "authors": "Matthew D. Hoffman and Matthew J. Johnson and Dustin Tran", "title": "Autoconj: Recognizing and Exploiting Conjugacy Without a Domain-Specific\n  Language", "comments": "Appears in Neural Information Processing Systems, 2018. Code\n  available at https://github.com/google-research/autoconj", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deriving conditional and marginal distributions using conjugacy relationships\ncan be time consuming and error prone. In this paper, we propose a strategy for\nautomating such derivations. Unlike previous systems which focus on\nrelationships between pairs of random variables, our system (which we call\nAutoconj) operates directly on Python functions that compute log-joint\ndistribution functions. Autoconj provides support for conjugacy-exploiting\nalgorithms in any Python embedded PPL. This paves the way for accelerating\ndevelopment of novel inference algorithms and structure-exploiting modeling\nstrategies.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 02:13:37 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Hoffman", "Matthew D.", ""], ["Johnson", "Matthew J.", ""], ["Tran", "Dustin", ""]]}, {"id": "1811.11960", "submitter": "Gaurav Sheni", "authors": "Gaurav Sheni, Benjamin Schreck, Roy Wedge, James Max Kanter, Kalyan\n  Veeramachaneni", "title": "Prediction Factory: automated development and collaborative evaluation\n  of predictive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a data science automation system called Prediction\nFactory. The system uses several key automation algorithms to enable data\nscientists to rapidly develop predictive models and share them with domain\nexperts. To assess the system's impact, we implemented 3 different interfaces\nfor creating predictive modeling projects: baseline automation, full\nautomation, and optional automation. With a dataset of online grocery shopper\nbehaviors, we divided data scientists among the interfaces to specify\nprediction problems, learn and evaluate models, and write a report for domain\nexperts to judge whether or not to fund to continue working on. In total, 22\ndata scientists created 94 reports that were judged 296 times by 26 experts. In\na head-to-head trial, reports generated utilizing full data science automation\ninterface reports were funded 57.5% of the time, while the ones that used\nbaseline automation were only funded 42.5% of the time. An intermediate\ninterface which supports optional automation generated reports were funded\n58.6% more often compared to the baseline. Full automation and optional\nautomation reports were funded about equally when put head-to-head. These\nresults demonstrate that Prediction Factory has implemented a critical amount\nof automation to augment the role of data scientists and improve business\noutcomes.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 04:34:48 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Sheni", "Gaurav", ""], ["Schreck", "Benjamin", ""], ["Wedge", "Roy", ""], ["Kanter", "James Max", ""], ["Veeramachaneni", "Kalyan", ""]]}, {"id": "1811.11987", "submitter": "Laurent Bou\\'e", "authors": "Laurent Bou\\'e", "title": "Deep learning for pedestrians: backpropagation in CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.SC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this document is to provide a pedagogical introduction to the\nmain concepts underpinning the training of deep neural networks using gradient\ndescent; a process known as backpropagation. Although we focus on a very\ninfluential class of architectures called \"convolutional neural networks\"\n(CNNs) the approach is generic and useful to the machine learning community as\na whole. Motivated by the observation that derivations of backpropagation are\noften obscured by clumsy index-heavy narratives that appear somewhat\nmathemagical, we aim to offer a conceptually clear, vectorized description that\narticulates well the higher level logic. Following the principle of \"writing is\nnature's way of letting you know how sloppy your thinking is\", we try to make\nthe calculations meticulous, self-contained and yet as intuitive as possible.\nTaking nothing for granted, ample illustrations serve as visual guides and an\nextensive bibliography is provided for further explorations.\n  (For the sake of clarity, long mathematical derivations and visualizations\nhave been broken up into short \"summarized views\" and longer \"detailed views\"\nencoded into the PDF as optional content groups. Some figures contain\nanimations designed to illustrate important concepts in a more engaging style.\nFor these reasons, we advise to download the document locally and open it using\nAdobe Acrobat Reader. Other viewers were not tested and may not render the\ndetailed views, animations correctly.)\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 07:00:09 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Bou\u00e9", "Laurent", ""]]}, {"id": "1811.11989", "submitter": "Quanquan Gu", "authors": "Dongruo Zhou, Pan Xu, Quanquan Gu", "title": "Sample Efficient Stochastic Variance-Reduced Cubic Regularization Method", "comments": "24 pages, 2 figures, 1 table. The first version of this paper was\n  submitted to UAI 2018 on March 9, 2018. This is the second version with\n  improved presentation and additional baselines in the experiments, and was\n  submitted to NeurIPS 2018 on May 18, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sample efficient stochastic variance-reduced cubic\nregularization (Lite-SVRC) algorithm for finding the local minimum efficiently\nin nonconvex optimization. The proposed algorithm achieves a lower sample\ncomplexity of Hessian matrix computation than existing cubic regularization\nbased methods. At the heart of our analysis is the choice of a constant batch\nsize of Hessian matrix computation at each iteration and the stochastic\nvariance reduction techniques. In detail, for a nonconvex function with $n$\ncomponent functions, Lite-SVRC converges to the local minimum within\n$\\tilde{O}(n+n^{2/3}/\\epsilon^{3/2})$ Hessian sample complexity, which is\nfaster than all existing cubic regularization based methods. Numerical\nexperiments with different nonconvex optimization problems conducted on real\ndatasets validate our theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 07:10:44 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Zhou", "Dongruo", ""], ["Xu", "Pan", ""], ["Gu", "Quanquan", ""]]}, {"id": "1811.12019", "submitter": "Kazuki Osawa", "authors": "Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota,\n  and Satoshi Matsuoka", "title": "Large-Scale Distributed Second-Order Optimization Using\n  Kronecker-Factored Approximate Curvature for Deep Convolutional Neural\n  Networks", "comments": "10 pages, 7 figures. Accepted at CVPR 2019, Long Beach, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale distributed training of deep neural networks suffer from the\ngeneralization gap caused by the increase in the effective mini-batch size.\nPrevious approaches try to solve this problem by varying the learning rate and\nbatch size over epochs and layers, or some ad hoc modification of the batch\nnormalization. We propose an alternative approach using a second-order\noptimization method that shows similar generalization capability to first-order\nmethods, but converges faster and can handle larger mini-batches. To test our\nmethod on a benchmark where highly optimized first-order methods are available\nas references, we train ResNet-50 on ImageNet. We converged to 75% Top-1\nvalidation accuracy in 35 epochs for mini-batch sizes under 16,384, and\nachieved 75% even with a mini-batch size of 131,072, which took only 978\niterations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 08:52:04 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 12:46:17 GMT"}, {"version": "v3", "created": "Sun, 3 Mar 2019 08:45:32 GMT"}, {"version": "v4", "created": "Thu, 28 Mar 2019 08:20:19 GMT"}, {"version": "v5", "created": "Sat, 30 Mar 2019 04:24:57 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Osawa", "Kazuki", ""], ["Tsuji", "Yohei", ""], ["Ueno", "Yuichiro", ""], ["Naruse", "Akira", ""], ["Yokota", "Rio", ""], ["Matsuoka", "Satoshi", ""]]}, {"id": "1811.12040", "submitter": "Brendan Avent", "authors": "Brendan Avent, Yatharth Dubey, Aleksandra Korolova", "title": "The Power of The Hybrid Model for Mean Estimation", "comments": "Proceedings on Privacy Enhancing Technologies 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the power of the hybrid model of differential privacy (DP), in\nwhich some users desire the guarantees of the local model of DP and others are\ncontent with receiving the trusted-curator model guarantees. In particular, we\nstudy the utility of hybrid model estimators that compute the mean of arbitrary\nreal-valued distributions with bounded support. When the curator knows the\ndistribution's variance, we design a hybrid estimator that, for realistic\ndatasets and parameter settings, achieves a constant factor improvement over\nnatural baselines. We then analytically characterize how the estimator's\nutility is parameterized by the problem setting and parameter choices. When the\ndistribution's variance is unknown, we design a heuristic hybrid estimator and\nanalyze how it compares to the baselines. We find that it often performs better\nthan the baselines, and sometimes almost as well as the known-variance\nestimator. We then answer the question of how our estimator's utility is\naffected when users' data are not drawn from the same distribution, but rather\nfrom distributions dependent on their trust model preference. Concretely, we\nexamine the implications of the two groups' distributions diverging and show\nthat in some cases, our estimators maintain fairly high utility. We then\ndemonstrate how our hybrid estimator can be incorporated as a sub-component in\nmore complex, higher-dimensional applications. Finally, we propose a new\nprivacy amplification notion for the hybrid model that emerges due to\ninteraction between the groups, and derive corresponding amplification results\nfor our hybrid estimators.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 09:52:17 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 21:14:29 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Avent", "Brendan", ""], ["Dubey", "Yatharth", ""], ["Korolova", "Aleksandra", ""]]}, {"id": "1811.12044", "submitter": "David Charte", "authors": "David Charte (1) and Francisco Charte (2) and Salvador Garc\\'ia (1)\n  and Francisco Herrera (1) ((1) Universidad de Granada, (2) Universidad de\n  Ja\\'en)", "title": "A snapshot on nonstandard supervised learning problems: taxonomy,\n  relationships and methods", "comments": null, "journal-ref": "Charte, D., Charte, F., Garc\\'ia, S. et al. Prog Artif Intell\n  (2018)", "doi": "10.1007/s13748-018-00167-7", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is a field which studies how machines can alter and adapt\ntheir behavior, improving their actions according to the information they are\ngiven. This field is subdivided into multiple areas, among which the best known\nare supervised learning (e.g. classification and regression) and unsupervised\nlearning (e.g. clustering and association rules).\n  Within supervised learning, most studies and research are focused on well\nknown standard tasks, such as binary classification, multiclass classification\nand regression with one dependent variable. However, there are many other less\nknown problems. These are what we generically call nonstandard supervised\nlearning problems. The literature about them is much more sparse, and each\nstudy is directed to a specific task. Therefore, the definitions, relations and\napplications of this kind of learners are hard to find.\n  The goal of this paper is to provide the reader with a broad view on the\ndistinct variations of nonstandard supervised problems. A comprehensive\ntaxonomy summarizing their traits is proposed. A review of the common\napproaches followed to accomplish them and their main applications is provided\nas well.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 10:03:06 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Charte", "David", ""], ["Charte", "Francisco", ""], ["Garc\u00eda", "Salvador", ""], ["Herrera", "Francisco", ""]]}, {"id": "1811.12050", "submitter": "Daniel J. Trosten", "authors": "Daniel J. Trosten, Andreas S. Strauman, Michael Kampffmeyer, Robert\n  Jenssen", "title": "Recurrent Deep Divergence-based Clustering for simultaneous feature\n  learning and clustering of variable length time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of clustering unlabeled time series and sequences entails a\nparticular set of challenges, namely to adequately model temporal relations and\nvariable sequence lengths. If these challenges are not properly handled, the\nresulting clusters might be of suboptimal quality. As a key solution, we\npresent a joint clustering and feature learning framework for time series based\non deep learning. For a given set of time series, we train a recurrent network\nto represent, or embed, each time series in a vector space such that a\ndivergence-based clustering loss function can discover the underlying cluster\nstructure in an end-to-end manner. Unlike previous approaches, our model\ninherently handles multivariate time series of variable lengths and does not\nrequire specification of a distance-measure in the input space. On a diverse\nset of benchmark datasets we illustrate that our proposed Recurrent Deep\nDivergence-based Clustering approach outperforms, or performs comparable to,\nprevious approaches.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 10:23:01 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 12:50:43 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Trosten", "Daniel J.", ""], ["Strauman", "Andreas S.", ""], ["Kampffmeyer", "Michael", ""], ["Jenssen", "Robert", ""]]}, {"id": "1811.12064", "submitter": "David Saltiel", "authors": "David Saltiel, Eric Benhamou", "title": "Feature selection with optimal coordinate ascent (OCA)", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, Feature Selection (FS) is a major part of efficient\nalgorithm. It fuels the algorithm and is the starting block for our prediction.\nIn this paper, we present a new method, called Optimal Coordinate Ascent (OCA)\nthat allows us selecting features among block and individual features. OCA\nrelies on coordinate ascent to find an optimal solution for gradient boosting\nmethods score (number of correctly classified samples). OCA takes into account\nthe notion of dependencies between variables forming blocks in our\noptimization. The coordinate ascent optimization solves the issue of the NP\nhard original problem where the number of combinations rapidly explode making a\ngrid search unfeasible. It reduces considerably the number of iterations\nchanging this NP hard problem into a polynomial search one. OCA brings\nsubstantial differences and improvements compared to previous coordinate ascent\nfeature selection method: we group variables into block and individual\nvariables instead of a binary selection. Our initial guess is based on the\nk-best group variables making our initial point more robust. We also introduced\nnew stopping criteria making our optimization faster. We compare these two\nmethods on our data set. We found that our method outperforms the initial one.\nWe also compare our method to the Recursive Feature Elimination (RFE) method\nand find that OCA leads to the minimum feature set with the highest score. This\nis a nice byproduct of our method as it provides empirically the most compact\ndata set with optimal performance.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 11:04:44 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 07:51:06 GMT"}, {"version": "v3", "created": "Mon, 3 Dec 2018 08:25:53 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Saltiel", "David", ""], ["Benhamou", "Eric", ""]]}, {"id": "1811.12065", "submitter": "Lile Cai", "authors": "Lile Cai, Anne-Maelle Barneche, Arthur Herbout, Chuan Sheng Foo, Jie\n  Lin, Vijay Ramaseshan Chandrasekhar and Mohamed M. Sabry", "title": "TEA-DNN: the Quest for Time-Energy-Accuracy Co-optimized Deep Neural\n  Networks", "comments": "Accepted by ISLPED2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedded deep learning platforms have witnessed two simultaneous\nimprovements. First, the accuracy of convolutional neural networks (CNNs) has\nbeen significantly improved through the use of automated neural-architecture\nsearch (NAS) algorithms to determine CNN structure. Second, there has been\nincreasing interest in developing hardware accelerators for CNNs that provide\nimproved inference performance and energy consumption compared to GPUs. Such\nembedded deep learning platforms differ in the amount of compute resources and\nmemory-access bandwidth, which would affect performance and energy consumption\nof CNNs. It is therefore critical to consider the available hardware resources\nin the network architecture search. To this end, we introduce TEA-DNN, a NAS\nalgorithm targeting multi-objective optimization of execution time, energy\nconsumption, and classification accuracy of CNN workloads on embedded\narchitectures. TEA-DNN leverages energy and execution time measurements on\nembedded hardware when exploring the Pareto-optimal curves across accuracy,\nexecution time, and energy consumption and does not require additional effort\nto model the underlying hardware. We apply TEA-DNN for image classification on\nactual embedded platforms (NVIDIA Jetson TX2 and Intel Movidius Neural Compute\nStick). We highlight the Pareto-optimal operating points that emphasize the\nnecessity to explicitly consider hardware characteristics in the search\nprocess. To the best of our knowledge, this is the most comprehensive study of\nPareto-optimal models across a range of hardware platforms using actual\nmeasurements on hardware to obtain objective values.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 11:05:28 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 07:39:19 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Cai", "Lile", ""], ["Barneche", "Anne-Maelle", ""], ["Herbout", "Arthur", ""], ["Foo", "Chuan Sheng", ""], ["Lin", "Jie", ""], ["Chandrasekhar", "Vijay Ramaseshan", ""], ["Sabry", "Mohamed M.", ""]]}, {"id": "1811.12069", "submitter": "Sangnam Park", "authors": "Jason Lee, Inkyu Park, Sangnam Park", "title": "Multi-Scale Distributed Representation for Deep Learning and its\n  Application to b-Jet Tagging", "comments": "13 pages, 8 figures", "journal-ref": "J.Korean Phys.Soc. 72 (2018) no.11, 1292-1300", "doi": "10.3938/jkps.72.1292", "report-no": null, "categories": "hep-ex cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently machine learning algorithms based on deep layered artificial neural\nnetworks (DNNs) have been applied to a wide variety of high energy physics\nproblems such as jet tagging or event classification. We explore a simple but\neffective preprocessing step which transforms each real-valued observational\nquantity or input feature into a binary number with a fixed number of digits.\nEach binary digit represents the quantity or magnitude in different scales. We\nhave shown that this approach improves the performance of DNNs significantly\nfor some specific tasks without any further complication in feature\nengineering. We apply this multi-scale distributed binary representation to\ndeep learning on b-jet tagging using daughter particles' momenta and vertex\ninformation.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 11:16:06 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Lee", "Jason", ""], ["Park", "Inkyu", ""], ["Park", "Sangnam", ""]]}, {"id": "1811.12081", "submitter": "Fernando Fernandes Neto", "authors": "Fernando Fernandes Neto, Alemayehu Admasu Solomon, Rodrigo de Losso,\n  Claudio Garcia, Pedro Delano Cavalcanti", "title": "Deep Haar Scattering Networks in Pattern Recognition: A promising\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to discuss the use of Haar scattering networks,\nwhich is a very simple architecture that naturally supports a large number of\nstacked layers, yet with very few parameters, in a relatively broad set of\npattern recognition problems, including regression and classification tasks.\nThis architecture, basically, consists of stacking convolutional filters, that\ncan be thought as a generalization of Haar wavelets, followed by non-linear\noperators which aim to extract symmetries and invariances that are later fed in\na classification/regression algorithm. We show that good results can be\nobtained with the proposed method for both kind of tasks. We have outperformed\nthe best available algorithms in 4 out of 18 important data classification\nproblems, and have obtained a more robust performance than ARIMA and ETS time\nseries methods in regression problems for data with strong periodicities.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 11:50:58 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Neto", "Fernando Fernandes", ""], ["Solomon", "Alemayehu Admasu", ""], ["de Losso", "Rodrigo", ""], ["Garcia", "Claudio", ""], ["Cavalcanti", "Pedro Delano", ""]]}, {"id": "1811.12084", "submitter": "Andreas Selmar Hauptmann", "authors": "Simon Arridge and Andreas Hauptmann", "title": "Networks for Nonlinear Diffusion Problems in Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.AP math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multitude of imaging and vision tasks have seen recently a major\ntransformation by deep learning methods and in particular by the application of\nconvolutional neural networks. These methods achieve impressive results, even\nfor applications where it is not apparent that convolutions are suited to\ncapture the underlying physics.\n  In this work we develop a network architecture based on nonlinear diffusion\nprocesses, named DiffNet. By design, we obtain a nonlinear network architecture\nthat is well suited for diffusion related problems in imaging. Furthermore, the\nperformed updates are explicit, by which we obtain better interpretability and\ngeneralisability compared to classical convolutional neural network\narchitectures. The performance of DiffNet tested on the inverse problem of\nnonlinear diffusion with the Perona-Malik filter on the STL-10 image dataset.\nWe obtain competitive results to the established U-Net architecture, with a\nfraction of parameters and necessary training data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 11:54:54 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Arridge", "Simon", ""], ["Hauptmann", "Andreas", ""]]}, {"id": "1811.12119", "submitter": "Lia Ahrens", "authors": "Lia Ahrens, Julian Ahrens, Hans D. Schotten", "title": "A Machine-Learning Phase Classification Scheme for Anomaly Detection in\n  Signals with Periodic Characteristics", "comments": "25 pages, 15 figures", "journal-ref": null, "doi": "10.1186/s13634-019-0619-3", "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel machine-learning method for anomaly\ndetection applicable to data with periodic characteristics where randomly\nvarying period lengths are explicitly allowed. A multi-dimensional time series\nanalysis is conducted by training a data-adapted classifier consisting of deep\nconvolutional neural networks performing phase classification. The entire\nalgorithm including data pre-processing, period detection, segmentation, and\neven dynamic adjustment of the neural networks is implemented for fully\nautomatic execution. The proposed method is evaluated on three example datasets\nfrom the areas of cardiology, intrusion detection, and signal processing,\npresenting reasonable performance.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 13:13:41 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 10:34:53 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Ahrens", "Lia", ""], ["Ahrens", "Julian", ""], ["Schotten", "Hans D.", ""]]}, {"id": "1811.12142", "submitter": "Xu Li", "authors": "Chunlin Gong, Xu Li, Hua Su, Jinlei Guo, Liangxian Gu", "title": "Global optimization of expensive black-box models based on asynchronous\n  hybrid-criterion with interval reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, a new sequential surrogate-based optimization (SSBO) algorithm\nis developed, which aims to improve the global search ability and local search\nefficiency for the global optimization of expensive black-box models. The\nproposed method involves three basic sub-criteria to infill new samples\nasynchronously to balance the global exploration and local exploitation. First,\nto capture the promising possible global optimal region, searching for the\nglobal optimum with genetic algorithm (GA) based on the current surrogate\nmodels of the objective and constraint functions. Second, to infill samples in\nthe region with sparse samples to improve the global accuracy of the surrogate\nmodels, a grid searching with Latin hypercube sampling (LHS) with the current\nsurrogate model is adopted to explore the sample space. Third, to accelerate\nthe local searching efficiency, searching for a local optimum with sequential\nquadratic programming (SQP) based on the local surrogate models in the reduced\ninterval, which involves some samples near the current optimum. When the new\nsample is too close to the existing ones, the new sample should be abandoned,\ndue to the poor additional information. According to the three sub-criteria,\nthe new samples are placed in the regions which have not been fully explored\nand includes the possible global optimum point. When a possible global optimum\npoint is found, the local searching sub-criterion captures the local optimum\naround it rapidly. Numerical and engineering examples are used to verify the\nefficiency of the proposed method. The statistical results show that the\nproposed method has good global searching ability and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 13:50:47 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Gong", "Chunlin", ""], ["Li", "Xu", ""], ["Su", "Hua", ""], ["Guo", "Jinlei", ""], ["Gu", "Liangxian", ""]]}, {"id": "1811.12143", "submitter": "Imanol Schlag", "authors": "Imanol Schlag, J\\\"urgen Schmidhuber", "title": "Learning to Reason with Third-Order Tensor Products", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We combine Recurrent Neural Networks with Tensor Product Representations to\nlearn combinatorial representations of sequential data. This improves symbolic\ninterpretation and systematic generalisation. Our architecture is trained\nend-to-end through gradient descent on a variety of simple natural language\nreasoning tasks, significantly outperforming the latest state-of-the-art models\nin single-task and all-tasks settings. We also augment a subset of the data\nsuch that training and test data exhibit large systematic differences and show\nthat our approach generalises better than the previous state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 13:50:58 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 10:36:24 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Schlag", "Imanol", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1811.12156", "submitter": "Jayaraman J. Thiagarajan", "authors": "Huan Song and Jayaraman J. Thiagarajan", "title": "Improved Deep Embeddings for Inferencing with Multi-Layered Networks", "comments": "IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferencing with network data necessitates the mapping of its nodes into a\nvector space, where the relationships are preserved. However, with\nmulti-layered networks, where multiple types of relationships exist for the\nsame set of nodes, it is crucial to exploit the information shared between\nlayers, in addition to the distinct aspects of each layer. In this paper, we\npropose a novel approach that first obtains node embeddings in all layers\njointly via DeepWalk on a \\textit{supra} graph, which allows interactions\nbetween layers, and then fine-tunes the embeddings to encourage cohesive\nstructure in the latent space. With empirical studies in node classification,\nlink prediction and multi-layered community detection, we show that the\nproposed approach outperforms existing single- and multi-layered network\nembedding algorithms on several benchmarks. In addition to effectively scaling\nto a large number of layers (tested up to $37$), our approach consistently\nproduces highly modular community structure, even when compared to methods that\ndirectly optimize for the modularity function.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 19:07:45 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 22:40:17 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Song", "Huan", ""], ["Thiagarajan", "Jayaraman J.", ""]]}, {"id": "1811.12160", "submitter": "Jianguo Chen", "authors": "Jianguo Chen, Kenli Li, Kashif Bilal, Ahmed A. Metwally, Keqin Li,\n  Philip S. Yu", "title": "Parallel Protein Community Detection in Large-scale PPI Networks Based\n  on Multi-source Learning", "comments": "IEEE/ACM Transactions on Computational Biology and Bioinformatics,\n  2018", "journal-ref": null, "doi": "10.1109/TCBB.2018.2868088", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein interactions constitute the fundamental building block of almost\nevery life activity. Identifying protein communities from Protein-Protein\nInteraction (PPI) networks is essential to understand the principles of\ncellular organization and explore the causes of various diseases. It is\ncritical to integrate multiple data resources to identify reliable protein\ncommunities that have biological significance and improve the performance of\ncommunity detection methods for large-scale PPI networks. In this paper, we\npropose a Multi-source Learning based Protein Community Detection (MLPCD)\nalgorithm by integrating Gene Expression Data (GED) and a parallel solution of\nMLPCD using cloud computing technology. To effectively discover the biological\nfunctions of proteins that participating in different cellular processes, GED\nunder different conditions is integrated with the original PPI network to\nreconstruct a Weighted-PPI (WPPI) network. To flexibly identify protein\ncommunities of different scales, we define community modularity and functional\ncohesion measurements and detect protein communities from WPPI using an\nagglomerative method. In addition, we respectively compare the detected\ncommunities with known protein complexes and evaluate the functional enrichment\nof protein function modules using Gene Ontology annotations. Moreover, we\nimplement a parallel version of the MLPCD algorithm on the Apache Spark\nplatform to enhance the performance of the algorithm for large-scale realistic\nPPI networks. Extensive experimental results indicate the superiority and\nnotable advantages of the MLPCD algorithm over the relevant algorithms in terms\nof accuracy and performance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 20:43:50 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Chen", "Jianguo", ""], ["Li", "Kenli", ""], ["Bilal", "Kashif", ""], ["Metwally", "Ahmed A.", ""], ["Li", "Keqin", ""], ["Yu", "Philip S.", ""]]}, {"id": "1811.12162", "submitter": "Jonathan Eskreis-Winkler", "authors": "Jonathan Eskreis-Winkler and Risi Kondor", "title": "Effective Resistance-based Germination of Seed Sets for Community\n  Detection", "comments": "10 pages, 4 figures, currently under review for conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is, at its core, an attempt to attach an interpretable\nfunction to an otherwise indecipherable form. The importance of labeling\ncommunities has obvious implications for identifying clusters in social\nnetworks, but it has a number of equally relevant applications in product\nrecommendations, biological systems, and many forms of classification. The\nlocal variety of community detection starts with a small set of labeled seed\nnodes, and aims to estimate the community containing these nodes. One of the\nmost ubiquitous methods - due to its simplicity and efficiency - is\npersonalized PageRank. The most obvious bottleneck for deploying this form of\nPageRank successfully is the quality of the seeds. We introduce a \"germination\"\nstage for these seeds, where an effective resistance-based approach is used to\nincrease the quality and number of seeds from which a community is detected. By\nbreaking seed set expansion into a two-step process, we aim to utilize two\ndistinct random walk-based approaches in the regimes in which they excel. In\nsynthetic and real network data, a simple, greedy algorithm which minimizes the\neffective resistance diameter combined with PageRank achieves clear\nimprovements in precision and recall over a standalone PageRank procedure.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 13:32:22 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Eskreis-Winkler", "Jonathan", ""], ["Kondor", "Risi", ""]]}, {"id": "1811.12164", "submitter": "Jiankai Sun", "authors": "Jiankai Sun, and Srinivasan Parthasarathy", "title": "Symmetrization for Embedding Directed Graphs", "comments": "has been accepted to The Thirty-Third AAAI Conference on Artificial\n  Intelligence (AAAI 2019) Student Abstract and Poster Program", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, one has seen a surge of interest in developing such methods\nincluding ones for learning such representations for (undirected) graphs (while\npreserving important properties). However, most of the work to date on\nembedding graphs has targeted undirected networks and very little has focused\non the thorny issue of embedding directed networks. In this paper, we instead\npropose to solve the directed graph embedding problem via a two-stage approach:\nin the first stage, the graph is symmetrized in one of several possible ways,\nand in the second stage, the so-obtained symmetrized graph is embedded using\nany state-of-the-art (undirected) graph embedding algorithm. Note that it is\nnot the objective of this paper to propose a new (undirected) graph embedding\nalgorithm or discuss the strengths and weaknesses of existing ones; all we are\nsaying is that whichever be the suitable graph embedding algorithm, it will fit\nin the above proposed symmetrization framework.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 14:20:44 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Sun", "Jiankai", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1811.12166", "submitter": "Ryohei Hisano", "authors": "Ryohei Hisano, Didier Sornette and Takayuki Mizuno", "title": "Prediction of ESG Compliance using a Heterogeneous Information Network", "comments": null, "journal-ref": "J Big Data 7, 22 (2020)", "doi": "10.1186/s40537-020-00295-9", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Negative screening is one method to avoid interactions with inappropriate\nentities. For example, financial institutions keep investment exclusion lists\nof inappropriate firms that have environmental, social, and government (ESG)\nproblems. They create their investment exclusion lists by gathering information\nfrom various news sources to keep their portfolios profitable as well as green.\nInternational organizations also maintain smart sanctions lists that are used\nto prohibit trade with entities that are involved in illegal activities. In the\npresent paper, we focus on the prediction of investment exclusion lists in the\nfinance domain. We construct a vast heterogeneous information network that\ncovers the necessary information surrounding each firm, which is assembled\nusing seven professionally curated datasets and two open datasets, which\nresults in approximately 50 million nodes and 400 million edges in total.\nExploiting these vast datasets and motivated by how professional investigators\nand journalists undertake their daily investigations, we propose a model that\ncan learn to predict firms that are more likely to be added to an investment\nexclusion list in the near future. Our approach is tested using the negative\nnews investment exclusion list data of more than 35,000 firms worldwide from\nJanuary 2012 to May 2018. Comparing with the state-of-the-art methods with and\nwithout using the network, we show that the predictive accuracy is\nsubstantially improved when using the vast information stored in the\nheterogeneous information network. This work suggests new ways to consolidate\nthe diffuse information contained in big data to monitor dominant firms on a\nglobal scale for better risk management and more socially responsible\ninvestment.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 16:54:18 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 04:51:16 GMT"}, {"version": "v3", "created": "Wed, 13 Nov 2019 12:33:08 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Hisano", "Ryohei", ""], ["Sornette", "Didier", ""], ["Mizuno", "Takayuki", ""]]}, {"id": "1811.12169", "submitter": "Zhou Yang", "authors": "Zhou Yang, Long Nguyen and Fang Jin", "title": "Predicting Opioid Relapse Using Social Media Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opioid addiction is a severe public health threat in the U.S, causing massive\ndeaths and many social problems. Accurate relapse prediction is of practical\nimportance for recovering patients since relapse prediction promotes timely\nrelapse preventions that help patients stay clean. In this paper, we introduce\na Generative Adversarial Networks (GAN) model to predict the addiction relapses\nbased on sentiment images and social influences. Experimental results on real\nsocial media data from Reddit.com demonstrate that the GAN model delivers a\nbetter performance than comparable alternative techniques. The sentiment images\ngenerated by the model show that relapse is closely connected with two emotions\n`joy' and `negative'. This work is one of the first attempts to predict\nrelapses using massive social media data and generative adversarial nets. The\nproposed method, combined with knowledge of social media mining, has the\npotential to revolutionize the practice of opioid addiction prevention and\ntreatment.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 00:20:49 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Yang", "Zhou", ""], ["Nguyen", "Long", ""], ["Jin", "Fang", ""]]}, {"id": "1811.12174", "submitter": "Amit Juneja", "authors": "Samuel Matzek, Max Grossman, Minsik Cho, Anar Yusifov, Bryant Nelson,\n  Amit Juneja", "title": "Data-parallel distributed training of very large models beyond GPU\n  capacity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs have limited memory and it is difficult to train wide and/or deep models\nthat cause the training process to go out of memory. It is shown in this paper\nhow an open source tool called Large Model Support (LMS) can utilize a high\nbandwidth NVLink connection between CPUs and GPUs to accomplish training of\ndeep convolutional networks. LMS performs tensor swapping between CPU memory\nand GPU memory such that only a minimal number of tensors required in a\ntraining step are kept in the GPU memory. It is also shown how LMS can be\ncombined with an MPI based distributed deep learning module to train models in\na data-parallel fashion across multiple GPUs, such that each GPU is utilizing\nthe CPU memory for tensor swapping. The hardware architecture that enables the\nhigh bandwidth GPU link with the CPU is discussed as well as the associated set\nof software tools that are available as the PowerAI package.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 14:22:05 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Matzek", "Samuel", ""], ["Grossman", "Max", ""], ["Cho", "Minsik", ""], ["Yusifov", "Anar", ""], ["Nelson", "Bryant", ""], ["Juneja", "Amit", ""]]}, {"id": "1811.12181", "submitter": "Irene Li", "authors": "Irene Li, Alexander R. Fabbri, Robert R. Tung and Dragomir R. Radev", "title": "What Should I Learn First: Introducing LectureBank for NLP Education and\n  Prerequisite Chain Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the rising popularity of Natural Language\nProcessing (NLP) and related fields such as Artificial Intelligence (AI) and\nMachine Learning (ML). Many online courses and resources are available even for\nthose without a strong background in the field. Often the student is curious\nabout a specific topic but does not quite know where to begin studying. To\nanswer the question of \"what should one learn first,\" we apply an\nembedding-based method to learn prerequisite relations for course concepts in\nthe domain of NLP. We introduce LectureBank, a dataset containing 1,352 English\nlecture files collected from university courses which are each classified\naccording to an existing taxonomy as well as 208 manually-labeled prerequisite\nrelation topics, which is publicly available. The dataset will be useful for\neducational purposes such as lecture preparation and organization as well as\napplications such as reading list generation. Additionally, we experiment with\nneural graph-based networks and non-neural classifiers to learn these\nprerequisite relations from our dataset.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 21:09:20 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Li", "Irene", ""], ["Fabbri", "Alexander R.", ""], ["Tung", "Robert R.", ""], ["Radev", "Dragomir R.", ""]]}, {"id": "1811.12182", "submitter": "Vahid Pourahmadi Dr.", "authors": "Peyman Yazdanian and Vahid Pourahmadi", "title": "DeepPos: Deep Supervised Autoencoder Network for CSI Based Indoor\n  Localization", "comments": "10 pages, 15 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread mobile devices facilitated the emergence of many new\napplications and services. Among them are location-based services (LBS) that\nprovide services based on user's location. Several techniques have been\npresented to enable LBS even in indoor environments where Global Positioning\nSystem (GPS) has low localization accuracy. These methods use some environment\nmeasurements (like Channel State Information (CSI) or Received Signal Strength\n(RSS)) for user localization. In this paper, we will use CSI and a novel deep\nlearning algorithm to design a robust and efficient system for indoor\nlocalization. More precisely, we use supervised autoencoder (SAE) to model the\nenvironment using the data collected during the training phase. Then, during\nthe testing phase, we use the trained model and estimate the coordinates of the\nunknown point by checking different possible labels. Unlike the previous\nfingerprinting approaches, in this work, we do not store the {CSI/RSS} of\nfingerprints and instead we model the environment only with a single SAE. The\nperformance of the proposed scheme is then evaluated in two indoor environments\nand compared with that of similar approaches.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 20:30:36 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Yazdanian", "Peyman", ""], ["Pourahmadi", "Vahid", ""]]}, {"id": "1811.12185", "submitter": "Tanushyam Chattopadhyay", "authors": "Abhisek Das, Satanik Panda, Suman Datta, Soumitra Naskar, Pratep\n  Misra, Tanushyam Chattopadhyay", "title": "AI based Safety System for Employees of Manufacturing Industries in\n  Developing Countries", "comments": "Presented at NIPS 2018 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper authors are going to present a Markov Decision Process (MDP)\nbased algorithm in Industrial Internet of Things (IIoT) as a safety compliance\nlayer for human in loop system. Though some industries are moving towards\nIndustry 4.0 and attempting to automate the systems as much as possible by\nusing robots, still human in loop systems are very common in developing\ncountries like India. When ever there is a need for human machine interaction,\nthere is a scope of health hazard. In this work we have developed a system for\none such industry using MDP. The proposed algorithm used in this system learned\nthe probability of state transition from experience as well as the system is\nadaptable to new changes by incorporating the concept of transfer learning. The\nsystem was evaluated on the data set obtained from 39 sensors connected to a\ncomputer numerically controlled (CNC) machine pushing data every second in a\n24x7 scenario. The state changes are typically instructed by a human which\nsubsequently lead to some intentional or unintentional mistakes and errors. The\nproposed system raises an alarm for the operator to warn which he may or may\nnot overlook depending on his own perception about the present condition of the\nsystem. Repeated ignorance of the operator for a particular state transition\nwarning guides the system to retrain the model. We observed 95.61% alarms\nraised by the said system are taken care of by the operator. 3.2% alarms are\ncoming from the changes in the system which in turn used to retrain the model\nand 1.19% alarms are false alarms. We could not compute the error coming from\nthe mistake performed by the human operator as there is no ground truth\navailable for that.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 07:47:22 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Das", "Abhisek", ""], ["Panda", "Satanik", ""], ["Datta", "Suman", ""], ["Naskar", "Soumitra", ""], ["Misra", "Pratep", ""], ["Chattopadhyay", "Tanushyam", ""]]}, {"id": "1811.12188", "submitter": "Tim Pearce", "authors": "Tim Pearce, Mohamed Zaki, Andy Neely", "title": "Bayesian Neural Network Ensembles", "comments": "arXiv admin note: substantial text overlap with arXiv:1810.05546", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembles of neural networks (NNs) have long been used to estimate predictive\nuncertainty; a small number of NNs are trained from different initialisations\nand sometimes on differing versions of the dataset. The variance of the\nensemble's predictions is interpreted as its epistemic uncertainty. The appeal\nof ensembling stems from being a collection of regular NNs - this makes them\nboth scalable and easily implementable. They have achieved strong empirical\nresults in recent years, often presented as a practical alternative to more\ncostly Bayesian NNs (BNNs). The departure from Bayesian methodology is of\nconcern since the Bayesian framework provides a principled, widely-accepted\napproach to handling uncertainty. In this extended abstract we derive and\nimplement a modified NN ensembling scheme, which provides a consistent\nestimator of the Bayesian posterior in wide NNs - regularising parameters about\nvalues drawn from a prior distribution.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 19:16:09 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Pearce", "Tim", ""], ["Zaki", "Mohamed", ""], ["Neely", "Andy", ""]]}, {"id": "1811.12194", "submitter": "Antonio H. Ribeiro", "authors": "Ant\\^onio H. Ribeiro, Manoel Horta Ribeiro, Gabriela Paix\\~ao, Derick\n  Oliveira, Paulo R. Gomes, J\\'essica A. Canazart, Milton Pifano, Wagner Meira\n  Jr., Thomas B. Sch\\\"on, Antonio Luiz Ribeiro", "title": "Automatic Diagnosis of Short-Duration 12-Lead ECG using a Deep\n  Convolutional Network", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/82", "categories": "eess.SP cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model for predicting electrocardiogram (ECG) abnormalities in\nshort-duration 12-lead ECG signals which outperformed medical doctors on the\n4th year of their cardiology residency. Such exams can provide a full\nevaluation of heart activity and have not been studied in previous end-to-end\nmachine learning papers. Using the database of a large telehealth network, we\nbuilt a novel dataset with more than 2 million ECG tracings, orders of\nmagnitude larger than those used in previous studies. Moreover, our dataset is\nmore realistic, as it consist of 12-lead ECGs recorded during standard\nin-clinics exams. Using this data, we trained a residual neural network with 9\nconvolutional layers to map 7 to 10 second ECG signals to 6 classes of ECG\nabnormalities. Future work should extend these results to cover a large range\nof ECG abnormalities, which could improve the accessibility of this diagnostic\ntool and avoid wrong diagnosis from medical doctors.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 07:39:10 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 07:18:36 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Ribeiro", "Ant\u00f4nio H.", ""], ["Ribeiro", "Manoel Horta", ""], ["Paix\u00e3o", "Gabriela", ""], ["Oliveira", "Derick", ""], ["Gomes", "Paulo R.", ""], ["Canazart", "J\u00e9ssica A.", ""], ["Pifano", "Milton", ""], ["Meira", "Wagner", "Jr."], ["Sch\u00f6n", "Thomas B.", ""], ["Ribeiro", "Antonio Luiz", ""]]}, {"id": "1811.12199", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "Marco Cavallo and \\c{C}a\\u{g}atay Demiralp", "title": "A Visual Interaction Framework for Dimensionality Reduction Based Data\n  Exploration", "comments": "CHI'18. arXiv admin note: text overlap with arXiv:1707.04281", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction is a common method for analyzing and visualizing\nhigh-dimensional data. However, reasoning dynamically about the results of a\ndimensionality reduction is difficult. Dimensionality-reduction algorithms use\ncomplex optimizations to reduce the number of dimensions of a dataset, but\nthese new dimensions often lack a clear relation to the initial data\ndimensions, thus making them difficult to interpret. Here we propose a visual\ninteraction framework to improve dimensionality-reduction based exploratory\ndata analysis. We introduce two interaction techniques, forward projection and\nbackward projection, for dynamically reasoning about dimensionally reduced\ndata. We also contribute two visualization techniques, prolines and feasibility\nmaps, to facilitate the effective use of the proposed interactions. We apply\nour framework to PCA and autoencoder-based dimensionality reductions. Through\ndata-exploration examples, we demonstrate how our visual interactions can\nimprove the use of dimensionality reduction in exploratory data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 01:47:49 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Cavallo", "Marco", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "1811.12210", "submitter": "Arpit Arun Dhobale", "authors": "Kathleen Campbell Garwood, Ph.D., Arpit Arun Dhobale", "title": "A comparison of cluster algorithms as applied to unsupervised surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When considering answering important questions with data, unsupervised data\noffers extensive insight opportunity and unique challenges. This study\nconsiders student survey data with a specific goal of clustering students into\nlike groups with underlying concept of identifying different poverty levels.\nFuzzy logic is considered during the data cleaning and organizing phase helping\nto create a logical dependent variable for analysis comparison. Using multiple\ndata reduction techniques, the survey was reduced and cleaned. Finally,\nmultiple clustering techniques (k-means, k-modes, and hierarchical clustering)\nare applied and compared. Though each method has strengths, the goal was to\nidentify which was most viable when applied to survey data and specifically\nwhen trying to identify the most impoverished students.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 06:48:55 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 05:23:48 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Garwood", "Kathleen Campbell", ""], ["D.", "Ph.", ""], ["Dhobale", "Arpit Arun", ""]]}, {"id": "1811.12223", "submitter": "Wenfu Wang", "authors": "Wenfu Wang, Weijie Yang, An Chen, Zhijie Pan", "title": "A Scoring Method for Driving Safety Credit Using Trajectory Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban traffic systems worldwide are suffering from severe traffic safety\nproblems. Traffic safety is affected by many complex factors, and heavily\nrelated to all drivers' behaviors involved in traffic system. Drivers with\naggressive driving behaviors increase the risk of traffic accidents. In order\nto manage the safety level of traffic system, we propose Driving Safety Credit\ninspired by credit score in financial security field, and design a scoring\nmethod using trajectory data and violation records. First, we extract driving\nhabits, aggressive driving behaviors and traffic violation behaviors from\ndriver's trajectories and traffic violation records. Next, we train a\nclassification model to filtered out irrelevant features. And at last, we score\neach driver with selected features. We verify our proposed scoring method using\n40 days of traffic simulation, and proves the effectiveness of our scoring\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 11:54:20 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Wang", "Wenfu", ""], ["Yang", "Weijie", ""], ["Chen", "An", ""], ["Pan", "Zhijie", ""]]}, {"id": "1811.12227", "submitter": "Wen Wang", "authors": "Wen Wang, Rema Padman, Nirav Shah", "title": "Early Stratification of Patients at Risk for Postoperative Complications\n  after Elective Colectomy", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/55", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stratifying patients at risk for postoperative complications may facilitate\ntimely and accurate workups and reduce the burden of adverse events on patients\nand the health system. Currently, a widely-used surgical risk calculator\ncreated by the American College of Surgeons, NSQIP, uses 21 preoperative\ncovariates to assess risk of postoperative complications, but lacks dynamic,\nreal-time capabilities to accommodate postoperative information. We propose a\nnew Hidden Markov Model sequence classifier for analyzing patients'\npostoperative temperature sequences that incorporates their time-invariant\ncharacteristics in both transition probability and initial state probability in\norder to develop a postoperative \"real-time\" complication detector. Data from\nelective Colectomy surgery indicate that our method has improved classification\nperformance compared to 8 other machine learning classifiers when using the\nfull temperature sequence associated with the patients' length of stay.\nAdditionally, within 44 hours after surgery, the performance of the model is\nclose to that of full-length temperature sequence.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 15:00:30 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Wang", "Wen", ""], ["Padman", "Rema", ""], ["Shah", "Nirav", ""]]}, {"id": "1811.12231", "submitter": "Robert Geirhos", "authors": "Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge,\n  Felix A. Wichmann, Wieland Brendel", "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias\n  improves accuracy and robustness", "comments": "Accepted at ICLR 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are commonly thought to recognise\nobjects by learning increasingly complex representations of object shapes. Some\nrecent studies suggest a more important role of image textures. We here put\nthese conflicting hypotheses to a quantitative test by evaluating CNNs and\nhuman observers on images with a texture-shape cue conflict. We show that\nImageNet-trained CNNs are strongly biased towards recognising textures rather\nthan shapes, which is in stark contrast to human behavioural evidence and\nreveals fundamentally different classification strategies. We then demonstrate\nthat the same standard architecture (ResNet-50) that learns a texture-based\nrepresentation on ImageNet is able to learn a shape-based representation\ninstead when trained on \"Stylized-ImageNet\", a stylized version of ImageNet.\nThis provides a much better fit for human behavioural performance in our\nwell-controlled psychophysical lab setting (nine experiments totalling 48,560\npsychophysical trials across 97 observers) and comes with a number of\nunexpected emergent benefits such as improved object detection performance and\npreviously unseen robustness towards a wide range of image distortions,\nhighlighting advantages of a shape-based representation.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 15:04:05 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 13:59:09 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Geirhos", "Robert", ""], ["Rubisch", "Patricia", ""], ["Michaelis", "Claudio", ""], ["Bethge", "Matthias", ""], ["Wichmann", "Felix A.", ""], ["Brendel", "Wieland", ""]]}, {"id": "1811.12234", "submitter": "Thomas Janssoone", "authors": "Thomas Janssoone and Cl\\'emence Bic and Dorra Kanoun and Pierre Hornus\n  and Pierre Rinder", "title": "Machine Learning on Electronic Health Records: Models and Features\n  Usages to predict Medication Non-Adherence", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adherence can be defined as \"the extent to which patients take their\nmedications as prescribed by their healthcare providers\"[Osterberg and\nBlaschke, 2005]. World Health Organization's reports point out that, in\ndeveloped countries, only about 50% of patients with chronic diseases correctly\nfollow their treatments. This severely compromises the efficiency of long-term\ntherapy and increases the cost of health services. We propose in this paper\ndifferent models of patient drug consumption in breast cancer treatments. The\naim of these different approaches is to predict medication non-adherence while\ngiving insights to doctors of the underlying reasons of these illegitimate\ndrop-outs. Working with oncologists, we show the interest of Machine- Learning\nalgorithms fined tune by the feedback of experts to estimate a risk score of a\npatient's non-adherence and thus improve support throughout their care path.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 15:08:55 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Janssoone", "Thomas", ""], ["Bic", "Cl\u00e9mence", ""], ["Kanoun", "Dorra", ""], ["Hornus", "Pierre", ""], ["Rinder", "Pierre", ""]]}, {"id": "1811.12239", "submitter": "Carolin Lawrence", "authors": "Carolin Lawrence and Stefan Riezler", "title": "Counterfactual Learning from Human Proofreading Feedback for Semantic\n  Parsing", "comments": "\"Learning by Instruction\" Workshop at the 32nd Conference on Neural\n  Information Processing Systems (NIPS 2018), Montr\\'eal, Canada. arXiv admin\n  note: substantial text overlap with arXiv:1805.01252", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In semantic parsing for question-answering, it is often too expensive to\ncollect gold parses or even gold answers as supervision signals. We propose to\nconvert model outputs into a set of human-understandable statements which allow\nnon-expert users to act as proofreaders, providing error markings as learning\nsignals to the parser. Because model outputs were suggested by a historic\nsystem, we operate in a counterfactual, or off-policy, learning setup. We\nintroduce new estimators which can effectively leverage the given feedback and\nwhich avoid known degeneracies in counterfactual learning, while still being\napplicable to stochastic gradient optimization for neural semantic parsing.\nFurthermore, we discuss how our feedback collection method can be seamlessly\nintegrated into deployed virtual personal assistants that embed a semantic\nparser. Our work is the first to show that semantic parsers can be improved\nsignificantly by counterfactual learning from logged human feedback data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 15:20:30 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Lawrence", "Carolin", ""], ["Riezler", "Stefan", ""]]}, {"id": "1811.12253", "submitter": "Anshuka Rangi", "authors": "Anshuka Rangi, Massimo Franceschetti and Long Tran-Thanh", "title": "Unifying the stochastic and the adversarial Bandits with Knapsack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the adversarial Bandits with Knapsack (BwK) online\nlearning problem, where a player repeatedly chooses to perform an action, pays\nthe corresponding cost, and receives a reward associated with the action. The\nplayer is constrained by the maximum budget $B$ that can be spent to perform\nactions, and the rewards and the costs of the actions are assigned by an\nadversary. This problem has only been studied in the restricted setting where\nthe reward of an action is greater than the cost of the action, while we\nprovide a solution in the general setting. Namely, we propose EXP3.BwK, a novel\nalgorithm that achieves order optimal regret. We also propose EXP3++.BwK, which\nis order optimal in the adversarial BwK setup, and incurs an almost optimal\nexpected regret with an additional factor of $\\log(B)$ in the stochastic BwK\nsetup. Finally, we investigate the case of having large costs for the actions\n(i.e., they are comparable to the budget size $B$), and show that for the\nadversarial setting, achievable regret bounds can be significantly worse,\ncompared to the case of having costs bounded by a constant, which is a common\nassumption within the BwK literature.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 04:34:30 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Rangi", "Anshuka", ""], ["Franceschetti", "Massimo", ""], ["Tran-Thanh", "Long", ""]]}, {"id": "1811.12254", "submitter": "Aparna Balagopalan", "authors": "Aparna Balagopalan, Jekaterina Novikova, Frank Rudzicz and Marzyeh\n  Ghassemi", "title": "The Effect of Heterogeneous Data for Alzheimer's Disease Detection from\n  Speech", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/147", "categories": "cs.LG cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech datasets for identifying Alzheimer's disease (AD) are generally\nrestricted to participants performing a single task, e.g. describing an image\nshown to them. As a result, models trained on linguistic features derived from\nsuch datasets may not be generalizable across tasks. Building on prior work\ndemonstrating that same-task data of healthy participants helps improve AD\ndetection on a single-task dataset of pathological speech, we augment an\nAD-specific dataset consisting of subjects describing a picture with multi-task\nhealthy data. We demonstrate that normative data from multiple speech-based\ntasks helps improve AD detection by up to 9%. Visualization of decision\nboundaries reveals that models trained on a combination of structured picture\ndescriptions and unstructured conversational speech have the least out-of-task\nerror and show the most potential to generalize to multiple tasks. We analyze\nthe impact of age of the added samples and if they affect fairness in\nclassification. We also provide explanations for a possible inductive bias\neffect across tasks using model-agnostic feature anchors. This work highlights\nthe need for heterogeneous datasets for encoding changes in multiple facets of\ncognition and for developing a task-independent AD detection model.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 15:37:45 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Balagopalan", "Aparna", ""], ["Novikova", "Jekaterina", ""], ["Rudzicz", "Frank", ""], ["Ghassemi", "Marzyeh", ""]]}, {"id": "1811.12257", "submitter": "Adriano Pastore", "authors": "Adriano Pastore, Michael Gastpar", "title": "Locally Differentially-Private Randomized Response for Discrete\n  Distribution Learning", "comments": "47 pages, under revision at JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a setup in which confidential i.i.d. samples $X_1,\\dotsc,X_n$\nfrom an unknown finite-support distribution $\\boldsymbol{p}$ are passed through\n$n$ copies of a discrete privatization channel (a.k.a. mechanism) producing\noutputs $Y_1,\\dotsc,Y_n$. The channel law guarantees a local differential\nprivacy of $\\epsilon$. Subject to a prescribed privacy level $\\epsilon$, the\noptimal channel should be designed such that an estimate of the source\ndistribution based on the channel outputs $Y_1,\\dotsc,Y_n$ converges as fast as\npossible to the exact value $\\boldsymbol{p}$. For this purpose we study the\nconvergence to zero of three distribution distance metrics: $f$-divergence,\nmean-squared error and total variation. We derive the respective normalized\nfirst-order terms of convergence (as $n\\to\\infty$), which for a given target\nprivacy $\\epsilon$ represent a rule-of-thumb factor by which the sample size\nmust be augmented so as to achieve the same estimation accuracy as that of a\nnon-randomizing channel. We formulate the privacy-fidelity trade-off problem as\nbeing that of minimizing said first-order term under a privacy constraint\n$\\epsilon$. We further identify a scalar quantity that captures the essence of\nthis trade-off, and prove bounds and data-processing inequalities on this\nquantity. For some specific instances of the privacy-fidelity trade-off\nproblem, we derive inner and outer bounds on the optimal trade-off curve.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 15:39:38 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Pastore", "Adriano", ""], ["Gastpar", "Michael", ""]]}, {"id": "1811.12258", "submitter": "Olga Isupova", "authors": "Olga Isupova and Yunpeng Li and Danil Kuzin and Stephen J Roberts and\n  Katherine Willis and Steven Reece", "title": "BCCNet: Bayesian classifier combination neural network", "comments": "Presented at NeurIPS 2018 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning research for developing countries can demonstrate clear\nsustainable impact by delivering actionable and timely information to\nin-country government organisations (GOs) and NGOs in response to their\ncritical information requirements. We co-create products with UK and in-country\ncommercial, GO and NGO partners to ensure the machine learning algorithms\naddress appropriate user needs whether for tactical decision making or\nevidence-based policy decisions. In one particular case, we developed and\ndeployed a novel algorithm, BCCNet, to quickly process large quantities of\nunstructured data to prevent and respond to natural disasters. Crowdsourcing\nprovides an efficient mechanism to generate labels from unstructured data to\nprime machine learning algorithms for large scale data analysis. However, these\nlabels are often imperfect with qualities varying among different citizen\nscientists, which prohibits their direct use with many state-of-the-art machine\nlearning techniques. We describe BCCNet, a framework that simultaneously\naggregates biased and contradictory labels from the crowd and trains an\nautomatic classifier to process new data. Our case studies, mosquito sound\ndetection for malaria prevention and damage detection for disaster response,\nshow the efficacy of our method in the challenging context of developing world\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 15:40:03 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Isupova", "Olga", ""], ["Li", "Yunpeng", ""], ["Kuzin", "Danil", ""], ["Roberts", "Stephen J", ""], ["Willis", "Katherine", ""], ["Reece", "Steven", ""]]}, {"id": "1811.12273", "submitter": "Haytham Fayek", "authors": "Haytham M. Fayek, Lawrence Cavedon, Hong Ren Wu", "title": "On the Transferability of Representations in Neural Networks Between\n  Datasets and Tasks", "comments": "Accepted Paper in the Continual Learning Workshop, NeurIPS 2018", "journal-ref": "Continual Learning Workshop, 32nd Neural Information Processing\n  Systems (NeurIPS 2018), Montreal, Canada", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks, composed of multiple layers of hierarchical distributed\nrepresentations, tend to learn low-level features in initial layers and\ntransition to high-level features towards final layers. Paradigms such as\ntransfer learning, multi-task learning, and continual learning leverage this\nnotion of generic hierarchical distributed representations to share knowledge\nacross datasets and tasks. Herein, we study the layer-wise transferability of\nrepresentations in deep networks across a few datasets and tasks and note some\ninteresting empirical observations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 16:06:57 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Fayek", "Haytham M.", ""], ["Cavedon", "Lawrence", ""], ["Wu", "Hong Ren", ""]]}, {"id": "1811.12276", "submitter": "Parminder Bhatia", "authors": "Mengqi Jin, Mohammad Taha Bahadori, Aaron Colak, Parminder Bhatia,\n  Busra Celikkaya, Ram Bhakta, Selvan Senthivel, Mohammed Khalilia, Daniel\n  Navarro, Borui Zhang, Tiberiu Doman, Arun Ravi, Matthieu Liger, Taha\n  Kass-hout", "title": "Improving Hospital Mortality Prediction with Medical Named Entities and\n  Multimodal Learning", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical text provides essential information to estimate the acuity of a\npatient during hospital stays in addition to structured clinical data. In this\nstudy, we explore how clinical text can complement a clinical predictive\nlearning task. We leverage an internal medical natural language processing\nservice to perform named entity extraction and negation detection on clinical\nnotes and compose selected entities into a new text corpus to train document\nrepresentations. We then propose a multimodal neural network to jointly train\ntime series signals and unstructured clinical text representations to predict\nthe in-hospital mortality risk for ICU patients. Our model outperforms the\nbenchmark by 2% AUC.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 16:10:41 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 01:37:07 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Jin", "Mengqi", ""], ["Bahadori", "Mohammad Taha", ""], ["Colak", "Aaron", ""], ["Bhatia", "Parminder", ""], ["Celikkaya", "Busra", ""], ["Bhakta", "Ram", ""], ["Senthivel", "Selvan", ""], ["Khalilia", "Mohammed", ""], ["Navarro", "Daniel", ""], ["Zhang", "Borui", ""], ["Doman", "Tiberiu", ""], ["Ravi", "Arun", ""], ["Liger", "Matthieu", ""], ["Kass-hout", "Taha", ""]]}, {"id": "1811.12290", "submitter": "Quan Wang", "authors": "Li Wan, Prashant Sridhar, Yang Yu, Quan Wang, Ignacio Lopez Moreno", "title": "Tuplemax Loss for Language Identification", "comments": "Submitted to ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scenarios of a language identification task, the user will specify a\nsmall set of languages which he/she can speak instead of a large set of all\npossible languages. We want to model such prior knowledge into the way we train\nour neural networks, by replacing the commonly used softmax loss function with\na novel loss function named tuplemax loss. As a matter of fact, a typical\nlanguage identification system launched in North America has about 95% users\nwho could speak no more than two languages. Using the tuplemax loss, our system\nachieved a 2.33% error rate, which is a relative 39.4% improvement over the\n3.85% error rate of standard softmax loss method.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 16:28:49 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 20:07:06 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Wan", "Li", ""], ["Sridhar", "Prashant", ""], ["Yu", "Yang", ""], ["Wang", "Quan", ""], ["Moreno", "Ignacio Lopez", ""]]}, {"id": "1811.12295", "submitter": "Sim\\'on Ram\\'irez-Amaya", "authors": "Adolfo Quiroz, Sim\\'on Ram\\'irez-Amaya, \\'Alvaro Riascos", "title": "Regression by clustering using Metropolis-Hastings", "comments": "Presented at NIPS 2018 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High quality risk adjustment in health insurance markets weakens insurer\nincentives to engage in inefficient behavior to attract lower-cost enrollees.\nWe propose a novel methodology based on Markov Chain Monte Carlo methods to\nimprove risk adjustment by clustering diagnostic codes into risk groups optimal\nfor health expenditure prediction. We test the performance of our methodology\nagainst common alternatives using panel data from 500 thousand enrollees of the\nColombian Healthcare System. Results show that our methodology outperforms\ncommon alternatives and suggest that it has potential to improve access to\nquality healthcare for the chronically ill.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 16:36:32 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 19:06:17 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Quiroz", "Adolfo", ""], ["Ram\u00edrez-Amaya", "Sim\u00f3n", ""], ["Riascos", "\u00c1lvaro", ""]]}, {"id": "1811.12296", "submitter": "Vinkle Kumar Srivastav", "authors": "Thibaut Issenhuth, Vinkle Srivastav, Afshin Gangi, Nicolas Padoy", "title": "Face Detection in the Operating Room: Comparison of State-of-the-art\n  Methods and a Self-supervised Approach", "comments": "13 pages", "journal-ref": null, "doi": "10.1007/s11548-019-01944-y", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Purpose: Face detection is a needed component for the automatic analysis and\nassistance of human activities during surgical procedures. Efficient face\ndetection algorithms can indeed help to detect and identify the persons present\nin the room, and also be used to automatically anonymize the data. However,\ncurrent algorithms trained on natural images do not generalize well to the\noperating room (OR) images. In this work, we provide a comparison of\nstate-of-the-art face detectors on OR data and also present an approach to\ntrain a face detector for the OR by exploiting non-annotated OR images.\nMethods: We propose a comparison of 6 state-of-the-art face detectors on\nclinical data using Multi-View Operating Room Faces (MVOR-Faces), a dataset of\noperating room images capturing real surgical activities. We then propose to\nuse self-supervision, a domain adaptation method, for the task of face\ndetection in the OR. The approach makes use of non-annotated images to\nfine-tune a state-of-the-art detector for the OR without using any human\nsupervision. Results: The results show that the best model, namely the tiny\nface detector, yields an average precision of 0.536 at Intersection over Union\n(IoU) of 0.5. Our self-supervised model using non-annotated clinical data\noutperforms this result by 9.2%. Conclusion: We present the first comparison of\nstate-of-the-art face detectors on operating room images and show that results\ncan be significantly improved by using self-supervision on non-annotated data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 16:38:16 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 11:08:53 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Issenhuth", "Thibaut", ""], ["Srivastav", "Vinkle", ""], ["Gangi", "Afshin", ""], ["Padoy", "Nicolas", ""]]}, {"id": "1811.12297", "submitter": "Benjamin Planche", "authors": "Benjamin Planche, Xuejian Rong, Ziyan Wu, Srikrishna Karanam, Harald\n  Kosch, YingLi Tian, Jan Ernst, Andreas Hutter", "title": "Incremental Scene Synthesis", "comments": null, "journal-ref": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to incrementally generate complete 2D or 3D scenes with\nthe following properties: (a) it is globally consistent at each step according\nto a learned scene prior, (b) real observations of a scene can be incorporated\nwhile observing global consistency, (c) unobserved regions can be hallucinated\nlocally in consistence with previous observations, hallucinations and global\npriors, and (d) hallucinations are statistical in nature, i.e., different\nscenes can be generated from the same observations. To achieve this, we model\nthe virtual scene, where an active agent at each step can either perceive an\nobserved part of the scene or generate a local hallucination. The latter can be\ninterpreted as the agent's expectation at this step through the scene and can\nbe applied to autonomous navigation. In the limit of observing real data at\neach point, our method converges to solving the SLAM problem. It can otherwise\nsample entirely imagined scenes from prior distributions. Besides autonomous\nagents, applications include problems where large data is required for building\nrobust real-world applications, but few samples are available. We demonstrate\nefficacy on various 2D as well as 3D data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 16:41:44 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 08:57:48 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 20:41:46 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2019 19:50:54 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Planche", "Benjamin", ""], ["Rong", "Xuejian", ""], ["Wu", "Ziyan", ""], ["Karanam", "Srikrishna", ""], ["Kosch", "Harald", ""], ["Tian", "YingLi", ""], ["Ernst", "Jan", ""], ["Hutter", "Andreas", ""]]}, {"id": "1811.12323", "submitter": "C\\'edric Beaulac", "authors": "C\\'edric Beaulac, Jeffrey S. Rosenthal, David Hodgson", "title": "A Deep Latent-Variable Model Application to Select Treatment Intensity\n  in Survival Analysis", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/53", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the following short article we adapt a new and popular machine learning\nmodel for inference on medical data sets. Our method is based on the\nVariational AutoEncoder (VAE) framework that we adapt to survival analysis on\nsmall data sets with missing values. In our model, the true health status\nappears as a set of latent variables that affects the observed covariates and\nthe survival chances. We show that this flexible model allows insightful\ndecision-making using a predicted distribution and outperforms a classic\nsurvival analysis model.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 17:19:04 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Beaulac", "C\u00e9dric", ""], ["Rosenthal", "Jeffrey S.", ""], ["Hodgson", "David", ""]]}, {"id": "1811.12335", "submitter": "Artur Bekasov", "authors": "Artur Bekasov, Iain Murray", "title": "Bayesian Adversarial Spheres: Bayesian Inference and Adversarial\n  Examples in a Noiseless Setting", "comments": "To appear in the third workshop on Bayesian Deep Learning (NeurIPS\n  2018), Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural network models suffer from adversarial examples, i.e.\nconfidently misclassified points in the input space. It has been shown that\nBayesian neural networks are a promising approach for detecting adversarial\npoints, but careful analysis is problematic due to the complexity of these\nmodels. Recently Gilmer et al. (2018) introduced adversarial spheres, a toy\nset-up that simplifies both practical and theoretical analysis of the problem.\nIn this work, we use the adversarial sphere set-up to understand the properties\nof approximate Bayesian inference methods for a linear model in a noiseless\nsetting. We compare predictions of Bayesian and non-Bayesian methods,\nshowcasing the advantages of the former, although revealing open challenges for\ndeep learning applications.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 17:37:22 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Bekasov", "Artur", ""], ["Murray", "Iain", ""]]}, {"id": "1811.12337", "submitter": "Freweyni Kidane Teklehaymanot", "authors": "Freweyni K. Teklehaymanot, Michael Muma, and Abdelhak M. Zoubir", "title": "Robust Bayesian Cluster Enumeration Based on the $t$ Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in cluster analysis is that the number of data clusters is\nmostly unknown and it must be estimated prior to clustering the observed data.\nIn real-world applications, the observed data is often subject to heavy tailed\nnoise and outliers which obscure the true underlying structure of the data.\nConsequently, estimating the number of clusters becomes challenging. To this\nend, we derive a robust cluster enumeration criterion by formulating the\nproblem of estimating the number of clusters as maximization of the posterior\nprobability of multivariate $t_\\nu$ distributed candidate models. We utilize\nBayes' theorem and asymptotic approximations to come up with a robust criterion\nthat possesses a closed-form expression. Further, we refine the derivation and\nprovide a robust cluster enumeration criterion for data sets with finite sample\nsize. The robust criteria require an estimate of cluster parameters for each\ncandidate model as an input. Hence, we propose a two-step cluster enumeration\nalgorithm that uses the expectation maximization algorithm to partition the\ndata and estimate cluster parameters prior to the calculation of one of the\nrobust criteria. The performance of the proposed algorithm is tested and\ncompared to existing cluster enumeration methods using numerical and real data\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 17:38:27 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 09:41:43 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Teklehaymanot", "Freweyni K.", ""], ["Muma", "Michael", ""], ["Zoubir", "Abdelhak M.", ""]]}, {"id": "1811.12345", "submitter": "Jia Chen", "authors": "Jia Chen and Gang Wang and Georgios B. Giannakis", "title": "Graph Multiview Canonical Correlation Analysis", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2910475", "report-no": null, "categories": "eess.SP cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiview canonical correlation analysis (MCCA) seeks latent low-dimensional\nrepresentations encountered with multiview data of shared entities (a.k.a.\ncommon sources). However, existing MCCA approaches do not exploit the geometry\nof the common sources, which may be available \\emph{a priori}, or can be\nconstructed using certain domain knowledge. This prior information about the\ncommon sources can be encoded by a graph, and be invoked as a regularizer to\nenrich the maximum variance MCCA framework. In this context, the present\npaper's novel graph-regularized (G) MCCA approach minimizes the distance\nbetween the wanted canonical variables and the common low-dimensional\nrepresentations, while accounting for graph-induced knowledge of the common\nsources. Relying on a function capturing the extent low-dimensional\nrepresentations of the multiple views are similar, a generalization bound of\nGMCCA is established based on Rademacher's complexity. Tailored for setups\nwhere the number of data pairs is smaller than the data vector dimensions, a\ngraph-regularized dual MCCA approach is also developed. To further deal with\nnonlinearities present in the data, graph-regularized kernel MCCA variants are\nput forward too. Interestingly, solutions of the graph-regularized linear,\ndual, and kernel MCCA, are all provided in terms of generalized eigenvalue\ndecomposition. Several corroborating numerical tests using real datasets are\nprovided to showcase the merits of the graph-regularized MCCA variants relative\nto several competing alternatives including MCCA, Laplacian-regularized MCCA,\nand (graph-regularized) PCA.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 17:46:51 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 17:29:46 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Chen", "Jia", ""], ["Wang", "Gang", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1811.12346", "submitter": "Xi-Lin Li", "authors": "Xi-Lin Li", "title": "A Multiclass Multiple Instance Learning Method with Exact Likelihood", "comments": "McMIL maximizing exact model likelihood. Implementation:\n  https://github.com/lixilinx/MCMIL", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a multiclass multiple instance learning (MIL) problem where the\nlabels only suggest whether any instance of a class exists or does not exist in\na training sample or example. No further information, e.g., the number of\ninstances of each class, relative locations or orders of all instances in a\ntraining sample, is exploited. Such a weak supervision learning problem can be\nexactly solved by maximizing the model likelihood fitting given observations,\nand finds applications to tasks like multiple object detection and localization\nfor image understanding. We discuss its relationship to the classic\nclassification problem, the traditional MIL, and connectionist temporal\nclassification (CTC). We use image recognition as the example task to develop\nour method, although it is applicable to data with higher or lower dimensions\nwithout much modification. Experimental results show that our method can be\nused to learn all convolutional neural networks for solving real-world multiple\nobject detection and localization tasks with weak annotations, e.g.,\ntranscribing house number sequences from the Google street view imagery\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 17:51:24 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 21:52:34 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Li", "Xi-Lin", ""]]}, {"id": "1811.12351", "submitter": "Nils M\\\"onning", "authors": "Nils M\\\"onning and Suresh Manandhar", "title": "Evaluation of Complex-Valued Neural Networks on Real-Valued\n  Classification Tasks", "comments": "preprint, 18 pages, 8 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex-valued neural networks are not a new concept, however, the use of\nreal-valued models has often been favoured over complex-valued models due to\ndifficulties in training and performance. When comparing real-valued versus\ncomplex-valued neural networks, existing literature often ignores the number of\nparameters, resulting in comparisons of neural networks with vastly different\nsizes. We find that when real and complex neural networks of similar capacity\nare compared, complex models perform equal to or slightly worse than\nreal-valued models for a range of real-valued classification tasks. The use of\ncomplex numbers allows neural networks to handle noise on the complex plane.\nWhen classifying real-valued data with a complex-valued neural network, the\nimaginary parts of the weights follow their real parts. This behaviour is\nindicative for a task that does not require a complex-valued model. We further\ninvestigated this in a synthetic classification task. We can transfer many\nactivation functions from the real to the complex domain using different\nstrategies. The weight initialisation of complex neural networks, however,\nremains a significant problem.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 17:57:45 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["M\u00f6nning", "Nils", ""], ["Manandhar", "Suresh", ""]]}, {"id": "1811.12354", "submitter": "Yoav Artzi", "authors": "Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi", "title": "Touchdown: Natural Language Navigation and Spatial Reasoning in Visual\n  Street Environments", "comments": "arXiv admin note: text overlap with arXiv:1809.00786", "journal-ref": "Published in CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of jointly reasoning about language and vision through a\nnavigation and spatial reasoning task. We introduce the Touchdown task and\ndataset, where an agent must first follow navigation instructions in a\nreal-life visual urban environment, and then identify a location described in\nnatural language to find a hidden object at the goal position. The data\ncontains 9,326 examples of English instructions and spatial descriptions paired\nwith demonstrations. Empirical analysis shows the data presents an open\nchallenge to existing methods, and qualitative linguistic analysis shows that\nthe data displays richer use of spatial reasoning compared to related\nresources.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 18:06:22 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 02:17:42 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 17:27:38 GMT"}, {"version": "v4", "created": "Sat, 6 Apr 2019 17:35:24 GMT"}, {"version": "v5", "created": "Wed, 10 Apr 2019 18:43:40 GMT"}, {"version": "v6", "created": "Mon, 3 Jun 2019 16:24:49 GMT"}, {"version": "v7", "created": "Sat, 16 May 2020 23:36:36 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Chen", "Howard", ""], ["Suhr", "Alane", ""], ["Misra", "Dipendra", ""], ["Snavely", "Noah", ""], ["Artzi", "Yoav", ""]]}, {"id": "1811.12359", "submitter": "Francesco Locatello", "authors": "Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar R\\\"atsch,\n  Sylvain Gelly, Bernhard Sch\\\"olkopf, Olivier Bachem", "title": "Challenging Common Assumptions in the Unsupervised Learning of\n  Disentangled Representations", "comments": null, "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning (ICML 2019)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key idea behind the unsupervised learning of disentangled representations\nis that real-world data is generated by a few explanatory factors of variation\nwhich can be recovered by unsupervised learning algorithms. In this paper, we\nprovide a sober look at recent progress in the field and challenge some common\nassumptions. We first theoretically show that the unsupervised learning of\ndisentangled representations is fundamentally impossible without inductive\nbiases on both the models and the data. Then, we train more than 12000 models\ncovering most prominent methods and evaluation metrics in a reproducible\nlarge-scale experimental study on seven different data sets. We observe that\nwhile the different methods successfully enforce properties ``encouraged'' by\nthe corresponding losses, well-disentangled models seemingly cannot be\nidentified without supervision. Furthermore, increased disentanglement does not\nseem to lead to a decreased sample complexity of learning for downstream tasks.\nOur results suggest that future work on disentanglement learning should be\nexplicit about the role of inductive biases and (implicit) supervision,\ninvestigate concrete benefits of enforcing disentanglement of the learned\nrepresentations, and consider a reproducible experimental setup covering\nseveral data sets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 18:10:40 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 16:42:28 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 17:15:53 GMT"}, {"version": "v4", "created": "Tue, 18 Jun 2019 08:58:18 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Locatello", "Francesco", ""], ["Bauer", "Stefan", ""], ["Lucic", "Mario", ""], ["R\u00e4tsch", "Gunnar", ""], ["Gelly", "Sylvain", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Bachem", "Olivier", ""]]}, {"id": "1811.12361", "submitter": "Aravindan Vijayaraghavan", "authors": "Aditya Bhaskara, Aidao Chen, Aidan Perreault and Aravindan\n  Vijayaraghavan", "title": "Smoothed Analysis in Unsupervised Learning via Decoupling", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smoothed analysis is a powerful paradigm in overcoming worst-case\nintractability in unsupervised learning and high-dimensional data analysis.\nWhile polynomial time smoothed analysis guarantees have been obtained for\nworst-case intractable problems like tensor decompositions and learning\nmixtures of Gaussians, such guarantees have been hard to obtain for several\nother important problems in unsupervised learning. A core technical challenge\nin analyzing algorithms is obtaining lower bounds on the least singular value\nfor random matrix ensembles with dependent entries, that are given by\nlow-degree polynomials of a few base underlying random variables.\n  In this work, we address this challenge by obtaining high-confidence lower\nbounds on the least singular value of new classes of structured random matrix\nensembles of the above kind. We then use these bounds to design algorithms with\npolynomial time smoothed analysis guarantees for the following three important\nproblems in unsupervised learning:\n  1. Robust subspace recovery, when the fraction $\\alpha$ of inliers in the\nd-dimensional subspace $T \\subset \\mathbb{R}^n$ is at least $\\alpha >\n(d/n)^\\ell$ for any constant integer $\\ell>0$. This contrasts with the known\nworst-case intractability when $\\alpha< d/n$, and the previous smoothed\nanalysis result which needed $\\alpha > d/n$ (Hardt and Moitra, 2013).\n  2. Learning overcomplete hidden markov models, where the size of the state\nspace is any polynomial in the dimension of the observations. This gives the\nfirst polynomial time guarantees for learning overcomplete HMMs in a smoothed\nanalysis model.\n  3. Higher order tensor decompositions, where we generalize the so-called\nFOOBI algorithm of Cardoso to find order-$\\ell$ rank-one tensors in a subspace.\nThis allows us to obtain polynomially robust decomposition algorithms for\n$2\\ell$'th order tensors with rank $O(n^{\\ell})$.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 18:12:32 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 02:12:23 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Bhaskara", "Aditya", ""], ["Chen", "Aidao", ""], ["Perreault", "Aidan", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1811.12373", "submitter": "Ke Li", "authors": "Ke Li, Tianhao Zhang, Jitendra Malik", "title": "Diverse Image Synthesis from Semantic Layouts via Conditional IMLE", "comments": "18 pages, 16 figures; IEEE International Conference on Computer\n  Vision (ICCV), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing methods for conditional image synthesis are only able to\ngenerate a single plausible image for any given input, or at best a fixed\nnumber of plausible images. In this paper, we focus on the problem of\ngenerating images from semantic segmentation maps and present a simple new\nmethod that can generate an arbitrary number of images with diverse appearance\nfor the same semantic layout. Unlike most existing approaches which adopt the\nGAN framework, our method is based on the recently introduced Implicit Maximum\nLikelihood Estimation (IMLE) framework. Compared to the leading approach, our\nmethod is able to generate more diverse images while producing fewer artifacts\ndespite using the same architecture. The learned latent space also has sensible\nstructure despite the lack of supervision that encourages such behaviour.\nVideos and code are available at\nhttps://people.eecs.berkeley.edu/~ke.li/projects/imle/scene_layouts/.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 18:36:00 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 17:54:53 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Li", "Ke", ""], ["Zhang", "Tianhao", ""], ["Malik", "Jitendra", ""]]}, {"id": "1811.12386", "submitter": "Josue Nassar", "authors": "Josue Nassar, Scott W. Linderman, Monica Bugallo, Il Memming Park", "title": "Tree-Structured Recurrent Switching Linear Dynamical Systems for\n  Multi-Scale Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world systems studied are governed by complex, nonlinear dynamics.\nBy modeling these dynamics, we can gain insight into how these systems work,\nmake predictions about how they will behave, and develop strategies for\ncontrolling them. While there are many methods for modeling nonlinear dynamical\nsystems, existing techniques face a trade off between offering interpretable\ndescriptions and making accurate predictions. Here, we develop a class of\nmodels that aims to achieve both simultaneously, smoothly interpolating between\nsimple descriptions and more complex, yet also more accurate models. Our\nprobabilistic model achieves this multi-scale property through a hierarchy of\nlocally linear dynamics that jointly approximate global nonlinear dynamics. We\ncall it the tree-structured recurrent switching linear dynamical system. To fit\nthis model, we present a fully-Bayesian sampling procedure using Polya-Gamma\ndata augmentation to allow for fast and conjugate Gibbs sampling. Through a\nvariety of synthetic and real examples, we show how these models outperform\nexisting methods in both interpretability and predictive capability.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 18:53:11 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 16:28:33 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2018 15:31:45 GMT"}, {"version": "v4", "created": "Wed, 30 Jan 2019 04:00:31 GMT"}, {"version": "v5", "created": "Fri, 22 Feb 2019 22:43:13 GMT"}, {"version": "v6", "created": "Tue, 4 Jun 2019 16:11:31 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Nassar", "Josue", ""], ["Linderman", "Scott W.", ""], ["Bugallo", "Monica", ""], ["Park", "Il Memming", ""]]}, {"id": "1811.12395", "submitter": "Tsui-Wei Weng", "authors": "Akhilan Boopathy, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu and Luca\n  Daniel", "title": "CNN-Cert: An Efficient Framework for Certifying Robustness of\n  Convolutional Neural Networks", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verifying robustness of neural network classifiers has attracted great\ninterests and attention due to the success of deep neural networks and their\nunexpected vulnerability to adversarial perturbations. Although finding minimum\nadversarial distortion of neural networks (with ReLU activations) has been\nshown to be an NP-complete problem, obtaining a non-trivial lower bound of\nminimum distortion as a provable robustness guarantee is possible. However,\nmost previous works only focused on simple fully-connected layers (multilayer\nperceptrons) and were limited to ReLU activations. This motivates us to propose\na general and efficient framework, CNN-Cert, that is capable of certifying\nrobustness on general convolutional neural networks. Our framework is general\n-- we can handle various architectures including convolutional layers,\nmax-pooling layers, batch normalization layer, residual blocks, as well as\ngeneral activation functions; our approach is efficient -- by exploiting the\nspecial structure of convolutional layers, we achieve up to 17 and 11 times of\nspeed-up compared to the state-of-the-art certification algorithms (e.g.\nFast-Lin, CROWN) and 366 times of speed-up compared to the dual-LP approach\nwhile our algorithm obtains similar or even better verification bounds. In\naddition, CNN-Cert generalizes state-of-the-art algorithms e.g. Fast-Lin and\nCROWN. We demonstrate by extensive experiments that our method outperforms\nstate-of-the-art lower-bound-based certification algorithms in terms of both\nbound quality and speed.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 18:57:43 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Boopathy", "Akhilan", ""], ["Weng", "Tsui-Wei", ""], ["Chen", "Pin-Yu", ""], ["Liu", "Sijia", ""], ["Daniel", "Luca", ""]]}, {"id": "1811.12402", "submitter": "Ke Li", "authors": "Ke Li and Jitendra Malik", "title": "On the Implicit Assumptions of GANs", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial nets (GANs) have generated a lot of excitement.\nDespite their popularity, they exhibit a number of well-documented issues in\npractice, which apparently contradict theoretical guarantees. A number of\nenlightening papers have pointed out that these issues arise from unjustified\nassumptions that are commonly made, but the message seems to have been lost\namid the optimism of recent years. We believe the identified problems deserve\nmore attention, and highlight the implications on both the properties of GANs\nand the trajectory of research on probabilistic models. We recently proposed an\nalternative method that sidesteps these problems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 18:59:55 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1811.12403", "submitter": "Lam Nguyen", "authors": "Lam M. Nguyen, Phuong Ha Nguyen, Peter Richt\\'arik, Katya Scheinberg,\n  Martin Tak\\'a\\v{c}, Marten van Dijk", "title": "New Convergence Aspects of Stochastic Gradient Algorithms", "comments": "Journal of Machine Learning Research. arXiv admin note: substantial\n  text overlap with arXiv:1802.03801", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical convergence analysis of SGD is carried out under the assumption\nthat the norm of the stochastic gradient is uniformly bounded. While this might\nhold for some loss functions, it is violated for cases where the objective\nfunction is strongly convex. In Bottou et al. (2018), a new analysis of\nconvergence of SGD is performed under the assumption that stochastic gradients\nare bounded with respect to the true gradient norm. We show that for stochastic\nproblems arising in machine learning such bound always holds; and we also\npropose an alternative convergence analysis of SGD with diminishing learning\nrate regime. We then move on to the asynchronous parallel setting, and prove\nconvergence of Hogwild! algorithm in the same regime in the case of diminished\nlearning rate. It is well-known that SGD converges if a sequence of learning\nrates $\\{\\eta_t\\}$ satisfies $\\sum_{t=0}^\\infty \\eta_t \\rightarrow \\infty$ and\n$\\sum_{t=0}^\\infty \\eta^2_t < \\infty$. We show the convergence of SGD for\nstrongly convex objective function without using bounded gradient assumption\nwhen $\\{\\eta_t\\}$ is a diminishing sequence and $\\sum_{t=0}^\\infty \\eta_t\n\\rightarrow \\infty$. In other words, we extend the current state-of-the-art\nclass of learning rates satisfying the convergence of SGD.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 03:55:25 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 02:27:11 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Nguyen", "Lam M.", ""], ["Nguyen", "Phuong Ha", ""], ["Richt\u00e1rik", "Peter", ""], ["Scheinberg", "Katya", ""], ["Tak\u00e1\u010d", "Martin", ""], ["van Dijk", "Marten", ""]]}, {"id": "1811.12408", "submitter": "Dorien Herremans", "authors": "Ching-Hua Chuan, Kat Agres, Dorien Herremans", "title": "From Context to Concept: Exploring Semantic Relationships in Music with\n  Word2Vec", "comments": "Accepted for publication in Neural Computing and Applications,\n  Springer. In Press", "journal-ref": "Neural Computing and Applications, Springer. 2019", "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the potential of a popular distributional semantics vector space\nmodel, word2vec, for capturing meaningful relationships in ecological (complex\npolyphonic) music. More precisely, the skip-gram version of word2vec is used to\nmodel slices of music from a large corpus spanning eight musical genres. In\nthis newly learned vector space, a metric based on cosine distance is able to\ndistinguish between functional chord relationships, as well as harmonic\nassociations in the music. Evidence, based on cosine distance between\nchord-pair vectors, suggests that an implicit circle-of-fifths exists in the\nvector space. In addition, a comparison between pieces in different keys\nreveals that key relationships are represented in word2vec space. These results\nsuggest that the newly learned embedded vector representation does in fact\ncapture tonal and harmonic characteristics of music, without receiving explicit\ninformation about the musical content of the constituent slices. In order to\ninvestigate whether proximity in the discovered space of embeddings is\nindicative of `semantically-related' slices, we explore a music generation\ntask, by automatically replacing existing slices from a given piece of music\nwith new slices. We propose an algorithm to find substitute slices based on\nspatial proximity and the pitch class distribution inferred in the chosen\nsubspace. The results indicate that the size of the subspace used has a\nsignificant effect on whether slices belonging to the same key are selected. In\nsum, the proposed word2vec model is able to learn music-vector embeddings that\ncapture meaningful tonal and harmonic relationships in music, thereby providing\na useful tool for exploring musical properties and comparisons across pieces,\nas a potential input representation for deep learning models, and as a music\ngeneration device.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 13:52:13 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Chuan", "Ching-Hua", ""], ["Agres", "Kat", ""], ["Herremans", "Dorien", ""]]}, {"id": "1811.12436", "submitter": "Jiaqi Jiang", "authors": "Jiaqi Jiang, David Sell, Stephan Hoyer, Jason Hickey, Jianji Yang, and\n  Jonathan A. Fan", "title": "Freeform Diffractive Metagrating Design Based on Generative Adversarial\n  Networks", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": "10.1021/acsnano.9b02371", "report-no": null, "categories": "physics.optics cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in metasurface design is the development of algorithms that\ncan effectively and efficiently produce high performance devices. Design\nmethods based on iterative optimization can push the performance limits of\nmetasurfaces, but they require extensive computational resources that limit\ntheir implementation to small numbers of microscale devices. We show that\ngenerative neural networks can train from images of periodic,\ntopology-optimized metagratings to produce high-efficiency, topologically\ncomplex devices operating over a broad range of deflection angles and\nwavelengths. Further iterative optimization of these designs yields devices\nwith enhanced robustness and efficiencies, and these devices can be utilized as\nadditional training data for network refinement. In this manner, generative\nnetworks can be trained, with a onetime computation cost, and used as a design\ntool to facilitate the production of near-optimal, topologically-complex device\ndesigns. We envision that such data-driven design methodologies can apply to\nother physical sciences domains that require the design of functional elements\noperating across a wide parameter space.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 19:14:13 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 02:09:17 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Jiang", "Jiaqi", ""], ["Sell", "David", ""], ["Hoyer", "Stephan", ""], ["Hickey", "Jason", ""], ["Yang", "Jianji", ""], ["Fan", "Jonathan A.", ""]]}, {"id": "1811.12444", "submitter": "Xian Yeow Lee", "authors": "Xian Yeow Lee, Aditya Balu, Daniel Stoecklein, Baskar\n  Ganapathysubramanian and Soumik Sarkar", "title": "Flow Shape Design for Microfluidic Devices Using Deep Reinforcement\n  Learning", "comments": "Neurips 2018 Deep RL workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microfluidic devices are utilized to control and direct flow behavior in a\nwide variety of applications, particularly in medical diagnostics. A\nparticularly popular form of microfluidics -- called inertial microfluidic flow\nsculpting -- involves placing a sequence of pillars to controllably deform an\ninitial flow field into a desired one. Inertial flow sculpting can be formally\ndefined as an inverse problem, where one identifies a sequence of pillars\n(chosen, with replacement, from a finite set of pillars, each of which produce\na specific transformation) whose composite transformation results in a\nuser-defined desired transformation. Endemic to most such problems in\nengineering, inverse problems are usually quite computationally intractable,\nwith most traditional approaches based on search and optimization strategies.\nIn this paper, we pose this inverse problem as a Reinforcement Learning (RL)\nproblem. We train a DoubleDQN agent to learn from this environment. The results\nsuggest that learning is possible using a DoubleDQN model with the success\nfrequency reaching 90% in 200,000 episodes and the rewards converging. While\nmost of the results are obtained by fixing a particular target flow shape to\nsimplify the learning problem, we later demonstrate how to transfer the\nlearning of an agent based on one target shape to another, i.e. from one design\nto another and thus be useful for a generic design of a flow shape.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 19:29:24 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Lee", "Xian Yeow", ""], ["Balu", "Aditya", ""], ["Stoecklein", "Daniel", ""], ["Ganapathysubramanian", "Baskar", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1811.12465", "submitter": "Danil Kuzin", "authors": "Danil Kuzin and Olga Isupova and Lyudmila Mihaylova", "title": "Uncertainty propagation in neural networks for sparse coding", "comments": "Presented at the third workshop on Bayesian Deep Learning (NeurIPS\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel method to propagate uncertainty through the soft-thresholding\nnonlinearity is proposed in this paper. At every layer the current distribution\nof the target vector is represented as a spike and slab distribution, which\nrepresents the probabilities of each variable being zero, or\nGaussian-distributed. Using the proposed method of uncertainty propagation, the\ngradients of the logarithms of normalisation constants are derived, that can be\nused to update a weight distribution. A novel Bayesian neural network for\nsparse coding is designed utilising both the proposed method of uncertainty\npropagation and Bayesian inference algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 20:16:03 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Kuzin", "Danil", ""], ["Isupova", "Olga", ""], ["Mihaylova", "Lyudmila", ""]]}, {"id": "1811.12469", "submitter": "Vitaly Feldman", "authors": "\\'Ulfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan,\n  Kunal Talwar, Abhradeep Thakurta", "title": "Amplification by Shuffling: From Local to Central Differential Privacy\n  via Anonymity", "comments": "Stated amplification bounds for epsilon > 1 explicitly and also\n  stated the bounds for for Renyi DP. Fixed an incorrect statement in one of\n  the proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensitive statistics are often collected across sets of users, with repeated\ncollection of reports done over time. For example, trends in users' private\npreferences or software usage may be monitored via such reports. We study the\ncollection of such statistics in the local differential privacy (LDP) model,\nand describe an algorithm whose privacy cost is polylogarithmic in the number\nof changes to a user's value.\n  More fundamentally---by building on anonymity of the users' reports---we also\ndemonstrate how the privacy cost of our LDP algorithm can actually be much\nlower when viewed in the central model of differential privacy. We show, via a\nnew and general privacy amplification technique, that any permutation-invariant\nalgorithm satisfying $\\varepsilon$-local differential privacy will satisfy\n$(O(\\varepsilon \\sqrt{\\log(1/\\delta)/n}), \\delta)$-central differential\nprivacy. By this, we explain how the high noise and $\\sqrt{n}$ overhead of LDP\nprotocols is a consequence of them being significantly more private in the\ncentral model. As a practical corollary, our results imply that several\nLDP-based industrial deployments may have much lower privacy cost than their\nadvertised $\\varepsilon$ would indicate---at least if reports are anonymized.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 20:24:45 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 01:37:51 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Erlingsson", "\u00dalfar", ""], ["Feldman", "Vitaly", ""], ["Mironov", "Ilya", ""], ["Raghunathan", "Ananth", ""], ["Talwar", "Kunal", ""], ["Thakurta", "Abhradeep", ""]]}, {"id": "1811.12470", "submitter": "Arjun Nitin Bhagoji", "authors": "Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, Seraphin\n  Calo", "title": "Analyzing Federated Learning through an Adversarial Lens", "comments": "Extended version of paper accepted to ICML 2019, code available at\n  https://github.com/inspire-group/ModelPoisoning; 19 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning distributes model training among a multitude of agents,\nwho, guided by privacy concerns, perform training using their local data but\nshare only model parameter updates, for iterative aggregation at the server. In\nthis work, we explore the threat of model poisoning attacks on federated\nlearning initiated by a single, non-colluding malicious agent where the\nadversarial objective is to cause the model to misclassify a set of chosen\ninputs with high confidence. We explore a number of strategies to carry out\nthis attack, starting with simple boosting of the malicious agent's update to\novercome the effects of other agents' updates. To increase attack stealth, we\npropose an alternating minimization strategy, which alternately optimizes for\nthe training loss and the adversarial objective. We follow up by using\nparameter estimation for the benign agents' updates to improve on attack\nsuccess. Finally, we use a suite of interpretability techniques to generate\nvisual explanations of model decisions for both benign and malicious models and\nshow that the explanations are nearly visually indistinguishable. Our results\nindicate that even a highly constrained adversary can carry out model poisoning\nattacks while simultaneously maintaining stealth, thus highlighting the\nvulnerability of the federated learning setting and the need to develop\neffective defense strategies.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 20:27:14 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 17:14:03 GMT"}, {"version": "v3", "created": "Sat, 2 Mar 2019 22:04:18 GMT"}, {"version": "v4", "created": "Mon, 25 Nov 2019 00:34:14 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Bhagoji", "Arjun Nitin", ""], ["Chakraborty", "Supriyo", ""], ["Mittal", "Prateek", ""], ["Calo", "Seraphin", ""]]}, {"id": "1811.12471", "submitter": "D\\\"om\\\"ot\\\"or P\\'alv\\\"olgyi", "authors": "D\\\"om\\\"ot\\\"or P\\'alv\\\"olgyi and G\\'abor Tardos", "title": "Unlabeled Compression Schemes Exceeding the VC-dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we disprove a conjecture of Kuzmin and Warmuth claiming that\nevery family whose VC-dimension is at most d admits an unlabeled compression\nscheme to a sample of size at most d. We also study the unlabeled compression\nschemes of the joins of some families and conjecture that these give a larger\ngap between the VC-dimension and the size of the smallest unlabeled compression\nscheme for them.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 20:28:36 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["P\u00e1lv\u00f6lgyi", "D\u00f6m\u00f6t\u00f6r", ""], ["Tardos", "G\u00e1bor", ""]]}, {"id": "1811.12488", "submitter": "Fahad Shamshad", "authors": "Fahad Shamshad, Muhammad Awais, Muhammad Asim, Zain ul Aabidin Lodhi,\n  Muhammad Umair, Ali Ahmed", "title": "Leveraging Deep Stein's Unbiased Risk Estimator for Unsupervised X-ray\n  Denoising", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/223", "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the plethora of techniques devised to curb the prevalence of noise in\nmedical images, deep learning based approaches have shown the most promise.\nHowever, one critical limitation of these deep learning based denoisers is the\nrequirement of high-quality noiseless ground truth images that are difficult to\nobtain in many medical imaging applications such as X-rays. To circumvent this\nissue, we leverage recently proposed approach of [7] that incorporates Stein's\nUnbiased Risk Estimator (SURE) to train a deep convolutional neural network\nwithout requiring denoised ground truth X-ray data. Our experimental results\ndemonstrate the effectiveness of SURE based approach for denoising X-ray\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 21:04:38 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Shamshad", "Fahad", ""], ["Awais", "Muhammad", ""], ["Asim", "Muhammad", ""], ["Lodhi", "Zain ul Aabidin", ""], ["Umair", "Muhammad", ""], ["Ahmed", "Ali", ""]]}, {"id": "1811.12495", "submitter": "Dushyant Mehta", "authors": "Dushyant Mehta, Kwang In Kim, Christian Theobalt", "title": "On Implicit Filter Level Sparsity in Convolutional Neural Networks", "comments": "Accepted at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate filter level sparsity that emerges in convolutional neural\nnetworks (CNNs) which employ Batch Normalization and ReLU activation, and are\ntrained with adaptive gradient descent techniques and L2 regularization or\nweight decay. We conduct an extensive experimental study casting our initial\nfindings into hypotheses and conclusions about the mechanisms underlying the\nemergent filter level sparsity. This study allows new insight into the\nperformance gap obeserved between adapative and non-adaptive gradient descent\nmethods in practice. Further, analysis of the effect of training strategies and\nhyperparameters on the sparsity leads to practical suggestions in designing CNN\ntraining strategies enabling us to explore the tradeoffs between feature\nselectivity, network capacity, and generalization performance. Lastly, we show\nthat the implicit sparsity can be harnessed for neural network speedup at par\nor better than explicit sparsification / pruning approaches, with no\nmodifications to the typical training pipeline required.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 21:29:31 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 15:40:40 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Mehta", "Dushyant", ""], ["Kim", "Kwang In", ""], ["Theobalt", "Christian", ""]]}, {"id": "1811.12500", "submitter": "Tiehang Duan", "authors": "Tiehang Duan, Qi Lou, Sargur N. Srihari, Xiaohui Xie", "title": "Sequential Embedding Induced Text Clustering, a Non-parametric Bayesian\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art nonparametric Bayesian text clustering methods model\ndocuments through multinomial distribution on bags of words. Although these\nmethods can effectively utilize the word burstiness representation of documents\nand achieve decent performance, they do not explore the sequential information\nof text and relationships among synonyms. In this paper, the documents are\nmodeled as the joint of bags of words, sequential features and word embeddings.\nWe proposed Sequential Embedding induced Dirichlet Process Mixture Model\n(SiDPMM) to effectively exploit this joint document representation in text\nclustering. The sequential features are extracted by the encoder-decoder\ncomponent. Word embeddings produced by the continuous-bag-of-words (CBOW) model\nare introduced to handle synonyms. Experimental results demonstrate the\nbenefits of our model in two major aspects: 1) improved performance across\nmultiple diverse text datasets in terms of the normalized mutual information\n(NMI); 2) more accurate inference of ground truth cluster numbers with\nregularization effect on tiny outlier clusters.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 21:39:44 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Duan", "Tiehang", ""], ["Lou", "Qi", ""], ["Srihari", "Sargur N.", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1811.12507", "submitter": "Bangalore Ravi Kiran", "authors": "Jean Serra, Jesus Angulo, B Ravi Kiran", "title": "Regression and Classification by Zonal Kriging", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a family $Z=\\{\\boldsymbol{x_{i}},y_{i}$,$1\\leq i\\leq N\\}$ of $N$\npairs of vectors $\\boldsymbol{x_{i}} \\in \\mathbb{R}^d$ and scalars $y_{i}$ that\nwe aim to predict for a new sample vector $\\mathbf{x}_0$. Kriging models $y$ as\na sum of a deterministic function $m$, a drift which depends on the point\n$\\boldsymbol{x}$, and a random function $z$ with zero mean. The zonality\nhypothesis interprets $y$ as a weighted sum of $d$ random functions of a single\nindependent variables, each of which is a kriging, with a quadratic form for\nthe variograms drift. We can therefore construct an unbiased estimator\n$y^{*}(\\boldsymbol{x_{0}})=\\sum_{i}\\lambda^{i}z(\\boldsymbol{x_{i}})$ de\n$y(\\boldsymbol{x_{0}})$ with minimal variance\n$E[y^{*}(\\boldsymbol{x_{0}})-y(\\boldsymbol{x_{0}})]^{2}$, with the help of the\nknown training set points. We give the explicitly closed form for $\\lambda^{i}$\nwithout having calculated the inverse of the matrices.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 22:00:26 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 17:57:57 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Serra", "Jean", ""], ["Angulo", "Jesus", ""], ["Kiran", "B Ravi", ""]]}, {"id": "1811.12520", "submitter": "Eli Sherman", "authors": "Eli Sherman, Hitinder Gurm, Ulysses Balis, Scott Owens, Jenna Wiens", "title": "Leveraging Clinical Time-Series Data for Prediction: A Cautionary Tale", "comments": "In Proceedings of American Medical Informatics Annual Symposium 2017\n  PMID: 29854227", "journal-ref": "AMIA Annu Symp Proc. 2018 Apr 16;2017:1571-1580. eCollection 2017", "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In healthcare, patient risk stratification models are often learned using\ntime-series data extracted from electronic health records. When extracting data\nfor a clinical prediction task, several formulations exist, depending on how\none chooses the time of prediction and the prediction horizon. In this paper,\nwe show how the formulation can greatly impact both model performance and\nclinical utility. Leveraging a publicly available ICU dataset, we consider two\nclinical prediction tasks: in-hospital mortality, and hypokalemia. Through\nthese case studies, we demonstrate the necessity of evaluating models using an\noutcome-independent reference point, since choosing the time of prediction\nrelative to the event can result in unrealistic performance. Further, an\noutcome-independent scheme outperforms an outcome-dependent scheme on both\ntasks (In-Hospital Mortality AUROC .882 vs. .831; Serum Potassium: AUROC .829\nvs. .740) when evaluated on test sets that mimic real-world use.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 22:43:23 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Sherman", "Eli", ""], ["Gurm", "Hitinder", ""], ["Balis", "Ulysses", ""], ["Owens", "Scott", ""], ["Wiens", "Jenna", ""]]}, {"id": "1811.12530", "submitter": "Anurag Koul", "authors": "Anurag Koul, Sam Greydanus, Alan Fern", "title": "Learning Finite State Representations of Recurrent Policy Networks", "comments": "Preprint. Under review at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are an effective representation of control\npolicies for a wide range of reinforcement and imitation learning problems. RNN\npolicies, however, are particularly difficult to explain, understand, and\nanalyze due to their use of continuous-valued memory vectors and observation\nfeatures. In this paper, we introduce a new technique, Quantized Bottleneck\nInsertion, to learn finite representations of these vectors and features. The\nresult is a quantized representation of the RNN that can be analyzed to improve\nour understanding of memory use and general behavior. We present results of\nthis approach on synthetic environments and six Atari games. The resulting\nfinite representations are surprisingly small in some cases, using as few as 3\ndiscrete memory states and 10 observations for a perfect Pong policy. We also\nshow that these finite policy representations lead to improved\ninterpretability.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 23:07:59 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Koul", "Anurag", ""], ["Greydanus", "Sam", ""], ["Fern", "Alan", ""]]}, {"id": "1811.12535", "submitter": "Jiaming Zeng", "authors": "Jiaming Zeng, Adam Lesnikowski, Jose M. Alvarez", "title": "The Relevance of Bayesian Layer Positioning to Model Uncertainty in Deep\n  Bayesian Active Learning", "comments": null, "journal-ref": "Third workshop on Bayesian Deep Learning (NeurIPS 2018)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges of deep learning tools is their inability to\ncapture model uncertainty. While Bayesian deep learning can be used to tackle\nthe problem, Bayesian neural networks often require more time and computational\npower to train than deterministic networks. Our work explores whether fully\nBayesian networks are needed to successfully capture model uncertainty. We vary\nthe number and position of Bayesian layers in a network and compare their\nperformance on active learning with the MNIST dataset. We found that we can\nfully capture the model uncertainty by using only a few Bayesian layers near\nthe output of the network, combining the advantages of deterministic and\nBayesian networks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 23:36:25 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zeng", "Jiaming", ""], ["Lesnikowski", "Adam", ""], ["Alvarez", "Jose M.", ""]]}, {"id": "1811.12539", "submitter": "Zhehan Yi", "authors": "Jiajun Duan, Zhehan Yi, Di Shi, Hao Xu, Zhiwei Wang", "title": "A Neural-Network-Based Optimal Control of Ultra-Capacitors with System\n  Uncertainties", "comments": "IEEE ISGT NA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a neural-network (NN)-based online optimal control method\n(NN-OPT) is proposed for ultra-capacitors (UCs) energy storage system (ESS) in\nhybrid AC/DC microgrids involving multiple distributed generations (e.g.,\nPhotovoltaic (PV) system, battery storage, diesel generator). Conventional\ncontrol strategies usually produce large disturbances to buses during charging\nand discharging (C&D) processes of UCs, which significantly degrades the power\nquality and system performance, especially under fast C&D modes. Therefore, the\noptimal control theory is adopted to optimize the C&D profile as well as to\nsuppress the disturbances caused by UCs implementation. Specifically, an\nNN-based intelligent algorithm is developed to learn the optimal control policy\nfor bidirectional-converter-interfaced UCs. The inaccuracies of system modeling\nare also considered in the control design. Since the designed NN-OPT method is\ndecentralized that only requires the local measurements, plug & play of UCs can\nbe easily realized with minimal communication efforts. In addition, the PV\nsystem is under the maximum power point tracking (MPPT) control to extract the\nmaximum benefit. Both islanded and grid-tied modes are considered during the\ncontroller design. Extensive case studies have been conducted to evaluate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 23:47:39 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 01:35:46 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Duan", "Jiajun", ""], ["Yi", "Zhehan", ""], ["Shi", "Di", ""], ["Xu", "Hao", ""], ["Wang", "Zhiwei", ""]]}, {"id": "1811.12541", "submitter": "Zhehan Yi", "authors": "Yao Cui, Zhehan Yi, Jiajun Duan, Di Shi, Zhiwei Wang", "title": "A Rprop-Neural-Network-Based PV Maximum Power Point Tracking Algorithm\n  with Short-Circuit Current Limitation", "comments": "2019 IEEE ISGT NA", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a resilient-backpropagation-neural-network-(Rprop-NN)\nbased algorithm for Photovoltaic (PV) maximum power point tracking (MPPT). A\nsupervision mechanism is proposed to calibrate the Rprop-NN-MPPT reference and\nlimit short-circuit current caused by incorrect prediction. Conventional MPPT\nalgorithms (e.g., perturb and observe (P&O), hill climbing, and incremental\nconductance (Inc-Cond) etc.) are trial-and-error-based, which may result in\nsteady-state oscillations and loss of tracking direction under fast-changing\nambient environment. In addition, partial shading is also a challenge due to\nthe difficulty of finding the global maximum power point on a multi-peak\ncharacteristic curve. As an attempt to address the aforementioned issues, a\nnovel Rprop-NN MPPT algorithm is developed and elaborated in this work.\nMultiple case studies are carried out to verify the effectiveness of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 23:48:03 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 18:25:12 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Cui", "Yao", ""], ["Yi", "Zhehan", ""], ["Duan", "Jiajun", ""], ["Shi", "Di", ""], ["Wang", "Zhiwei", ""]]}, {"id": "1811.12556", "submitter": "Dhaval Adjodah", "authors": "Dhaval Adjodah, Dan Calacci, Abhimanyu Dubey, Peter Krafft, Esteban\n  Moro, Alex `Sandy' Pentland", "title": "How to Organize your Deep Reinforcement Learning Agents: The Importance\n  of Communication Topology", "comments": "please refer to arXiv:1902.06740 for updated paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this empirical paper, we investigate how learning agents can be arranged\nin more efficient communication topologies for improved learning. This is an\nimportant problem because a common technique to improve speed and robustness of\nlearning in deep reinforcement learning and many other machine learning\nalgorithms is to run multiple learning agents in parallel. The standard\ncommunication architecture typically involves all agents intermittently\ncommunicating with each other (fully connected topology) or with a centralized\nserver (star topology). Unfortunately, optimizing the topology of communication\nover the space of all possible graphs is a hard problem, so we borrow results\nfrom the networked optimization and collective intelligence literatures which\nsuggest that certain families of network topologies can lead to strong\nimprovements over fully-connected networks. We start by introducing alternative\nnetwork topologies to DRL benchmark tasks under the Evolution Strategies\nparadigm which we call Network Evolution Strategies. We explore the relative\nperformance of the four main graph families and observe that one such family\n(Erdos-Renyi random graphs) empirically outperforms all other families,\nincluding the de facto fully-connected communication topologies. Additionally,\nthe use of alternative network topologies has a multiplicative performance\neffect: we observe that when 1000 learning agents are arranged in a carefully\ndesigned communication topology, they can compete with 3000 agents arranged in\nthe de facto fully-connected topology. Overall, our work suggests that\ndistributed machine learning algorithms would learn more efficiently if the\ncommunication topology between learning agents was optimized.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 00:36:34 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2019 21:10:11 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Adjodah", "Dhaval", ""], ["Calacci", "Dan", ""], ["Dubey", "Abhimanyu", ""], ["Krafft", "Peter", ""], ["Moro", "Esteban", ""], ["Pentland", "Alex `Sandy'", ""]]}, {"id": "1811.12557", "submitter": "Tegg Sung", "authors": "Aleksandra Malysheva, Tegg Taekyong Sung, Chae-Bong Sohn, Daniel\n  Kudenko, Aleksei Shpilman", "title": "Deep Multi-Agent Reinforcement Learning with Relevance Graphs", "comments": "The first two authors contributed equally. Author ordering determined\n  by coin flip over a Google Hangout. Accepted at NIPS 2018 Deep RL Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over recent years, deep reinforcement learning has shown strong successes in\ncomplex single-agent tasks, and more recently this approach has also been\napplied to multi-agent domains. In this paper, we propose a novel approach,\ncalled MAGnet, to multi-agent reinforcement learning (MARL) that utilizes a\nrelevance graph representation of the environment obtained by a self-attention\nmechanism, and a message-generation technique inspired by the NerveNet\narchitecture. We applied our MAGnet approach to the Pommerman game and the\nresults show that it significantly outperforms state-of-the-art MARL solutions,\nincluding DQN, MADDPG, and MCTS.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 00:38:18 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Malysheva", "Aleksandra", ""], ["Sung", "Tegg Taekyong", ""], ["Sohn", "Chae-Bong", ""], ["Kudenko", "Daniel", ""], ["Shpilman", "Aleksei", ""]]}, {"id": "1811.12560", "submitter": "Vincent Francois-Lavet", "authors": "Vincent Francois-Lavet, Peter Henderson, Riashat Islam, Marc G.\n  Bellemare, Joelle Pineau", "title": "An Introduction to Deep Reinforcement Learning", "comments": null, "journal-ref": "Foundations and Trends in Machine Learning: Vol. 11, No. 3-4, 2018", "doi": "10.1561/2200000071", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning is the combination of reinforcement learning (RL)\nand deep learning. This field of research has been able to solve a wide range\nof complex decision-making tasks that were previously out of reach for a\nmachine. Thus, deep RL opens up many new applications in domains such as\nhealthcare, robotics, smart grids, finance, and many more. This manuscript\nprovides an introduction to deep reinforcement learning models, algorithms and\ntechniques. Particular focus is on the aspects related to generalization and\nhow deep RL can be used for practical applications. We assume the reader is\nfamiliar with basic machine learning concepts.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 00:57:30 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 09:10:53 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Francois-Lavet", "Vincent", ""], ["Henderson", "Peter", ""], ["Islam", "Riashat", ""], ["Bellemare", "Marc G.", ""], ["Pineau", "Joelle", ""]]}, {"id": "1811.12565", "submitter": "Juhan Bae", "authors": "Juhan Bae, Guodong Zhang, Roger Grosse", "title": "Eigenvalue Corrected Noisy Natural Gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayesian neural networks combine the flexibility of deep learning\nwith Bayesian uncertainty estimation. However, inference procedures for\nflexible variational posteriors are computationally expensive. A recently\nproposed method, noisy natural gradient, is a surprisingly simple method to fit\nexpressive posteriors by adding weight noise to regular natural gradient\nupdates. Noisy K-FAC is an instance of noisy natural gradient that fits a\nmatrix-variate Gaussian posterior with minor changes to ordinary K-FAC.\nNevertheless, a matrix-variate Gaussian posterior does not capture an accurate\ndiagonal variance. In this work, we extend on noisy K-FAC to obtain a more\nflexible posterior distribution called eigenvalue corrected matrix-variate\nGaussian. The proposed method computes the full diagonal re-scaling factor in\nKronecker-factored eigenbasis. Empirically, our approach consistently\noutperforms existing algorithms (e.g., noisy K-FAC) on regression and\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 01:12:51 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Bae", "Juhan", ""], ["Zhang", "Guodong", ""], ["Grosse", "Roger", ""]]}, {"id": "1811.12569", "submitter": "Kailas Vodrahalli", "authors": "Kailas Vodrahalli, Ke Li, Jitendra Malik", "title": "Are All Training Examples Created Equal? An Empirical Study", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern computer vision algorithms often rely on very large training datasets.\nHowever, it is conceivable that a carefully selected subsample of the dataset\nis sufficient for training. In this paper, we propose a gradient-based\nimportance measure that we use to empirically analyze relative importance of\ntraining images in four datasets of varying complexity. We find that in some\ncases, a small subsample is indeed sufficient for training. For other datasets,\nhowever, the relative differences in importance are negligible. These results\nhave important implications for active learning on deep networks. Additionally,\nour analysis method can be used as a general tool to better understand\ndiversity of training examples in datasets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 01:16:42 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Vodrahalli", "Kailas", ""], ["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1811.12583", "submitter": "Bret Nestor", "authors": "Bret Nestor, Matthew B. A. McDermott, Geeticka Chauhan, Tristan\n  Naumann, Michael C. Hughes, Anna Goldenberg, Marzyeh Ghassemi", "title": "Rethinking clinical prediction: Why machine learning must consider year\n  of care and feature aggregation", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/189", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning for healthcare often trains models on de-identified datasets\nwith randomly-shifted calendar dates, ignoring the fact that data were\ngenerated under hospital operation practices that change over time. These\nchanging practices induce definitive changes in observed data which confound\nevaluations which do not account for dates and limit the generalisability of\ndate-agnostic models. In this work, we establish the magnitude of this problem\non MIMIC, a public hospital dataset, and showcase a simple solution. We augment\nMIMIC with the year in which care was provided and show that a model trained\nusing standard feature representations will significantly degrade in quality\nover time. We find a deterioration of 0.3 AUC when evaluating mortality\nprediction on data from 10 years later. We find a similar deterioration of 0.15\nAUC for length-of-stay. In contrast, we demonstrate that clinically-oriented\naggregates of raw features significantly mitigate future deterioration. Our\nsuggested aggregated representations, when retrained yearly, have prediction\nquality comparable to year-agnostic models.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 02:30:10 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Nestor", "Bret", ""], ["McDermott", "Matthew B. A.", ""], ["Chauhan", "Geeticka", ""], ["Naumann", "Tristan", ""], ["Hughes", "Michael C.", ""], ["Goldenberg", "Anna", ""], ["Ghassemi", "Marzyeh", ""]]}, {"id": "1811.12587", "submitter": "Muneki Yasuda", "authors": "Yuuki Yokoyama, Tomu Katsumata, Muneki Yasuda", "title": "Restricted Boltzmann Machine with Multivalued Hidden Variables: a model\n  suppressing over-fitting", "comments": null, "journal-ref": "The Review of Socionetwork Strategies, Vol.13, no.2, pp.253-266,\n  2019", "doi": "10.1007/s12626-019-00042-4", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization is one of the most important issues in machine learning\nproblems. In this study, we consider generalization in restricted Boltzmann\nmachines (RBMs). We propose an RBM with multivalued hidden variables, which is\na simple extension of conventional RBMs. We demonstrate that the proposed model\nis better than the conventional model via numerical experiments for contrastive\ndivergence learning with artificial data and a classification problem with\nMNIST.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 02:40:40 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 03:27:26 GMT"}, {"version": "v3", "created": "Mon, 24 Jun 2019 13:22:58 GMT"}, {"version": "v4", "created": "Wed, 8 Jan 2020 07:32:04 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Yokoyama", "Yuuki", ""], ["Katsumata", "Tomu", ""], ["Yasuda", "Muneki", ""]]}, {"id": "1811.12589", "submitter": "Benjamin Glicksberg", "authors": "Beau Norgeot, Dmytro Lituiev, Benjamin S. Glicksberg, Atul J. Butte", "title": "Time Aggregation and Model Interpretation for Deep Multivariate\n  Longitudinal Patient Outcome Forecasting Systems in Chronic Ambulatory Care", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/121", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical data for ambulatory care, which accounts for 90% of the nations\nhealthcare spending, is characterized by relatively small sample sizes of\nlongitudinal data, unequal spacing between visits for each patient, with\nunequal numbers of data points collected across patients. While deep learning\nhas become state-of-the-art for sequence modeling, it is unknown which methods\nof time aggregation may be best suited for these challenging temporal use\ncases. Additionally, deep models are often considered uninterpretable by\nphysicians which may prevent the clinical adoption, even of well performing\nmodels. We show that time-distributed-dense layers combined with GRUs produce\nthe most generalizable models. Furthermore, we provide a framework for the\nclinical interpretation of the models.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 02:43:33 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Norgeot", "Beau", ""], ["Lituiev", "Dmytro", ""], ["Glicksberg", "Benjamin S.", ""], ["Butte", "Atul J.", ""]]}, {"id": "1811.12591", "submitter": "Yuheng Bu", "authors": "Yuheng Bu, Kevin Small", "title": "Active Learning in Recommendation Systems with Multi-level User\n  Preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recommendation systems generally observe user behavior passively, there\nhas been an increased interest in directly querying users to learn their\nspecific preferences. In such settings, considering queries at different levels\nof granularity to optimize user information acquisition is crucial to\nefficiently providing a good user experience. In this work, we study the active\nlearning problem with multi-level user preferences within the collective matrix\nfactorization (CMF) framework. CMF jointly captures multi-level user\npreferences with respect to items and relations between items (e.g., book\ngenre, cuisine type), generally resulting in improved predictions. Motivated by\nfinite-sample analysis of the CMF model, we propose a theoretically optimal\nactive learning strategy based on the Fisher information matrix and use this to\nderive a realizable approximation algorithm for practical recommendations.\nExperiments are conducted using both the Yelp dataset directly and an\nillustrative synthetic dataset in the three settings of personalized active\nlearning, cold-start recommendations, and noisy data -- demonstrating strong\nimprovements over several widely used active learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 03:05:51 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Bu", "Yuheng", ""], ["Small", "Kevin", ""]]}, {"id": "1811.12601", "submitter": "Angus Galloway", "authors": "Angus Galloway and Anna Golubeva and Graham W. Taylor", "title": "Adversarial Examples as an Input-Fault Tolerance Problem", "comments": "NIPS 2018 Workshop on Security and Machine Learning. Source available\n  at https://github.com/uoguelph-mlrg/nips18-secml-advex-input-fault", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the adversarial examples problem in terms of a model's fault\ntolerance with respect to its input. Whereas previous work focuses on\narbitrarily strict threat models, i.e., $\\epsilon$-perturbations, we consider\narbitrary valid inputs and propose an information-based characteristic for\nevaluating tolerance to diverse input faults.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 03:32:44 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Galloway", "Angus", ""], ["Golubeva", "Anna", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1811.12615", "submitter": "Kangcheng Lin", "authors": "Chaofan Chen, Kangcheng Lin, Cynthia Rudin, Yaron Shaposhnik, Sijia\n  Wang, Tong Wang", "title": "An Interpretable Model with Globally Consistent Explanations for Credit\n  Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a possible solution to a public challenge posed by the Fair Isaac\nCorporation (FICO), which is to provide an explainable model for credit risk\nassessment. Rather than present a black box model and explain it afterwards, we\nprovide a globally interpretable model that is as accurate as other neural\nnetworks. Our \"two-layer additive risk model\" is decomposable into subscales,\nwhere each node in the second layer represents a meaningful subscale, and all\nof the nonlinearities are transparent. We provide three types of explanations\nthat are simpler than, but consistent with, the global model. One of these\nexplanation methods involves solving a minimum set cover problem to find\nhigh-support globally-consistent explanations. We present a new online\nvisualization tool to allow users to explore the global model and its\nexplanations.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 04:59:00 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Chen", "Chaofan", ""], ["Lin", "Kangcheng", ""], ["Rudin", "Cynthia", ""], ["Shaposhnik", "Yaron", ""], ["Wang", "Sijia", ""], ["Wang", "Tong", ""]]}, {"id": "1811.12624", "submitter": "Elham J. Barezi Ms", "authors": "Elham J. Barezi, Pascale Fung", "title": "Modality-based Factorization for Multimodal Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method, Modality-based Redundancy Reduction Fusion (MRRF),\nfor understanding and modulating the relative contribution of each modality in\nmultimodal inference tasks. This is achieved by obtaining an $(M+1)$-way tensor\nto consider the high-order relationships between $M$ modalities and the output\nlayer of a neural network model. Applying a modality-based tensor factorization\nmethod, which adopts different factors for different modalities, results in\nremoving information present in a modality that can be compensated by other\nmodalities, with respect to model outputs. This helps to understand the\nrelative utility of information in each modality. In addition it leads to a\nless complicated model with less parameters and therefore could be applied as a\nregularizer avoiding overfitting. We have applied this method to three\ndifferent multimodal datasets in sentiment analysis, personality trait\nrecognition, and emotion recognition. We are able to recognize relationships\nand relative importance of different modalities in these tasks and achieves a\n1\\% to 4\\% improvement on several evaluation measures compared to the\nstate-of-the-art for all three tasks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 05:43:10 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 04:15:48 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Barezi", "Elham J.", ""], ["Fung", "Pascale", ""]]}, {"id": "1811.12627", "submitter": "Hyungu Kahng", "authors": "Hyungu Kahng, Yonghyun Jeong, Yoon Sang Cho, Gonie Ahn, Young Joon\n  Park, Uk Jo, Hankyu Lee, Hyungrok Do, Junseung Lee, Hyunjin Choi, Iljoo Yoon,\n  Hyunjae Lee, Daehun Jun, Changhyeon Bae, Seoung Bum Kim", "title": "Clear the Fog: Combat Value Assessment in Incomplete Information Games\n  with Convolutional Encoder-Decoders", "comments": "7 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  StarCraft, one of the most popular real-time strategy games, is a compelling\nenvironment for artificial intelligence research for both micro-level unit\ncontrol and macro-level strategic decision making. In this study, we address an\neminent problem concerning macro-level decision making, known as the\n'fog-of-war', which rises naturally from the fact that information regarding\nthe opponent's state is always provided in the incomplete form. For intelligent\nagents to play like human players, it is obvious that making accurate\npredictions of the opponent's status under incomplete information will increase\nits chance of winning. To reflect this fact, we propose a convolutional\nencoder-decoder architecture that predicts potential counts and locations of\nthe opponent's units based on only partially visible and noisy information. To\nevaluate the performance of our proposed method, we train an additional\nclassifier on the encoder-decoder output to predict the game outcome (win or\nlose). Finally, we designed an agent incorporating the proposed method and\nconducted simulation games against rule-based agents to demonstrate both\neffectiveness and practicality. All experiments were conducted on actual game\nreplay data acquired from professional players.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 06:02:09 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 09:42:35 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Kahng", "Hyungu", ""], ["Jeong", "Yonghyun", ""], ["Cho", "Yoon Sang", ""], ["Ahn", "Gonie", ""], ["Park", "Young Joon", ""], ["Jo", "Uk", ""], ["Lee", "Hankyu", ""], ["Do", "Hyungrok", ""], ["Lee", "Junseung", ""], ["Choi", "Hyunjin", ""], ["Yoon", "Iljoo", ""], ["Lee", "Hyunjae", ""], ["Jun", "Daehun", ""], ["Bae", "Changhyeon", ""], ["Kim", "Seoung Bum", ""]]}, {"id": "1811.12629", "submitter": "Dianbo Liu Dr", "authors": "Li Huang, Yifeng Yin, Zeng Fu, Shifa Zhang, Hao Deng, Dianbo Liu", "title": "LoAdaBoost: loss-based AdaBoost federated machine learning with reduced\n  computational complexity on IID and non-IID intensive care data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intensive care data are valuable for improvement of health care, policy\nmaking and many other purposes. Vast amount of such data are stored in\ndifferent locations, on many different devices and in different data silos.\nSharing data among different sources is a big challenge due to regulatory,\noperational and security reasons. One potential solution is federated machine\nlearning, which is a method that sends machine learning algorithms\nsimultaneously to all data sources, trains models in each source and aggregates\nthe learned models. This strategy allows utilization of valuable data without\nmoving them. One challenge in applying federated machine learning is the\npossibly different distributions of data from diverse sources. To tackle this\nproblem, we proposed an adaptive boosting method named LoAdaBoost that\nincreases the efficiency of federated machine learning. Using intensive care\nunit data from hospitals, we investigated the performance of learning in IID\nand non-IID data distribution scenarios, and showed that the proposed\nLoAdaBoost method achieved higher predictive accuracy with lower computational\ncomplexity than the baseline method.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 06:05:21 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 16:39:50 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 17:17:27 GMT"}, {"version": "v4", "created": "Wed, 12 Aug 2020 06:14:13 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Huang", "Li", ""], ["Yin", "Yifeng", ""], ["Fu", "Zeng", ""], ["Zhang", "Shifa", ""], ["Deng", "Hao", ""], ["Liu", "Dianbo", ""]]}, {"id": "1811.12634", "submitter": "Sooyeon Lee", "authors": "Sooyeon Lee, Huy Kang Kim", "title": "ADSaS: Comprehensive Real-time Anomaly Detection System", "comments": "6 pages, 4 figures, In Proceedings of the 19th World Conference on\n  Information Security and Applications (WISA) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since with massive data growth, the need for autonomous and generic anomaly\ndetection system is increased. However, developing one stand-alone generic\nanomaly detection system that is accurate and fast is still a challenge. In\nthis paper, we propose conventional time-series analysis approaches, the\nSeasonal Autoregressive Integrated Moving Average (SARIMA) model and Seasonal\nTrend decomposition using Loess (STL), to detect complex and various anomalies.\nUsually, SARIMA and STL are used only for stationary and periodic time-series,\nbut by combining, we show they can detect anomalies with high accuracy for data\nthat is even noisy and non-periodic. We compared the algorithm to Long Short\nTerm Memory (LSTM), a deep-learning-based algorithm used for anomaly detection\nsystem. We used a total of seven real-world datasets and four artificial\ndatasets with different time-series properties to verify the performance of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 06:27:44 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Lee", "Sooyeon", ""], ["Kim", "Huy Kang", ""]]}, {"id": "1811.12640", "submitter": "Sudeshna Roy", "authors": "Sudeshna Roy, Meghana Madhyastha, Sheril Lawrence, Vaibhav Rajan", "title": "Inferring Concept Prerequisite Relations from Online Educational\n  Resources", "comments": "Accepted at the AAAI Conference on Innovative Applications of\n  Artificial Intelligence (IAAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet has rich and rapidly increasing sources of high quality\neducational content. Inferring prerequisite relations between educational\nconcepts is required for modern large-scale online educational technology\napplications such as personalized recommendations and automatic curriculum\ncreation. We present PREREQ, a new supervised learning method for inferring\nconcept prerequisite relations. PREREQ is designed using latent representations\nof concepts obtained from the Pairwise Latent Dirichlet Allocation model, and a\nneural network based on the Siamese network architecture. PREREQ can learn\nunknown concept prerequisites from course prerequisites and labeled concept\nprerequisite data. It outperforms state-of-the-art approaches on benchmark\ndatasets and can effectively learn from very less training data. PREREQ can\nalso use unlabeled video playlists, a steadily growing source of training data,\nto learn concept prerequisites, thus obviating the need for manual annotation\nof course prerequisites.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 06:55:20 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 00:39:00 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Roy", "Sudeshna", ""], ["Madhyastha", "Meghana", ""], ["Lawrence", "Sheril", ""], ["Rajan", "Vaibhav", ""]]}, {"id": "1811.12642", "submitter": "Tomasz Rutkowski", "authors": "Tomasz M. Rutkowski, Qibin Zhao, Masao S. Abe, Mihoko Otake", "title": "AI Neurotechnology for Aging Societies -- Task-load and Dementia EEG\n  Digital Biomarker Development Using Information Geometry Machine Learning\n  Methods", "comments": "5 pages, 2 figures, NeurIPS 2018 AI for Social Good Workshop at the\n  Neural Information Processing Systems (NeurIPS = formerly NIPS) 2018.\n  Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dementia and especially Alzheimer's disease (AD) are the most common causes\nof cognitive decline in elderly people. A spread of the above mentioned mental\nhealth problems in aging societies is causing a significant medical and\neconomic burden in many countries around the world. According to a recent World\nHealth Organization (WHO) report, it is approximated that currently, worldwide,\nabout 47 million people live with a dementia spectrum of neurocognitive\ndisorders. This number is expected to triple by 2050, which calls for possible\napplication of AI-based technologies to support an early screening for\npreventive interventions and a subsequent mental wellbeing monitoring as well\nas maintenance with so-called digital-pharma or beyond a pill therapeutical\napproaches. This paper discusses our attempt and preliminary results of\nbrainwave (EEG) techniques to develop digital biomarkers for dementia progress\ndetection and monitoring. We present an information geometry-based\nclassification approach for automatic EEG-derived event related responses\n(ERPs) discrimination of low versus high task-load auditory or tactile stimuli\nrecognition, of which amplitude and latency variabilities are similar to those\nin dementia. The discussed approach is a step forward to develop AI, and\nespecially machine learning (ML) approaches, for the subsequent application to\nmild-cognitive impairment (MCI) and AD diagnostics.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 06:58:16 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Rutkowski", "Tomasz M.", ""], ["Zhao", "Qibin", ""], ["Abe", "Masao S.", ""], ["Otake", "Mihoko", ""]]}, {"id": "1811.12670", "submitter": "Ziwei Liu", "authors": "Weidong Yin, Ziwei Liu, Chen Change Loy", "title": "Instance-level Facial Attributes Transfer with Geometry-Aware Flow", "comments": "To appear in AAAI 2019. Code and models are available at:\n  https://github.com/wdyin/GeoGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of instance-level facial attribute transfer without\npaired training data, e.g. faithfully transferring the exact mustache from a\nsource face to a target face. This is a more challenging task than the\nconventional semantic-level attribute transfer, which only preserves the\ngeneric attribute style instead of instance-level traits. We propose the use of\ngeometry-aware flow, which serves as a well-suited representation for modeling\nthe transformation between instance-level facial attributes. Specifically, we\nleverage the facial landmarks as the geometric guidance to learn the\ndifferentiable flows automatically, despite of the large pose gap existed.\nGeometry-aware flow is able to warp the source face attribute into the target\nface context and generate a warp-and-blend result. To compensate for the\npotential appearance gap between source and target faces, we propose a\nhallucination sub-network that produces an appearance residual to further\nrefine the warp-and-blend result. Finally, a cycle-consistency framework\nconsisting of both attribute transfer module and attribute removal module is\ndesigned, so that abundant unpaired face images can be used as training data.\nExtensive evaluations validate the capability of our approach in transferring\ninstance-level facial attributes faithfully across large pose and appearance\ngaps. Thanks to the flow representation, our approach can readily be applied to\ngenerate realistic details on high-resolution images.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 08:43:00 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Yin", "Weidong", ""], ["Liu", "Ziwei", ""], ["Loy", "Chen Change", ""]]}, {"id": "1811.12693", "submitter": "Oliver Joseph David Barrowclough", "authors": "Konstantinos Gavriil, Georg Muntingh and Oliver J. D. Barrowclough", "title": "Void Filling of Digital Elevation Models with Deep Generative Models", "comments": "5 pages; 4 figures; corrected names in references; clarifications\n  regarding the two generators in the paper; added reference (Borji 2018) on\n  GAN evaluation measures; extended future work discussion; changed (Fig. 4.f)\n  to show a failure case", "journal-ref": null, "doi": "10.1109/LGRS.2019.2902222", "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, advances in machine learning algorithms, cheap computational\nresources, and the availability of big data have spurred the deep learning\nrevolution in various application domains. In particular, supervised learning\ntechniques in image analysis have led to superhuman performance in various\ntasks, such as classification, localization, and segmentation, while\nunsupervised learning techniques based on increasingly advanced generative\nmodels have been applied to generate high-resolution synthetic images\nindistinguishable from real images.\n  In this paper we consider a state-of-the-art machine learning model for image\ninpainting, namely a Wasserstein Generative Adversarial Network based on a\nfully convolutional architecture with a contextual attention mechanism. We show\nthat this model can successfully be transferred to the setting of digital\nelevation models (DEMs) for the purpose of generating semantically plausible\ndata for filling voids. Training, testing and experimentation is done on\nGeoTIFF data from various regions in Norway, made openly available by the\nNorwegian Mapping Authority.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 10:04:30 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 13:24:22 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Gavriil", "Konstantinos", ""], ["Muntingh", "Georg", ""], ["Barrowclough", "Oliver J. D.", ""]]}, {"id": "1811.12739", "submitter": "Yedid Hoshen", "authors": "Tavi Halperin and Ariel Ephrat and Yedid Hoshen", "title": "Neural separation of observed and unobserved distributions", "comments": "ICML'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Separating mixed distributions is a long standing challenge for machine\nlearning and signal processing. Most current methods either rely on making\nstrong assumptions on the source distributions or rely on having training\nsamples of each source in the mixture. In this work, we introduce a new\nmethod---Neural Egg Separation---to tackle the scenario of extracting a signal\nfrom an unobserved distribution additively mixed with a signal from an observed\ndistribution. Our method iteratively learns to separate the known distribution\nfrom progressively finer estimates of the unknown distribution. In some\nsettings, Neural Egg Separation is initialization sensitive, we therefore\nintroduce Latent Mixture Masking which ensures a good initialization. Extensive\nexperiments on audio and image separation tasks show that our method\noutperforms current methods that use the same level of supervision, and often\nachieves similar performance to full supervision.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 11:38:54 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 12:17:47 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Halperin", "Tavi", ""], ["Ephrat", "Ariel", ""], ["Hoshen", "Yedid", ""]]}, {"id": "1811.12752", "submitter": "Debarghya Ghoshdastidar", "authors": "Debarghya Ghoshdastidar, Ulrike von Luxburg", "title": "Practical methods for graph two-sample testing", "comments": "To appear in Neural Information Processing Systems 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis testing for graphs has been an important tool in applied research\nfields for more than two decades, and still remains a challenging problem as\none often needs to draw inference from few replicates of large graphs. Recent\nstudies in statistics and learning theory have provided some theoretical\ninsights about such high-dimensional graph testing problems, but the\npracticality of the developed theoretical methods remains an open question.\n  In this paper, we consider the problem of two-sample testing of large graphs.\nWe demonstrate the practical merits and limitations of existing theoretical\ntests and their bootstrapped variants. We also propose two new tests based on\nasymptotic distributions. We show that these tests are computationally less\nexpensive and, in some cases, more reliable than the existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 12:04:33 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Ghoshdastidar", "Debarghya", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1811.12776", "submitter": "Nikit Begwani", "authors": "Nikit Begwani, Shrutendra Harsola, Rahul Agrawal", "title": "Learning From Weights: A Cost-Sensitive Approach For Ad Retrieval", "comments": "7 pages, 5 figures, DAPA, WSDM Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieval models such as CLSM is trained on click-through data which treats\neach clicked query-document pair as equivalent. While training on click-through\ndata is reasonable, this paper argues that it is sub-optimal because of its\nnoisy and long-tail nature (especially for sponsored search). In this paper, we\ndiscuss the impact of incorporating or disregarding the long tail pairs in the\ntraining set. Also, we propose a weighing based strategy using which we can\nlearn semantic representations for tail pairs without compromising the quality\nof retrieval. We conducted our experiments on Bing sponsored search and also on\nAmazon product recommendation to demonstrate that the methodology is domain\nagnostic.\n  Online A/B testing on live search engine traffic showed improvements in\nclicks (11.8\\% higher CTR) and as well as improvement in quality (8.2\\% lower\nbounce rate) when compared to the unweighted model. We also conduct the\nexperiment on Amazon Product Recommendation data where we see slight\nimprovements in NDCG Scores calculated by retrieving among co-purchased\nproduct.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 13:13:38 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 06:21:36 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Begwani", "Nikit", ""], ["Harsola", "Shrutendra", ""], ["Agrawal", "Rahul", ""]]}, {"id": "1811.12783", "submitter": "Shuai Li", "authors": "Shuai Li", "title": "Measure, Manifold, Learning, and Optimization: A Theory Of Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a formal measure-theoretical theory of neural networks (NN) built\non probability coupling theory. Our main contributions are summarized as\nfollows.\n  * Built on the formalism of probability coupling theory, we derive an\nalgorithm framework, named Hierarchical Measure Group and Approximate System\n(HMGAS), nicknamed S-System, that is designed to learn the complex\nhierarchical, statistical dependency in the physical world.\n  * We show that NNs are special cases of S-System when the probability kernels\nassume certain exponential family distributions. Activation Functions are\nderived formally. We further endow geometry on NNs through information\ngeometry, show that intermediate feature spaces of NNs are stochastic\nmanifolds, and prove that \"distance\" between samples is contracted as layers\nstack up.\n  * S-System shows NNs are inherently stochastic, and under a set of realistic\nboundedness and diversity conditions, it enables us to prove that for large\nsize nonlinear deep NNs with a class of losses, including the hinge loss, all\nlocal minima are global minima with zero loss errors, and regions around the\nminima are flat basins where all eigenvalues of Hessians are concentrated\naround zero, using tools and ideas from mean field theory, random matrix\ntheory, and nonlinear operator equations.\n  * S-System, the information-geometry structure and the optimization behaviors\ncombined completes the analog between Renormalization Group (RG) and NNs. It\nshows that a NN is a complex adaptive system that estimates the statistic\ndependency of microscopic object, e.g., pixels, in multiple scales. Unlike\nclear-cut physical quantity produced by RG in physics, e.g., temperature, NNs\nrenormalize/recompose manifolds emerging through learning/optimization that\ndivide the sample space into highly semantically meaningful groups that are\ndictated by supervised labels (in supervised NNs).\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 13:22:01 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Li", "Shuai", ""]]}, {"id": "1811.12799", "submitter": "Anna Guitart", "authors": "Pei Pei Chen, Anna Guitart, Ana Fern\\'andez del R\\'io and \\'Africa\n  Peri\\'a\\~nez", "title": "Customer Lifetime Value in Video Games Using Deep Learning and\n  Parametric Models", "comments": null, "journal-ref": "IEEE International Conference on Big Data (Big Data), p. 2134-2140\n  , 2018", "doi": "10.1109/BigData.2018.8622151", "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, video game developers record every virtual action performed by\ntheir players. As each player can remain in the game for years, this results in\nan exceptionally rich dataset that can be used to understand and predict player\nbehavior. In particular, this information may serve to identify the most\nvaluable players and foresee the amount of money they will spend in in-app\npurchases during their lifetime. This is crucial in free-to-play games, where\nup to 50% of the revenue is generated by just around 2% of the players, the\nso-called whales.\n  To address this challenge, we explore how deep neural networks can be used to\npredict customer lifetime value in video games, and compare their performance\nto parametric models such as Pareto/NBD. Our results suggest that convolutional\nneural network structures are the most efficient in predicting the economic\nvalue of individual players. They not only perform better in terms of accuracy,\nbut also scale to big data and significantly reduce computational time, as they\ncan work directly with raw sequential data and thus do not require any feature\nengineering process. This becomes important when datasets are very large, as is\noften the case with video game logs.\n  Moreover, convolutional neural networks are particularly well suited to\nidentify potential whales. Such an early identification is of paramount\nimportance for business purposes, as it would allow developers to implement\nin-game actions aimed at retaining big spenders and maximizing their lifetime,\nwhich would ultimately translate into increased revenue.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 09:41:35 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Chen", "Pei Pei", ""], ["Guitart", "Anna", ""], ["del R\u00edo", "Ana Fern\u00e1ndez", ""], ["Peri\u00e1\u00f1ez", "\u00c1frica", ""]]}, {"id": "1811.12801", "submitter": "Vaibhav Kulkarni", "authors": "Vaibhav Kulkarni, Natasa Tagasovska, Thibault Vatter, Benoit Garbinato", "title": "Generative Models for Simulating Mobility Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobility datasets are fundamental for evaluating algorithms pertaining to\ngeographic information systems and facilitating experimental reproducibility.\nBut privacy implications restrict sharing such datasets, as even aggregated\nlocation-data is vulnerable to membership inference attacks. Current synthetic\nmobility dataset generators attempt to superficially match a priori modeled\nmobility characteristics which do not accurately reflect the real-world\ncharacteristics. Modeling human mobility to generate synthetic yet semantically\nand statistically realistic trajectories is therefore crucial for publishing\ntrajectory datasets having satisfactory utility level while preserving user\nprivacy. Specifically, long-range dependencies inherent to human mobility are\nchallenging to capture with both discriminative and generative models. In this\npaper, we benchmark the performance of recurrent neural architectures (RNNs),\ngenerative adversarial networks (GANs) and nonparametric copulas to generate\nsynthetic mobility traces. We evaluate the generated trajectories with respect\nto their geographic and semantic similarity, circadian rhythms, long-range\ndependencies, training and generation time. We also include two sample tests to\nassess statistical similarity between the observed and simulated distributions,\nand we analyze the privacy tradeoffs with respect to membership inference and\nlocation-sequence attacks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 14:14:44 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Kulkarni", "Vaibhav", ""], ["Tagasovska", "Natasa", ""], ["Vatter", "Thibault", ""], ["Garbinato", "Benoit", ""]]}, {"id": "1811.12802", "submitter": "Qiuyi Wu", "authors": "Qiuyi Wu, Ernest Fokoue", "title": "Naive Dictionary On Musical Corpora: From Knowledge Representation To\n  Pattern Recognition", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and develop the novel idea of treating musical\nsheets as literary documents in the traditional text analytics parlance, to\nfully benefit from the vast amount of research already existing in statistical\ntext mining and topic modelling. We specifically introduce the idea of\nrepresenting any given piece of music as a collection of \"musical words\" that\nwe codenamed \"muselets\", which are essentially musical words of various\nlengths. Given the novelty and therefore the extremely difficulty of properly\nforming a complete version of a dictionary of muselets, the present paper\nfocuses on a simpler albeit naive version of the ultimate dictionary, which we\nrefer to as a Naive Dictionary because of the fact that all the words are of\nthe same length. We specifically herein construct a naive dictionary featuring\na corpus made up of African American, Chinese, Japanese and Arabic music, on\nwhich we perform both topic modelling and pattern recognition. Although some of\nthe results based on the Naive Dictionary are reasonably good, we anticipate\nphenomenal predictive performances once we get around to actually building a\nfull scale complete version of our intended dictionary of muselets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 02:10:57 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Wu", "Qiuyi", ""], ["Fokoue", "Ernest", ""]]}, {"id": "1811.12808", "submitter": "Sebastian Raschka", "authors": "Sebastian Raschka", "title": "Model Evaluation, Model Selection, and Algorithm Selection in Machine\n  Learning", "comments": "v3 (Nov 2020): Fixes SD from pooled proportions in Sec 4.2 Fixes\n  exact binomial p-value in Sec 4.4 by using max(B, C) instead of B in the sum.\n  Fixes typo using wrong symbols in Looney's F-test's SSA in Sec 4.7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The correct use of model evaluation, model selection, and algorithm selection\ntechniques is vital in academic machine learning research as well as in many\nindustrial settings. This article reviews different techniques that can be used\nfor each of these three subtasks and discusses the main advantages and\ndisadvantages of each technique with references to theoretical and empirical\nstudies. Further, recommendations are given to encourage best yet feasible\npractices in research and applications of machine learning. Common methods such\nas the holdout method for model evaluation and selection are covered, which are\nnot recommended when working with small datasets. Different flavors of the\nbootstrap technique are introduced for estimating the uncertainty of\nperformance estimates, as an alternative to confidence intervals via normal\napproximation if bootstrapping is computationally feasible. Common\ncross-validation techniques such as leave-one-out cross-validation and k-fold\ncross-validation are reviewed, the bias-variance trade-off for choosing k is\ndiscussed, and practical tips for the optimal choice of k are given based on\nempirical evidence. Different statistical tests for algorithm comparisons are\npresented, and strategies for dealing with multiple comparisons such as omnibus\ntests and multiple-comparison corrections are discussed. Finally, alternative\nmethods for algorithm selection, such as the combined F-test 5x2\ncross-validation and nested cross-validation, are recommended for comparing\nmachine learning algorithms when datasets are small.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 15:36:42 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 02:03:11 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 00:59:17 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Raschka", "Sebastian", ""]]}, {"id": "1811.12809", "submitter": "Luis Lamb", "authors": "Felipe Grando and Luis C. Lamb", "title": "Computing Vertex Centrality Measures in Massive Real Networks with a\n  Neural Learning Model", "comments": "8 pages, 5 tables, 2 figures, version accepted at IJCNN 2018. arXiv\n  admin note: text overlap with arXiv:1810.11760", "journal-ref": "IEEE International Joint Conference on Neural Networks, IJCNN\n  2018: 1-8", "doi": "10.1109/IJCNN.2018.8489690", "report-no": null, "categories": "cs.SI cs.LG cs.NE cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vertex centrality measures are a multi-purpose analysis tool, commonly used\nin many application environments to retrieve information and unveil knowledge\nfrom the graphs and network structural properties. However, the algorithms of\nsuch metrics are expensive in terms of computational resources when running\nreal-time applications or massive real world networks. Thus, approximation\ntechniques have been developed and used to compute the measures in such\nscenarios. In this paper, we demonstrate and analyze the use of neural network\nlearning algorithms to tackle such task and compare their performance in terms\nof solution quality and computation time with other techniques from the\nliterature. Our work offers several contributions. We highlight both the pros\nand cons of approximating centralities though neural learning. By empirical\nmeans and statistics, we then show that the regression model generated with a\nfeedforward neural networks trained by the Levenberg-Marquardt algorithm is not\nonly the best option considering computational resources, but also achieves the\nbest solution quality for relevant applications and large-scale networks.\nKeywords: Vertex Centrality Measures, Neural Networks, Complex Network Models,\nMachine Learning, Regression Model\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 03:37:44 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Grando", "Felipe", ""], ["Lamb", "Luis C.", ""]]}, {"id": "1811.12817", "submitter": "Fabian Mentzer", "authors": "Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte,\n  Luc Van Gool", "title": "Practical Full Resolution Learned Lossless Image Compression", "comments": "Updated preprocessing and Table 1, see A.1 in supplementary. Code and\n  models: https://github.com/fab-jul/L3C-PyTorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the first practical learned lossless image compression system,\nL3C, and show that it outperforms the popular engineered codecs, PNG, WebP and\nJPEG 2000. At the core of our method is a fully parallelizable hierarchical\nprobabilistic model for adaptive entropy coding which is optimized end-to-end\nfor the compression task. In contrast to recent autoregressive discrete\nprobabilistic models such as PixelCNN, our method i) models the image\ndistribution jointly with learned auxiliary representations instead of\nexclusively modeling the image distribution in RGB space, and ii) only requires\nthree forward-passes to predict all pixel probabilities instead of one for each\npixel. As a result, L3C obtains over two orders of magnitude speedups when\nsampling compared to the fastest PixelCNN variant (Multiscale-PixelCNN).\nFurthermore, we find that learning the auxiliary representation is crucial and\noutperforms predefined auxiliary representations such as an RGB pyramid\nsignificantly.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 14:32:47 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 13:24:44 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 15:57:56 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Mentzer", "Fabian", ""], ["Agustsson", "Eirikur", ""], ["Tschannen", "Michael", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1811.12823", "submitter": "Daniil Polykovskiy", "authors": "Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling,\n  Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey\n  Artamonov, Vladimir Aladinskiy, Mark Veselov, Artur Kadurin, Simon Johansson,\n  Hongming Chen, Sergey Nikolenko, Alan Aspuru-Guzik, Alex Zhavoronkov", "title": "Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generative models are becoming a tool of choice for exploring the molecular\nspace. These models learn on a large training dataset and produce novel\nmolecular structures with similar properties. Generated structures can be\nutilized for virtual screening or training semi-supervised predictive models in\nthe downstream tasks. While there are plenty of generative models, it is\nunclear how to compare and rank them. In this work, we introduce a benchmarking\nplatform called Molecular Sets (MOSES) to standardize training and comparison\nof molecular generative models. MOSES provides a training and testing datasets,\nand a set of metrics to evaluate the quality and diversity of generated\nstructures. We have implemented and compared several molecular generation\nmodels and suggest to use our results as reference points for further\nadvancements in generative chemistry research. The platform and source code are\navailable at https://github.com/molecularsets/moses.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 08:48:20 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 20:03:21 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 07:23:45 GMT"}, {"version": "v4", "created": "Fri, 29 May 2020 16:15:59 GMT"}, {"version": "v5", "created": "Wed, 28 Oct 2020 14:11:16 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Polykovskiy", "Daniil", ""], ["Zhebrak", "Alexander", ""], ["Sanchez-Lengeling", "Benjamin", ""], ["Golovanov", "Sergey", ""], ["Tatanov", "Oktai", ""], ["Belyaev", "Stanislav", ""], ["Kurbanov", "Rauf", ""], ["Artamonov", "Aleksey", ""], ["Aladinskiy", "Vladimir", ""], ["Veselov", "Mark", ""], ["Kadurin", "Artur", ""], ["Johansson", "Simon", ""], ["Chen", "Hongming", ""], ["Nikolenko", "Sergey", ""], ["Aspuru-Guzik", "Alan", ""], ["Zhavoronkov", "Alex", ""]]}, {"id": "1811.12824", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr, Carsten Witt, Jing Yang", "title": "Runtime Analysis for Self-adaptive Mutation Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a self-adaptive version of the $(1,\\lambda)$\nevolutionary algorithm in which the current mutation rate is part of the\nindividual and thus also subject to mutation. A rigorous runtime analysis on\nthe OneMax benchmark function reveals that a simple local mutation scheme for\nthe rate leads to an expected optimization time (number of fitness evaluations)\nof $O(n\\lambda/\\log\\lambda+n\\log n)$ when $\\lambda$ is at least $C \\ln n$ for\nsome constant $C > 0$. For all values of $\\lambda \\ge C \\ln n$, this\nperformance is asymptotically best possible among all $\\lambda$-parallel\nmutation-based unbiased black-box algorithms.\n  Our result shows that self-adaptation in evolutionary computation can find\ncomplex optimal parameter settings on the fly. At the same time, it proves that\na relatively complicated self-adjusting scheme for the mutation rate proposed\nby Doerr, Gie{\\ss}en, Witt, and Yang~(GECCO~2017) can be replaced by our simple\nendogenous scheme.\n  On the technical side, the paper contributes new tools for the analysis of\ntwo-dimensional drift processes arising in the analysis of dynamic parameter\nchoices in EAs, including bounds on occupation probabilities in processes with\nnon-constant drift.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 14:38:05 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Doerr", "Benjamin", ""], ["Witt", "Carsten", ""], ["Yang", "Jing", ""]]}, {"id": "1811.12830", "submitter": "Sarah Hamilton", "authors": "S. J. Hamilton, A. H\\\"anninen, A. Hauptmann, and V. Kolehmainen", "title": "Beltrami-Net: Domain Independent Deep D-bar Learning for Absolute\n  Imaging with Electrical Impedance Tomography (a-EIT)", "comments": "15 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To develop, and demonstrate the feasibility of, a novel image\nreconstruction method for absolute Electrical Impedance Tomography (a-EIT) that\npairs deep learning techniques with real-time robust D-bar methods. Approach: A\nD-bar method is paired with a trained Convolutional Neural Network (CNN) as a\npost-processing step. Training data is simulated for the network using no\nknowledge of the boundary shape by using an associated nonphysical Beltrami\nequation rather than simulating the traditional current and voltage data\nspecific to a given domain. This allows the training data to be boundary shape\nindependent. The method is tested on experimental data from two EIT systems\n(ACT4 and KIT4). Main Results: Post processing the D-bar images with a CNN\nproduces significant improvements in image quality measured by Structural\nSIMilarity indices (SSIMs) as well as relative $\\ell_2$ and $\\ell_1$ image\nerrors. Significance: This work demonstrates that more general networks can be\ntrained without being specific about boundary shape, a key challenge in EIT\nimage reconstruction. The work is promising for future studies involving\ndatabases of anatomical atlases.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 14:55:33 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Hamilton", "S. J.", ""], ["H\u00e4nninen", "A.", ""], ["Hauptmann", "A.", ""], ["Kolehmainen", "V.", ""]]}, {"id": "1811.12843", "submitter": "Abdullah Al-Dujaili", "authors": "Tom Schmiedlechner and Ignavier Ng Zhi Yong and Abdullah Al-Dujaili\n  and Erik Hemberg and Una-May O'Reilly", "title": "Lipizzaner: A System That Scales Robust Generative Adversarial Network\n  Training", "comments": "Systems for ML Workshop (MLSYS) at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GANs are difficult to train due to convergence pathologies such as mode and\ndiscriminator collapse. We introduce Lipizzaner, an open source software system\nthat allows machine learning engineers to train GANs in a distributed and\nrobust way. Lipizzaner distributes a competitive coevolutionary algorithm\nwhich, by virtue of dual, adapting, generator and discriminator populations, is\nrobust to collapses. The algorithm is well suited to efficient distribution\nbecause it uses a spatial grid abstraction. Training is local to each cell and\nstrong intermediate training results are exchanged among overlapping\nneighborhoods allowing high performing solutions to propagate and improve with\nmore rounds of training. Experiments on common image datasets overcome critical\ncollapses. Communication overhead scales linearly when increasing the number of\ncompute instances and we observe that increasing scale leads to improved model\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 15:23:03 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Schmiedlechner", "Tom", ""], ["Yong", "Ignavier Ng Zhi", ""], ["Al-Dujaili", "Abdullah", ""], ["Hemberg", "Erik", ""], ["O'Reilly", "Una-May", ""]]}, {"id": "1811.12852", "submitter": "Odysseas Kanavetas", "authors": "Apostolos N. Burnetas, Odysseas Kanavetas, Michael N. Katehakis", "title": "Optimal Data Driven Resource Allocation under Multi-Armed Bandit\n  Observations", "comments": "arXiv admin note: text overlap with arXiv:1509.02857", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the first asymptotically optimal strategy for a multi\narmed bandit (MAB) model under side constraints. The side constraints model\nsituations in which bandit activations are limited by the availability of\ncertain resources that are replenished at a constant rate. The main result\ninvolves the derivation of an asymptotic lower bound for the regret of feasible\nuniformly fast policies and the construction of policies that achieve this\nlower bound, under pertinent conditions. Further, we provide the explicit form\nof such policies for the case in which the unknown distributions are Normal\nwith unknown means and known variances, for the case of Normal distributions\nwith unknown means and unknown variances and for the case of arbitrary discrete\ndistributions with finite support.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 17:28:31 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 05:21:13 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Burnetas", "Apostolos N.", ""], ["Kanavetas", "Odysseas", ""], ["Katehakis", "Michael N.", ""]]}, {"id": "1811.12866", "submitter": "Tom Tirer", "authors": "Tom Tirer, Raja Giryes", "title": "Super-Resolution via Image-Adapted Denoising CNNs: Incorporating\n  External and Internal Learning", "comments": "Accepted to IEEE Signal Processing Letters (extended version)", "journal-ref": null, "doi": "10.1109/LSP.2019.2920250", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks exhibit state-of-the-art results in the task of\nimage super-resolution (SR) with a fixed known acquisition process (e.g., a\nbicubic downscaling kernel), they experience a huge performance loss when the\nreal observation model mismatches the one used in training. Recently, two\ndifferent techniques suggested to mitigate this deficiency, i.e., enjoy the\nadvantages of deep learning without being restricted by the training phase. The\nfirst one follows the plug-and-play (P&P) approach that solves general inverse\nproblems (e.g., SR) by using Gaussian denoisers for handling the prior term in\nmodel-based optimization schemes. The second builds on internal recurrence of\ninformation inside a single image, and trains a super-resolver network at test\ntime on examples synthesized from the low-resolution image. Our work\nincorporates these two independent strategies, enjoying the impressive\ngeneralization capabilities of deep learning, captured by the first, and\nfurther improving it through internal learning at test time. First, we apply a\nrecent P&P strategy to SR. Then, we show how it may become image-adaptive in\ntest time. This technique outperforms the above two strategies on popular\ndatasets and gives better results than other state-of-the-art methods in\npractical cases where the observation model is inexact or unknown in advance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 16:15:19 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 23:27:53 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 10:50:58 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Tirer", "Tom", ""], ["Giryes", "Raja", ""]]}, {"id": "1811.12911", "submitter": "Mehdi Shafiei", "authors": "Mehdi Shafiei, Aaron Liu, Gerard Ledwich, Geoffery Walker, Gian-Marco\n  Morosini, Jack Terry", "title": "Solar Enablement Initiative in Australia: Report on Efficiently\n  Identifying Critical Cases for Evaluating the Voltage Impact of Large PV\n  Investment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing quantity of PV generation connected to distribution networks\nis creating challenges in maintaining and controlling voltages in those\ndistribution networks. Determining the maximum hosting capacity for new PV\ninstallations based on the historical data is an essential task for\ndistribution networks. Analyzing all historical data in large distribution\nnetworks is impractical. Therefore, this paper focuses on how to time\nefficiently identify the critical cases for evaluating the voltage impacts of\nthe new large PV applications in medium voltage (MV) distribution networks. A\nsystematic approach is proposed to cluster medium voltage nodes based on\nelectrical adjacency and time blocks. MV nodes are clustered along with the\nvoltage magnitudes and time blocks. Critical cases of each cluster can be used\nfor further power flow study. This method is scalable and can time efficiently\nidentify cases for evaluating PV investment on medium voltage networks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 01:10:36 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Shafiei", "Mehdi", ""], ["Liu", "Aaron", ""], ["Ledwich", "Gerard", ""], ["Walker", "Geoffery", ""], ["Morosini", "Gian-Marco", ""], ["Terry", "Jack", ""]]}, {"id": "1811.12929", "submitter": "Ondrej Biza", "authors": "Ondrej Biza and Robert Platt", "title": "Online Abstraction with MDP Homomorphisms for Deep Learning", "comments": null, "journal-ref": "Proceedings of the 18th International Conference on Autonomous\n  Agents and MultiAgent Systems (AAMAS '19). 2019. 1125 - 1133", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstraction of Markov Decision Processes is a useful tool for solving complex\nproblems, as it can ignore unimportant aspects of an environment, simplifying\nthe process of learning an optimal policy. In this paper, we propose a new\nalgorithm for finding abstract MDPs in environments with continuous state\nspaces. It is based on MDP homomorphisms, a structure-preserving mapping\nbetween MDPs. We demonstrate our algorithm's ability to learn abstractions from\ncollected experience and show how to reuse the abstractions to guide\nexploration in new tasks the agent encounters. Our novel task transfer method\noutperforms baselines based on a deep Q-network in the majority of our\nexperiments. The source code is at https://github.com/ondrejba/aamas_19.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 18:29:29 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 11:20:50 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Biza", "Ondrej", ""], ["Platt", "Robert", ""]]}, {"id": "1811.12932", "submitter": "Arthur Pesah", "authors": "Arthur Pesah, Antoine Wehenkel, Gilles Louppe", "title": "Recurrent machines for likelihood-free inference", "comments": "2nd Workshop on Meta-Learning at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG hep-ph physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Likelihood-free inference is concerned with the estimation of the parameters\nof a non-differentiable stochastic simulator that best reproduce real\nobservations. In the absence of a likelihood function, most of the existing\ninference methods optimize the simulator parameters through a handcrafted\niterative procedure that tries to make the simulated data more similar to the\nobservations. In this work, we explore whether meta-learning can be used in the\nlikelihood-free context, for learning automatically from data an iterative\noptimization procedure that would solve likelihood-free inference problems. We\ndesign a recurrent inference machine that learns a sequence of parameter\nupdates leading to good parameter estimates, without ever specifying some\nexplicit notion of divergence between the simulated data and the real data\ndistributions. We demonstrate our approach on toy simulators, showing promising\nresults both in terms of performance and robustness.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 18:39:12 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 14:00:10 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Pesah", "Arthur", ""], ["Wehenkel", "Antoine", ""], ["Louppe", "Gilles", ""]]}, {"id": "1811.12938", "submitter": "Marek Rei", "authors": "Marek Rei, Joshua Oppenheimer, Marek Sirendi", "title": "Advance Prediction of Ventricular Tachyarrhythmias using Patient\n  Metadata and Multi-Task Networks", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/158", "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel neural network architecture for the prediction of\nventricular tachyarrhythmias. The model receives input features that capture\nthe change in RR intervals and ectopic beats, along with features based on\nheart rate variability and frequency analysis. Patient age is also included as\na trainable embedding, while the whole network is optimized with multi-task\nobjectives. Each of these modifications provides a consistent improvement to\nthe model performance, achieving 74.02% prediction accuracy and 77.22%\nspecificity 60 seconds in advance of the episode.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 18:51:41 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Rei", "Marek", ""], ["Oppenheimer", "Joshua", ""], ["Sirendi", "Marek", ""]]}, {"id": "1811.12941", "submitter": "Kai Rothauge", "authors": "Noah Golmant and Nikita Vemuri and Zhewei Yao and Vladimir Feinberg\n  and Amir Gholami and Kai Rothauge and Michael W. Mahoney and Joseph Gonzalez", "title": "On the Computational Inefficiency of Large Batch Sizes for Stochastic\n  Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing the mini-batch size for stochastic gradient descent offers\nsignificant opportunities to reduce wall-clock training time, but there are a\nvariety of theoretical and systems challenges that impede the widespread\nsuccess of this technique. We investigate these issues, with an emphasis on\ntime to convergence and total computational cost, through an extensive\nempirical analysis of network training across several architectures and problem\ndomains, including image classification, image segmentation, and language\nmodeling. Although it is common practice to increase the batch size in order to\nfully exploit available computational resources, we find a substantially more\nnuanced picture. Our main finding is that across a wide range of network\narchitectures and problem domains, increasing the batch size beyond a certain\npoint yields no decrease in wall-clock time to convergence for \\emph{either}\ntrain or test loss. This batch size is usually substantially below the capacity\nof current systems. We show that popular training strategies for large batch\nsize optimization begin to fail before we can populate all available compute\nresources, and we show that the point at which these methods break down depends\nmore on attributes like model architecture and data complexity than it does\ndirectly on the size of the dataset.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 18:58:59 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Golmant", "Noah", ""], ["Vemuri", "Nikita", ""], ["Yao", "Zhewei", ""], ["Feinberg", "Vladimir", ""], ["Gholami", "Amir", ""], ["Rothauge", "Kai", ""], ["Mahoney", "Michael W.", ""], ["Gonzalez", "Joseph", ""]]}]