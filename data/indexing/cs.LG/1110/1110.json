[{"id": "1110.0061", "submitter": "Sergey Pankov", "authors": "Sergey Pankov", "title": "Learning image transformations without training examples", "comments": "15 pages, 1 figure, ISVC11", "journal-ref": "Proc. 7th International Symposium on Visual Computing, part II, pp\n  168-179, 2011", "doi": "10.1007/978-3-642-24031-7_17", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of image transformations is essential for efficient modeling and\nlearning of visual data. But the class of relevant transformations is large:\naffine transformations, projective transformations, elastic deformations, ...\nthe list goes on. Therefore, learning these transformations, rather than hand\ncoding them, is of great conceptual interest. To the best of our knowledge, all\nthe related work so far has been concerned with either supervised or weakly\nsupervised learning (from correlated sequences, video streams, or\nimage-transform pairs). In this paper, on the contrary, we present a simple\nmethod for learning affine and elastic transformations when no examples of\nthese transformations are explicitly given, and no prior knowledge of space\n(such as ordering of pixels) is included either. The system has only access to\na moderately large database of natural images arranged in no particular order.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2011 01:07:03 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Pankov", "Sergey", ""]]}, {"id": "1110.0214", "submitter": "Ridwan Al Iqbal", "authors": "Ridwan Al Iqbal", "title": "Eclectic Extraction of Propositional Rules from Neural Networks", "comments": "ICCIT 2011, Dhaka, Bangladesh", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Network is among the most popular algorithm for supervised\nlearning. However, Neural Networks have a well-known drawback of being a \"Black\nBox\" learner that is not comprehensible to the Users. This lack of transparency\nmakes it unsuitable for many high risk tasks such as medical diagnosis that\nrequires a rational justification for making a decision. Rule Extraction\nmethods attempt to curb this limitation by extracting comprehensible rules from\na trained Network. Many such extraction algorithms have been developed over the\nyears with their respective strengths and weaknesses. They have been broadly\ncategorized into three types based on their approach to use internal model of\nthe Network. Eclectic Methods are hybrid algorithms that combine the other\napproaches to attain more performance. In this paper, we present an Eclectic\nmethod called HERETIC. Our algorithm uses Inductive Decision Tree learning\ncombined with information of the neural network structure for extracting\nlogical rules. Experiments and theoretical analysis show HERETIC to be better\nin terms of speed and performance.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2011 18:59:42 GMT"}], "update_date": "2011-10-04", "authors_parsed": [["Iqbal", "Ridwan Al", ""]]}, {"id": "1110.0413", "submitter": "Guillaume Obozinski", "authors": "Guillaume Obozinski (LIENS, INRIA Paris - Rocquencourt), Laurent\n  Jacob, Jean-Philippe Vert (CBIO)", "title": "Group Lasso with Overlaps: the Latent Group Lasso approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a norm for structured sparsity which leads to sparse linear\npredictors whose supports are unions of prede ned overlapping groups of\nvariables. We call the obtained formulation latent group Lasso, since it is\nbased on applying the usual group Lasso penalty on a set of latent variables. A\ndetailed analysis of the norm and its properties is presented and we\ncharacterize conditions under which the set of groups associated with latent\nvariables are correctly identi ed. We motivate and discuss the delicate choice\nof weights associated to each group, and illustrate this approach on simulated\ndata and on the problem of breast cancer prognosis from gene expression data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2011 16:49:45 GMT"}], "update_date": "2011-10-05", "authors_parsed": [["Obozinski", "Guillaume", "", "LIENS, INRIA Paris - Rocquencourt"], ["Jacob", "Laurent", "", "CBIO"], ["Vert", "Jean-Philippe", "", "CBIO"]]}, {"id": "1110.0593", "submitter": "Duncan Blythe", "authors": "Duncan A. J. Blythe", "title": "Two Projection Pursuit Algorithms for Machine Learning under\n  Non-Stationarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis derives, tests and applies two linear projection algorithms for\nmachine learning under non-stationarity. The first finds a direction in a\nlinear space upon which a data set is maximally non-stationary. The second aims\nto robustify two-way classification against non-stationarity. The algorithm is\ntested on a key application scenario, namely Brain Computer Interfacing.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2011 07:34:13 GMT"}], "update_date": "2011-10-05", "authors_parsed": [["Blythe", "Duncan A. J.", ""]]}, {"id": "1110.0718", "submitter": "Maxim Raginsky", "authors": "Maxim Raginsky", "title": "Directed information and Pearl's causal calculus", "comments": "8 pages, uses ieeeconf.cls; to appear in Proc. 49th Annual Allerton\n  Conf. on Communication, Control and Computing (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic graphical models are a fundamental tool in statistics, machine\nlearning, signal processing, and control. When such a model is defined on a\ndirected acyclic graph (DAG), one can assign a partial ordering to the events\noccurring in the corresponding stochastic system. Based on the work of Judea\nPearl and others, these DAG-based \"causal factorizations\" of joint probability\nmeasures have been used for characterization and inference of functional\ndependencies (causal links). This mostly expository paper focuses on several\nconnections between Pearl's formalism (and in particular his notion of\n\"intervention\") and information-theoretic notions of causality and feedback\n(such as causal conditioning, directed stochastic kernels, and directed\ninformation). As an application, we show how conditional directed information\ncan be used to develop an information-theoretic version of Pearl's \"back-door\"\ncriterion for identifiability of causal effects from passive observations. This\nsuggests that the back-door criterion can be thought of as a causal analog of\nstatistical sufficiency.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2011 15:15:08 GMT"}], "update_date": "2011-10-05", "authors_parsed": [["Raginsky", "Maxim", ""]]}, {"id": "1110.0879", "submitter": "Subhransu Maji", "authors": "Subhransu Maji", "title": "Linearized Additive Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the additive model learning literature and adapt a penalized\nspline formulation due to Eilers and Marx, to train additive classifiers\nefficiently. We also propose two new embeddings based two classes of orthogonal\nbasis with orthogonal derivatives, which can also be used to efficiently learn\nadditive classifiers. This paper follows the popular theme in the current\nliterature where kernel SVMs are learned much more efficiently using a\napproximate embedding and linear machine. In this paper we show that spline\nbasis are especially well suited for learning additive models because of their\nsparsity structure and the ease of computing the embedding which enables one to\ntrain these models in an online manner, without incurring the memory overhead\nof precomputing the storing the embeddings. We show interesting connections\nbetween B-Spline basis and histogram intersection kernel and show that for a\nparticular choice of regularization and degree of the B-Splines, our proposed\nlearning algorithm closely approximates the histogram intersection kernel SVM.\nThis enables one to learn additive models with almost no memory overhead\ncompared to fast a linear solver, such as LIBLINEAR, while being only 5-6X\nslower on average. On two large scale image classification datasets, MNIST and\nDaimler Chrysler pedestrians, the proposed additive classifiers are as accurate\nas the kernel SVM, while being two orders of magnitude faster to train.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2011 02:11:38 GMT"}], "update_date": "2011-10-06", "authors_parsed": [["Maji", "Subhransu", ""]]}, {"id": "1110.0957", "submitter": "Florent Couzinie-Devy", "authors": "Florent Couzinie-Devy and Julien Mairal and Francis Bach and Jean\n  Ponce", "title": "Dictionary Learning for Deblurring and Digital Zoom", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach to image deblurring and digital zooming\nusing sparse local models of image appearance. These models, where small image\npatches are represented as linear combinations of a few elements drawn from\nsome large set (dictionary) of candidates, have proven well adapted to several\nimage restoration tasks. A key to their success has been to learn dictionaries\nadapted to the reconstruction of small image patches. In contrast, recent works\nhave proposed instead to learn dictionaries which are not only adapted to data\nreconstruction, but also tuned for a specific task. We introduce here such an\napproach to deblurring and digital zoom, using pairs of blurry/sharp (or\nlow-/high-resolution) images for training, as well as an effective stochastic\ngradient algorithm for solving the corresponding optimization task. Although\nthis learning problem is not convex, once the dictionaries have been learned,\nthe sharp/high-resolution image can be recovered via convex optimization at\ntest time. Experiments with synthetic and real data demonstrate the\neffectiveness of the proposed approach, leading to state-of-the-art performance\nfor non-blind image deblurring and digital zoom.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2011 11:49:09 GMT"}], "update_date": "2011-10-07", "authors_parsed": [["Couzinie-Devy", "Florent", ""], ["Mairal", "Julien", ""], ["Bach", "Francis", ""], ["Ponce", "Jean", ""]]}, {"id": "1110.1073", "submitter": "C. A. Knoblock", "authors": "C. A. Knoblock, S. Minton, I. Muslea", "title": "Active Learning with Multiple Views", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 27, pages\n  203-233, 2006", "doi": "10.1613/jair.2005", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learners alleviate the burden of labeling large amounts of data by\ndetecting and asking the user to label only the most informative examples in\nthe domain. We focus here on active learning for multi-view domains, in which\nthere are several disjoint subsets of features (views), each of which is\nsufficient to learn the target concept. In this paper we make several\ncontributions. First, we introduce Co-Testing, which is the first approach to\nmulti-view active learning. Second, we extend the multi-view learning framework\nby also exploiting weak views, which are adequate only for learning a concept\nthat is more general/specific than the target concept. Finally, we empirically\nshow that Co-Testing outperforms existing active learners on a variety of real\nworld domains such as wrapper induction, Web page classification, advertisement\nremoval, and discourse tree parsing.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2011 18:59:49 GMT"}], "update_date": "2011-10-06", "authors_parsed": [["Knoblock", "C. A.", ""], ["Minton", "S.", ""], ["Muslea", "I.", ""]]}, {"id": "1110.1075", "submitter": "Pantelis Bouboulis", "authors": "Pantelis Bouboulis, Sergios Theodoridis, Michael Mavroforakis", "title": "The Augmented Complex Kernel LMS", "comments": "manuscript submitted to IEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2012.2200479", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a unified framework for adaptive kernel based signal processing of\ncomplex data was presented by the authors, which, besides offering techniques\nto map the input data to complex Reproducing Kernel Hilbert Spaces, developed a\nsuitable Wirtinger-like Calculus for general Hilbert Spaces. In this short\npaper, the extended Wirtinger's calculus is adopted to derive complex\nkernel-based widely-linear estimation filters. Furthermore, we illuminate\nseveral important characteristics of the widely linear filters. We show that,\nalthough in many cases the gains from adopting widely linear estimation\nfilters, as alternatives to ordinary linear ones, are rudimentary, for the case\nof kernel based widely linear filters significant performance improvements can\nbe obtained.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2011 19:03:35 GMT"}], "update_date": "2015-05-30", "authors_parsed": [["Bouboulis", "Pantelis", ""], ["Theodoridis", "Sergios", ""], ["Mavroforakis", "Michael", ""]]}, {"id": "1110.1514", "submitter": "Matus Telgarsky", "authors": "Matus Telgarsky", "title": "Blackwell Approachability and Minimax Theory", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript investigates the relationship between Blackwell\nApproachability, a stochastic vector-valued repeated game, and minimax theory,\na single-play scalar-valued scenario. First, it is established in a general\nsetting --- one not permitting invocation of minimax theory --- that\nBlackwell's Approachability Theorem and its generalization due to Hou are still\nvalid. Second, minimax structure grants a result in the spirit of Blackwell's\nweak-approachability conjecture, later resolved by Vieille, that any set is\neither approachable by one player, or avoidable by the opponent. This analysis\nalso reveals a strategy for the opponent.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2011 13:04:14 GMT"}], "update_date": "2011-10-10", "authors_parsed": [["Telgarsky", "Matus", ""]]}, {"id": "1110.1769", "submitter": "Jose Bento", "authors": "Jos\\'e Bento, Andrea Montanari", "title": "On the trade-off between complexity and correlation decay in structural\n  learning algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the structure of Ising models (pairwise\nbinary Markov random fields) from i.i.d. samples. While several methods have\nbeen proposed to accomplish this task, their relative merits and limitations\nremain somewhat obscure. By analyzing a number of concrete examples, we show\nthat low-complexity algorithms often fail when the Markov random field develops\nlong-range correlations. More precisely, this phenomenon appears to be related\nto the Ising model phase transition (although it does not coincide with it).\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2011 21:24:36 GMT"}], "update_date": "2011-10-11", "authors_parsed": [["Bento", "Jos\u00e9", ""], ["Montanari", "Andrea", ""]]}, {"id": "1110.1781", "submitter": "Aditya Kurve", "authors": "G. Kesidis and A. Kurve", "title": "A Study of Unsupervised Adaptive Crowdsourcing", "comments": "Technical Report, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider unsupervised crowdsourcing performance based on the model wherein\nthe responses of end-users are essentially rated according to how their\nresponses correlate with the majority of other responses to the same\nsubtasks/questions. In one setting, we consider an independent sequence of\nidentically distributed crowdsourcing assignments (meta-tasks), while in the\nother we consider a single assignment with a large number of component\nsubtasks. Both problems yield intuitive results in which the overall\nreliability of the crowd is a factor.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2011 02:18:50 GMT"}], "update_date": "2011-10-11", "authors_parsed": [["Kesidis", "G.", ""], ["Kurve", "A.", ""]]}, {"id": "1110.1796", "submitter": "Dipnarayan Ray", "authors": "Dip Narayan Ray, Somajyoti Majumder, Sumit Mukhopadhyay", "title": "A Behavior-based Approach for Multi-agent Q-learning for Autonomous\n  Exploration", "comments": "15 pages;(ISSN:2045-8711)", "journal-ref": "International Journal of Innovative Technology & Creative\n  Engineering, Vol.1 No.7 July 2011, page 1-15", "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of mobile robots is being popular over the world mainly for\nautonomous explorations in hazardous/ toxic or unknown environments. This\nexploration will be more effective and efficient if the explorations in unknown\nenvironment can be aided with the learning from past experiences. Currently\nreinforcement learning is getting more acceptances for implementing learning in\nrobots from the system-environment interactions. This learning can be\nimplemented using the concept of both single-agent and multiagent. This paper\ndescribes such a multiagent approach for implementing a type of reinforcement\nlearning using a priority based behaviour-based architecture. This proposed\nmethodology has been successfully tested in both indoor and outdoor\nenvironments.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2011 06:16:57 GMT"}], "update_date": "2011-10-11", "authors_parsed": [["Ray", "Dip Narayan", ""], ["Majumder", "Somajyoti", ""], ["Mukhopadhyay", "Sumit", ""]]}, {"id": "1110.2098", "submitter": "John Sun", "authors": "John Z. Sun, Kush R. Varshney and Karthik Subbian", "title": "Dynamic Matrix Factorization: A State Space Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization from a small number of observed entries has recently\ngarnered much attention as the key ingredient of successful recommendation\nsystems. One unresolved problem in this area is how to adapt current methods to\nhandle changing user preferences over time. Recent proposals to address this\nissue are heuristic in nature and do not fully exploit the time-dependent\nstructure of the problem. As a principled and general temporal formulation, we\npropose a dynamical state space model of matrix factorization. Our proposal\nbuilds upon probabilistic matrix factorization, a Bayesian model with Gaussian\npriors. We utilize results in state tracking, such as the Kalman filter, to\nprovide accurate recommendations in the presence of both process and\nmeasurement noise. We show how system parameters can be learned via\nexpectation-maximization and provide comparisons to current published\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 16:35:51 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2012 22:28:47 GMT"}, {"version": "v3", "created": "Sat, 4 Aug 2012 22:11:49 GMT"}], "update_date": "2012-08-07", "authors_parsed": [["Sun", "John Z.", ""], ["Varshney", "Kush R.", ""], ["Subbian", "Karthik", ""]]}, {"id": "1110.2136", "submitter": "Ron Begleiter", "authors": "Nir Ailon and Ron Begleiter and Esther Ezra", "title": "Active Learning Using Smooth Relative Regret Approximations with\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The disagreement coefficient of Hanneke has become a central data independent\ninvariant in proving active learning rates. It has been shown in various ways\nthat a concept class with low complexity together with a bound on the\ndisagreement coefficient at an optimal solution allows active learning rates\nthat are superior to passive learning ones.\n  We present a different tool for pool based active learning which follows from\nthe existence of a certain uniform version of low disagreement coefficient, but\nis not equivalent to it. In fact, we present two fundamental active learning\nproblems of significant interest for which our approach allows nontrivial\nactive learning bounds. However, any general purpose method relying on the\ndisagreement coefficient bounds only fails to guarantee any useful bounds for\nthese problems.\n  The tool we use is based on the learner's ability to compute an estimator of\nthe difference between the loss of any hypotheses and some fixed \"pivotal\"\nhypothesis to within an absolute error of at most $\\eps$ times the\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 18:32:32 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2012 10:41:11 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2012 13:56:24 GMT"}], "update_date": "2012-06-21", "authors_parsed": [["Ailon", "Nir", ""], ["Begleiter", "Ron", ""], ["Ezra", "Esther", ""]]}, {"id": "1110.2162", "submitter": "Ruben Sipos", "authors": "Ruben Sipos, Pannaga Shivaswamy, Thorsten Joachims", "title": "Large-Margin Learning of Submodular Summarization Methods", "comments": "update: improved formatting (figure placement) and algorithm\n  pseudocode clarity (Fig. 3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a supervised learning approach to training\nsubmodular scoring functions for extractive multi-document summarization. By\ntaking a structured predicition approach, we provide a large-margin method that\ndirectly optimizes a convex relaxation of the desired performance measure. The\nlearning method applies to all submodular summarization methods, and we\ndemonstrate its effectiveness for both pairwise as well as coverage-based\nscoring functions on multiple datasets. Compared to state-of-the-art functions\nthat were tuned manually, our method significantly improves performance and\nenables high-fidelity models with numbers of parameters well beyond what could\nreasonbly be tuned by hand.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 19:54:57 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2011 17:51:20 GMT"}], "update_date": "2011-10-14", "authors_parsed": [["Sipos", "Ruben", ""], ["Shivaswamy", "Pannaga", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1110.2211", "submitter": "L. P. Kaelbling", "authors": "L. P. Kaelbling, H. M. Pasula, L. S. Zettlemoyer", "title": "Learning Symbolic Models of Stochastic Domains", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 29, pages\n  309-352, 2007", "doi": "10.1613/jair.2113", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we work towards the goal of developing agents that can learn\nto act in complex worlds. We develop a probabilistic, relational planning rule\nrepresentation that compactly models noisy, nondeterministic action effects,\nand show how such rules can be effectively learned. Through experiments in\nsimple planning domains and a 3D simulated blocks world with realistic physics,\nwe demonstrate that this learning algorithm allows agents to effectively model\nworld dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 21:58:58 GMT"}], "update_date": "2011-10-12", "authors_parsed": [["Kaelbling", "L. P.", ""], ["Pasula", "H. M.", ""], ["Zettlemoyer", "L. S.", ""]]}, {"id": "1110.2306", "submitter": "Marco Cuturi", "authors": "Marco Cuturi, David Avis", "title": "Ground Metric Learning", "comments": "32 pages, 4 figures", "journal-ref": "Journal of Machine Learning Research, 15, 533-564. 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transportation distances have been used for more than a decade now in machine\nlearning to compare histograms of features. They have one parameter: the ground\nmetric, which can be any metric between the features themselves. As is the case\nfor all parameterized distances, transportation distances can only prove useful\nin practice when this parameter is carefully chosen. To date, the only option\navailable to practitioners to set the ground metric parameter was to rely on a\npriori knowledge of the features, which limited considerably the scope of\napplication of transportation distances. We propose to lift this limitation and\nconsider instead algorithms that can learn the ground metric using only a\ntraining set of labeled histograms. We call this approach ground metric\nlearning. We formulate the problem of learning the ground metric as the\nminimization of the difference of two polyhedral convex functions over a convex\nset of distance matrices. We follow the presentation of our algorithms with\npromising experimental results on binary classification tasks using GIST\ndescriptors of images taken in the Caltech-256 set.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2011 09:04:56 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Cuturi", "Marco", ""], ["Avis", "David", ""]]}, {"id": "1110.2392", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "A Variant of Azuma's Inequality for Martingales with Subgaussian Tails", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a variant of Azuma's concentration inequality for martingales, in\nwhich the standard boundedness requirement is replaced by the milder\nrequirement of a subgaussian tail.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2011 14:53:35 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2011 19:04:19 GMT"}], "update_date": "2011-10-14", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1110.2416", "submitter": "Frank-Michael Schleif", "authors": "F.-M. Schleif, A. Gisbrecht, B. Hammer", "title": "Supervised learning of short and high-dimensional temporal sequences for\n  life science measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": "DPA-11341", "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The analysis of physiological processes over time are often given by\nspectrometric or gene expression profiles over time with only few time points\nbut a large number of measured variables. The analysis of such temporal\nsequences is challenging and only few methods have been proposed. The\ninformation can be encoded time independent, by means of classical expression\ndifferences for a single time point or in expression profiles over time.\nAvailable methods are limited to unsupervised and semi-supervised settings. The\npredictive variables can be identified only by means of wrapper or\npost-processing techniques. This is complicated due to the small number of\nsamples for such studies. Here, we present a supervised learning approach,\ntermed Supervised Topographic Mapping Through Time (SGTM-TT). It learns a\nsupervised mapping of the temporal sequences onto a low dimensional grid. We\nutilize a hidden markov model (HMM) to account for the time domain and\nrelevance learning to identify the relevant feature dimensions most predictive\nover time. The learned mapping can be used to visualize the temporal sequences\nand to predict the class of a new sequence. The relevance learning permits the\nidentification of discriminating masses or gen expressions and prunes\ndimensions which are unnecessary for the classification task or encode mainly\nnoise. In this way we obtain a very efficient learning system for temporal\nsequences. The results indicate that using simultaneous supervised learning and\nmetric adaptation significantly improves the prediction accuracy for\nsynthetically and real life data in comparison to the standard techniques. The\ndiscriminating features, identified by relevance learning, compare favorably\nwith the results of alternative methods. Our method permits the visualization\nof the data on a low dimensional grid, highlighting the observed temporal\nstructure.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2011 16:19:06 GMT"}], "update_date": "2011-10-12", "authors_parsed": [["Schleif", "F. -M.", ""], ["Gisbrecht", "A.", ""], ["Hammer", "B.", ""]]}, {"id": "1110.2529", "submitter": "John Duchi", "authors": "Alekh Agarwal and John C. Duchi", "title": "The Generalization Ability of Online Algorithms for Dependent Data", "comments": "26 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the generalization performance of online learning algorithms trained\non samples coming from a dependent source of data. We show that the\ngeneralization error of any stable online algorithm concentrates around its\nregret--an easily computable statistic of the online performance of the\nalgorithm--when the underlying ergodic process is $\\beta$- or $\\phi$-mixing. We\nshow high probability error bounds assuming the loss function is convex, and we\nalso establish sharp convergence rates and deviation bounds for strongly convex\nlosses and several linear prediction problems such as linear and logistic\nregression, least-squares SVM, and boosting on dependent data. In addition, our\nresults have straightforward applications to stochastic optimization with\ndependent data, and our analysis requires only martingale convergence\narguments; we need not rely on more powerful statistical tools such as\nempirical process theory.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2011 23:27:42 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2012 03:12:48 GMT"}], "update_date": "2012-06-08", "authors_parsed": [["Agarwal", "Alekh", ""], ["Duchi", "John C.", ""]]}, {"id": "1110.2610", "submitter": "Parul Agarwal", "authors": "Parul Agarwal, M.Afshar Alam, Ranjit Biswas", "title": "Issues,Challenges and Tools of Clustering Algorithms", "comments": "6 PAGES", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 3, No. 2, May 2011 ISSN (Online): 1694-0814 page numbers 523-528", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is an unsupervised technique of Data Mining. It means grouping\nsimilar objects together and separating the dissimilar ones. Each object in the\ndata set is assigned a class label in the clustering process using a distance\nmeasure. This paper has captured the problems that are faced in real when\nclustering algorithms are implemented .It also considers the most extensively\nused tools which are readily available and support functions which ease the\nprogramming. Once algorithms have been implemented, they also need to be tested\nfor its validity. There exist several validation indexes for testing the\nperformance and accuracy which have also been discussed here.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 09:27:58 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Agarwal", "Parul", ""], ["Alam", "M. Afshar", ""], ["Biswas", "Ranjit", ""]]}, {"id": "1110.2626", "submitter": "Kuruba Usha Rani", "authors": "K. Usha Rani", "title": "Analysis of Heart Diseases Dataset using Neural Network Approach", "comments": "8 pages, 2 figures, 1 table; International Journal of Data Mining &\n  Knowledge Management Process (IJDKP) Vol.1, No.5, September 2011", "journal-ref": null, "doi": "10.5121/ijdkp.2011.1501", "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the important techniques of Data mining is Classification. Many real\nworld problems in various fields such as business, science, industry and\nmedicine can be solved by using classification approach. Neural Networks have\nemerged as an important tool for classification. The advantages of Neural\nNetworks helps for efficient classification of given data. In this study a\nHeart diseases dataset is analyzed using Neural Network approach. To increase\nthe efficiency of the classification process parallel approach is also adopted\nin the training phase.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 10:56:29 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Rani", "K. Usha", ""]]}, {"id": "1110.2755", "submitter": "Tamas Linder", "authors": "Andr\\'as Gyorgy, Tam\\'as Linder, G\\'abor Lugosi", "title": "Efficient Tracking of Large Classes of Experts", "comments": "17 pages, to appear in the IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of prediction of individual sequences, sequential prediction\nmethods are to be constructed that perform nearly as well as the best expert\nfrom a given class. We consider prediction strategies that compete with the\nclass of switching strategies that can segment a given sequence into several\nblocks, and follow the advice of a different \"base\" expert in each block. As\nusual, the performance of the algorithm is measured by the regret defined as\nthe excess loss relative to the best switching strategy selected in hindsight\nfor the particular sequence to be predicted. In this paper we construct\nprediction strategies of low computational cost for the case where the set of\nbase experts is large. In particular we provide a method that can transform any\nprediction algorithm $\\A$ that is designed for the base class into a tracking\nalgorithm. The resulting tracking algorithm can take advantage of the\nprediction performance and potential computational efficiency of $\\A$ in the\nsense that it can be implemented with time and space complexity only\n$O(n^{\\gamma} \\ln n)$ times larger than that of $\\A$, where $n$ is the time\nhorizon and $\\gamma \\ge 0$ is a parameter of the algorithm. With $\\A$ properly\nchosen, our algorithm achieves a regret bound of optimal order for $\\gamma>0$,\nand only $O(\\ln n)$ times larger than the optimal order for $\\gamma=0$ for all\ntypical regret bound types we examined. For example, for predicting binary\nsequences with switching parameters under the logarithmic loss, our method\nachieves the optimal $O(\\ln n)$ regret rate with time complexity\n$O(n^{1+\\gamma}\\ln n)$ for any $\\gamma\\in (0,1)$.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 18:48:09 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2011 13:21:38 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2012 23:24:32 GMT"}], "update_date": "2012-07-12", "authors_parsed": [["Gyorgy", "Andr\u00e1s", ""], ["Linder", "Tam\u00e1s", ""], ["Lugosi", "G\u00e1bor", ""]]}, {"id": "1110.2842", "submitter": "Daniel Hsu", "authors": "Daniel Hsu and Sham M. Kakade and Tong Zhang", "title": "A tail inequality for quadratic forms of subgaussian random vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove an exponential probability tail inequality for positive semidefinite\nquadratic forms in a subgaussian random vector. The bound is analogous to one\nthat holds when the vector has independent Gaussian entries.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 04:56:17 GMT"}], "update_date": "2011-10-14", "authors_parsed": [["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""], ["Zhang", "Tong", ""]]}, {"id": "1110.2855", "submitter": "Louise Benoit", "authors": "Louise Beno\\^it (INRIA Paris - Rocquencourt, LIENS, INRIA Paris -\n  Rocquencourt), Julien Mairal (INRIA Paris - Rocquencourt, LIENS), Francis\n  Bach (INRIA Paris - Rocquencourt), Jean Ponce (INRIA Paris - Rocquencourt)", "title": "Sparse Image Representation with Epitomes", "comments": "Computer Vision and Pattern Recognition, Colorado Springs : United\n  States (2011)", "journal-ref": "Computer Vision and Pattern Recognition, Colorado Springs :\n  \\'Etats-Unis (2011)", "doi": "10.1109/CVPR.2011.5995636", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding, which is the decomposition of a vector using only a few basis\nelements, is widely used in machine learning and image processing. The basis\nset, also called dictionary, is learned to adapt to specific data. This\napproach has proven to be very effective in many image processing tasks.\nTraditionally, the dictionary is an unstructured \"flat\" set of atoms. In this\npaper, we study structured dictionaries which are obtained from an epitome, or\na set of epitomes. The epitome is itself a small image, and the atoms are all\nthe patches of a chosen size inside this image. This considerably reduces the\nnumber of parameters to learn and provides sparse image decompositions with\nshiftinvariance properties. We propose a new formulation and an algorithm for\nlearning the structured dictionaries associated with epitomes, and illustrate\ntheir use in image denoising tasks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 07:35:05 GMT"}], "update_date": "2011-12-13", "authors_parsed": [["Beno\u00eet", "Louise", "", "INRIA Paris - Rocquencourt, LIENS, INRIA Paris -\n  Rocquencourt"], ["Mairal", "Julien", "", "INRIA Paris - Rocquencourt, LIENS"], ["Bach", "Francis", "", "INRIA Paris - Rocquencourt"], ["Ponce", "Jean", "", "INRIA Paris - Rocquencourt"]]}, {"id": "1110.2897", "submitter": "Christos Boutsidis", "authors": "Christos Boutsidis and Anastasios Zouzias and Michael W. Mahoney and\n  Petros Drineas", "title": "Randomized Dimensionality Reduction for k-means Clustering", "comments": "IEEE Transactions on Information Theory, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the topic of dimensionality reduction for $k$-means clustering.\nDimensionality reduction encompasses the union of two approaches: \\emph{feature\nselection} and \\emph{feature extraction}. A feature selection based algorithm\nfor $k$-means clustering selects a small subset of the input features and then\napplies $k$-means clustering on the selected features. A feature extraction\nbased algorithm for $k$-means clustering constructs a small set of new\nartificial features and then applies $k$-means clustering on the constructed\nfeatures. Despite the significance of $k$-means clustering as well as the\nwealth of heuristic methods addressing it, provably accurate feature selection\nmethods for $k$-means clustering are not known. On the other hand, two provably\naccurate feature extraction methods for $k$-means clustering are known in the\nliterature; one is based on random projections and the other is based on the\nsingular value decomposition (SVD).\n  This paper makes further progress towards a better understanding of\ndimensionality reduction for $k$-means clustering. Namely, we present the first\nprovably accurate feature selection method for $k$-means clustering and, in\naddition, we present two feature extraction methods. The first feature\nextraction method is based on random projections and it improves upon the\nexisting results in terms of time complexity and number of features needed to\nbe extracted. The second feature extraction method is based on fast approximate\nSVD factorizations and it also improves upon the existing results in terms of\ntime complexity. The proposed algorithms are randomized and provide\nconstant-factor approximation guarantees with respect to the optimal $k$-means\nobjective value.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 11:24:59 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2013 09:47:52 GMT"}, {"version": "v3", "created": "Tue, 4 Nov 2014 19:40:43 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Boutsidis", "Christos", ""], ["Zouzias", "Anastasios", ""], ["Mahoney", "Michael W.", ""], ["Drineas", "Petros", ""]]}, {"id": "1110.2899", "submitter": "Ryota Tomioka", "authors": "Toshimitsu Takahashi, Ryota Tomioka, Kenji Yamanishi", "title": "Discovering Emerging Topics in Social Streams via Link Anomaly Detection", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of emerging topics are now receiving renewed interest motivated by\nthe rapid growth of social networks. Conventional term-frequency-based\napproaches may not be appropriate in this context, because the information\nexchanged are not only texts but also images, URLs, and videos. We focus on the\nsocial aspects of theses networks. That is, the links between users that are\ngenerated dynamically intentionally or unintentionally through replies,\nmentions, and retweets. We propose a probability model of the mentioning\nbehaviour of a social network user, and propose to detect the emergence of a\nnew topic from the anomaly measured through the model. We combine the proposed\nmention anomaly score with a recently proposed change-point detection technique\nbased on the Sequentially Discounting Normalized Maximum Likelihood (SDNML), or\nwith Kleinberg's burst model. Aggregating anomaly scores from hundreds of\nusers, we show that we can detect emerging topics only based on the\nreply/mention relationships in social network posts. We demonstrate our\ntechnique in a number of real data sets we gathered from Twitter. The\nexperiments show that the proposed mention-anomaly-based approaches can detect\nnew topics at least as early as the conventional term-frequency-based approach,\nand sometimes much earlier when the keyword is ill-defined.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 11:34:21 GMT"}], "update_date": "2011-10-14", "authors_parsed": [["Takahashi", "Toshimitsu", ""], ["Tomioka", "Ryota", ""], ["Yamanishi", "Kenji", ""]]}, {"id": "1110.3001", "submitter": "Peng Cheng", "authors": "Peng Cheng", "title": "Step size adaptation in first-order method for stochastic strongly\n  convex programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a first-order method for stochastic strongly convex optimization\nthat attains $O(1/n)$ rate of convergence, analysis show that the proposed\nmethod is simple, easily to implement, and in worst case, asymptotically four\ntimes faster than its peers. We derive this method from several intuitive\nobservations that are generalized from existing first order optimization\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 17:25:42 GMT"}], "update_date": "2011-10-14", "authors_parsed": [["Cheng", "Peng", ""]]}, {"id": "1110.3076", "submitter": "Xiaohui Xie", "authors": "Gui-Bo Ye, Yuanfeng Wang, Yifei Chen, and Xiaohui Xie", "title": "Efficient Latent Variable Graphical Model Selection via Split Bregman\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of covariance matrix estimation in the presence of\nlatent variables. Under suitable conditions, it is possible to learn the\nmarginal covariance matrix of the observed variables via a tractable convex\nprogram, where the concentration matrix of the observed variables is decomposed\ninto a sparse matrix (representing the graphical structure of the observed\nvariables) and a low rank matrix (representing the marginalization effect of\nlatent variables). We present an efficient first-order method based on split\nBregman to solve the convex problem. The algorithm is guaranteed to converge\nunder mild conditions. We show that our algorithm is significantly faster than\nthe state-of-the-art algorithm on both artificial and real-world data. Applying\nthe algorithm to a gene expression data involving thousands of genes, we show\nthat most of the correlation between observed variables can be explained by\nonly a few dozen latent factors.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 21:48:04 GMT"}], "update_date": "2011-10-17", "authors_parsed": [["Ye", "Gui-Bo", ""], ["Wang", "Yuanfeng", ""], ["Chen", "Yifei", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1110.3109", "submitter": "Zhiwu Lu", "authors": "Zhiwu Lu and Yuxin Peng", "title": "Robust Image Analysis by L1-Norm Semi-supervised Learning", "comments": "This is an extension of our long paper in ACM MM 2012", "journal-ref": "IEEE Trans. Image Processing 24(1): 176-188 (2015)", "doi": "10.1109/TIP.2014.2375641", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel L1-norm semi-supervised learning algorithm for\nrobust image analysis by giving new L1-norm formulation of Laplacian\nregularization which is the key step of graph-based semi-supervised learning.\nSince our L1-norm Laplacian regularization is defined directly over the\neigenvectors of the normalized Laplacian matrix, we successfully formulate\nsemi-supervised learning as an L1-norm linear reconstruction problem which can\nbe effectively solved with sparse coding. By working with only a small subset\nof eigenvectors, we further develop a fast sparse coding algorithm for our\nL1-norm semi-supervised learning. Due to the sparsity induced by sparse coding,\nthe proposed algorithm can deal with the noise in the data to some extent and\nthus has important applications to robust image analysis, such as noise-robust\nimage classification and noise reduction for visual and textual bag-of-words\n(BOW) models. In particular, this paper is the first attempt to obtain robust\nimage representation by sparse co-refinement of visual and textual BOW models.\nThe experimental results have shown the promising performance of the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2011 02:05:14 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2013 23:22:48 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Lu", "Zhiwu", ""], ["Peng", "Yuxin", ""]]}, {"id": "1110.3239", "submitter": "Giorgio Corani", "authors": "Giorgio Corani and Cassio P. De Campos", "title": "Improving parameter learning of Bayesian nets from incomplete data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the estimation of parameters of a Bayesian network from\nincomplete data. The task is usually tackled by running the\nExpectation-Maximization (EM) algorithm several times in order to obtain a high\nlog-likelihood estimate. We argue that choosing the maximum log-likelihood\nestimate (as well as the maximum penalized log-likelihood and the maximum a\nposteriori estimate) has severe drawbacks, being affected both by overfitting\nand model uncertainty. Two ideas are discussed to overcome these issues: a\nmaximum entropy approach and a Bayesian model averaging approach. Both ideas\ncan be easily applied on top of EM, while the entropy idea can be also\nimplemented in a more sophisticated way, through a dedicated non-linear solver.\nA vast set of experiments shows that these ideas produce significantly better\nestimates and inferences than the traditional and widely used maximum\n(penalized) log-likelihood and maximum a posteriori estimates. In particular,\nif EM is adopted as optimization engine, the model averaging approach is the\nbest performing one; its performance is matched by the entropy approach when\nimplemented using the non-linear solver. The results suggest that the\napplicability of these ideas is immediate (they are easy to implement and to\nintegrate in currently available inference engines) and that they constitute a\nbetter way to learn Bayesian network parameters.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 12:17:51 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Corani", "Giorgio", ""], ["De Campos", "Cassio P.", ""]]}, {"id": "1110.3347", "submitter": "Javad Azimi", "authors": "Javad Azimi, Ali Jalali, Xiaoli Fern", "title": "Dynamic Batch Bayesian Optimization", "comments": "6 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization (BO) algorithms try to optimize an unknown function\nthat is expensive to evaluate using minimum number of evaluations/experiments.\nMost of the proposed algorithms in BO are sequential, where only one experiment\nis selected at each iteration. This method can be time inefficient when each\nexperiment takes a long time and more than one experiment can be ran\nconcurrently. On the other hand, requesting a fix-sized batch of experiments at\neach iteration causes performance inefficiency in BO compared to the sequential\npolicies. In this paper, we present an algorithm that asks a batch of\nexperiments at each time step t where the batch size p_t is dynamically\ndetermined in each step. Our algorithm is based on the observation that the\nsequence of experiments selected by the sequential policy can sometimes be\nalmost independent from each other. Our algorithm identifies such scenarios and\nrequest those experiments at the same time without degrading the performance.\nWe evaluate our proposed method using the Expected Improvement policy and the\nresults show substantial speedup with little impact on the performance in eight\nreal and synthetic benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2011 21:47:12 GMT"}], "update_date": "2011-10-18", "authors_parsed": [["Azimi", "Javad", ""], ["Jalali", "Ali", ""], ["Fern", "Xiaoli", ""]]}, {"id": "1110.3564", "submitter": "Sewoong Oh", "authors": "David R. Karger and Sewoong Oh and Devavrat Shah", "title": "Budget-Optimal Task Allocation for Reliable Crowdsourcing Systems", "comments": "38 pages, 4 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing systems, in which numerous tasks are electronically distributed\nto numerous \"information piece-workers\", have emerged as an effective paradigm\nfor human-powered solving of large scale problems in domains such as image\nclassification, data entry, optical character recognition, recommendation, and\nproofreading. Because these low-paid workers can be unreliable, nearly all such\nsystems must devise schemes to increase confidence in their answers, typically\nby assigning each task multiple times and combining the answers in an\nappropriate manner, e.g. majority voting.\n  In this paper, we consider a general model of such crowdsourcing tasks and\npose the problem of minimizing the total price (i.e., number of task\nassignments) that must be paid to achieve a target overall reliability. We give\na new algorithm for deciding which tasks to assign to which workers and for\ninferring correct answers from the workers' answers. We show that our\nalgorithm, inspired by belief propagation and low-rank matrix approximation,\nsignificantly outperforms majority voting and, in fact, is optimal through\ncomparison to an oracle that knows the reliability of every worker. Further, we\ncompare our approach with a more general class of algorithms which can\ndynamically assign tasks. By adaptively deciding which questions to ask to the\nnext arriving worker, one might hope to reduce uncertainty more efficiently. We\nshow that, perhaps surprisingly, the minimum price necessary to achieve a\ntarget reliability scales in the same manner under both adaptive and\nnon-adaptive scenarios. Hence, our non-adaptive approach is order-optimal under\nboth scenarios. This strongly relies on the fact that workers are fleeting and\ncan not be exploited. Therefore, architecturally, our results suggest that\nbuilding a reliable worker-reputation system is essential to fully harnessing\nthe potential of adaptive designs.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 02:52:20 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2011 18:49:14 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2012 21:23:24 GMT"}, {"version": "v4", "created": "Tue, 26 Mar 2013 07:28:04 GMT"}], "update_date": "2013-03-27", "authors_parsed": [["Karger", "David R.", ""], ["Oh", "Sewoong", ""], ["Shah", "Devavrat", ""]]}, {"id": "1110.3592", "submitter": "David Balduzzi", "authors": "David Balduzzi", "title": "Information, learning and falsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are (at least) three approaches to quantifying information. The first,\nalgorithmic information or Kolmogorov complexity, takes events as strings and,\ngiven a universal Turing machine, quantifies the information content of a\nstring as the length of the shortest program producing it. The second, Shannon\ninformation, takes events as belonging to ensembles and quantifies the\ninformation resulting from observing the given event in terms of the number of\nalternate events that have been ruled out. The third, statistical learning\ntheory, has introduced measures of capacity that control (in part) the expected\nrisk of classifiers. These capacities quantify the expectations regarding\nfuture data that learning algorithms embed into classifiers.\n  This note describes a new method of quantifying information, effective\ninformation, that links algorithmic information to Shannon information, and\nalso links both to capacities arising in statistical learning theory. After\nintroducing the measure, we show that it provides a non-universal analog of\nKolmogorov complexity. We then apply it to derive basic capacities in\nstatistical learning theory: empirical VC-entropy and empirical Rademacher\ncomplexity. A nice byproduct of our approach is an interpretation of the\nexplanatory power of a learning algorithm in terms of the number of hypotheses\nit falsifies, counted in two different ways for the two capacities. We also\ndiscuss how effective information relates to information gain, Shannon and\nmutual information.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 07:51:59 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2011 06:56:52 GMT"}], "update_date": "2011-11-29", "authors_parsed": [["Balduzzi", "David", ""]]}, {"id": "1110.3717", "submitter": "Gunnar W. Klau", "authors": "C. Staiger, S. Cadot, R. Kooter, M. Dittrich, T. Mueller, G. W. Klau,\n  L. F. A. Wessels", "title": "A critical evaluation of network and pathway based classifiers for\n  outcome prediction in breast cancer", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0034796", "report-no": null, "categories": "cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several classifiers that combine primary tumor data, like gene\nexpression data, and secondary data sources, such as protein-protein\ninteraction networks, have been proposed for predicting outcome in breast\ncancer. In these approaches, new composite features are typically constructed\nby aggregating the expression levels of several genes. The secondary data\nsources are employed to guide this aggregation. Although many studies claim\nthat these approaches improve classification performance over single gene\nclassifiers, the gain in performance is difficult to assess. This stems mainly\nfrom the fact that different breast cancer data sets and validation procedures\nare employed to assess the performance. Here we address these issues by\nemploying a large cohort of six breast cancer data sets as benchmark set and by\nperforming an unbiased evaluation of the classification accuracies of the\ndifferent approaches. Contrary to previous claims, we find that composite\nfeature classifiers do not outperform simple single gene classifiers. We\ninvestigate the effect of (1) the number of selected features; (2) the specific\ngene set from which features are selected; (3) the size of the training set and\n(4) the heterogeneity of the data set on the performance of composite feature\nand single gene classifiers. Strikingly, we find that randomization of\nsecondary data sources, which destroys all biological information in these\nsources, does not result in a deterioration in performance of composite feature\nclassifiers. Finally, we show that when a proper correction for gene set size\nis performed, the stability of single gene sets is similar to the stability of\ncomposite feature sets. Based on these results there is currently no reason to\nprefer prognostic classifiers based on composite features over single gene\nclassifiers for predicting outcome in breast cancer.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 16:13:15 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2011 06:29:11 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Staiger", "C.", ""], ["Cadot", "S.", ""], ["Kooter", "R.", ""], ["Dittrich", "M.", ""], ["Mueller", "T.", ""], ["Klau", "G. W.", ""], ["Wessels", "L. F. A.", ""]]}, {"id": "1110.3741", "submitter": "Kevin Xu", "authors": "Ko-Jen Hsiao, Kevin S. Xu, Jeff Calder, and Alfred O. Hero III", "title": "Multi-criteria Anomaly Detection using Pareto Depth Analysis", "comments": "Removed an unnecessary line from Algorithm 1", "journal-ref": "Advances in Neural Information Processing Systems 25 (2012)\n  854-862", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying patterns in a data set that exhibit\nanomalous behavior, often referred to as anomaly detection. In most anomaly\ndetection algorithms, the dissimilarity between data samples is calculated by a\nsingle criterion, such as Euclidean distance. However, in many cases there may\nnot exist a single dissimilarity measure that captures all possible anomalous\npatterns. In such a case, multiple criteria can be defined, and one can test\nfor anomalies by scalarizing the multiple criteria using a linear combination\nof them. If the importance of the different criteria are not known in advance,\nthe algorithm may need to be executed multiple times with different choices of\nweights in the linear combination. In this paper, we introduce a novel\nnon-parametric multi-criteria anomaly detection method using Pareto depth\nanalysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies\nunder multiple criteria without having to run an algorithm multiple times with\ndifferent choices of weights. The proposed PDA approach scales linearly in the\nnumber of criteria and is provably better than linear combinations of the\ncriteria.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 17:48:22 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2012 22:12:52 GMT"}, {"version": "v3", "created": "Mon, 7 Jan 2013 17:18:42 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Hsiao", "Ko-Jen", ""], ["Xu", "Kevin S.", ""], ["Calder", "Jeff", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1110.3917", "submitter": "Bassam Mokbel", "authors": "Wouter Lueks, Bassam Mokbel, Michael Biehl, Barbara Hammer", "title": "How to Evaluate Dimensionality Reduction? - Improving the Co-ranking\n  Matrix", "comments": "This is an article for the Dagstuhl Preprint Archive, belonging to\n  Dagstuhl Seminar No. 11341 \"Learning in the context of very high dimensional\n  data\"", "journal-ref": null, "doi": null, "report-no": "DPA-11341", "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing number of dimensionality reduction methods available for data\nvisualization has recently inspired the development of quality assessment\nmeasures, in order to evaluate the resulting low-dimensional representation\nindependently from a methods' inherent criteria. Several (existing) quality\nmeasures can be (re)formulated based on the so-called co-ranking matrix, which\nsubsumes all rank errors (i.e. differences between the ranking of distances\nfrom every point to all others, comparing the low-dimensional representation to\nthe original data). The measures are often based on the partioning of the\nco-ranking matrix into 4 submatrices, divided at the K-th row and column,\ncalculating a weighted combination of the sums of each submatrix. Hence, the\nevaluation process typically involves plotting a graph over several (or even\nall possible) settings of the parameter K. Considering simple artificial\nexamples, we argue that this parameter controls two notions at once, that need\nnot necessarily be combined, and that the rectangular shape of submatrices is\ndisadvantageous for an intuitive interpretation of the parameter. We debate\nthat quality measures, as general and flexible evaluation tools, should have\nparameters with a direct and intuitive interpretation as to which specific\nerror types are tolerated or penalized. Therefore, we propose to replace K with\ntwo parameters to control these notions separately, and introduce a differently\nshaped weighting on the co-ranking matrix. The two new parameters can then\ndirectly be interpreted as a threshold up to which rank errors are tolerated,\nand a threshold up to which the rank-distances are significant for the\nevaluation. Moreover, we propose a color representation of local quality to\nvisually support the evaluation process for a given mapping, where every point\nin the mapping is colored according to its local contribution to the overall\nquality.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2011 09:17:29 GMT"}], "update_date": "2011-10-19", "authors_parsed": [["Lueks", "Wouter", ""], ["Mokbel", "Bassam", ""], ["Biehl", "Michael", ""], ["Hammer", "Barbara", ""]]}, {"id": "1110.4181", "submitter": "Nikolaus Hansen", "authors": "Nikolaus Hansen (INRIA Saclay - Ile de France, LRI, MSR - INRIA)", "title": "Injecting External Solutions Into CMA-ES", "comments": "No. RR-7748 (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report considers how to inject external candidate solutions into the\nCMA-ES algorithm. The injected solutions might stem from a gradient or a Newton\nstep, a surrogate model optimizer or any other oracle or search mechanism. They\ncan also be the result of a repair mechanism, for example to render infeasible\nsolutions feasible. Only small modifications to the CMA-ES are necessary to\nturn injection into a reliable and effective method: too long steps need to be\ntightly renormalized. The main objective of this report is to reveal this\nsimple mechanism. Depending on the source of the injected solutions,\ninteresting variants of CMA-ES arise. When the best-ever solution is always\n(re-)injected, an elitist variant of CMA-ES with weighted multi-recombination\narises. When \\emph{all} solutions are injected from an \\emph{external} source,\nthe resulting algorithm might be viewed as \\emph{adaptive encoding} with\nstep-size control. In first experiments, injected solutions of very good\nquality lead to a convergence speed twice as fast as on the (simple) sphere\nfunction without injection. This means that we observe an impressive speed-up\non otherwise difficult to solve functions. Single bad injected solutions on the\nother hand do no significant harm.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2011 04:42:33 GMT"}], "update_date": "2011-10-20", "authors_parsed": [["Hansen", "Nikolaus", "", "INRIA Saclay - Ile de France, LRI, MSR - INRIA"]]}, {"id": "1110.4198", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal, Olivier Chapelle, Miroslav Dudik, John Langford", "title": "A Reliable Effective Terascale Linear Learning System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system and a set of techniques for learning linear predictors\nwith convex losses on terascale datasets, with trillions of features, {The\nnumber of features here refers to the number of non-zero entries in the data\nmatrix.} billions of training examples and millions of parameters in an hour\nusing a cluster of 1000 machines. Individually none of the component techniques\nare new, but the careful synthesis required to obtain an efficient\nimplementation is. The result is, up to our knowledge, the most scalable and\nefficient linear learning system reported in the literature (as of 2011 when\nour experiments were conducted). We describe and thoroughly evaluate the\ncomponents of the system, showing the importance of the various design choices.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2011 07:34:19 GMT"}, {"version": "v2", "created": "Sun, 12 Feb 2012 18:31:21 GMT"}, {"version": "v3", "created": "Fri, 12 Jul 2013 03:28:17 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Agarwal", "Alekh", ""], ["Chapelle", "Olivier", ""], ["Dudik", "Miroslav", ""], ["Langford", "John", ""]]}, {"id": "1110.4322", "submitter": "Nicol\\`o Cesa-Bianchi", "authors": "Nicol\\`o Cesa-Bianchi and Sham Kakade", "title": "An Optimal Algorithm for Linear Bandits", "comments": "This paper is superseded by S. Bubeck, N. Cesa-Bianchi, and S.M.\n  Kakade, \"Towards minimax policies for online linear optimization with bandit\n  feedback\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first algorithm for online bandit linear optimization whose\nregret after T rounds is of order sqrt{Td ln N} on any finite class X of N\nactions in d dimensions, and of order d*sqrt{T} (up to log factors) when X is\ninfinite. These bounds are not improvable in general. The basic idea utilizes\ntools from convex geometry to construct what is essentially an optimal\nexploration basis. We also present an application to a model of linear bandits\nwith expert advice. Interestingly, these results show that bandit linear\noptimization with expert advice in d dimensions is no more difficult (in terms\nof the achievable regret) than the online d-armed bandit problem with expert\nadvice (where EXP4 is optimal).\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2011 15:57:27 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2011 14:30:27 GMT"}, {"version": "v3", "created": "Tue, 14 Feb 2012 16:14:39 GMT"}], "update_date": "2012-02-15", "authors_parsed": [["Cesa-Bianchi", "Nicol\u00f2", ""], ["Kakade", "Sham", ""]]}, {"id": "1110.4412", "submitter": "Ari Arapostathis", "authors": "Georgios C. Chasparis, Ari Arapostathis and Jeff S. Shamma", "title": "Aspiration Learning in Coordination Games", "comments": "27 pages", "journal-ref": "SIAM J. Control Optim. 51 (2013), no. 1, 465-490", "doi": "10.1137/110852462", "report-no": null, "categories": "cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of distributed convergence to efficient outcomes in\ncoordination games through dynamics based on aspiration learning. Under\naspiration learning, a player continues to play an action as long as the\nrewards received exceed a specified aspiration level. Here, the aspiration\nlevel is a fading memory average of past rewards, and these levels also are\nsubject to occasional random perturbations. A player becomes dissatisfied\nwhenever a received reward is less than the aspiration level, in which case the\nplayer experiments with a probability proportional to the degree of\ndissatisfaction. Our first contribution is the characterization of the\nasymptotic behavior of the induced Markov chain of the iterated process in\nterms of an equivalent finite-state Markov chain. We then characterize\nexplicitly the behavior of the proposed aspiration learning in a generalized\nversion of coordination games, examples of which include network formation and\ncommon-pool games. In particular, we show that in generic coordination games\nthe frequency at which an efficient action profile is played can be made\narbitrarily large. Although convergence to efficient outcomes is desirable, in\nseveral coordination games, such as common-pool games, attainability of fair\noutcomes, i.e., sequences of plays at which players experience highly rewarding\nreturns with the same frequency, might also be of special interest. To this\nend, we demonstrate through analysis and simulations that aspiration learning\nalso establishes fair outcomes in all symmetric coordination games, including\ncommon-pool games.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2011 22:30:03 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Chasparis", "Georgios C.", ""], ["Arapostathis", "Ari", ""], ["Shamma", "Jeff S.", ""]]}, {"id": "1110.4416", "submitter": "Guy Lever Dr", "authors": "Guy Lever, Tom Diethe and John Shawe-Taylor", "title": "Data-dependent kernels in nearly-linear time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to efficiently construct data-dependent kernels which can\nmake use of large quantities of (unlabeled) data. Our construction makes an\napproximation in the standard construction of semi-supervised kernels in\nSindhwani et al. 2005. In typical cases these kernels can be computed in\nnearly-linear time (in the amount of data), improving on the cubic time of the\nstandard construction, enabling large scale semi-supervised learning in a\nvariety of contexts. The methods are validated on semi-supervised and\nunsupervised problems on data sets containing upto 64,000 sample points.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2011 00:56:53 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Lever", "Guy", ""], ["Diethe", "Tom", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1110.4481", "submitter": "Julien Mairal", "authors": "Julien Mairal, Rodolphe Jenatton (LIENS, INRIA Paris - Rocquencourt),\n  Guillaume Obozinski (LIENS, INRIA Paris - Rocquencourt), Francis Bach (LIENS,\n  INRIA Paris - Rocquencourt)", "title": "Learning Hierarchical and Topographic Dictionaries with Structured\n  Sparsity", "comments": null, "journal-ref": "SPIE Wavelets and Sparsity XIV 81381P (2011)", "doi": "10.1117/12.893811", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in signal processing and statistics have focused on defining new\nregularization functions, which not only induce sparsity of the solution, but\nalso take into account the structure of the problem. We present in this paper a\nclass of convex penalties introduced in the machine learning community, which\ntake the form of a sum of l_2 and l_infinity-norms over groups of variables.\nThey extend the classical group-sparsity regularization in the sense that the\ngroups possibly overlap, allowing more flexibility in the group design. We\nreview efficient optimization methods to deal with the corresponding inverse\nproblems, and their application to the problem of learning dictionaries of\nnatural image patches: On the one hand, dictionary learning has indeed proven\neffective for various signal processing tasks. On the other hand, structured\nsparsity provides a natural framework for modeling dependencies between\ndictionary elements. We thus consider a structured sparse regularization to\nlearn dictionaries embedded in a particular structure, for instance a tree or a\ntwo-dimensional grid. In the latter case, the results we obtain are similar to\nthe dictionaries produced by topographic independent component analysis.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2011 09:50:58 GMT"}], "update_date": "2011-10-21", "authors_parsed": [["Mairal", "Julien", "", "LIENS, INRIA Paris - Rocquencourt"], ["Jenatton", "Rodolphe", "", "LIENS, INRIA Paris - Rocquencourt"], ["Obozinski", "Guillaume", "", "LIENS, INRIA Paris - Rocquencourt"], ["Bach", "Francis", "", "LIENS,\n  INRIA Paris - Rocquencourt"]]}, {"id": "1110.4713", "submitter": "Philipp Hennig PhD", "authors": "Philipp Hennig, David Stern, Ralf Herbrich and Thore Graepel", "title": "Kernel Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet Allocation models discrete data as a mixture of discrete\ndistributions, using Dirichlet beliefs over the mixture weights. We study a\nvariation of this concept, in which the documents' mixture weight beliefs are\nreplaced with squashed Gaussian distributions. This allows documents to be\nassociated with elements of a Hilbert space, admitting kernel topic models\n(KTM), modelling temporal, spatial, hierarchical, social and other structure\nbetween documents. The main challenge is efficient approximate inference on the\nlatent Gaussian. We present an approximate algorithm cast around a Laplace\napproximation in a transformed basis. The KTM can also be interpreted as a type\nof Gaussian process latent variable model, or as a topic model conditional on\ndocument features, uncovering links between earlier work in these areas.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2011 07:29:36 GMT"}], "update_date": "2011-10-24", "authors_parsed": [["Hennig", "Philipp", ""], ["Stern", "David", ""], ["Herbrich", "Ralf", ""], ["Graepel", "Thore", ""]]}, {"id": "1110.4784", "submitter": "Matthieu Cristelli", "authors": "Ilaria Bordino, Stefano Battiston, Guido Caldarelli, Matthieu\n  Cristelli, Antti Ukkonen, Ingmar Weber", "title": "Web search queries can predict stock market volumes", "comments": "29 pages, 11 figures, 11 tables + Supporting Information", "journal-ref": null, "doi": "10.1371/journal.pone.0040014", "report-no": null, "categories": "q-fin.ST cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We live in a computerized and networked society where many of our actions\nleave a digital trace and affect other people's actions. This has lead to the\nemergence of a new data-driven research field: mathematical methods of computer\nscience, statistical physics and sociometry provide insights on a wide range of\ndisciplines ranging from social science to human mobility. A recent important\ndiscovery is that query volumes (i.e., the number of requests submitted by\nusers to search engines on the www) can be used to track and, in some cases, to\nanticipate the dynamics of social phenomena. Successful exemples include\nunemployment levels, car and home sales, and epidemics spreading. Few recent\nworks applied this approach to stock prices and market sentiment. However, it\nremains unclear if trends in financial markets can be anticipated by the\ncollective wisdom of on-line users on the web. Here we show that trading\nvolumes of stocks traded in NASDAQ-100 are correlated with the volumes of\nqueries related to the same stocks. In particular, query volumes anticipate in\nmany cases peaks of trading by one day or more. Our analysis is carried out on\na unique dataset of queries, submitted to an important web search engine, which\nenable us to investigate also the user behavior. We show that the query volume\ndynamics emerges from the collective but seemingly uncoordinated activity of\nmany users. These findings contribute to the debate on the identification of\nearly warnings of financial systemic risk, based on the activity of users of\nthe www.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2011 13:15:59 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2012 14:07:49 GMT"}, {"version": "v3", "created": "Mon, 4 Jun 2012 15:42:35 GMT"}], "update_date": "2015-05-30", "authors_parsed": [["Bordino", "Ilaria", ""], ["Battiston", "Stefano", ""], ["Caldarelli", "Guido", ""], ["Cristelli", "Matthieu", ""], ["Ukkonen", "Antti", ""], ["Weber", "Ingmar", ""]]}, {"id": "1110.5051", "submitter": "Dell Zhang", "authors": "Dell Zhang", "title": "Wikipedia Edit Number Prediction based on Temporal Dynamics Only", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we describe our approach to the Wikipedia Participation\nChallenge which aims to predict the number of edits a Wikipedia editor will\nmake in the next 5 months. The best submission from our team, \"zeditor\",\nachieved 41.7% improvement over WMF's baseline predictive model and the final\nrank of 3rd place among 96 teams. An interesting characteristic of our approach\nis that only temporal dynamics features (i.e., how the number of edits changes\nin recent periods, etc.) are used in a self-supervised learning framework,\nwhich makes it easy to be generalised to other application domains.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2011 14:41:21 GMT"}], "update_date": "2011-10-25", "authors_parsed": [["Zhang", "Dell", ""]]}, {"id": "1110.5383", "submitter": "Hyokun Yun", "authors": "Hyokun Yun, S. V. N. Vishwanathan", "title": "Quilting Stochastic Kronecker Product Graphs to Generate Multiplicative\n  Attribute Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the first sub-quadratic sampling algorithm for the Multiplicative\nAttribute Graph Model (MAGM) of Kim and Leskovec (2010). We exploit the close\nconnection between MAGM and the Kronecker Product Graph Model (KPGM) of\nLeskovec et al. (2010), and show that to sample a graph from a MAGM it suffices\nto sample small number of KPGM graphs and \\emph{quilt} them together. Under a\nrestricted set of technical conditions our algorithm runs in $O((\\log_2(n))^3\n|E|)$ time, where $n$ is the number of nodes and $|E|$ is the number of edges\nin the sampled graph. We demonstrate the scalability of our algorithm via\nextensive empirical evaluation; we can sample a MAGM graph with 8 million nodes\nand 20 billion edges in under 6 hours.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2011 23:47:21 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2012 13:54:17 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Yun", "Hyokun", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1110.5447", "submitter": "Aurelien Garivier", "authors": "S\\'ebastien Bubeck, Damien Ernst, Aur\\'elien Garivier", "title": "Optimal discovery with probabilistic expert advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an original problem that arises from the issue of security\nanalysis of a power system and that we name optimal discovery with\nprobabilistic expert advice. We address it with an algorithm based on the\noptimistic paradigm and the Good-Turing missing mass estimator. We show that\nthis strategy uniformly attains the optimal discovery rate in a macroscopic\nlimit sense, under some assumptions on the probabilistic experts. We also\nprovide numerical experiments suggesting that this optimal behavior may still\nhold under weaker assumptions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2011 09:01:15 GMT"}], "update_date": "2011-10-26", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Ernst", "Damien", ""], ["Garivier", "Aur\u00e9lien", ""]]}, {"id": "1110.5667", "submitter": "Noah Goodman", "authors": "Irvin Hwang, Andreas Stuhlm\\\"uller, Noah D. Goodman", "title": "Inducing Probabilistic Programs by Bayesian Program Merging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report outlines an approach to learning generative models from data. We\nexpress models as probabilistic programs, which allows us to capture abstract\npatterns within the examples. By choosing our language for programs to be an\nextension of the algebraic data type of the examples, we can begin with a\nprogram that generates all and only the examples. We then introduce greater\nabstraction, and hence generalization, incrementally to the extent that it\nimproves the posterior probability of the examples given the program. Motivated\nby previous approaches to model merging and program induction, we search for\nsuch explanatory abstractions using program transformations. We consider two\ntypes of transformation: Abstraction merges common subexpressions within a\nprogram into new functions (a form of anti-unification). Deargumentation\nsimplifies functions by reducing the number of arguments. We demonstrate that\nthis approach finds key patterns in the domain of nested lists, including\nparameterized sub-functions and stochastic recursion.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2011 21:06:39 GMT"}], "update_date": "2011-10-27", "authors_parsed": [["Hwang", "Irvin", ""], ["Stuhlm\u00fcller", "Andreas", ""], ["Goodman", "Noah D.", ""]]}, {"id": "1110.5688", "submitter": "Nicholas M. Ball", "authors": "Nicholas M. Ball (Herzberg Institute of Astrophysics, Victoria, BC,\n  Canada)", "title": "Discussion on \"Techniques for Massive-Data Machine Learning in\n  Astronomy\" by A. Gray", "comments": "6 pages, 1 figure. Invited commentary, Statistical Challenges in\n  Modern Astronomy V, Penn State, Jun 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astronomy is increasingly encountering two fundamental truths: (1) The field\nis faced with the task of extracting useful information from extremely large,\ncomplex, and high dimensional datasets; (2) The techniques of astroinformatics\nand astrostatistics are the only way to make this tractable, and bring the\nrequired level of sophistication to the analysis. Thus, an approach which\nprovides these tools in a way that scales to these datasets is not just\ndesirable, it is vital. The expertise required spans not just astronomy, but\nalso computer science, statistics, and informatics. As a computer scientist and\nexpert in machine learning, Alex's contribution of expertise and a large number\nof fast algorithms designed to scale to large datasets, is extremely welcome.\nWe focus in this discussion on the questions raised by the practical\napplication of these algorithms to real astronomical datasets. That is, what is\nneeded to maximally leverage their potential to improve the science return?\nThis is not a trivial task. While computing and statistical expertise are\nrequired, so is astronomical expertise. Precedent has shown that, to-date, the\ncollaborations most productive in producing astronomical science results (e.g,\nthe Sloan Digital Sky Survey), have either involved astronomers expert in\ncomputer science and/or statistics, or astronomers involved in close, long-term\ncollaborations with experts in those fields. This does not mean that the\nastronomers are giving the most important input, but simply that their input is\ncrucial in guiding the effort in the most fruitful directions, and coping with\nthe issues raised by real data. Thus, the tools must be useable and\nunderstandable by those whose primary expertise is not computing or statistics,\neven though they may have quite extensive knowledge of those fields.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2011 00:22:36 GMT"}], "update_date": "2011-10-28", "authors_parsed": [["Ball", "Nicholas M.", "", "Herzberg Institute of Astrophysics, Victoria, BC,\n  Canada"]]}, {"id": "1110.6084", "submitter": "Vianney Perchet", "authors": "Vianney Perchet, Philippe Rigollet", "title": "The multi-armed bandit problem with covariates", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1101 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 2, 693-721", "doi": "10.1214/13-AOS1101", "report-no": "IMS-AOS-AOS1101", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multi-armed bandit problem in a setting where each arm produces\na noisy reward realization which depends on an observable random covariate. As\nopposed to the traditional static multi-armed bandit problem, this setting\nallows for dynamically changing rewards that better describe applications where\nside information is available. We adopt a nonparametric model where the\nexpected rewards are smooth functions of the covariate and where the hardness\nof the problem is captured by a margin parameter. To maximize the expected\ncumulative reward, we introduce a policy called Adaptively Binned Successive\nElimination (abse) that adaptively decomposes the global problem into suitably\n\"localized\" static bandit problems. This policy constructs an adaptive\npartition using a variant of the Successive Elimination (se) policy. Our\nresults include sharper regret bounds for the se policy in a static bandit\nproblem and minimax optimal regret bounds for the abse policy in the dynamic\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2011 14:09:12 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2012 21:38:07 GMT"}, {"version": "v3", "created": "Fri, 24 May 2013 09:35:28 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Perchet", "Vianney", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1110.6287", "submitter": "Micha{\\l} Cholewa", "authors": "Micha{\\l} Cholewa and Przemys{\\l}aw G{\\l}omb", "title": "Deciding of HMM parameters based on number of critical points for\n  gesture recognition from motion capture data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method of choosing number of states of a HMM based on\nnumber of critical points of the motion capture data. The choice of Hidden\nMarkov Models(HMM) parameters is crucial for recognizer's performance as it is\nthe first step of the training and cannot be corrected automatically within\nHMM. In this article we define predictor of number of states based on number of\ncritical points of the sequence and test its effectiveness against sample data.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2011 10:20:25 GMT"}], "update_date": "2011-10-31", "authors_parsed": [["Cholewa", "Micha\u0142", ""], ["G\u0142omb", "Przemys\u0142aw", ""]]}, {"id": "1110.6755", "submitter": "Yevgeny Seldin", "authors": "Yevgeny Seldin, Nicol\\`o Cesa-Bianchi, Peter Auer, Fran\\c{c}ois\n  Laviolette, John Shawe-Taylor", "title": "PAC-Bayes-Bernstein Inequality for Martingales and its Application to\n  Multiarmed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new tool for data-dependent analysis of the\nexploration-exploitation trade-off in learning under limited feedback. Our tool\nis based on two main ingredients. The first ingredient is a new concentration\ninequality that makes it possible to control the concentration of weighted\naverages of multiple (possibly uncountably many) simultaneously evolving and\ninterdependent martingales. The second ingredient is an application of this\ninequality to the exploration-exploitation trade-off via importance weighted\nsampling. We apply the new tool to the stochastic multiarmed bandit problem,\nhowever, the main importance of this paper is the development and understanding\nof the new tool rather than improvement of existing algorithms for stochastic\nmultiarmed bandits. In the follow-up work we demonstrate that the new tool can\nimprove over state-of-the-art in structurally richer problems, such as\nstochastic multiarmed bandits with side information (Seldin et al., 2011a).\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 11:36:49 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2012 15:46:58 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Seldin", "Yevgeny", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""], ["Auer", "Peter", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1110.6886", "submitter": "Yevgeny Seldin", "authors": "Yevgeny Seldin, Fran\\c{c}ois Laviolette, Nicol\\`o Cesa-Bianchi, John\n  Shawe-Taylor, Peter Auer", "title": "PAC-Bayesian Inequalities for Martingales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a set of high-probability inequalities that control the\nconcentration of weighted averages of multiple (possibly uncountably many)\nsimultaneously evolving and interdependent martingales. Our results extend the\nPAC-Bayesian analysis in learning theory from the i.i.d. setting to martingales\nopening the way for its application to importance weighted sampling,\nreinforcement learning, and other interactive learning domains, as well as many\nother domains in probability theory and statistics, where martingales are\nencountered.\n  We also present a comparison inequality that bounds the expectation of a\nconvex function of a martingale difference sequence shifted to the [0,1]\ninterval by the expectation of the same function of independent Bernoulli\nvariables. This inequality is applied to derive a tighter analog of\nHoeffding-Azuma's inequality.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 18:22:24 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2012 11:56:07 GMT"}, {"version": "v3", "created": "Mon, 30 Jul 2012 14:02:53 GMT"}], "update_date": "2012-07-31", "authors_parsed": [["Seldin", "Yevgeny", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""], ["Shawe-Taylor", "John", ""], ["Auer", "Peter", ""]]}]