[{"id": "1005.0027", "submitter": "Maayan Harel Maayan Harel", "authors": "Maayan Harel and Shie Mannor", "title": "Learning from Multiple Outlooks", "comments": "with full proofs of theorems and all experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel problem formulation of learning a single task when the\ndata are provided in different feature spaces. Each such space is called an\noutlook, and is assumed to contain both labeled and unlabeled data. The\nobjective is to take advantage of the data from all the outlooks to better\nclassify each of the outlooks. We devise an algorithm that computes optimal\naffine mappings from different outlooks to a target outlook by matching moments\nof the empirical distributions. We further derive a probabilistic\ninterpretation of the resulting algorithm and a sample complexity bound\nindicating how many samples are needed to adequately find the mapping. We\nreport the results of extensive experiments on activity recognition tasks that\nshow the value of the proposed approach in boosting performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2010 21:52:17 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2011 06:56:25 GMT"}], "update_date": "2011-06-15", "authors_parsed": [["Harel", "Maayan", ""], ["Mannor", "Shie", ""]]}, {"id": "1005.0047", "submitter": "Arvind Agarwal", "authors": "Arvind Agarwal and Hal Daume III", "title": "A Geometric View of Conjugate Priors", "comments": "16 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian machine learning, conjugate priors are popular, mostly due to\nmathematical convenience. In this paper, we show that there are deeper reasons\nfor choosing a conjugate prior. Specifically, we formulate the conjugate prior\nin the form of Bregman divergence and show that it is the inherent geometry of\nconjugate priors that makes them appropriate and intuitive. This geometric\ninterpretation allows one to view the hyperparameters of conjugate priors as\nthe {\\it effective} sample points, thus providing additional intuition. We use\nthis geometric understanding of conjugate priors to derive the hyperparameters\nand expression of the prior used to couple the generative and discriminative\ncomponents of a hybrid model for semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2010 06:06:36 GMT"}], "update_date": "2010-05-04", "authors_parsed": [["Agarwal", "Arvind", ""], ["Daume", "Hal", "III"]]}, {"id": "1005.0063", "submitter": "Manas Pathak", "authors": "Manas A. Pathak and Bhiksha Raj", "title": "Large Margin Multiclass Gaussian Classification with Differential\n  Privacy", "comments": "14 pages", "journal-ref": "Proceedings of the ECML/PKDD Workshop on Privacy and Security\n  issues in Data Mining and Machine Learning, 2010", "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As increasing amounts of sensitive personal information is aggregated into\ndata repositories, it has become important to develop mechanisms for processing\nthe data without revealing information about individual data instances. The\ndifferential privacy model provides a framework for the development and\ntheoretical analysis of such mechanisms. In this paper, we propose an algorithm\nfor learning a discriminatively trained multi-class Gaussian classifier that\nsatisfies differential privacy using a large margin loss function with a\nperturbed regularization term. We present a theoretical upper bound on the\nexcess risk of the classifier introduced by the perturbation.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2010 11:06:12 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2010 23:24:04 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Pathak", "Manas A.", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1005.0075", "submitter": "Ying Cui", "authors": "Ying Cui and Vincent K.N.Lau", "title": "Distributive Stochastic Learning for Delay-Optimal OFDMA Power and\n  Subband Allocation", "comments": "To appear in Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2010.2050062", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the distributive queue-aware power and subband\nallocation design for a delay-optimal OFDMA uplink system with one base\nstation, $K$ users and $N_F$ independent subbands. Each mobile has an uplink\nqueue with heterogeneous packet arrivals and delay requirements. We model the\nproblem as an infinite horizon average reward Markov Decision Problem (MDP)\nwhere the control actions are functions of the instantaneous Channel State\nInformation (CSI) as well as the joint Queue State Information (QSI). To\naddress the distributive requirement and the issue of exponential memory\nrequirement and computational complexity, we approximate the subband allocation\nQ-factor by the sum of the per-user subband allocation Q-factor and derive a\ndistributive online stochastic learning algorithm to estimate the per-user\nQ-factor and the Lagrange multipliers (LM) simultaneously and determine the\ncontrol actions using an auction mechanism. We show that under the proposed\nauction mechanism, the distributive online learning converges almost surely\n(with probability 1). For illustration, we apply the proposed distributive\nstochastic learning framework to an application example with exponential packet\nsize distribution. We show that the delay-optimal power control has the {\\em\nmulti-level water-filling} structure where the CSI determines the instantaneous\npower allocation and the QSI determines the water-level. The proposed algorithm\nhas linear signaling overhead and computational complexity $\\mathcal O(KN)$,\nwhich is desirable from an implementation perspective.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2010 13:57:15 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Cui", "Ying", ""], ["Lau", "Vincent K. N.", ""]]}, {"id": "1005.0125", "submitter": "Dotan Di Castro", "authors": "Dotan Di Castro and Shie Mannor", "title": "Adaptive Bases for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reinforcement learning using function\napproximation, where the approximating basis can change dynamically while\ninteracting with the environment. A motivation for such an approach is\nmaximizing the value function fitness to the problem faced. Three errors are\nconsidered: approximation square error, Bellman residual, and projected Bellman\nresidual. Algorithms under the actor-critic framework are presented, and shown\nto converge. The advantage of such an adaptive basis is demonstrated in\nsimulations.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2010 06:40:21 GMT"}], "update_date": "2010-05-04", "authors_parsed": [["Di Castro", "Dotan", ""], ["Mannor", "Shie", ""]]}, {"id": "1005.0188", "submitter": "Nishant Mehta", "authors": "Nishant A. Mehta and Alexander G. Gray", "title": "Generative and Latent Mean Map Kernels", "comments": "16 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two kernels that extend the mean map, which embeds probability\nmeasures in Hilbert spaces. The generative mean map kernel (GMMK) is a smooth\nsimilarity measure between probabilistic models. The latent mean map kernel\n(LMMK) generalizes the non-iid formulation of Hilbert space embeddings of\nempirical distributions in order to incorporate latent variable models. When\ncomparing certain classes of distributions, the GMMK exhibits beneficial\nregularization and generalization properties not shown for previous generative\nkernels. We present experiments comparing support vector machine performance\nusing the GMMK and LMMK between hidden Markov models to the performance of\nother methods on discrete and continuous observation sequence data. The results\nsuggest that, in many cases, the GMMK has generalization error competitive with\nor better than other methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2010 05:59:41 GMT"}], "update_date": "2010-05-04", "authors_parsed": [["Mehta", "Nishant A.", ""], ["Gray", "Alexander G.", ""]]}, {"id": "1005.0340", "submitter": "Moazzam Tiwana", "authors": "Moazzam Islam Tiwana, Berna Sayrac and Zwi Altman", "title": "Statistical Learning in Automated Troubleshooting: Application to LTE\n  Interference Mitigation", "comments": "IEEE Transactions On Vehicular Technology 2010 IEEE transactions on\n  vehicular technology", "journal-ref": null, "doi": "10.1109/TVT.2010.2050081", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper presents a method for automated healing as part of off-line\nautomated troubleshooting. The method combines statistical learning with\nconstraint optimization. The automated healing aims at locally optimizing radio\nresource management (RRM) or system parameters of cells with poor performance\nin an iterative manner. The statistical learning processes the data using\nLogistic Regression (LR) to extract closed form (functional) relations between\nKey Performance Indicators (KPIs) and Radio Resource Management (RRM)\nparameters. These functional relations are then processed by an optimization\nengine which proposes new parameter values. The advantage of the proposed\nformulation is the small number of iterations required by the automated healing\nmethod to converge, making it suitable for off-line implementation. The\nproposed method is applied to heal an Inter-Cell Interference Coordination\n(ICIC) process in a 3G Long Term Evolution (LTE) network which is based on\nsoft-frequency reuse scheme. Numerical simulations illustrate the benefits of\nthe proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2010 16:35:49 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Tiwana", "Moazzam Islam", ""], ["Sayrac", "Berna", ""], ["Altman", "Zwi", ""]]}, {"id": "1005.0390", "submitter": "Adam Gauci", "authors": "Adam Gauci, Kristian Zarb Adami, John Abela", "title": "Machine Learning for Galaxy Morphology Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.GA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, decision tree learning algorithms and fuzzy inferencing systems\nare applied for galaxy morphology classification. In particular, the CART, the\nC4.5, the Random Forest and fuzzy logic algorithms are studied and reliable\nclassifiers are developed to distinguish between spiral galaxies, elliptical\ngalaxies or star/unknown galactic objects. Morphology information for the\ntraining and testing datasets is obtained from the Galaxy Zoo project while the\ncorresponding photometric and spectra parameters are downloaded from the SDSS\nDR7 catalogue.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2010 20:01:38 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2010 07:54:29 GMT"}], "update_date": "2010-06-02", "authors_parsed": [["Gauci", "Adam", ""], ["Adami", "Kristian Zarb", ""], ["Abela", "John", ""]]}, {"id": "1005.0437", "submitter": "Marius Kloft", "authors": "Marius Kloft, Ulrich R\\\"uckert and Peter L. Bartlett", "title": "A Unifying View of Multiple Kernel Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on multiple kernel learning has lead to a number of\napproaches for combining kernels in regularized risk minimization. The proposed\napproaches include different formulations of objectives and varying\nregularization strategies. In this paper we present a unifying general\noptimization criterion for multiple kernel learning and show how existing\nformulations are subsumed as special cases. We also derive the criterion's dual\nrepresentation, which is suitable for general smooth optimization algorithms.\nFinally, we evaluate multiple kernel learning in this framework analytically\nusing a Rademacher complexity bound on the generalization error and empirically\nin a set of experiments.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2010 06:05:51 GMT"}], "update_date": "2010-05-05", "authors_parsed": [["Kloft", "Marius", ""], ["R\u00fcckert", "Ulrich", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1005.0530", "submitter": "Mohak Shah", "authors": "Mohak Shah, Mario Marchand and Jacques Corbeil", "title": "Feature Selection with Conjunctions of Decision Stumps and Learning from\n  Microarray Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the objectives of designing feature selection learning algorithms is\nto obtain classifiers that depend on a small number of attributes and have\nverifiable future performance guarantees. There are few, if any, approaches\nthat successfully address the two goals simultaneously. Performance guarantees\nbecome crucial for tasks such as microarray data analysis due to very small\nsample sizes resulting in limited empirical evaluation. To the best of our\nknowledge, such algorithms that give theoretical bounds on the future\nperformance have not been proposed so far in the context of the classification\nof gene expression data. In this work, we investigate the premise of learning a\nconjunction (or disjunction) of decision stumps in Occam's Razor, Sample\nCompression, and PAC-Bayes learning settings for identifying a small subset of\nattributes that can be used to perform reliable classification tasks. We apply\nthe proposed approaches for gene identification from DNA microarray data and\ncompare our results to those of well known successful approaches proposed for\nthe task. We show that our algorithm not only finds hypotheses with much\nsmaller number of genes while giving competitive classification accuracy but\nalso have tight risk guarantees on future performance unlike other approaches.\nThe proposed approaches are general and extensible in terms of both designing\nnovel algorithms and application to other domains.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2010 14:01:10 GMT"}], "update_date": "2010-05-05", "authors_parsed": [["Shah", "Mohak", ""], ["Marchand", "Mario", ""], ["Corbeil", "Jacques", ""]]}, {"id": "1005.0794", "submitter": "Xiaoran Yan", "authors": "Xiaoran Yan, Yaojia Zhu, Jean-Baptiste Rouquier, and Cristopher Moore", "title": "Active Learning for Hidden Attributes in Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.IT cs.LG math.IT physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many networks, vertices have hidden attributes, or types, that are\ncorrelated with the networks topology. If the topology is known but these\nattributes are not, and if learning the attributes is costly, we need a method\nfor choosing which vertex to query in order to learn as much as possible about\nthe attributes of the other vertices. We assume the network is generated by a\nstochastic block model, but we make no assumptions about its assortativity or\ndisassortativity. We choose which vertex to query using two methods: 1)\nmaximizing the mutual information between its attributes and those of the\nothers (a well-known approach in active learning) and 2) maximizing the average\nagreement between two independent samples of the conditional Gibbs\ndistribution. Experimental results show that both these methods do much better\nthan simple heuristics. They also consistently identify certain vertices as\nimportant by querying them early on.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2010 17:11:26 GMT"}], "update_date": "2010-05-25", "authors_parsed": [["Yan", "Xiaoran", ""], ["Zhu", "Yaojia", ""], ["Rouquier", "Jean-Baptiste", ""], ["Moore", "Cristopher", ""]]}, {"id": "1005.0826", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko (INRIA Lille - Nord Europe)", "title": "Clustering processes", "comments": "in proceedings of ICML 2010. arXiv-admin Note: This is a newer\n  version of the article arXiv:1004.5194v1, please see that article for any\n  previous version", "journal-ref": "27th International Conference on Machine Learning (2010) 919-926", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of clustering is considered, for the case when each data point is\na sample generated by a stationary ergodic process. We propose a very natural\nasymptotic notion of consistency, and show that simple consistent algorithms\nexist, under most general non-parametric assumptions. The notion of consistency\nis as follows: two samples should be put into the same cluster if and only if\nthey were generated by the same distribution. With this notion of consistency,\nclustering generalizes such classical statistical problems as homogeneity\ntesting and process classification. We show that, for the case of a known\nnumber of clusters, consistency can be achieved under the only assumption that\nthe joint distribution of the data is stationary ergodic (no parametric or\nMarkovian assumptions, no assumptions of independence, neither between nor\nwithin the samples). If the number of clusters is unknown, consistency can be\nachieved under appropriate assumptions on the mixing rates of the processes.\n(again, no parametric or independence assumptions). In both cases we give\nexamples of simple (at most quadratic in each argument) algorithms which are\nconsistent.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2010 19:41:55 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2013 09:32:30 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Ryabko", "Daniil", "", "INRIA Lille - Nord Europe"]]}, {"id": "1005.0897", "submitter": "Pantelis Bouboulis", "authors": "Pantelis Bouboulis and Sergios Theodoridis", "title": "The Complex Gaussian Kernel LMS algorithm", "comments": "10 pages, 3 figures Manuscript submitted to ICANN 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the real reproducing kernels are used in an increasing number of\nmachine learning problems, complex kernels have not, yet, been used, in spite\nof their potential interest in applications such as communications. In this\nwork, we focus our attention on the complex gaussian kernel and its possible\napplication in the complex Kernel LMS algorithm. In order to derive the\ngradients needed to develop the complex kernel LMS (CKLMS), we employ the\npowerful tool of Wirtinger's Calculus, which has recently attracted much\nattention in the signal processing community. Writinger's calculus simplifies\ncomputations and offers an elegant tool for treating complex signals. To this\nend, the notion of Writinger's calculus is extended to include complex RKHSs.\nExperiments verify that the CKLMS offers significant performance improvements\nover the traditional complex LMS or Widely Linear complex LMS (WL-LMS)\nalgorithms, when dealing with nonlinearities.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2010 06:42:51 GMT"}], "update_date": "2010-05-07", "authors_parsed": [["Bouboulis", "Pantelis", ""], ["Theodoridis", "Sergios", ""]]}, {"id": "1005.0902", "submitter": "Pantelis Bouboulis", "authors": "Pantelis Bouboulis, Sergios Theodoridis", "title": "Extension of Wirtinger Calculus in RKH Spaces and the Complex Kernel LMS", "comments": "6 pages, 3 figures manuscript submitted to MLSP 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, kernel methods for nonlinear processing have\nsuccessfully been used in the machine learning community. However, so far, the\nemphasis has been on batch techniques. It is only recently, that online\nadaptive techniques have been considered in the context of signal processing\ntasks. To the best of our knowledge, no kernel-based strategy has been\ndeveloped, so far, that is able to deal with complex valued signals. In this\npaper, we take advantage of a technique called complexification of real RKHSs\nto attack this problem. In order to derive gradients and subgradients of\noperators that need to be defined on the associated complex RKHSs, we employ\nthe powerful tool ofWirtinger's Calculus, which has recently attracted much\nattention in the signal processing community. Writinger's calculus simplifies\ncomputations and offers an elegant tool for treating complex signals. To this\nend, in this paper, the notion of Writinger's calculus is extended, for the\nfirst time, to include complex RKHSs and use it to derive the Complex Kernel\nLeast-Mean-Square (CKLMS) algorithm. Experiments verify that the CKLMS can be\nused to derive nonlinear stable algorithms, which offer significant performance\nimprovements over the traditional complex LMS orWidely Linear complex LMS\n(WL-LMS) algorithms, when dealing with nonlinearities.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2010 06:59:22 GMT"}, {"version": "v2", "created": "Tue, 25 May 2010 17:57:00 GMT"}], "update_date": "2010-05-26", "authors_parsed": [["Bouboulis", "Pantelis", ""], ["Theodoridis", "Sergios", ""]]}, {"id": "1005.1120", "submitter": "Sumit Ganguly", "authors": "Sumit Ganguly", "title": "Estimating small moments of data stream in nearly optimal space-time", "comments": "Withdrawn due to error in analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For each $p \\in (0,2]$, we present a randomized algorithm that returns an\n$\\epsilon$-approximation of the $p$th frequency moment of a data stream $F_p =\n\\sum_{i = 1}^n \\abs{f_i}^p$. The algorithm requires space $O(\\epsilon^{-2} \\log\n(mM)(\\log n))$ and processes each stream update using time $O((\\log n) (\\log\n\\epsilon^{-1}))$. It is nearly optimal in terms of space (lower bound\n$O(\\epsilon^{-2} \\log (mM))$ as well as time and is the first algorithm with\nthese properties. The technique separates heavy hitters from the remaining\nitems in the stream using an appropriate threshold and estimates the\ncontribution of the heavy hitters and the light elements to $F_p$ separately. A\nkey component is the design of an unbiased estimator for $\\abs{f_i}^p$ whose\ndata structure has low update time and low variance.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2010 02:29:27 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2010 17:39:04 GMT"}], "update_date": "2010-06-21", "authors_parsed": [["Ganguly", "Sumit", ""]]}, {"id": "1005.1545", "submitter": "Zhi-Hua Zhou", "authors": "Yu-Feng Li, Zhi-Hua Zhou", "title": "Improving Semi-Supervised Support Vector Machines Through Unlabeled\n  Instances Selection", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised support vector machines (S3VMs) are a kind of popular\napproaches which try to improve learning performance by exploiting unlabeled\ndata. Though S3VMs have been found helpful in many situations, they may\ndegenerate performance and the resultant generalization ability may be even\nworse than using the labeled data only. In this paper, we try to reduce the\nchance of performance degeneration of S3VMs. Our basic idea is that, rather\nthan exploiting all unlabeled data, the unlabeled instances should be selected\nsuch that only the ones which are very likely to be helpful are exploited,\nwhile some highly risky unlabeled instances are avoided. We propose the\nS3VM-\\emph{us} method by using hierarchical clustering to select the unlabeled\ninstances. Experiments on a broad range of data sets over eighty-eight\ndifferent settings show that the chance of performance degeneration of\nS3VM-\\emph{us} is much smaller than that of existing S3VMs.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2010 13:49:01 GMT"}, {"version": "v2", "created": "Mon, 9 May 2011 13:08:45 GMT"}], "update_date": "2011-05-10", "authors_parsed": [["Li", "Yu-Feng", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1005.1918", "submitter": "Alexey Chernov", "authors": "Alexey Chernov and Fedor Zhdanov", "title": "Prediction with Expert Advice under Discounted Loss", "comments": "26 pages; expanded (2 remarks -> theorems), some misprints corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study prediction with expert advice in the setting where the losses are\naccumulated with some discounting---the impact of old losses may gradually\nvanish. We generalize the Aggregating Algorithm and the Aggregating Algorithm\nfor Regression to this case, propose a suitable new variant of exponential\nweights algorithm, and prove respective loss bounds.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2010 19:27:35 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2010 19:13:37 GMT"}], "update_date": "2010-06-07", "authors_parsed": [["Chernov", "Alexey", ""], ["Zhdanov", "Fedor", ""]]}, {"id": "1005.2146", "submitter": "Ankan Saha", "authors": "Ankan Saha and Ambuj Tewari", "title": "On the Finite Time Convergence of Cyclic Coordinate Descent Methods", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyclic coordinate descent is a classic optimization method that has witnessed\na resurgence of interest in machine learning. Reasons for this include its\nsimplicity, speed and stability, as well as its competitive performance on\n$\\ell_1$ regularized smooth optimization problems. Surprisingly, very little is\nknown about its finite time convergence behavior on these problems. Most\nexisting results either just prove convergence or provide asymptotic rates. We\nfill this gap in the literature by proving $O(1/k)$ convergence rates (where\n$k$ is the iteration counter) for two variants of cyclic coordinate descent\nunder an isotonicity assumption. Our analysis proceeds by comparing the\nobjective values attained by the two variants with each other, as well as with\nthe gradient descent algorithm. We show that the iterates generated by the\ncyclic coordinate descent methods remain better than those of gradient descent\nuniformly over time.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2010 16:25:46 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Saha", "Ankan", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1005.2179", "submitter": "Zhongmou Li", "authors": "Zhongmou Li, Hui Xiong, Yanchi Liu", "title": "Detecting Blackholes and Volcanoes in Directed Networks", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we formulate a novel problem for finding blackhole and volcano\npatterns in a large directed graph. Specifically, a blackhole pattern is a\ngroup which is made of a set of nodes in a way such that there are only inlinks\nto this group from the rest nodes in the graph. In contrast, a volcano pattern\nis a group which only has outlinks to the rest nodes in the graph. Both\npatterns can be observed in real world. For instance, in a trading network, a\nblackhole pattern may represent a group of traders who are manipulating the\nmarket. In the paper, we first prove that the blackhole mining problem is a\ndual problem of finding volcanoes. Therefore, we focus on finding the blackhole\npatterns. Along this line, we design two pruning schemes to guide the blackhole\nfinding process. In the first pruning scheme, we strategically prune the search\nspace based on a set of pattern-size-independent pruning rules and develop an\niBlackhole algorithm. The second pruning scheme follows a divide-and-conquer\nstrategy to further exploit the pruning results from the first pruning scheme.\nIndeed, a target directed graphs can be divided into several disconnected\nsubgraphs by the first pruning scheme, and thus the blackhole finding can be\nconducted in each disconnected subgraph rather than in a large graph. Based on\nthese two pruning schemes, we also develop an iBlackhole-DC algorithm. Finally,\nexperimental results on real-world data show that the iBlackhole-DC algorithm\ncan be several orders of magnitude faster than the iBlackhole algorithm, which\nhas a huge computational advantage over a brute-force method.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2010 19:53:29 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Li", "Zhongmou", ""], ["Xiong", "Hui", ""], ["Liu", "Yanchi", ""]]}, {"id": "1005.2243", "submitter": "Huan Xu Dr.", "authors": "Huan Xu, Shie Mannor", "title": "Robustness and Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive generalization bounds for learning algorithms based on their\nrobustness: the property that if a testing sample is \"similar\" to a training\nsample, then the testing error is close to the training error. This provides a\nnovel approach, different from the complexity or stability arguments, to study\ngeneralization of learning algorithms. We further show that a weak notion of\nrobustness is both sufficient and necessary for generalizability, which implies\nthat robustness is a fundamental property for learning algorithms to work.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2010 01:59:57 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Xu", "Huan", ""], ["Mannor", "Shie", ""]]}, {"id": "1005.2263", "submitter": "Christos Dimitrakakis", "authors": "Christos Dimitrakakis", "title": "Context models on sequences of covers", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a class of models that, via a simple construction, enables exact,\nincremental, non-parametric, polynomial-time, Bayesian inference of conditional\nmeasures. The approach relies upon creating a sequence of covers on the\nconditioning variable and maintaining a different model for each set within a\ncover. Inference remains tractable by specifying the probabilistic model in\nterms of a random walk within the sequence of covers. We demonstrate the\napproach on problems of conditional density estimation, which, to our knowledge\nis the first closed-form, non-parametric Bayesian approach to this problem.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2010 07:02:54 GMT"}, {"version": "v2", "created": "Mon, 30 May 2011 11:22:44 GMT"}], "update_date": "2011-05-31", "authors_parsed": [["Dimitrakakis", "Christos", ""]]}, {"id": "1005.2296", "submitter": "Ohad Shamir", "authors": "Nicol\\`o Cesa-Bianchi, Shai Shalev-Shwartz and Ohad Shamir", "title": "Online Learning of Noisy Data with Kernels", "comments": "This is a full version of the paper appearing in the 23rd\n  International Conference on Learning Theory (COLT 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online learning when individual instances are corrupted by\nadversarially chosen random noise. We assume the noise distribution is unknown,\nand may change over time with no restriction other than having zero mean and\nbounded variance. Our technique relies on a family of unbiased estimators for\nnon-linear functions, which may be of independent interest. We show that a\nvariant of online gradient descent can learn functions in any dot-product\n(e.g., polynomial) or Gaussian kernel space with any analytic convex loss\nfunction. Our variant uses randomized estimates that need to query a random\nnumber of noisy copies of each instance, where with high probability this\nnumber is upper bounded by a constant. Allowing such multiple queries cannot be\navoided: Indeed, we show that online learning is in general impossible when\nonly one noisy copy of each instance can be accessed.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2010 10:56:01 GMT"}, {"version": "v2", "created": "Thu, 20 May 2010 12:43:57 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Cesa-Bianchi", "Nicol\u00f2", ""], ["Shalev-Shwartz", "Shai", ""], ["Shamir", "Ohad", ""]]}, {"id": "1005.2364", "submitter": "Volker Nannen", "authors": "Volker Nannen", "title": "A Short Introduction to Model Selection, Kolmogorov Complexity and\n  Minimum Description Length (MDL)", "comments": "20 pages, Chapter 1 of The Paradox of Overfitting, Master's thesis,\n  Rijksuniversiteit Groningen, 2003", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of overfitting in model selection is explained and demonstrated\nwith an example. After providing some background information on information\ntheory and Kolmogorov complexity, we provide a short explanation of Minimum\nDescription Length and error minimization. We conclude with a discussion of the\ntypical features of overfitting in model selection.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2010 15:59:01 GMT"}, {"version": "v2", "created": "Fri, 14 May 2010 11:28:03 GMT"}], "update_date": "2010-05-17", "authors_parsed": [["Nannen", "Volker", ""]]}, {"id": "1005.2603", "submitter": "Andri Mirzal", "authors": "Andri Mirzal and Masashi Furukawa", "title": "Eigenvectors for clustering: Unipartite, bipartite, and directed graph\n  cases", "comments": "9 pages, no figure, to appear in ICEIE 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper presents a concise tutorial on spectral clustering for broad\nspectrum graphs which include unipartite (undirected) graph, bipartite graph,\nand directed graph. We show how to transform bipartite graph and directed graph\ninto corresponding unipartite graph, therefore allowing a unified treatment to\nall cases. In bipartite graph, we show that the relaxed solution to the $K$-way\nco-clustering can be found by computing the left and right eigenvectors of the\ndata matrix. This gives a theoretical basis for $K$-way spectral co-clustering\nalgorithms proposed in the literatures. We also show that solving row and\ncolumn co-clustering is equivalent to solving row and column clustering\nseparately, thus giving a theoretical support for the claim: ``column\nclustering implies row clustering and vice versa''. And in the last part, we\ngeneralize the Ky Fan theorem---which is the central theorem for explaining\nspectral clustering---to rectangular complex matrix motivated by the results\nfrom bipartite graph analysis.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2010 18:52:24 GMT"}, {"version": "v2", "created": "Mon, 17 May 2010 06:19:08 GMT"}, {"version": "v3", "created": "Mon, 24 May 2010 08:17:11 GMT"}, {"version": "v4", "created": "Sat, 12 Jun 2010 03:02:35 GMT"}, {"version": "v5", "created": "Mon, 12 Jul 2010 13:38:57 GMT"}], "update_date": "2010-07-13", "authors_parsed": [["Mirzal", "Andri", ""], ["Furukawa", "Masashi", ""]]}, {"id": "1005.2638", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh and Pedro Contreras", "title": "Hierarchical Clustering for Finding Symmetries and Other Patterns in\n  Massive, High Dimensional Datasets", "comments": "41 pages, 13 figures, 6 tables. 81 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysis and data mining are concerned with unsupervised pattern finding\nand structure determination in data sets. \"Structure\" can be understood as\nsymmetry and a range of symmetries are expressed by hierarchy. Such symmetries\ndirectly point to invariants, that pinpoint intrinsic properties of the data\nand of the background empirical domain of interest. We review many aspects of\nhierarchy here, including ultrametric topology, generalized ultrametric,\nlinkages with lattices and other discrete algebraic structures and with p-adic\nnumber representations. By focusing on symmetries in data we have a powerful\nmeans of structuring and analyzing massive, high dimensional data stores. We\nillustrate the powerfulness of hierarchical clustering in case studies in\nchemistry and finance, and we provide pointers to other published case studies.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2010 23:12:03 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Murtagh", "Fionn", ""], ["Contreras", "Pedro", ""]]}, {"id": "1005.2714", "submitter": "James P. Crutchfield", "authors": "James P. Crutchfield and Sean Whalen", "title": "Structural Drift: The Population Dynamics of Sequential Learning", "comments": "15 pages, 9 figures;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/sdrift.htm", "journal-ref": null, "doi": null, "report-no": "Santa Fe Institute Working Paper 10-05", "categories": "q-bio.PE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a theory of sequential causal inference in which learners in a\nchain estimate a structural model from their upstream teacher and then pass\nsamples from the model to their downstream student. It extends the population\ndynamics of genetic drift, recasting Kimura's selectively neutral theory as a\nspecial case of a generalized drift process using structured populations with\nmemory. We examine the diffusion and fixation properties of several drift\nprocesses and propose applications to learning, inference, and evolution. We\nalso demonstrate how the organization of drift process space controls fidelity,\nfacilitates innovations, and leads to information loss in sequential learning\nwith and without memory.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2010 23:50:50 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2012 18:09:51 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Crutchfield", "James P.", ""], ["Whalen", "Sean", ""]]}, {"id": "1005.3566", "submitter": "Varun Kanade", "authors": "Varun Kanade, Leslie G. Valiant and Jennifer Wortman Vaughan", "title": "Evolution with Drifting Targets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the question of the stability of evolutionary algorithms to\ngradual changes, or drift, in the target concept. We define an algorithm to be\nresistant to drift if, for some inverse polynomial drift rate in the target\nfunction, it converges to accuracy 1 -- \\epsilon , with polynomial resources,\nand then stays within that accuracy indefinitely, except with probability\n\\epsilon , at any one time. We show that every evolution algorithm, in the\nsense of Valiant (2007; 2009), can be converted using the Correlational Query\ntechnique of Feldman (2008), into such a drift resistant algorithm. For certain\nevolutionary algorithms, such as for Boolean conjunctions, we give bounds on\nthe rates of drift that they can resist. We develop some new evolution\nalgorithms that are resistant to significant drift. In particular, we give an\nalgorithm for evolving linear separators over the spherically symmetric\ndistribution that is resistant to a drift rate of O(\\epsilon /n), and another\nalgorithm over the more general product normal distributions that resists a\nsmaller drift rate.\n  The above translation result can be also interpreted as one on the robustness\nof the notion of evolvability itself under changes of definition. As a second\nresult in that direction we show that every evolution algorithm can be\nconverted to a quasi-monotonic one that can evolve from any starting point\nwithout the performance ever dipping significantly below that of the starting\npoint. This permits the somewhat unnatural feature of arbitrary performance\ndegradations to be removed from several known robustness translations.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2010 22:58:53 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Kanade", "Varun", ""], ["Valiant", "Leslie G.", ""], ["Vaughan", "Jennifer Wortman", ""]]}, {"id": "1005.3579", "submitter": "Xi Chen", "authors": "Xi Chen and Seyoung Kim and Qihang Lin and Jaime G. Carbonell and Eric\n  P. Xing", "title": "Graph-Structured Multi-task Regression and an Efficient Optimization\n  Method for General Fused Lasso", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a structured multi-task regression, where\nthe output consists of multiple responses that are related by a graph and the\ncorrelated response variables are dependent on the common inputs in a sparse\nbut synergistic manner. Previous methods such as l1/l2-regularized multi-task\nregression assume that all of the output variables are equally related to the\ninputs, although in many real-world problems, outputs are related in a complex\nmanner. In this paper, we propose graph-guided fused lasso (GFlasso) for\nstructured multi-task regression that exploits the graph structure over the\noutput variables. We introduce a novel penalty function based on fusion penalty\nto encourage highly correlated outputs to share a common set of relevant\ninputs. In addition, we propose a simple yet efficient proximal-gradient method\nfor optimizing GFlasso that can also be applied to any optimization problems\nwith a convex smooth loss and the general class of fusion penalty defined on\narbitrary graph structures. By exploiting the structure of the non-smooth\n''fusion penalty'', our method achieves a faster convergence rate than the\nstandard first-order method, sub-gradient method, and is significantly more\nscalable than the widely adopted second-order cone-programming and\nquadratic-programming formulations. In addition, we provide an analysis of the\nconsistency property of the GFlasso model. Experimental results not only\ndemonstrate the superiority of GFlasso over the standard lasso but also show\nthe efficiency and scalability of our proximal-gradient method.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2010 01:59:42 GMT"}], "update_date": "2010-05-21", "authors_parsed": [["Chen", "Xi", ""], ["Kim", "Seyoung", ""], ["Lin", "Qihang", ""], ["Carbonell", "Jaime G.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1005.3681", "submitter": "Ohad Shamir", "authors": "Shai Shalev-Shwartz, Ohad Shamir and Karthik Sridharan", "title": "Learning Kernel-Based Halfspaces with the Zero-One Loss", "comments": "This is a full version of the paper appearing in the 23rd\n  International Conference on Learning Theory (COLT 2010). Compared to the\n  previous arXiv version, this version contains some small corrections in the\n  proof of Lemma 3 and in appendix A", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and analyze a new algorithm for agnostically learning\nkernel-based halfspaces with respect to the \\emph{zero-one} loss function.\nUnlike most previous formulations which rely on surrogate convex loss functions\n(e.g. hinge-loss in SVM and log-loss in logistic regression), we provide finite\ntime/sample guarantees with respect to the more natural zero-one loss function.\nThe proposed algorithm can learn kernel-based halfspaces in worst-case time\n$\\poly(\\exp(L\\log(L/\\epsilon)))$, for $\\emph{any}$ distribution, where $L$ is a\nLipschitz constant (which can be thought of as the reciprocal of the margin),\nand the learned classifier is worse than the optimal halfspace by at most\n$\\epsilon$. We also prove a hardness result, showing that under a certain\ncryptographic assumption, no algorithm can learn kernel-based halfspaces in\ntime polynomial in $L$.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2010 12:39:56 GMT"}, {"version": "v2", "created": "Sun, 1 Aug 2010 08:31:29 GMT"}], "update_date": "2010-08-03", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Shamir", "Ohad", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1005.4298", "submitter": "Sameer Singh", "authors": "Sameer Singh and Michael Wick and Andrew McCallum", "title": "Distantly Labeling Data for Large Scale Cross-Document Coreference", "comments": "16 pages, submitted to ECML 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-document coreference, the problem of resolving entity mentions across\nmulti-document collections, is crucial to automated knowledge base construction\nand data mining tasks. However, the scarcity of large labeled data sets has\nhindered supervised machine learning research for this task. In this paper we\ndevelop and demonstrate an approach based on ``distantly-labeling'' a data set\nfrom which we can train a discriminative cross-document coreference model. In\nparticular we build a dataset of more than a million people mentions extracted\nfrom 3.5 years of New York Times articles, leverage Wikipedia for distant\nlabeling with a generative model (and measure the reliability of such\nlabeling); then we train and evaluate a conditional random field coreference\nmodel that has factors on cross-document entities as well as mention-pairs.\nThis coreference model obtains high accuracy in resolving mentions and entities\nthat are not present in the training data, indicating applicability to\nnon-Wikipedia data. Given the large amount of data, our work is also an\nexercise demonstrating the scalability of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2010 10:35:50 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Singh", "Sameer", ""], ["Wick", "Michael", ""], ["McCallum", "Andrew", ""]]}, {"id": "1005.4717", "submitter": "Xi Chen", "authors": "Xi Chen, Qihang Lin, Seyoung Kim, Jaime G. Carbonell, Eric P. Xing", "title": "Smoothing proximal gradient method for general structured sparse\n  regression", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS514 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 2, 719-752", "doi": "10.1214/11-AOAS514", "report-no": "IMS-AOAS-AOAS514", "categories": "stat.ML cs.LG math.OC stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating high-dimensional regression models\nregularized by a structured sparsity-inducing penalty that encodes prior\nstructural information on either the input or output variables. We consider two\nwidely adopted types of penalties of this kind as motivating examples: (1) the\ngeneral overlapping-group-lasso penalty, generalized from the group-lasso\npenalty; and (2) the graph-guided-fused-lasso penalty, generalized from the\nfused-lasso penalty. For both types of penalties, due to their nonseparability\nand nonsmoothness, developing an efficient optimization method remains a\nchallenging problem. In this paper we propose a general optimization approach,\nthe smoothing proximal gradient (SPG) method, which can solve structured sparse\nregression problems with any smooth convex loss under a wide spectrum of\nstructured sparsity-inducing penalties. Our approach combines a smoothing\ntechnique with an effective proximal gradient method. It achieves a convergence\nrate significantly faster than the standard first-order methods, subgradient\nmethods, and is much more scalable than the most widely used interior-point\nmethods. The efficiency and scalability of our method are demonstrated on both\nsimulation experiments and real genetic data sets.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2010 00:50:17 GMT"}, {"version": "v2", "created": "Sun, 21 Nov 2010 21:24:00 GMT"}, {"version": "v3", "created": "Sat, 26 Mar 2011 01:17:05 GMT"}, {"version": "v4", "created": "Fri, 29 Jun 2012 05:53:50 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Chen", "Xi", ""], ["Lin", "Qihang", ""], ["Kim", "Seyoung", ""], ["Carbonell", "Jaime G.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1005.5141", "submitter": "Pierre-Francois Marteau", "authors": "Pierre-Fran\\c{c}ois Marteau (IRISA), Sylvie Gibet (IRISA)", "title": "On Recursive Edit Distance Kernels with Application to Time Series\n  Classification", "comments": "14 pages", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems (2014)\n  1-14", "doi": "10.1109/TNNLS.2014.2333876", "report-no": "DRAFT-2013-PositiveDefiniteElasticKernels", "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes some extensions to the work on kernels dedicated to\nstring or time series global alignment based on the aggregation of scores\nobtained by local alignments. The extensions we propose allow to construct,\nfrom classical recursive definition of elastic distances, recursive edit\ndistance (or time-warp) kernels that are positive definite if some sufficient\nconditions are satisfied. The sufficient conditions we end-up with are original\nand weaker than those proposed in earlier works, although a recursive\nregularizing term is required to get the proof of the positive definiteness as\na direct consequence of the Haussler's convolution theorem. The classification\nexperiment we conducted on three classical time warp distances (two of which\nbeing metrics), using Support Vector Machine classifier, leads to conclude\nthat, when the pairwise distance matrix obtained from the training data is\n\\textit{far} from definiteness, the positive definite recursive elastic kernels\noutperform in general the distance substituting kernels for the classical\nelastic distances we have tested.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2010 18:11:15 GMT"}, {"version": "v10", "created": "Wed, 27 Nov 2013 07:57:25 GMT"}, {"version": "v11", "created": "Thu, 5 Dec 2013 14:04:32 GMT"}, {"version": "v12", "created": "Mon, 26 May 2014 06:17:30 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2010 12:45:28 GMT"}, {"version": "v3", "created": "Tue, 7 Dec 2010 11:02:10 GMT"}, {"version": "v4", "created": "Mon, 3 Jan 2011 10:32:07 GMT"}, {"version": "v5", "created": "Wed, 6 Feb 2013 16:32:52 GMT"}, {"version": "v6", "created": "Mon, 27 May 2013 08:58:31 GMT"}, {"version": "v7", "created": "Tue, 30 Jul 2013 09:21:26 GMT"}, {"version": "v8", "created": "Thu, 1 Aug 2013 14:41:48 GMT"}, {"version": "v9", "created": "Mon, 25 Nov 2013 20:31:46 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Marteau", "Pierre-Fran\u00e7ois", "", "IRISA"], ["Gibet", "Sylvie", "", "IRISA"]]}, {"id": "1005.5170", "submitter": "Pantelis Bouboulis", "authors": "P. Bouboulis", "title": "Wirtinger's Calculus in general Hilbert Spaces", "comments": "Report completed for the department of Informatics and\n  Telecommunications of the University of Athens", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present report, has been inspired by the need of the author and its\ncolleagues to understand the underlying theory of Wirtinger's Calculus and to\nfurther extend it to include the kernel case. The aim of the present manuscript\nis twofold: a) it endeavors to provide a more rigorous presentation of the\nrelated material, focusing on aspects that the author finds more insightful and\nb) it extends the notions of Wirtinger's calculus on general Hilbert spaces\n(such as Reproducing Hilbert Kernel Spaces).\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2010 16:07:25 GMT"}], "update_date": "2010-06-02", "authors_parsed": [["Bouboulis", "P.", ""]]}, {"id": "1005.5197", "submitter": "Aleksandrs Slivkins", "authors": "Aleksandrs Slivkins, Filip Radlinski and Sreenivas Gollapudi", "title": "Ranked bandits in metric spaces: learning optimally diverse rankings\n  over large document collections", "comments": "This is the full version of a paper in ICML 2010, with full proofs\n  and a significantly revised presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most learning to rank research has assumed that the utility of different\ndocuments is independent, which results in learned ranking functions that\nreturn redundant results. The few approaches that avoid this have rather\nunsatisfyingly lacked theoretical foundations, or do not scale. We present a\nlearning-to-rank formulation that optimizes the fraction of satisfied users,\nwith several scalable algorithms that explicitly takes document similarity and\nranking context into account. Our formulation is a non-trivial common\ngeneralization of two multi-armed bandit models from the literature: \"ranked\nbandits\" (Radlinski et al., ICML 2008) and \"Lipschitz bandits\" (Kleinberg et\nal., STOC 2008). We present theoretical justifications for this approach, as\nwell as a near-optimal algorithm. Our evaluation adds optimizations that\nimprove empirical performance, and shows that our algorithms learn orders of\nmagnitude more quickly than previous approaches.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2010 00:37:22 GMT"}, {"version": "v2", "created": "Sat, 1 Sep 2012 22:23:31 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["Slivkins", "Aleksandrs", ""], ["Radlinski", "Filip", ""], ["Gollapudi", "Sreenivas", ""]]}, {"id": "1005.5253", "submitter": "Sergio Guadarrama", "authors": "Sergio Guadarrama (1) and David P. Pancho (1) ((1) European Centre for\n  Soft Computing)", "title": "Using Soft Constraints To Learn Semantic Models Of Descriptions Of\n  Shapes", "comments": "8 pages, 8 figures, WCCI'10 Conference", "journal-ref": null, "doi": null, "report-no": "FSC 2009-22", "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contribution of this paper is to provide a semantic model (using soft\nconstraints) of the words used by web-users to describe objects in a language\ngame; a game in which one user describes a selected object of those composing\nthe scene, and another user has to guess which object has been described. The\ngiven description needs to be non ambiguous and accurate enough to allow other\nusers to guess the described shape correctly.\n  To build these semantic models the descriptions need to be analyzed to\nextract the syntax and words' classes used. We have modeled the meaning of\nthese descriptions using soft constraints as a way for grounding the meaning.\n  The descriptions generated by the system took into account the context of the\nobject to avoid ambiguous descriptions, and allowed users to guess the\ndescribed object correctly 72% of the times.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2010 09:41:50 GMT"}], "update_date": "2010-05-31", "authors_parsed": [["Guadarrama", "Sergio", ""], ["Pancho", "David P.", ""]]}, {"id": "1005.5337", "submitter": "James Demers", "authors": "Marten F. Byl, James T. Demers, and Edward A. Rietman", "title": "Using a Kernel Adatron for Object Classification with RCS Data", "comments": "This material is based upon work supported by US Army Space & Missile\n  Command under Contract Number W9113M-07-C-0204. Any opinions, findings and\n  conclusions or recommendations expressed in this material are those of the\n  authors and do not necessarily re flect the views of US Army Space & Missile\n  Command", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid identification of object from radar cross section (RCS) signals is\nimportant for many space and military applications. This identification is a\nproblem in pattern recognition which either neural networks or support vector\nmachines should prove to be high-speed. Bayesian networks would also provide\nvalue but require significant preprocessing of the signals. In this paper, we\ndescribe the use of a support vector machine for object identification from\nsynthesized RCS data. Our best results are from data fusion of X-band and\nS-band signals, where we obtained 99.4%, 95.3%, 100% and 95.6% correct\nidentification for cylinders, frusta, spheres, and polygons, respectively. We\nalso compare our results with a Bayesian approach and show that the SVM is\nthree orders of magnitude faster, as measured by the number of floating point\noperations.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2010 17:25:05 GMT"}], "update_date": "2010-05-31", "authors_parsed": [["Byl", "Marten F.", ""], ["Demers", "James T.", ""], ["Rietman", "Edward A.", ""]]}, {"id": "1005.5462", "submitter": "Andri Mirzal", "authors": "Andri Mirzal and Masashi Furukawa", "title": "On the clustering aspect of nonnegative matrix factorization", "comments": "4 pages, no figure, to appear in ICEIE 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper provides a theoretical explanation on the clustering aspect of\nnonnegative matrix factorization (NMF). We prove that even without imposing\northogonality nor sparsity constraint on the basis and/or coefficient matrix,\nNMF still can give clustering results, thus providing a theoretical support for\nmany works, e.g., Xu et al. [1] and Kim et al. [2], that show the superiority\nof the standard NMF as a clustering method.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2010 15:27:16 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2010 10:40:53 GMT"}], "update_date": "2010-06-15", "authors_parsed": [["Mirzal", "Andri", ""], ["Furukawa", "Masashi", ""]]}, {"id": "1005.5556", "submitter": "Ridwan Al Iqbal", "authors": "Ridwan Al Iqbal", "title": "Empirical learning aided by weak domain knowledge in the form of feature\n  importance", "comments": "9 pages, 1 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard hybrid learners that use domain knowledge require stronger knowledge\nthat is hard and expensive to acquire. However, weaker domain knowledge can\nbenefit from prior knowledge while being cost effective. Weak knowledge in the\nform of feature relative importance (FRI) is presented and explained. Feature\nrelative importance is a real valued approximation of a feature's importance\nprovided by experts. Advantage of using this knowledge is demonstrated by IANN,\na modified multilayer neural network algorithm. IANN is a very simple\nmodification of standard neural network algorithm but attains significant\nperformance gains. Experimental results in the field of molecular biology show\nhigher performance over other empirical learning algorithms including standard\nbackpropagation and support vector machines. IANN performance is even\ncomparable to a theory refinement system KBANN that uses stronger domain\nknowledge. This shows Feature relative importance can improve performance of\nexisting empirical learning algorithms significantly with minimal effort.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2010 19:28:01 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2010 14:45:54 GMT"}], "update_date": "2010-06-04", "authors_parsed": [["Iqbal", "Ridwan Al", ""]]}, {"id": "1005.5581", "submitter": "Zhi-Hua Zhou", "authors": "Wei Wang, Zhi-Hua Zhou", "title": "Multi-View Active Learning in the Non-Realizable Case", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sample complexity of active learning under the realizability assumption\nhas been well-studied. The realizability assumption, however, rarely holds in\npractice. In this paper, we theoretically characterize the sample complexity of\nactive learning in the non-realizable case under multi-view setting. We prove\nthat, with unbounded Tsybakov noise, the sample complexity of multi-view active\nlearning can be $\\widetilde{O}(\\log\\frac{1}{\\epsilon})$, contrasting to\nsingle-view setting where the polynomial improvement is the best possible\nachievement. We also prove that in general multi-view setting the sample\ncomplexity of active learning with unbounded Tsybakov noise is\n$\\widetilde{O}(\\frac{1}{\\epsilon})$, where the order of $1/\\epsilon$ is\nindependent of the parameter in Tsybakov noise, contrasting to previous\npolynomial bounds where the order of $1/\\epsilon$ is related to the parameter\nin Tsybakov noise.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2010 03:59:35 GMT"}, {"version": "v2", "created": "Fri, 29 Oct 2010 09:44:44 GMT"}], "update_date": "2010-11-01", "authors_parsed": [["Wang", "Wei", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1005.5603", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko (INRIA Lille)", "title": "On the Relation between Realizable and Nonrealizable Cases of the\n  Sequence Prediction Problem", "comments": "Earlier conference version appeared as: \"Sequence prediction in\n  realizable and non-realizable cases,\" Conference on Learning Theory (2010)\n  119-131", "journal-ref": "Journal of Machine Learning Research, vol. 12: 2161-2180, 2011", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sequence $x_1,\\dots,x_n,\\dots$ of discrete-valued observations is generated\naccording to some unknown probabilistic law (measure) $\\mu$. After observing\neach outcome, one is required to give conditional probabilities of the next\nobservation. The realizable case is when the measure $\\mu$ belongs to an\narbitrary but known class $\\mathcal C$ of process measures. The non-realizable\ncase is when $\\mu$ is completely arbitrary, but the prediction performance is\nmeasured with respect to a given set $\\mathcal C$ of process measures. We are\ninterested in the relations between these problems and between their solutions,\nas well as in characterizing the cases when a solution exists and finding these\nsolutions. We show that if the quality of prediction is measured using the\ntotal variation distance, then these problems coincide, while if it is measured\nusing the expected average KL divergence, then they are different. For some of\nthe formalizations we also show that when a solution exists, it can be obtained\nas a Bayes mixture over a countable subset of $\\mathcal C$. We also obtain\nseveral characterization of those sets $\\mathcal C$ for which solutions to the\nconsidered problems exist. As an illustration to the general results obtained,\nwe show that a solution to the non-realizable case of the sequence prediction\nproblem exists for the set of all finite-memory processes, but does not exist\nfor the set of all stationary processes.\n  It should be emphasized that the framework is completely general: the\nprocesses measures considered are not required to be i.i.d., mixing,\nstationary, or to belong to any parametric family.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2010 06:58:11 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2011 12:04:30 GMT"}, {"version": "v3", "created": "Sat, 27 Dec 2014 00:16:49 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Ryabko", "Daniil", "", "INRIA Lille"]]}]